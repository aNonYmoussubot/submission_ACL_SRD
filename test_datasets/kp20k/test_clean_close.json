[
  {
    "title": "a feedback vertex set of 2-degenerate graphs.",
    "abstract": "a feedback vertex set of a graph g is a set s of its vertices such that the subgraph induced by v(g)?s is a forest. the cardinality of a minimum feedback vertex set of g is denoted by ?(g). a graph g is 2-degenerate if each subgraph g? of g has a vertex v such that dg?(v)?2. in this paper, we prove that ?(g)?2n/5 for any 2-degenerate n-vertex graph g and moreover, we show that this bound is tight. as a consequence, we derive a polynomial time algorithm, which for a given 2-degenerate n-vertex graph returns its feedback vertex set of cardinality at most 2n/5.",
    "present_kp": [
      "feedback vertex set",
      "2-degenerate graphs"
    ],
    "absent_kp": [
      "decycling set"
    ]
  },
  {
    "title": "hybrid analytical modeling of pending cache hits, data prefetching, and mshrs.",
    "abstract": "this article proposes techniques to predict the performance impact of pending cache hits, hardware prefetching, and miss status holding register resources on superscalar microprocessors using hybrid analytical models. the proposed models focus on timeliness of pending hits and prefetches and account for a limited number of mshrs. they improve modeling accuracy of pending hits by 3.9x and when modeling data prefetching, a limited number of mshrs, or both, these techniques result in average errors of 9.5% to 17.8%. the impact of non-uniform dram memory latency is shown to be approximated well by using a moving average of memory access latency.",
    "present_kp": [
      "performance",
      "analytical modeling",
      "pending hit",
      "data prefetching",
      "miss status holding register"
    ],
    "absent_kp": []
  },
  {
    "title": "autoimmune polyendocrinopathy candidiasis ectodermal dystrophy: known and novel aspects of the syndrome.",
    "abstract": "autoimmune polyendocrinopathy candidiasis ectodermal dystrophy (apeced) is a monogenic autosomal recessive disease caused by mutations in the autoimmune regulator (aire) gene and, as a syndrome, is characterized by chronic mucocutaneous candidiasis and the presentation of various autoimmune diseases. during the last decade, research on apeced and aire has provided immunologists with several invaluable lessons regarding tolerance and autoimmunity. this review describes the clinical and immunological features of apeced and discusses emerging alternative models to explain the pathogenesis of the disease.",
    "present_kp": [
      "apeced",
      "aire",
      "chronic mucocutaneous candidiasis"
    ],
    "absent_kp": [
      "il-17",
      "il-22"
    ]
  },
  {
    "title": "numerical solution of a three-dimensional solidification problem in aluminium casting.",
    "abstract": "in this paper, we consider an enthalpy formulation for a two-phase stefan problem arising from the solidification of aluminium during casting process. we solve this free boundary problem in a time varying three-dimensional domain and consider convective heat transfer in the liquid phase. the resulting equations are discretized using a characteristics method in time and a finite element method in space, and we propose a numerical algorithm to solve the obtained nonlinear discretized problem. finally, numerical results are given which are compared with industrial experimental measurements.",
    "present_kp": [
      "casting",
      "finite element"
    ],
    "absent_kp": [
      "thermal",
      "conduction",
      "convection"
    ]
  },
  {
    "title": "definition and recognition of rib features in aircraft structural part.",
    "abstract": "in this research, a new type of manufacturing feature that is commonly observed in aircraft structural parts, known as ribs, is defined and implemented using the object-oriented software engineering approach. the rib feature type is defined as a set of constrained and adjacent faces of a part which are associated with a set of specific rib machining operations. computerized numerical control (cnc) operation experience and the machining knowledge are leveraged by analysing typical geometry interactions when generating machining tool paths where such knowledge and experience are abstracted as rules of process planning. then those abstracted machining process rules are implemented in a feature recognition algorithm on top of an existing and holistic attribute adjacency graph solution to extract seed faces, identify individual local rib elements and further cluster these newly identified local rib elements into groups for the ease of machining operations. out of the potentially different combinations of local rib elements, those optimised cluster groups are merged into the top-level rib features. the enhanced recognition algorithm is presented in details. a pilot system has already been developed and applied for machining many advanced aircraft structural parts in a large aircraft manufacturer. observations and conclusions are presented at the end.",
    "present_kp": [
      "feature recognition",
      "rib",
      "aircraft structural part"
    ],
    "absent_kp": [
      "machining feature"
    ]
  },
  {
    "title": "an algebraic approach to guarantee harmonic balance method using grobner base.",
    "abstract": "harmonic balance (hb) method is well known principle for analyzing periodic oscillations on nonlinear networks and systems. because the hb method has a truncation error, approximated solutions have been guaranteed by error bounds. however, its numerical computation is very time-consuming compared with solving the hb equation. this paper proposes an algebraic representation of the error bound using grobner base. the algebraic representation enables to decrease the computational cost of the error bound considerably. moreover, using singular points of the algebraic representation, we can obtain accurate break points of the error bound by collisions.",
    "present_kp": [
      "harmonic balance method",
      "error bound",
      "grobner base",
      "algebraic representation",
      "singular point"
    ],
    "absent_kp": [
      "quadratic approximation"
    ]
  },
  {
    "title": "a graph coloring based tdma scheduling algorithm for wireless sensor networks.",
    "abstract": "wireless sensor networks should provide with valuable service, which is called service-oriented requirement. to meet this need, a novel distributed graph coloring based time division multiple access scheduling algorithm (gcsa), considering real-time performance for clustering-based sensor network, is proposed in this paper, to determine the smallest length of conflict-free assignment of timeslots for intra-cluster transmissions. gcsa involves two phases. in coloring phase, networks are modeled using graph theory, and a distributed vertex coloring algorithm, which is a distance-2 coloring algorithm and can get colors near to ((updelta +1)), is proposed to assign a color to each node in the network. then, in scheduling phase, each independent set is mapped to a unique timeslot according to the sets priority which is obtained by considering network structure. the experimental results indicate that gcsa can significantly decrease intra-cluster delay and increase intra-cluster throughput, which satisfies real-time performance as well as communication reliability.",
    "present_kp": [
      "tdma",
      "distributed",
      "graph coloring",
      "clustering",
      "real-time"
    ],
    "absent_kp": []
  },
  {
    "title": "building model as a service to support geosciences.",
    "abstract": "model as a service (maas) concept and architecture is introduced to support geoscience modeling. maas enables various geoscience models to be published as services that can be accessed through a simple web interface. maas automates the processes of configuring machines, setting up and running models, and managing model outputs. maas provides new guidance for geoscientists seeking solutions to address the computing demands for geoscience models.",
    "present_kp": [],
    "absent_kp": [
      "cloud computing",
      "web service",
      "geospatial data",
      "model web",
      "earthcube",
      "big data"
    ]
  },
  {
    "title": "shot change detection using scene-based constraint.",
    "abstract": "a key step for managing a large video database is to partition the video sequences into shots. past approaches to this problem tend to confuse gradual shot changes with changes caused by smooth camera motions. this is in part due to the fact that camera motion has not been dealt with in a more fundamental way. we propose an approach that is based on a physical constraint used in optical flow analysis, namely, the total brightness of a scene point across two frames should remain constant if the change across two frames is a result of smooth camera motion. since the brightness constraint would be violated across a shot change, the detection can be based on detecting the violation of this constraint. it is robust because it uses only the qualitative aspect of the brightness constraint-detecting a scene change rather than estimating the scene itself. moreover, by tapping on the significant know-how in using this constraint, the algorithm's robustness is further enhanced. experimental results are presented to demonstrate the performance of various algorithms. it was shown that our algorithm is less likely to interpret gradual camera motions as shot changes, resulting in a significantly better precision performance than most other algorithms.",
    "present_kp": [
      "shot change detection",
      "optical flow"
    ],
    "absent_kp": [
      "video segmentation"
    ]
  },
  {
    "title": "tail asymptotics for hol priority queues handling a large number of independent stationary sources.",
    "abstract": "in this paper we study the asymptotics of the tail of the buffer occupancy distribution in buffers accessed by a large number of stationary independent sources and which are served according to a strict hol priority rule. as in the case of single buffers, the results are valid for a very general class of sources which include long-range dependent sources with bounded instantaneous rates. we first consider the case of two buffers with one of them having strict priority over the other and we obtain asymptotic upper bound for the buffer tail probability for lower priority buffer. we discuss the conditions to have asymptotic equivalents. the asymptotics are studied in terms of a scaling parameter which reflects the server speed, buffer level and the number of sources in such a way that the ratios remain constant. the results are then generalized to the case of m buffers which leads to the source pooling idea. we conclude with numerical validation of our formulae against simulations which show that the asymptotic bounds are tight. we also show that the commonly suggested reduced service rate approximation can give extremely low estimates.",
    "present_kp": [
      "priority queues"
    ],
    "absent_kp": [
      "bahadur-rao theorem",
      "large deviations",
      "cell loss",
      "stationary processes",
      "tail distributions"
    ]
  },
  {
    "title": "a variant of parallel plane sweep algorithm for multicore systems.",
    "abstract": "parallel algorithms used in very large scale integration physical design bring significant challenges for their efficient and effective design and implementation. the rectangle intersection problem is a subset of the plane sweep problem, a topic of computational geometry and a component in design rule checking, parasitic resistance-capacitance extraction, and mask processing flows. a variant of a plane sweep algorithm that is embarrassingly parallel and therefore easily scalable on multicore machines and clusters, while exceeding the best-known parallel plane sweep algorithms on real-world tests, is presented in this letter.",
    "present_kp": [
      "computational geometry",
      "multicore",
      "physical design",
      "plane sweep",
      "rectangle intersection"
    ],
    "absent_kp": []
  },
  {
    "title": "the antecedents and consequents of user perceptions in information technology adoption.",
    "abstract": "a common theme underlying various models that explain information technology adoption is the inclusion of perceptions of an innovation as key independent variables. although a fairly significant body of research that empirically tests these models is now in existence, some questions with regard to both the antecedents as well as the consequents of perceptions remain unanswered. this paper reports the results of a field study examining adoption of an information technology innovation represented by an expert systems application. two research objectives that have both theoretical and practical relevance motivated and guided the study. one, the study challenges an assumption which is implicit in technology acceptance models: that of the non-existence of moderating influences on the relationship between perceptions and adoption decisions. specifically, the study examines the effects of an important moderating influence personal innovativeness on this relationship. two, the study seeks to shed further light on the determinants of perceptions by examining the relative efficacy of mass media and interpersonal communication channels in facilitating perception development. theoretical and practical implications that follow from the results are discussed.",
    "present_kp": [
      "information technology adoption",
      "personal innovativeness",
      "communication channels"
    ],
    "absent_kp": [
      "expert system adoption"
    ]
  },
  {
    "title": "improving classification with latent variable models by sequential constraint optimization.",
    "abstract": "in this paper we propose a method to use multiple generative models with latent variables for classification tasks. the standard approach to use generative models for classification is to train a separate model for each class. a novel data point is then classified by the model that attributes the highest probability. the algorithm we propose modifies the parameters of the models to improve the classification accuracy. our approach is made computationally tractable by assuming that each of the models is deterministic, by which we mean that a data-point is associated to only a single latent state. the resulting algorithm is a variant of the support vector machine learning algorithm and in a limiting case the method is similar to the standard perceptron learning algorithm. we apply the method to two types of latent variable models. the first has a discrete latent state space and the second, principal component analysis, has a continuous latent state space. we compare the effectiveness of both approaches on a handwritten digit recognition problem and on a satellite image recognition problem.",
    "present_kp": [
      "latent variable models"
    ],
    "absent_kp": [
      "semi-supervised learning",
      "support vector machines",
      "pca",
      "vector quantization",
      "image and character recognition"
    ]
  },
  {
    "title": "a framework for a real time intelligent and interactive brain computer interface.",
    "abstract": "a framework for a real time implementation of a brain computer interface. implementation & comparison of different feature extraction methods and classifiers. accuracy & processing time comparison for detection of event related potentials-erp. an implementation of a prototype system using the proposed bci framework. real time eeg data collection and classification of erps using hex-o-speller.",
    "present_kp": [
      "data collection",
      "classification"
    ],
    "absent_kp": [
      "electroencephalography ",
      "braincomputer interface ",
      "event-related potentials "
    ]
  },
  {
    "title": "characterizing output processes of e-m/e-k/1 queues.",
    "abstract": "our goal is to study which conditions of the output process of a queue preserve the increasing failure rate (ifr) property in the interdeparture time. we found that the interdeparture time does not always preserve the ifr property, even if the interarrival time and service time are both erlang distributions with ifr. we give a theoretical analysis and present numerical results of e-m/e-k/1 queues. we show, by numerical examples, that the interdeparture time of e-m/e-k/1 retains the ifr property if m >= k.",
    "present_kp": [
      "ifr",
      "erlang distribution"
    ],
    "absent_kp": [
      "departure process",
      "ph/g/1",
      "queueing theory"
    ]
  },
  {
    "title": "a low-complexity down-mixing structure on quadraphonic headsets for surround audio.",
    "abstract": "this work presents a four-channel headset achieving a 5.1-channel-like hearing experience using a low-complexity head-related transfer function (hrtf) model and a simplified reverberator. the proposed down-mixing architecture enhances the sound localization capability of a headset using the hrtf and by simulating multiple sound reflections in a room using moorer's reverberator. since the hrtf has large memory and computation requirements, the common-acoustical-pole and zero (capz) model can be used to reshape the lower-order hrtf model. from a power consumption viewpoint, the capz model reduces computation complexity by approximately 40%. the subjective listening tests in this study shows that the proposed four-channel headset performs much better than stereo headphones. on the other hand, the four-channel headset that can be implemented by off-the-shelf components preserves the privacy with low cost.",
    "present_kp": [
      "surround audio",
      "head-related transfer function"
    ],
    "absent_kp": [
      "virtual loudspeaker",
      "reverberation"
    ]
  },
  {
    "title": "security personalization for internet and web services.",
    "abstract": "the growth of the internet has been accompanied by the growth of internet services (e.g., e-commerce, e-health). this proliferation of services and the increasing attacks on them by malicious individuals have highlighted the need for service security. the security requirements of an internet or pleb service may be specified in a security policy. the provider of the service is then responsible.,for implementing the security measures contained in the policy. however, a service customer or consumer may have security preferences that are not reflected in the provider security policy in order for set-vice providers to attract and retain customers, as well as reach a wider market, a way of personalizing a security policy to a particular customer is needed we derive the content of an internet or web service security policy and propose a flexible security personalization approach that will allow an internet or web service provider and customer to negotiate to an agreed-upon personalized security policy. in addition, we present two application examples of security policy personalization, and overview the design of our security personalization prototype.",
    "present_kp": [
      "internet services",
      "personalization",
      "security",
      "security policy",
      "web services"
    ],
    "absent_kp": [
      "negotiation"
    ]
  },
  {
    "title": "two efficient synchronous double left right arrow asynchronous converters well-suited for networks-on-chip in gals architectures.",
    "abstract": "this paper presents two high-throughput, low-latency converters that can be used to convert synchronous communication protocol to asynchronous one and vice versa. we have designed these two hardware components to be used in a globally asynchronous locally synchronous clusterized multi-processor system-on-chip communicating by a fully asynchronous network-on-chip. the proposed architecture is rather generic, and allows the system designer to make various trade-offs between latency and robustness, depending on the selected synchronizer. we have physically implemented the two converters with portable alliance cmos standard cell library and evaluated the architectures by spice simulation for a 90 nm cmos fabrication process.",
    "present_kp": [
      "globally asynchronous locally synchronous",
      "networks-on-chip"
    ],
    "absent_kp": [
      "multi-processor systems-on-chip",
      "synchronization",
      "asynchronous fifo"
    ]
  },
  {
    "title": "simulation aided design of organizational structures in manufacturing systems using structuring strategies.",
    "abstract": "this paper presents a simulation aided approach for designing organizational structures in manufacturing systems. the approach is based on a detailed modeling and characterization of the forecasted order program, especially of elementary processes, activity networks and manufacturing orders. under the use of the organization modeling system form, that has been developed at the ifab-institute of human and industrial engineering of the university of karlsruhe, structuring strategies-e.g., a process-oriented strategy can be applied in order to design organizational structures in manufacturing systems in a flexible and efficient way. following that, a dynamical analysis of the created manufacturing structures can be carried out with the simulation tool femos, that has also been developed at the ifab-institute. the evaluation module of femos enables to measure the designed solutions with the help of logistical-e.g., lead time degree and organizational-e.g., degree of autonomy key data. this evaluation is the basis for the identification of effective manufacturing systems and also of improvement potentialities. finally, a case study is presented in this paper designing and analyzing different organizational structures of a manufacturing system where gear boxes and robot grip arms were manufactured.",
    "present_kp": [],
    "absent_kp": [
      "modeling and simulation of manufacturing systems",
      "strategies for production systems design"
    ]
  },
  {
    "title": "explicit constructions of selectors and related combinatorial structures, with applications.",
    "abstract": "in this paper we present explicit constructions of several combinatorial objects: selectors [cgr00] and selective families [cggpr00], pseudo-random generators for proof systems [abrw00] and fixed waking schedules [gpp00]. as a result, we obtain almost optimal deterministic protocols for broadcasting in unknown directed radio networks [cgr00] and wake-up problem [gpp00]. we also show application of selectors (and its variants) to explicit construction of test sets for coin-weighting problems [dh00]. the parameters of our constructions come close to the best known non-constructive bounds. the constructions are achieved using a common technique, which could be of use for other problems.",
    "present_kp": [
      "object",
      "direct",
      "applications",
      "use",
      "paper",
      "test"
    ],
    "absent_kp": [
      "optimality",
      "randomization"
    ]
  },
  {
    "title": "selective finite element refinement in torsional problems based on the membrane analogy.",
    "abstract": "this work presents a selective finite element refinement strategy based on the h-refinement type, in the context of a posteriori error estimates considerations (error computed after the application of the proposed refining scheme), based on a graphical procedure to determine progressively better estimates for the maximum shearing stress in prismatic torsional members. it is structured in an integrated fortran code and delphi based environment to refine an initial arbitrary finite element mesh. the proposed procedure is founded on the membrane analogy that exists between membrane deflections and the torsion problem in the sense that the location of the membrane largest gradient drives the refining procedure. it is shown that multiple level application of the proposed method to two members with different cross sectional geometries with known analytic solutions leads to progressively more accurate estimates (< 1.0% error in most cases) for the maximum shearing stresses calculations. finally, the proposed method is applied to the torsional analysis of an l section member, showing that for this practical case the procedure results in a very accurate calculation as well.",
    "present_kp": [
      "membrane analogy",
      "torsion",
      "maximum shearing stress"
    ],
    "absent_kp": [
      "selective h-refinement",
      "finite elements"
    ]
  },
  {
    "title": "rns montgomery multiplication algorithm for duplicate processing of base transformations.",
    "abstract": "this paper proposes a new algorithm to achieve about two-times speedup of modular exponentiation which is implemented by montgomery multiplication based on residue number systems (rns). in rns montgomery multiplication, its performance is determined by two base transformations dominantly. for the purpose of realizing parallel processing of these base transformations, i.e. \"duplicate processing,\" we present two procedures of rns montgomery multiplication, in which rns bases a and b are interchanged, and perform them alternately in modular exponentiation iteration. in an investigation of implementation, 1.87-times speedup has been obtained for 1024-bit modular multiplication. the proposed rns montgomery multiplication algorithm has an advantage in achieving the performance corresponding to that the upper limit of the number of parallel processing units is doubled.",
    "present_kp": [
      "modular exponentiation",
      "residue number systems",
      "montgomery multiplication",
      "base transformation"
    ],
    "absent_kp": [
      "rsa cryptography"
    ]
  },
  {
    "title": "from quality in use to value in the world.",
    "abstract": "this paper argues that a focus on quality in use limits the potential of hci. it summarizes how novel approaches such as grounded design can let us go beyond usability to reveal the fit between designs and expected contexts of use. this however is still not enough. it cannot resolve dilemmas about what is and is not a usability problem, or when fit is or is not essential. such dilemmas can only be resolved by an understanding of the value that artifacts aim to deliver in the world. hci must move beyond contextual description to prescriptive approaches to value in the world.",
    "present_kp": [
      "value",
      "fit",
      "quality",
      "design"
    ],
    "absent_kp": []
  },
  {
    "title": "comparative study of family 2 gpcrs in fugu rubripes.",
    "abstract": "abstract: in this study, members of family 2 gpcrs, one of the largest families of receptors in vertebrates, were isolated and characterized in the genome of the japanese pufferfish, fugu rubripes, and compared with the orthologous genes in other vertebrates. phylogenetic analysis carried out with all vertebrate family 2 gpcr members indicated that calr/cgrpr and crf are the most divergent receptor group within this family and that the remaining members appear to originate from a common ancestral gene precursor.",
    "present_kp": [
      "family 2 gpcrs"
    ],
    "absent_kp": [
      "teleost",
      "duplicated genes",
      "evolution"
    ]
  },
  {
    "title": "blotto game-based low-complexity fair multiuser subcarrier allocation for uplink ofdma networks.",
    "abstract": "this article presents a subcarrier allocation scheme based on a blotto game (sabg) for orthogonal frequency-division multiple access (ofdma) networks where correlation between adjacent subcarriers is considered. in the proposed game, users simultaneously compete for subcarriers using a limited budget. in order to win as many good subcarriers as possible in this game, users are required to wisely allocate their budget. efficient power and budget allocation strategies are derived for users for obtaining optimal throughput. by manipulating the total budget available for each user, competitive fairness can be enforced for the sabg. in addition, the conditions to ensure the existence and uniqueness of nash equilibrium (ne) for the sabg are also established. an low-complexity algorithm that ensures convergence to ne is proposed. simulation results show that the proposed low-complexity sabg can allocate resources fairly and efficiently for both uncorrelated and correlated fading channels.",
    "present_kp": [
      "ofdma",
      "subcarrier allocation",
      "blotto game",
      "fairness",
      "complexity",
      "correlated fading"
    ],
    "absent_kp": [
      "efficiency"
    ]
  },
  {
    "title": "polymerization conditions influence on the thermomechanical and dielectric properties of unsaturated polyesterstyrene-copolymers.",
    "abstract": "the influence of different polymerization conditions like curing agent (mekp) amount and styrene content on the glass transition temperature, the relative dielectric constant as well as loss factors of unsaturated polyesterstyrene-polymer systems after solidification was investigated in depth. with respect to a high average molecular mass and vickers hardness a curing agent content of 3wt% is recommendable. increasing mekp-concentrations cause a slight elevation of the polymers relative dielectric constant as well as of the loss factor. regarding an easy film formation using tape casting a.o. higher styrene amounts lower the viscosity of the resin significantly, the relative dielectric constant and the loss factor decrease also. as an average value a relative dielectric constant of 3 under ambient conditions can be obtained.",
    "present_kp": [
      "dielectric properties"
    ],
    "absent_kp": [
      "unsaturated polyester resin",
      "embedded capacitors"
    ]
  },
  {
    "title": "an integration of online and pseudo-online information for cursive word recognition.",
    "abstract": "in this paper, we present a novel method to extract stroke order independent information from online data. this information, which we term pseudo-online, conveys relevant information on the offline representation of the word. based on this information, a combination of classification decisions from online and pseudo- online cursive word recognizers is performed to improve the recognition of online cursive words. one of the most valuable aspects of this approach with respect to similar methods that combine online and offline classifiers for word recognition is that the pseudo- online representation is similar to the online signal and, hence, word recognition is based on a single engine. results demonstrate that the pseudo- online representation is useful as the combination of classifiers perform better than those based solely on pure online information.",
    "present_kp": [
      "online",
      "offline",
      "cursive",
      "word recognition"
    ],
    "absent_kp": [
      "handwriting",
      "classifier combination"
    ]
  },
  {
    "title": "generation of quasi-gaussian pulses based on correlation techniques.",
    "abstract": "the gaussian pulses have been mostly used within communications, where some applications can be emphasized: mobile telephony (gsm), where gmsk signals are used, as well as the uwb communications, where short-period pulses based on gaussian waveform are generated. since the gaussian function signifies a theoretical concept, which cannot be accomplished from the physical point of view, this should be expressed by using various functions, able to determine physical implementations. new techniques of generating the gaussian pulse responses of good precision are approached, proposed and researched in this paper. the second and third order derivatives with regard to the gaussian pulse response are accurately generated. the third order derivates is composed of four individual rectangular pulses of fixed amplitudes, being easily to be generated by standard techniques. in order to generate pulses able to satisfy the spectral mask requirements, an adequate filter is necessary to be applied. this paper emphasizes a comparative analysis based on the relative error and the energy spectra of the proposed pulses.",
    "present_kp": [
      "correlation techniques",
      "gaussian pulse"
    ],
    "absent_kp": [
      "digital signal processing",
      "spectral analysis",
      "ultra-wideband"
    ]
  },
  {
    "title": "learning linear pca with convex semi-definite programming.",
    "abstract": "the aim of this paper is to learn a linear principal component using the nature of support vector machines (svms). to this end, a complete svm-like framework of linear pca (svpca) for deciding the projection direction is constructed, where new expected risk and margin are introduced. within this framework, a new semi-definite programming problem for maximizing the margin is formulated and a new definition of support vectors is established. as a weighted case of regular pca, our svpca coincides with the regular pca if all the samples play the same part in data compression. theoretical explanation indicates that svpca is based on a margin-based generalization bound and thus good prediction ability is ensured. furthermore, the robust form of svpca with a interpretable parameter is achieved using the soft idea in svms. the great advantage lies in the fact that svpca is a learning algorithm without local minima because of the convexity of the semi-definite optimization problems. to validate the performance of svpca, several experiments are conducted and numerical results have demonstrated that their generalization ability is better than that of regular pca. finally, some existing problems are also discussed.",
    "present_kp": [
      "support vector machines",
      "margin",
      "semi-definite programming"
    ],
    "absent_kp": [
      "principal component analysis",
      "statistical learning theory",
      "maximal margin algorithm",
      "robustness"
    ]
  },
  {
    "title": "the neighborhood auditing tool: a hybrid interface for auditing the umls.",
    "abstract": "the umls's integration of more than 100 source vocabularies, not necessarily consistent with one another, causes some inconsistencies. the purpose of auditing the umls is to detect such inconsistencies and to suggest how to resolve them while observing the requirement of fully representing the content of each source in the umls. a software tool, called the neighborhood auditing tool (nat), that facilitates umls auditing is presented. the nat supports \"neighborhood-based\" auditing, where, at any given time, an auditor concentrates on a single-focus concept and one of a variety of neighborhoods of its closely related concepts. typical diagrammatic displays of concept networks have a number of shortcomings, so the nat utilizes a hybrid diagram/text interface that features stylized neighborhood views which retain some of the best features of both the diagrammatic layouts and text windows while avoiding the shortcomings. the nat allows an auditor to display knowledge from both the metathesaurus (concept) level and the semantic network (semantic type) level. various additional features of the nat that support the auditing process are described. the usefulness of the nat is demonstrated through a group of case studies. its impact is tested with a study involving a select group of auditors.",
    "present_kp": [
      "software tool",
      "auditing tool"
    ],
    "absent_kp": [
      "unified medical language system",
      "auditing of terminologies",
      "auditing of ontologies",
      "auditing of the umls",
      "user interface",
      "hybrid diagram/text user interface"
    ]
  },
  {
    "title": "exclusion regions for optimization problems.",
    "abstract": "branch and bound methods for finding all solutions of a global optimization problem in a box frequently have the difficulty that subboxes containing no solution cannot be easily eliminated if they are close to the global minimum. this has the effect that near each global minimum, and in the process of solving the problem also near the currently best found local minimum, many small boxes are created by repeated splitting, whose processing often dominates the total work spent on the global search. this paper discusses the reasons for the occurrence of this so-called cluster effect, and how to reduce the cluster effect by defining exclusion regions around each local minimum found, that are guaranteed to contain no other local minimum and hence can safely be discarded. in addition, we will introduce a method for verifying the existence of a feasible point close to an approximate local minimum. these exclusion regions are constructed using uniqueness tests based on the krawczyk operator and make use of first, second and third order information on the objective and constraint functions.",
    "present_kp": [
      "global optimization",
      "uniqueness test",
      "exclusion region",
      "branch and bound",
      "cluster effect",
      "krawczyk operator"
    ],
    "absent_kp": [
      "validated enclosure",
      "existence test",
      "inclusion region",
      "kantorovich theorem",
      "backboxing",
      "affine invariant",
      "primary "
    ]
  },
  {
    "title": "learning about meetings.",
    "abstract": "most people participate in meetings almost every day, multiple times a day. the study of meetings is important, but also challenging, as it requires an understanding of social signals and complex interpersonal dynamics. our aim in this work is to use a data-driven approach to the science of meetings. we provide tentative evidence that: (i) it is possible to automatically detect when during the meeting a key decision is taking place, from analyzing only the local dialogue acts, (ii) there are common patterns in the way social dialogue acts are interspersed throughout a meeting, (iii) at the time key decisions are made, the amount of time left in the meeting can be predicted from the amount of time that has passed, (iv) it is often possible to predict whether a proposal during a meeting will be accepted or rejected based entirely on the language (the set of persuasive words) used by the speaker.",
    "present_kp": [
      "persuasive words"
    ],
    "absent_kp": [
      "analysis of meetings",
      "applications of machine learning"
    ]
  },
  {
    "title": "design and implementation of an expert interface system for integration of photogrammetric and geographic information systems for intelligent preparation and structuring of spatial data.",
    "abstract": "preparation of spatial data for geographic information system (gis) simultaneously during feature digitizing process from photogrammetric models reduces data editing phases after feature digitizing process. therefore, the problems, caused by separating spatial data production process from preparation of this data, are overcome as far as possible. to achieve this purpose, specialty and expertise required for spatial data structuring and preparation for gis, should be available in an interface system which establishes a direct connection between photogrammetric and gis systems. in this case, when a user digitizes a feature from a photogrammetric model, decision making process about the method of editing, structuring, layering, and storing of the feature in gis database, can be carried out by such an interface system. thus, according to the capabilities of expert systems for modeling the knowledge and deduction methods of experts, generating an expert interface system between photogrammetric and gis systems, offers a suitable solution for this integration. in this paper, the capabilities of expert systems for intelligent spatial data structuring and preparation simultaneously during feature digitizing process from photogrammetric models, have been investigated. also, design, implementation and test of an expert interface system for integration of photogrammetric and gis systems in order to take advantages of capabilities of both systems simultaneously as one integrated system, has been described.",
    "present_kp": [
      "expert system",
      "gis",
      "integration",
      "spatial data"
    ],
    "absent_kp": [
      "photogrammetry"
    ]
  },
  {
    "title": "wevan a mechanism for evidence creation and verification in vanets.",
    "abstract": "there are traffic situations (e.g. incorrect speeding tickets) in which a given vehicles driving behavior at some point in time has to be proved to a third party. vehicle-mounted sensorial devices are not suitable for this matter since they can be maliciously manipulated. however, surrounding vehicles may give their vision on another ones behavior. furthermore, these data may be shared with the affected vehicle through vanets. in this paper, a vanet-enabled data exchange mechanism called wevan is presented. the goal of this mechanism is to build and verify evidences based on surrounding vehicles (called witnesses) testimonies. due to the short-range nature of vanets, the connectivity to witnesses may be reduced with time the later their testimonies are requested, the lower the amount of witnesses may be. simulation results show that if testimonies are ordered 5s later, an average of 38 testimonies may be collected in highway scenarios. other intervals and road settings are studied as well.",
    "present_kp": [
      "driving behavior",
      "witness"
    ],
    "absent_kp": [
      "digital evidence",
      "vehicular ad-hoc networks "
    ]
  },
  {
    "title": "extensional normalisation and type-directed partial evaluation for typed lambda calculus with sums.",
    "abstract": "we present a notion of eta-long beta-normal term for the typed lambda calculus with sums and prove, using grothendieck logical relations, that every term is equivalent to one in normal form. based on this development we give the first type-directed partial evaluator that constructs normal forms of terms in this calculus.",
    "present_kp": [
      "typed lambda calculus",
      "grothendieck logical relations",
      "normalisation",
      "type-directed partial evaluation"
    ],
    "absent_kp": [
      "strong sums"
    ]
  },
  {
    "title": "yet another write-optimized dbms layer for flash-based solid state storage.",
    "abstract": "flash-based solid state storage (flashsss) has write-oriented problems such as low write throughput, and limited life-time. especially, flashssds have a characteristic vulnerable to random-writes, due to its control logic utilizing parallelism between the flash memory chips. in this paper, we present a write-optimized layer of dbmss to address the write-oriented problems of flashsss in on-line transaction processing environments. the layer consists of a write-optimized buffer, a corresponding log space, and an in-memory mapping table, closely associated with a novel logging scheme called incremental logging (icl). the icl scheme enables dbmss to reduce page-writes at the least expense of additional page-reads, while replacing random-writes into sequential-writes. through experiments, our approach demonstrated up-to an order of magnitude performance enhancement in i/o processing time compared to the original dbms, increasing the longevity of flashsss by approximately a factor of two.",
    "present_kp": [
      "icl",
      "ssd",
      "incremental logging",
      "flash memory"
    ],
    "absent_kp": [
      "write performance",
      "database"
    ]
  },
  {
    "title": "wrinkle development analysis in thin sail-like structures using mitc shell finite elements.",
    "abstract": "we propose a method of modelling sail type structures which captures the wrinkling behaviour of such structures. the method is validated through experimental and analytical test cases, particularly in terms of wrinkling prediction. an enhanced wrinkling index is proposed as a valuable measure characterizing the global wrinkling development on the deformed structure. the method is based on a pseudo-dynamic finite element procedure involving non-linear mitc shell elements. the major advantage compared to membrane models generally used for this type of analysis is that no ad hoc wrinkling model is required to control the stability of the structure. we demonstrate our approach to analyse the behaviour of various structures with spherical and cylindrical shapes, characteristic of downwind sails over a rather wide range of shape and constitutive parameters. in all cases convergence is reached and the overall flying shape is most adequately represented, which shows that our approach is a most valuable alternative to standard techniques to provide deeper insight into the physical behaviour. limitations appear only in some very special instances in which local wrinkling-related instabilities are extremely high and would require specific additional treatments, out of the scope of the present study.",
    "present_kp": [
      "wrinkling",
      "wrinkling index"
    ],
    "absent_kp": [
      "sail modelling",
      "mitc shells"
    ]
  },
  {
    "title": "cliques, holes and the vertex coloring polytope.",
    "abstract": "certain subgraphs of a given graph g restrict the minimum number chi(g) of colors that can be assigned to the vertices of g such that the endpoints of all edges receive distinct colors. some of such subgraphs are related to the celebrated strong perfect graph theorem. as it implies that every graph g contains a clique of size chi(g), or an odd hole or an odd anti-hole as an induced subgraph. in this paper, we investigate the impact of induced maximal cliques, odd holes and odd anti-holes on the polytope associated with a new 0-1 integer programming formulation of the graph coloring problem. we show that they induce classes of facet defining, inequalities.",
    "present_kp": [
      "integer programming"
    ],
    "absent_kp": [
      "combinatorial problems",
      "facets of polyhedra",
      "graph colorings"
    ]
  },
  {
    "title": "an experimental validation of a novel clustering approach to pwarx identification.",
    "abstract": "in this paper, the problem of clustering based procedure for the identification of piecewise auto-regressive exogenous (pwarx) models is addressed. this problem involves both the estimation of the parameters of the affine sub-models and the hyperplanes defining the partitions of the state-input regression. in fact, we propose the use of the chiu's clustering algorithm in order to overcome the main drawbacks of the existing methods which are the poor initialization and the presence of outliers. in addition, our approach is able to generate automatically the number of sub-models. simulation results are presented to illustrate the performance of the proposed method. an application of the developed approach to an olive oil esterification reactor is also suggested in order to validate simulation results.",
    "present_kp": [
      "clustering",
      "identification",
      "experimental validation"
    ],
    "absent_kp": [
      "hybrid systems",
      "pwarx models",
      "chiu's clustering technique"
    ]
  },
  {
    "title": "a primal-dual approximation algorithm for the asymmetric prize-collecting tsp.",
    "abstract": "we present a primal-dual ?log(n)?-approximation algorithm for the version of the asymmetric prize collecting traveling salesman problem, where the objective is to find a directed tour that visits a subset of vertices such that the length of the tour plus the sum of penalties associated with vertices not in the tour is as small as possible. the previous algorithm for the problem (v.h. nguyen and t.t nguyen in int. j. math. oper. res. 4(3):294301, 2012) which is not combinatorial, is based on the held-karp relaxation and heuristic methods such as the frieze et al.s heuristic (frieze et al. in networks 12:2339, 1982) or the recent asadpour et al.s heuristic for the atsp (asadpour etal. in 21st acm-siam symposium on discrete algorithms, 2010). depending on which of the two heuristics is used, it gives respectively 1+?log(n)? and (3+ 8frac{log(n)}{log(log(n))}) as an approximation ratio. our algorithm achieves an approximation ratio of ?log(n)? which is weaker than (3+ 8frac{log(n)}{log(log(n))}) but represents the first combinatorial approximation algorithm for the asymmetric prize-collecting tsp.",
    "present_kp": [
      "prize collecting traveling salesman",
      "approximation algorithm"
    ],
    "absent_kp": [
      "primal-dual algorithm"
    ]
  },
  {
    "title": "second order ambient intelligence.",
    "abstract": "this text attempts to describe an imagined future of ambient intelligence. it assumes that one day most of the current issues within ambient intelligence will be solved and that a second order ambient intelligence will be formulated, one with new research agendas. it describes several topics and ideas that might be part of this agenda and surmises on the prerequisites for this change.",
    "present_kp": [
      "second order ambient intelligence"
    ],
    "absent_kp": [
      "critique of ambient intelligence",
      "temporal design",
      "adaptive systems",
      "long-term behavior",
      "animal machine interaction",
      "critical futurism"
    ]
  },
  {
    "title": "pdms prism-glass optical coupling for surface plasmon resonance sensors based on mems technology.",
    "abstract": "a miniaturized surface plasmon resonance (spr) chip has been developed for biomedical and chemical analysis with low cost and high performance. the techniques of bulk silicon micromachining and polymer replication were used to fabricate the kretschmann spr sensor composed of a polydimethylsiloxane (pdms) prism, a coupling glass and microchannels. the plasmon properties of thin metal films were investigated theoretically based on fresnel analysis, with optical boundary conditions pertaining to the surface plasmon resonance at the gold/water and gold/air interfaces. the theoretical results show that difference in the refractive index (ri) between the pdms prism and the coupling glass layer affect the precision of the spr angle and the spr curve. meanwhile, the period of the interference fringe attaching on the spr curve increases with an increase in wavelength and a decrease in the refractive index of the coupling glass layer. the gold thickness of 50 nm is required while employing a fixed incident wavelength of 650 nm, to achieve optimum spr excitation conditions and the sensor sensitivity. the characteristics of this spr sensor were evaluated in the angular interrogation mode of employing the incident wavelength of 650 nm in air and water media, respectively. the obtained spr angles were approximately consistent with the theoretical ones.",
    "present_kp": [
      "polymer",
      "pdms"
    ],
    "absent_kp": [
      "surface plasmon resolance",
      "microfluidic"
    ]
  },
  {
    "title": "isogeometric analysis for strain field measurements.",
    "abstract": "in this paper, the potential of isogeometric analysis for strain field measurement by digital image correlation is investigated. digital image correlation (dic) is a full field kinematics measurement technique based on gray level conservation principle and the formulation we adopt allows for using arbitrary displacement bases. the high continuity properties of non-uniform rational b-spline (nurbs) functions are exploited herein as an additional regularization of the initial ill-posed problem. k-refinement is analyzed on an artificial test case where the proposed methodology is shown to outperform the usual finite element-based dic. finally a fatigue tensile test on a thin aluminum sheet is analyzed. strain localization occurs after a certain number of cycles and combination of nurbs into a dic algorithm clearly shows a great potential to improve the robustness of non-linear constitutive law identification.",
    "present_kp": [
      "digital image correlation",
      "nurbs",
      "strain field measurement",
      "isogeometric analysis"
    ],
    "absent_kp": []
  },
  {
    "title": "stable computation of the functional variation of the dirichletneumann operator.",
    "abstract": "this paper presents an accurate and stable numerical scheme for computation of the first variation of the dirichletneumann operator in the context of eulers equations for ideal free-surface fluid flows. the transformed field expansion methodology we use is not only numerically stable, but also employs a spectrally accurate fourier/chebyshev collocation method which delivers high-fidelity solutions. this implementation follows directly from the authors previous theoretical work on analyticity properties of functional variations of dirichletneumann operators. these variations can be computed in a number of ways, but we establish, via a variety of computational experiments, the superior effectiveness of our new approach as compared with another popular boundary perturbation algorithm (the method of operator expansions).",
    "present_kp": [
      "dirichletneumann operators",
      "functional variations"
    ],
    "absent_kp": [
      "boundary perturbation methods",
      "high-order/spectral methods",
      "water waves"
    ]
  },
  {
    "title": "optimal tool selection for 2.5d milling, part 1: a solid-modeling approach for construction of the voronoi mountain.",
    "abstract": "cutter selection is a critical subtask of machining process planning. in this two-part series, we develop a robust approach for the selection of an optimal set of milling cutters for a 2.5d generalized pocket. in the first article ( part 1), we present a solid modeling approach for the construction of the voronoi mountain for the pocket geometry, which is a 3d extension of the voronoi diagram. the major contributions of this work include: ( 1) the development of a robust and systematic procedure for construction of the voronoi mountain for a multiply-connected curvilinear polygon; and ( b) an extension of the voronoi mountain concept to handle open edges.",
    "present_kp": [
      "voronoi mountain ",
      "2.5d milling",
      "open edges"
    ],
    "absent_kp": [
      "cutter  selection",
      "solid modelling"
    ]
  },
  {
    "title": "evaluating the novelty of text-mined rules using lexical knowledge.",
    "abstract": "in this paper, we present a new method of estimating the novelty of rules discovered by data-mining methods using wordnet, a lexical knowledge-base of english words. we assess the novelty of a rule by the average semantic distance in a knowledge hierarchy between the words in the antecedent and the consequent of the rule - the more the average distance, more is the novelty of the rule. the novelty of rules extracted by the discotex text-mining system on amazon.com book descriptions were evaluated by both human subjects and by our algorithm. by computing correlation coefficients between pairs of human ratings and between human and automatic ratings, we found that the automatic scoring of rules based on our novelty measure correlates with human judgments about as well as human judgments correlate with one another. @text mining",
    "present_kp": [
      "novelty",
      "semantic distance",
      "knowledge hierarchy",
      "wordnet"
    ],
    "absent_kp": [
      "interesting rules"
    ]
  },
  {
    "title": "visor: vast independence system optimization routine.",
    "abstract": "an algorithm is sketched that generates all k maximal independent sets and all m minimal dependent sets of an arbitrary independence system, based on a set of cardinality n having at most 2(n) subsets. with access to an oracle that decides if a set is independent or not. because the algorithm generates all those sets, it solves the problems of finding all maximum independent and minimum dependent sets. those problems are known to be impossible to solve in general in time polynomial in n, k, and m, and they are np hard. the algorithm proposed and used is efficient in the sense that it requires only o(nk + m) or o(k + nm) visits to the oracle, the nonpolynomial part is only related to bitstring comparisons and the like, which can be performed rather quickly and, to some degree, in parallel on a sequential machine. this complexity compares favorably with another algorithm that is o(n(2)k(2)). the design of a computer routine that implements the algorithm in a highly optimized way is discussed. the routine behaves as expected, as is shown by numerical experiments on a range of randomly generated independence systems with n up to n = 34. application on an engineering design problem with n = 28 shows the routine requires almost 10(6) times less visits to the oracle than an exhaustive search, while the time spent in visiting the oracle is still significantly larger than that spent for all other computations together.",
    "present_kp": [
      "independence system",
      "maximal independent set"
    ],
    "absent_kp": [
      "combinatorial optimization",
      "input/output selection"
    ]
  },
  {
    "title": "superconvergence in high-order galerkin finite element methods.",
    "abstract": "in this paper, we shall use local estimates to give the superconvergence of high-order galerkin finite element method for the elliptic equation of second order with constant coefficients by using the symmetric technique and integral identity. we get improved superconvergence on the inner locally symmetric mesh with respect to a point x0 for rectangular and triangular meshes.",
    "present_kp": [
      "superconvergence"
    ],
    "absent_kp": [
      "integral identities",
      "locally symmetric meshes",
      "elliptic equations of second order"
    ]
  },
  {
    "title": "a review of design pattern mining techniques.",
    "abstract": "the quality of a software system highly depends on its architectural design. high quality software systems typically apply expert design experience which has been captured as design patterns. as demonstrated solutions to recurring problems, design patterns help to reuse expert experience in software system design. they have been extensively applied in the industry. mining the instances of design patterns from the source code of software systems can assist in the understanding of the systems and the process of re-engineering them. more importantly, it also helps to trace back to the original design decisions, which are typically missing in legacy systems. this paper presents a review on current techniques and tools for mining design patterns from source code or design of software systems. we classify different approaches and analyze their results in a comparative study. we also examine the disparity of the discovery results of different approaches and analyze possible reasons with some insight.",
    "present_kp": [
      "design pattern",
      "discovery"
    ],
    "absent_kp": [
      "reverse engineering"
    ]
  },
  {
    "title": "stabilization of second-order nonholonomic systems in canonical chained form.",
    "abstract": "stabilization of a class of second-order nonholonomic systems in canonical chained form is investigated in this paper. first, the models of two typical second-order nonholonomic systems, namely, a three-link planar manipulator with the third joint unactuated, and a kinematic redundant manipulator with all joints free and driven by forces/torques imposing on the end-effector, are presented and converted to second-order chained form by transformations of coordinate and input. a discontinuous control law is then proposed to stabilize all states of the system to the desired equilibrium point exponentially. computer simulation is given to show the effectiveness of the proposed controller.",
    "present_kp": [
      "second-order nonholonomic systems"
    ],
    "absent_kp": [
      "canonical second-order chained form",
      "underactuated manipulator",
      "discontinuous coordinate transformation",
      "discontinuous stabilization"
    ]
  },
  {
    "title": "truthful mechanisms for two-range-values variant of unrelated scheduling.",
    "abstract": "in this paper, we consider a restricted variant of the scheduling problem, where the machines are the strategic players. for this multi-parameter mechanism design problem, the only known truthful mechanisms use task independent allocation algorithms and only have approximation ratio o(m). lavi and swamy first use the cycle monotone condition and design a 3-approximation truthful mechanism for a two value variant in lavi, where the processing time of task j on machine i, say t(ij), can only be either a lower value l(j) or a higher value h(j). we consider a generalized variant. where t(ij) lies in [l(j), l(j)(1+epsilon)] boolean or [h(j), h(j)(1+epsilon)] and epsilon is a parameter satisfying some condition. we consider two special cases, case a when h(j)/l(j) > 2,for all(j) and case b when h(j)/l(j) <= 2, for all j, and give randomized truthful mechanisms with approximation ratio 4(1+epsilon) for both cases. based on these two cases' results, we are also able to deal with the general case of our two-range-values scheduling problem. we use a combination of two mechanisms, which is also a novel method in mechanism design for scheduling problems, and finally we give a randomized truthful mechanism with approximation ratio 7(1+epsilon). although the generalization seems a little incremental, we actually use a very novel technique in the key step of proving truthfulness for case a, as well as a new mechanism scheme for case b. besides, the results in this paper are the first truthful mechanisms with constant approximation ratios when a machine (player) can report infinitely possible values, which is quite different from the two value variant, in which only finite values are available. furthermore, together with lavi and swamy's work, our results suggest that such a task-dependent approach can really do much better for the scheduling unrelated machines problem.",
    "present_kp": [
      "truthful mechanism",
      "scheduling"
    ],
    "absent_kp": [
      "approximation algorithm"
    ]
  },
  {
    "title": "a note on the relationships among certified discrete log cryptosystems.",
    "abstract": "the certified discrete logarithm problem modulo p prime is a discrete logarithm problem under the conditions that the complete factorization of p - 1 is given and by which the base g is certified to be a primitive root mod p. for the cryptosystems based on the intractability of certified discrete logarithm problem, sakurai-shizuya showed that breaking the diffie-hellman key exchange scheme reduces to breaking the shamir 3-pass key transmission scheme with respect to the expected polynomial-time turing reducibility. in this paper, we show that we can remove randomness from the reduction above, and replace the reducibility with the polynomial-time many-one. since the converse reduction is known to hold with respect to the polynomial-time many-one reducibility, our result gives a stronger evidence for that the two schemes are completely equivalent as certified discrete log cryptosystems.",
    "present_kp": [
      "certified discrete logarithm problem",
      "primitive root"
    ],
    "absent_kp": [
      "order",
      "probabilistic reducibility",
      "deterministic reducibility"
    ]
  },
  {
    "title": "hydraulic performance of a large slanted axial-flow pump.",
    "abstract": "purpose - the pump of the taipuhe pump station, larger flow discharge, lower head, is one of the largest 150 slanted axial-flow pumps in the world. however, few studies have been done for the larger slanted axial-flow pump on safe operation. the purpose of this paper is to analyze the impeller elevation, unsteady flow, hydraulic thrust and the zero-head flow characteristics of the pump. design/methodology/approach - the flow field in and through the pump was analyzed numerically during the initial stages of the pump design process, then the entire flow passage through the pump was analyzed to calculate the hydraulic thrust to prevent damage to the bearings and improve the operating stability the zero-head pump flow characteristics were analyzed to ensure that the pump will work reliably at much lower heads. findings - the calculated results are in good agreement with experimental data for the pump elevation effects, the performance curve, pressure oscillations, hydraulic thrust and zero-head performance. research limitations/implications - since it is assumed that there is no gap between blades and shroud, gap cavitations are beyond the scope of the paper. originality/value - the paper indicates the slanted axial-flow pump characteristics including the characteristic curves, pressure fluctuations, hydraulic thrust and radial force for normal operating conditions and zero-head conditions. it shows how to guarantee the pump safety operating by computational fluid dynamics.",
    "present_kp": [
      "pumps",
      "fluid dynamics"
    ],
    "absent_kp": [
      "force measurement",
      "water supply engineering",
      "china"
    ]
  },
  {
    "title": "high-radix montgomery modular exponentiation on reconfigurable hardware.",
    "abstract": "it is widely recognized that security issues will play a crucial role in the majority of future computer and communication systems. central tools for achieving system security are cryptographic algorithms. this contribution proposes arithmetic architectures which are optimized for modern field programmable gate arrays (fpgas). the proposed architectures perform modular exponentiation with very long integers. this operation is at the heart of many practical public-key algorithms such as rsa and discrete logarithm schemes. we combine a high-radix montgomery modular multiplication algorithm with a new systolic array design. the designs are flexible, allowing any choice of operand and modulus. the new architecture also allows the use of high radices. unlike previous approaches, we systematically implement and compare several variants of our new architecture for different bit lengths. we provide absolute area and timing measures for each architecture. the results allow conclusions about the feasibility and time-space trade-offs of our architecture for implementation on commercially available fpgas. we found that 1,024-bit rsa decryption can be done in 3.1 ms with our fastest architecture.",
    "present_kp": [
      "montgomery",
      "fpga",
      "exponentiation",
      "rsa",
      "systolic array"
    ],
    "absent_kp": [
      "modular arithmetic"
    ]
  },
  {
    "title": "combined use of supervised and unsupervised learning for power system dynamic security mapping.",
    "abstract": "this paper proposes a new methodology which combines supervised and unsupervised learning for evaluating power system dynamic security. based on the concept of stability margin, pre-fault power system conditions are assigned to the output neurons on the two-dimensional grid with the growing hierarchical self-organizing map technique (ghsom) via supervised artificial neural networks (anns) which perform an estimation of post-fault power system state. the technique estimates the dynamic stability index that corresponds to the most critical value of synchronizing and damping torques of multimachine power systems. ann-based pattern recognition is carried out with the growing hierarchical self-organizing feature mapping in order to provide adaptive neural network architecture during its unsupervised training process. numerical tests, carried out on a ieee 9 bus power system are presented and discussed. the analysis using such method provides accurate results and improves the effectiveness of system security evaluation.",
    "present_kp": [
      "synchronizing and damping torques",
      "growing hierarchical self-organizing feature map",
      "supervised and unsupervised learning"
    ],
    "absent_kp": [
      "dynamic security assessment",
      "stability criteria"
    ]
  },
  {
    "title": "0.35 mu m cmos t/r switch for 2.4 ghz short range wireless applications.",
    "abstract": "this paper describes the design and implementation of a transmit/receive switch for 2.4 ghz ism band applications. the t/r switch is implemented in a 0.35 mum bulk cmos process and it occupies 150 mum . 170 mum of die area. a parasitic mosfet model including bulk resistance is used to optimize the physical dimensions of the transistors with regard to insertion loss and isolation. the measured insertion loss is 1.3 db without port matching. simulations using measured s-parameters indicate that an insertion loss of 0.8 db can be obtained with a conjugate match. the measured isolation is 42 db and the maximum transmit power is 16 dbm.",
    "present_kp": [
      "t/r switch"
    ],
    "absent_kp": [
      "mosfet switch",
      "rf cmos",
      "spdt switch"
    ]
  },
  {
    "title": "probabilistic equivalence checking of multiple-valued functions.",
    "abstract": "this paper describes a probabilistic method for verifying the equivalence of two multiple-valued functions. each function is hashed to an integer code by transforming it to a integer-valued polynomial and evaluating it for values of variables taken independently and uniformly at random from a finite field. since the polynomial is unique for a given function, if two hash codes are different, then the functions are not equivalent. however, if two hash codes are the same, the functions may or may not be equivalent, because different polynomials may happen to hash to the same code. thus, the method presented in this paper determines the equivalence of two functions with a known (small) probability of error, arising from collisions between inequivalent functions. such a method seems to be an attractive alternative for verifying functions that are too large to be handled by deterministic equivalence checking methods.",
    "present_kp": [
      "multiple-valued function",
      "equivalence checking"
    ],
    "absent_kp": [
      "probabilistic verification"
    ]
  },
  {
    "title": "a model and environment for improving multimedia scholarly reading practices.",
    "abstract": "the evolution of multimedia document production and diffusion technologies has lead to a significant spread of knowledge in form of pictures and recordings. however, scholarly reading tasks are still principally performed on textual contents. we argue that this is due to a lack of critical and structured tools: (1) to handle the wide spectrum of interpretive operations involved by the polymorphous scholarly reading process; (2) to perform these operations on a heterogeneous multimedia corpus. this firstly calls for identifying fundamental document requirements for such reading practices. then, we present a flexible model and a software environment which enable the reader to structure, annotate, link, fragment, compare, freely organise and spatially lay out documents, and to prepare the writing of their critical comment. we eventually discuss experiments with humanities scholars, and explore new academic reading practices which take advantage of document engineering principles such as multimedia document structuring, publication or sharing.",
    "present_kp": [
      "multimedia scholarly reading"
    ],
    "absent_kp": [
      "multimedia corpus modelling",
      "document annotation and structuring",
      "spatial hypertexts",
      "graphical user interfaces for critical reading"
    ]
  },
  {
    "title": "sensor selection for energy-efficient ambulatory medical monitoring.",
    "abstract": "epilepsy affects over three million americans of all ages. despite recent advances, more than 20% of individuals with epilepsy never achieve adequate control of their seizures. the use of a small, portable, non-invasive seizure monitor could benefit these individuals tremendously. however, in order for such a device to be suitable for long-term wear, it must be both comfortable and lightweight. typical state-of-the-art non-invasive seizure onset detection algorithms require 21 scalp electrodes to be placed on the head. these electrodes are used to generate 18 data streams, called channels. the large number of electrodes is inconvenient for the patient and processing 18 channels can consume a considerable amount of energy, a problem for a battery-powered device. in this paper, we describe an automated way to construct detectors that use fewer channels, and thus fewer electrodes. starting from an existing technique for constructing 18 channel patient-specific detectors, we use machine learning to automatically construct reduced channel detectors. we evaluate our algorithm on data from 16 patients used in an earlier study. on average, our algorithm reduced the number of channels from 18 to 4.6 while decreasing the mean fraction of seizure onsets detected from 99% to 97%. for 12 out of the 16 patients, there was no degradation in the detection rate. while the average detection latency increased from 7.8 s to 11.2 s, the average rate of false alarms per hour decreased from 0.35 to 0.19. we also describe a prototype implementation of a single channel eeg monitoring device built using off-the-shelf components, and use this implementation to derive an energy consumption model. using fewer channels reduced the average energy consumption by 69%, which amounts to a 3.3x increase in battery lifetime. finally, we show how additional energy savings can be realized by using a low-power screening detector to rule out segments of data that are obviously not seizures. though this technique does not reduce the number of electrodes needed, it does reduce the energy consumption by an additional 16%.",
    "present_kp": [
      "epilepsy",
      "ambulatory medical monitoring"
    ],
    "absent_kp": [
      "reducing energy consumption",
      "channel selection",
      "electroencephalography "
    ]
  },
  {
    "title": "process synchronization without long-term interlock.",
    "abstract": "a technique is presented for replacing long-term interlocking of shared data by the possible repetition of unprivileged code in case a version number (associated with the shared data) has been changed by another process. four principles of operating system architecture (which have desirable effects on the intrinsic reliability of a system) are presented; implementation of a system adhering to these principles requires that long-term lockout be avoided.",
    "present_kp": [
      "synchronization",
      "reliability",
      "code",
      "architecture",
      "operating system",
      "implementation",
      "process",
      "data",
      "case",
      "version",
      "effect"
    ],
    "absent_kp": [
      "sharing",
      "systems",
      "association"
    ]
  },
  {
    "title": "application of an artificial immune system-based fuzzy neural network to a rfid-based positioning system.",
    "abstract": "due to the rapid development of globalization, which makes supply chain management more complicated, more companies are applying radio frequency identification (rfid), in warehouse management. the obvious advantages of rfid are its ability to scan at high-speed, its penetration and memory. in addition to recycling, use of a rfid system can also reduce business costs, by indentifying the position of goods and picking carts. this study proposes an artificial immune system (ais)-based fuzzy neural network (fnn), to learn the relationship between the rfid signals and the picking cart's position. since the proposed network has the merits of both ais and fnn. it is able to avoid falling into the local optimum and possesses a learning capability. the results of the evaluation of the model show that the proposed ais-based fnn really can predict the picking cart position more precisely than conventional fnn and, unlike an artificial neural network, it is much easier to interpret the training results, since they are in the form of fuzzy if-then rules.",
    "present_kp": [
      "radio frequency identification ",
      "artificial immune system ",
      "fuzzy neural network "
    ],
    "absent_kp": [
      "genetic algorithms "
    ]
  },
  {
    "title": "single-dimension multidimensional software pipelining for loops.",
    "abstract": "traditionally, software pipelining is applied either to the innermost loop of a given loop nest or from the innermost loop to outer loops. this paper proposes a three-step approach, called single-dimension software pipelining (ssp), to software pipeline a loop nest at an arbitrary loop level that has a rectangular iteration space and contains no sibling inner loops in it. the first step identifies the most profitable loop level for software pipelining in terms of initiation rate, data reuse potential, or any other optimization criteria. the second step simplifies the multidimensional data-dependence graph (ddg) of the selected loop level into a one-dimensional ddg and constructs a one-dimensional (1d) schedule. based on the one-dimensional schedule, the third step derives a simple mapping function that specifies the schedule time for the operation instances in the multidimensional loop. the classical modulo scheduling is subsumed by ssp as a special case. ssp is also closely related to hyperplane scheduling, and, in fact, extends it to be resource constrained. we prove that ssp schedules are correct and at least as efficient as those schedules generated by traditional modulo scheduling methods. we extend ssp to schedule imperfect loop nests, which are most common at the instruction level. multiple initiation intervals are naturally allowed to improve execution efficiency. feasibility and correctness of our approach are verified by a prototype implementation in the orc compiler for the ia-64 architecture, tested with loop nests from livermore and spec2000 floating-point benchmarks. preliminary experimental results reveal that, compared to modulo scheduling, software pipelining at an appropriate loop level results in significant performance improvement. software pipelining is beneficial even with prior loop transformations.",
    "present_kp": [
      "software pipelining",
      "modulo scheduling",
      "loop transformation"
    ],
    "absent_kp": [
      "algorithms",
      "languages"
    ]
  },
  {
    "title": "a geometric-based method for recognizing overlapping polygonal-shaped and semi-transparent particles in gray tone images.",
    "abstract": "a geometric-based method is proposed to recognize the overlapping particles of different polygonal shapes such as rectangular, regular and/or irregular prismatic particles in a gray tone image. the first step consists in extracting the salient corners, identified by their locations and orientations, of the overlapping particles. although there are certain difficulties like the perspective geometric projection, out of focus, transparency and superposition of the studied particles. then, a new clustering technique is applied to detect the shape by grouping its correspondent salient corners according to the geometric properties of each shape. a simulation process is carried out for evaluating the performance of the proposed method. then, it is particularly applied on a real application of batch cooling crystallization of the ammonium oxalate in pure water. the experimental results show that the method is efficient to recognize the overlapping particles of different shapes and sizes.",
    "present_kp": [],
    "absent_kp": [
      "salient corner detection",
      "contour detection",
      "clustering method",
      "overlapping particles recognition"
    ]
  },
  {
    "title": "explicit dimension reduction and its applications.",
    "abstract": "we construct a small set of explicit linear transformations mapping r-n to r-t, where t = o(log(gamma(-1))epsilon(-2)), such that the l-2 norm of any vector in r-n is distorted by at most 1 +/- epsilon in at least a fraction of 1 - gamma of the transformations in the set. albeit the tradeoff between the size of the set and the success probability is suboptimal compared with probabilistic arguments, we nevertheless are able to apply our construction to a number of problems. in particular, we use it to construct an epsilon-sample (or pseudorandom generator) for linear threshold functions on sn-1 for epsilon = o(1). we also use it to construct an epsilon-sample for spherical digons in sn-1 for epsilon = o(1). this construction leads to an efficient oblivious derandomization of the goemans-williamson max-cut algorithm and similar approximation algorithms (i.e., we construct a small set of hyperplanes such that for any instance we can choose one of them to generate a good solution). our technique for constructing an epsilon-sample for linear threshold functions on the sphere is considerably different than previous techniques that rely on k-wise independent sample spaces.",
    "present_kp": [
      "dimension reduction",
      "pseudorandom generator",
      "linear threshold functions",
      "max-cut",
      "digons"
    ],
    "absent_kp": [
      "johnson lindenstrauss"
    ]
  },
  {
    "title": "capital one financial and a decade of experience with newly vulnerable markets: some propositions concerning the competitive advantage of new entrants.",
    "abstract": "market share and brand recognition have historically provided advantage to established players in mature industries. the success of capital one, an attacker in the mature credit card industry is therefore interesting, both to researchers and to executives developing strategies. a partial explanation is offered by the theory of newly vulnerable markets. the success of capital one can be partially attributed to its application of information-based strategies to several newly vulnerable markets, allowing it to target and retain the most profitable customers. these strategies sustained double-digit return on equity and double-digit increase in sales volume and profits every year of our study.",
    "present_kp": [
      "newly vulnerable markets",
      "capital one financial"
    ],
    "absent_kp": [
      "information-based strategy",
      "market entry",
      "differential pricing",
      "customer profitability gradient",
      "information economics"
    ]
  },
  {
    "title": "scene analysis and geometric homology.",
    "abstract": "during the last 10-12 years there has been a dramatic revival of interest in applied geometric problems. geometers have reconsidered a number of questions in infinitesimal mechanics, questions treated by j.c. maxwell and l. cremona in 1864-70, further developed under the banner of graphical statics , but left largely untouched since the end of the nineteenth century. at the same time, computer scientists have come to recognize that the tools of graphical statics and of applied projective geometry are fundamental to research in scene analysis. a good deal of the recent revival of interest is due to the efforts of the structural topology research group at the university of montreal. the work of this group, reported in the pages of the journal structural topology (and elsewhere), was a biproduct of research on infinitesimal mechanics, using methods derived from graphical statics, as well as from exterior algebra and its modern offspring, the doubilet-rota-stein double algebra . independently, huffman , duda and hart , and others recognized that maxwell's reciprocal figures could help in deciding whether a given plane image is the projection of a 3d polyhedral scene. more recently, sugihara and his colleagues in nagoya created what may be considered a pilot project for automated descriptive geometry. they wrote a software package capable of modifying a rough plane sketch, so as to make it a true projection of a 3d scene. the starting point of the during the last 10-12 years there has been a dramatic revival of interest in applied geometric problems. geometers have reconsidered a number of questions in infinitesimal mechanics, questions treated by j.c. maxwell and l. cremona in 1864-70, further developed under the banner of graphical statics , but left largely untouched since the end of the nineteenth century. at the same time, computer scientists have come to recognize that the tools of graphical statics and of applied projective geometry are fundamental to research in scene analysis. a good deal of the recent revival of interest is due to the efforts of the structural topology research group at the university of montreal. the work of this group, reported in the pages of the journal structural topology (and elsewhere), was a biproduct of research on infinitesimal mechanics, using methods derived from graphical statics, as well as from exterior a. independently, huffman , duda and hart , and others recognized that maxwell's reciprocal figures could help in deciding whether a given plane image is the projection of a 3d polyhedral scene. more recently, sugihara and his colleagues in nagoya created what may be considered a pilot project for automated descriptive geometry. they wrote a software package capable of modifying a rough plane sketch, so as to make it a true projection of a 3d scene. the starting point of the projective geometric analysis of scenes is the observation that the set of all three-dimensional realizations ( scenes ) having a given two-dimensional projection (a drawing, or image ) form a linear space. much information about an image, and about its possible spatial interpretations, can be obtained simply by calculating (either locally or globally) the linear dimension (or rank ) of its linear space of scenes. in practice, the image is a pattern on a cathode-ray tube, an aerial photograph, an engineer's or architect's drawing, or an x-ray or nmr scan. the rank of its space of scenes will reveal whether there is ambiguity or uniqueness in the construction of its spatial interpretation, or whether such a construction is in fact impossible, as would be the case for a poorly conceived engineering drawing, or even in an otherwise correctly conceived drawing, if too many hypotheses are made concerning the 3d structure of the scene. calculation of the rank of the space of scenes having a given image should, in principle, be accomplished using simple combinatorial algorithms based on easily-remembered rules-of-thumb. this is the goal, and it shows every sign of being achievable. the problem has, however, a certain degree of unavoidable difficulty. the requirement that a given image be an accurate projection of a non-trivial (non-planar) 3d scene imposes conditions on the image, conditions which are perhaps best described in terms of not-always-elementary constructions with straight-edge and pencil. in this paper, we begin to sort out the interplay of these projective conditions by creating a new homology theory for geometric configurations. the new homology theory applies to geometric objects which are more rigid, less pliable, than the rubber sheets studied by the topology of henri poincar and his school. the passage to this higher degree of invariance is made possible by the creation of a homology theory with (restricted) vector, rather than (unrestricted) scalar, coefficients, or equivalently, by the use of a cohomology theory based on locally linear, rather than on locally constant, functions. we have verified that the new theory agrees with the cohomology theory for the sheaf of locally linear functions on a certain (combinatorially defined) topological space. the basic objects about which this new homology theory has something non-trivial to say are extremely general. from the geometric point of view, they are simply finite sets of points in a projective space or finite sets of vectors in a vector space. in order to emphasize the departure we take from linear algebra as it is usually practiced, we should say that we study vector spaces with a selected basis , that is, concrete vector spaces , in their usual representation as spaces f p of functions from a set p into a field f. finally, we might say we are simply studying rectangular matrices . since such objects are found throughout applied mathematics, the resulting homology theory has a very broad range of potential application. indeed, potential applications of this new homology theory are to any domain where one is interested in the global behavior of systems determined locally by linear constraints.",
    "present_kp": [
      "point",
      "applications",
      "geometry",
      "mathematics",
      "use",
      "invariance",
      "analysis",
      "linear algebra",
      "project",
      "theory",
      "group",
      "representation",
      "paper",
      "constraint",
      "scan",
      "spatial",
      "research",
      "method",
      "behavior",
      "3d",
      "image",
      "algorithm",
      "algebra",
      "rules",
      "global",
      "tools",
      "help",
      "order",
      "drawing",
      "engine",
      "object",
      "case",
      "general",
      "pattern",
      "software",
      "structure",
      "systems",
      "space",
      "interpretation"
    ],
    "absent_kp": [
      "requirements",
      "sorting",
      "timing",
      "graphics",
      "automation",
      "informal",
      "packaging",
      "sketching",
      "configurability",
      "ambiguities",
      "computation",
      "topologies",
      "practical",
      "observability",
      "vectorization",
      "ranking"
    ]
  },
  {
    "title": "imagesense: towards contextual image advertising.",
    "abstract": "the daunting volumes of community-contributed media contents on the internet have become one of the primary sources for online advertising. however, conventional advertising treats image and video advertising as general text advertising by displaying relevant ads based on the contents of the web page, without considering the inherent characteristics of visual contents. this article presents a contextual advertising system driven by images, which automatically associates relevant ads with an image rather than the entire text in a web page and seamlessly inserts the ads in the nonintrusive areas within each individual image. the proposed system, called imagesense, supports scalable advertising of, from root to node, web sites, pages, and images. in imagesense, the ads are selected based on not only textual relevance but also visual similarity, so that the ads yield contextual relevance to both the text in the web page and the image content. the ad insertion positions are detected based on image salience, as well as face and text detection, to minimize intrusiveness to the user. we evaluate imagesense on a large-scale real-world images and web pages, and demonstrate the effectiveness of imagesense for online image advertising.",
    "present_kp": [],
    "absent_kp": [
      "algorithms",
      "experimentation",
      "human factors"
    ]
  },
  {
    "title": "construction and blind estimation of phase sequences for subcarrier-phase control based papr reduction in ldpc coded ofdm systems.",
    "abstract": "as described in this paper construction and blind estimation methods of phase sequences are proposed for subcarrier phase control based peak to average power ratio (papr) reduction in low density parity check (ldpc) coded orthogonal frequency division multiplexing (ofdm) systems. on the transmitter side phase sequence patterns are constructed based on a given parity check matrix. the papr of the ofdm signal is reduced by multiplying the constructed phase sequence selected from the same number of candidates as the number of weighting factor (wf) combinations in a partial transmit sequence (pts) method. on the receiver side the phase sequence is estimated blindly using the decoding function i e the most likely phase sequence among a limited number of possible phase sequence candidates is inferred by comparing the sum product calculation results of each candidate. computer simulation results show that papr of qpsk ofdm and 16qam ofdm signals can be reduced respectively by about 3 7 db and 4 0 db without marked degradation of the block error rate (bler) performance as compared to perfect estimation in an attenuated 12 path rayleigh fading condition.",
    "present_kp": [
      "ofdm",
      "peak to average power ratio ",
      "papr reduction",
      "ldpc code"
    ],
    "absent_kp": []
  },
  {
    "title": "mathsat: tight integration of sat and mathematical decision procedures.",
    "abstract": "recent improvements in propositional satisfiability techniques (sat) made it possible to tackle successfully some hard real-world problems (e.g., model-checking, circuit testing, propositional planning) by encoding into sat. however, a purely boolean representation is not expressive enough for many other real-world applications, including the verification of timed and hybrid systems, of proof obligations in software, and of circuit design at rtl level. these problems can be naturally modeled as satisfiability in linear arithmetic logic (lal), that is, the boolean combination of propositional variables and linear constraints over numerical variables. in this paper we present mathsat, a new, sat-based decision procedure for lal, based on the (known approach) of integrating a state-of-the-art sat solver with a dedicated mathematical solver for lal. we improve mathsat in two different directions. first, the top-level line procedure is enhanced and now features a tighter integration between the boolean search and the mathematical solver. in particular, we allow for theory-driven backjumping and learning, and theory-driven deduction; we use static learning in order to reduce the number of boolean models that are mathematically inconsistent; we exploit problem clustering in order to partition mathematical reasoning; and we define a stack-based interface that allows us to implement mathematical reasoning in an incremental and backtrackable way. second, the mathematical solver is based on layering; that is, the consistency of (partial) assignments is checked in theories of increasing strength (equality and uninterpreted functions, linear arithmetic over the reals, linear arithmetic over the integers). for each of these layers, a dedicated (sub)solver is used. cheaper solvers are called first, and detection of inconsistency makes call of the subsequent solvers superfluous. we provide a through experimental evaluation of our approach, by taking into account a large set of previously proposed benchmarks. we first investigate the relative benefits and drawbacks of each proposed technique by comparison with respect to a reference option setting. we then demonstrate the global effectiveness of our approach by a comparison with several state-of-the-art decision procedures. we show that the behavior of mathsat is often superior to its competitors, both on lal and in the subclass of difference logic.",
    "present_kp": [
      "linear arithmetic logic",
      "propositional satisfiability"
    ],
    "absent_kp": [
      "satisfiability module theory",
      "integrated decision procedures"
    ]
  },
  {
    "title": "strongly regular graphs with the (7)-vertex condition.",
    "abstract": "the (t)-vertex condition, for an integer (tge 2), was introduced by hestenes and higman (siam am math soc proc 4:41160, 1971) providing a combinatorial invariant defined on edges and non-edges of a graph. finite rank 3 graphs satisfy the condition for all values of (t). moreover, a long-standing conjecture of klin asserts the existence of an integer (t_0) such that a graph satisfies the (t_0)-vertex condition if and only if it is a rank 3 graph. we present the first infinite family of non-rank 3 strongly regular graphs satisfying the (7)-vertex condition. this implies that the klin parameter (t_0) is at least 8. the examples are the point graphs of a certain family of generalized quadrangles.",
    "present_kp": [
      "strongly regular graph",
      "generalized quadrangle"
    ],
    "absent_kp": [
      "t-vertex condition"
    ]
  },
  {
    "title": "((epsilon )-)efficiency in difference vector optimization.",
    "abstract": "the paper deals with the problem of characterizing pareto optima (efficient solutions) for the difference of two mappings vector-valued in a finite or infinite-dimensional preordered space. closely related to the well-known optimality criterion of scalar dc optimization, a mixed vectorial condition is obtained in terms of both strong (fenchel) and weak (pareto) (epsilon )-subdifferentials that completely characterizes the exact or approximate weak efficiency. this condition also allows to deal with some special restricted mappings. moreover, the condition established in the literature in terms of strong (epsilon )-subdifferentials for characterizing the strongly efficient solutions (usual optima), is shown here to remain valid without assuming that the objective space is order-complete.",
    "present_kp": [
      "vector optimization",
      "efficiency"
    ],
    "absent_kp": [
      "dc objective",
      "optimality criteria",
      "-solutions",
      "vector -subdifferentials"
    ]
  },
  {
    "title": "iterative visual clustering for unstructured text mining.",
    "abstract": "this paper proposes the iterative visual clustering (ivc) on unstructured text sequences to form and evaluate keyword clusters, based on which users can use visual analysis, domain knowledge to discover knowledge in the text. the text sequence data are broken down into a list representative keywords after textual evaluation, and the keywords are then grouped to form keyword clusters via an iterative stochastic process and are visualized as distributions over the time lines. the visual evaluation model provides shape evaluations as quantitative tools and users' interactions as qualitative tools to visually investigate the trends, patterns represented by the keyword clusters' distributions. the keyword clustering model, guided by the feedback of visual evaluations, step-wisely enumerates newer generations of keyword clusters and their patterns, therefore narrows down the search space. then the proposed ivc is applied onto nursing narratives and is able to identify interesting keyword clusters implying hidden knowledge regarding to the working patterns and environment of registered nurses. the loop of producing next generation of keyword clusters in ivc is driven and controlled by users' perception, domain knowledge and interactions, and it is also guided by a stochastic search model. so both semantic and distribution features enable ivc to have significant applications as a text mining tool, on many other data sets, such as biomedical literatures.",
    "present_kp": [],
    "absent_kp": [
      "text and document visualization",
      "nursing data processing"
    ]
  },
  {
    "title": "economic growth, telecommunications development and productivity growth of the telecommunications sector: evidence around the world.",
    "abstract": "this paper studies the relationships between economic growth, telecommunications development and productivity growth of the telecommunications sector in different countries and regions of the world. in particular, this study assesses the impact of mobile telecommunications on economic growth and telecommunications productivity. the results indicate that there is a bidirectional relationship between real gross domestic product (gdp) and telecommunications development (as measured by teledensity) for european and high-income countries. however, when the impact of mobile telecommunications development on economic growth is measured separately, the bi-directional relationship is no longer restricted to european and high-income countries. this study also finds that countries in the upper-middle income group have achieved a higher average total factor productivity (tfp) growth than other countries. countries with competition and privatization in telecommunications have achieved a higher tfp growth than those without competition and privatization. the diffusion of mobile telecommunications services is found to be a significant factor that has improved the tfp growth of the telecommunications sector in central and eastern europe (cee).",
    "present_kp": [
      "telecommunications",
      "economic growth",
      "total factor productivity"
    ],
    "absent_kp": []
  },
  {
    "title": "examining learning from text and pictures for different task types: does the multimedia effect differ for conceptual, causal, and procedural tasks.",
    "abstract": "the multimedia effect (me) is a well-researched effect in the field of learning and instruction. in this article, two views that explain the me are compared. the outcome-oriented view focuses on the beneficial effect of text and pictures on mental representations, whereas the process-oriented view focuses on the beneficial effect of text and pictures for information processing. to contrast these views, the me sizes for different task types were compared (i.e., conceptual, causal, procedural tasks). whereas the outcome-oriented view predicts no differences in me size, the process-oriented view predicts that the me is largest in causal tasks, smaller in procedural tasks, and smallest in conceptual tasks. sixty-five students learnt with text only or with text and pictures. task type and information source (i.e., whether the text, picture, or text and picture provided the answer to a post-test question) were varied within subjects. the results showed that, in line with the process-oriented view, the me was smaller for conceptual tasks than for procedural tasks. contrary to the expectations, the me was larger in procedural tasks than in causal tasks. moreover, the pattern of results varied with information source. research and practical implications are described, so that pictures can be deployed optimally.",
    "present_kp": [
      "multimedia effect",
      "conceptual"
    ],
    "absent_kp": [
      "learning with text and pictures",
      "causal and procedural tasks",
      "static visualisations"
    ]
  },
  {
    "title": "sufficient conditions for lambda '-optimality of graphs with small conditional diameter.",
    "abstract": "a restricted edge-cut s of a connected graph g is an edge-cut such that g - s has no isolated vertex. the restricted edge-connectivity lambda'(g) is the minimum cardinality over all restricted edge-cuts. a graph is said to lambda'-optimal if lambda'(g) = xi(g), where xi(g) denotes the minimum edge-degree of g defined as xi(g) = min{d(u) + d(nu) - 2: u nu is an element of e(g)}. the p-diameter of g measures how far apart a pair of subgraphs satisfying a given property p can be, and hence it generalizes the standard concept of diameter. in this paper we prove two kind of results, according to which property p is chosen. first, let d-1 (resp. d-2) be the p-diameter where p is the property that the corresponding subgraphs have minimum degree at least one (resp. two). we prove that a graph with odd girth g is lambda'-optimal if d-1 = 2, being the minimum degree of g. using the property q of being vertices of g - f we prove that a graph with girth g is not an element of {4, 6, 8} is lambda'-optimal if this q-diameter is at most 2[(g - 3)/2].",
    "present_kp": [
      "restricted edge-connectivity",
      "conditional diameter"
    ],
    "absent_kp": [
      "fault tolerance"
    ]
  },
  {
    "title": "comparative analysis of clicks and judgments for ir evaluation.",
    "abstract": "queries and click-through data taken from search engine transaction logs is an attractive alternative to traditional test collections, due to its volume and the direct relation to end-user querying. the overall aim of this paper is to answer the question: how does click-through data differ from explicit human relevance judgments in information retrieval evaluation? we compare a traditional test collection with manual judgments to transaction log based test collections---by using queries as topics and subsequent clicks as pseudo-relevance judgments for the clicked results. specifically, we investigate the following two research questions: firstly, are there significant differences between clicks and relevance judgments. earlier research suggests that although clicks and explicit judgments show reasonable agreement, clicks are different from static absolute relevance judgments. secondly, are there significant differences between system ranking based on clicks and based on relevance judgments? this is an open question, but earlier research suggests that comparative evaluation in terms of system ranking is remarkably robust.",
    "present_kp": [],
    "absent_kp": [
      "transaction log analysis",
      "wikipedia",
      "web information retrieval"
    ]
  },
  {
    "title": "s2-quasicontinuous posets.",
    "abstract": "in this paper, we consider a common generalization of both s2-continuous posets and quasicontinuous domains, and we introduce new concepts of way below relations and s2-quasicontinuous posets. the main results are: (1) the way below relation on an s2-quasicontinuous poset has the interpolation property; (2) the 2-topology on an s2-quasicontinuous poset is completely regular; (3) a poset is s2-continuous iff it is meet s2-continuous and s2-quasicontinuous.",
    "present_kp": [
      "s2-continuous poset",
      "-topology"
    ],
    "absent_kp": [
      "meet s2-continuous poset s2",
      "s2-quasicontinuous poset s2"
    ]
  },
  {
    "title": "does the polynomial hierarchy collapse if onto functions are invertible.",
    "abstract": "the class tfnp, defined by megiddo and papadimitriou, consists of multivalued functions with values that are polynomially verifiable and guaranteed to exist. do we have evidence that such functions are hard, for example, if tfnp is computable in polynomial-time does this imply the polynomial-time hierarchy collapses? by computing a multivalued function in deterministic polynomial-time we mean on every input producing one of the possible values of the function on that input. we give a relativized negative answer to this question by exhibiting an oracle under which tfnp functions are easy to compute but the polynomial-time hierarchy is infinite. we also show that relative to this same oracle, p not equal up and tfnp(np) functions are not computable in polynomial-time with an np oracle.",
    "present_kp": [
      "polynomial-time hierarchy"
    ],
    "absent_kp": [
      "computational complexity",
      "multi-valued functions",
      "kolmogorov complexity"
    ]
  },
  {
    "title": "unsupervised object segmentation with a hybrid graph model (hgm).",
    "abstract": "in this work, we address the problem of performing class-specific unsupervised object segmentation, i.e., automatic segmentation without annotated training images. object segmentation can be regarded as a special data clustering problem where both class-specific information and local texture/color similarities have to be considered. to this end, we propose a hybrid graph model (hgm) that can make effective use of both symmetric and asymmetric relationship among samples. the vertices of a hybrid graph represent the samples and are connected by directed edges and/or undirected ones, which represent the asymmetric and/or symmetric relationship between them, respectively. when applied to object segmentation, vertices are superpixels, the asymmetric relationship is the conditional dependence of occurrence, and the symmetric relationship is the color/texture similarity. by combining the markov chain formed by the directed subgraph and the minimal cut of the undirected subgraph, the object boundaries can be determined for each image. using the hgm, we can conveniently achieve simultaneous segmentation and recognition by integrating both top-down and bottom-up information into a unified process. experiments on 42 object classes (9,415 images in total) show promising results.",
    "present_kp": [
      "segmentation"
    ],
    "absent_kp": [
      "graph-theoretic methods",
      "spectral clustering"
    ]
  },
  {
    "title": "dispersion free wave splittings for structural elements.",
    "abstract": "wave splittings are derived for three types of structural elements: membranes, timoshenko beams, and mindlin plates. the timoshenko beam equation and the mindlin plate equation are inherently dispersive, as is each fourier component of the membrane equation in an angular decomposition of the field. the distinctive feature of the wave splittings derived in the present paper is that, in homogeneous regions, they transform the dispersive wave equations into simple one-way wave equations without dispersion. such splittings have uses both for radial scattering problems in the 2d cases and for scattering problems in dispersive media. as an example of how the splittings may be applied, a direct scattering problem is solved for a membrane with radially varying density. the imbedding method is utilized, and agreement is obtained with an fe simulation.",
    "present_kp": [
      "wave splitting",
      "imbedding",
      "membrane",
      "timoshenko beam",
      "mindlin plate"
    ],
    "absent_kp": [
      "time domain methods",
      "greens operator"
    ]
  },
  {
    "title": "designing a practical data filter cache to improve both energy efficiency and performance.",
    "abstract": "conventional data filter cache (dfc) designs improve processor energy efficiency, but degrade performance. furthermore, the single-cycle line transfer suggested in prior studies adversely affects level-1 data cache (l1 dc) area and energy efficiency. we propose a practical dfc that is accessed early in the pipeline and transfers a line over multiple cycles. our dfc design improves performance and eliminates a substantial fraction of l1 dc accesses for loads, l1 dc tag checks on stores, and data translation lookaside buffer accesses for both loads and stores. our evaluation shows that the proposed dfc can reduce the data access energy by 42.5% and improve execution time by 4.2%.",
    "present_kp": [
      "filter cache"
    ],
    "absent_kp": [
      "speculation"
    ]
  },
  {
    "title": "analytical model for anomalous positive bias temperature instability in la-based hfo2 nfets based on independent characterization of charging components.",
    "abstract": "pbti improvement in hfo2 nfets achieved by a controlled insertion of la. anomalous negative ?vth due to charge exchange between high-k and metal gate. anomalous and conventional pbti components are decoupled and studied separately. analytical model including both components for lifetime extrapolation is presented.",
    "present_kp": [
      "bias temperature instability"
    ],
    "absent_kp": [
      "metaloxidesemiconductor field-effect transistor ",
      "hafnium oxide",
      "silicon oxide"
    ]
  },
  {
    "title": "a social behaviour evolution approach for evolutionary optimisation.",
    "abstract": "evolutionary algorithms were originally designed to locate basins of optimum solutions in a stationary environment. therefore, additional techniques and modifications have been introduced to deal with further requirements such as handling dynamic fitness functions or finding multiple optima. in this paper, we present a new approach for building evolutionary algorithms that is based on concepts borrowed from social behaviour evolution. algorithms built with the proposed paradigm operate on a population of individuals that move in the search space as they interact and form groups. the interaction follows a set of social behaviours evolved by each group to enhance its adaptation to the environment (and other groups) and to achieve different desirable goals such as finding multiple optima, maintaining diversity, or tracking a moving peak in a changing environment. each group has two sets of behaviours: one for intra-group interactions and one for inter-group interactions. these behaviours are evolved using mathematical models from the field of evolutionary game theory. this paper describes the proposed paradigm and starts studying it characteristics by building a new evolutionary algorithm and studying its behavior. the algorithm has been tested using a benchmark problem generator with promising initial results, which are also reported.",
    "present_kp": [
      "social behaviour evolution",
      "evolutionary optimisation",
      "evolutionary game theory",
      "evolutionary algorithms"
    ],
    "absent_kp": [
      "social adaptive groups",
      "dynamic optimisation problems"
    ]
  },
  {
    "title": "planar c1 hermite interpolation with ph cuts of degree (1,3) of laurent series.",
    "abstract": "we introduce a new class of ph curves, ph cuts of degree (1,3) of laurent series. we show how to find ph skew cut interpolants to a c1 hermite data-set. we show that two of these interpolants are short, simple curves with stable shape. our curves are fair with different shapes to those of other interpolants. we can obtain regular ph interpolants for collinear c1 hermite data-sets.",
    "present_kp": [
      "c1 hermite interpolation",
      "ph skew cut",
      "ph skew cut interpolant"
    ],
    "absent_kp": [
      "pythagorean hodograph  curve",
      "complex representation",
      "cut of degree of a laurent series"
    ]
  },
  {
    "title": "bicepstrum based blind identification of the acoustic emission (ae) signal in precision turning.",
    "abstract": "it is believed that the acoustic emissions (ae) signal contains potentially valuable information for monitoring precision cutting processes, as well as to be employed as a control feedback signal. however, ae stress waves produced in the cutting zone are distorted by the transmission path and the measurement systems. in this article, a bicepstrum based blind system identification technique is proposed as a valid tool for estimating both, transmission path and sensor impulse response. assumptions under which application of bicepstrum is valid are discussed and diamond turning experiments are presented, which demonstrate the feasibility of employing bicepstrum for ae blind identification.",
    "present_kp": [
      "acoustic emissions",
      "blind identification"
    ],
    "absent_kp": [
      "higher-order statistics",
      "precision machining"
    ]
  },
  {
    "title": "on solving hierarchical problems with top down control.",
    "abstract": "we review recent work on the hierarchical-if-and-only-if problem and present a new hierarchical problem, hiff-m that does not fit with previous explanations for evolutionary difficulty on hierarchical problems decomposed by levels for rmhc2. rmhc2 is a hill climbing algorithm augmented with a multi-level selection scheme. when used with the \"ideal\" sieve for a problem, as is done in this paper, rmhc2 exerts top-down control on the evolutionary dynamics, in the sense that adaptation of higher levels are given priority over adaptation of lower levels, and creates stabilizing selection pressure with potential to increase evolvability. through hiff-m, we discovered that the summary statistic, fitness distance correlation by level, is not a reliable indicator of when a hierarchical problem is solvable by rmhc2, and that the two properties proposed to explain search easiness for rmhc2 are inadequate. our investigation of this anomaly led us to propose an additional property for hierarchical evolution difficulty under rmhc2: inter-level conflict. we also discuss how hierarchical control can be subverted through the information transfer capacity of the transposition operation.",
    "present_kp": [
      "hierarchical control",
      "transposition"
    ],
    "absent_kp": [
      "hierarchical test problems",
      "level decomposition"
    ]
  },
  {
    "title": "extracting semantic frames from thai medical-symptom unstructured text with unknown target-phrase boundaries.",
    "abstract": "due to the limitations of language-processing tools for the thai language, pattern-based information extraction from thai documents requires supplementary techniques. based on sliding-window rule application and extraction filtering, we present a framework for extracting semantic information from medical-symptom phrases with unknown boundaries in thai unstructured-text information entries. a supervised rule learning algorithm is employed for automatic construction of information extraction rules from hand-tagged training symptom phrases. two filtering components are introduced: one uses a classification model to predict rule application across a symptom-phrase boundary based on instantiation features of rule internal wildcards, the other uses weighted classification confidence to resolve conflicts arising from overlapping extractions. in our experimental study, we focus our attention on two basic types of symptom phrasal descriptions: one is concerned with abnormal characteristics of some observable entities and the other with human-body locations at which primitive symptoms appear. the experimental results show that the filtering components improve precision while preserving recall satisfactorily.",
    "present_kp": [
      "information extraction",
      "rule learning"
    ],
    "absent_kp": [
      "medical informatics"
    ]
  },
  {
    "title": "robust doa estimation for uncorrelated and coherent signals.",
    "abstract": "a new direction of arrival (doa) estimation method is introduced with arbitrary array geometry when uncorrelated and coherent signals coexist. the doas of uncorrelated signals are first estimated via subspace-based high resolution doa estimation technique. then a matrix that only contains the information of coherent signals can be formulated by eliminating the contribution of uncorrelated signals. finally a subspace block sparse reconstruction approach is taken for doa estimations of the coherent signals.",
    "present_kp": [
      "coherent signals",
      "direction of arrival",
      "sparse reconstruction"
    ],
    "absent_kp": []
  },
  {
    "title": "checkpoint allocation and release.",
    "abstract": "out-of-order speculative processors need a bookkeeping method to recover from incorrect speculation. in recent years, several microarchitectures that employ checkpoints have been proposed, either extending the reorder buffer or entirely replacing it. this work presents an in-dept-study of checkpointing in checkpoint-based microarchitectures, from the desired content of a checkpoint, via implementation trade-offs, and to checkpoint allocation and release policies. a major contribution of the article is a novel adaptive checkpoint allocation policy that outperforms known policies. the adaptive policy controls checkpoint allocation according to dynamic events, such as second-level cache misses and rollback history. it achieves 6.8% and 2.2% speedup for the integer and floating point benchmarks, respectively, and does not require a branch confidence estimator. the results show that the proposed adaptive policy achieves most of the potential of an oracle policy whose performance improvement is 9.8% and 3.9% for the integer and floating point benchmarks, respectively. we exploit known techniques for saving leakage power by adapting and applying them to checkpoint-based microarchitectures. the proposed applications combine to reduce the leakage power of the register file to about one half of its original value.",
    "present_kp": [
      "performance",
      "checkpoint",
      "rollback",
      "leakage"
    ],
    "absent_kp": [
      "design",
      "misprediction",
      "out-of-order execution",
      "early register release"
    ]
  },
  {
    "title": "a quadratic spline approximation using detail multi-layer for soft shadow generation in augmented reality.",
    "abstract": "implementation of shadows is crucial to enhancement of images in ar environments. without shadows, virtual objects would look floating over the scene resulting in unrealistic rendering of ar environments. casting hard shadows would provide only spatial information while soft shadows help improve realism of ar environments. several algorithms have been proposed to render realistic shadows which often incurred high computational costs. little attention has been directed towards the balanced trade-off between shadow quality and computational costs. in this study, two approaches are proposed: quadratic spline interpolation (qsi) to soften the outline of the shadow and detail multi-layer (dml) technique to optimize the volume of computations for the generation of soft shadows based on real light sources. qsi estimates boarder hard shadow samples while dml involves three main phases: real light sources estimation, soft shadow production and reduction of the complexity of 3-dimensional objects shadows. to be more precise, a reflective hemisphere is used to capture real light and to create an environment map. the median cut algorithm is implemented to locate the direction of real light sources on the environment map. subsequently, the original hard shadows are retrieved and a sample of multilayer hard shadows is produced where each layer has its unique size and colour. these layers overlap to produce soft shadows based on the real light sources directions. finally, the level of details (lod) algorithm is implemented to increase the efficiency of soft shadows by decreasing the complexity of vertex transformations. the proposed technique is tested using three samples of multilayer hard shadows with varying numbers of light sources generated from the median cut algorithm. the experimental results show that the proposed technique successfully produces realistic soft shadows at low computational costs.",
    "present_kp": [
      "augmented reality",
      "shadow generation",
      "soft shadows",
      "environment map"
    ],
    "absent_kp": [
      "reflective sphere"
    ]
  },
  {
    "title": "the enhanced optical coupling in a quantum well infrared photodetector based on a resonant mode of an airdielectricmetal waveguide.",
    "abstract": "the hybrid structure consisting of periodic gold stripes and an overlaying gold film is proposed to enhance the optical coupling of a quantum well infrared photodetector. an airdielectricmetal waveguide is formed when the hybrid structure is integrated on the top of the quantum well detector with the substrate being removed. finite difference time-domain method is used to numerically obtain the reflection spectrum and the field distribution of the waveguide. the results show that a strong electric field component is induced in parallel to the growth direction of quantum well when the waveguide resonant mode occurs at the detective wavelength of the quantum well infrared photodetector. the relationship between the structural parameters and the resonant wavelength is derived by using the effective refractive index method of the airdielectricmetal waveguide. a high coupling efficiency can be obtained and the performance of the qwip can be greatly improved.",
    "present_kp": [
      "periodic gold stripes",
      "quantum well infrared photodetector",
      "effective refractive index",
      "coupling efficiency"
    ],
    "absent_kp": [
      "airdielectricmetal waveguide resonance"
    ]
  },
  {
    "title": "redirection based recovery for mpls network systems.",
    "abstract": "to provide a reliable backbone network, fault tolerance should be considered in the network design. for a multiprotocol label switching (mpls) based backbone network, the fault-tolerant issue focuses on how to protect the traffic of a label switched paths (lsp) against node and link failures. in ietf, two well-known recovery mechanisms (protection switching and rerouting) have been proposed. to further enhance the fault-tolerant performance of the two recovery mechanisms, the proposed approach utilizes the failure-free lsps to transmit the traffic of the failed lsp (the affected traffic). to avoid affecting the original traffic of each failure-free lsp, the proposed approach applies the solution of the minimum cost flow to determine the amount of affected traffic to be transmitted by each failure-free lsp. for transmitting the affected traffic along a failure-free working lsp, ip tunneling technique is used. we also propose a permission token scheme to solve the packet disorder problem. finally, simulation experiments are performed to show the effectiveness of the proposed approach.",
    "present_kp": [
      "mpls",
      "fault tolerance",
      "label switched path",
      "affected traffic",
      "minimum cost flow"
    ],
    "absent_kp": []
  },
  {
    "title": "designing a cross-language comparison-shopping agent.",
    "abstract": "this research pertains to the design and development of a shopbot called webshopper+. this shopbot is intended to help customers find and compare e-tailers that market their wares using different languages. webshopper+ is built with a multilingual ontology to overcome the language barriers that arise with global e-commerce. this research proposes a semi-automatic method of constructing a multilingual ontology by using the formal concept analysis and association analysis. it also proposes an automatic method for the categorization of product data into predefined classes, with the aim of alleviating administrators' task load. additionally, a semantic search mechanism based on concept similarity is designed to assist customers in finding more desirable products. the experimental results show that these methods perform well and the shopbot can help customers find real bargains on the web and to find products that cannot be bought locally.",
    "present_kp": [
      "shopbot",
      "comparison-shopping",
      "ontology",
      "formal concept analysis"
    ],
    "absent_kp": [
      "semantic similarity"
    ]
  },
  {
    "title": "medical informaticsthe state of the art in the hospital authority.",
    "abstract": "since its inception in 1990, the hospital authority (ha) has strongly supported the development and implementation of information systems both to improve the delivery of care and to make better information available to managers. this paper summarizes the progress to date and discusses current and future developments. following the first two phases of the ha information technology strategy the basic infrastructural elements were laid in place. these included the foundation administrative and financial systems and databases; establishment of a wide area network linking all hospitals and clinics together; laboratory, radiology and pharmacy systems with access to results in the ward. a major push into clinical systems began in 1994 with the clinical management system (cms), which established a clinical workstation for use in both ward and ambulatory settings. the cms is now running at all major hospitals, and provides single logon access to almost all the electronically collected clinical data in the ha. the next phase of development is focussed on further support for clinical activities in the cms. key elements include the longitudinal electronic patient record (epr), clinical order entry, generic support for clinical reports, broadening the scope to include allied health and the rehabilitative phase, clinical decision support, an improved clinical documentation framework, sharing of clinical information with other health care providers and a comprehensive data repository for analysis and reporting purposes.",
    "present_kp": [],
    "absent_kp": [
      "hospital information systems",
      "clinical information systems",
      "hong kong"
    ]
  },
  {
    "title": "prioritization of potential candidate disease genes by topological similarity of proteinprotein interaction network and phenotype data.",
    "abstract": "we construct a reliable heterogeneous network by fusing multiple networks. we devise a random walk based algorithm on the reliable heterogeneous network. combining topological similarity with phenotype data helps to predict causal genes. the algorithm is still in good performance at low parameter values.",
    "present_kp": [
      "disease genes",
      "random walk",
      "topological similarity",
      "phenotype"
    ],
    "absent_kp": [
      "proteinprotein interaction networks"
    ]
  },
  {
    "title": "extended beta regression in r: shaken, stirred, mixed, and partitioned.",
    "abstract": "beta regression -an increasingly popular approach for modeling rates and proportions - is extended in various directions: (a) bias correction/reduction of the maximum likelihood estimator, (b) beta regression tree models by means of recursive partitioning, (c) latent class beta regression by means of finite mixture models. all three extensions may be of importance for enhancing the beta regression toolbox in practice to provide more reliable inference and capture both observed and unobserved/latent heterogeneity in the data. using the analogy of smithson and verkuilen (2006), these extensions make beta regression not only \"a better lemon squeezer\" (compared to classical least squares regression) but a full-fledged modern juicer offering lemon-based drinks: shaken and stirred (bias correction and reduction), mixed (finite mixture model), or partitioned (tree model). all three extensions are provided in the r package betareg (at least 2.4-0), building on generic algorithms and implementations for bias correction/reduction, model-based recursive partioning, and finite mixture models, respectively. specifically, the new functions betatree () and betamix () reuse the object-oriented flexible implementation from the r packages party and flexmix, respectively.",
    "present_kp": [
      "beta regression",
      "bias correction",
      "recursive partitioning",
      "finite mixture",
      "r"
    ],
    "absent_kp": [
      "bias reduction"
    ]
  },
  {
    "title": "four-layer framework for combinatorial optimization problems domain.",
    "abstract": "four-layer framework for combinatorial optimization problems/models domain is suggested for applied problems structuring and solving: (1) basic combinatorial models and multicriteria decision making problems (e.g., clustering, knapsack problem, multiple choice problem, multicriteria ranking, assignment/allocation); (2) composite models/procedures (e.g., multicriteria combinatorial problems, morphological clique problem); (3) basic (standard) solving frameworks, e.g.: (i) hierarchical morphological multicriteria design (hmmd) (ranking, combinatorial synthesis based on morphological clique problem), (ii) multi-stage design (two-level hmmd), (iii) special multi-stage composite framework (clustering, assignment/location, multiple choice problem); and (4) domain-oriented solving frameworks, e.g.: (a) design of modular software, (b) design of test inputs for multi-function system testing, (c) combinatorial planning of medical treatment, (d) design and improvement of communication network topology, (e) multi-stage framework for information retrieval, (f) combinatorial evolution and forecasting of software, devices. the multi-layer approach covers decision cycle, i.e., problem statement, models, algorithms/procedures, solving schemes, decisions, decision analysis and improvement.",
    "present_kp": [
      "combinatorial optimization",
      "decision making"
    ],
    "absent_kp": [
      "problem solving environment",
      "system architecture",
      "problem structuring",
      "system design"
    ]
  },
  {
    "title": "computing monodromy via continuation methods on random riemann surfaces.",
    "abstract": "we consider a riemann surface x defined by a polynomial f (x, y) of degree d, whose coefficients are chosen randomly. hence, we can suppose that x is smooth, that the discriminant delta(x) of f has d(d - 1) simple roots, delta, and that delta(0) not equal 0, i.e. the corresponding fiber has d distinct points {y(1), ..., y(d)}. when we lift a loop 0 is an element of gamma subset of c - delta by a continuation method, we get d paths in x connecting {y(1), ..., y(d)}, hence defining a permutation of that set. this is called monodromy. here we present experimentations in maple to get statistics on the distribution of transpositions corresponding to loops around each point of delta. multiplying families of \"neighbor\" transpositions, we construct permutations and the subgroups of the symmetric group they generate. this allows us to establish and study experimentally two conjectures on the distribution of these transpositions and on transitivity of the generated subgroups. assuming that these two conjectures are true, we develop tools allowing fast probabilistic algorithms for absolute multivariate polynomial factorization, under the hypothesis that the factors behave like random polynomials whose coefficients follow uniform distributions.",
    "present_kp": [
      "random riemann surface",
      "continuation methods",
      "monodromy",
      "symmetric group",
      "algorithms"
    ],
    "absent_kp": [
      "bivariate polynomial",
      "plane curve",
      "absolute factorization",
      "algebraic geometry",
      "maple code"
    ]
  },
  {
    "title": "feature-based decision aggregation in modular neural network classifiers.",
    "abstract": "in several modular neural network (mnn) architectures, the individual decisions at the module level have to be integrated together using a voting scheme. all these voting schemes use the outputs of the individual modules to produce a global output without inferring explicit information from the problem feature space. this makes the choice of the aggregation procedure very subjective. in this work, a new mnn architecture will be presented. this architecture integrates learning into the voting scheme. we will be focusing on making the decision fusion a more dynamic process. in this context, dynamic means the aggregation procedure which has the flexibility to adapt to changes in the input. this approach requires the aggregation procedure to gather information about the input to help better understand how to dynamically aggregate decisions.",
    "present_kp": [],
    "absent_kp": [
      "classification",
      "classifier combination",
      "dynamic decision fusion",
      "modular neural networks"
    ]
  },
  {
    "title": "efficient bootstrap with weakly dependent processes.",
    "abstract": "the efficient bootstrap methodology is developed for overidentified moment conditions models with weakly dependent observation. the resulting bootstrap procedure is shown to be asymptotically valid and can be used to approximate the distributions of t-statistics, the j-statistic for overidentifying restrictions, and wald, lagrange multiplier and distance statistics for nonlinear hypotheses. the asymptotic validity of the efficient bootstrap based on a computationally less demanding approximate k-step estimator is also shown. the finite sample performance of the proposed bootstrap is assessed using simulations in an intertemporal consumption based asset pricing model.",
    "present_kp": [],
    "absent_kp": [
      "alpha-mixing",
      "consumption capm",
      "gel",
      "gmm",
      "hypothesis testing"
    ]
  },
  {
    "title": "multi-relay cooperative diversity protocol with improved spectral efficiency.",
    "abstract": "cooperative diversity protocols have attracted a great deal of attention since they are thought to be capable of providing diversity multiplexing tradeoff among single antenna wireless devices. in the high signal to noise ratio (snr) region, cooperation is rarely required; hence, the spectral efficiency of the cooperative protocol can be improved by applying a proper cooperation selection technique. in this paper, we present a simple \"cooperation selection\" technique based on instantaneous channel measurement to improve the spectral efficiency of cooperative protocols. we show that the same instantaneous channel measurement can also be used for relay selection. in this paper two protocols are proposed-proactive and reactive; the selection of one of these protocols depends on whether the decision of cooperation selection is made before or after the transmission of the source. these protocols can successfully select cooperation along with the best relay from a set of available m relays. if the instantaneous source to destination channel is strong enough to support the system requirements, then the source simply transmits to the destination as a noncooperative direct transmission; otherwise, a cooperative transmission with the help of the selected best relay is chosen by the system. analysis and simulation results show that these protocols can achieve higher order diversity with improved spectral efficiency, i.e., a higher diversity-multiplexing tradeoff in a slow-fading environment.",
    "present_kp": [
      "cooperative diversity",
      "diversity-multiplexing tradeoff",
      "relay selection",
      "spectral efficiency"
    ],
    "absent_kp": [
      "fading channel",
      "outage probability"
    ]
  },
  {
    "title": "a motion-tolerant dissolve detection algorithm.",
    "abstract": "gradual shot change detection is one of the most important research issues in the field of video indexing/retrieval. among the numerous types of gradual transitions, the dissolve-type gradual transition is considered the most common one, but it is also the most difficult one to detect. in most of the existing dissolve detection algorithms, the false/miss detection problem caused by motion is very serious. in this paper, we present a novel dissolve-type transition detection algorithm that can correctly distinguish dissolves from disturbance caused by motion. we carefully model a dissolve based on its nature and then use the model to filter out possible confusion caused by the effect of motion. experimental results show that the proposed algorithm is indeed powerful.",
    "present_kp": [
      "dissolve detection",
      "shot change detection"
    ],
    "absent_kp": [
      "fade detection"
    ]
  },
  {
    "title": "gmm-based evaluation of emotional style transformation in czech and slovak.",
    "abstract": "in the development of the voice conversion and the emotional speech style transformation in the text-to-speech systems, it is very important to obtain feedback information about the users opinion on the resulting synthetic speech quality. for this reason, the evaluations of the quality of the produced synthetic speech must often be performed for comparison. the main aim of the experiments described in this paper was to find out whether the classifier based on gaussian mixture models (gmms) could be applied for evaluation of male and female resynthesized speech that had been transformed from neutral to four emotional states (joy, surprise, sadness, and anger) spoken in czech and slovak languages. we suppose that it is possible to combine this gmm-based statistical evaluation with the classical one in the form of listening tests or it can replace them. for verification of our working hypothesis, a simple gmm emotional speech classifier with a one-level structure was realized. the next task of the performed experiment was to investigate the influence of different types and values (mean, median, standard deviation, relative maximum, etc.) of the used speech features (spectral and/or supra-segmental) on the gmm classification accuracy. the obtained gmm evaluation scores are compared with the results of the conventional listening tests based on the mean opinion scores. in addition, correctness of the gmm classification is analyzed with respect to the influence of the setting of the parameters during the gmm trainingthe number of mixture components and the types of speech features. the paper also describes the comparison experiment with the reference speech corpus taken from the berlin database of emotional speech in german language as the benchmark for the evaluation of the performance of our one-level gmm classifier. the obtained results confirm practical usability of the developed gmm classifier, so we will continue in this research with the aim to increase the classification accuracy and compare it with other approaches like the support vector machines.",
    "present_kp": [],
    "absent_kp": [
      "emotional speech transformation",
      "spectral and prosodic features of speech",
      "gmm-based emotion classification"
    ]
  },
  {
    "title": "collage of two-dimensional words.",
    "abstract": "we consider a new operation on one-dimensional (resp. two-dimensional) word languages, obtained by piling up, one on top of the other, words of a given recognizable language (resp. two-dimensional recognizable language) on a previously empty one-dimensional (resp. two-dimensional) array. the resulting language is the set of words \"seen from above\": a position in the array is labeled by the topmost letter. we show that in the one-dimensional case, the language is always recognizable. this is no longer true in the two-dimensional case which is shown by a counter-example, and we investigate in which particular cases the result may still hold.",
    "present_kp": [],
    "absent_kp": [
      "regular languages",
      "picture languages"
    ]
  },
  {
    "title": "inference management, trust and obfuscation principles for quality of information in emerging pervasive environments.",
    "abstract": "the emergence of large scale, distributed, sensor-enabled, machine-to-machine pervasive applications necessitates engaging with providers of information on demand to collect the information, of varying quality levels, to be used to infer about the state of the world and decide actions in response. in these highly fluid operational environments, involving information providers and consumers of various degrees of trust and intentions, information transformation, such as obfuscation, is used to manage the inferences that could be made to protect providers from misuses of the information they share, while still providing benefits to their information consumers. in this paper, we develop the initial principles for relating to inference management and the role that trust and obfuscation plays in it within the context of this emerging breed of applications. we start by extending the definitions of trust and obfuscation into this emerging application space. we, then, highlight their role as we move from the tightly-coupled to loosely-coupled sensory-inference systems and describe how quality, value and risk of information relate in collaborative and adversarial systems. next, we discuss quality distortion illustrated through a human activity recognition sensory system. we then present a system architecture to support an inference firewall capability in a publish/subscribe system for sensory information and conclude with a discussion and closing remarks.",
    "present_kp": [
      "quality of information",
      "risk of information",
      "obfuscation",
      "inference management"
    ],
    "absent_kp": [
      "value of information",
      "qoi",
      "voi",
      "roi"
    ]
  },
  {
    "title": "digging in the digg social news website.",
    "abstract": "the rise of social media aggregating websites provides platforms where users can actively publish, evaluate, and disseminate content in a collaborative way. in this paper, we present a large-scale empirical study about \"digg.com\", one of the biggest social media aggregating websites. our analysis is based on crawls of 1.5 million users and 10 million published stories on digg. we study the distinct network structure, the collaborative user characteristics, and the content dissemination process on digg. we empirically illustrate that friendship relations are used effectively in disseminating half of the content, although there exists a high overlap between the interests of friends. a successful content dissemination process can also be performed by random users who are browsing and digging stories. since 88% of the published content on digg is defined as news, it is important for the content to obtain sufficient votes in a short period of time before becoming obsolete. finally, we show that the synchronization of users' activities in time is the key to a successful content dissemination process. the dynamics between users' voting activities consequently decrease the efficiency of friendship relations during content dissemination. the results presented in this paper define basic observations and measurements to understand the underlying mechanism of disseminating content in current online social news aggregators. these findings are helpful to understand the influence of service interfaces and user behaviors on content dissemination.",
    "present_kp": [
      "content dissemination",
      "friendship relations",
      "user characteristics"
    ],
    "absent_kp": [
      "social media website"
    ]
  },
  {
    "title": "usage of agents in document management.",
    "abstract": "extensible java-based agent framework (xjaf) is a pluggable architecture of the hierarchical intelligent agents system with communication based on kqml. workers, inc. is a workflow management system implemented using mobile agents. it is especially suited for highly distributed and heterogeneous environments. the application of the above-mentioned systems will be considered in the area of document management systems.",
    "present_kp": [
      "mobile agents",
      "document management"
    ],
    "absent_kp": [
      "workflow management systems"
    ]
  },
  {
    "title": "ezpal: environment for composing constraint axioms by instantiating templates.",
    "abstract": "many ontology-development tools allow users to supplement frame-based representations with arbitrary logical sentences. however, few users actually take advantage of this opportunity. for example, in the ontolingua ontology library, only 20% of the ontologies have any user-defined axioms. we believe the difficulty of composing axioms primarily accounts for the lack of axioms in these knowledge bases: many domain experts cannot translate their thoughts into abstract and symbolic representations. we attempt to remedy the difficulties by identifying groups of axioms that manifest common patterns, creating templates that allow users to compose axioms by filling in the blanks. we studied axioms in two public ontology libraries, and derived 20 templates that cover 85% of all the user-defined axioms. we describe our methodology for identifying the templates and present examples. we constructed an interface that allows users to create constraints on knowledge bases by filling in blanks; our usability testing shows that users could use templates to encode axioms with a success rate similar to that of experts writing directly in an axiom language. our approach should foster the introduction of axioms and constraints that are currently missing in many ontologies.",
    "present_kp": [],
    "absent_kp": [
      "frame-based system",
      "knowledge acquisition",
      "knowledge representation"
    ]
  },
  {
    "title": "critical infrastructure dependencies: a holistic, dynamic and quantitative approach.",
    "abstract": "the proper functioning of critical infrastructures is crucial to societal well-being. however, critical infrastructures are not isolated, but instead are tightly coupled, creating a complex system of interconnected infrastructures. dependencies between critical infrastructures can cause a failure to propagate from one critical infrastructure to other critical infrastructures, aggravating and prolonging the societal impact. for this reason, critical infrastructure operators must understand the complexity of critical infrastructures and the effects of critical infrastructure dependencies. however, a major problem is posed by the fact that detailed information about critical infrastructure dependencies is highly sensitive and is usually not publicly available. moreover, except for a small number of holistic and dynamic research efforts, studies are limited to a few critical infrastructures and generally do not consider time-dependent behavior. this paper analyzes how a failed critical infrastructure that cannot deliver products and services impacts other critical infrastructures, and how a critical infrastructure is affected when another critical infrastructure fails. the approach involves a holistic analysis involving multiple critical infrastructures while incorporating a dynamic perspective based on the time period that a critical infrastructure is non-operational and how the impacts evolve over time. this holistic approach, which draws on the results of a survey of critical infrastructure experts from several countries, is intended to assist critical infrastructure operators in preparing for future crises.",
    "present_kp": [
      "critical infrastructure dependencies"
    ],
    "absent_kp": [
      "holistic treatment",
      "dynamic analysis",
      "quantitative analysis"
    ]
  },
  {
    "title": "traffic distribution for end-to-end qos routing with multicast multichannel services.",
    "abstract": "with the development of multimedia group applications and multicasting demands, the construction of multicast routing tree satisfying quality of service (qos) is more important. a multicast tree, which is constructed by existing multicast algorithms, suffers three major weaknesses: (1) it cannot be constructed by multichannel routing, transmitting a message using all available links, thus the data traffic cannot be preferably distributed; (2) it does not formulate duplication capacity; consequently, duplication capacity in each node cannot be optimally distributed; (3) it cannot change the number of links and nodes used optimally. in fact, it cannot employ and cover unused backup multichannel paths optimally. to overcome these weaknesses, this paper presents a polynomial time algorithm for distributed optimal multicast routing and quality of service (qos) guarantees in networks with multichannel paths which is called distributed optimal multicast multichannel routing algorithm (dommr). the aim of this algorithm is: (1) to minimize end-to-end delay across the multichannel paths, (2) to minimize consumption of bandwidth by using all available links, and (3) to maximize data rate by formulating network resources. dommr is based on the linear programming formulation (lpf) and presents an iterative optimal solution to obtain the best distributed routes for traffic demands between all edge nodes. computational experiments and numerical simulation results will show that the proposed algorithm is more efficient than the existing methods. the simulation results are obtained by applying network simulation tools such as qsb, opnet and matlb to some samples of network. we then introduce a generalized problem, called the delay-constrained multicast multichannel routing problem, and show that this generalized problem can be solved in polynomial time.",
    "present_kp": [
      "multicasting",
      "multichannel path",
      "traffic distribution",
      "linear programming"
    ],
    "absent_kp": [
      "optimized routing",
      "quality of services"
    ]
  },
  {
    "title": "improved error exponent for time-invariant and periodically time-variant convolutional codes.",
    "abstract": "an improved upper bound on the error probability (first error event) of time-invariant convolutional codes, and the resulting error exponent, is derived ill this paper. the improved error bound depends on both the delay of the code k and its width (the number of symbols that enter the delay line in parallel) b. determining the error exponent of time-invariant convolutional codes is an open problem. while the previously known bounds on the error probability of time-invariant codes led to the block-coding exponent, obtain a better error exponent (strictly better for b > 1). in the limit b --> infinity our error exponent equals the yudkin-viterbi exponent derived for time-variant convolutional codes. these results are also used to derive an improved error exponent for periodically time-variant codes.",
    "present_kp": [
      "convolutional codes",
      "error exponent",
      "error probability",
      "periodically time-variant codes",
      "time-invariant codes",
      "yudkin-viterbi exponent"
    ],
    "absent_kp": []
  },
  {
    "title": "online reputation management for improving marketing by using a hybrid mcdm model.",
    "abstract": "online reputation management (orm) has been considered as a significant tool of internet marketing. the purpose of this paper is to construct a decision model for evaluating performances and improving professional services of marketing. to investigate the interrelationship and influential weights among criteria, this study uses a hybrid mcdm model including decision-making trial and evaluation laboratory (dematel), dematel-based analytic network process (called danp). the empirical findings reveal that criteria have self-effect relationships based on dematel technique. according to the network relation map (nrm), the dimension that professional services of marketing should improve first when carrying out orm is online reputation. in the five criteria for evaluation, distributed reputation systems is the most important criterion impacting orm, followed by employees and social responsibility.",
    "present_kp": [
      "online reputation management ",
      "professional services of marketing",
      "dematel",
      "danp",
      "mcdm"
    ],
    "absent_kp": []
  },
  {
    "title": "feasibility of a primarily digital research library.",
    "abstract": "this position paper explores the issues related to the feasibility of having a primarily digital research library support the teaching and research needs of a university. the asian university for women (auw), a new university in chittagong, bangladesh, will open in september 2009. it must make a decision regarding the investment to be made in research resources to support the university. mass digitization efforts now make it possible to consider establishing a research library that consists primarily of digital resources rather than print. there are, however, many issues that make this consideration quite complex and far from certain. in this paper we explore the issues at a preliminary level. we focus on four broad perspectives in order to begin addressing the complex interactions that must be considered in transitioning to a primarily digital research environment: technical, economic, policy and social issues. the purpose of this paper is to begin to explore a research agenda for transitioning from a model for libraries where resources are primarily print to one that is predominantly digital. our research in this area is just beginning, so our purpose is to raise the issues rather than offer firm conclusions.",
    "present_kp": [
      "libraries",
      "digital research",
      "mass digitization"
    ],
    "absent_kp": [
      "digital libraries"
    ]
  },
  {
    "title": "targeting multiple myeloma cells and their bone marrow microenvironment.",
    "abstract": "although multiple myeloma (mm) is sensitive to chemotherapy and radiation therapy, long-term disease-free survival is rare, and mm remains incurable despite conventional and high-dose therapies. direct (cell-cell contact) and soluble (via cytokines) forms of interactions between mm cells and bone marrow stroma regulate growth, survival, and homing of mm cells. these interactions also play a critical role in angiogenesis and in myeloma bone disease. in recent years, several studies have established the biologic significance of cytokines in mm pathogenesis and delineated signaling cascades mediating their effects, providing the framework for related novel therapies targeting not only the mm cell, but also the bone marrow microenvironment.",
    "present_kp": [
      "multiple myeloma",
      "bone marrow microenvironment",
      "novel therapies"
    ],
    "absent_kp": []
  },
  {
    "title": "a network service curve approach for the stochastic analysis of networks.",
    "abstract": "the stochastic network calculus is an evolving new methodology for backlog and delay analysis of networks that can account for statistical multiplexing gain. this paper advances the stochastic network calculus by deriving a network service curve, which expresses the service given to a flow by the network as a whole in terms of a probabilistic bound. the presented network service curve permits the calculation of statistical end-to-end delay and backlog bounds for broad classes of arrival and service distributions. the benefits of the derived service curve are illustrated for the exponentially bounded burstiness (ebb) traffic model. it is shown that end-to-end performance measures computed with a network service curve are bounded by o ( h log h ), where h is the number of nodes traversed by a flow. using currently available techniques that compute end-to-end bounds by adding single node results, the corresponding performance measures are bounded by o ( h 3 ).",
    "present_kp": [
      "network service curve",
      "stochastic network calculus"
    ],
    "absent_kp": [
      "quality-of-service"
    ]
  },
  {
    "title": "mitigating kinematic locking in the material point method.",
    "abstract": "the material point method exhibits kinematic locking when traditional linear shape functions are used with a rectangular grid. the locking affects both the strain and the stress fields, which can lead to inaccurate results and nonphysical behavior. this paper presents a new anti-locking approach that mitigates the accumulation of fictitious strains and stresses, significantly improving the kinematic response and the quality of all field variables. the technique relies on the huwashizu multi-field variational principle, with separate approximations for the volumetric and the deviatoric portions of the strain and stress fields. the proposed approach is validated using a series of benchmark examples from both solid and fluid mechanics, demonstrating the broad range of modeling possibilities within the mpm framework when combined with appropriate anti-locking techniques and algorithms.",
    "present_kp": [
      "material point method",
      "locking"
    ],
    "absent_kp": [
      "meshfree methods",
      "particle methods"
    ]
  },
  {
    "title": "computational dialectic and rhetorical invention.",
    "abstract": "this paper has three dimensions, historical, theoretical and social. the historical dimension is to show how the ciceronian system of dialectical argumentation served as a precursor to computational models of argumentation schemes such as araucaria and carneades. the theoretical dimension is to show concretely how these argumentation schemes reveal the interdependency of rhetoric and logic, and so the interdependency of the normative with the empirical. it does this by identifying points of disagreement in a dialectical format through using argumentation schemes and critical questions. the social dimension is to show how the ciceronian dialectical viewpoint integrates with the use of computational tools that can be used to support the principle of reason-based deliberation and facilitate deliberative democracy.",
    "present_kp": [
      "argumentation schemes",
      "deliberative democracy"
    ],
    "absent_kp": [
      "informal logic",
      "ciceronian rhetoric",
      "carneades model",
      "fallacies",
      "persuasion"
    ]
  },
  {
    "title": "the roadmaker's algorithm for the discrete pulse transform.",
    "abstract": "the discrete pulse transform (dpt) is a decomposition of an observed signal into a sum of pulses, i.e., signals that are constant on a connected set and zero elsewhere. originally developed for 1-d signal processing, the dpt has recently been generalized to more dimensions. applications in image processing are currently being investigated. the time required to compute the dpt as originally defined via the successive application of lulu operators (members of a class of minimax filters studied by rohwer) has been a severe drawback to its applicability. this paper introduces a fast method for obtaining such a decomposition, called the roadmaker's algorithm because it involves filling pits and razing bumps. it acts selectively only on those features actually present in the signal, flattening them in order of increasing size by sub-tracing an appropriate positive or negative pulse, which is then appended to the decomposition. the implementation described here covers 1-d signal as well as two and 3-d image processing in a single framework. this is achieved by considering the signal or image as a function defined on a graph, with the geometry specified by the edges of the graph. whenever a feature is flattened, nodes in the graph are merged, until eventually only one node remains. at that stage, a new set of edges for the same nodes as the graph, forming a tree structure, defines the obtained decomposition. the roadmaker's algorithm is shown to be equivalent to the dpt in the sense of obtaining the same decomposition. however, its simpler operators are not in general equivalent to the lulu operators in situations where those operators are not applied successively. a by-product of the roadmaker's algorithm is that it yields a proof of the so-called highlight conjecture, stated as an open problem in 2006. we pay particular attention to algorithmic details and complexity, including a demonstration that in the 1-d case, and also in the case of a complete graph, the roadmaker's algorithm has optimal complexity: it runs in time o(m), where m is the number of arcs in the graph.",
    "present_kp": [],
    "absent_kp": [
      "clustering algorithms",
      "digital filters",
      "digital signal processing",
      "discrete transforms",
      "multidimensional signal processing",
      "nonlinear filters",
      "signal analysis",
      "tree graphs"
    ]
  },
  {
    "title": "implementation relations and test generation for systems with distributed interfaces.",
    "abstract": "some systems interact with their environment at physically distributed interfaces called ports and we separately observe sequences of inputs and outputs at each port. as a result we cannot reconstruct the global sequence that occurred and this reduces our ability to distinguish different systems in testing or in use. in this paper we explore notions of conformance for an input output transition system that has multiple ports, adapting the widely used ioco implementation relation to this situation. we consider two different scenarios. in the first scenario the agents at the different ports are entirely independent. alternatively, it may be feasible for some external agent to receive information from more than one of the agents at the ports of the system, these local behaviours potentially being brought together and here we require a stronger implementation relation. we define implementation relations for these scenarios and prove that in the case of a single-port system the new implementation relations are equivalent to ioco. in addition, we define what it means for a test case to be controllable and give an algorithm that decides whether this condition holds. we give a test generation algorithm to produce sound and complete test suites. finally, we study two implementation relations to deal with partially specified systems.",
    "present_kp": [],
    "absent_kp": [
      "formal approaches to testing",
      "systems with distributed ports",
      "formal methodologies to develop distributed software systems"
    ]
  },
  {
    "title": "dual-centers type-2 fuzzy clustering framework and its verification and validation indices.",
    "abstract": "the clustering model considers dual-centers rather than single centers. the dual-centers type-2 clustering model and algorithm are proposed. the relations among parameters of the proposed model are explained. the degrees of belonging to the clusters are defined by type-2 fuzzy numbers. the verification and verification indices are developed for model evaluation.",
    "present_kp": [],
    "absent_kp": [
      "dual-centers clustering",
      "interval type-2 fuzzy clustering",
      "pcm",
      "cluster center uncertainty",
      "validation index",
      "verification index"
    ]
  },
  {
    "title": "generalized sharing in survivable optical networks.",
    "abstract": "shared path protection has been demonstrated to be a very efficient survivability scheme for optical networking. in this scheme, multiple backup paths can share a given optical channel if their corresponding primary routes are not expected to fail simultaneously. the focus in this area has been the optimization of the total channels (i.e., bandwidth) provisioned in the network through the intelligent routing of primary and backup routes. in this work, we extend the current path protection sharing scheme and introduce the generalized sharing concept. in this concept, we allow for additional sharing of important node devices. these node devices (e.g., optical-electronic-optical regenerators (oeos), pure all-optical converters, etc.) constitute the dominant cost factor in an optical backbone network and the reduction of their number is of paramount importance. for demonstration purposes, we extend the concept of 1:n shared path protection to allow for the sharing of electronic regenerators needed for coping with optical transmission impairments. both design and control plane issues are discussed through numerical examples. considerable cost reductions in electronic budget are demonstrated.",
    "present_kp": [
      "optical networks"
    ],
    "absent_kp": [
      "shared protection"
    ]
  },
  {
    "title": "on the usefulness of knowledge of error variances in the consistent estimation of an unreplicated ultrastructural model.",
    "abstract": "this article considers an unreplicated ultrastructural model and discusses the asymptotic properties of three consistent estimators of slope parameter arising from the knowledge of measurement error variances. conditions are deduced when knowing the error variances associated with both the study and the explanatory variables is more/less beneficial than using a single error variance in the formulation of slope estimators.",
    "present_kp": [
      "error variance",
      "ultrastructural model"
    ],
    "absent_kp": [
      "measurement errors",
      "reliability ratio"
    ]
  },
  {
    "title": "exact solution of the heat equation with boundary condition of the fourth kind by hes variational iteration method.",
    "abstract": "in this paper, solutions of the heat equation with the boundary condition of the fourth kind are presented. the proposed solution is based on hes variational iteration method, after the application of which the exact solution of the problem is obtained.",
    "present_kp": [
      "heat equation",
      "variational iteration method"
    ],
    "absent_kp": []
  },
  {
    "title": "efficient neighborhood search for the one-machine earlinesstardiness scheduling problem.",
    "abstract": "this paper addresses the one-machine scheduling problem where the objective is to minimize a sum of costs such as earlinesstardiness costs. since the sequencing problem is np-hard, local search is very useful for finding good solutions. unlike scheduling problems with regular cost functions, the scheduling (or timing) problem is not trivial when the sequence is fixed. therefore, the local search approaches must deal with both job interchanges in the sequence and the timing of the sequenced jobs. we present a new approach that efficiently searches in a large neighborhood and always returns a solution for which the timing is optimal.",
    "present_kp": [
      "scheduling",
      "earlinesstardiness cost"
    ],
    "absent_kp": [
      "single machine",
      "neighborhoods",
      "search method"
    ]
  },
  {
    "title": "power characteristics of inductive interconnect.",
    "abstract": "the width of an interconnect line affects the total power consumed by a circuit. the effect of wire sizing on the power characteristics of an inductive interconnect line is presented in this paper. the matching condition between the driver and the load affects the power consumption since the short-circuit power dissipation may decrease and the dynamic power will increase with wider lines. a tradeoff, therefore, exists between short-circuit and dynamic power in inductive interconnects. the short-circuit power increases with wider linewidths only if the line is underdriven. the power characteristics of inductive interconnects therefore may have a great influence on wire sizing optimization techniques. an analytic solution of the transition time of a signal propagating along an inductive interconnect with an error of less than 15% is presented. the solution is useful in wire sizing synthesis techniques to decrease the overall power dissipation. the optimum linewidth that minimizes the total transient power dissipation is determined. an analytic solution for the optimum width with an error of less than 6% is presented. for a specific set of line parameters and resistivities, a reduction in power approaching 80% is achieved as compared to the minimum wire width. considering the driver size in the design process, the optimum wire and driver size that minimizes the total transient power is also determined.",
    "present_kp": [
      "dynamic power",
      "inductive interconnect",
      "short-circuit power",
      "transient power dissipation"
    ],
    "absent_kp": [
      "characteristic impedance",
      "underdamped systems"
    ]
  },
  {
    "title": "convergence acceleration of rungekutta schemes for solving the navierstokes equations.",
    "abstract": "the convergence of a rungekutta (rk) scheme with multigrid is accelerated by preconditioning with a fully implicit operator. with the extended stability of the rungekutta scheme, cfl numbers as high as 1000 can be used. the implicit preconditioner addresses the stiffness in the discrete equations associated with stretched meshes. this rk/implicit scheme is used as a smoother for multigrid. fourier analysis is applied to determine damping properties. numerical dissipation operators based on the roe scheme, a matrix dissipation, and the cusp scheme are considered in evaluating the rk/implicit scheme. in addition, the effect of the number of rk stages is examined. both the numerical and computational efficiency of the scheme with the different dissipation operators are discussed. the rk/implicit scheme is used to solve the two-dimensional (2-d) and three-dimensional (3-d) compressible, reynolds-averaged navierstokes equations. turbulent flows over an airfoil and wing at subsonic and transonic conditions are computed. the effects of the cell aspect ratio on convergence are investigated for reynolds numbers between 5.7106 5.7 10 6 and 100106 100 10 6 . it is demonstrated that the implicit preconditioner can reduce the computational time of a well-tuned standard rk scheme by a factor between 4 and 10.",
    "present_kp": [
      "fourier analysis",
      "multigrid"
    ],
    "absent_kp": [
      "navier\u2013stokes",
      "runge\u2013kutta",
      "implicit preconditioning"
    ]
  },
  {
    "title": "realizations of the game domination number.",
    "abstract": "domination game is a game on a finite graph which includes two players. first player, dominator, tries to dominate a graph in as few moves as possible; meanwhile the second player, staller, tries to hold him back and delay the end of the game as long as she can. in each move at least one additional vertex has to be dominated. the number of all moves in the game in which dominator makes the first move and both players play optimally is called the game domination number and is denoted by (gamma _g). the total number of moves in a staller-start game is denoted by (gamma _g^{prime }). it is known that (|gamma _g(g)-gamma _g^{prime }(g)|le 1) for any graph (g). graph (g) realizes a pair ((k,l)) if (gamma _g(g)=k) and (gamma _g^{prime }(g)=l). it is shown that pairs ((2k,2k-1)) for all (kge 2) can be realized by a family of 2-connected graphs. we also present 2-connected classes which realize pairs ((k,k)) and ((k,k+1)). exact game domination number for combs and 1-connected realization of the pair ((2k+1,2k)) are also given.",
    "present_kp": [
      "domination game",
      "game domination number",
      "realizations"
    ],
    "absent_kp": []
  },
  {
    "title": "lumiproxy: a hybrid representation of image-based models.",
    "abstract": "in this paper, we present a hybrid representation of image-based models combining the textured planes and the hierarchical points. taking a set of depth images as input, our method starts from classifying input pixels into two categories, indicating the planar and non-planar surfaces respectively. for the planar surfaces, the geometric coefficients are reconstructed to form the uniformly sampled textures. for nearly planar surfaces, some textured planes, called lumiproxies, are constructed to represent the equivalent visual appearance. the hough transform is used to find the positions of these textured planes, and optic flow measures are used to determine their textures. for remaining pixels corresponding to the non-planar geometries, the point primitive is applied, reorganized as the obb-tree structure. then, texture mapping and point splatting are employed together to render the novel views, with the hardware acceleration.",
    "present_kp": [
      "lumiproxy"
    ],
    "absent_kp": [
      "sampling",
      "surface fitting",
      "image-based rendering"
    ]
  },
  {
    "title": "constraint based methods for biological sequence analysis.",
    "abstract": "the need for processing biological information is rapidly growing, owing to the masses of new information in digital form being produced at this time. old methodologies for processing it can no longer keep up with this rate of growth. the methods of artificial intelligence (ai) in general and of language processing in particular can offer much towards solving this problem. however, interdisciplinary research between language processing and molecular biology is not yet widespread, partly because of the effort needed for each specialist to understand the other one's jargon. we argue that by looking at the problems of molecular biology from a language processing perspective, and using constraint based logic methodologies we can shorten the gap and make interdisciplinary collaborations more effective. we shall discuss several sequence analysis problems in terms of constraint based formalisms such concept formation rules, constraint handling rules (chr) and their grammatical counterpart, chrg. we postulate that genetic structure analysis can also benefit from these methods, for instance to reconstruct from a given rna secondary structure, a nucleotide sequence that folds into it. our proposed methodologies lend direct executability to high level descriptions of the problems at hand and thus contribute to rapid while efficient prototyping.",
    "present_kp": [
      "rna secondary structure",
      "concept formation",
      "constraint handling rules"
    ],
    "absent_kp": [
      "protein structure",
      "gene prediction",
      "constraint handling rule grammars"
    ]
  },
  {
    "title": "adaptive load balancing algorithm for multiple homing mobile nodes.",
    "abstract": "in places where mobile users can access multiple wireless networks simultaneously, a multipath scheduling algorithm can benefit the performance of wireless networks and improve the experience of mobile users. however, existing literature shows that it may not be the case, especially for tcp flows. according to early investigations, there are mainly two reasons that result in bad performance of tcp flows in wireless networks. one is the occurrence of out-of-order packets due to different delays in multiple paths. the other is the packet loss which is resulted from the limited bandwidth of wireless networks. to better exploit multipath scheduling for tcp flows, this paper presents a new scheduling algorithm named adaptive load balancing algorithm (albam) to split traffic across multiple wireless links within the isp infrastructure. targeting at solving the two adverse impacts on tcp flows, albam develops two techniques. firstly, albam takes advantage of the bursty nature of tcp flows and performs scheduling at the flowlet granularity where the packet interval is large enough to compensate for the different path delays. secondly, albam develops a packet number estimation algorithm (pnea) to predict the buffer usage in each path. with pnea, albam can prevent buffer overflow and schedule the tcp flow to a less congested path before it suffers packet loss. simulations show that albam can provide better performance to tcp connections than its other counterparts.",
    "present_kp": [],
    "absent_kp": [
      "multiple path scheduling",
      "multiple interface",
      "local domain"
    ]
  },
  {
    "title": "query by output.",
    "abstract": "it has recently been asserted that the usability of a database is as important as its capability. understanding the database schema, the hidden relationships among attributes in the data all play an important role in this context. subscribing to this viewpoint, in this paper, we present a novel data-driven approach, called query by output (qbo) , which can enhance the usability of database systems. the central goal of qbo is as follows: given the output of some query q on a database d , denoted by q ( d ), we wish to construct an alternative query q ? such that q ( d ) and q ? ( d ) are instance-equivalent. to generate instance-equivalent queries from q ( d ), we devise a novel data classification-based technique that can handle the at-least-one semantics that is inherent in the query derivation. in addition to the basic framework, we design several optimization techniques to reduce processing overhead and introduce a set of criteria to rank order output queries by various notions of utility. our framework is evaluated comprehensively on three real data sets and the results show that the instance-equivalent queries we obtain are interesting and that the approach is scalable and robust to queries of different selectivities.",
    "present_kp": [
      "query by output",
      "at-least-one semantics",
      "instance-equivalent queries"
    ],
    "absent_kp": []
  },
  {
    "title": "three-dimensional quantitative structureactivity relationships study on hiv-1 reverse transcriptase inhibitors in the class of dipyridodiazepinone derivatives, using comparative molecular field analysis1.",
    "abstract": "a three-dimensional quantitative structureactivity relationships (3d qsar) method, comparative molecular field analysis (comfa), was applied to a set of dipyridodiazepinone (nevirapine) derivatives active against wild-type (wt) and mutant-type (y181c) hiv-1 reverse transcriptase. the starting geometry of dipyridodiazepinone was taken from x-ray crystallographic data. all 75 derivatives, divided into a training set of 53 compounds and a test set of 22 molecules, were then constructed and full geometrical optimizations were performed, based on a semiempirical molecular orbital method (am1). comfa was used to discriminate between structural requirements for wt and y181c inhibitory activities. the resulting comfa models yield satisfactory predictive ability regarding wt and y181c inhibitions, with r2cv = 0.624 and 0.726, respectively. comfa contour maps reveal that steric and electrostatic interactions corresponding to the wt inhibition amount to 58.5% and 41.5%, respectively, while steric and electrostatic effects have approximately equal contributions for the explanation of inhibitory activities against y181c. the contour maps highlight different characteristics for different types of wild-type and mutant-type hiv-1 rt. in addition, these contour maps agree with experimental data for the binding topology. consequently, the results obtained provide information for a better understanding of the inhibitorreceptor interactions of dipyridodiazepinone analogs. 2000 elsevier science inc.",
    "present_kp": [
      "hiv-1 rt",
      "nevirapine",
      "comfa"
    ],
    "absent_kp": [
      "nnrti",
      "3d-qsar",
      "quantum chemical calculations",
      "molecular modeling"
    ]
  },
  {
    "title": "a shared-memory implementation of the hierarchical radiosity method.",
    "abstract": "the radiosity method is a simulation method from computer graphics to visualize the global illumination in scenes containing diffuse objects within an enclosure. a variety of realizations (including parallel approaches) were proposed to achieve a high efficiency while guaranteeing the same accuracy of the graphical representation. the hierarchical radiosity method reduces the computational costs considerably but results in a highly irregular algorithm which makes a parallel implementation more difficult. we investigate a task-oriented shared memory implementation and present optimizations with different behavior concerning locality and granularity. to be able to concentrate on load balancing and scalability issues, we use a shared-memory machine with uniform memory access time, the sb-pram.",
    "present_kp": [
      "hierarchical radiosity method",
      "shared memory implementation",
      "scalability",
      "granularity"
    ],
    "absent_kp": [
      "task-parallelism"
    ]
  },
  {
    "title": "a transaction mapping algorithm for frequent itemsets mining.",
    "abstract": "in this paper, we present a novel algorithm for mining complete frequent itemsets. this algorithm is referred to as the tm ( transaction mapping) algorithm from hereon. in this algorithm, transaction ids of each itemset are mapped and compressed to continuous transaction intervals in a different space and the counting of itemsets is performed by intersecting these interval lists in a depth-first order along the lexicographic tree. when the compression coefficient becomes smaller than the average number of comparisons for intervals intersection at a certain level, the algorithm switches to transaction id intersection. we have evaluated the algorithm against two popular frequent itemset mining algorithms, fp-growth and declat, using a variety of data sets with short and long frequent patterns. experimental data show that the tm algorithm outperforms these two algorithms.",
    "present_kp": [
      "algorithms",
      "frequent itemsets"
    ],
    "absent_kp": [
      "association rule mining",
      "data mining"
    ]
  },
  {
    "title": "semi-continuous network flow problems.",
    "abstract": "we consider semi-continuous network flow problems, that is, a class of network flow problems where some of the variables are restricted to be semi-continuous. we introduce the semi-continuous inflow set with variable upper bounds as a relaxation of general semi-continuous network flow problems. two particular cases of this set are considered, for which we present complete descriptions of the convex hull in terms of linear inequalities and extended formulations. we consider a class of semi-continuous transportation problems where inflow systems arise as substructures, for which we investigate complexity questions. finally, we study the computational efficacy of the developed polyhedral results in solving randomly generated instances of semi-continuous transportation problems.",
    "present_kp": [
      "network flow problems",
      ""
    ],
    "absent_kp": [
      "mixed-integer programming",
      "semi-continuous variables"
    ]
  },
  {
    "title": "construct message authentication code with one-way hash functions and block ciphers.",
    "abstract": "we suggest an mac scheme which combines a hash function and an block cipher in order. we strengthen this scheme to prevent the problem of leaking the intermediate hash value between the hash function and the block cipher by additional random bits. the requirements to the used hash function are loosely. security of the proposed scheme is heavily dependent on the underlying block cipher. this scheme is efficient on software implementation for processing long messages and has clear security properties.",
    "present_kp": [
      "mac",
      "one-way hash function",
      "block cipher"
    ],
    "absent_kp": [
      "cryptography"
    ]
  },
  {
    "title": "estimation of uncertainty in dynamic simulation results.",
    "abstract": "this paper presents a new approach for calculation of uncertainty in dynamic simulation results. the statistical moments (mean, variance, skewness etc.) of the simulation results are calculated using gaussian-quadrature with ''customized'' weight function. based on these moments, an approximating probability density function (pdf) is created by expansion into orthogonal polynomial series. the percentiles of the distribution can then be calculated. the method is computationally less demanding than monte-carlo simulation when the number of uncertain parameters are limited. a number of examples are used to illustrate the applicability of the proposed framework.",
    "present_kp": [
      "dynamic simulation"
    ],
    "absent_kp": [
      "uncertainty propagation",
      "stochastic simulation"
    ]
  },
  {
    "title": "a neural implementation of the jade algorithm (njade) using higher-order neurons.",
    "abstract": "a neural implementation of the jade algorithm, called njade, is developed which adaptively determines the mixing matrices to be jointly diagonalized with the jade algorithm. this alleviates the problem of algebraically determining these mixing matrices which becomes a very tedious if not impossible undertaking with high-dimensional data. the new learning rule uses higher-order neurons and generalizes oja's pca learning rule. as a test case the new njade algorithm is applied to high-dimensional natural image ensembles to learn appropriate edge filter structures. quantitative comparison concerning various filter characteristics is made with results obtained with a probabilistic ica algorithm with kernel-based source density estimation.",
    "present_kp": [
      "njade"
    ],
    "absent_kp": [
      "independent component analysis",
      "higher order neurons",
      "neural network",
      "natural images"
    ]
  },
  {
    "title": "jpeg 2000 encoding method for reducing tiling artifacts.",
    "abstract": "this paper proposes an effective jpeg 2000 encoding method for reducing tiling artifacts, which cause one of the biggest problems in jpeg 2000 encoders. symmetric pixel extension is generally thought to be the main factor in causing artifacts. however this paper shows that differences in quantization accuracy between tiles are a more significant reason for tiling artifacts at middle or low bit rates. this paper also proposes an algorithm that predicts whether tiling artifacts will occur at a tile boundary in the rate control process and that locally improves quantization accuracy by the original post quantization control. this paper further proposes a method for reducing processing time which is yet another serious problem in the jpeg 2000 encoder. the method works by predicting truncation points using the entropy of wavelet transform coefficients prior to the arithmetic coding. these encoding methods require no additional processing in the decoder. the experiments confirmed that tiling artifacts were greatly reduced and that the coding process was considerably accelerated.",
    "present_kp": [
      "jpeg 2000",
      "tiling artifacts",
      "rate control"
    ],
    "absent_kp": [
      "acceleration of coding process"
    ]
  },
  {
    "title": "laparoscopic myomectomy.",
    "abstract": "the appearance of uterine myomas has been linked to infertility. it has been suggested that surgical management of myomas by laparoscopic myomectomy improves fertility rates in these group of patients. in this paper we initially describe specific aspects of the surgical technique of laparoscopic myomectomy including the set-up, precise technique for hysteroromy, enucleation of the myoma, suturing of the uterus, and extraction of the myoma. we detail recent findings that demonstrate improved fertility rates in women undergoing laparoscopic myomectomy. we recommend that, when criteria for selection of patients is strictly adhered to and patients present with no other associated infertility, laparoscopic myomectomy be used to increase the implantation rate.",
    "present_kp": [
      "laparoscopic myomectomy",
      "uterine myoma",
      "fertility"
    ],
    "absent_kp": [
      "laparotomy"
    ]
  },
  {
    "title": "theoretical study on the antioxidant properties of 2'-hydroxychalcones: h-atom vs. electron transfer mechanism.",
    "abstract": "the free radical scavenging activity of six 2'-hydroxychalcones has been studied in gas phase and solvents using the density functional theory (dft) method. the three main working mechanisms, hydrogen atom transfer (hat), stepwise electron-transfer-proton-transfer (et-pt) and sequential-proton-loss-electron-transfer (splet) have been considered. the o-h bond dissociation enthalpy (bde), ionization potential (ip), proton affinity (pa) and electron transfer energy (ete) parameters have been computed in gas phase and solvents. the theoretical results confirmed the important role of the b ring in the antioxidant properties of hydroxychalcones. in addition, the calculated results matched well with experimental values. the results suggested that hat would be the most favorable mechanism for explaining the radical-scavenging activity of hydroxychalcone in gas phase, whereas splet mechanism is thermodynamically preferred pathway in aqueous solution.",
    "present_kp": [
      "dft",
      "hydrogen atom transfer",
      "hydroxychalcones",
      "radical scavenging",
      "sequential-proton-loss-electron-transfer",
      "stepwise electron-transfer-proton-transfer"
    ],
    "absent_kp": []
  },
  {
    "title": "program analysis for event-based distributed systems.",
    "abstract": "designing distributed applications around the idiom of events has several benefits including extensibility and scalability. to improve conciseness, safety, and efficiency of corresponding programs, several authors have recently proposed programming languages or language extensions with support for event-based programming. the presence of a dedicated programming language and compilation process offers avenues for program analyses to further improve simplicity, safety, and expressiveness of distributed event-based software. this paper presents three program analyses specifically designed for event-based programs: immutability analysis avoids costly cloning of events in the presence of co-located handlers for same events; guard analysis allows for simple yet expressive subscriptions which can be further simplified and handled efficiently; causality analysis determines causal dependencies among events which are related, allowing unrelated events to be transferred independently for efficiency. we convey the benefits of our approach by empirically evaluating their performance benefits.",
    "present_kp": [
      "language",
      "program analysis",
      "event",
      "distributed"
    ],
    "absent_kp": [
      "correlation"
    ]
  },
  {
    "title": "automatic determination of envelopes and other derived curves within a graphic environment.",
    "abstract": "dynamic geometry programs provide environments where accurate construction of geometric configurations can be done. nevertheless, intrinsic limitations in their standard development technology mostly produce objects that are equationally unknown and so can not be further used in constructions. in this paper, we pursue the development of a geometric system that uses in the background the symbolic capabilities of two computer algebra systems, cocoa and mathematica. the cooperation between the geometric and symbolic modules of the software is illustrated by the computation of plane envelopes and other derived curves. these curves are described both graphically and analytically. since the equations of these curves are known, the system allows the construction of new elements depending on them.",
    "present_kp": [
      "dynamic geometry",
      "envelopes"
    ],
    "absent_kp": [
      "symbolic computing",
      "symbolic-numeric interface",
      "groebner bases",
      "caustics",
      "pedals"
    ]
  },
  {
    "title": "a platform for okapi-based contextual information retrieval.",
    "abstract": "we present an extensible java-based platform for contextual retrieval based on the probabilistic information retrieval model. modules for dual indexes, relevance feedback with blind or machine learning approaches and query expansion with context are integrated into the okapi system to deal with the contextual information. this platform allows easy extension to include other types of contextual information.",
    "present_kp": [
      "contextual information retrieval"
    ],
    "absent_kp": [
      "probabilistic model"
    ]
  },
  {
    "title": "the ternary description language as a formalism for the parametric general systems theory: part iii.",
    "abstract": "this part is a continuation of the first and second parts of my article that were published in the international journal of general systems, vol. 28 (4-5), pp. 351-366; vol. 31 (2), pp. 131-151. in part iii, we deal with the construction of the axiomatic system of the ternary description language (tdl). axioms and rules of inference are formulated. on the basis of these axioms and rules some theorems of tdl are proved. several system-theoretical laws, which concern the values of systems parameters, are proved as theorems of tdl. thus the deductive construction of general systems theory is made.",
    "present_kp": [
      "axioms",
      "system-theoretical laws"
    ],
    "absent_kp": [
      "syntactical priority",
      "synonymy",
      "rules of substitution",
      "rules of replacement",
      "theorems of the tdl"
    ]
  },
  {
    "title": "definitions and approaches to model quality in model-based software development - a review of literature.",
    "abstract": "more attention is paid to the quality of models along with the growing importance of modelling in software development. we performed a systematic review of studies discussing model quality published since 2000 to identify what model quality means and how it can be improved. from forty studies covered in the review, six model quality goals were identified; i.e., correctness, completeness, consistency, comprehensibility, confinement and changeability. we further present six practices proposed for developing high-quality models together with examples of empirical evidence. the contributions of the article are identifying and classifying definitions of model quality and identifying gaps for future research.",
    "present_kp": [
      "systematic review",
      "modelling",
      "model quality"
    ],
    "absent_kp": [
      "model-driven development",
      "uml"
    ]
  },
  {
    "title": "animal identification: introduction and history.",
    "abstract": "in the beginning of the 1970 research institutes in different countries developed the first electronic animal identification systems. these systems were tested on experimental farms. the first systems were all built with the conventional components and attached to a collar around the cows neck. in the 1980s however special integrated circuits were developed minimising the size of the transponders. now in the 1990s, official organisations are testing systems for identification and registration of all animals to control movements from birth to slaughterhouse. this will enable farm livestock to be traced at the outbreak of diseases and residues in slaughter animals to be followed up. injectable transponders, electronic eartags and rumenal bolusses are being used.",
    "present_kp": [
      "transponder",
      "electronic eartag"
    ],
    "absent_kp": [
      "electronic identification",
      "ruminant bolus"
    ]
  },
  {
    "title": "a fractional variational iteration method for solving fractional nonlinear differential equations.",
    "abstract": "recently, fractional differential equations have been investigated by employing the famous variational iteration method. however, all the previous works avoid the fractional order term and only handle it as a restricted variation. a fractional variational iteration method was first proposed in wu and gave a generalized lagrange multiplier. in this paper, two fractional differential equations are approximately solved with the fractional variational iteration method.",
    "present_kp": [
      "fractional variational iteration method"
    ],
    "absent_kp": [
      "modified riemannliouville derivative",
      "fractional corrected functional"
    ]
  },
  {
    "title": "three-level privacy control for sensing-based real-world content digital diorama.",
    "abstract": "digital diorama, the sensing-based real-world content, can be constructed by integrating real-time information obtained from sensors monitoring the real world. in order to increase the benefits of viewers without violating the privacy of monitored persons, this paper proposes three-level privacy control over the monitored persons based on their agreement to the usage of their information obtained from sensors: privacy control with i) no agreement, ii) partial agreement. and iii) mutual agreement. i) presents only their positions to show where persons are. ii) additionally presents the information which can be automatically obtained from sensors such as age and gender to show what kinds of persons are where without disclosing the visual appearances. iii) presents their visual appearances based on their mutual agreement with specific viewers. our evaluation indicated that the representation simulating each privacy control presented information from sensors with acceptable privacy protection.",
    "present_kp": [
      "privacy protection",
      "sensing-based real-world content",
      "benefits of viewers"
    ],
    "absent_kp": []
  },
  {
    "title": "enhancing wireless video streaming using lightweight approximate authentication.",
    "abstract": "in this paper we propose a novel lightweight approximate authentication algorithm that provides efficient protection for wireless video streaming where bit errors are frequent. the benefits of the proposed algorithm over other algorithms are fast execution, due to its simplicity, and small message authentication code size. the algorithm is capable of detecting even a small number of bit errors in relatively small packets that are used in video streaming. these features have never previously been available at the same time. another benefit of the approximate authentication is that it supports error resilient video decoding by dropping packets with too many bit errors, thus improving the perceived quality of the video stream. the performance of the algorithm is demonstrated via simulations and measurements",
    "present_kp": [
      "approximate authentication",
      "wireless video streaming"
    ],
    "absent_kp": [
      "error resilient video coding",
      "security",
      "qos"
    ]
  },
  {
    "title": "distributed federative qos resource management.",
    "abstract": "in a distributed multimedia system qos resources have to be managed carefully to utilize the resource pool in a way that bottlenecks can be avoided. our key idea is to let the applications participate on the resource management. we propose a distributed architecture with a fine granulated, balanced resource management with explicit qos characteristics. the architecture is based on a distributed cooperative resource manager which combines both the adaption and reservation principle for guaranteeing qos. we have designed and implemented a prototype of our federative qos resource manager (fqrm) in the java environment.",
    "present_kp": [
      "qos resource management"
    ],
    "absent_kp": [
      "distributed resources",
      "cooperative resource sharing"
    ]
  },
  {
    "title": "algorithms for on-line order batching in an order picking warehouse.",
    "abstract": "in manual order picking systems, order pickers walk or ride through a distribution warehouse in order to collect items required by (internal or external) customers. order batching consists of combining these indivisible customer orders into picking orders. with respect to order batching, two problem types can be distinguished: in off-line (static) batching, all customer orders are known in advance; in on-line (dynamic) batching, customer orders become available dynamically over time. this paper considers an on-line order batching problem in which the maximum completion time of the customer orders arriving within a certain time period has to be minimized. the author shows how heuristic approaches for off-line order batching can be modified in order to deal with the on-line situation. in a competitive analysis, lower and upper bounds for the competitive ratios of the proposed algorithms are presented. the proposed algorithms are evaluated in a series of extensive numerical experiments. it is demonstrated that the choice of an appropriate batching method can lead to a substantial reduction of the maximum completion time.",
    "present_kp": [
      "order picking",
      "order batching"
    ],
    "absent_kp": [
      "warehouse management",
      "on-line optimization"
    ]
  },
  {
    "title": "passive and active reduction techniques for on-chip high-frequency digital power supply noise.",
    "abstract": "signal integrity has become a major problem in digital ic design. one cause is device scaling that results in a sharp reduction of supply voltage, creating stringent noise margin requirements to ensure functionality. this paper introduces both a novel on-chip decoupling capacitance methodology and active noise cancellation (anc) structure. the decoupling methodology focuses on quantification and location. the anc structure, with an area of 50 mu m x 55 mu m, uses decoupling capacitance to sense noise and inject a proportional current into v(ss) as a method of reduction. a chip has been designed and fabricated using tsmc's 90-nm technology. measurements show that the decoupling methodology improved the average voltage headroom loss by 17% while the anc structure improved the average voltage headroom loss by 18%.",
    "present_kp": [
      "active noise cancellation ",
      "decoupling capacitance",
      "power supply noise"
    ],
    "absent_kp": [
      "on-chip interconnect",
      "power distribution"
    ]
  },
  {
    "title": "on modal mu-calculus over finite graphs with small components or small tree width.",
    "abstract": "this paper is a continuation and correction of a paper presented by the same authors at the conference gandalf 2010. we consider the modal mu-calculus and some fragments of it. for every positive integer k we consider the class scck of all finite graphs whose strongly connected components have size at most k, and the class twk of all finite graphs of tree width at most k. as upper bounds, we show that for every k, the temporal logic ctl* collapses to alternation free mu-calculus in scck; and in tw1, the winning condition for parity games of any index n belongs to the level delta(2) of modal mu-calculus. as lower bounds, we show that buchi automata are not closed under complement in tw2 and cobuchi nondeterministic and alternating automata differ in tw1.",
    "present_kp": [
      "modal mu-calculus",
      "strongly connected component",
      "tree width"
    ],
    "absent_kp": []
  },
  {
    "title": "product line selection and pricing analysis: impact of genetic relaxations.",
    "abstract": "a model for the product line selection and pricing problem (plsp) is presented and three solution procedures based on a genetic algorithm are developed to analyze the results based on consumer preference patterns. since the plsp model is nonlinear and integer, two of the solution procedures use genetic encoding to \"relax\" the np hard model. the relaxations result in linear integer and shortest path models for the fitness evaluation which are solved using branch and bound and labeling algorithms, respectively. performance of the quality of solutions generated by the procedures is evaluated for various problem sizes and customer preference structures. the results show that the genetic relaxations provide efficient and effective solution methodologies for the problem, when compared to the pure artificial intelligence technique of genetic search. the impact of the preference structure on the product line and the managerial implications of the solution characteristics generated by the genetic relaxations are also discussed. the models can be used to explicitly consider tradeoffs between marketing and operations concerns in designing a product line.",
    "present_kp": [
      "product line",
      "pricing"
    ],
    "absent_kp": [
      "heuristics",
      "genetic algorithms"
    ]
  },
  {
    "title": "a decomposed-model predictive functional control approach to air-vehicle pitch-angle control.",
    "abstract": "the requirements for the pitch-angle control of an air vehicle are a very fast response with as few vibrations as possible. the vibrations can damage the equipment that is carried within the body of the vehicle. the main problem to deal with is the relatively fast and under damped dynamics of the vehicle and the slow actuators and sensors. we have solved the problem by using a predictive approach. the main idea of this approach is a process output prediction based on a decomposed process model. the decomposition enables the extension of the model-based approach to processes with integrative behavior such as in the case of a rocket's pitch-angle control. the proposed approach is not only useful in this case but it gives us a framework to design the control for a wide range of processes. we compared the predictive design methodology with the classical compensator control approach, known from aerospace system control. the advantage of the new approach is the reduced vibrations during the transient response.",
    "present_kp": [
      "vibrations"
    ],
    "absent_kp": [
      "compensation",
      "decomposition methods",
      "modelling",
      "predictive control"
    ]
  },
  {
    "title": "the impact of electronic medical record systems on outpatient workflows: a longitudinal evaluation of its workflow effects.",
    "abstract": "the promise of the electronic medical record (emr)1 lies in its ability to reduce the costs of health care delivery and improve the overall quality of care a promise that is realized through major changes in workflows within the health care organization. yet little systematic information exists about the workflow effects of emrs. moreover, some of the research to-date points to reduced satisfaction among physicians after implementation of the emr and increased time, i.e., negative workflow effects. a better understanding of the impact of the emr on workflows is, hence, vital to understanding what the technology really does offer that is new and unique. (i) to empirically develop a physician centric conceptual model of the workflow effects of emrs; (ii) to use the model to understand the antecedents to the physicians workflow expectation from the new emr; (iii) to track physicians satisfaction overtime, 3 months and 20 months after implementation of the emr; (iv) to explore the impact of technology learning curves on physicians reported satisfaction levels. the current research uses the mixed-method technique of concept mapping to empirically develop the conceptual model of an emr's workflow effects. the model is then used within a controlled study to track physician expectations from a new emr system as well as their assessments of the emr's performance 3 months and 20 months after implementation. the research tracks the actual implementation of a new emr within the outpatient clinics of a large northeastern research hospital. the pre-implementation survey netted 20 physician responses; post-implementation time 1 survey netted 22 responses, and time 2 survey netted 26 physician responses. the implementation of the actual emr served as the intervention. since the study was conducted within the same setting and tracked a homogenous group of respondents, the overall study design ensured against extraneous influences on the results. outcome measures were derived empirically from the conceptual model. they included 85 items that measured physician perceptions of the emr's workflow effect on the following eight issues: (1) administration, (2) efficiency in patient processing, (3) basic clinical processes, (4) documentation of patient encounter, (5) economic challenges and reimbursement, (6) technical issues, (7) patient safety and care, and (8) communication and confidentiality. the items were used to track expectations prior to implementation and they served as retrospective measures of satisfaction with the emr in post-implementation time 1 and time 2. the findings suggest that physicians conceptualize emrs as an incremental extension of older computerized provider order entries (cpoes) rather than as a new innovation. the emrs major functional advantages are seen to be very similar to, if not the same as, those of cpoes. technology learning curves play a statistically significant though minor role in shaping physician perceptions. the physicians expectations from the emr are based on their prior beliefs rather than on a rational evaluation of the emr's fit, functionality, or performance. their decision regarding the usefulness of the emr is made very early, within the first few months of use of the emr. these early perceptions then remain stable and become the lens through which subsequent experience with the emr is interpreted. the findings suggest a need for communication based interventions aimed at explaining the value, fit, and usefulness of emrs to physicians early in the pre- and immediate post-emr implementation stages.",
    "present_kp": [],
    "absent_kp": [
      "physicians adoption of emrs",
      "diffusion of technology",
      "health information technology",
      "work flow effects",
      "outpatient emr use"
    ]
  },
  {
    "title": "application of the lattice boltzmann method to flow in aneurysm with ring-shaped stent obstacles.",
    "abstract": "to resolve the characteristics of a highly complex flow, a lattice boltzmann method with an extrapolation boundary technique was used in aneurysms with and without transverse objects oil the upper wall, and results were compared with the non-stented aneurysm. the extrapolation boundary concept allows the use of cartesian grids even when the boundaries do not conform to cartesian coordinates. to case the code development and facilitate the incorporation of new physics, a new scientific programming strategy based on object-oriented concepts was developed. the reduced flow, smaller vorticity magnitude and wall shear stress, and smaller du/dy near the dome of the aneurysm were observed when the proposed stent obstacles were used. the height of the stent obstacles was more effective to reduce the vorticity near the dome of the aneurysm than the width of the stent. the rectangular stent with 20% height-of-vessel radius was observed to be optimal and decreased the magnitude of the vorticity by 21% near the dome of the aneurysm.",
    "present_kp": [
      "aneurysm",
      "lattice boltzmann method",
      "stent"
    ],
    "absent_kp": [
      "computational fluid dynamics",
      "object-oriented program"
    ]
  },
  {
    "title": "simplification rules for the coherent probability assessment problem.",
    "abstract": "in this paper we develop a procedure for checking the consistency (coherence) of a partial probability assessment. the general problem (called cpa) is np-complete, hence, to have a reasonable application some heuristic is needed. our proposal differs from others because it is based on a skilful use of the logical relations present among the events. in other approaches the consistency problem is reduced directly to the satisfiability of a system of linear constraints. here, thanks to the characterization of particular configurations and to the elimination of variables, an instance of the problem is reduced to smaller instances. to obtain such results, we introduce a procedure based on rules resembling those given by davis-putnam for the satisfiability of boolean formulas. at the end a particularized description of an actual implementation is given.",
    "present_kp": [
      "coherent probability assessment",
      "simplification rules"
    ],
    "absent_kp": [
      "probabilistic satisfiability"
    ]
  },
  {
    "title": "constructing special k-dominating sets using variations on the greedy algorithm.",
    "abstract": "this paper focuses on the efficient selection of a special type of subset of network nodes, which we call a k-spr set, for the purpose of coordinating the routing of messages through a network. such a set is a special k-hop-connected k-dominating set that has an additional property that promotes the regular occurrence of routers in all directions. the distributed algorithms introduced here for obtaining a k-spr set require that each node broadcast at most three messages to its k-hop neighbors. these transmissions can be made asynchronously. the time required to send these messages and the sizes of the resulting sets are compared by means of data collected from simulations. the main contribution is the adaptation of some variations of the distributed greedy algorithms to the problem of generating a small k-spr set. these variations are much faster than the standard distributed greedy algorithm. yet, when used with a sensible choice for a certain parameter, our empirical evidence strongly suggests that the resulting set size will generally be very close to the set size for the standard greedy algorithms.",
    "present_kp": [
      "distributed greedy algorithm",
      "routing",
      "dominating set"
    ],
    "absent_kp": [
      "ad hoc network"
    ]
  },
  {
    "title": "the convergence test of transformation performance of resource cities in china considering undesirable output.",
    "abstract": "the main challenge for sustainable development of resource cities is to work out a feasible strategy for transformation processes. this paper introduces a new approach for analysis of transformation performance. using the environmental production technology and a malmquist resource performance index (mrpi), we conduct sigma, absolute beta and conditional beta convergence tests for the transformation performance of 21 resource cities in china. the results show that mrpi does not follow the same trend as economic strength of three chinese regions. in addition, the transformation performance results exhibit a convergence trend for the 21 resource cities.",
    "present_kp": [
      "resource cities",
      "transformation performance",
      "convergence test"
    ],
    "absent_kp": []
  },
  {
    "title": "ant colony optimization based clustering methodology.",
    "abstract": "a novel aco based methodology (aco-c) is proposed for spatial clustering. it works in data sets with no a priori information. it includes solution evaluation, neighborhood construction and data set reduction. it has a multi-objective framework, and yields a set of non-dominated solutions. experimental results show that aco-c outperforms other competing approaches.",
    "present_kp": [
      "clustering",
      "ant colony optimization",
      "data set reduction"
    ],
    "absent_kp": [
      "multiple objectives"
    ]
  },
  {
    "title": "breaching euclidean distance-preserving data perturbation using few known inputs.",
    "abstract": "we examine euclidean distance-preserving data perturbation as a tool for privacy-preserving data mining. such perturbations allow many important data mining algorithms (e.g. hierarchical and k-means clustering), with only minor modification, to be applied to the perturbed data and produce exactly the same results as if applied to the original data. however, the issue of how well the privacy of the original data is preserved needs careful study. we engage in this study by assuming the role of an attacker armed with a small set of known original data tuples (inputs). little work has been done examining this kind of attack when the number of known original tuples is less than the number of data dimensions. we focus on this important case, develop and rigorously analyze an attack that utilizes any number of known original tuples. the approach allows the attacker to estimate the original data tuple associated with each perturbed tuple and calculate the probability that the estimation results in a privacy breach. on a real 16-dimensional dataset, we show that the attacker, with 4 known original tuples, can estimate an original unknown tuple with less than 7% error with probability exceeding 0.8.",
    "present_kp": [
      "euclidean distance",
      "privacy",
      "data mining",
      "data perturbation"
    ],
    "absent_kp": []
  },
  {
    "title": "consistent interactive augmentation of live camera images with correct near-field illumination.",
    "abstract": "inserting virtual objects in real camera images with correct lighting is an active area of research. current methods use a high dynamic range camera with a fish-eye lens to capture the incoming illumination. the main problem with this approach is the limitation to distant illumination. therefore, the focus of our work is a real-time description of both near - and far-field illumination for interactive movement of virtual objects in the camera image of a real room. the daylight, which is coming in through the windows, produces a spatially varying distribution of indirect light in the room; therefore a near-field description of incoming light is necessary. our approach is to measure the daylight from outside and to simulate the resulting indirect light in the room. to accomplish this, we develop a special dynamic form of the irradiance volume for real-time updates of indirect light in the room and combine this with importance sampling and shadow maps for light from outside. this separation allows object movements with interactive frame rates (10--17 fps). to verify the correctness of our approach, we compare images of synthetic objects with real objects.",
    "present_kp": [],
    "absent_kp": [
      "global illumination",
      "augmented image synthesis"
    ]
  },
  {
    "title": "dynamic programming based approximation algorithms for sequence alignment with constraints.",
    "abstract": "given two sequences x and y, the classical dynamic programming solution to the local alignment problem searches for two subsequences i subset of or equal to x and j subset of or equal to y with maximum similarity score under a given scoring scheme. in several applications, variants of this problem arise with different objectives and with length constraints on the subsequences i and j. this constraint can be explicit, such as requiring i + j greater than or equal to t, or j < t, or may be implicit such as in cyclic sequence comparison, or as in the maximization of length-normalized scores, and driven by practical considerations. we present a survey of approximation algorithms for various alignment problems with constraints, and several new approximation algorithms. these approximations are in two distinct senses: in one the constraints are satisfied but the score computed is within a prescribed tolerance of the optimum instead of the exact optimum. in another, the alignment returned is assured to have at least the optimum score with respect to the given constraints, but the length constraints are satisfied to within a prescribed tolerance from the required values. the algorithms proposed involve applications of techniques from fractional programming and dynamic programming.",
    "present_kp": [
      "local alignment",
      "cyclic sequence comparison",
      "approximation algorithm",
      "dynamic programming",
      "fractional programming"
    ],
    "absent_kp": [
      "normalized local alignment",
      "length-restricted local alignment",
      "ratio maximization"
    ]
  },
  {
    "title": "a model of multisecond timing behaviour under peak-interval procedures.",
    "abstract": "in this study, the authors developed a fundamental theory of interval timing behaviour, inspired by the learning-to-time (let) model and the scalar expectancy theory (set) model, and based on quantitative analyses of such timing behaviour. our experiments used the peak-interval procedure with rats. the proposed model of timing behaviour comprises clocks, a regulator, a mixer, a response, and memory. using our model, we calculated the basic clock speeds indicated by the subjects behaviour under such peak procedures. in this model, the scalar property can be defined as a kind of transposition, which can then be measured quantitatively. the akaike information criterion (aic) values indicated that the current model fit the data slightly better than did the set model. our model may therefore provide a useful addition to set for the analysis of timing behaviour.",
    "present_kp": [
      "akaike information criterion",
      "basic clock speed",
      "peak procedure",
      "scalar property",
      "scalar expectancy theory"
    ],
    "absent_kp": [
      "learning-to-time model"
    ]
  },
  {
    "title": "the support vector machine under test.",
    "abstract": "support vector machines (svms) are rarely benchmarked against other classification or regression methods. we compare a popular svm implementation (libsvm) to 16 classification methods and 9 regression methodsall accessible through the software rby the means of standard performance measures (classification error and mean squared error) which are also analyzed by the means of bias-variance decompositions. svms showed mostly good performances both on classification and regression tasks, but other methods proved to be very competitive.",
    "present_kp": [
      "benchmark",
      "support vector machines",
      "regression",
      "classification"
    ],
    "absent_kp": [
      "comparative study"
    ]
  },
  {
    "title": "using cascade method for table access on small devices.",
    "abstract": "users increasingly expect access to web data from a wide range of devices, both wired and wireless. the goal of our research is to inform the design of applications that support data access by providing reasonably seamless migration of web data among internet-compatible devices with minimal loss of effectiveness and efficiency. this study focuses on the tables of data on small mobile devices. in this paper we report on the results of a user study that compare effectiveness, efficiency and preference of two methods for the display and use of tables on small screens: column/row expansion and cascade, a cell based expansion method.",
    "present_kp": [
      "small screen",
      "tables"
    ],
    "absent_kp": [
      "handheld device",
      "pda",
      "auto-transformation",
      "focus + context"
    ]
  },
  {
    "title": "practical use of polynomials over the reals in proofs of termination.",
    "abstract": "nowadays, polynomial interpretations are an essential ingredient in the development of tools for proving termination. we have recently proven that polynomial interpretations over the reals are strictly better for proving polynomial termination of rewriting than those which only use integer coefficients. some essential aspects of their practical use, though, remain unexplored or underdeveloped. in this paper, we compare the two current frameworks for using polynomial intepretations over the reals and show that one of them is strictly better than the other, thus making a suitable choice for implementations. we also prove that the use of algebraic real co-efficients in the interpretations suffice for termination proofs. we also discuss the use of algorithms and techniques from tarski's first-order logic of the real closed fields for implementing their use in proofs of termination. we argue that more standard constraint-solving techniques are better suited for this. we propose an algorithm to solve the polynomial constraints which arise when specific finite subsets of rational (or even algebraic real) numbers are considered for giving value to the coefficients. we provide a preliminary experimental evaluation of the algorithm which has been implemented as part of the termination tool mu-term.",
    "present_kp": [
      "termination"
    ],
    "absent_kp": [
      "program analysis",
      "term rewriting",
      "polynomial orderings"
    ]
  },
  {
    "title": "detecting coherent energy.",
    "abstract": "we apply the mathematics of cognitive radio to a single receiver to obtain a new coherent energy metric. this allows us to derive the time correlation law separating gaussian colored noise from coherent signal energy.",
    "present_kp": [
      "cognitive radio",
      "coherent energy",
      "colored noise"
    ],
    "absent_kp": [
      "random matrix theory"
    ]
  },
  {
    "title": "optimal, quality-aware scheduling of data consumption in mobile ad hoc networks.",
    "abstract": "in this paper we study the delivery of quality contextual information in mobile ad-hoc networks. we consider that information has a certain quality level that fades over time. mobile context-aware applications receive and process disseminated information given that the corresponding quality is above the lowest level. the necessity for optimally scheduling information delivery arises from the dynamic nature of the network, e.g., probabilistic spreading, caching, deferred delivery, and mobility of nodes. we propose two policies for optimal scheduling information delivery consumption based on the optimal stopping theory. the mobile nodes delay the reporting of information to mobile context-aware applications in search for better quality. the proposed policies efficiently deal with the delivery of quality information in mobile ad-hoc networks.",
    "present_kp": [
      "mobile ad-hoc networks",
      "optimal stopping theory"
    ],
    "absent_kp": [
      "quality information delivery"
    ]
  },
  {
    "title": "partnering enhanced-nlp with semantic analysis in support of information extraction.",
    "abstract": "information extraction using natural language processing (nlp) tools focuses on extracting explicitly stated information from textual material. this includes named entity recognition (ner), which produces entities and some of the relationships that may exist among them. intelligent analysis requires examining the entities in the context of the entire document. while some of the relationships among the recognized entities may be preserved during extraction, the overall context of a document may not be preserved. in order to perform intelligent analysis on the extracted information, we provide an ontology, which describes the domain of the extracted information, in addition to rules that govern the classification and interpretation of added elements. the ontology is at the core of an interactive system that assists analysts with the collection, extraction, organization, analysis and retrieval of information, with the topic of \"terrorism financing\" as a case study. user interaction provides valuable assistance in assigning meaning to extracted information. the system is designed as a set of tools to provide the user with the flexibility and power to ensure accurate inference. this case study demonstrates the information extraction features as well as the inference power that is supported by the ontology.",
    "present_kp": [
      "semantic analysis",
      "information extraction",
      "nlp",
      "natural language processing",
      "intelligent analysis",
      "ontology"
    ],
    "absent_kp": [
      "modeling",
      "owl"
    ]
  },
  {
    "title": "on stabilization of gradient-based training strategies for computationally intelligent systems.",
    "abstract": "this paper develops a novel training methodology for computationally intelligent systems utilizing gradient information in parameter updating. the devised scheme uses the first-order dynamic model of the training procedure and applies the variable structure systems approach to control the training dynamics. this results in an optimal selection of the learning rate, which is continually updated as prescribed by the adopted strategy. the parameter update rule is then mixed with the conventional error backpropagation method in a weighted average. the paper presents an analysis of the imposed dynamics, which is the response of the training dynamics driven solely by the inputs designed by variable structure control approach. the analysis continues with the global stability proof of the mixed training methodology and the restrictions on the design parameters. the simulation studies presented are focused on the advantages of the proposed scheme with regards to the compensation of the adverse effects of the environmental disturbances and its capability to alleviate the inherently nonlinear behavior of the system under investigation. the performance of the scheme is compared with that of a conventional backpropagation, it is observed that the method presented is robust under noisy observations and time varying parameters due to the integration of gradient descent technique with variable structure systems methodology, in the application example studied, control of a two degrees of freedom direct-drive robotic manipulator is considered. a standard fuzzy system is chosen as the controller in which the adaptation is carried out only on the defuzzifier parameters.",
    "present_kp": [
      "gradient descent",
      "variable structure systems"
    ],
    "absent_kp": [
      "fuzzy control",
      "stable training"
    ]
  },
  {
    "title": "improving recruit distribution decisions in the us marine corps.",
    "abstract": "the united states marine corps (usmc) accomplishes its mission to put the right marine in the right place at the right time with the right skills and quality of life in various ways. one of these is a recruit distribution modeling (rdm) and information system that assigns new recruits to entry-level schools, thereby determining the entire career paths. this article proposes improvements to the existing marine corps decision processes and information systems for recruit distribution. the proposed system, recruit distribution decision support system (rddss), provides intuitive navigation through a hierarchy of switchboards, and promotes data integrity by eliminating manual data entry for data already available in the system. it incorporates four objective measures for understanding the quality of proposed distributions, and allows the user to generate and compare multiple solutions based on the trade-off between these objectives. it is a fully functional working prototype system that was installed into the usmc manpower environment, and demonstrated to provide several improvements over the current technology.",
    "present_kp": [
      "decision support system",
      "recruit distribution"
    ],
    "absent_kp": [
      "manpower modeling"
    ]
  },
  {
    "title": "boundary effects on the soil water characteristic curves obtained from lattice boltzmann simulations.",
    "abstract": "pore-scale simulations using a lattice boltzmann method (lbm)-based numerical model were conducted to examine how the capillary pressure (pc) ( p c ) and saturation (s ) evolve within a virtual porous medium subjected to drainage and imbibition cycles. the results show the presence of a sharp front (interface separating the wetting and non-wetting fluids) across the cell during the test, which expectably moves up and down as the controlling non-wetting fluid pressure at the upper boundary varies to simulate different pc p c levels over the drainage and imbibition cycle. this phenomenon, representing inhomogeneity at the simulated scale, is in conflict with the homogenization applied to the pressure cell for deriving the constitutive pcs p c s relationship. different boundary conditions, adopted to achieve more homogeneous states in the virtual soil, resulted in different pcs p c s curves. no unique relationship between pc p c and s , even with the interfacial area (anw) ( a nw ) included, could be found. this study shows dependence of the lbm-predicted pcs p c s relation on the chosen boundary conditions. this effect should be taken into account in future numerical studies of multiphase flow within porous media.",
    "present_kp": [],
    "absent_kp": [
      "lattice boltzmann methods",
      "unsaturated soil physics"
    ]
  },
  {
    "title": "a generic sampling framework for improving anomaly detection in the next generation network.",
    "abstract": "the heterogeneous nature of network traffic in next generation networks (ngns) may impose scalability issue to traffic monitoring applications. while this issue can be well addressed by existing sampling approaches, owing to their inherent 'lossy' characteristic and data reduction principle, traditional sampling techniques suffer from incomplete traffic statistics, which can lead to inaccurate inferences of the network traffic. by focusing on two distinct traffic monitoring applications, namely, anomaly detection and traffic measurement, we highlight the possibility of addressing the accuracy of both applications without having to sacrifice one for the sake of the other. in light of this, we propose a generic sampling framework, which is capable of providing creditable network traffic statistics for accurate anomaly detection in the non, while at the same time preserves the principal purpose of sampling (i.e., to sample dominant traffic flows for accurate traffic measurement), and thus addressing the accuracy of both applications concurrently. with the emphasize on the accuracy of anomaly detection and the scalability of monitoring devices, the performance evaluation over real network traces demonstrates the superiority of the proposed framework over traditional sampling techniques.",
    "present_kp": [
      "sampling framework",
      "accuracy",
      "scalability",
      "anomaly detection",
      "traffic measurement"
    ],
    "absent_kp": [
      "next generation network "
    ]
  },
  {
    "title": "the (k)-separator problem: polyhedra, complexity and approximation results.",
    "abstract": "given a vertex-weighted undirected graph (g=(v,e,w)) and a positive integer (k), we consider the (k)-separator problem: it consists in finding a minimum-weight subset of vertices whose removal leads to a graph where the size of each connected component is less than or equal to (k). we show that this problem can be solved in polynomial time for some graph classes including bounded treewidth, (m k_2)-free, ((g_1, g_2, g_3, p_6))-free, interval-filament, asteroidal triple-free, weakly chordal, interval and circular-arc graphs. polyhedral results with respect to the convex hull of the incidence vectors of (k)-separators are reported. approximation algorithms are also presented.",
    "present_kp": [
      "approximation algorithms",
      "polyhedra"
    ],
    "absent_kp": [
      "graph partitioning",
      "complexity theory",
      "optimization"
    ]
  },
  {
    "title": "structural and electronic properties of z isomers of (4 alpha -> 6 '',2 alpha -> o -> 1 '')-phenylflavans substituted with r=h, oh and och3 calculated in aqueous solution with pcm solvation model.",
    "abstract": "in the search for new antioxidants, flavan structures called our attention, as substructures of many important natural compounds, including catechins (flavan-3-ols), simple and dimeric proanthocyanidins, and condensed tannins. in this work the conformational space of the z-isomers of (4 alpha -> 6 '', 2 alpha -> o -> 1 '')-phenylflavans substituted with r=h, oh and och3 was scanned in aqueous solution, simulating the solvent by the polarizable continuum model (pcm). geometry optimizations were performed at b3lyp/6-31 g** level. electronic distributions were analyzed at a better calculation level, thus improving the basis set (6-311++g**). a topological study based on bader's theory (atoms in molecules) and natural bond orbital (nbo) framework was performed. furthermore, molecular electrostatic potential maps (meps) were obtained and thoroughly analyzed. the stereochemistry was discussed, and the effect of the solvent was addressed. moreover, intrinsic properties were identified, focusing on factors that may be related to their antioxidant properties. hyperconjugative and inductive effects were described. the coordinated nbo/aim analysis allowed us to rationalize the changes of meps in a polar solvent. to investigate the molecular and structural properties of these compounds in biological media, the polarizabilities and dipolar moments were predicted which were further used to enlighten stability and reactivity properties. all conformers were taken into account. relevant stereoelectronic aspects were described for understanding the stabilization and antioxidant function of these structures.",
    "present_kp": [
      "-phenylflavans",
      "antioxidants",
      "atoms in molecules"
    ],
    "absent_kp": [
      "aqueous solvent effect",
      "molecular dipole moment",
      "natural bond orbital analysis"
    ]
  },
  {
    "title": "re-thinking metaphor, experience and aesthetic awareness.",
    "abstract": "purpose - the purpose of this paper is to explore current questions about metaphor, experience and aesthetic awareness that persist through the variations of critical approaches and projective research in architectural theory and practice. design/methodology/approach - further considerations focus on the advanced technological possibilities which re-invest the relations between principles of cybernetics and architecture. findings - the current between art and architecture is more than ever manifested in fields related to the computer sciences and its conceptual background: cybernetic sciences. originality/value - the paper re-thinks the aesthetic value of architecture and architectural experience in this time of digital productivity.",
    "present_kp": [
      "cybernetics",
      "architecture",
      "experience",
      "metaphor"
    ],
    "absent_kp": [
      "aesthetics",
      "technology"
    ]
  },
  {
    "title": "agent technologies for sensor networks.",
    "abstract": "the development of agent technologies for sensor networks has received increasing research attention within both the sensor network and multi-agent systems research communities. the international workshops on agent technologies for sensor networks (atsn) held in 2007, 2008 and 2009 sought to bring these communities together, and this special issue of the computer journal presents extended versions of some of the papers that appeared at these workshops, along with new submissions specifically for this journal.",
    "present_kp": [
      "agent technologies",
      "sensor networks",
      "networks"
    ],
    "absent_kp": []
  },
  {
    "title": "optimal consensus of fuzzy opinions under group decision making environment.",
    "abstract": "the gist of this paper is to propose a new method for aggregating individual fuzzy opinions into an optimal group consensus. by optimality, we mean the sum of weighted dissimilarity among aggregated consensus and individual opinions is minimized. we propose an iterative procedure for approximating the optimal consensus of expert opinions. finally, the importance of each expert is taken into consideration in the process of aggregation.",
    "present_kp": [],
    "absent_kp": [
      "fuzzy individual opinions",
      "fuzzy opinions aggregation",
      "group consensus opinion",
      "fuzzy numbers",
      "multi-criteria decision making"
    ]
  },
  {
    "title": "multiscale bagging and its applications.",
    "abstract": "we propose multiscale bagging as a modification of the bagging procedure. in ordinary bagging, the bootstrap resampling is used for generating bootstrap samples. we replace it with the multiscale bootstrap algorithm. in multiscale bagging, the sample size in of bootstrap samples may be altered from the sample size n of learning dataset. for assessing the output of a classifier, we compute bootstrap probability of class label; the frequency of observing a specified class label in the outputs of classifiers learned from bootstrap samples. a scaling-law of bootstrap probability with respect to sigma(2) = n/m has been developed in connection with the geometrical theory. we consider two different ways for using multiscale bagging of classifiers. the first usage is to construct a confidence set of class labels, instead of a single label. the second usage is to find. inputs close to decision boundaries in the context of query by bagging for active learning. it turned out, interestingly, that an appropriate choice of m is m = -n, i.e., sigma(2) = -1, for the first usage, and m = infinity, i.e., sigma(2) = 0, for the second usage.",
    "present_kp": [
      "bagging",
      "active learning"
    ],
    "absent_kp": [
      "confidence level",
      "classification"
    ]
  },
  {
    "title": "estimation of lower and upper bounds on the power consumption from scheduled data flow graphs.",
    "abstract": "in this paper, we present an approach for the calculation of lower and upper bounds on the power consumption of data path resources like functional units, registers, i/o ports, and busses from scheduled data flow graphs executing a specified input data stream. the low power allocation and binding problem is formulated, first, it is shown that this problem without constraining the number of resources can be relaxed to the bipartite weighted matching problem which is solvable in o(n)(3). n is the number of arithmetic operations, variables, i/o-access or bus-access operations which have to be bound to data path resources, in a second step we demonstrate that the relaxation can be efficiently extended by including lagrange multipliers in the problem formulation to handle a resource constraint, the estimated bounds take into account the effects of resource sharing. the technique can be used, for example, to prune the design space in high-level synthesis for low power before the allocation and binding of the resources. the application of the technique on benchmarks with real application input data shows the tightness of the bounds.",
    "present_kp": [
      "high-level synthesis"
    ],
    "absent_kp": [
      "bound estimation",
      "low-power",
      "power dissipation",
      "power estimation",
      "switching activity"
    ]
  },
  {
    "title": "the effects of perceived risk and technology type on users acceptance of technologies.",
    "abstract": "previous studies on technology adoption disagree regarding the relative magnitude of the effects of perceived usefulness and perceived ease of use. however these studies did not consider moderating variables. we investigated four potential moderating variables perceived risk, technology type, user experience, and gender in users technology adoption. their moderating effects were tested in an empirical study of 161 subjects. results showed that perceived risk, technology type, and gender were significant moderating variables. however the effects of user experience were marginal after the variance of errors was removed.",
    "present_kp": [
      "perceived risk",
      "technology type",
      "user experience",
      "gender",
      "moderating variable"
    ],
    "absent_kp": [
      "technology acceptance",
      "utaut"
    ]
  },
  {
    "title": "estimation of process parameter variations in a pre-defined process window using a latin hypercube method.",
    "abstract": "the aim of this paper is to present a methodology that provides an analytical tool for estimation of robustness and response variation within a pre-defined process window. to exemplify the developed methodology, the stochastic simulation technique is used for a sheet-metal forming application. a sampling plan based on the latin hypercube sampling method for variation of design parameters is utilized, and the thickness reduction is specified as the response. moreover, the response surface methodology is applied for understanding the quantitative relationship between design parameters and response value. the conclusions of this study are that the applied method gives a possibility to illustrate and interpret the variation of the response versus a design parameter variation. consequently, it gives significant insights into the usefulness of individual design parameters. it has been shown that the method enables us to estimate the admissible design parameter variations and to predict the actual safe margin for given process parameters. furthermore, the dominating design parameters can be predicated using sensitivity analysis, and this in its turn clarifies how the reliability criteria are met. finally, the developed software can be used as an additional module for set-up of stochastic finite element simulations and to collect the numerical results from different solvers within different applications.",
    "present_kp": [
      "sheet-metal forming"
    ],
    "absent_kp": [
      "stochastical analysis",
      "sensitivity indicator",
      "admissible process parameter variation",
      "finite element method"
    ]
  },
  {
    "title": "architecture and applications of the fingermouse: a smart stereo camera for wearable computing hci.",
    "abstract": "in this paper we present a visual input hci system for wearable computers, the fingermouse. it is a fully integrated stereo camera and vision processing system, with a specifically designed asic performing stereo block matching at 5mpixel/s (e.g. qvga 320240at 30fps) and a disparity range of 47, consuming 187mw (78mw in the asic). it is button-sized (43mm18mm) and can be worn on the body, capturing the users hand and processing in real-time its coordinates as well as a 1-bit image of the hand segmented from the background. alternatively, the system serves as a smart depth camera, delivering foreground segmentation and tracking, depth maps and standard images, with a processing latency smaller than 1ms. this paper describes the fingermouse functionality and its applications, and how the specific architecture outperforms other systems in size, latency and power consumption.",
    "present_kp": [
      "wearable computing",
      "foreground segmentation",
      "hci"
    ],
    "absent_kp": [
      "stereo vision",
      "mobile embedded vision",
      "hand tracking"
    ]
  },
  {
    "title": "wireless sensor networking for rain-fed farming decision support.",
    "abstract": "wireless sensor networks (wsns) can be a valuable decision-support tool for farmers. this motivated our deployment of a wsn system to support rain-fed agriculture in india. we defined promising use cases and resolved technical challenges throughout a two-year deployment of our common-sense net system, which provided farmers with environment data. however, the direct use of this technology in the field did not foster the expected participation of the population. this made it difficult to develop the intended decision-support system. based on this experience, we take the following position in this paper: currently, the deployment of wsn technology in developing regions is more likely to be effective if it targets scientists and technical personnel as users, rather than the farmers themselves. we base this claim on the lessons learned from the common-sense system deployment and the results of an extensive user experiment with agriculture scientists, which we describe in this paper.",
    "present_kp": [
      "user experiment",
      "agriculture",
      "wireless sensor network"
    ],
    "absent_kp": [
      "developing country"
    ]
  },
  {
    "title": "on stable parametric finite element methods for the stefan problem and the mullinssekerka problem with applications to dendritic growth.",
    "abstract": "we introduce a parametric finite element approximation for the stefan problem with the gibbsthomson law and kinetic undercooling, which mimics the underlying energy structure of the problem. the proposed method is also applicable to certain quasi-stationary variants, such as the mullinssekerka problem. in addition, fully anisotropic energies are easily handled. the approximation has good mesh properties, leading to a well-conditioned discretization, even in three space dimensions. several numerical computations, including for dendritic growth and for snow crystal growth, are presented.",
    "present_kp": [
      "stefan problem",
      "mullinssekerka problem",
      "kinetic undercooling",
      "gibbsthomson law",
      "dendritic growth",
      "snow crystal growth"
    ],
    "absent_kp": [
      "surface tension",
      "anisotropy",
      "parametric finite elements"
    ]
  },
  {
    "title": "binomial moments of the distance distribution: bounds and applications.",
    "abstract": "we study a combinatorial invariant of codes which counts the number of ordered pairs of codewords in all subcodes of restricted support in a code. this invariant can be expressed as a linear form of the components of the distance distribution of the code with binomial numbers as coefficients. for this reason we call it a binomial moment of the distance distribution. binomial moments appear in the proof of the macwilliams identities and in many other problems of combinatorial coding theory. we introduce a linear programming problem for bounding these linear forms from below. it turns out that some known codes (1-error-correcting perfect codes, golay codes, nordstrom-robinson code, etc.) yield optimal solutions of this problem, i.e., have minimal possible binomial moments of the distance distribution. we derive several general feasible solutions of this problem, which give lower bounds on the binomial moments of codes with given parameters, and derive the corresponding asymptotic bounds. applications of these bounds include new lower bounds on the probability of undetected error for binary codes used over the binary-symmetric channel with crossover probability p and optimality of many codes for error detection. asymptotic analysis of the bounds enables us to extend the range of code rates in which the upper bound on the undetected error exponent is tight.",
    "present_kp": [
      "binomial moments",
      "distance distribution",
      "linear programming",
      "undetected error"
    ],
    "absent_kp": [
      "extremal codes",
      "rodemich theorem"
    ]
  },
  {
    "title": "characteristics of wap traffic.",
    "abstract": "this paper considers the characteristics of wireless application protocol (wap) traffic. we start by constructing a wap traffic model by analysing the behaviour of users accessing public wap sites via a monitoring system. a wide range of different traffic scenarios were considered, but most of these scenarios resolve to one of two basic types. the paper then uses this traffic model to consider the effects of large quantities of wap traffic on the core network. one traffic characteristic which is of particular interest in network dimensioning is the degree of self-similarity, so the paper looks at the characteristics of aggregated traffic with wap, web and packet speech components to estimate its self-similarity. the results indicate that, while wap traffic alone does not exhibit a significant degree of self-similarity, a combined load from various traffic sources retains almost the same degree of self-similarity as the most self-similar individual source.",
    "present_kp": [
      "wap",
      "self-similarity"
    ],
    "absent_kp": [
      "traffic modelling",
      "mobile data"
    ]
  },
  {
    "title": "multimodal interactions in typically and atypically developing children: natural versus artificial environments.",
    "abstract": "this review addresses the central role played by multimodal interactions in neurocognitive development. we first analyzed our studies of multimodal verbal and nonverbal cognition and emotional interactions within neuronal, that is, natural environments in typically developing children. we then tried to relate them to the topic of creating artificial environments using mobile toy robots to neurorehabilitate severely autistic children. by doing so, both neural/natural and artificial environments are considered as the basis of neuronal organization and reorganization. the common thread underlying the thinking behind this approach revolves around the brains intrinsic properties: neuroplasticity and the fact that the brain is neurodynamic. in our approach, neural organization and reorganization using natural or artificial environments aspires to bring computational perspectives into cognitive developmental neuroscience.",
    "present_kp": [
      "multimodal interactions",
      "mobile toy robot"
    ],
    "absent_kp": [
      "verbal/nonverbal development",
      "autism",
      "free game play",
      "positive emotion",
      "neural mediator"
    ]
  },
  {
    "title": "theoretical demonstration of symmetric iv curves in asymmetric molecular junction of monothiolate alkane.",
    "abstract": "a molecular junction of an asymmetric molecule generally demonstrates an asymmetric currentvoltage (iv) curve, due to the unequal voltage drops at the two moleculeelectrode contacts. however, for asymmetric s(ch2)nch3 molecules, symmetric iv curves are always obtained in the experimental measurements. here, we investigate the electronic transport of the aus(ch2)7ch3au molecular junction in order to reveal the mechanism of the symmetric iv curve with atk package, in which the density functional theory is combined with keldysh nonequilibrium green's function method to calculate the electronic and transport properties of nanoscale systems. and the symmetric iv curve can be interpreted by the curved surface model, which reproduces curved surface of the top electrode in the experiment.",
    "present_kp": [
      "iv curve"
    ],
    "absent_kp": [
      "first-principles",
      "symmetry",
      "thiolalkane monolayer junction"
    ]
  },
  {
    "title": "fuzzy reliability analysis of repairable industrial systems using soft-computing based hybridized techniques.",
    "abstract": "the present study analyzes the fuzzy reliability of a repairable industrial system utilizing uncertain data. one traditional (flt) and two soft-computing based hybridized techniques (gablt and ngablt) are used. some very important fuzzy reliability indices of a washing system in a paper plant have been computed. it is observed that gablt performs consistently well in comparison to other two techniques. the analysis may be helpful for improving the performance of the considered system.",
    "present_kp": [],
    "absent_kp": [
      "flt technique",
      "gablt technique",
      "ngablt technique",
      "nonlinear programming",
      "genetic algorithm",
      "artificial neural networks "
    ]
  },
  {
    "title": "demagnetization properties of ipm and spm motors used in the high demanding automotive application.",
    "abstract": "purpose - in order to reduce co2 emissions of new cars many hydraulic and mechanical systems like e.g.: water pump, oil pump, power steering, clime compressor have been exchanged with pure electromechanical systems, which are driven only on request. this helps to reduce fuel consumption. this trend requires of utilization of modern brushless electric motors, which are controlled from power electronic control unit - ecu. in today's car can be found between 30 to 150 electric motors. many of them are still simple brush type with ferrite magnets. also in this area, drift in the direction of brushless motors can bee seen, because of higher efficiency, longer lifetime, lower noise, better emc and more controllable torque vs speed characteristic. there are different technological solutions, which can been used in the area of brushless motors in order to reduce size and cost of single component. one major factor of bldc/ac motor is rear earth permanent magnet material used during production. a magnet material cost could be in the range from 30 percent (basis price 2010) up to 90 percent (basis price 2011) of total material motor cost, depends on actual rear earth material price level. in order to reduce magnet cost, the aim of this paper is to find the most robust motor design, which can be resistant against maximum temperature and phase current amplitude for the same magnet material properties, coercive force - hcj. this behaviour is called demagnetization property. design/methodology/approach - analysis was performed based on review of literature, own theoretical and practical research and experience in the area of electromechanical systems for automotive application. during motor analysis computer numerical simulation method, cad and experiment were used. findings - as a result, comparison of different motors' topologies with different properties of magnet materials is presented. the worked out methodology shows very good correlation between simulations and measurements. this work can be used in order to reduce test effort and reduce cost of design. practical implications - the presented methodology reduces for new designs test effort and development cost and gives an implication of robust motor topology for demagnetization effects. originality/value - it is the first paper where demagnetization effects have been studied theoretically and in laboratory in order to find the most robust design, reduce magnet cost by reduction of dysprosium content and develop simulation procedure for analysis of demagnetizations behaviours of interior and surface permanent magnet.",
    "present_kp": [
      "brushless motor",
      "coercive force",
      "automotive application",
      "dysprosium"
    ],
    "absent_kp": [
      "interior permanent magnet - ipm",
      "low weight",
      "mass production",
      "cost-effectiveness",
      "neodymium",
      "automotive components industry"
    ]
  },
  {
    "title": "a novel direct search approach for combined heat and power dispatch.",
    "abstract": "a novel approach based on the direct search method (dsm) is proposed for the solution of combined heat and power (chp) dispatch problem. to deal with the mutual dependency of multiple-demand and heatpower capacity of cogeneration units, the penalty functions should be considered in dsm to enforce the corresponding violated constraints from the infeasible region into the feasible region. many nonlinear characteristics of the generator can be handled properly in the direct search procedure. to increase the possibility of exploring the search space where the global optimal solution exists, another effective strategy based on a successive refinement search technique is also proposed to guarantee a possibly complete examination of the solution space. numerical experiments are included to demonstrate the proposed direct search approach can obtain a higher quality solution than many existing techniques.",
    "present_kp": [
      "cogeneration",
      "combined heat and power dispatch",
      "direct search method"
    ],
    "absent_kp": [
      "economic dispatch"
    ]
  },
  {
    "title": "fourth- and tenth-order compact finite difference solutions of perturbed circular vortex flows.",
    "abstract": "in this study, high-order compact finite difference calculations are reported for 2d unsteady incompressible circular vortex flow in primitive variable formulation. the fourth-order runge-kutta temporal discretization is used together with fourth- or tenth-order compact spatial discretization. dependent on the perturbation initially imposed, the solutions display a tripole, triangular or square vortex. the comparison of the predictions with the detailed spectral calculations of kloosterziel and carnevale (j. fluid mech. 1999; 388:217-257) shows that the vorticity fields are very well captured. the spectral resolution of the present method was quantified from the decomposition of the vorticity distribution in its azimuthal components and compared with reported spectral results. using identical grid resolution to the reference results yields negligible differences in the main features of the flow. the perturbation amplitude and its first harmonic are virtually identical to the reference results for both fourth- or tenth-order spatial discretization, as theoretically expected but seldom a posteriori verified. the differences between the two spatial discretizations appear only for coarser grids, favouring the tenth-order discretization.",
    "present_kp": [
      "high-order",
      "circular vortex"
    ],
    "absent_kp": [
      "compact finite differences",
      "incompressible navier stokes"
    ]
  },
  {
    "title": "simulation based study of wireless rf interconnects for practical cmos implementation.",
    "abstract": "an electromagnetic analysis for the practical implementation of on-chip antennas to be used as wireless ic interconnects is presented. the undesired electromagnetic signal coupling between the on-chip antennas and the metal interconnects is characterized under varying geometries and placement of the metal interconnects. the variations in the transmission gain between the antenna pair due to the typical complementary metal oxide semiconductor (cmos) manufacturing requirements are presented. using a 3-d finite element method (fem) based full wave electromagnetic solver, it is shown that the antenna characteristics are significantly impacted by the presence of the essential epitaxial layer and the required minimum metal utilization. it is also shown in a 250nm cmos technology that there can be a significant electromagnetic signal coupling between the on-chip transmitting antenna and the metal interconnects on a die (-12.09db for a 1.6mm long, 2?m wide interconnect at a distance of 1?m from the antenna). design considerations are presented for the metal interconnects in the presence of on-chip antennas in order to minimize the undesired electromagnetic signal coupling.",
    "present_kp": [
      "interconnects",
      "electromagnetic",
      "on-chip antennas"
    ],
    "absent_kp": [
      "vlsi"
    ]
  },
  {
    "title": "identification of tumor-immune system via recurrent neural network.",
    "abstract": "cancer immunotherapy is an emerging therapy for cancer disease treatment which stimulates immune systems to fight against tumor cells. in this paper, a back propagation neural network with some feedbacks from hidden layer is used as a method of identification for one validated mathematical model. since it is not possible to model complex system due to void of information and knowledge to model all complexity of complex system, identification methods are effective tools for modeling ill-defined system. afterward, it is possible to perform control methods on the estimated model to reach the clinical goals. the simulation results have shown the correctness of the identification process.",
    "present_kp": [
      "identification",
      "immune system"
    ],
    "absent_kp": [
      "multi-layer perceptron",
      "artificial neural network"
    ]
  },
  {
    "title": "an efficient iterative algorithm for the approximation of the fast and slow dynamics of stiff systems.",
    "abstract": "the relation between the iterative algorithms based on the computational singular perturbation (csp) and the invariance equation (ie) methods is examined. the success of the two methods is based on the appearance of fast and slow time scales in the dynamics of stiff systems. both methods can identify the low-dimensional surface in the phase space (slow invariant manifold, sim), where the state vector is attracted under the action of fast dynamics. it is shown that this equivalence of the two methods can be expressed by simple algebraic relations. csp can also construct the simplified non-stiff system that models the slow dynamics of the state vector on the sim. an extended version of ie is presented which can also perform this task. this new ie version is shown to be exactly similar to a modified version of csp, which results in a very efficient algorithm, especially in cases where the sim dimension is small, so that significant model simplifications are possible.",
    "present_kp": [],
    "absent_kp": [
      "invariant manifolds",
      "model reduction",
      "multiple time scales",
      "asymptotic analysis",
      "singular perturbation analysis"
    ]
  },
  {
    "title": "an adaptive comb filter with flexible notch gain.",
    "abstract": "this paper proposes an adaptive comb filter with flexible notch gain. it can appropriately remove a periodic noise from an observed signal. the proposed adaptive comb filter uses a simple lms algorithm to update the notch gain coefficient for removing the noise and preserving a desired signal, simultaneously. simulation results show the effectiveness of the proposed comb filter.",
    "present_kp": [
      "comb filter",
      "lms"
    ],
    "absent_kp": [
      "adaptive algorithm"
    ]
  },
  {
    "title": "does the use of structured reporting improve usability? a comparative evaluation of the usability of two approaches for findings reporting in a large-scale telecardiology context.",
    "abstract": "poor usability leads to a low adoption rate of telemedicine systems. mode of input, free-text or structured report, influences usability. usability and user satisfaction are higher for structured report interfaces in telecardiology.",
    "present_kp": [
      "telemedicine",
      "telecardiology"
    ],
    "absent_kp": [
      "system usability scale",
      "keystroke-level model",
      "heuristic usability evaluation",
      "dicom sr"
    ]
  },
  {
    "title": "fixed points of correspondences defined on cone metric spaces.",
    "abstract": "in the present note, we investigate the fixed points of correspondences defined on cone metric spaces satisfying a conditionally contractive condition.",
    "present_kp": [
      "fixed point",
      "cone metric space",
      "correspondence"
    ],
    "absent_kp": [
      "banach lattice"
    ]
  },
  {
    "title": "personal content management system: a semantic approach.",
    "abstract": "the amount of multimedia resources that is created and needs to be managed is increasing considerably. additionally, a significant increase of metadata, either structured (metadata fields of standardized metadata formats) or unstructured (free tagging or annotations) is noticed. this increasing amount of data and metadata, combined with the substantial diversity in terms of used metadata fields and constructs, results in severe problems to manage and retrieve these multimedia resources. standardized metadata schemes can be used but the plethora of these schemes results in interoperability issues. in this paper, we propose a metadata model suited for personal content management systems. we create a layered metadata service that implements the presented model as an upper layer and combines different metadata schemes in the lower layers. semantic web technologies are used to define and link formal representations of these schemes. specifically, we create an ontology for the dig35 metadata standard and elaborate on how it is used within this metadata service. to illustrate the service, we present a representative use case scenario consisting of the upload, annotation, and retrieval of multimedia content within a personal content management system.",
    "present_kp": [
      "personal content management",
      "semantic web",
      "metadata",
      "annotation",
      "ontology",
      "interoperability"
    ],
    "absent_kp": [
      "multimedia standards",
      "reasoning"
    ]
  },
  {
    "title": "a next generation multimedia call center for internet commerce: imc.",
    "abstract": "human assistance, as well as automated service, is necessary for providing more convenient services to customers on the internet-based commerce system. call centers have been typically human-based service systems. however, the services of existing public switched telephone network-based call centers are not enough to meet the needs of customers on the internet. most of them have been designed without considering the interaction involved in shopping on the internet in our research, we design a call center named imc (internet-based multimedia call center) that can be integrated with an internet shopping mall. it contains 2 parts: an internet multimedia dialogue system and a human agent assisting system. the internet multimedia dialogue system is an internet and multimedia version of the interactive voice response service of computer telephony integration-based call centers because it provides access to the multimedia web page along with the recorded voice explanation through the internet. the human agent assisting system aims to select the most appropriate human agents in the call center and support them in providing high-quality individualized information for each customer. imc is a real-time, human-embedded system that can provide high-quality services cost-effectively for internet commerce.",
    "present_kp": [
      "call center",
      "internet commerce",
      "computer telephony integration",
      "internet shopping mall",
      "human-embedded system"
    ],
    "absent_kp": [
      "electronic commerce"
    ]
  },
  {
    "title": "wind tunnel experiments of tracer dispersion downwind from a small-scale physical model of a landfill.",
    "abstract": "wind tunnel experiments have been carried out on a small-scale physical model of a municipal waste landfill (mwl) in the criaciv (research centre of building aerodynamics and wind engineering) environmental wind tunnel in prato (italy). the mwl model simulates a landfill whose surface is higher than the surrounding surface, applying a 1:200 scaling factor. modelling an area source such as landfill is a difficult task for numerical models due to turbulence phenomena that modifies the flow near the source increasing ground level concentration (glc). for the specific task, a new set-up of the wind tunnel has been developed, with respect to previous studies carried out on line and point sources physical models. the tracer used in the experiments was ethylene, suitable for non-buoyant plume conditions, typical for mwl emissions. a detailed result database has been obtained in terms of glc and concentration profiles as well as flow turbulence and velocity field characterisation.",
    "present_kp": [
      "wind tunnel",
      "concentration profiles",
      "landfill"
    ],
    "absent_kp": [
      "physical modelling",
      "experimental data"
    ]
  },
  {
    "title": "exclusively your's: dynamic individuate search by extending user profile.",
    "abstract": "a universal search engine is unable to provide a personal touch to a user query. to overcome the deficiency of a universal search engine, vertical search engines are used, which return search results from a specific domain. an alternate option is to use a personalized search system. in our endeavor to provide personalized search results, the proposed system, exclusively your's, observes a user browsing behavior and his actions. based on the observed user behavior, it dynamically constructs user profile which consists of some terms that are related to user's interest. the constructed profile is later used for query expansion. the goal of research work in this paper is not to provide all the relevant results, but a few high quality personalized search results at the top of ranked list, which in other words means high precision. we performed experiments by personalizing google, yahoo, and naver (widely used search engine in korea). the results show that using exclusively your's, a search engine yields significant improvement. we also compared the user profile constructed by the proposed approach with other similar personalization approaches; the results show a marginal increase in precision.",
    "present_kp": [
      "search engine",
      "personalization"
    ],
    "absent_kp": [
      "summarization",
      "weighted index",
      "anchor text",
      "hyperlink"
    ]
  },
  {
    "title": "constrained diffusion-limited aggregation in 3 dimensions.",
    "abstract": "diffusion-limited aggregation (dla) has usually been studied in 2 dimensions as a model of fractal growth processes such as river networks, plant branching, frost on glass, electro-deposition, lightning, mineral deposits, and coral. here, the basic principles are extended into 3 dimensions and used to create, among other things, believable models of root systems. an additional innovation is a means of constraining the growth of the 3d dla by a surface or containing it within a vessel.",
    "present_kp": [
      "fractal",
      "dla",
      "branching",
      "diffusion",
      "aggregation"
    ],
    "absent_kp": [
      "brownian motion"
    ]
  },
  {
    "title": "skill-specific spoken dialogs in a reading tutor that listens.",
    "abstract": "project listen's reading tutor listens to children read aloud. a controlled study indicates that the reading tutor helps children's reading comprehension. however, the results for word attack (decoding) skills and word identification skills were not statistically better than in the control condition. our thesis therefore proposes to develop skill-specific dialogs based on cognitive skill models and successful tutoring strategies. these dialogs will be dynamically assembled by the reading tutor and include text, speech, illustrations, and dialog parameters. we hypothesize that such dialogs will improve elementary students' reading abilities.",
    "present_kp": [
      "children",
      "spoken dialog",
      "reading"
    ],
    "absent_kp": [
      "intelligent tutoring systems",
      "speech recognition"
    ]
  },
  {
    "title": "a difference expansion oriented data hiding scheme for restoring the original host images.",
    "abstract": "this paper proposes a lossless data embedding scheme that exploits the difference expansion of the pixels to conceal large amount of message data in a digital image. the proposed scheme takes into consideration the correlation between the pixel and its surrounding pixels to determine the degree of the difference expansion for message data embedding. the performance has been evaluated in terms of image distortion, payload capacity, as well as embedding rate. the experimental results show that the scheme is capable of providing a great payload capacity, and the image quality of the embedded image is better than that of tians and celiks schemes for a gray-level image. what is more, for a color image, the proposed scheme outperforms alattars scheme at low psnr. in addition, the proposed scheme can completely restore the original image after data extraction.",
    "present_kp": [
      "lossless data embedding",
      "difference expansion"
    ],
    "absent_kp": [
      "information hiding",
      "reversible data hiding"
    ]
  },
  {
    "title": "bit-parallel random number generation for discrete uniform distributions.",
    "abstract": "when a die is cast, the outcome is one of the six sides, i.e. the outcome is discrete and uniformly distributed over the range r = {1, 2, 3, 4, 5, 6}. generating random numbers with such a distribution is very easy: obtain a random number w epsilon w, the domain of the random numbers, and take (w mod r) + 1. however, many uniform discrete distributions have a rather short range, e.g., r = 6 in a dice game, and r = 3 for the walking directions of a 2-dimensional nonreversal random walk. the number w is typically a machine word, i.e. log(2)(w) approximate to 32 in a 32-bit computer, so generating a log(2)(r)-bit random number has consumed about 32 random bits. when w much greater than r, it is wasteful and hence inefficient. this paper presents an efficient algorithm for generating random numbers for the distributions with r discrete uniform outcomes. the algorithm uses parallel bit-wise operations on machine words. the performance results of the algorithm are presented. the statistical quality of the random numbers generated from this algorithm is also discussed.",
    "present_kp": [
      "discrete uniform distribution"
    ],
    "absent_kp": [
      "random number generators",
      "monte carlo simulation",
      "random walks"
    ]
  },
  {
    "title": "a metaobject protocol for clforjava.",
    "abstract": "clforjava is a new implementation of common lisp that intertwines its architecture and operation with java. the authors describe a new architecture for a clos mop that supports transparent, bi-directional access between lisp and java. the access requires no special techniques nor syntactic mechanisms on the part of the programmer - being either java or lisp. the core of the new mop is a data structure that melds the fundamental structures of java instances (n-tuples) and clos instances (2-tuples) in such a way that the respective object systems can interact without cumbersome translations. methods from their respective object systems can interact freely. we discuss certain aspects of the respective mops that prevent a complete integration and replacement of one system by the other.",
    "present_kp": [
      "clos",
      "java",
      "lisp"
    ],
    "absent_kp": [
      "interoperation"
    ]
  },
  {
    "title": "a systematic evaluation of disk imaging in encase 6.8 and linen 6.1.",
    "abstract": "tools for disk imaging (or more generally speaking, digital acquisition) are a foundation for forensic examination of digital evidence. therefore it is crucial that such tools work as expected. the only way to determine whether this is the case or not is through systematic testing of each tool. in this paper we present such an evaluation of the disk imaging functions of encase 6.8 and linen 6.1, conducted on behalf of the swedish national laboratory of forensic science. although both tools performed as expected under most circumstances, we identified cases where flaws that can lead to inaccurate and incomplete acquisition results in linen 6.1 were exposed. we have also identified limitations in the tool that were not evident from its documentation. in addition summarizing the test results, we present our testing methodology, which has novel elements that we think can benefit other evaluation projects.",
    "present_kp": [
      "encase",
      "linen"
    ],
    "absent_kp": [
      "acquisition of digital data",
      "hard drive imaging",
      "testing forensic tools",
      "linux"
    ]
  },
  {
    "title": "parallel simulation of devs and cell-devs models on windows-based pc cluster systems.",
    "abstract": "the growing popularity of networks of workstations (now) in scientific computation has drawn increasing interest from the m&s community. this paper addresses the issue of parallel discrete-event simulation of devs and cell-devs models on a microsoft windows-based cluster system comprising interconnected general-purpose personal computers. we present the architecture and features of pcd++win, a parallel simulator that takes advantage of the multi-purpose graphical user interface of the deinompi middleware for construction of ad-hoc pc clusters and configuration of simulation environment. this environment significantly reduces the learning curve for general users and the cost of the simulation platform. pcd++win has been developed using a modular approach that promotes code reuse and allows for easy switching to other middleware technologies. the portability of the simulator is enhanced with multi-platform programming and compilation techniques. moreover, it leaves open the possibility of further extensions such as web-based distributed simulation and database-based model construction by leveraging the native support of microsoft visual studio. the experiments demonstrate the capability of the new simulator, making it an ideal m&s toolkit for tapping the computational power of general-purpose desktop computers.",
    "present_kp": [
      "devs",
      "cell-devs",
      "parallel simulation",
      "cluster systems"
    ],
    "absent_kp": [
      "discrete event simulation"
    ]
  },
  {
    "title": "a computable version of the daniell-stone theorem on integration and linear functionals.",
    "abstract": "for every measure mu, the integral i: f bar right arrow integral f d mu is a linear functional on the set of real measurable functions. by the daniell-stone theorem, for every abstract integral lambda: f --> r on a stone vector lattice f of real functions f : omega --> r there is a measure mu such that integral f d mu = lambda(f) for all f is an element of f. in this paper we prove a computable version of this theorem.",
    "present_kp": [
      "daniell-stone theorem"
    ],
    "absent_kp": [
      "computability",
      "computable analysis",
      "measure theory"
    ]
  },
  {
    "title": "factors influencing intention to use e-government services among citizens in malaysia.",
    "abstract": "this study is an exploratory study on the e-government in malaysia. with the liberalization and globalization, internet has been used as a medium of transaction in almost all aspects of human living. this study investigates the factors that influencing the intention to use e-government service among malaysians. this study integrates constructs from the models of technology acceptance model (tam), diffusion of innovation (doi) which been moderated by culture factor and trust model with five dimensions. the study was conducted by surveying a broad diversity of citizens in malaysia community. a structured questionnaire was used to collect data from 195 respondents but only 150 of the respondents with complete answers participating in the study. the result of the analysis showed that trust, perceived usefulness, perceived relative advantage and perceived image, respectively, has a direct positive significant relationship towards intention to use e-government service and perceived complexity has a significant negative relationship towards intention to use e-government service. while perceived strength of online privacy and perceived strength of non-repudiation have a positive impact on a citizen's trust to use e-government service. however, the uncertainty avoidance (moderating factor) used in the study has no significant effect on the relationship between the innovation factors (complexity, relative advantage and image) and intention to use e-government service. finally in comparing the explanatory power of the entire intention based model (tam, doi and trust) with the studied model, it has been found that the doi model has a better explanatory power.",
    "present_kp": [
      "e-government",
      "innovation"
    ],
    "absent_kp": [
      "adoptions",
      "malaysian services"
    ]
  },
  {
    "title": "reducing the energy dissipation of the issue queue by exploiting narrow immediate operands.",
    "abstract": "in contemporary superscalar microprocessors, issue queue is a considerable energy dissipating component due its complex scheduling logic. in addition to the energy dissipated for scheduling activities, read and write lines of the issue queue entries are also high energy consuming pieces of the issue queue. when these lines are used for reading and writing unnecessary information bits, such as the immediate operand part of an instruction that does not use the immediate field or the insignificant higher order bits of an immediate operand that are in fact not needed, significant amount of energy is wasted. in this paper, we propose two techniques to reduce the energy dissipation of the issue queue by exploiting the immediate operand files of the stored instructions: firstly by storing immediate operands in separate immediate operand files rather than storing them inside the issue queue entries and secondly by issue queue partitioning based on widths of immediate operands of instructions. we present our performance results and energy savings using a cycle accurate simulator and testing the design with spec2k benchmarks and 90 nm cmos (umc) technology.",
    "present_kp": [
      "issue queue",
      "immediate operands"
    ],
    "absent_kp": [
      "encoding",
      "energy consumption",
      "low power"
    ]
  },
  {
    "title": "study of speed-dependent packet error rate for wireless sensor on rotating mechanical structures.",
    "abstract": "wireless sensors on rotating mechanical structures have rich and fast changing multipath that cannot be easily predicted by conventional regression approaches in time for effective transmission coding or power control, resulting in deteriorated transmission quality. this study aims to study the speed-dependent packet error rate (per) of wireless sensor radios on rotating mechanical structures. a series of rotating ieee 802.15.4 sensor radio transmission experiments and vector network analyzer measurements have been conducted to derive and validate a predictive per model for a fast rotating sensor radio channel based on channel impulse response measurements. the proposed predictive per model, including power attenuation, bit error rate (ber) and per sub-models, captures the channel property of rotating sensors based on the received signal strength and the radio receiving sensitivity. the per model has accurately predicted the per profile of sensors on a rotating machine tool spindle as well as a rotating plate of a prototype rotation system. the analysis provides an in-depth understanding of how multipath propagation causes the fast power variation and the resulting speed-dependent per for wireless sensors on rotating mechanical structures.",
    "present_kp": [
      "packet error rate",
      "rotating mechanical structure",
      "wireless sensor"
    ],
    "absent_kp": [
      "transmission performance"
    ]
  },
  {
    "title": "crosstalk in vlsi interconnections.",
    "abstract": "we address the problem of crosstalk computation and reduction using circuit and layout techniques in this paper, we provide easily computable expressions for crosstalk amplitude and pulse width in resistive, capacitively coupled lines, the expressions hold for nets with arbitrary number of pins and of arbitrary topology under any specified input excitation. experimental results show that the average error is about 10% and the maximum error is less than 20%. the expressions are used to motivate circuit techniques, such as transistor sizing, and layout techniques, such as wire ordering and wire width optimization to reduce crosstalk.",
    "present_kp": [],
    "absent_kp": [
      "coupled noise",
      "signal integrity",
      "timing optimization"
    ]
  },
  {
    "title": "a hybrid genetic algorithm for the energy-efficient virtual machine placement problem in data centers.",
    "abstract": "server consolidation using virtualization technology has become an important technology to improve the energy efficiency of data centers. virtual machine placement is the key in the server consolidation technology. in the past few years, many approaches to the virtual machine placement have been proposed. however, existing virtual machine placement approaches consider the energy consumption by physical machines only, but do not consider the energy consumption in communication network, in a data center. however, the energy consumption in the communication network in a data center is not trivial, and therefore should be considered in the virtual machine placement. in our preliminary research, we have proposed a genetic algorithm for a new virtual machine placement problem that considers the energy consumption in both physical machines and the communication network in a data center. aiming at improving the performance and efficiency of the genetic algorithm, this paper presents a hybrid genetic algorithm for the energy-efficient virtual machine placement problem. experimental results show that the hybrid genetic algorithm significantly outperforms the original genetic algorithm, and that the hybrid genetic algorithm is scalable.",
    "present_kp": [
      "virtual machine placement",
      "server consolidation",
      "data center",
      "hybrid genetic algorithm"
    ],
    "absent_kp": [
      "cloud computing"
    ]
  },
  {
    "title": "sensitivity of tapered optical fiber surface plasmon resonance sensors.",
    "abstract": "the effect of tapered profiles on the sensitivity of spr sensor is studied. it is observed that as the taper ratio decreases the sensitivity of proposed sensor for each profile increases up to certain taper ratio where the plasmonic condition is satisfied. in all considered cases, the maximum sensitivity is obtained for sinusoidal tapered profile.",
    "present_kp": [
      "sensitivity",
      "tapered profiles",
      "taper ratio",
      "sinusoidal tapered profile",
      "surface plasmon resonance"
    ],
    "absent_kp": []
  },
  {
    "title": "an image contrast enhancement method based on genetic algorithm.",
    "abstract": "contrast enhancement plays a fundamental role in image/video processing. histogram equalization (he) is one of the most commonly used methods for image contrast enhancement. however, he and most other contrast enhancement methods may produce un-natural looking images and the images obtained by these methods are not desirable in applications such as consumer electronic products where brightness preservation is necessary to avoid annoying artifacts. to solve such problems, we proposed an efficient contrast enhancement method based on genetic algorithm in this paper. the proposed method uses a simple and novel chromosome representation together with corresponding operators. experimental results showed that this method makes natural looking images especially when the dynamic range of input image is high. also, it has been shown by simulation results that the proposed genetic method had better results than related ones in terms of contrast and detail enhancement and the resulted images were suitable for consumer electronic products.",
    "present_kp": [
      "contrast enhancement",
      "genetic algorithm",
      "natural looking images"
    ],
    "absent_kp": []
  },
  {
    "title": "limits of a conjecture on a leakage-resilient cryptosystem.",
    "abstract": "we introduce the hidden shares number problem, a variant of the hidden number problem. we give a leakage-resilience bound for elgamal cryptosystem with stateful decryption. we have implemented our attack and give some details about our implementation.",
    "present_kp": [
      "elgamal",
      "hidden number problem"
    ],
    "absent_kp": [
      "cryptography",
      "leakage-resilient cryptography",
      "lattice-based attacks"
    ]
  },
  {
    "title": "robust reconstruction of low-resolution document images by exploiting repetitive character behaviour.",
    "abstract": "in this paper, we present a new approach for reconstructing low-resolution document images. unlike other conventional reconstruction methods, the unknown pixel values are not estimated based on their local surrounding neighbourhood, but on the whole image. in particular, we exploit the multiple occurrence of characters in the scanned document. in order to take advantage of this repetitive behaviour, we divide the image into character segments and match similar character segments to filter relevant information before the reconstruction. a great advantage of our proposed approach over conventional approaches is that we have more information at our disposal, which leads to a better reconstruction of the high-resolution (hr) image. experimental results confirm the effectiveness of our proposed method, which is expressed in a better optical character recognition (ocr) accuracy and visual superiority to other traditional interpolation and restoration methods.",
    "present_kp": [
      "restoration",
      "interpolation",
      "ocr"
    ],
    "absent_kp": [
      "repetition",
      "character segmentation",
      "bimodal distribution"
    ]
  },
  {
    "title": "complexity of inflammatory responses in endothelial cells and vascular smooth muscle cells determined by microarray analysis.",
    "abstract": "to better understand the molecular basis of vascular cell system behavior in inflammation, we used gene expression microarrays to analyze the expression of 7,075 genes and their response to il-1? and tnf? in cultures of coronary artery endothelium and smooth muscle derived from a single coronary artery. the most noticeable difference between the cell types was the considerably greater magnitude and complexity of the transcriptional response in the endothelial cells. two hundred and nine genes were regulated in the endothelium and only 39 in vascular smooth muscle. among the 209 regulated genes in the endothelium, 99 have not been previously associated with endothelial cell activation and many implicate the endothelium in unconventional roles. for example, the induced genes include several that have only been associated with leukocyte function (e.g., il-7 receptor, ebi-3 receptor) and others related to antiviral and antibacterial defense (e.g., oligoadenylate synthetase, lmp7, toll-like receptor 4, complement component 3). in addition, 43 genes likely to participate in signal transduction (eg. il-18 receptor, stk2 kinase, staf50, anp receptor, vip receptor, rac3, ifp35) were regulated providing evidence that a major effect of tnf? and il-1? is to alter the potential of the endothelial cell to respond to various other external stimuli.",
    "present_kp": [
      "inflammation"
    ],
    "absent_kp": [
      "tnf-?",
      "interleukin-1 beta",
      "gene expression profiling",
      "gene expression regulation"
    ]
  },
  {
    "title": "partial x-ray photoelectron spectroscopy to constructing neural network model of plasma etching surface.",
    "abstract": "a new model to control plasma processes was constructed by combining a backpropagation neural network (bpnn) with x-ray photoelectron spectroscopy (xps). this technique was evaluated with the data collected during the etching of silicon carbide films at nf3 inductively coupled plasma. the etching characteristics modeled were the etch rate and surface roughness measured by scanning electron microscope and atomic force microscopy, respectively. for systematic modeling, the etching was characterized by means of 24 full factorial experiment plus one center point. the bpnn was trained by the training data composed of xps spectra corresponding to five major peaks. prediction performance of trained bpnn model was tested with a test data set, not belonging to the training data. in modeling surface roughness, pure xps model yielded an improvement of about 24% over pca-xps (99% data variance) model. for the etch rate data, the improvement was more than 40% irrespective of the data variances. these results indicate that non-reduced xps spectra are more effective in constructing a prediction model. xps models can be utilized to diagnose or control plasma processes.",
    "present_kp": [
      "x-ray photoelectron spectroscopy",
      "surface roughness",
      "neural network",
      "atomic force microscopy",
      "plasma etching",
      "model"
    ],
    "absent_kp": []
  },
  {
    "title": "aggregate profit-based caching replacement algorithms for streaming media transcoding proxy systems.",
    "abstract": "this work derives a generalized video object profit function from the extended weighted transcoding graph to calculate the individual cache profit of certain versions of a video object, and the aggregate profit from caching multiple versions of the same video object. this proposed function takes into account the popularity of certain versions of an object, the transcoding delay among versions, and the average duration of access of each version. based on the profit function, cache-replacement algorithms are proposed to reduce the startup delay and network traffic by efficiently caching video objects with the most profits. two kinds of simulations were conducted to evaluate the performance of the proposed algorithms. these simulations exploit partial viewing traces and complete viewing traces, separately. the results demonstrate that the proposed algorithms outperform the competing algorithms by 15%-39% in delay saving ratio and 5%-10% in byte-hit ratio.",
    "present_kp": [],
    "absent_kp": [
      "computer networks",
      "multimedia communication",
      "multimedia streaming",
      "prefix caching",
      "proxy caching",
      "streaming media distribution"
    ]
  },
  {
    "title": "refining and reasoning about nonfunctional requirements.",
    "abstract": "nonfunctional requirements (nfr) must be addressed early in the software development cycle to avoid the cost of revisiting those requirements or re-factoring at the later stages of the development cycle. methods and frameworks that identify and incorporate nfr at each stage of development cycle reduce this cost. the methodology used in this work for refining and reasoning about nfr is based on the nfr framework. this work identifies four nfr types and provides the methodology for developing domain specific nfr by using techniques for converting the requirements into design artifacts per nfr type. the contribution is four nfr types: functionally restrictive, additive restrictive, policy restrictive, and architecture restrictive and the software engineering process that provides specific refinements that result in unique architectural and design artifacts. by applying the same functional requirement focus to the different nfr domains it enhances the development process and promotes software quality attributes such as composability, maintainability, evolvability, and traceability.",
    "present_kp": [
      "software engineering",
      "nonfunctional requirements"
    ],
    "absent_kp": [
      "aspect-oriented programming"
    ]
  },
  {
    "title": "model-updated image guidance: initial clinical experiences with gravity-induced brain deformation.",
    "abstract": "image-guided neurosurgery relies on accurate registration of the patient, the preoperative image series, and the surgical instruments in the same coordinate space. recent clinical reports have documented the magnitude of gravity-induced brain deformation in the operating room and suggest these levels of tissue motion may compromise the integrity of such systems, we are investigating a model-based strategy which exploits the wealth of readily-available preoperative information in conjunction with intraoperatively acquired data to construct and drive a three dimensional (3-d) computational model which estimates volumetric displacements in order to update the neuronavigational image set. using model calculations, the preoperative image database can be deformed to generate a more accurate representation of the surgical focus during an operation, in this paper, we present a preliminary study of four patients that experienced substantial brain deformation from gravity and correlate cortical shift measurements with model predictions, additionally, me illustrate our image deforming algorithm and demonstrate that preoperative image resolution is maintained. results over the four cases show that the brain shifted, on average, 5.7 mm in the direction of gravity and that model predictions could reduce this misregistration error to an average of 1.2 mm.",
    "present_kp": [
      "brain shift",
      "image guidance"
    ],
    "absent_kp": [
      "brain deformation model",
      "consolidation",
      "finite element model",
      "porous media"
    ]
  },
  {
    "title": "communication structure and collective actions in social media.",
    "abstract": "in this paper i present results a study of different types of social media communication and networking channels that allow for collective action (ca): twitter, jaiku / qaiku, ning and facebook. my preliminary findings indicate, that the visual outlining and the structure of communication create different kinds of collectivity and collective actions. a status stream is effective for simple and fast repetitive mass actions and for individual mass broadcasting, while channels and threads are needed as a backchannel for more complicated, coordinated and iterative tasks and support a sense of community. when planning collective action on the internet, ranging from citizen participation to marketing campaigns, it is essential to note that different social media tools support different forms of collective action and feelings of collectiveness.",
    "present_kp": [
      "backchannel",
      "collective action",
      "social media"
    ],
    "absent_kp": [
      "crowdsourcing",
      "collective intelligence",
      "real-time web",
      "micro channel"
    ]
  },
  {
    "title": "better reporting of randomized trials in biomedical journal and conference abstracts.",
    "abstract": "well reported research published in conference and journal abstracts is important as individuals reading these reports often base their initial assessment of a study based on information reported in the abstract. however, there is growing concern about the reliability and quality of information published in these reports. this article provides an overview of research evidence underpinning the need for better reporting of abstracts reported in conference proceedings and abstracts of journal articles; with a particular focus in the area of health care. where available we highlight evidence which refers specifically to abstracts reporting randomized trials. we seek to identify current initiatives aimed at improving the reporting of these reports and recommend that an extension of the consort statement (consolidated standards of reporting trials), consort for abstracts, be developed. this checklist would include a list of essential items to be reported in any conference or journal abstract reporting the results of a randomized trial.",
    "present_kp": [
      "conference proceedings"
    ],
    "absent_kp": [
      "randomized controlled trial",
      "methodological quality",
      "structured abstracts",
      "checklists"
    ]
  },
  {
    "title": "continuous testing in eclipse.",
    "abstract": "continuous testing uses excess cycles on a developer's workstation to continuously run regression tests in the background, providing rapid feedback about test failures as code is edited. it reduces the time and energy required to keep code well-tested, and it prevents regression errors from persisting uncaught for long periods of time.",
    "present_kp": [
      "continuous testing",
      "testing"
    ],
    "absent_kp": [
      "development environments"
    ]
  },
  {
    "title": "bayesian analysis of the logit model and comparison of two metropolishastings strategies.",
    "abstract": "we examine some markov chain monte carlo (mcmc) methods for a generalized non-linear regression model, the logit model. it is first shown that mcmc algorithms may be used since the posterior is proper under the choice of non-informative priors. then two non-standard mcmc methods are compared: a metropolishastings algorithm with a bivariate normal proposal resulting from an approximation, and a metropolishastings algorithm with an adaptive proposal. the results presented here are illustrated by simulations, and show the good behavior of both methods, and superior performances of the method with an adaptive proposal in terms of convergence to the stationary distribution and exploration of the posterior distribution surface.",
    "present_kp": [
      "markov chain monte carlo",
      "metropolishastings algorithm"
    ],
    "absent_kp": [
      "bayesian statistic",
      "adaptive algorithm",
      "stationarity",
      "convergence assessment"
    ]
  },
  {
    "title": "aggregate features and adaboost for music classification.",
    "abstract": "we present an algorithm that predicts musical genre and artist from an audio waveform. our method uses the ensemble learner adaboost to select from a set of audio features that have been extracted from segmented audio and then aggregated. our classifier proved to be the most effective method for genre classification at the recent mirex 2005 international contests in music information extraction, and the second-best method for recognizing artists. this paper describes our method in detail, from feature extraction to song classification, and presents an evaluation of our method on three genre databases and two artist-recognition databases. furthermore, we present evidence collected from a variety of popular features and classifiers that the technique of classifying features aggregated over segments of audio is better than classifying either entire songs or individual short-timescale features.",
    "present_kp": [
      "genre classification",
      "mirex"
    ],
    "absent_kp": [
      "artist recognition",
      "audio feature aggregation",
      "multiclass adaboost"
    ]
  },
  {
    "title": "weak limits and their calculation in analog signal theory.",
    "abstract": "purpose - this paper aims to improve the mathematical justification of certain analog signal theory concepts and offer a rigorous framework for it. design/methodology/approach - the framework relies on functional analysis, namely theory of distributions and the concept of weak limit. its notation is adjusted to resemble the notation usually used in engineering signal theory. it can be used to prove in a rigorous manner already established results in signal theory, but also to establish new ones. findings - examples have shown the lack of rigour caused by using ordinary calculus in proving fundamental signal theoretic results. on that basis, concepts of limit, fourier transform and derivative are revisited in the spirit of functional analysis. a new useful formula for weak limit computation is proved. originality/value - functional analysis is efficiently used in signal theory in a manner approachable by engineers. an original and efficient formula for weak limit computation is presented and proved.",
    "present_kp": [
      "analog signal theory",
      "functional analysis",
      "weak limit"
    ],
    "absent_kp": [
      "schwartz distribution",
      "fourier transforms"
    ]
  },
  {
    "title": "the social information infrastructure.",
    "abstract": "the division of social, behavioral, and economic research of the national science foundation has explored aggressively the potential involvement of the social sciences in the national information infrastructure. we invision the nii as a global network of computer communications, which will evolve out of the internet, linking all social scientists to massive digital libraries and to myriad smaller distributed data sources containing information of every imaginable sort. five workshops have charted applications of high-performance computing in the social and behavioral sciences: cognitive science, computational geography computational economics, artificial social intelligence, and electronic networks. a survey of seer programs revealed that many are helping to create the information infrastructure, and substantial investment in six ''flagship'' digital library projects will develop the systems necessary for the nii of the 21st century.",
    "present_kp": [
      "information infrastructure",
      "digital library",
      "cognitive science",
      "computational geography",
      "computational economics",
      "internet"
    ],
    "absent_kp": [
      "artificial intelligence"
    ]
  },
  {
    "title": "web-based public participation geographical information systems: an aid to local environmental decision-making.",
    "abstract": "current research examining the potential of the world-wide web as a means of increasing public participation in local environmental decision making in the uk is discussed. the paper considers traditional methods of public participation and argues that new internet-based technologies have the potential to widen participation in the uk planning system. evidence is provided of the potential and actual benefits of online spatial decision support systems in the uk through a real environmental decision support problem in a village in northern england. the paper identifies key themes developing in this area of web-based geographical information systems (gis) and provides a case-study example of an online public participation gis from inception to the final phase in a public participation process. it is shown that in certain uk planning problems and policy formulation processes, participatory online systems are a useful means of informing and engaging the public and can potentially bring the public closer to a participatory planning system.",
    "present_kp": [
      "gis",
      "web",
      "public participation"
    ],
    "absent_kp": [
      "planning for real"
    ]
  },
  {
    "title": "medical image analysis for cancer management in natural computing framework.",
    "abstract": "natural computing, through its repertoire of nature-inspired strategies, is playing a major role in the development of intelligent decision-making systems. the objective is to provide flexible, application-oriented solutions to current medical image analysis problems. it encompasses fuzzy sets, neural networks, genetic algorithms, rough sets, swarm intelligence, and a host of other paradigms, mimicking biological and physical processes from nature. radiographic imaging modalities, like computed tomography (ct), positron emission tomography (pet), and magnetic resonance imaging (mri), help in providing improved diagnosis, prognosis and treatment planning for cancer. this survey highlights the role of natural computing, in efficiently analyzing radiographic medical images, for improved tumor management. we also provide a categorization of the segmentation, feature extraction and selection methods, based on different natural computing technologies, with reference to the application involving malignancy of the brain, breast, prostate, skin, lung, and liver.",
    "present_kp": [
      "segmentation"
    ],
    "absent_kp": [
      "quantitative imaging",
      "feature selection",
      "radiomics",
      "evolutionary algorithms",
      "biomedical imaging"
    ]
  },
  {
    "title": "parallelisation of the lagrangian model in a mixed eulerian-lagrangian cfd algorithm.",
    "abstract": "this manuscript presents an algorithm implemented in a commercial computational fluid dynamics (cfd) code for parallelisation of the lagrangian particle tracking model in a mixed eulerian-lagrangian cfd algorithm. the algorithm is based on the domain decomposition parallelisation strategy and asynchronous message passing protocol. the methodology is tested on two industrial cfd test cases and the parallelisation results are presented. further, it is discussed how the parallel efficiency of the runs can be improved by adopting the domain decomposition scattering technique.",
    "present_kp": [],
    "absent_kp": [
      "parallel eulerian-lagrangian cfd",
      "parallel particle tracking",
      "parallelisation of the spray model"
    ]
  },
  {
    "title": "sequential and parallel triangulating algorithms for elimination game and new insights on minimum degree.",
    "abstract": "elimination game is a well-known algorithm that simulates gaussian elimination of matrices on graphs, and it computes a triangulation of the input graph. the number of fill edges in the computed triangulation is highly dependent on the order in which elimination game processes the vertices, and in general the produced triangulations are neither minimum nor minimal. in order to obtain a triangulation which is close to minimum, the minimum degree heuristic is widely used in practice, but until now little was known on the theoretical mechanisms involved. in this paper we show some interesting properties of elimination came; in particular that it is able to compute a partial minimal triangulation of the input graph regardless of the order in which the vertices are processed. this results in a new algorithm to compute minimal triangulations that are sandwiched between the input graph and the triangulation resulting from elimination came. one of the strengths of the new approach is that it is easily parallelizable, and thus we are able to present the first parallel algorithm to compute such sandwiched minimal triangulations. in addition, the insight that we gain through elimination game is used to partly explain the good behavior of the minimum degree algorithm. we also give a new algorithm for producing minimal triangulations that is able to use the minimum degree idea to a wider extent.",
    "present_kp": [
      "minimum degree",
      "minimal triangulation"
    ],
    "absent_kp": [
      "chordal graphs",
      "parallel and sequential algorithms"
    ]
  },
  {
    "title": "quality evaluation of e-government digital services.",
    "abstract": "in this paper we present a \"quality estimation model\" for digital e-government services suitable for quality evaluation, monitoring, discovery, selection and composition.",
    "present_kp": [
      "e-government"
    ],
    "absent_kp": [
      "quality model",
      "quality of service"
    ]
  },
  {
    "title": "throughput improvement of incremental redundancy ldpc coded mimo v-blast system.",
    "abstract": "in this paper, we present ensembles of incremental redundancy low-density parity-check (ir-ldpc) codes to improve the throughput performance of hybrid forward error correction (fec)/automatic repeat request (arq) schemes in a vertical bell lab layered space-time (v-blast) system. these ensembles are designed to have good error rate performance at short block lengths, which result in higher throughput performance. the throughput simulations in various fading conditions show that these ensembles outperform a conventional random punctured ensemble by 3 db eb/n0 at a throughput region of 0.8. to reduce the traffic of feedback channels, we consider using an adaptive code selection algorithm. in these adaptive hybrid fec/arq schemes, the number of negative acknowledgement signals for retransmission is greatly reduced at operating snr ranges without any significant throughput loss.",
    "present_kp": [
      "hybrid fec/arq scheme"
    ],
    "absent_kp": [
      "mimo system",
      "ldpc codes"
    ]
  },
  {
    "title": "benefits of averaging lateration estimates obtained using overlapped subgroups of sensor data.",
    "abstract": "in this paper, we suggest averaging lateration estimates obtained using overlapped subgroups of distance measurements as opposed to obtaining a single lateration estimate from all of the measurements directly if a redundant number of measurements are available. least squares based closed form equations are used in the lateration. in the case of gaussian measurement noise the performances are similar in general and for some subgroup sizes marginal gains are attained. averaging laterations method becomes especially beneficial if the lateration estimates are classified as useful or not in the presence of outlier measurements whose distributions are modeled by a mixture of gaussians (mog) pdf. a new modified trimmed mean robust averager helps to regain the performance loss caused by the outliers. if the measurement noise is gaussian, large subgroup sizes are preferable. on the contrary, in robust averaging small subgroup sizes are more effective for eliminating measurements highly contaminated with mog noise. the effect of high-variance noise was almost totally eliminated when robust averaging of estimates is applied to qr decomposition based location estimator. the performance of this estimator is just 1 cm worse in root mean square error compared to the cramrrao lower bound (crlb) on the variance both for gaussian and mog noise cases. theoretical crlbs in the case of mog noise are derived both for time of arrival and time difference of arrival measurement data.",
    "present_kp": [
      "averaging",
      "lateration",
      "robust averaging"
    ],
    "absent_kp": [
      "least squares time of arrival location estimator",
      "least squares time difference of arrival location estimator",
      "robust estimator"
    ]
  },
  {
    "title": "em-based iterative receiver for coded mimo systems in unknown spatially correlated noise.",
    "abstract": "we present iterative channel estimation and decoding schemes for multi-input multi-output (mimo) rayleigh block fading channels in spatially correlated noise. an expectation-maximization (em) algorithm is utilized to find the maximum likelihood (ml) estimates of the channel and spatial noise covariance matrices, and to compute soft information of coded symbols which is sent to an error-control decoder. the extrinsic information produced by the decoder is then used to refine channel estimation. several iterations are performed between the above channel estimation and decoding steps. we derive modified cramer-rao bound (mcrb) for the unknown channel and noise parameters, and show that the proposed em-based channel estimation scheme achieves the mcrb at medium and high snrs. for a bit error rate of 10(-6) and long frame length, there is negligible performance difference between the proposed scheme and the ideal coherent detector that utilizes the true channel and noise covariance matrices.",
    "present_kp": [
      "mimo",
      "channel estimation",
      "expectation-maximization",
      "crb"
    ],
    "absent_kp": [
      "turbo code",
      "ber"
    ]
  },
  {
    "title": "parameter exploration in science and engineering using many-task computing.",
    "abstract": "robust scientific methods require the exploration of the parameter space of a system (some of which can be run in parallel on distributed resources), and may involve complete state space exploration, experimental design, or numerical optimization techniques. many-task computing (mtc) provides a framework for performing robust design, because it supports the execution of a large number of otherwise independent processes. further, scientific workflow engines facilitate the specification and execution of complex software pipelines, such as those found in real science and engineering design problems. however, most existing workflow engines do not support a wide range of experimentation techniques, nor do they support a large number of independent tasks. in this paper, we discuss nimrod/k-a set of add in components and a new run time machine for a general workflow engine, kepler. nimrod/k provides an execution architecture based on the tagged dataflow concepts, developed in 1980s for highly parallel machines. this is embodied in a new kepler \"director\" that supports many-task computing by orchestrating execution of tasks on on clusters, grids, and clouds. further, nimrod/k provides a set of \"actors\" that facilitate the various modes of parameter exploration discussed above. we demonstrate the power of nimrod/k to solve real problems in cardiac science.",
    "present_kp": [
      "parameter exploration",
      "kepler",
      "many-task computing"
    ],
    "absent_kp": [
      "scientific workflows"
    ]
  },
  {
    "title": "group context-based adaptations for recommendation.",
    "abstract": "in groupware or community based applications the user interface is usually static or tailored to the individual user's needs. newer developments try to adapt the user interface automatically in regard to user contexts. even though these techniques are proven useful, there exists no contextadaptive system taking the current context of a group or community in regard. in this paper, we briefly discuss the problems of defining context and present our understanding of context as a subset of the current information state. we provide an exemplary scenario to present different approaches how to compute group contexts based on semantic models and user contexts, and the consequences for the adaptation goals - in the interface or through changes at system functionalities or tools. we additionally discuss the problems occurring at evaluating adaptations and the value of group context for collaborative work.",
    "present_kp": [
      "user context",
      "semantic models",
      "group context",
      "group context-based adaptation",
      "context"
    ],
    "absent_kp": [
      "content recommendation"
    ]
  },
  {
    "title": "issues of trust and control on agent autonomy.",
    "abstract": "the relationship between trust and control is quite relevant both for the very notion of trust and for modelling and implementing trust-control relations with autonomous systems. we claim that control is antagonistic of the strict form of trust: 'trust in y'; but also that it completes and complements it for arriving to a global trust. in other words, putting control and guaranties is trust-building; it produces a sufficient trust, when trust in y's autonomous willingness and competence would not be enough. we also argue that control requires new forms of trust: trust in the control itself or in the controller, trust in y as for being monitored and controlled, trust in possible authorities, etc. finally, we show that paradoxically control could not be antagonistic of strict trust in y, but it can even create, increase it by making y more willing or more effective. in conclusion, depending on the circumstances, control makes y more reliable or less reliable; control can either decrease or increase trust. a good theory of trust cannot be complete without a theory of control.",
    "present_kp": [
      "trust",
      "control"
    ],
    "absent_kp": [
      "autonomous agents",
      "cooperation",
      "adjustable autonomy"
    ]
  },
  {
    "title": "youubi: open software for ubiquitous learning.",
    "abstract": "we propose a reference architecture for u-learning environments. we propose a method development and validation of u-learning environments. we present a u-learning environment that combines playful aspects with learning strategies.",
    "present_kp": [
      "ubiquitous learning"
    ],
    "absent_kp": [
      "ubiquitous computing",
      "software engineering",
      "design interactive",
      "gamification"
    ]
  },
  {
    "title": "single-symbol ml decodable distributed stbcs for cooperative networks.",
    "abstract": "in this correspondence, the distributed orthogonal space-time block codes (dostbcs), which achieve the single-symbol maximum likelihood (ml) decodability and full diversity order, are first considered. however, systematic construction of the dostbcs is very hard, since the noise covariance matrix is not diagonal in general. thus, some special dostbcs, which have diagonal noise covariance matrices at the destination terminal, are investigated. these codes are referred to as the row-monomial dostbcs. an upper bound of the data-rate of the row-monomial dostbc is derived and it is approximately twice higher than that of the repetition-based cooperative strategy. furthermore, systematic construction methods of the row-monomial dostbcs achieving the upper bound of the data-rate are developed when the number of relays and/or the number of information-bearing symbols are even.",
    "present_kp": [
      "cooperative networks",
      "diversity"
    ],
    "absent_kp": [
      "distributed space-time block codes",
      "single-symbol maximum likelihood  decoding"
    ]
  },
  {
    "title": "ontology-based data mining approach implemented for sport marketing.",
    "abstract": "since sport marketing is a commercial activity, precise customer and marketing segmentation must be investigated frequently and it would help to know the sport market after a specific customer profile, segmentation, or pattern come with marketing activities has found. such knowledge would not only help sport firms, but would also contribute to the broader field of sport customer behavior and marketing. this paper proposes using the apriori algorithm of association rules, and clustering analysis based on an ontology-based data mining approach, for mining customer knowledge from the database. knowledge extracted from data mining results is illustrated as knowledge patterns, rules, and maps in order to propose suggestions and solutions to the case firm, taiwan adidas, for possible product promotion and sport marketing.",
    "present_kp": [
      "sport marketing",
      "ontology",
      "data mining",
      "apriori algorithm",
      "clustering analysis"
    ],
    "absent_kp": [
      "endorser",
      "media"
    ]
  },
  {
    "title": "layered acting for character animation.",
    "abstract": "we introduce an acting-based animation system for creating and editing character animation at interactive speeds. our system requires minimal training, typically under an hour, and is well suited for rapidly prototyping and creating expressive motion. a real-time motion-capture framework records the user's motions for simultaneous analysis and playback on a large screen. the animator's real-world, expressive motions are mapped into the character's virtual world. visual feedback maintains a tight coupling between the animator and character. complex motion is created by layering multiple passes of acting. we also introduce a novel motion-editing technique, which derives implicit relationships between the animator and character. the animator mimics some aspect of the character motion, and the system infers the association between features of the animator's motion and those of the character. the animator modifies the mimic by acting again, and the system maps the changes onto the character. we demonstrate our system with several examples and present the results from informal user studies with expert and novice animators.",
    "present_kp": [
      " framework ",
      "examples",
      "association",
      "maps",
      "analysis",
      "express",
      "novice",
      "real-time",
      "informal",
      "motion",
      "user",
      "minimal",
      "change",
      "virtual world",
      "records",
      "training",
      "character",
      "aspect",
      "relationships",
      "motion-capture",
      "character animation",
      "edit",
      "layer",
      "demonstrate",
      "feedback",
      "feature",
      "animation",
      "user studies"
    ],
    "absent_kp": [
      "statistical analysis",
      "3d user interfaces",
      "interaction",
      "motion transformation",
      "visualization",
      "systems",
      "motion editing",
      "couples",
      "prototype",
      "complexity"
    ]
  },
  {
    "title": "an extensible architecture-based framework for coordination languages.",
    "abstract": "the dynamic and heterogeneous nature of distributed systems makes the development of distributed applications a difficult task. various tools, such as middleware systems, component systems, and coordination languages, offer support the application developer at different levels. there are several coordination systems that integrate such tools into a complete environment to build applications from heterogeneous components. to achieve extensibility they usually have a layered architecture: an application is first mapped to a middle layer and then to a target system. but this approach hides the specific features of a target system from the developer, as they are not represented in the middle layer, and often induces additional run-time overhead. in this paper, we introduce the extensible coordination framework ecf that allows developers to build efficient distributed applications which exploit the specific features of the target systems. support for target systems and application domains are encapsulated by extension modules. modules can be built on top of other modules to support refined functionality.",
    "present_kp": [
      "coordination language",
      "distributed systems"
    ],
    "absent_kp": [
      "component technology",
      "developer framework",
      "software architectures"
    ]
  },
  {
    "title": "novel approaches to the measurement of arterial blood flow from dynamic digital x-ray images.",
    "abstract": "we have developed two new algorithms for the measurement of blood flow from dynamic x-ray angiographic images. both algorithms aim to improve on existing techniques. first, a model-based (mb) algorithm is used to constrain the concentration-distance curve matching approach. second, a weighted optical flow algorithm (op) is used to improve on point-based optical flow methods by averaging velocity estimates along a vessel with weighting based on the magnitude of the spatial derivative. the op algorithm was validated using a computer simulation of pulsatile blood flow. both the op and the mb algorithms were validated using a physiological blood flow circuit. dynamic biplane digital x-ray images were acquired following injection of iodine contrast medium into a variety of simulated arterial vessels. the image data were analyzed using our integrated angiographic analysis software sara to give blood how waveforms using the nib and op algorithms. these waveforms were compared to flow measured using an electromagnetic flow meter (emf). in total 4935 instantaneous measurements of flow were made and compared to the emf recordings. it was found that the new algorithms showed low measurement bias and narrow limits of agreement and also outperformed the concentration-distance curve matching algorithm (org) and a modification of this algorithm (pa) in all studies.",
    "present_kp": [],
    "absent_kp": [
      "blood flow measurement",
      "x-ray angiography",
      "x-ray measurements"
    ]
  },
  {
    "title": "his-monitor: an approach to assess the quality of information processing in hospitals.",
    "abstract": "hospital information systems (his) are a substantial quality and cost factor for hospitals. systematic monitoring of his quality is an important task; however, this task is often seen to be insufficiently supported. to support systematic his monitoring, we developed his-monitor, comprising about 107 questions, focusing on how a hospital information system does efficiently support clinical and administrative tasks. the structure of his-monitor consists of a matrix, crossing his quality criteria on one axis with a list of process steps within patient care on the other axis. his-monitor was developed based on several pretests and was now tested in a larger feasibility study with 102 participants. his-monitor intends to describe strengths and weaknesses of information processing in a hospital. results of the feasibility study show that his-monitor was able to highlight certain his problems such as insufficiently supported cross-departmental communication, legibility of drug orders and other paper-based documents, and overall time needed for documentation. we discuss feasibility of his-monitor and the reliability and validity of the results. further refinement and more formal validation of his-monitor are planned.",
    "present_kp": [
      "quality",
      "hospital information systems"
    ],
    "absent_kp": [
      "healthcare information systems",
      "information management",
      "evaluation",
      "questionnaire"
    ]
  },
  {
    "title": "recursive channel estimation based on finite parameter model using reduced-complexity maximum likelihood equalizer for ofdm over doubly-selective channels.",
    "abstract": "to take intercarrier interference (ici) attributed to time variations of the channel into consideration, the time- and frequency-selective (doubly-selective) channel is parameterized by a finite parameter model. by capitalizing on the finite parameter model to approximate the doubly-selective channel, a kalman filter is developed for channel estimation. the ici suppressing, reduced-complexity viterbi-type maximum likelihood (rml) equalizer is incorporated into the kalman filter for recursive channel tracking and equalization to improve the system performance. an enhancement in the channel tracking ability is validated by theoretical analysis, and a significant improvement in ber performance using the channel estimates obtained by the recursive channel estimation method is verified by monte-carlo simulations.",
    "present_kp": [],
    "absent_kp": [
      "polynomial model",
      "oversampled basis expansion model",
      "recursive kalman",
      "reduced-complexity ml equalizer"
    ]
  },
  {
    "title": "a web-service agent-based decision support system for securities exception management.",
    "abstract": "with rising trading volumes and increasing risks in securities transactions, the securities industry is making an effort to shorten the trade lifecycle and minimize transaction risks. while attempting to achieve this, exception management is crucial to pass trade information within the trade lifecycle in a timely and accurate fashion. for a competitive solution to exception management, a web-service-agent-based decision support system is developed in this paper. agent technology is applied to deal with the dynamic, complex, and distributed processes in exception management; web services techniques are proposed for more scalability and interoperability in network-based business environment. by integrating agent technology with web services to make use of the advantages from both, this approach leads to more intelligence, flexibility and collaboration in business exception management. the effectiveness of this approach is evaluated through a use case and demonstration feedback.",
    "present_kp": [
      "web services",
      "exception management"
    ],
    "absent_kp": [
      "intelligent agents",
      "decision support systems",
      "securities trading"
    ]
  },
  {
    "title": "measuring energy consumption using eml (energy measurement library).",
    "abstract": "energy consumption and efficiency is a main issue in high performance computing systems in order to reach exascale computing. researchers in the field are focusing their effort in reducing the first and increasing the latter while there is no current standard for energy measurement. current energy measurement tools are specific and architectural dependent and this has to be addressed. by creating a standard tool, it is possible to generate independence between the experiments and the hardware, and thus, researchers effort can be focused in energy, by maximizing the portability of the code used for experimentation with the multiple architectures we have access nowadays. we present the energy measurement library (eml) library, a software library that eases the access to the energy measurement tools and can be easily extended to add new measurement systems. using eml, it is viable to obtain architectural and algorithmic parameters that affect energy consumption and efficiency. the use of this library is tested in the field of the analytic modeling of the energy consumed by parallel programs.",
    "present_kp": [
      "energy measurement library",
      "eml"
    ],
    "absent_kp": [
      "energy-aware computing",
      "energy efficiency"
    ]
  },
  {
    "title": "methylation-sensitive representational difference analysis and its application to cancer research.",
    "abstract": "methylation-sensitive representational difference analysis (ms-rda) was previously established to detect differences in the methylation status of two genomes. this method uses the digestion of genomic dna with a methylation-sensitive restriction enzyme, hpaii, and pcr to prepare hpaii-amplicons, followed by rda. an hpaii-amplicon prepared using betaine and reverse electrophoresis was enriched 3.6-fold (compared with the hpaii-amplicon prepared by the original method) with dna fragments originating from cpg islands (cgis). as for the specificity of ms-rda, it was shown that dna fragments that are unmethylated in the tester and almost completely methylated in the driver are efficiently isolated. this indicated that genes that are in biallelic methylation or in monoallelic methylation with loss of the other allele are efficiently isolated. further, by use of two additional methylation-sensitive six-base recognition restriction enzymes, sacii and nari, more dna fragments were isolated from cgis in the 5? regions of genes. after analysis of human lung, gastric, and breast cancers, 12 genes were seen to be silenced and additional genes seen to show decreased expression in association with methylation of genomic regions outside cgis in the 5? regions of genes. ms-rda is effective in identifying silenced genes in various cancers.",
    "present_kp": [
      "cpg island",
      "cancer"
    ],
    "absent_kp": [
      "dna methylation",
      "gene silencing",
      "genome scanning"
    ]
  },
  {
    "title": "simple ptass for families of graphs excluding a minor.",
    "abstract": "we show that very simple algorithms based on local search are polynomial-time approximation schemes for maximum independent set, minimum vertex cover and minimum dominating set, when the input graphs have a fixed forbidden minor.",
    "present_kp": [
      "polynomial-time approximation scheme",
      "forbidden minor",
      "maximum independent set",
      "minimum vertex cover",
      "minimum dominating set"
    ],
    "absent_kp": [
      "separator",
      "local optimization"
    ]
  },
  {
    "title": "a modification of the index of liou and wang for ranking fuzzy number.",
    "abstract": "different methods have been proposed for ranking fuzzy numbers. these include methods based on distances, centroid point, coefficient of variation, and weighted mean value. however, there is still no method that can always give a satisfactory result to every situation; some are counterintuitive and not discriminating. this paper presents an approach for ranking fuzzy numbers with integral value that is an extension of the index of liou and wang. this method, that is independent of the type of membership function used, can rank more than two fuzzy numbers simultaneously. this ranking method use an index of optimism to reflect the decision maker's optimistic attitude, but rather it also contains an index of modality that represents the neutrality of the decision maker. the approach is illustrated with numerical examples.",
    "present_kp": [
      "ranking fuzzy numbers",
      "integral value",
      "index of optimism",
      "index of modality"
    ],
    "absent_kp": []
  },
  {
    "title": "a mobility-based load control scheme in hierarchical mobile ipv6 networks.",
    "abstract": "by introducing a mobility anchor point (map), hierarchical mobile ipv6 (hmipv6) reduces the signaling overhead and handoff latency associated with mobile ipv6. in this paper, we propose a mobility-based load control (mlc) scheme, which mitigates the burden of the map in fully distributed and adaptive manners. the mlc scheme combines two algorithms: a threshold-based admission control algorithm and a session-to-mobility ratio (smr)-based replacement algorithm. the threshold-based admission control algorithm gives higher priority to ongoing mobile nodes (mns) than new mns, by blocking new mns when the number of mns being serviced by the map is greater than a predetermined threshold. on the other hand, the smr-based replacement algorithm achieves efficient map load distribution by considering mns traffic and mobility patterns. we analyze the mlc scheme using the continuous time markov chain in terms of the new mn blocking probability, ongoing mn dropping probability, and binding update cost. also, the map processing latency is evaluated based on the m/g/1 queueing model. analytical and simulation results demonstrate that the mlc scheme outperforms other schemes and thus it is a viable solution for scalable hmipv6 networks.",
    "present_kp": [
      "hierarchical mobile ipv6",
      "mobility-based load control",
      "mobility anchor point",
      "admission control algorithm",
      "session-to-mobility ratio"
    ],
    "absent_kp": []
  },
  {
    "title": "randomized diffusion for indivisible loads.",
    "abstract": "presentation of new algorithm for balancing discrete loads. algorithm is very natural and avoids nodes having negative loads. quality measure is the gap between maximum and minimum load, called discrepancy. upper bounds on discrepancy after algorithm has run as long as its continuous counterpart.",
    "present_kp": [
      "diffusion"
    ],
    "absent_kp": [
      "distributed computing",
      "load balancing",
      "randomized algorithms",
      "random walk"
    ]
  },
  {
    "title": "automatic image segmentation by dynamic region merging.",
    "abstract": "this paper addresses the automatic image segmentation problem in a region merging style. with an initially oversegmented image, in which many regions (or superpixels) with homogeneous color are detected, an image segmentation is performed by iteratively merging the regions according to a statistical test. there are two essential issues in a region-merging algorithm: order of merging and the stopping criterion. in the proposed algorithm, these two issues are solved by a novel predicate, which is defined by the sequential probability ratio test and the minimal cost criterion. starting from an oversegmented image, neighboring regions are progressively merged if there is an evidence for merging according to this predicate. we show that the merging order follows the principle of dynamic programming. this formulates the image segmentation as an inference problem, where the final segmentation is established based on the observed image. we also prove that the produced segmentation satisfies certain global properties. in addition, a faster algorithm is developed to accelerate the region-merging process, which maintains a nearest neighbor graph in each iteration. experiments on real natural images are conducted to demonstrate the performance of the proposed dynamic region-merging algorithm.",
    "present_kp": [
      "image segmentation",
      "region merging"
    ],
    "absent_kp": [
      "dynamic programming ",
      "wald's sequential probability ratio test "
    ]
  },
  {
    "title": "boundary properties of the inconsistency of pairwise comparisons in group decisions.",
    "abstract": "we study the inconsistency of pairwise comparisons in group decision making. we study the effect of the aggregation of pairwise comparisons on their consistency. we derive general results and provide a complete study for well-known inconsistency indices. we start a discussion on the meaning of inconsistency of aggregated preferences.",
    "present_kp": [
      "inconsistency indices",
      "boundary properties",
      "group decision making"
    ],
    "absent_kp": [
      "pairwise comparison matrix",
      "analytic hierarchy process"
    ]
  },
  {
    "title": "sensitivity of optimal prices to system parameters in a steady-state service facility.",
    "abstract": "we consider the problem of maximizing the long-run average reward in a service facility with dynamic pricing. we investigate sensitivity of optimal pricing policies to the parameters of the service facility which is modelled as an m/m/s/k m / m / s / k queueing system. arrival process to the facility is poisson with arrival rate a decreasing function of the price currently being charged by the facility. we prove structural results on the optimal pricing policies when the parameters in the facility change. namely, we show that optimal prices decrease when the capacity of the facility or the number of servers in the facility increase. under a reasonable assumption, we also show that optimal prices increase as the overall demand for the service provided by the facility increases or when the service rate of the facility decreases. we illustrate how these structural results simplify the required computational effort while finding the optimal policy.",
    "present_kp": [
      "queueing",
      "pricing"
    ],
    "absent_kp": [
      "stochastic programming",
      "markov decision processes",
      "robustness and sensitivity analysis"
    ]
  },
  {
    "title": "a self-organizing p2p system with multi-dimensional structure.",
    "abstract": "this paper presents and analyzes self-can, a self-organizing p2p system that, while relying on the multi-dimensional structured organization of peers provided by can, exploits the operations of ant-based mobile agents to sort the resource keys and distribute them to peers. the benefits of the self-organization approach are remarkable, starting from increased flexibility and robustness, to better load balancing characteristics. most notably, peer indexes and resource keys can be defined on different and independent spaces, which overcomes the main limitation of standard structured p2p systems, i.e., the necessity of assigning each key to a peer having a specified index. this decoupling opens the possibility of giving a semantic meaning to resource keys and enables the efficient execution of multi-dimensional range queries, which are essential in some types of distributed systems, for example in grids.",
    "present_kp": [
      "self-organizing"
    ],
    "absent_kp": [
      "bio-inspired",
      "peer-to-peer",
      "key-value storage",
      "resource discovery"
    ]
  },
  {
    "title": "self-organized traffic control.",
    "abstract": "in this paper we propose and present preliminary results on the migration of traffic lights as roadside-based infrastructures to in-vehicle virtual signs supported only by vehicle- to-vehicle communications. we design a virtual traffic light protocol that can dynamically optimize the flow of traffic in road intersections without requiring any roadside infrastructure. elected vehicles act as temporary road junction infrastructures and broadcast traffic light messages that are shown to drivers through in-vehicle displays. this approach renders signalized control of intersections truly ubiquitous, which significantly increases the overall traffic flow. we pro- vide compelling evidence that our proposal is a scalable and cost-effective solution to urban traffic control.",
    "present_kp": [
      "self-organized traffic",
      "traffic lights"
    ],
    "absent_kp": [
      "v2v communication"
    ]
  },
  {
    "title": "search strategies on a new health information retrieval system.",
    "abstract": "purpose - the goals of this study are: to evaluate the merits of a newly developed health information retrieval system; to investigate users' search strategies when using the new search system; and to study the relationships between users' search strategies and their prior topic knowledge. design/methodology/approach - the paper developed a new health information retrieval system called meshmed. a term browser and a tree browser are included in the new system in addition to the traditional search box. the term browser allows a user to search medical subject heading (mesh) terms using natural language. the tree browser presents a hierarchical tree structure of related mesh terms. a user study with 30 participants was conducted to evaluate the benefits of meshmed. findings - the paper found that meshmed provides a user with more choices to select an appropriate searching component and form more effective search strategies. based on the time a participant spent using different meshmed components, the paper identified three different search styles: the traditional style, the novel style, and the balanced style, which falls in between. meshmed was particularly helpful for users with low topic knowledge. originality/value - a new health information retrieval system (meshmed) was designed and developed (and is currently available at <url>9/meshmed). this is the first study to explore users' search strategies on such a system. the study results can inform the design of future clinical-oriented health information retrieval systems.",
    "present_kp": [
      "information retrieval"
    ],
    "absent_kp": [
      "knowledge management",
      "hospitals",
      "information systems"
    ]
  },
  {
    "title": "anfis approach to the scour depth prediction at a bridge abutment.",
    "abstract": "an accurate estimation of the maximum possible scour depth at bridge abutments is of paramount importance in decision-making for the safe abutment foundation depth and also for the degree of scour counter-measure to be implemented against excessive scouring. despite analysis of innumerable prototype and hydraulic model studies in the past, the scour depth prediction at the bridge abutments has remained inconclusive. this paper presents an alternative to the conventional regression model (rm) in the form of an adaptive network-based fuzzy inference system (anfis) modelling. the performance of anfis over rm and artificial neural networks (anns) is assessed here. it was found that the anfis model performed best among of these methods. the causative variables in raw form result in a more accurate prediction of the scour depth than that of their grouped form.",
    "present_kp": [
      "bridge abutments",
      "neural network"
    ],
    "absent_kp": [
      "anfis and regression analysis",
      "local scour"
    ]
  },
  {
    "title": "an improved strength pareto evolutionary algorithm 2 with application to the optimization of distributed generations.",
    "abstract": "this paper presents an improved strength pareto evolutionary algorithm 2 (ispea2), which introduces a penalty factor in objective function constraints, uses adaptive crossover and a mutation operator in the evolutionary process, and combines simulated annealing iterative process over spea2. the testing result of ispea2 by authoritative testing functions meets the requirement of petro-optimum fronts. the case study result shows that the proposed algorithm provides a rapid convergence in obtaining pareto-optimal solutions during the calculation process of evolution. based on the fuzzy set theory, ispea2 is able to solve the multi-objective problems in the ieee 33-bus system, and its validity and practicality are demonstrated by the utilization on dgs economic dispatch and optimal operation in the field of power industry.",
    "present_kp": [
      "improved strength pareto evolutionary algorithm",
      "simulated annealing",
      "distributed generation"
    ],
    "absent_kp": [
      "distribution power system",
      "coordinative optimization"
    ]
  },
  {
    "title": "a distributed instrument for performance analysis of real-time ethernet networks.",
    "abstract": "ethernet technology is widely used in real-time industrial automation. thanks to real-time ethernet (rte) protocols, defined in iec61784-2 standard, new top-performance automation solutions can be created. such systems may have communication cycle time down to tens of mu s and cycle jitter less than 1 mu s, making network testing and debugging very critical. existing network and protocol analyzers can perform detailed local analysis, but characterization of high-performance rte systems requires measurement of transmission delays and these instruments cannot be adequately synchronized among them to realize a distributed measurement network. this paper introduces a new low-cost distributed measurement instrument to measure timing characteristics of rte nodes (end-to-end delays, synchronization, etc.). the proposed instrument has multiple fpga-based probes that allow for simultaneous/synchronized logging on different place of the target rte network. a pc-based \"monitor station\" stores all the data, ready for further elaboration. architecture details are discussed, a prototype has been realized, and some experimental results are presented. for instance, synchronization accuracy between probes is below 100 ns.",
    "present_kp": [
      "ethernet network",
      "performance analysis"
    ],
    "absent_kp": [
      "fieldbus",
      "network synchronization",
      "real time"
    ]
  },
  {
    "title": "documentation standards for beginning students.",
    "abstract": "the importance of writing programs that are readable has finally gained preeminence in the struggle with such competing and contradictory goals as cuteness and optimization of code. as a result, a much greater stress on documentation standards is found in computer science education these days. industry and government standards for documentation are being more widely adhered to and certain points of agreement have emerged. some excellent books have been written that cover the subject (van tassel, 1974; ledgard, 1975; kernighan & plauger, 1974); however it is safe to say that both the exhaustive treatment of the subject in such publications and the extremely high standards proposed probably preclude wholesale adoption by instructors of beginning level programming courses. what is proposed here is a set of common sense, scaled down documentation standards for the student in a first programming course in, say, fortran, pl/i, algol, or basic. the following represents an amalgam of documentation requirements achieved as a result of teaching introductory programming to college students for nine years. the actual sources have been the literature, colleagues, and last but not least, experience. they are not intended to represent an only or best approach; the author has recently encountered other efforts in this direction that must surely be as reasonable and effective. it does represent one educator's approach; it is sufficiently scaled down so that one might reasonably expect to use it as a standard for beginning students; and it may be most useful as a contributor of components to be integrated into a more effective set of standards. the basics of documentation and readable programming include comments, meaningful variable names, labelled output, flowcharts, and clear program flow. the major components of and basic rules for each of these categories will be presented in the context of the needs and limitations of the beginning student.",
    "present_kp": [
      "requirements",
      "point",
      "use",
      "goals",
      "computer science education",
      "context",
      "teaching",
      "direct",
      "experience",
      "writing",
      "student",
      "component",
      "program",
      "introductory programming",
      "author",
      "public",
      "code",
      "flow",
      "adopt",
      " stress ",
      "documentation",
      "effect",
      "rules"
    ],
    "absent_kp": [
      "standardization",
      "variability",
      "industrial",
      "governance",
      "integrability"
    ]
  },
  {
    "title": "differential space-time modulation for ds-cdma systems.",
    "abstract": "differential space-time modulation (dstm) schemes were recently proposed to fully exploit the transmit and receive antenna diversities without the need for channel state information. dstm is attractive in fast flat fading channels since accurate channel estimation is difficult to achieve. in this paper. we propose a new modulation scheme to improve the performance of ds-cdma systems in fast time-dispersive fading channels. this scheme is referred to as the differential space-time modulation for ds-cdma (dst-cdma) systems. the new modulation and demodulation schemes are especially studied for the fast fading down-link transmission in ds-cdma systems employing multiple transmit antennas and one receive antenna. we present three demodulation schemes. referred to as the differential space-time rake (dstr) receiver, differential space-time deterministic (dstd) receiver, and differential space-time deterministic de-prefix (dstdd) receiver, respectively. the dstd receiver exploits the known information of the spreading sequences and their delayed paths deterministically besides the rake-type combination: consequently, it can outperform the dstr receiver. which employs the rake-type combination only, especially for moderate-to-high snr. the dstdd receiver avoids the effect of intersymbol interference and hence can offer better performance than the dstd receiver.",
    "present_kp": [
      "ds-cdma"
    ],
    "absent_kp": [
      "wireless communications",
      "space-time coding",
      "smart antennas",
      "spread spectrum",
      "rake receiver"
    ]
  },
  {
    "title": "on the normalization of interval and fuzzy weights.",
    "abstract": "the normalization of interval and fuzzy weights is often necessary in multiple criteria decision analysis (mcda) under uncertainty, especially in analytic hierarchy process (ahp) with interval or fuzzy judgements. the existing normalization methods based on interval arithmetic and fuzzy arithmetic are found flawed and need to be revised. this paper presents the correct normalization methods for interval and fuzzy weights and offers relevant theorems in support of them. numerical examples are examined to show the correctness of the proposed normalization methods and their differences from those existing normalization methods.",
    "present_kp": [
      "fuzzy weights",
      "normalization"
    ],
    "absent_kp": [
      "interval weights",
      "multiple criteria decision making"
    ]
  },
  {
    "title": "a queue with semi-markovian batch plus poisson arrivals with application to the mpeg frame sequence.",
    "abstract": "we consider a queueing system with a single server having a mixture of a semi-markov process (smp) and a poisson process as the arrival process, where each smp arrival contains a batch of customers. the service times are exponentially distributed. we derive the distributions of the queue length of both smp and poisson customers when the sojourn time distributions of the smp have rational laplace-stieltjes transforms. we prove that the number of unknown constants contained in the generating function for the queue length distribution equals the number of zeros of the denominator of this generating function in the case where the sojourn times of the smp follow exponential distributions. the linear independence of the equations generated by those zeros is discussed for the same case with additional assumption. the necessary and sufficient condition for the stability of the system is also analyzed. the distributions of the waiting times of both smp and poisson customers are derived. the results are applied to the case in which the smp arrivals correspond to the exact sequence of motion picture experts group ( mpeg) frames. poisson arrivals are regarded as interfering traffic. in the numerical examples, the mean and variance of the waiting time of the atm cells generated from the mpeg frames of real video data are evaluated.",
    "present_kp": [
      "semi-markov process",
      "queue",
      "waiting time",
      "mpeg"
    ],
    "absent_kp": [
      "batch arrival",
      "group of pictures "
    ]
  },
  {
    "title": "fractal dimension as a descriptor of urban growth dynamics.",
    "abstract": "the objective of this paper is to examine the development of the urban form of the city of olomouc since the 1920s in terms of fractal dimension, and to link the observation with two other descriptors of shape - area and perimeter. the fractal dimension of built-up areas and fractal dimension of the boundary of the city are calculated employing the box-counting method; the possibilities of their interpretation and usage in urban planning are discussed. the process of urban growth is observed with respect to its fractality and perspectives of this approach are discussed. an interesting dependence between area and its fractal dimension is derived.",
    "present_kp": [
      "urban growth",
      "fractal dimension",
      "box-counting method"
    ],
    "absent_kp": [
      "area/perimeter relation"
    ]
  },
  {
    "title": "adaptive critics for dynamic optimization.",
    "abstract": "a novel action-dependent adaptive critic design (acd) is developed for dynamic optimization. the proposed combination of a particle swarm optimization-based actor and a neural network critic is demonstrated through dynamic sleep scheduling of wireless sensor motes for wildlife monitoring. the objective of the sleep scheduler is to dynamically adapt the sleep duration to nodes battery capacity and movement pattern of animals in its environment in order to obtain snapshots of the animal on its trajectory uniformly. simulation results show that the sleep time of the node determined by the actor critic yields superior quality of sensory data acquisition and enhanced node longevity.",
    "present_kp": [
      "adaptive critic design",
      "sleep scheduling",
      "wildlife monitoring"
    ],
    "absent_kp": [
      "energy efficiency",
      "wireless sensor networks"
    ]
  },
  {
    "title": "linearity and recursion in a typed lambda-calculus.",
    "abstract": "we show that the full pcf language can be encoded in l _rec, a syntactically linear ?-calculus extended with numbers, pairs, and an unbounded recursor that preserves the syntactic linearity of the calculus. we give call-by-name and call-by-value evaluation strategies and discuss implementation techniques for l _rec, exploiting its linearity.",
    "present_kp": [
      "recursion",
      "pcf"
    ],
    "absent_kp": [
      "linear lambda calculus"
    ]
  },
  {
    "title": "power aware page allocation.",
    "abstract": "one of the major challenges of post-pc computing is the need to reduce energy consumption, thereby extending the lifetime of the batteries that power these mobile devices. memory is a particularly important target for efforts to improve energy efficiency. memory technology is becoming available that offers power management features such as the ability to put individual chips in any one of several different power modes. in this paper we explore the interaction of page placement with static and dynamic hardware policies to exploit these emerging hardware features. in particular, we consider page allocation policies that can be employed by an informed operating system to complement the hardware power management strategies. we perform experiments using two complementary simulation environments: a trace-driven simulator with workload traces that are representative of mobile computing and an execution-driven simulator with a detailed processor/memory model and a more memory-intensive set of benchmarks (spec2000). our results make a compelling case for a cooperative hardware/software approach for exploiting power-aware memory, with down to as little as 45% of the energy delay for the best static policy and 1% to 20% of the energy delay for a traditional full-power memory.",
    "present_kp": [
      "challenge",
      "lifetime",
      "traces",
      "policy",
      "simulation",
      "benchmark",
      "memory model",
      "batteries",
      "delay",
      "case",
      "paper",
      "power-aware",
      "processor",
      "interaction",
      "software",
      "strategies",
      "environments",
      "energy consumption",
      "mobile device",
      "workload",
      "exploit",
      "dynamic",
      "energy",
      "operating system",
      "placement",
      "power",
      "hardware",
      "mobile computing",
      "feature",
      "energy efficiency",
      "allocation"
    ],
    "absent_kp": [
      "paging",
      "computation",
      "experience",
      "informal",
      "exploration",
      "technologies",
      "cooperation",
      "memorialized",
      "power-management"
    ]
  },
  {
    "title": "change rules for hierarchical beliefs.",
    "abstract": "the paper builds a belief hierarchy as a framework common to all uncertainty measures expressing that an actor is ambiguous about his uncertain beliefs. the belief hierarchy is further interpreted by distinguishing physical and psychical worlds, associated to objective and subjective probabilities. various rules of transformation of a belief hierarchy are introduced, especially changing subjective beliefs into objective ones. these principles are applied in order to relate different contexts of belief change, revising, updating and even focusing. the numerous belief change rules already proposed in the literature receive epistemic justifications by associating them to specific belief hierarchies and change contexts. as a result, it is shown that the resiliency of probability judgments may have some limits and be reconciled with the possibility of learning from factual messages.",
    "present_kp": [
      "belief change",
      "focusing",
      "hierarchical belief",
      "revising",
      "updating"
    ],
    "absent_kp": [
      "objective probability",
      "subjective probability"
    ]
  },
  {
    "title": "a characterization of the two-commodity network design problem.",
    "abstract": "we study the uncapacitated version of the two-commodity network design problem. we characterize optimal solutions and show that when flow costs are zero there is an optimal solution with at most one shared path. using this characterization, we solve the problem on a transformed graph with o(n) nodes and o(m) arcs based on a shortest path algorithm. next, we describe a linear programming reformulation of the problem using o(m) variables and o(n) constraints and show that it always has an integer optimal solution. we also interpret the dual constraints and variables as generalizations of the are constraints and node potentials for the shortest path problem. we show that the polyhedron described by the constraints of the reformulation always has an integer optimal solution for a more general two-commodity problem with flow costs and an additional condition on the cost function.",
    "present_kp": [
      "network design",
      "integer optima"
    ],
    "absent_kp": [
      "two commodity"
    ]
  },
  {
    "title": "on the minimum number of negations leading to super-polynomial savings.",
    "abstract": "we show that an explicit sequence of monotone functions f(n): {0, 1}(n) --> {0, 1}(m) (m less than or equal to n) can be computed by boolean circuits with polynomial (in n) number of and, or and not gates, but every such circuit must use at least log n - o(log log n) not gates. this is almost optimal because results of markov and fisher imply that, with only small increase of the total number of gates, any circuit in n variables can be simulated by a circuit with at most [log (n + 1)] not gates.",
    "present_kp": [],
    "absent_kp": [
      "computational complexity",
      "negation-limited circuits"
    ]
  },
  {
    "title": "differential log-domain wave active filters.",
    "abstract": "in this paper, the design of differential log-domain wave filters is outlined which is based on the log-domain wave technique. the differential configuration is achieved by introducing the differential log-domain wave equivalent. the resulting filters inherit the good characteristics of the wave active filters, are very modular and easy to design. the method is demonstrated by a design example and its validity is verified through simulations results.",
    "present_kp": [
      "wave active filters"
    ],
    "absent_kp": [
      "analogue filters",
      "differential filters",
      "log-domain filters"
    ]
  },
  {
    "title": "a hybrid method based on linear programming and tabu search for routing of logging trucks.",
    "abstract": "in this paper, we consider an operational routing problem to decide the daily routes of logging trucks in forestry. this industrial problem is difficult and includes aspects such as pickup and delivery with split pickups, multiple products, time windows, several time periods, multiple depots, driver changes and a heterogeneous truck fleet. in addition, the problem size is large and the solution time limited. we describe a two-phase solution approach which transforms the problem into a standard vehicle routing problem with time windows. in the first phase, we solve an lp problem in order to find a destination of flow from supply points to demand points. based on this solution, we create transport nodes which each defines the origin(s) and destination for a full truckload. in phase two, we make use of a standard tabu search method to combine these transport nodes, which can be considered to be customers in vehicle routing problems, into actual routes. the tabu search method is extended to consider some new features. the solution approach is tested on a set of industrial cases from major forest companies in sweden.",
    "present_kp": [
      "forestry",
      "routing",
      "tabu search",
      "linear programming"
    ],
    "absent_kp": [
      "or in practice"
    ]
  },
  {
    "title": "the definition of assembly line balancing difficulty and evaluation of balance solution quality.",
    "abstract": "assembly line balancing is a classic ill-structured problem where total enumeration is infeasible and optimal solutions uncertain for industrial problems. a quantitative approach to classifying problem difficulty and solution quality is therefore important. two existing measures of difficulty, order strength and west ratio are compared to a new compound expression of difficulty, project index. project index is based on individual assessment of precedence (precedence index) and task time (task time index). the current working definition of project index is given. early criteria for judging assembly lines use balance delay and smoothness index, both are flawed as criteria. line and balance efficiency are developed as more appropriate. project index, line and balance efficiency will be illustrated for a published test-case examined by the a?line balancing package. the potential for a learning approach, selecting models to suit problems using the measures of difficulty, will form part of the conclusions within this paper.",
    "present_kp": [
      "balancing",
      "evaluation"
    ],
    "absent_kp": [
      "assembly-line"
    ]
  },
  {
    "title": "a class-oriented feature selection approach for multi-class imbalanced network traffic datasets based on local and global metrics fusion.",
    "abstract": "feature selection is often used as a pre-processing step for machine learning based network traffic classification. many feature selection techniques have been developed to find an optimal subset of relevant features and to improve overall classification accuracy. but such techniques ignore the class imbalance problem encountered in network traffic classification. the selected feature subset may bias towards the traffic class that occupies the majority of traffic flows on the internet. to address this issue, this paper proposes a new approach, called class-oriented feature selection (cofs), to identify a relevant feature subset for every class. it combines the proposed local metric and the existing global metric to yield a potentially optimal feature subset for each class, and then removes the redundant features in each feature subset based on the weighted symmetric uncertainty. additionally, to enhance the generalization on network traffic data, an ensemble learning based scheme is presented with cofs to overcome the negative impacts of the data drift on a traffic classifier. experiments on real-world network traffic data show that cofs outperforms existing feature selection techniques in most cases. moreover, our approach achieves >96% flow accuracy and >93% byte accuracy on average.",
    "present_kp": [
      "feature selection",
      "multi-class imbalance",
      "data drift",
      "network traffic"
    ],
    "absent_kp": [
      "local metrics"
    ]
  },
  {
    "title": "identifying and quantifying structural nonlinearities in engineering applications from measured frequency response functions.",
    "abstract": "engineering structures seldom behave linearly and, as a result, linearity checks are common practice in the testing of critical structures exposed to dynamic loading to define the boundary of validity of the linear regime. however, in large scale industrial applications, there is no general methodology for dynamicists to extract nonlinear parameters from measured vibration data so that these can be then included in the associated numerical models. in this paper, a simple method based on the information contained in the frequency response function (frf) properties of a structure is studied. this technique falls within the category of single-degree-of-freedom (sdof) modal analysis methods. the principle upon which it is based is effectively a linearisation whereby it is assumed that at given amplitude of displacement response the system responds at the same frequency as the excitation and that stiffness and damping are constants. in so doing, by extracting this information at different amplitudes of vibration response, it is possible to estimate the amplitude-dependent natural frequency and modal loss factor. because of its mathematical simplicity and practical implementation during standard vibration testing, this method is particularly suitable for practical applications. in this paper, the method is illustrated and new analyses are carried out to validate its performance on numerical simulations before applying it to data measured on a complex aerospace test structure as well as a full-scale helicopter.",
    "present_kp": [],
    "absent_kp": [
      "nonlinear identification",
      "nonlinear modal testing",
      "nonlinear modal analysis",
      "experimental",
      "nonlinear modal analysis"
    ]
  },
  {
    "title": "on the formal specification and verification of multi-agent systems.",
    "abstract": "this article describes first steps towards the formal specification and verification of multiagent systems, through the use of temporal belief logics. the article first describes concurrent metatem, a multi-agent programming language, and then develops a logic that may be used to reason about concurrent metatem systems. the utility of this logic for specifying and verifying concurrent metatem systems is demonstrated through a number of examples. the article concludes with a brief discussion on the wider implications of the work, and in particular on the use of similar logics for reasoning about multi-agent systems in general.",
    "present_kp": [
      "formal specification and verification",
      "multi-agent systems"
    ],
    "absent_kp": []
  },
  {
    "title": "a contribution to multimedia document modeling and querying.",
    "abstract": "metadata on multimedia documents may help to describe their content and make their processing easier, for example by identifying events in temporal media, as well as carrying descriptive information for the overall resource. metadata is essentially static and may be associated with, or embedded in, the multimedia contents. the aim of this paper is to present a proposal for multimedia documents annotation, based on modeling and unifying features elicited from content and structure mining. our approach relies on the availability of annotated metadata representing segment content and structure as well as segment transcripts. temporal and spatial operators are also taken into account when annotating documents. any feature is identified into a descriptor called \"meta-document\". these meta-documents are the basis of querying by adapted query languages.",
    "present_kp": [
      "metadata",
      "annotation",
      "querying"
    ],
    "absent_kp": [
      "spatiotemporal operators"
    ]
  },
  {
    "title": "a comparison of pressure and tilt input techniques for cursor control.",
    "abstract": "three experiments were conducted in this study to investigate the human ability to control pen pressure and pen tilt input, by coupling this control with cursor position, angle and scale. comparisons between pen pressure input and pen tilt input have been made in the three experiments. experimental results show that decreasing pressure input resulted in very poor performance and was not a good input technique for any of the three experiments. in \"experiment 1-coupling to cursor position\", the tilt input technique performed relatively better than the increasing pressure input technique in terms of time. even though the tilt technique had a slightly higher error rate. in \"experiment 2-coupling to cursor angle\", the tilt input performed a little better than the increasing pressure input in terms of time, but the gap between them is not so apparent as experiment 1. in \"experiment 3-coupling to cursor scale\", tilt input performed a little better than increasing pressure input in terms of adjustment time. based on the results of our experiments, we have inferred several design implications and guidelines.",
    "present_kp": [
      "pressure input",
      "tilt input"
    ],
    "absent_kp": [
      "target selection tasks",
      "pen-based interfaces"
    ]
  },
  {
    "title": "a framework for analyzing the cognitive complexity of computer-assisted clinical ordering.",
    "abstract": "computer-assisted provider order entry is a technology that is designed to expedite medical ordering and to reduce the frequency of preventable errors. this paper presents a multifaceted cognitive methodology for the characterization of cognitive demands of a medical information system. our investigation was informed by the distributed resources (dr) model, a novel approach designed to describe the dimensions of user interfaces that introduce unnecessary cognitive complexity. this method evaluates the relative distribution of external (system) and internal (user) representations embodied in system interaction. we conducted an expert walkthrough evaluation of a commercial order entry system, followed by a simulated clinical ordering task performed by seven clinicians. the dr model was employed to explain variation in user performance and to characterize the relationship of resource distribution and ordering errors. the analysis revealed that the configuration of resources in this ordering application placed unnecessarily heavy cognitive demands on the user, especially on those who lacked a robust conceptual model of the system. the resources model also provided some insight into clinicians interactive strategies and patterns of associated errors. implications for user training and interface design based on the principles of humancomputer interaction in the medical domain are discussed.",
    "present_kp": [
      "provider order entry"
    ],
    "absent_kp": [
      "medical errors",
      "cognitive evaluation",
      "distributed cognition",
      "information systems"
    ]
  },
  {
    "title": "crack propagation analysis in composite materials by using moving mesh and multiscale techniques.",
    "abstract": "a novel multiscale method for crack propagation analysis in composites is proposed. an adaptive model refinement is used during crack propagation to improve efficiency. competition between different damage mechanisms is handled during crack simulation. matrix cracking is modeled by a novel optimization strategy based on moving meshes. the proposed approach is validated by original comparisons with existing methods.",
    "present_kp": [
      "composite materials",
      "crack propagation",
      "moving mesh"
    ],
    "absent_kp": [
      "concurrent multiscale methods",
      "micromechanics",
      "interface debonding"
    ]
  },
  {
    "title": "motivations in virtual health communities and their relationship to community, connectedness and stress.",
    "abstract": "this study explores the relationships between motivations for joining virtual health communities, online behaviors, and psycho-social outcomes. a sample of 144 women from two virtual health communities focusing on infertility completed survey measures assessing motivations, posting and receiving support, connectedness, community, and stress. our results indicate that socio-emotional support motivations for joining the community were associated with posting support within the virtual community, while informational motivations were related to receiving support. further, receiving support was associated with greater sense of virtual community as well as more general feelings of connectedness, which was related to less stress. implications for virtual health community research are discussed.",
    "present_kp": [
      "virtual health communities",
      "motivations",
      "connectedness",
      "stress",
      "sense of virtual community",
      "infertility"
    ],
    "absent_kp": []
  },
  {
    "title": "enterprise resource planning: implementation procedures and critical success factors.",
    "abstract": "enterprise resource planning (erp) systems are highly complex information systems. the implementation of these systems is a difficult and high cost proposition that places tremendous demands on corporate time and resources. many erp implementations have been classified as failures because they did not achieve predetermined corporate goals. this article identifies success factors, software selection steps, and implementation procedures critical to a successful implementation. a case study of a largely successful erp implementation is presented and discussed in terms of these key factors.",
    "present_kp": [
      "enterprise resource planning",
      "critical success factors",
      "implementation procedures"
    ],
    "absent_kp": [
      "business process reengineering",
      "project management"
    ]
  },
  {
    "title": "dflowz: a free program to evaluate the area potentially inundated by a debris flow.",
    "abstract": "debris flow inundated area can be estimated using scaling relationships. we provide a free, open-source program to evaluate debris flow hazard. the model considers the uncertainties in scaling relationships and input data. a graphical user interface facilitate the process of susceptibility mapping.",
    "present_kp": [
      "debris flow",
      "scaling relationships"
    ],
    "absent_kp": [
      "flooding",
      "hazard assessment"
    ]
  },
  {
    "title": "a novel approach to identify optimal access point and capacity of multiple dgs in a small, medium and large scale radial distribution systems.",
    "abstract": "distributed generation (dg) sources are predicated to play major role in distribution systems due to the demand growth for electrical energy. location and sizing of dg sources found to be important on the system losses and voltage stability in a distribution network. in this paper an efficient technique is presented for optimal placement and sizing of dgs in a large scale radial distribution system. the main objective is to minimize network power losses and to improve the voltage stability. a detailed performance analysis is carried out on 33-bus, 69-bus and 118-bus large scale radial distribution systems to demonstrate the effectiveness of the proposed technique. performing multiple power flow analysis on 118-bus system, the effect of dg sources on the most sensitive buses to voltage collapse is also carried out.",
    "present_kp": [
      "distributed generation",
      "large scale radial distribution system",
      "voltage"
    ],
    "absent_kp": [
      "simulated annealing",
      "stability index"
    ]
  },
  {
    "title": "robust discrete control of nonlinear processes: application to chemical reactors.",
    "abstract": "trajectory tracking or rejecting persistent disturbances with digital controllers in nonlinear processes is a class of problems where classical control methods breakdown since it is very difficult to describe the dynamic behavior over the entire trajectory. in this paper, a model-based robust control scheme is proposed as a potential solution approach for these systems. the proposed control algorithm is a robust error feedback controller that allows us to track predetermined operation profiles while attenuating the disturbances and maintaining the stability conditions of the nonlinear processes. various numerical simulation examples demonstrate the effectiveness of this robust scheme. two examples deal with effective trajectory tracking in chemical reactors over a wide range of operating conditions. the third example analyses the attenuation of periodic load in a biological reactor. all examples illustrate the ability of the robust control scheme to provide good control in the face of parameter uncertainties and load disturbances.",
    "present_kp": [
      "robust discrete control"
    ],
    "absent_kp": [
      "tracking control",
      "reactors control"
    ]
  },
  {
    "title": "wavelet-based statistical approach for speckle reduction in medical ultrasound images.",
    "abstract": "a novel speckle-reduction method is introduced, based on soft thresholding of the wavelet coefficients of a logarithmically transformed medical ultrasound image. the method is based on the generalised gaussian distributed (ggd) modelling of sub-band coefficients. the method used was a variant of the recently published bayesshrink method by chang and vetterli, derived in the bayesian framework for denoising natural images. it was scale adaptive, because the parameters required for estimating the threshold depend on scale and sub-band data. the threshold was computed by ksigma(2)/sigma(x), where sigma and sigma(x) were the standard deviation of the noise and the sub-band data of the noise-free image, respectively, and k was a scale parameter. experimental results showed that the proposed method outperformed the median filter and the homomorphic wiener filter by 29% in terms of the coefficient of correlation and 4% in terms of the edge preservation parameter. the numerical values of these quantitative parameters indicated the good feature preservation performance of the algorithm, as desired for better diagnosis in medical image processing.",
    "present_kp": [
      "speckle reduction",
      "soft thresholding",
      "wiener filter",
      "median filter",
      "bayesshrink"
    ],
    "absent_kp": [
      "discrete wavelet transform"
    ]
  },
  {
    "title": "computer-based imaging and interventional mri: applications for neurosurgery.",
    "abstract": "advances in computer technology and the development of open mri systems definitely enhanced intraoperative image-guidance in neurosurgery. based upon the integration of previously acquired and processed 3d information and the corresponding anatomy of the patient, this requires computerized image-processing methods (segmentation, registration, and display) and fast image integration techniques. open mr systems equipped with instrument tracking systems, provide an interactive environment in which biopsies and minimally invasive interventions or open surgeries can be performed. enhanced by the integration of multimodal imaging these techniques significantly improve the available treatment options and can change the prognosis for patients with surgically treatable diseases.",
    "present_kp": [
      "interventional mri",
      "image-guidance",
      "neurosurgery"
    ],
    "absent_kp": [
      "minimally invasive therapy",
      "surgical planning"
    ]
  },
  {
    "title": "dynamic response of a pile embedded in a porous medium subjected to plane sh waves.",
    "abstract": "in this paper, the frequency domain dynamic response of a pile embedded in a porous medium subjected to sh seismic waves is investigated. the surrounding porous medium of the pile is described by biots poro-elastic theory, while the pile embedded in the porous medium is treated as a beam and described by a beam vibration theory. using the hankel transformation method, the fundamental solution for a half-space porous medium subjected to a horizontal circular patch load is established. according to the fictitious pile methodology, the second kind of fredholm integral equation for the pile is established in terms of the obtained fundamental solution and free wave field. the solution of the integral equation yields the dynamic response of the pile to plane sh waves. numerical results indicate that the parameters of the porous medium, the pile and incident waves have considerable influences on the dynamic response of the pile and the porous medium.",
    "present_kp": [
      "pile",
      "sh waves",
      "fredholm integral equation"
    ],
    "absent_kp": [
      "biots theory",
      "porous media"
    ]
  },
  {
    "title": "towards computational models of animal cognition, an introduction for computer scientists.",
    "abstract": "the last few years of the twentieth century witnessed the emerging convergence of biology and computer science and this trend has been accelerating since then. the study of animal behavior or behavior biology has been one of the major contributors for this convergence. behavior is fascinating because it is the response of an organism to internal and external signals and it is controlled by complex interactions among nerves, the sensory and the motor systems. to some extent, behavior is similar to the output (or response) of a computer system or a network node if we consider an animal brain as a computer node. this paper is the first in a two-part series in which i review the state-of-the-art research in behavior biology inspired computing and communication, with the first part focusing on animal cognition and the second part on animal communication ( ma, 2014). the present article also assumes the task of presenting a general introduction on behavior biology literature, which sets a foundation for synthesizing both parts of the series but the synthesis will be performed in the second part of the series. i sets three objectives in this cognition part: (i) to present a brief overview on the literature of behavior biology for computer scientists; (ii) to summarize the state-of-the-art studies in several cognitive aspects of animal behavior: focusing on emerging research in cognitive ecology, social learning and innovation, as well as animal logics; (iii) to review some important existing studies inspired by animal behavior and further present a perspective on the future research. these cognition-related topics offer insights for research fields such as machine learning, human computer interactions (hci), brain computer interfaces (bcis), evolutionary computing, pervasive computing, etc. in perspective, i suggest that the interaction between behavioral biology and computer science should be bidirectional, and a new subject, behavioral informatics, or more general computational behavior biology, should be developed by the cooperative efforts between biologists and computer scientists.",
    "present_kp": [
      "animal cognition",
      "cognitive ecology",
      "social learning",
      "behavioral informatics",
      "computational behavior biology"
    ],
    "absent_kp": [
      "bioinspired computing and communication"
    ]
  }
]