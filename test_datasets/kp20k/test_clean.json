[
  {
    "title": "a feedback vertex set of 2-degenerate graphs.",
    "abstract": "a feedback vertex set of a graph g is a set s of its vertices such that the subgraph induced by v(g)?s is a forest. the cardinality of a minimum feedback vertex set of g is denoted by ?(g). a graph g is 2-degenerate if each subgraph g? of g has a vertex v such that dg?(v)?2. in this paper, we prove that ?(g)?2n/5 for any 2-degenerate n-vertex graph g and moreover, we show that this bound is tight. as a consequence, we derive a polynomial time algorithm, which for a given 2-degenerate n-vertex graph returns its feedback vertex set of cardinality at most 2n/5.",
    "present_kp": [
      "feedback vertex set",
      "2-degenerate graphs"
    ],
    "absent_kp": [
      "decycling set"
    ]
  },
  {
    "title": "hybrid analytical modeling of pending cache hits, data prefetching, and mshrs.",
    "abstract": "this article proposes techniques to predict the performance impact of pending cache hits, hardware prefetching, and miss status holding register resources on superscalar microprocessors using hybrid analytical models. the proposed models focus on timeliness of pending hits and prefetches and account for a limited number of mshrs. they improve modeling accuracy of pending hits by 3.9x and when modeling data prefetching, a limited number of mshrs, or both, these techniques result in average errors of 9.5% to 17.8%. the impact of non-uniform dram memory latency is shown to be approximated well by using a moving average of memory access latency.",
    "present_kp": [
      "performance",
      "analytical modeling",
      "pending hit",
      "data prefetching",
      "miss status holding register"
    ],
    "absent_kp": []
  },
  {
    "title": "autoimmune polyendocrinopathy candidiasis ectodermal dystrophy: known and novel aspects of the syndrome.",
    "abstract": "autoimmune polyendocrinopathy candidiasis ectodermal dystrophy (apeced) is a monogenic autosomal recessive disease caused by mutations in the autoimmune regulator (aire) gene and, as a syndrome, is characterized by chronic mucocutaneous candidiasis and the presentation of various autoimmune diseases. during the last decade, research on apeced and aire has provided immunologists with several invaluable lessons regarding tolerance and autoimmunity. this review describes the clinical and immunological features of apeced and discusses emerging alternative models to explain the pathogenesis of the disease.",
    "present_kp": [
      "apeced",
      "aire",
      "chronic mucocutaneous candidiasis"
    ],
    "absent_kp": [
      "il-17",
      "il-22"
    ]
  },
  {
    "title": "numerical solution of a three-dimensional solidification problem in aluminium casting.",
    "abstract": "in this paper, we consider an enthalpy formulation for a two-phase stefan problem arising from the solidification of aluminium during casting process. we solve this free boundary problem in a time varying three-dimensional domain and consider convective heat transfer in the liquid phase. the resulting equations are discretized using a characteristics method in time and a finite element method in space, and we propose a numerical algorithm to solve the obtained nonlinear discretized problem. finally, numerical results are given which are compared with industrial experimental measurements.",
    "present_kp": [
      "casting",
      "finite element"
    ],
    "absent_kp": [
      "thermal",
      "conduction",
      "convection"
    ]
  },
  {
    "title": "definition and recognition of rib features in aircraft structural part.",
    "abstract": "in this research, a new type of manufacturing feature that is commonly observed in aircraft structural parts, known as ribs, is defined and implemented using the object-oriented software engineering approach. the rib feature type is defined as a set of constrained and adjacent faces of a part which are associated with a set of specific rib machining operations. computerized numerical control (cnc) operation experience and the machining knowledge are leveraged by analysing typical geometry interactions when generating machining tool paths where such knowledge and experience are abstracted as rules of process planning. then those abstracted machining process rules are implemented in a feature recognition algorithm on top of an existing and holistic attribute adjacency graph solution to extract seed faces, identify individual local rib elements and further cluster these newly identified local rib elements into groups for the ease of machining operations. out of the potentially different combinations of local rib elements, those optimised cluster groups are merged into the top-level rib features. the enhanced recognition algorithm is presented in details. a pilot system has already been developed and applied for machining many advanced aircraft structural parts in a large aircraft manufacturer. observations and conclusions are presented at the end.",
    "present_kp": [
      "feature recognition",
      "rib",
      "aircraft structural part"
    ],
    "absent_kp": [
      "machining feature"
    ]
  },
  {
    "title": "an algebraic approach to guarantee harmonic balance method using grobner base.",
    "abstract": "harmonic balance (hb) method is well known principle for analyzing periodic oscillations on nonlinear networks and systems. because the hb method has a truncation error, approximated solutions have been guaranteed by error bounds. however, its numerical computation is very time-consuming compared with solving the hb equation. this paper proposes an algebraic representation of the error bound using grobner base. the algebraic representation enables to decrease the computational cost of the error bound considerably. moreover, using singular points of the algebraic representation, we can obtain accurate break points of the error bound by collisions.",
    "present_kp": [
      "harmonic balance method",
      "error bound",
      "grobner base",
      "algebraic representation",
      "singular point"
    ],
    "absent_kp": [
      "quadratic approximation"
    ]
  },
  {
    "title": "a graph coloring based tdma scheduling algorithm for wireless sensor networks.",
    "abstract": "wireless sensor networks should provide with valuable service, which is called service-oriented requirement. to meet this need, a novel distributed graph coloring based time division multiple access scheduling algorithm (gcsa), considering real-time performance for clustering-based sensor network, is proposed in this paper, to determine the smallest length of conflict-free assignment of timeslots for intra-cluster transmissions. gcsa involves two phases. in coloring phase, networks are modeled using graph theory, and a distributed vertex coloring algorithm, which is a distance-2 coloring algorithm and can get colors near to ((updelta +1)), is proposed to assign a color to each node in the network. then, in scheduling phase, each independent set is mapped to a unique timeslot according to the sets priority which is obtained by considering network structure. the experimental results indicate that gcsa can significantly decrease intra-cluster delay and increase intra-cluster throughput, which satisfies real-time performance as well as communication reliability.",
    "present_kp": [
      "tdma",
      "distributed",
      "graph coloring",
      "clustering",
      "real-time"
    ],
    "absent_kp": []
  },
  {
    "title": "building model as a service to support geosciences.",
    "abstract": "model as a service (maas) concept and architecture is introduced to support geoscience modeling. maas enables various geoscience models to be published as services that can be accessed through a simple web interface. maas automates the processes of configuring machines, setting up and running models, and managing model outputs. maas provides new guidance for geoscientists seeking solutions to address the computing demands for geoscience models.",
    "present_kp": [],
    "absent_kp": [
      "cloud computing",
      "web service",
      "geospatial data",
      "model web",
      "earthcube",
      "big data"
    ]
  },
  {
    "title": "shot change detection using scene-based constraint.",
    "abstract": "a key step for managing a large video database is to partition the video sequences into shots. past approaches to this problem tend to confuse gradual shot changes with changes caused by smooth camera motions. this is in part due to the fact that camera motion has not been dealt with in a more fundamental way. we propose an approach that is based on a physical constraint used in optical flow analysis, namely, the total brightness of a scene point across two frames should remain constant if the change across two frames is a result of smooth camera motion. since the brightness constraint would be violated across a shot change, the detection can be based on detecting the violation of this constraint. it is robust because it uses only the qualitative aspect of the brightness constraint-detecting a scene change rather than estimating the scene itself. moreover, by tapping on the significant know-how in using this constraint, the algorithm's robustness is further enhanced. experimental results are presented to demonstrate the performance of various algorithms. it was shown that our algorithm is less likely to interpret gradual camera motions as shot changes, resulting in a significantly better precision performance than most other algorithms.",
    "present_kp": [
      "shot change detection",
      "optical flow"
    ],
    "absent_kp": [
      "video segmentation"
    ]
  },
  {
    "title": "tail asymptotics for hol priority queues handling a large number of independent stationary sources.",
    "abstract": "in this paper we study the asymptotics of the tail of the buffer occupancy distribution in buffers accessed by a large number of stationary independent sources and which are served according to a strict hol priority rule. as in the case of single buffers, the results are valid for a very general class of sources which include long-range dependent sources with bounded instantaneous rates. we first consider the case of two buffers with one of them having strict priority over the other and we obtain asymptotic upper bound for the buffer tail probability for lower priority buffer. we discuss the conditions to have asymptotic equivalents. the asymptotics are studied in terms of a scaling parameter which reflects the server speed, buffer level and the number of sources in such a way that the ratios remain constant. the results are then generalized to the case of m buffers which leads to the source pooling idea. we conclude with numerical validation of our formulae against simulations which show that the asymptotic bounds are tight. we also show that the commonly suggested reduced service rate approximation can give extremely low estimates.",
    "present_kp": [
      "priority queues"
    ],
    "absent_kp": [
      "bahadur-rao theorem",
      "large deviations",
      "cell loss",
      "stationary processes",
      "tail distributions"
    ]
  },
  {
    "title": "a variant of parallel plane sweep algorithm for multicore systems.",
    "abstract": "parallel algorithms used in very large scale integration physical design bring significant challenges for their efficient and effective design and implementation. the rectangle intersection problem is a subset of the plane sweep problem, a topic of computational geometry and a component in design rule checking, parasitic resistance-capacitance extraction, and mask processing flows. a variant of a plane sweep algorithm that is embarrassingly parallel and therefore easily scalable on multicore machines and clusters, while exceeding the best-known parallel plane sweep algorithms on real-world tests, is presented in this letter.",
    "present_kp": [
      "computational geometry",
      "multicore",
      "physical design",
      "plane sweep",
      "rectangle intersection"
    ],
    "absent_kp": []
  },
  {
    "title": "the antecedents and consequents of user perceptions in information technology adoption.",
    "abstract": "a common theme underlying various models that explain information technology adoption is the inclusion of perceptions of an innovation as key independent variables. although a fairly significant body of research that empirically tests these models is now in existence, some questions with regard to both the antecedents as well as the consequents of perceptions remain unanswered. this paper reports the results of a field study examining adoption of an information technology innovation represented by an expert systems application. two research objectives that have both theoretical and practical relevance motivated and guided the study. one, the study challenges an assumption which is implicit in technology acceptance models: that of the non-existence of moderating influences on the relationship between perceptions and adoption decisions. specifically, the study examines the effects of an important moderating influence personal innovativeness on this relationship. two, the study seeks to shed further light on the determinants of perceptions by examining the relative efficacy of mass media and interpersonal communication channels in facilitating perception development. theoretical and practical implications that follow from the results are discussed.",
    "present_kp": [
      "information technology adoption",
      "personal innovativeness",
      "communication channels"
    ],
    "absent_kp": [
      "expert system adoption"
    ]
  },
  {
    "title": "improving classification with latent variable models by sequential constraint optimization.",
    "abstract": "in this paper we propose a method to use multiple generative models with latent variables for classification tasks. the standard approach to use generative models for classification is to train a separate model for each class. a novel data point is then classified by the model that attributes the highest probability. the algorithm we propose modifies the parameters of the models to improve the classification accuracy. our approach is made computationally tractable by assuming that each of the models is deterministic, by which we mean that a data-point is associated to only a single latent state. the resulting algorithm is a variant of the support vector machine learning algorithm and in a limiting case the method is similar to the standard perceptron learning algorithm. we apply the method to two types of latent variable models. the first has a discrete latent state space and the second, principal component analysis, has a continuous latent state space. we compare the effectiveness of both approaches on a handwritten digit recognition problem and on a satellite image recognition problem.",
    "present_kp": [
      "latent variable models"
    ],
    "absent_kp": [
      "semi-supervised learning",
      "support vector machines",
      "pca",
      "vector quantization",
      "image and character recognition"
    ]
  },
  {
    "title": "a framework for a real time intelligent and interactive brain computer interface.",
    "abstract": "a framework for a real time implementation of a brain computer interface. implementation & comparison of different feature extraction methods and classifiers. accuracy & processing time comparison for detection of event related potentials-erp. an implementation of a prototype system using the proposed bci framework. real time eeg data collection and classification of erps using hex-o-speller.",
    "present_kp": [
      "data collection",
      "classification"
    ],
    "absent_kp": [
      "electroencephalography ",
      "braincomputer interface ",
      "event-related potentials "
    ]
  },
  {
    "title": "characterizing output processes of e-m/e-k/1 queues.",
    "abstract": "our goal is to study which conditions of the output process of a queue preserve the increasing failure rate (ifr) property in the interdeparture time. we found that the interdeparture time does not always preserve the ifr property, even if the interarrival time and service time are both erlang distributions with ifr. we give a theoretical analysis and present numerical results of e-m/e-k/1 queues. we show, by numerical examples, that the interdeparture time of e-m/e-k/1 retains the ifr property if m >= k.",
    "present_kp": [
      "ifr",
      "erlang distribution"
    ],
    "absent_kp": [
      "departure process",
      "ph/g/1",
      "queueing theory"
    ]
  },
  {
    "title": "a low-complexity down-mixing structure on quadraphonic headsets for surround audio.",
    "abstract": "this work presents a four-channel headset achieving a 5.1-channel-like hearing experience using a low-complexity head-related transfer function (hrtf) model and a simplified reverberator. the proposed down-mixing architecture enhances the sound localization capability of a headset using the hrtf and by simulating multiple sound reflections in a room using moorer's reverberator. since the hrtf has large memory and computation requirements, the common-acoustical-pole and zero (capz) model can be used to reshape the lower-order hrtf model. from a power consumption viewpoint, the capz model reduces computation complexity by approximately 40%. the subjective listening tests in this study shows that the proposed four-channel headset performs much better than stereo headphones. on the other hand, the four-channel headset that can be implemented by off-the-shelf components preserves the privacy with low cost.",
    "present_kp": [
      "surround audio",
      "head-related transfer function"
    ],
    "absent_kp": [
      "virtual loudspeaker",
      "reverberation"
    ]
  },
  {
    "title": "security personalization for internet and web services.",
    "abstract": "the growth of the internet has been accompanied by the growth of internet services (e.g., e-commerce, e-health). this proliferation of services and the increasing attacks on them by malicious individuals have highlighted the need for service security. the security requirements of an internet or pleb service may be specified in a security policy. the provider of the service is then responsible.,for implementing the security measures contained in the policy. however, a service customer or consumer may have security preferences that are not reflected in the provider security policy in order for set-vice providers to attract and retain customers, as well as reach a wider market, a way of personalizing a security policy to a particular customer is needed we derive the content of an internet or web service security policy and propose a flexible security personalization approach that will allow an internet or web service provider and customer to negotiate to an agreed-upon personalized security policy. in addition, we present two application examples of security policy personalization, and overview the design of our security personalization prototype.",
    "present_kp": [
      "internet services",
      "personalization",
      "security",
      "security policy",
      "web services"
    ],
    "absent_kp": [
      "negotiation"
    ]
  },
  {
    "title": "two efficient synchronous double left right arrow asynchronous converters well-suited for networks-on-chip in gals architectures.",
    "abstract": "this paper presents two high-throughput, low-latency converters that can be used to convert synchronous communication protocol to asynchronous one and vice versa. we have designed these two hardware components to be used in a globally asynchronous locally synchronous clusterized multi-processor system-on-chip communicating by a fully asynchronous network-on-chip. the proposed architecture is rather generic, and allows the system designer to make various trade-offs between latency and robustness, depending on the selected synchronizer. we have physically implemented the two converters with portable alliance cmos standard cell library and evaluated the architectures by spice simulation for a 90 nm cmos fabrication process.",
    "present_kp": [
      "globally asynchronous locally synchronous",
      "networks-on-chip"
    ],
    "absent_kp": [
      "multi-processor systems-on-chip",
      "synchronization",
      "asynchronous fifo"
    ]
  },
  {
    "title": "simulation aided design of organizational structures in manufacturing systems using structuring strategies.",
    "abstract": "this paper presents a simulation aided approach for designing organizational structures in manufacturing systems. the approach is based on a detailed modeling and characterization of the forecasted order program, especially of elementary processes, activity networks and manufacturing orders. under the use of the organization modeling system form, that has been developed at the ifab-institute of human and industrial engineering of the university of karlsruhe, structuring strategies-e.g., a process-oriented strategy can be applied in order to design organizational structures in manufacturing systems in a flexible and efficient way. following that, a dynamical analysis of the created manufacturing structures can be carried out with the simulation tool femos, that has also been developed at the ifab-institute. the evaluation module of femos enables to measure the designed solutions with the help of logistical-e.g., lead time degree and organizational-e.g., degree of autonomy key data. this evaluation is the basis for the identification of effective manufacturing systems and also of improvement potentialities. finally, a case study is presented in this paper designing and analyzing different organizational structures of a manufacturing system where gear boxes and robot grip arms were manufactured.",
    "present_kp": [],
    "absent_kp": [
      "modeling and simulation of manufacturing systems",
      "strategies for production systems design"
    ]
  },
  {
    "title": "explicit constructions of selectors and related combinatorial structures, with applications.",
    "abstract": "in this paper we present explicit constructions of several combinatorial objects: selectors [cgr00] and selective families [cggpr00], pseudo-random generators for proof systems [abrw00] and fixed waking schedules [gpp00]. as a result, we obtain almost optimal deterministic protocols for broadcasting in unknown directed radio networks [cgr00] and wake-up problem [gpp00]. we also show application of selectors (and its variants) to explicit construction of test sets for coin-weighting problems [dh00]. the parameters of our constructions come close to the best known non-constructive bounds. the constructions are achieved using a common technique, which could be of use for other problems.",
    "present_kp": [
      "object",
      "direct",
      "applications",
      "use",
      "paper",
      "test"
    ],
    "absent_kp": [
      "optimality",
      "randomization"
    ]
  },
  {
    "title": "selective finite element refinement in torsional problems based on the membrane analogy.",
    "abstract": "this work presents a selective finite element refinement strategy based on the h-refinement type, in the context of a posteriori error estimates considerations (error computed after the application of the proposed refining scheme), based on a graphical procedure to determine progressively better estimates for the maximum shearing stress in prismatic torsional members. it is structured in an integrated fortran code and delphi based environment to refine an initial arbitrary finite element mesh. the proposed procedure is founded on the membrane analogy that exists between membrane deflections and the torsion problem in the sense that the location of the membrane largest gradient drives the refining procedure. it is shown that multiple level application of the proposed method to two members with different cross sectional geometries with known analytic solutions leads to progressively more accurate estimates (< 1.0% error in most cases) for the maximum shearing stresses calculations. finally, the proposed method is applied to the torsional analysis of an l section member, showing that for this practical case the procedure results in a very accurate calculation as well.",
    "present_kp": [
      "membrane analogy",
      "torsion",
      "maximum shearing stress"
    ],
    "absent_kp": [
      "selective h-refinement",
      "finite elements"
    ]
  },
  {
    "title": "rns montgomery multiplication algorithm for duplicate processing of base transformations.",
    "abstract": "this paper proposes a new algorithm to achieve about two-times speedup of modular exponentiation which is implemented by montgomery multiplication based on residue number systems (rns). in rns montgomery multiplication, its performance is determined by two base transformations dominantly. for the purpose of realizing parallel processing of these base transformations, i.e. \"duplicate processing,\" we present two procedures of rns montgomery multiplication, in which rns bases a and b are interchanged, and perform them alternately in modular exponentiation iteration. in an investigation of implementation, 1.87-times speedup has been obtained for 1024-bit modular multiplication. the proposed rns montgomery multiplication algorithm has an advantage in achieving the performance corresponding to that the upper limit of the number of parallel processing units is doubled.",
    "present_kp": [
      "modular exponentiation",
      "residue number systems",
      "montgomery multiplication",
      "base transformation"
    ],
    "absent_kp": [
      "rsa cryptography"
    ]
  },
  {
    "title": "from quality in use to value in the world.",
    "abstract": "this paper argues that a focus on quality in use limits the potential of hci. it summarizes how novel approaches such as grounded design can let us go beyond usability to reveal the fit between designs and expected contexts of use. this however is still not enough. it cannot resolve dilemmas about what is and is not a usability problem, or when fit is or is not essential. such dilemmas can only be resolved by an understanding of the value that artifacts aim to deliver in the world. hci must move beyond contextual description to prescriptive approaches to value in the world.",
    "present_kp": [
      "value",
      "fit",
      "quality",
      "design"
    ],
    "absent_kp": []
  },
  {
    "title": "comparative study of family 2 gpcrs in fugu rubripes.",
    "abstract": "abstract: in this study, members of family 2 gpcrs, one of the largest families of receptors in vertebrates, were isolated and characterized in the genome of the japanese pufferfish, fugu rubripes, and compared with the orthologous genes in other vertebrates. phylogenetic analysis carried out with all vertebrate family 2 gpcr members indicated that calr/cgrpr and crf are the most divergent receptor group within this family and that the remaining members appear to originate from a common ancestral gene precursor.",
    "present_kp": [
      "family 2 gpcrs"
    ],
    "absent_kp": [
      "teleost",
      "duplicated genes",
      "evolution"
    ]
  },
  {
    "title": "blotto game-based low-complexity fair multiuser subcarrier allocation for uplink ofdma networks.",
    "abstract": "this article presents a subcarrier allocation scheme based on a blotto game (sabg) for orthogonal frequency-division multiple access (ofdma) networks where correlation between adjacent subcarriers is considered. in the proposed game, users simultaneously compete for subcarriers using a limited budget. in order to win as many good subcarriers as possible in this game, users are required to wisely allocate their budget. efficient power and budget allocation strategies are derived for users for obtaining optimal throughput. by manipulating the total budget available for each user, competitive fairness can be enforced for the sabg. in addition, the conditions to ensure the existence and uniqueness of nash equilibrium (ne) for the sabg are also established. an low-complexity algorithm that ensures convergence to ne is proposed. simulation results show that the proposed low-complexity sabg can allocate resources fairly and efficiently for both uncorrelated and correlated fading channels.",
    "present_kp": [
      "ofdma",
      "subcarrier allocation",
      "blotto game",
      "fairness",
      "complexity",
      "correlated fading"
    ],
    "absent_kp": [
      "efficiency"
    ]
  },
  {
    "title": "polymerization conditions influence on the thermomechanical and dielectric properties of unsaturated polyesterstyrene-copolymers.",
    "abstract": "the influence of different polymerization conditions like curing agent (mekp) amount and styrene content on the glass transition temperature, the relative dielectric constant as well as loss factors of unsaturated polyesterstyrene-polymer systems after solidification was investigated in depth. with respect to a high average molecular mass and vickers hardness a curing agent content of 3wt% is recommendable. increasing mekp-concentrations cause a slight elevation of the polymers relative dielectric constant as well as of the loss factor. regarding an easy film formation using tape casting a.o. higher styrene amounts lower the viscosity of the resin significantly, the relative dielectric constant and the loss factor decrease also. as an average value a relative dielectric constant of 3 under ambient conditions can be obtained.",
    "present_kp": [
      "dielectric properties"
    ],
    "absent_kp": [
      "unsaturated polyester resin",
      "embedded capacitors"
    ]
  },
  {
    "title": "an integration of online and pseudo-online information for cursive word recognition.",
    "abstract": "in this paper, we present a novel method to extract stroke order independent information from online data. this information, which we term pseudo-online, conveys relevant information on the offline representation of the word. based on this information, a combination of classification decisions from online and pseudo- online cursive word recognizers is performed to improve the recognition of online cursive words. one of the most valuable aspects of this approach with respect to similar methods that combine online and offline classifiers for word recognition is that the pseudo- online representation is similar to the online signal and, hence, word recognition is based on a single engine. results demonstrate that the pseudo- online representation is useful as the combination of classifiers perform better than those based solely on pure online information.",
    "present_kp": [
      "online",
      "offline",
      "cursive",
      "word recognition"
    ],
    "absent_kp": [
      "handwriting",
      "classifier combination"
    ]
  },
  {
    "title": "generation of quasi-gaussian pulses based on correlation techniques.",
    "abstract": "the gaussian pulses have been mostly used within communications, where some applications can be emphasized: mobile telephony (gsm), where gmsk signals are used, as well as the uwb communications, where short-period pulses based on gaussian waveform are generated. since the gaussian function signifies a theoretical concept, which cannot be accomplished from the physical point of view, this should be expressed by using various functions, able to determine physical implementations. new techniques of generating the gaussian pulse responses of good precision are approached, proposed and researched in this paper. the second and third order derivatives with regard to the gaussian pulse response are accurately generated. the third order derivates is composed of four individual rectangular pulses of fixed amplitudes, being easily to be generated by standard techniques. in order to generate pulses able to satisfy the spectral mask requirements, an adequate filter is necessary to be applied. this paper emphasizes a comparative analysis based on the relative error and the energy spectra of the proposed pulses.",
    "present_kp": [
      "correlation techniques",
      "gaussian pulse"
    ],
    "absent_kp": [
      "digital signal processing",
      "spectral analysis",
      "ultra-wideband"
    ]
  },
  {
    "title": "learning linear pca with convex semi-definite programming.",
    "abstract": "the aim of this paper is to learn a linear principal component using the nature of support vector machines (svms). to this end, a complete svm-like framework of linear pca (svpca) for deciding the projection direction is constructed, where new expected risk and margin are introduced. within this framework, a new semi-definite programming problem for maximizing the margin is formulated and a new definition of support vectors is established. as a weighted case of regular pca, our svpca coincides with the regular pca if all the samples play the same part in data compression. theoretical explanation indicates that svpca is based on a margin-based generalization bound and thus good prediction ability is ensured. furthermore, the robust form of svpca with a interpretable parameter is achieved using the soft idea in svms. the great advantage lies in the fact that svpca is a learning algorithm without local minima because of the convexity of the semi-definite optimization problems. to validate the performance of svpca, several experiments are conducted and numerical results have demonstrated that their generalization ability is better than that of regular pca. finally, some existing problems are also discussed.",
    "present_kp": [
      "support vector machines",
      "margin",
      "semi-definite programming"
    ],
    "absent_kp": [
      "principal component analysis",
      "statistical learning theory",
      "maximal margin algorithm",
      "robustness"
    ]
  },
  {
    "title": "the neighborhood auditing tool: a hybrid interface for auditing the umls.",
    "abstract": "the umls's integration of more than 100 source vocabularies, not necessarily consistent with one another, causes some inconsistencies. the purpose of auditing the umls is to detect such inconsistencies and to suggest how to resolve them while observing the requirement of fully representing the content of each source in the umls. a software tool, called the neighborhood auditing tool (nat), that facilitates umls auditing is presented. the nat supports \"neighborhood-based\" auditing, where, at any given time, an auditor concentrates on a single-focus concept and one of a variety of neighborhoods of its closely related concepts. typical diagrammatic displays of concept networks have a number of shortcomings, so the nat utilizes a hybrid diagram/text interface that features stylized neighborhood views which retain some of the best features of both the diagrammatic layouts and text windows while avoiding the shortcomings. the nat allows an auditor to display knowledge from both the metathesaurus (concept) level and the semantic network (semantic type) level. various additional features of the nat that support the auditing process are described. the usefulness of the nat is demonstrated through a group of case studies. its impact is tested with a study involving a select group of auditors.",
    "present_kp": [
      "software tool",
      "auditing tool"
    ],
    "absent_kp": [
      "unified medical language system",
      "auditing of terminologies",
      "auditing of ontologies",
      "auditing of the umls",
      "user interface",
      "hybrid diagram/text user interface"
    ]
  },
  {
    "title": "exclusion regions for optimization problems.",
    "abstract": "branch and bound methods for finding all solutions of a global optimization problem in a box frequently have the difficulty that subboxes containing no solution cannot be easily eliminated if they are close to the global minimum. this has the effect that near each global minimum, and in the process of solving the problem also near the currently best found local minimum, many small boxes are created by repeated splitting, whose processing often dominates the total work spent on the global search. this paper discusses the reasons for the occurrence of this so-called cluster effect, and how to reduce the cluster effect by defining exclusion regions around each local minimum found, that are guaranteed to contain no other local minimum and hence can safely be discarded. in addition, we will introduce a method for verifying the existence of a feasible point close to an approximate local minimum. these exclusion regions are constructed using uniqueness tests based on the krawczyk operator and make use of first, second and third order information on the objective and constraint functions.",
    "present_kp": [
      "global optimization",
      "uniqueness test",
      "exclusion region",
      "branch and bound",
      "cluster effect",
      "krawczyk operator"
    ],
    "absent_kp": [
      "validated enclosure",
      "existence test",
      "inclusion region",
      "kantorovich theorem",
      "backboxing",
      "affine invariant",
      "primary "
    ]
  },
  {
    "title": "learning about meetings.",
    "abstract": "most people participate in meetings almost every day, multiple times a day. the study of meetings is important, but also challenging, as it requires an understanding of social signals and complex interpersonal dynamics. our aim in this work is to use a data-driven approach to the science of meetings. we provide tentative evidence that: (i) it is possible to automatically detect when during the meeting a key decision is taking place, from analyzing only the local dialogue acts, (ii) there are common patterns in the way social dialogue acts are interspersed throughout a meeting, (iii) at the time key decisions are made, the amount of time left in the meeting can be predicted from the amount of time that has passed, (iv) it is often possible to predict whether a proposal during a meeting will be accepted or rejected based entirely on the language (the set of persuasive words) used by the speaker.",
    "present_kp": [
      "persuasive words"
    ],
    "absent_kp": [
      "analysis of meetings",
      "applications of machine learning"
    ]
  },
  {
    "title": "design and implementation of an expert interface system for integration of photogrammetric and geographic information systems for intelligent preparation and structuring of spatial data.",
    "abstract": "preparation of spatial data for geographic information system (gis) simultaneously during feature digitizing process from photogrammetric models reduces data editing phases after feature digitizing process. therefore, the problems, caused by separating spatial data production process from preparation of this data, are overcome as far as possible. to achieve this purpose, specialty and expertise required for spatial data structuring and preparation for gis, should be available in an interface system which establishes a direct connection between photogrammetric and gis systems. in this case, when a user digitizes a feature from a photogrammetric model, decision making process about the method of editing, structuring, layering, and storing of the feature in gis database, can be carried out by such an interface system. thus, according to the capabilities of expert systems for modeling the knowledge and deduction methods of experts, generating an expert interface system between photogrammetric and gis systems, offers a suitable solution for this integration. in this paper, the capabilities of expert systems for intelligent spatial data structuring and preparation simultaneously during feature digitizing process from photogrammetric models, have been investigated. also, design, implementation and test of an expert interface system for integration of photogrammetric and gis systems in order to take advantages of capabilities of both systems simultaneously as one integrated system, has been described.",
    "present_kp": [
      "expert system",
      "gis",
      "integration",
      "spatial data"
    ],
    "absent_kp": [
      "photogrammetry"
    ]
  },
  {
    "title": "wevan a mechanism for evidence creation and verification in vanets.",
    "abstract": "there are traffic situations (e.g. incorrect speeding tickets) in which a given vehicles driving behavior at some point in time has to be proved to a third party. vehicle-mounted sensorial devices are not suitable for this matter since they can be maliciously manipulated. however, surrounding vehicles may give their vision on another ones behavior. furthermore, these data may be shared with the affected vehicle through vanets. in this paper, a vanet-enabled data exchange mechanism called wevan is presented. the goal of this mechanism is to build and verify evidences based on surrounding vehicles (called witnesses) testimonies. due to the short-range nature of vanets, the connectivity to witnesses may be reduced with time the later their testimonies are requested, the lower the amount of witnesses may be. simulation results show that if testimonies are ordered 5s later, an average of 38 testimonies may be collected in highway scenarios. other intervals and road settings are studied as well.",
    "present_kp": [
      "driving behavior",
      "witness"
    ],
    "absent_kp": [
      "digital evidence",
      "vehicular ad-hoc networks "
    ]
  },
  {
    "title": "extensional normalisation and type-directed partial evaluation for typed lambda calculus with sums.",
    "abstract": "we present a notion of eta-long beta-normal term for the typed lambda calculus with sums and prove, using grothendieck logical relations, that every term is equivalent to one in normal form. based on this development we give the first type-directed partial evaluator that constructs normal forms of terms in this calculus.",
    "present_kp": [
      "typed lambda calculus",
      "grothendieck logical relations",
      "normalisation",
      "type-directed partial evaluation"
    ],
    "absent_kp": [
      "strong sums"
    ]
  },
  {
    "title": "yet another write-optimized dbms layer for flash-based solid state storage.",
    "abstract": "flash-based solid state storage (flashsss) has write-oriented problems such as low write throughput, and limited life-time. especially, flashssds have a characteristic vulnerable to random-writes, due to its control logic utilizing parallelism between the flash memory chips. in this paper, we present a write-optimized layer of dbmss to address the write-oriented problems of flashsss in on-line transaction processing environments. the layer consists of a write-optimized buffer, a corresponding log space, and an in-memory mapping table, closely associated with a novel logging scheme called incremental logging (icl). the icl scheme enables dbmss to reduce page-writes at the least expense of additional page-reads, while replacing random-writes into sequential-writes. through experiments, our approach demonstrated up-to an order of magnitude performance enhancement in i/o processing time compared to the original dbms, increasing the longevity of flashsss by approximately a factor of two.",
    "present_kp": [
      "icl",
      "ssd",
      "incremental logging",
      "flash memory"
    ],
    "absent_kp": [
      "write performance",
      "database"
    ]
  },
  {
    "title": "wrinkle development analysis in thin sail-like structures using mitc shell finite elements.",
    "abstract": "we propose a method of modelling sail type structures which captures the wrinkling behaviour of such structures. the method is validated through experimental and analytical test cases, particularly in terms of wrinkling prediction. an enhanced wrinkling index is proposed as a valuable measure characterizing the global wrinkling development on the deformed structure. the method is based on a pseudo-dynamic finite element procedure involving non-linear mitc shell elements. the major advantage compared to membrane models generally used for this type of analysis is that no ad hoc wrinkling model is required to control the stability of the structure. we demonstrate our approach to analyse the behaviour of various structures with spherical and cylindrical shapes, characteristic of downwind sails over a rather wide range of shape and constitutive parameters. in all cases convergence is reached and the overall flying shape is most adequately represented, which shows that our approach is a most valuable alternative to standard techniques to provide deeper insight into the physical behaviour. limitations appear only in some very special instances in which local wrinkling-related instabilities are extremely high and would require specific additional treatments, out of the scope of the present study.",
    "present_kp": [
      "wrinkling",
      "wrinkling index"
    ],
    "absent_kp": [
      "sail modelling",
      "mitc shells"
    ]
  },
  {
    "title": "cliques, holes and the vertex coloring polytope.",
    "abstract": "certain subgraphs of a given graph g restrict the minimum number chi(g) of colors that can be assigned to the vertices of g such that the endpoints of all edges receive distinct colors. some of such subgraphs are related to the celebrated strong perfect graph theorem. as it implies that every graph g contains a clique of size chi(g), or an odd hole or an odd anti-hole as an induced subgraph. in this paper, we investigate the impact of induced maximal cliques, odd holes and odd anti-holes on the polytope associated with a new 0-1 integer programming formulation of the graph coloring problem. we show that they induce classes of facet defining, inequalities.",
    "present_kp": [
      "integer programming"
    ],
    "absent_kp": [
      "combinatorial problems",
      "facets of polyhedra",
      "graph colorings"
    ]
  },
  {
    "title": "an experimental validation of a novel clustering approach to pwarx identification.",
    "abstract": "in this paper, the problem of clustering based procedure for the identification of piecewise auto-regressive exogenous (pwarx) models is addressed. this problem involves both the estimation of the parameters of the affine sub-models and the hyperplanes defining the partitions of the state-input regression. in fact, we propose the use of the chiu's clustering algorithm in order to overcome the main drawbacks of the existing methods which are the poor initialization and the presence of outliers. in addition, our approach is able to generate automatically the number of sub-models. simulation results are presented to illustrate the performance of the proposed method. an application of the developed approach to an olive oil esterification reactor is also suggested in order to validate simulation results.",
    "present_kp": [
      "clustering",
      "identification",
      "experimental validation"
    ],
    "absent_kp": [
      "hybrid systems",
      "pwarx models",
      "chiu's clustering technique"
    ]
  },
  {
    "title": "a primal-dual approximation algorithm for the asymmetric prize-collecting tsp.",
    "abstract": "we present a primal-dual ?log(n)?-approximation algorithm for the version of the asymmetric prize collecting traveling salesman problem, where the objective is to find a directed tour that visits a subset of vertices such that the length of the tour plus the sum of penalties associated with vertices not in the tour is as small as possible. the previous algorithm for the problem (v.h. nguyen and t.t nguyen in int. j. math. oper. res. 4(3):294301, 2012) which is not combinatorial, is based on the held-karp relaxation and heuristic methods such as the frieze et al.s heuristic (frieze et al. in networks 12:2339, 1982) or the recent asadpour et al.s heuristic for the atsp (asadpour etal. in 21st acm-siam symposium on discrete algorithms, 2010). depending on which of the two heuristics is used, it gives respectively 1+?log(n)? and (3+ 8frac{log(n)}{log(log(n))}) as an approximation ratio. our algorithm achieves an approximation ratio of ?log(n)? which is weaker than (3+ 8frac{log(n)}{log(log(n))}) but represents the first combinatorial approximation algorithm for the asymmetric prize-collecting tsp.",
    "present_kp": [
      "prize collecting traveling salesman",
      "approximation algorithm"
    ],
    "absent_kp": [
      "primal-dual algorithm"
    ]
  },
  {
    "title": "second order ambient intelligence.",
    "abstract": "this text attempts to describe an imagined future of ambient intelligence. it assumes that one day most of the current issues within ambient intelligence will be solved and that a second order ambient intelligence will be formulated, one with new research agendas. it describes several topics and ideas that might be part of this agenda and surmises on the prerequisites for this change.",
    "present_kp": [
      "second order ambient intelligence"
    ],
    "absent_kp": [
      "critique of ambient intelligence",
      "temporal design",
      "adaptive systems",
      "long-term behavior",
      "animal machine interaction",
      "critical futurism"
    ]
  },
  {
    "title": "pdms prism-glass optical coupling for surface plasmon resonance sensors based on mems technology.",
    "abstract": "a miniaturized surface plasmon resonance (spr) chip has been developed for biomedical and chemical analysis with low cost and high performance. the techniques of bulk silicon micromachining and polymer replication were used to fabricate the kretschmann spr sensor composed of a polydimethylsiloxane (pdms) prism, a coupling glass and microchannels. the plasmon properties of thin metal films were investigated theoretically based on fresnel analysis, with optical boundary conditions pertaining to the surface plasmon resonance at the gold/water and gold/air interfaces. the theoretical results show that difference in the refractive index (ri) between the pdms prism and the coupling glass layer affect the precision of the spr angle and the spr curve. meanwhile, the period of the interference fringe attaching on the spr curve increases with an increase in wavelength and a decrease in the refractive index of the coupling glass layer. the gold thickness of 50 nm is required while employing a fixed incident wavelength of 650 nm, to achieve optimum spr excitation conditions and the sensor sensitivity. the characteristics of this spr sensor were evaluated in the angular interrogation mode of employing the incident wavelength of 650 nm in air and water media, respectively. the obtained spr angles were approximately consistent with the theoretical ones.",
    "present_kp": [
      "polymer",
      "pdms"
    ],
    "absent_kp": [
      "surface plasmon resolance",
      "microfluidic"
    ]
  },
  {
    "title": "isogeometric analysis for strain field measurements.",
    "abstract": "in this paper, the potential of isogeometric analysis for strain field measurement by digital image correlation is investigated. digital image correlation (dic) is a full field kinematics measurement technique based on gray level conservation principle and the formulation we adopt allows for using arbitrary displacement bases. the high continuity properties of non-uniform rational b-spline (nurbs) functions are exploited herein as an additional regularization of the initial ill-posed problem. k-refinement is analyzed on an artificial test case where the proposed methodology is shown to outperform the usual finite element-based dic. finally a fatigue tensile test on a thin aluminum sheet is analyzed. strain localization occurs after a certain number of cycles and combination of nurbs into a dic algorithm clearly shows a great potential to improve the robustness of non-linear constitutive law identification.",
    "present_kp": [
      "digital image correlation",
      "nurbs",
      "strain field measurement",
      "isogeometric analysis"
    ],
    "absent_kp": []
  },
  {
    "title": "stable computation of the functional variation of the dirichletneumann operator.",
    "abstract": "this paper presents an accurate and stable numerical scheme for computation of the first variation of the dirichletneumann operator in the context of eulers equations for ideal free-surface fluid flows. the transformed field expansion methodology we use is not only numerically stable, but also employs a spectrally accurate fourier/chebyshev collocation method which delivers high-fidelity solutions. this implementation follows directly from the authors previous theoretical work on analyticity properties of functional variations of dirichletneumann operators. these variations can be computed in a number of ways, but we establish, via a variety of computational experiments, the superior effectiveness of our new approach as compared with another popular boundary perturbation algorithm (the method of operator expansions).",
    "present_kp": [
      "dirichletneumann operators",
      "functional variations"
    ],
    "absent_kp": [
      "boundary perturbation methods",
      "high-order/spectral methods",
      "water waves"
    ]
  },
  {
    "title": "optimal tool selection for 2.5d milling, part 1: a solid-modeling approach for construction of the voronoi mountain.",
    "abstract": "cutter selection is a critical subtask of machining process planning. in this two-part series, we develop a robust approach for the selection of an optimal set of milling cutters for a 2.5d generalized pocket. in the first article ( part 1), we present a solid modeling approach for the construction of the voronoi mountain for the pocket geometry, which is a 3d extension of the voronoi diagram. the major contributions of this work include: ( 1) the development of a robust and systematic procedure for construction of the voronoi mountain for a multiply-connected curvilinear polygon; and ( b) an extension of the voronoi mountain concept to handle open edges.",
    "present_kp": [
      "voronoi mountain ",
      "2.5d milling",
      "open edges"
    ],
    "absent_kp": [
      "cutter  selection",
      "solid modelling"
    ]
  },
  {
    "title": "evaluating the novelty of text-mined rules using lexical knowledge.",
    "abstract": "in this paper, we present a new method of estimating the novelty of rules discovered by data-mining methods using wordnet, a lexical knowledge-base of english words. we assess the novelty of a rule by the average semantic distance in a knowledge hierarchy between the words in the antecedent and the consequent of the rule - the more the average distance, more is the novelty of the rule. the novelty of rules extracted by the discotex text-mining system on amazon.com book descriptions were evaluated by both human subjects and by our algorithm. by computing correlation coefficients between pairs of human ratings and between human and automatic ratings, we found that the automatic scoring of rules based on our novelty measure correlates with human judgments about as well as human judgments correlate with one another. @text mining",
    "present_kp": [
      "novelty",
      "semantic distance",
      "knowledge hierarchy",
      "wordnet"
    ],
    "absent_kp": [
      "interesting rules"
    ]
  },
  {
    "title": "visor: vast independence system optimization routine.",
    "abstract": "an algorithm is sketched that generates all k maximal independent sets and all m minimal dependent sets of an arbitrary independence system, based on a set of cardinality n having at most 2(n) subsets. with access to an oracle that decides if a set is independent or not. because the algorithm generates all those sets, it solves the problems of finding all maximum independent and minimum dependent sets. those problems are known to be impossible to solve in general in time polynomial in n, k, and m, and they are np hard. the algorithm proposed and used is efficient in the sense that it requires only o(nk + m) or o(k + nm) visits to the oracle, the nonpolynomial part is only related to bitstring comparisons and the like, which can be performed rather quickly and, to some degree, in parallel on a sequential machine. this complexity compares favorably with another algorithm that is o(n(2)k(2)). the design of a computer routine that implements the algorithm in a highly optimized way is discussed. the routine behaves as expected, as is shown by numerical experiments on a range of randomly generated independence systems with n up to n = 34. application on an engineering design problem with n = 28 shows the routine requires almost 10(6) times less visits to the oracle than an exhaustive search, while the time spent in visiting the oracle is still significantly larger than that spent for all other computations together.",
    "present_kp": [
      "independence system",
      "maximal independent set"
    ],
    "absent_kp": [
      "combinatorial optimization",
      "input/output selection"
    ]
  },
  {
    "title": "superconvergence in high-order galerkin finite element methods.",
    "abstract": "in this paper, we shall use local estimates to give the superconvergence of high-order galerkin finite element method for the elliptic equation of second order with constant coefficients by using the symmetric technique and integral identity. we get improved superconvergence on the inner locally symmetric mesh with respect to a point x0 for rectangular and triangular meshes.",
    "present_kp": [
      "superconvergence"
    ],
    "absent_kp": [
      "integral identities",
      "locally symmetric meshes",
      "elliptic equations of second order"
    ]
  },
  {
    "title": "a review of design pattern mining techniques.",
    "abstract": "the quality of a software system highly depends on its architectural design. high quality software systems typically apply expert design experience which has been captured as design patterns. as demonstrated solutions to recurring problems, design patterns help to reuse expert experience in software system design. they have been extensively applied in the industry. mining the instances of design patterns from the source code of software systems can assist in the understanding of the systems and the process of re-engineering them. more importantly, it also helps to trace back to the original design decisions, which are typically missing in legacy systems. this paper presents a review on current techniques and tools for mining design patterns from source code or design of software systems. we classify different approaches and analyze their results in a comparative study. we also examine the disparity of the discovery results of different approaches and analyze possible reasons with some insight.",
    "present_kp": [
      "design pattern",
      "discovery"
    ],
    "absent_kp": [
      "reverse engineering"
    ]
  },
  {
    "title": "stabilization of second-order nonholonomic systems in canonical chained form.",
    "abstract": "stabilization of a class of second-order nonholonomic systems in canonical chained form is investigated in this paper. first, the models of two typical second-order nonholonomic systems, namely, a three-link planar manipulator with the third joint unactuated, and a kinematic redundant manipulator with all joints free and driven by forces/torques imposing on the end-effector, are presented and converted to second-order chained form by transformations of coordinate and input. a discontinuous control law is then proposed to stabilize all states of the system to the desired equilibrium point exponentially. computer simulation is given to show the effectiveness of the proposed controller.",
    "present_kp": [
      "second-order nonholonomic systems"
    ],
    "absent_kp": [
      "canonical second-order chained form",
      "underactuated manipulator",
      "discontinuous coordinate transformation",
      "discontinuous stabilization"
    ]
  },
  {
    "title": "truthful mechanisms for two-range-values variant of unrelated scheduling.",
    "abstract": "in this paper, we consider a restricted variant of the scheduling problem, where the machines are the strategic players. for this multi-parameter mechanism design problem, the only known truthful mechanisms use task independent allocation algorithms and only have approximation ratio o(m). lavi and swamy first use the cycle monotone condition and design a 3-approximation truthful mechanism for a two value variant in lavi, where the processing time of task j on machine i, say t(ij), can only be either a lower value l(j) or a higher value h(j). we consider a generalized variant. where t(ij) lies in [l(j), l(j)(1+epsilon)] boolean or [h(j), h(j)(1+epsilon)] and epsilon is a parameter satisfying some condition. we consider two special cases, case a when h(j)/l(j) > 2,for all(j) and case b when h(j)/l(j) <= 2, for all j, and give randomized truthful mechanisms with approximation ratio 4(1+epsilon) for both cases. based on these two cases' results, we are also able to deal with the general case of our two-range-values scheduling problem. we use a combination of two mechanisms, which is also a novel method in mechanism design for scheduling problems, and finally we give a randomized truthful mechanism with approximation ratio 7(1+epsilon). although the generalization seems a little incremental, we actually use a very novel technique in the key step of proving truthfulness for case a, as well as a new mechanism scheme for case b. besides, the results in this paper are the first truthful mechanisms with constant approximation ratios when a machine (player) can report infinitely possible values, which is quite different from the two value variant, in which only finite values are available. furthermore, together with lavi and swamy's work, our results suggest that such a task-dependent approach can really do much better for the scheduling unrelated machines problem.",
    "present_kp": [
      "truthful mechanism",
      "scheduling"
    ],
    "absent_kp": [
      "approximation algorithm"
    ]
  },
  {
    "title": "a note on the relationships among certified discrete log cryptosystems.",
    "abstract": "the certified discrete logarithm problem modulo p prime is a discrete logarithm problem under the conditions that the complete factorization of p - 1 is given and by which the base g is certified to be a primitive root mod p. for the cryptosystems based on the intractability of certified discrete logarithm problem, sakurai-shizuya showed that breaking the diffie-hellman key exchange scheme reduces to breaking the shamir 3-pass key transmission scheme with respect to the expected polynomial-time turing reducibility. in this paper, we show that we can remove randomness from the reduction above, and replace the reducibility with the polynomial-time many-one. since the converse reduction is known to hold with respect to the polynomial-time many-one reducibility, our result gives a stronger evidence for that the two schemes are completely equivalent as certified discrete log cryptosystems.",
    "present_kp": [
      "certified discrete logarithm problem",
      "primitive root"
    ],
    "absent_kp": [
      "order",
      "probabilistic reducibility",
      "deterministic reducibility"
    ]
  },
  {
    "title": "hydraulic performance of a large slanted axial-flow pump.",
    "abstract": "purpose - the pump of the taipuhe pump station, larger flow discharge, lower head, is one of the largest 150 slanted axial-flow pumps in the world. however, few studies have been done for the larger slanted axial-flow pump on safe operation. the purpose of this paper is to analyze the impeller elevation, unsteady flow, hydraulic thrust and the zero-head flow characteristics of the pump. design/methodology/approach - the flow field in and through the pump was analyzed numerically during the initial stages of the pump design process, then the entire flow passage through the pump was analyzed to calculate the hydraulic thrust to prevent damage to the bearings and improve the operating stability the zero-head pump flow characteristics were analyzed to ensure that the pump will work reliably at much lower heads. findings - the calculated results are in good agreement with experimental data for the pump elevation effects, the performance curve, pressure oscillations, hydraulic thrust and zero-head performance. research limitations/implications - since it is assumed that there is no gap between blades and shroud, gap cavitations are beyond the scope of the paper. originality/value - the paper indicates the slanted axial-flow pump characteristics including the characteristic curves, pressure fluctuations, hydraulic thrust and radial force for normal operating conditions and zero-head conditions. it shows how to guarantee the pump safety operating by computational fluid dynamics.",
    "present_kp": [
      "pumps",
      "fluid dynamics"
    ],
    "absent_kp": [
      "force measurement",
      "water supply engineering",
      "china"
    ]
  },
  {
    "title": "high-radix montgomery modular exponentiation on reconfigurable hardware.",
    "abstract": "it is widely recognized that security issues will play a crucial role in the majority of future computer and communication systems. central tools for achieving system security are cryptographic algorithms. this contribution proposes arithmetic architectures which are optimized for modern field programmable gate arrays (fpgas). the proposed architectures perform modular exponentiation with very long integers. this operation is at the heart of many practical public-key algorithms such as rsa and discrete logarithm schemes. we combine a high-radix montgomery modular multiplication algorithm with a new systolic array design. the designs are flexible, allowing any choice of operand and modulus. the new architecture also allows the use of high radices. unlike previous approaches, we systematically implement and compare several variants of our new architecture for different bit lengths. we provide absolute area and timing measures for each architecture. the results allow conclusions about the feasibility and time-space trade-offs of our architecture for implementation on commercially available fpgas. we found that 1,024-bit rsa decryption can be done in 3.1 ms with our fastest architecture.",
    "present_kp": [
      "montgomery",
      "fpga",
      "exponentiation",
      "rsa",
      "systolic array"
    ],
    "absent_kp": [
      "modular arithmetic"
    ]
  },
  {
    "title": "combined use of supervised and unsupervised learning for power system dynamic security mapping.",
    "abstract": "this paper proposes a new methodology which combines supervised and unsupervised learning for evaluating power system dynamic security. based on the concept of stability margin, pre-fault power system conditions are assigned to the output neurons on the two-dimensional grid with the growing hierarchical self-organizing map technique (ghsom) via supervised artificial neural networks (anns) which perform an estimation of post-fault power system state. the technique estimates the dynamic stability index that corresponds to the most critical value of synchronizing and damping torques of multimachine power systems. ann-based pattern recognition is carried out with the growing hierarchical self-organizing feature mapping in order to provide adaptive neural network architecture during its unsupervised training process. numerical tests, carried out on a ieee 9 bus power system are presented and discussed. the analysis using such method provides accurate results and improves the effectiveness of system security evaluation.",
    "present_kp": [
      "synchronizing and damping torques",
      "growing hierarchical self-organizing feature map",
      "supervised and unsupervised learning"
    ],
    "absent_kp": [
      "dynamic security assessment",
      "stability criteria"
    ]
  },
  {
    "title": "0.35 mu m cmos t/r switch for 2.4 ghz short range wireless applications.",
    "abstract": "this paper describes the design and implementation of a transmit/receive switch for 2.4 ghz ism band applications. the t/r switch is implemented in a 0.35 mum bulk cmos process and it occupies 150 mum . 170 mum of die area. a parasitic mosfet model including bulk resistance is used to optimize the physical dimensions of the transistors with regard to insertion loss and isolation. the measured insertion loss is 1.3 db without port matching. simulations using measured s-parameters indicate that an insertion loss of 0.8 db can be obtained with a conjugate match. the measured isolation is 42 db and the maximum transmit power is 16 dbm.",
    "present_kp": [
      "t/r switch"
    ],
    "absent_kp": [
      "mosfet switch",
      "rf cmos",
      "spdt switch"
    ]
  },
  {
    "title": "probabilistic equivalence checking of multiple-valued functions.",
    "abstract": "this paper describes a probabilistic method for verifying the equivalence of two multiple-valued functions. each function is hashed to an integer code by transforming it to a integer-valued polynomial and evaluating it for values of variables taken independently and uniformly at random from a finite field. since the polynomial is unique for a given function, if two hash codes are different, then the functions are not equivalent. however, if two hash codes are the same, the functions may or may not be equivalent, because different polynomials may happen to hash to the same code. thus, the method presented in this paper determines the equivalence of two functions with a known (small) probability of error, arising from collisions between inequivalent functions. such a method seems to be an attractive alternative for verifying functions that are too large to be handled by deterministic equivalence checking methods.",
    "present_kp": [
      "multiple-valued function",
      "equivalence checking"
    ],
    "absent_kp": [
      "probabilistic verification"
    ]
  },
  {
    "title": "a model and environment for improving multimedia scholarly reading practices.",
    "abstract": "the evolution of multimedia document production and diffusion technologies has lead to a significant spread of knowledge in form of pictures and recordings. however, scholarly reading tasks are still principally performed on textual contents. we argue that this is due to a lack of critical and structured tools: (1) to handle the wide spectrum of interpretive operations involved by the polymorphous scholarly reading process; (2) to perform these operations on a heterogeneous multimedia corpus. this firstly calls for identifying fundamental document requirements for such reading practices. then, we present a flexible model and a software environment which enable the reader to structure, annotate, link, fragment, compare, freely organise and spatially lay out documents, and to prepare the writing of their critical comment. we eventually discuss experiments with humanities scholars, and explore new academic reading practices which take advantage of document engineering principles such as multimedia document structuring, publication or sharing.",
    "present_kp": [
      "multimedia scholarly reading"
    ],
    "absent_kp": [
      "multimedia corpus modelling",
      "document annotation and structuring",
      "spatial hypertexts",
      "graphical user interfaces for critical reading"
    ]
  },
  {
    "title": "sensor selection for energy-efficient ambulatory medical monitoring.",
    "abstract": "epilepsy affects over three million americans of all ages. despite recent advances, more than 20% of individuals with epilepsy never achieve adequate control of their seizures. the use of a small, portable, non-invasive seizure monitor could benefit these individuals tremendously. however, in order for such a device to be suitable for long-term wear, it must be both comfortable and lightweight. typical state-of-the-art non-invasive seizure onset detection algorithms require 21 scalp electrodes to be placed on the head. these electrodes are used to generate 18 data streams, called channels. the large number of electrodes is inconvenient for the patient and processing 18 channels can consume a considerable amount of energy, a problem for a battery-powered device. in this paper, we describe an automated way to construct detectors that use fewer channels, and thus fewer electrodes. starting from an existing technique for constructing 18 channel patient-specific detectors, we use machine learning to automatically construct reduced channel detectors. we evaluate our algorithm on data from 16 patients used in an earlier study. on average, our algorithm reduced the number of channels from 18 to 4.6 while decreasing the mean fraction of seizure onsets detected from 99% to 97%. for 12 out of the 16 patients, there was no degradation in the detection rate. while the average detection latency increased from 7.8 s to 11.2 s, the average rate of false alarms per hour decreased from 0.35 to 0.19. we also describe a prototype implementation of a single channel eeg monitoring device built using off-the-shelf components, and use this implementation to derive an energy consumption model. using fewer channels reduced the average energy consumption by 69%, which amounts to a 3.3x increase in battery lifetime. finally, we show how additional energy savings can be realized by using a low-power screening detector to rule out segments of data that are obviously not seizures. though this technique does not reduce the number of electrodes needed, it does reduce the energy consumption by an additional 16%.",
    "present_kp": [
      "epilepsy",
      "ambulatory medical monitoring"
    ],
    "absent_kp": [
      "reducing energy consumption",
      "channel selection",
      "electroencephalography "
    ]
  },
  {
    "title": "process synchronization without long-term interlock.",
    "abstract": "a technique is presented for replacing long-term interlocking of shared data by the possible repetition of unprivileged code in case a version number (associated with the shared data) has been changed by another process. four principles of operating system architecture (which have desirable effects on the intrinsic reliability of a system) are presented; implementation of a system adhering to these principles requires that long-term lockout be avoided.",
    "present_kp": [
      "synchronization",
      "reliability",
      "code",
      "architecture",
      "operating system",
      "implementation",
      "process",
      "data",
      "case",
      "version",
      "effect"
    ],
    "absent_kp": [
      "sharing",
      "systems",
      "association"
    ]
  },
  {
    "title": "application of an artificial immune system-based fuzzy neural network to a rfid-based positioning system.",
    "abstract": "due to the rapid development of globalization, which makes supply chain management more complicated, more companies are applying radio frequency identification (rfid), in warehouse management. the obvious advantages of rfid are its ability to scan at high-speed, its penetration and memory. in addition to recycling, use of a rfid system can also reduce business costs, by indentifying the position of goods and picking carts. this study proposes an artificial immune system (ais)-based fuzzy neural network (fnn), to learn the relationship between the rfid signals and the picking cart's position. since the proposed network has the merits of both ais and fnn. it is able to avoid falling into the local optimum and possesses a learning capability. the results of the evaluation of the model show that the proposed ais-based fnn really can predict the picking cart position more precisely than conventional fnn and, unlike an artificial neural network, it is much easier to interpret the training results, since they are in the form of fuzzy if-then rules.",
    "present_kp": [
      "radio frequency identification ",
      "artificial immune system ",
      "fuzzy neural network "
    ],
    "absent_kp": [
      "genetic algorithms "
    ]
  },
  {
    "title": "single-dimension multidimensional software pipelining for loops.",
    "abstract": "traditionally, software pipelining is applied either to the innermost loop of a given loop nest or from the innermost loop to outer loops. this paper proposes a three-step approach, called single-dimension software pipelining (ssp), to software pipeline a loop nest at an arbitrary loop level that has a rectangular iteration space and contains no sibling inner loops in it. the first step identifies the most profitable loop level for software pipelining in terms of initiation rate, data reuse potential, or any other optimization criteria. the second step simplifies the multidimensional data-dependence graph (ddg) of the selected loop level into a one-dimensional ddg and constructs a one-dimensional (1d) schedule. based on the one-dimensional schedule, the third step derives a simple mapping function that specifies the schedule time for the operation instances in the multidimensional loop. the classical modulo scheduling is subsumed by ssp as a special case. ssp is also closely related to hyperplane scheduling, and, in fact, extends it to be resource constrained. we prove that ssp schedules are correct and at least as efficient as those schedules generated by traditional modulo scheduling methods. we extend ssp to schedule imperfect loop nests, which are most common at the instruction level. multiple initiation intervals are naturally allowed to improve execution efficiency. feasibility and correctness of our approach are verified by a prototype implementation in the orc compiler for the ia-64 architecture, tested with loop nests from livermore and spec2000 floating-point benchmarks. preliminary experimental results reveal that, compared to modulo scheduling, software pipelining at an appropriate loop level results in significant performance improvement. software pipelining is beneficial even with prior loop transformations.",
    "present_kp": [
      "software pipelining",
      "modulo scheduling",
      "loop transformation"
    ],
    "absent_kp": [
      "algorithms",
      "languages"
    ]
  },
  {
    "title": "a geometric-based method for recognizing overlapping polygonal-shaped and semi-transparent particles in gray tone images.",
    "abstract": "a geometric-based method is proposed to recognize the overlapping particles of different polygonal shapes such as rectangular, regular and/or irregular prismatic particles in a gray tone image. the first step consists in extracting the salient corners, identified by their locations and orientations, of the overlapping particles. although there are certain difficulties like the perspective geometric projection, out of focus, transparency and superposition of the studied particles. then, a new clustering technique is applied to detect the shape by grouping its correspondent salient corners according to the geometric properties of each shape. a simulation process is carried out for evaluating the performance of the proposed method. then, it is particularly applied on a real application of batch cooling crystallization of the ammonium oxalate in pure water. the experimental results show that the method is efficient to recognize the overlapping particles of different shapes and sizes.",
    "present_kp": [],
    "absent_kp": [
      "salient corner detection",
      "contour detection",
      "clustering method",
      "overlapping particles recognition"
    ]
  },
  {
    "title": "explicit dimension reduction and its applications.",
    "abstract": "we construct a small set of explicit linear transformations mapping r-n to r-t, where t = o(log(gamma(-1))epsilon(-2)), such that the l-2 norm of any vector in r-n is distorted by at most 1 +/- epsilon in at least a fraction of 1 - gamma of the transformations in the set. albeit the tradeoff between the size of the set and the success probability is suboptimal compared with probabilistic arguments, we nevertheless are able to apply our construction to a number of problems. in particular, we use it to construct an epsilon-sample (or pseudorandom generator) for linear threshold functions on sn-1 for epsilon = o(1). we also use it to construct an epsilon-sample for spherical digons in sn-1 for epsilon = o(1). this construction leads to an efficient oblivious derandomization of the goemans-williamson max-cut algorithm and similar approximation algorithms (i.e., we construct a small set of hyperplanes such that for any instance we can choose one of them to generate a good solution). our technique for constructing an epsilon-sample for linear threshold functions on the sphere is considerably different than previous techniques that rely on k-wise independent sample spaces.",
    "present_kp": [
      "dimension reduction",
      "pseudorandom generator",
      "linear threshold functions",
      "max-cut",
      "digons"
    ],
    "absent_kp": [
      "johnson lindenstrauss"
    ]
  },
  {
    "title": "capital one financial and a decade of experience with newly vulnerable markets: some propositions concerning the competitive advantage of new entrants.",
    "abstract": "market share and brand recognition have historically provided advantage to established players in mature industries. the success of capital one, an attacker in the mature credit card industry is therefore interesting, both to researchers and to executives developing strategies. a partial explanation is offered by the theory of newly vulnerable markets. the success of capital one can be partially attributed to its application of information-based strategies to several newly vulnerable markets, allowing it to target and retain the most profitable customers. these strategies sustained double-digit return on equity and double-digit increase in sales volume and profits every year of our study.",
    "present_kp": [
      "newly vulnerable markets",
      "capital one financial"
    ],
    "absent_kp": [
      "information-based strategy",
      "market entry",
      "differential pricing",
      "customer profitability gradient",
      "information economics"
    ]
  },
  {
    "title": "scene analysis and geometric homology.",
    "abstract": "during the last 10-12 years there has been a dramatic revival of interest in applied geometric problems. geometers have reconsidered a number of questions in infinitesimal mechanics, questions treated by j.c. maxwell and l. cremona in 1864-70, further developed under the banner of graphical statics , but left largely untouched since the end of the nineteenth century. at the same time, computer scientists have come to recognize that the tools of graphical statics and of applied projective geometry are fundamental to research in scene analysis. a good deal of the recent revival of interest is due to the efforts of the structural topology research group at the university of montreal. the work of this group, reported in the pages of the journal structural topology (and elsewhere), was a biproduct of research on infinitesimal mechanics, using methods derived from graphical statics, as well as from exterior algebra and its modern offspring, the doubilet-rota-stein double algebra . independently, huffman , duda and hart , and others recognized that maxwell's reciprocal figures could help in deciding whether a given plane image is the projection of a 3d polyhedral scene. more recently, sugihara and his colleagues in nagoya created what may be considered a pilot project for automated descriptive geometry. they wrote a software package capable of modifying a rough plane sketch, so as to make it a true projection of a 3d scene. the starting point of the during the last 10-12 years there has been a dramatic revival of interest in applied geometric problems. geometers have reconsidered a number of questions in infinitesimal mechanics, questions treated by j.c. maxwell and l. cremona in 1864-70, further developed under the banner of graphical statics , but left largely untouched since the end of the nineteenth century. at the same time, computer scientists have come to recognize that the tools of graphical statics and of applied projective geometry are fundamental to research in scene analysis. a good deal of the recent revival of interest is due to the efforts of the structural topology research group at the university of montreal. the work of this group, reported in the pages of the journal structural topology (and elsewhere), was a biproduct of research on infinitesimal mechanics, using methods derived from graphical statics, as well as from exterior a. independently, huffman , duda and hart , and others recognized that maxwell's reciprocal figures could help in deciding whether a given plane image is the projection of a 3d polyhedral scene. more recently, sugihara and his colleagues in nagoya created what may be considered a pilot project for automated descriptive geometry. they wrote a software package capable of modifying a rough plane sketch, so as to make it a true projection of a 3d scene. the starting point of the projective geometric analysis of scenes is the observation that the set of all three-dimensional realizations ( scenes ) having a given two-dimensional projection (a drawing, or image ) form a linear space. much information about an image, and about its possible spatial interpretations, can be obtained simply by calculating (either locally or globally) the linear dimension (or rank ) of its linear space of scenes. in practice, the image is a pattern on a cathode-ray tube, an aerial photograph, an engineer's or architect's drawing, or an x-ray or nmr scan. the rank of its space of scenes will reveal whether there is ambiguity or uniqueness in the construction of its spatial interpretation, or whether such a construction is in fact impossible, as would be the case for a poorly conceived engineering drawing, or even in an otherwise correctly conceived drawing, if too many hypotheses are made concerning the 3d structure of the scene. calculation of the rank of the space of scenes having a given image should, in principle, be accomplished using simple combinatorial algorithms based on easily-remembered rules-of-thumb. this is the goal, and it shows every sign of being achievable. the problem has, however, a certain degree of unavoidable difficulty. the requirement that a given image be an accurate projection of a non-trivial (non-planar) 3d scene imposes conditions on the image, conditions which are perhaps best described in terms of not-always-elementary constructions with straight-edge and pencil. in this paper, we begin to sort out the interplay of these projective conditions by creating a new homology theory for geometric configurations. the new homology theory applies to geometric objects which are more rigid, less pliable, than the rubber sheets studied by the topology of henri poincar and his school. the passage to this higher degree of invariance is made possible by the creation of a homology theory with (restricted) vector, rather than (unrestricted) scalar, coefficients, or equivalently, by the use of a cohomology theory based on locally linear, rather than on locally constant, functions. we have verified that the new theory agrees with the cohomology theory for the sheaf of locally linear functions on a certain (combinatorially defined) topological space. the basic objects about which this new homology theory has something non-trivial to say are extremely general. from the geometric point of view, they are simply finite sets of points in a projective space or finite sets of vectors in a vector space. in order to emphasize the departure we take from linear algebra as it is usually practiced, we should say that we study vector spaces with a selected basis , that is, concrete vector spaces , in their usual representation as spaces f p of functions from a set p into a field f. finally, we might say we are simply studying rectangular matrices . since such objects are found throughout applied mathematics, the resulting homology theory has a very broad range of potential application. indeed, potential applications of this new homology theory are to any domain where one is interested in the global behavior of systems determined locally by linear constraints.",
    "present_kp": [
      "point",
      "applications",
      "geometry",
      "mathematics",
      "use",
      "invariance",
      "analysis",
      "linear algebra",
      "project",
      "theory",
      "group",
      "representation",
      "paper",
      "constraint",
      "scan",
      "spatial",
      "research",
      "method",
      "behavior",
      "3d",
      "image",
      "algorithm",
      "algebra",
      "rules",
      "global",
      "tools",
      "help",
      "order",
      "drawing",
      "engine",
      "object",
      "case",
      "general",
      "pattern",
      "software",
      "structure",
      "systems",
      "space",
      "interpretation"
    ],
    "absent_kp": [
      "requirements",
      "sorting",
      "timing",
      "graphics",
      "automation",
      "informal",
      "packaging",
      "sketching",
      "configurability",
      "ambiguities",
      "computation",
      "topologies",
      "practical",
      "observability",
      "vectorization",
      "ranking"
    ]
  },
  {
    "title": "imagesense: towards contextual image advertising.",
    "abstract": "the daunting volumes of community-contributed media contents on the internet have become one of the primary sources for online advertising. however, conventional advertising treats image and video advertising as general text advertising by displaying relevant ads based on the contents of the web page, without considering the inherent characteristics of visual contents. this article presents a contextual advertising system driven by images, which automatically associates relevant ads with an image rather than the entire text in a web page and seamlessly inserts the ads in the nonintrusive areas within each individual image. the proposed system, called imagesense, supports scalable advertising of, from root to node, web sites, pages, and images. in imagesense, the ads are selected based on not only textual relevance but also visual similarity, so that the ads yield contextual relevance to both the text in the web page and the image content. the ad insertion positions are detected based on image salience, as well as face and text detection, to minimize intrusiveness to the user. we evaluate imagesense on a large-scale real-world images and web pages, and demonstrate the effectiveness of imagesense for online image advertising.",
    "present_kp": [],
    "absent_kp": [
      "algorithms",
      "experimentation",
      "human factors"
    ]
  },
  {
    "title": "construction and blind estimation of phase sequences for subcarrier-phase control based papr reduction in ldpc coded ofdm systems.",
    "abstract": "as described in this paper construction and blind estimation methods of phase sequences are proposed for subcarrier phase control based peak to average power ratio (papr) reduction in low density parity check (ldpc) coded orthogonal frequency division multiplexing (ofdm) systems. on the transmitter side phase sequence patterns are constructed based on a given parity check matrix. the papr of the ofdm signal is reduced by multiplying the constructed phase sequence selected from the same number of candidates as the number of weighting factor (wf) combinations in a partial transmit sequence (pts) method. on the receiver side the phase sequence is estimated blindly using the decoding function i e the most likely phase sequence among a limited number of possible phase sequence candidates is inferred by comparing the sum product calculation results of each candidate. computer simulation results show that papr of qpsk ofdm and 16qam ofdm signals can be reduced respectively by about 3 7 db and 4 0 db without marked degradation of the block error rate (bler) performance as compared to perfect estimation in an attenuated 12 path rayleigh fading condition.",
    "present_kp": [
      "ofdm",
      "peak to average power ratio ",
      "papr reduction",
      "ldpc code"
    ],
    "absent_kp": []
  },
  {
    "title": "mathsat: tight integration of sat and mathematical decision procedures.",
    "abstract": "recent improvements in propositional satisfiability techniques (sat) made it possible to tackle successfully some hard real-world problems (e.g., model-checking, circuit testing, propositional planning) by encoding into sat. however, a purely boolean representation is not expressive enough for many other real-world applications, including the verification of timed and hybrid systems, of proof obligations in software, and of circuit design at rtl level. these problems can be naturally modeled as satisfiability in linear arithmetic logic (lal), that is, the boolean combination of propositional variables and linear constraints over numerical variables. in this paper we present mathsat, a new, sat-based decision procedure for lal, based on the (known approach) of integrating a state-of-the-art sat solver with a dedicated mathematical solver for lal. we improve mathsat in two different directions. first, the top-level line procedure is enhanced and now features a tighter integration between the boolean search and the mathematical solver. in particular, we allow for theory-driven backjumping and learning, and theory-driven deduction; we use static learning in order to reduce the number of boolean models that are mathematically inconsistent; we exploit problem clustering in order to partition mathematical reasoning; and we define a stack-based interface that allows us to implement mathematical reasoning in an incremental and backtrackable way. second, the mathematical solver is based on layering; that is, the consistency of (partial) assignments is checked in theories of increasing strength (equality and uninterpreted functions, linear arithmetic over the reals, linear arithmetic over the integers). for each of these layers, a dedicated (sub)solver is used. cheaper solvers are called first, and detection of inconsistency makes call of the subsequent solvers superfluous. we provide a through experimental evaluation of our approach, by taking into account a large set of previously proposed benchmarks. we first investigate the relative benefits and drawbacks of each proposed technique by comparison with respect to a reference option setting. we then demonstrate the global effectiveness of our approach by a comparison with several state-of-the-art decision procedures. we show that the behavior of mathsat is often superior to its competitors, both on lal and in the subclass of difference logic.",
    "present_kp": [
      "linear arithmetic logic",
      "propositional satisfiability"
    ],
    "absent_kp": [
      "satisfiability module theory",
      "integrated decision procedures"
    ]
  },
  {
    "title": "strongly regular graphs with the (7)-vertex condition.",
    "abstract": "the (t)-vertex condition, for an integer (tge 2), was introduced by hestenes and higman (siam am math soc proc 4:41160, 1971) providing a combinatorial invariant defined on edges and non-edges of a graph. finite rank 3 graphs satisfy the condition for all values of (t). moreover, a long-standing conjecture of klin asserts the existence of an integer (t_0) such that a graph satisfies the (t_0)-vertex condition if and only if it is a rank 3 graph. we present the first infinite family of non-rank 3 strongly regular graphs satisfying the (7)-vertex condition. this implies that the klin parameter (t_0) is at least 8. the examples are the point graphs of a certain family of generalized quadrangles.",
    "present_kp": [
      "strongly regular graph",
      "generalized quadrangle"
    ],
    "absent_kp": [
      "t-vertex condition"
    ]
  },
  {
    "title": "((epsilon )-)efficiency in difference vector optimization.",
    "abstract": "the paper deals with the problem of characterizing pareto optima (efficient solutions) for the difference of two mappings vector-valued in a finite or infinite-dimensional preordered space. closely related to the well-known optimality criterion of scalar dc optimization, a mixed vectorial condition is obtained in terms of both strong (fenchel) and weak (pareto) (epsilon )-subdifferentials that completely characterizes the exact or approximate weak efficiency. this condition also allows to deal with some special restricted mappings. moreover, the condition established in the literature in terms of strong (epsilon )-subdifferentials for characterizing the strongly efficient solutions (usual optima), is shown here to remain valid without assuming that the objective space is order-complete.",
    "present_kp": [
      "vector optimization",
      "efficiency"
    ],
    "absent_kp": [
      "dc objective",
      "optimality criteria",
      "-solutions",
      "vector -subdifferentials"
    ]
  },
  {
    "title": "iterative visual clustering for unstructured text mining.",
    "abstract": "this paper proposes the iterative visual clustering (ivc) on unstructured text sequences to form and evaluate keyword clusters, based on which users can use visual analysis, domain knowledge to discover knowledge in the text. the text sequence data are broken down into a list representative keywords after textual evaluation, and the keywords are then grouped to form keyword clusters via an iterative stochastic process and are visualized as distributions over the time lines. the visual evaluation model provides shape evaluations as quantitative tools and users' interactions as qualitative tools to visually investigate the trends, patterns represented by the keyword clusters' distributions. the keyword clustering model, guided by the feedback of visual evaluations, step-wisely enumerates newer generations of keyword clusters and their patterns, therefore narrows down the search space. then the proposed ivc is applied onto nursing narratives and is able to identify interesting keyword clusters implying hidden knowledge regarding to the working patterns and environment of registered nurses. the loop of producing next generation of keyword clusters in ivc is driven and controlled by users' perception, domain knowledge and interactions, and it is also guided by a stochastic search model. so both semantic and distribution features enable ivc to have significant applications as a text mining tool, on many other data sets, such as biomedical literatures.",
    "present_kp": [],
    "absent_kp": [
      "text and document visualization",
      "nursing data processing"
    ]
  },
  {
    "title": "economic growth, telecommunications development and productivity growth of the telecommunications sector: evidence around the world.",
    "abstract": "this paper studies the relationships between economic growth, telecommunications development and productivity growth of the telecommunications sector in different countries and regions of the world. in particular, this study assesses the impact of mobile telecommunications on economic growth and telecommunications productivity. the results indicate that there is a bidirectional relationship between real gross domestic product (gdp) and telecommunications development (as measured by teledensity) for european and high-income countries. however, when the impact of mobile telecommunications development on economic growth is measured separately, the bi-directional relationship is no longer restricted to european and high-income countries. this study also finds that countries in the upper-middle income group have achieved a higher average total factor productivity (tfp) growth than other countries. countries with competition and privatization in telecommunications have achieved a higher tfp growth than those without competition and privatization. the diffusion of mobile telecommunications services is found to be a significant factor that has improved the tfp growth of the telecommunications sector in central and eastern europe (cee).",
    "present_kp": [
      "telecommunications",
      "economic growth",
      "total factor productivity"
    ],
    "absent_kp": []
  },
  {
    "title": "examining learning from text and pictures for different task types: does the multimedia effect differ for conceptual, causal, and procedural tasks.",
    "abstract": "the multimedia effect (me) is a well-researched effect in the field of learning and instruction. in this article, two views that explain the me are compared. the outcome-oriented view focuses on the beneficial effect of text and pictures on mental representations, whereas the process-oriented view focuses on the beneficial effect of text and pictures for information processing. to contrast these views, the me sizes for different task types were compared (i.e., conceptual, causal, procedural tasks). whereas the outcome-oriented view predicts no differences in me size, the process-oriented view predicts that the me is largest in causal tasks, smaller in procedural tasks, and smallest in conceptual tasks. sixty-five students learnt with text only or with text and pictures. task type and information source (i.e., whether the text, picture, or text and picture provided the answer to a post-test question) were varied within subjects. the results showed that, in line with the process-oriented view, the me was smaller for conceptual tasks than for procedural tasks. contrary to the expectations, the me was larger in procedural tasks than in causal tasks. moreover, the pattern of results varied with information source. research and practical implications are described, so that pictures can be deployed optimally.",
    "present_kp": [
      "multimedia effect",
      "conceptual"
    ],
    "absent_kp": [
      "learning with text and pictures",
      "causal and procedural tasks",
      "static visualisations"
    ]
  },
  {
    "title": "sufficient conditions for lambda '-optimality of graphs with small conditional diameter.",
    "abstract": "a restricted edge-cut s of a connected graph g is an edge-cut such that g - s has no isolated vertex. the restricted edge-connectivity lambda'(g) is the minimum cardinality over all restricted edge-cuts. a graph is said to lambda'-optimal if lambda'(g) = xi(g), where xi(g) denotes the minimum edge-degree of g defined as xi(g) = min{d(u) + d(nu) - 2: u nu is an element of e(g)}. the p-diameter of g measures how far apart a pair of subgraphs satisfying a given property p can be, and hence it generalizes the standard concept of diameter. in this paper we prove two kind of results, according to which property p is chosen. first, let d-1 (resp. d-2) be the p-diameter where p is the property that the corresponding subgraphs have minimum degree at least one (resp. two). we prove that a graph with odd girth g is lambda'-optimal if d-1 = 2, being the minimum degree of g. using the property q of being vertices of g - f we prove that a graph with girth g is not an element of {4, 6, 8} is lambda'-optimal if this q-diameter is at most 2[(g - 3)/2].",
    "present_kp": [
      "restricted edge-connectivity",
      "conditional diameter"
    ],
    "absent_kp": [
      "fault tolerance"
    ]
  },
  {
    "title": "comparative analysis of clicks and judgments for ir evaluation.",
    "abstract": "queries and click-through data taken from search engine transaction logs is an attractive alternative to traditional test collections, due to its volume and the direct relation to end-user querying. the overall aim of this paper is to answer the question: how does click-through data differ from explicit human relevance judgments in information retrieval evaluation? we compare a traditional test collection with manual judgments to transaction log based test collections---by using queries as topics and subsequent clicks as pseudo-relevance judgments for the clicked results. specifically, we investigate the following two research questions: firstly, are there significant differences between clicks and relevance judgments. earlier research suggests that although clicks and explicit judgments show reasonable agreement, clicks are different from static absolute relevance judgments. secondly, are there significant differences between system ranking based on clicks and based on relevance judgments? this is an open question, but earlier research suggests that comparative evaluation in terms of system ranking is remarkably robust.",
    "present_kp": [],
    "absent_kp": [
      "transaction log analysis",
      "wikipedia",
      "web information retrieval"
    ]
  },
  {
    "title": "s2-quasicontinuous posets.",
    "abstract": "in this paper, we consider a common generalization of both s2-continuous posets and quasicontinuous domains, and we introduce new concepts of way below relations and s2-quasicontinuous posets. the main results are: (1) the way below relation on an s2-quasicontinuous poset has the interpolation property; (2) the 2-topology on an s2-quasicontinuous poset is completely regular; (3) a poset is s2-continuous iff it is meet s2-continuous and s2-quasicontinuous.",
    "present_kp": [
      "s2-continuous poset",
      "-topology"
    ],
    "absent_kp": [
      "meet s2-continuous poset s2",
      "s2-quasicontinuous poset s2"
    ]
  },
  {
    "title": "does the polynomial hierarchy collapse if onto functions are invertible.",
    "abstract": "the class tfnp, defined by megiddo and papadimitriou, consists of multivalued functions with values that are polynomially verifiable and guaranteed to exist. do we have evidence that such functions are hard, for example, if tfnp is computable in polynomial-time does this imply the polynomial-time hierarchy collapses? by computing a multivalued function in deterministic polynomial-time we mean on every input producing one of the possible values of the function on that input. we give a relativized negative answer to this question by exhibiting an oracle under which tfnp functions are easy to compute but the polynomial-time hierarchy is infinite. we also show that relative to this same oracle, p not equal up and tfnp(np) functions are not computable in polynomial-time with an np oracle.",
    "present_kp": [
      "polynomial-time hierarchy"
    ],
    "absent_kp": [
      "computational complexity",
      "multi-valued functions",
      "kolmogorov complexity"
    ]
  },
  {
    "title": "unsupervised object segmentation with a hybrid graph model (hgm).",
    "abstract": "in this work, we address the problem of performing class-specific unsupervised object segmentation, i.e., automatic segmentation without annotated training images. object segmentation can be regarded as a special data clustering problem where both class-specific information and local texture/color similarities have to be considered. to this end, we propose a hybrid graph model (hgm) that can make effective use of both symmetric and asymmetric relationship among samples. the vertices of a hybrid graph represent the samples and are connected by directed edges and/or undirected ones, which represent the asymmetric and/or symmetric relationship between them, respectively. when applied to object segmentation, vertices are superpixels, the asymmetric relationship is the conditional dependence of occurrence, and the symmetric relationship is the color/texture similarity. by combining the markov chain formed by the directed subgraph and the minimal cut of the undirected subgraph, the object boundaries can be determined for each image. using the hgm, we can conveniently achieve simultaneous segmentation and recognition by integrating both top-down and bottom-up information into a unified process. experiments on 42 object classes (9,415 images in total) show promising results.",
    "present_kp": [
      "segmentation"
    ],
    "absent_kp": [
      "graph-theoretic methods",
      "spectral clustering"
    ]
  },
  {
    "title": "dispersion free wave splittings for structural elements.",
    "abstract": "wave splittings are derived for three types of structural elements: membranes, timoshenko beams, and mindlin plates. the timoshenko beam equation and the mindlin plate equation are inherently dispersive, as is each fourier component of the membrane equation in an angular decomposition of the field. the distinctive feature of the wave splittings derived in the present paper is that, in homogeneous regions, they transform the dispersive wave equations into simple one-way wave equations without dispersion. such splittings have uses both for radial scattering problems in the 2d cases and for scattering problems in dispersive media. as an example of how the splittings may be applied, a direct scattering problem is solved for a membrane with radially varying density. the imbedding method is utilized, and agreement is obtained with an fe simulation.",
    "present_kp": [
      "wave splitting",
      "imbedding",
      "membrane",
      "timoshenko beam",
      "mindlin plate"
    ],
    "absent_kp": [
      "time domain methods",
      "greens operator"
    ]
  },
  {
    "title": "designing a practical data filter cache to improve both energy efficiency and performance.",
    "abstract": "conventional data filter cache (dfc) designs improve processor energy efficiency, but degrade performance. furthermore, the single-cycle line transfer suggested in prior studies adversely affects level-1 data cache (l1 dc) area and energy efficiency. we propose a practical dfc that is accessed early in the pipeline and transfers a line over multiple cycles. our dfc design improves performance and eliminates a substantial fraction of l1 dc accesses for loads, l1 dc tag checks on stores, and data translation lookaside buffer accesses for both loads and stores. our evaluation shows that the proposed dfc can reduce the data access energy by 42.5% and improve execution time by 4.2%.",
    "present_kp": [
      "filter cache"
    ],
    "absent_kp": [
      "speculation"
    ]
  },
  {
    "title": "analytical model for anomalous positive bias temperature instability in la-based hfo2 nfets based on independent characterization of charging components.",
    "abstract": "pbti improvement in hfo2 nfets achieved by a controlled insertion of la. anomalous negative ?vth due to charge exchange between high-k and metal gate. anomalous and conventional pbti components are decoupled and studied separately. analytical model including both components for lifetime extrapolation is presented.",
    "present_kp": [
      "bias temperature instability"
    ],
    "absent_kp": [
      "metaloxidesemiconductor field-effect transistor ",
      "hafnium oxide",
      "silicon oxide"
    ]
  },
  {
    "title": "a social behaviour evolution approach for evolutionary optimisation.",
    "abstract": "evolutionary algorithms were originally designed to locate basins of optimum solutions in a stationary environment. therefore, additional techniques and modifications have been introduced to deal with further requirements such as handling dynamic fitness functions or finding multiple optima. in this paper, we present a new approach for building evolutionary algorithms that is based on concepts borrowed from social behaviour evolution. algorithms built with the proposed paradigm operate on a population of individuals that move in the search space as they interact and form groups. the interaction follows a set of social behaviours evolved by each group to enhance its adaptation to the environment (and other groups) and to achieve different desirable goals such as finding multiple optima, maintaining diversity, or tracking a moving peak in a changing environment. each group has two sets of behaviours: one for intra-group interactions and one for inter-group interactions. these behaviours are evolved using mathematical models from the field of evolutionary game theory. this paper describes the proposed paradigm and starts studying it characteristics by building a new evolutionary algorithm and studying its behavior. the algorithm has been tested using a benchmark problem generator with promising initial results, which are also reported.",
    "present_kp": [
      "social behaviour evolution",
      "evolutionary optimisation",
      "evolutionary game theory",
      "evolutionary algorithms"
    ],
    "absent_kp": [
      "social adaptive groups",
      "dynamic optimisation problems"
    ]
  },
  {
    "title": "planar c1 hermite interpolation with ph cuts of degree (1,3) of laurent series.",
    "abstract": "we introduce a new class of ph curves, ph cuts of degree (1,3) of laurent series. we show how to find ph skew cut interpolants to a c1 hermite data-set. we show that two of these interpolants are short, simple curves with stable shape. our curves are fair with different shapes to those of other interpolants. we can obtain regular ph interpolants for collinear c1 hermite data-sets.",
    "present_kp": [
      "c1 hermite interpolation",
      "ph skew cut",
      "ph skew cut interpolant"
    ],
    "absent_kp": [
      "pythagorean hodograph  curve",
      "complex representation",
      "cut of degree of a laurent series"
    ]
  },
  {
    "title": "bicepstrum based blind identification of the acoustic emission (ae) signal in precision turning.",
    "abstract": "it is believed that the acoustic emissions (ae) signal contains potentially valuable information for monitoring precision cutting processes, as well as to be employed as a control feedback signal. however, ae stress waves produced in the cutting zone are distorted by the transmission path and the measurement systems. in this article, a bicepstrum based blind system identification technique is proposed as a valid tool for estimating both, transmission path and sensor impulse response. assumptions under which application of bicepstrum is valid are discussed and diamond turning experiments are presented, which demonstrate the feasibility of employing bicepstrum for ae blind identification.",
    "present_kp": [
      "acoustic emissions",
      "blind identification"
    ],
    "absent_kp": [
      "higher-order statistics",
      "precision machining"
    ]
  },
  {
    "title": "on solving hierarchical problems with top down control.",
    "abstract": "we review recent work on the hierarchical-if-and-only-if problem and present a new hierarchical problem, hiff-m that does not fit with previous explanations for evolutionary difficulty on hierarchical problems decomposed by levels for rmhc2. rmhc2 is a hill climbing algorithm augmented with a multi-level selection scheme. when used with the \"ideal\" sieve for a problem, as is done in this paper, rmhc2 exerts top-down control on the evolutionary dynamics, in the sense that adaptation of higher levels are given priority over adaptation of lower levels, and creates stabilizing selection pressure with potential to increase evolvability. through hiff-m, we discovered that the summary statistic, fitness distance correlation by level, is not a reliable indicator of when a hierarchical problem is solvable by rmhc2, and that the two properties proposed to explain search easiness for rmhc2 are inadequate. our investigation of this anomaly led us to propose an additional property for hierarchical evolution difficulty under rmhc2: inter-level conflict. we also discuss how hierarchical control can be subverted through the information transfer capacity of the transposition operation.",
    "present_kp": [
      "hierarchical control",
      "transposition"
    ],
    "absent_kp": [
      "hierarchical test problems",
      "level decomposition"
    ]
  },
  {
    "title": "extracting semantic frames from thai medical-symptom unstructured text with unknown target-phrase boundaries.",
    "abstract": "due to the limitations of language-processing tools for the thai language, pattern-based information extraction from thai documents requires supplementary techniques. based on sliding-window rule application and extraction filtering, we present a framework for extracting semantic information from medical-symptom phrases with unknown boundaries in thai unstructured-text information entries. a supervised rule learning algorithm is employed for automatic construction of information extraction rules from hand-tagged training symptom phrases. two filtering components are introduced: one uses a classification model to predict rule application across a symptom-phrase boundary based on instantiation features of rule internal wildcards, the other uses weighted classification confidence to resolve conflicts arising from overlapping extractions. in our experimental study, we focus our attention on two basic types of symptom phrasal descriptions: one is concerned with abnormal characteristics of some observable entities and the other with human-body locations at which primitive symptoms appear. the experimental results show that the filtering components improve precision while preserving recall satisfactorily.",
    "present_kp": [
      "information extraction",
      "rule learning"
    ],
    "absent_kp": [
      "medical informatics"
    ]
  },
  {
    "title": "robust doa estimation for uncorrelated and coherent signals.",
    "abstract": "a new direction of arrival (doa) estimation method is introduced with arbitrary array geometry when uncorrelated and coherent signals coexist. the doas of uncorrelated signals are first estimated via subspace-based high resolution doa estimation technique. then a matrix that only contains the information of coherent signals can be formulated by eliminating the contribution of uncorrelated signals. finally a subspace block sparse reconstruction approach is taken for doa estimations of the coherent signals.",
    "present_kp": [
      "coherent signals",
      "direction of arrival",
      "sparse reconstruction"
    ],
    "absent_kp": []
  },
  {
    "title": "checkpoint allocation and release.",
    "abstract": "out-of-order speculative processors need a bookkeeping method to recover from incorrect speculation. in recent years, several microarchitectures that employ checkpoints have been proposed, either extending the reorder buffer or entirely replacing it. this work presents an in-dept-study of checkpointing in checkpoint-based microarchitectures, from the desired content of a checkpoint, via implementation trade-offs, and to checkpoint allocation and release policies. a major contribution of the article is a novel adaptive checkpoint allocation policy that outperforms known policies. the adaptive policy controls checkpoint allocation according to dynamic events, such as second-level cache misses and rollback history. it achieves 6.8% and 2.2% speedup for the integer and floating point benchmarks, respectively, and does not require a branch confidence estimator. the results show that the proposed adaptive policy achieves most of the potential of an oracle policy whose performance improvement is 9.8% and 3.9% for the integer and floating point benchmarks, respectively. we exploit known techniques for saving leakage power by adapting and applying them to checkpoint-based microarchitectures. the proposed applications combine to reduce the leakage power of the register file to about one half of its original value.",
    "present_kp": [
      "performance",
      "checkpoint",
      "rollback",
      "leakage"
    ],
    "absent_kp": [
      "design",
      "misprediction",
      "out-of-order execution",
      "early register release"
    ]
  },
  {
    "title": "a quadratic spline approximation using detail multi-layer for soft shadow generation in augmented reality.",
    "abstract": "implementation of shadows is crucial to enhancement of images in ar environments. without shadows, virtual objects would look floating over the scene resulting in unrealistic rendering of ar environments. casting hard shadows would provide only spatial information while soft shadows help improve realism of ar environments. several algorithms have been proposed to render realistic shadows which often incurred high computational costs. little attention has been directed towards the balanced trade-off between shadow quality and computational costs. in this study, two approaches are proposed: quadratic spline interpolation (qsi) to soften the outline of the shadow and detail multi-layer (dml) technique to optimize the volume of computations for the generation of soft shadows based on real light sources. qsi estimates boarder hard shadow samples while dml involves three main phases: real light sources estimation, soft shadow production and reduction of the complexity of 3-dimensional objects shadows. to be more precise, a reflective hemisphere is used to capture real light and to create an environment map. the median cut algorithm is implemented to locate the direction of real light sources on the environment map. subsequently, the original hard shadows are retrieved and a sample of multilayer hard shadows is produced where each layer has its unique size and colour. these layers overlap to produce soft shadows based on the real light sources directions. finally, the level of details (lod) algorithm is implemented to increase the efficiency of soft shadows by decreasing the complexity of vertex transformations. the proposed technique is tested using three samples of multilayer hard shadows with varying numbers of light sources generated from the median cut algorithm. the experimental results show that the proposed technique successfully produces realistic soft shadows at low computational costs.",
    "present_kp": [
      "augmented reality",
      "shadow generation",
      "soft shadows",
      "environment map"
    ],
    "absent_kp": [
      "reflective sphere"
    ]
  },
  {
    "title": "the enhanced optical coupling in a quantum well infrared photodetector based on a resonant mode of an airdielectricmetal waveguide.",
    "abstract": "the hybrid structure consisting of periodic gold stripes and an overlaying gold film is proposed to enhance the optical coupling of a quantum well infrared photodetector. an airdielectricmetal waveguide is formed when the hybrid structure is integrated on the top of the quantum well detector with the substrate being removed. finite difference time-domain method is used to numerically obtain the reflection spectrum and the field distribution of the waveguide. the results show that a strong electric field component is induced in parallel to the growth direction of quantum well when the waveguide resonant mode occurs at the detective wavelength of the quantum well infrared photodetector. the relationship between the structural parameters and the resonant wavelength is derived by using the effective refractive index method of the airdielectricmetal waveguide. a high coupling efficiency can be obtained and the performance of the qwip can be greatly improved.",
    "present_kp": [
      "periodic gold stripes",
      "quantum well infrared photodetector",
      "effective refractive index",
      "coupling efficiency"
    ],
    "absent_kp": [
      "airdielectricmetal waveguide resonance"
    ]
  },
  {
    "title": "redirection based recovery for mpls network systems.",
    "abstract": "to provide a reliable backbone network, fault tolerance should be considered in the network design. for a multiprotocol label switching (mpls) based backbone network, the fault-tolerant issue focuses on how to protect the traffic of a label switched paths (lsp) against node and link failures. in ietf, two well-known recovery mechanisms (protection switching and rerouting) have been proposed. to further enhance the fault-tolerant performance of the two recovery mechanisms, the proposed approach utilizes the failure-free lsps to transmit the traffic of the failed lsp (the affected traffic). to avoid affecting the original traffic of each failure-free lsp, the proposed approach applies the solution of the minimum cost flow to determine the amount of affected traffic to be transmitted by each failure-free lsp. for transmitting the affected traffic along a failure-free working lsp, ip tunneling technique is used. we also propose a permission token scheme to solve the packet disorder problem. finally, simulation experiments are performed to show the effectiveness of the proposed approach.",
    "present_kp": [
      "mpls",
      "fault tolerance",
      "label switched path",
      "affected traffic",
      "minimum cost flow"
    ],
    "absent_kp": []
  },
  {
    "title": "designing a cross-language comparison-shopping agent.",
    "abstract": "this research pertains to the design and development of a shopbot called webshopper+. this shopbot is intended to help customers find and compare e-tailers that market their wares using different languages. webshopper+ is built with a multilingual ontology to overcome the language barriers that arise with global e-commerce. this research proposes a semi-automatic method of constructing a multilingual ontology by using the formal concept analysis and association analysis. it also proposes an automatic method for the categorization of product data into predefined classes, with the aim of alleviating administrators' task load. additionally, a semantic search mechanism based on concept similarity is designed to assist customers in finding more desirable products. the experimental results show that these methods perform well and the shopbot can help customers find real bargains on the web and to find products that cannot be bought locally.",
    "present_kp": [
      "shopbot",
      "comparison-shopping",
      "ontology",
      "formal concept analysis"
    ],
    "absent_kp": [
      "semantic similarity"
    ]
  },
  {
    "title": "medical informaticsthe state of the art in the hospital authority.",
    "abstract": "since its inception in 1990, the hospital authority (ha) has strongly supported the development and implementation of information systems both to improve the delivery of care and to make better information available to managers. this paper summarizes the progress to date and discusses current and future developments. following the first two phases of the ha information technology strategy the basic infrastructural elements were laid in place. these included the foundation administrative and financial systems and databases; establishment of a wide area network linking all hospitals and clinics together; laboratory, radiology and pharmacy systems with access to results in the ward. a major push into clinical systems began in 1994 with the clinical management system (cms), which established a clinical workstation for use in both ward and ambulatory settings. the cms is now running at all major hospitals, and provides single logon access to almost all the electronically collected clinical data in the ha. the next phase of development is focussed on further support for clinical activities in the cms. key elements include the longitudinal electronic patient record (epr), clinical order entry, generic support for clinical reports, broadening the scope to include allied health and the rehabilitative phase, clinical decision support, an improved clinical documentation framework, sharing of clinical information with other health care providers and a comprehensive data repository for analysis and reporting purposes.",
    "present_kp": [],
    "absent_kp": [
      "hospital information systems",
      "clinical information systems",
      "hong kong"
    ]
  },
  {
    "title": "prioritization of potential candidate disease genes by topological similarity of proteinprotein interaction network and phenotype data.",
    "abstract": "we construct a reliable heterogeneous network by fusing multiple networks. we devise a random walk based algorithm on the reliable heterogeneous network. combining topological similarity with phenotype data helps to predict causal genes. the algorithm is still in good performance at low parameter values.",
    "present_kp": [
      "disease genes",
      "random walk",
      "topological similarity",
      "phenotype"
    ],
    "absent_kp": [
      "proteinprotein interaction networks"
    ]
  },
  {
    "title": "extended beta regression in r: shaken, stirred, mixed, and partitioned.",
    "abstract": "beta regression -an increasingly popular approach for modeling rates and proportions - is extended in various directions: (a) bias correction/reduction of the maximum likelihood estimator, (b) beta regression tree models by means of recursive partitioning, (c) latent class beta regression by means of finite mixture models. all three extensions may be of importance for enhancing the beta regression toolbox in practice to provide more reliable inference and capture both observed and unobserved/latent heterogeneity in the data. using the analogy of smithson and verkuilen (2006), these extensions make beta regression not only \"a better lemon squeezer\" (compared to classical least squares regression) but a full-fledged modern juicer offering lemon-based drinks: shaken and stirred (bias correction and reduction), mixed (finite mixture model), or partitioned (tree model). all three extensions are provided in the r package betareg (at least 2.4-0), building on generic algorithms and implementations for bias correction/reduction, model-based recursive partioning, and finite mixture models, respectively. specifically, the new functions betatree () and betamix () reuse the object-oriented flexible implementation from the r packages party and flexmix, respectively.",
    "present_kp": [
      "beta regression",
      "bias correction",
      "recursive partitioning",
      "finite mixture",
      "r"
    ],
    "absent_kp": [
      "bias reduction"
    ]
  },
  {
    "title": "four-layer framework for combinatorial optimization problems domain.",
    "abstract": "four-layer framework for combinatorial optimization problems/models domain is suggested for applied problems structuring and solving: (1) basic combinatorial models and multicriteria decision making problems (e.g., clustering, knapsack problem, multiple choice problem, multicriteria ranking, assignment/allocation); (2) composite models/procedures (e.g., multicriteria combinatorial problems, morphological clique problem); (3) basic (standard) solving frameworks, e.g.: (i) hierarchical morphological multicriteria design (hmmd) (ranking, combinatorial synthesis based on morphological clique problem), (ii) multi-stage design (two-level hmmd), (iii) special multi-stage composite framework (clustering, assignment/location, multiple choice problem); and (4) domain-oriented solving frameworks, e.g.: (a) design of modular software, (b) design of test inputs for multi-function system testing, (c) combinatorial planning of medical treatment, (d) design and improvement of communication network topology, (e) multi-stage framework for information retrieval, (f) combinatorial evolution and forecasting of software, devices. the multi-layer approach covers decision cycle, i.e., problem statement, models, algorithms/procedures, solving schemes, decisions, decision analysis and improvement.",
    "present_kp": [
      "combinatorial optimization",
      "decision making"
    ],
    "absent_kp": [
      "problem solving environment",
      "system architecture",
      "problem structuring",
      "system design"
    ]
  },
  {
    "title": "computing monodromy via continuation methods on random riemann surfaces.",
    "abstract": "we consider a riemann surface x defined by a polynomial f (x, y) of degree d, whose coefficients are chosen randomly. hence, we can suppose that x is smooth, that the discriminant delta(x) of f has d(d - 1) simple roots, delta, and that delta(0) not equal 0, i.e. the corresponding fiber has d distinct points {y(1), ..., y(d)}. when we lift a loop 0 is an element of gamma subset of c - delta by a continuation method, we get d paths in x connecting {y(1), ..., y(d)}, hence defining a permutation of that set. this is called monodromy. here we present experimentations in maple to get statistics on the distribution of transpositions corresponding to loops around each point of delta. multiplying families of \"neighbor\" transpositions, we construct permutations and the subgroups of the symmetric group they generate. this allows us to establish and study experimentally two conjectures on the distribution of these transpositions and on transitivity of the generated subgroups. assuming that these two conjectures are true, we develop tools allowing fast probabilistic algorithms for absolute multivariate polynomial factorization, under the hypothesis that the factors behave like random polynomials whose coefficients follow uniform distributions.",
    "present_kp": [
      "random riemann surface",
      "continuation methods",
      "monodromy",
      "symmetric group",
      "algorithms"
    ],
    "absent_kp": [
      "bivariate polynomial",
      "plane curve",
      "absolute factorization",
      "algebraic geometry",
      "maple code"
    ]
  },
  {
    "title": "feature-based decision aggregation in modular neural network classifiers.",
    "abstract": "in several modular neural network (mnn) architectures, the individual decisions at the module level have to be integrated together using a voting scheme. all these voting schemes use the outputs of the individual modules to produce a global output without inferring explicit information from the problem feature space. this makes the choice of the aggregation procedure very subjective. in this work, a new mnn architecture will be presented. this architecture integrates learning into the voting scheme. we will be focusing on making the decision fusion a more dynamic process. in this context, dynamic means the aggregation procedure which has the flexibility to adapt to changes in the input. this approach requires the aggregation procedure to gather information about the input to help better understand how to dynamically aggregate decisions.",
    "present_kp": [],
    "absent_kp": [
      "classification",
      "classifier combination",
      "dynamic decision fusion",
      "modular neural networks"
    ]
  },
  {
    "title": "efficient bootstrap with weakly dependent processes.",
    "abstract": "the efficient bootstrap methodology is developed for overidentified moment conditions models with weakly dependent observation. the resulting bootstrap procedure is shown to be asymptotically valid and can be used to approximate the distributions of t-statistics, the j-statistic for overidentifying restrictions, and wald, lagrange multiplier and distance statistics for nonlinear hypotheses. the asymptotic validity of the efficient bootstrap based on a computationally less demanding approximate k-step estimator is also shown. the finite sample performance of the proposed bootstrap is assessed using simulations in an intertemporal consumption based asset pricing model.",
    "present_kp": [],
    "absent_kp": [
      "alpha-mixing",
      "consumption capm",
      "gel",
      "gmm",
      "hypothesis testing"
    ]
  },
  {
    "title": "multi-relay cooperative diversity protocol with improved spectral efficiency.",
    "abstract": "cooperative diversity protocols have attracted a great deal of attention since they are thought to be capable of providing diversity multiplexing tradeoff among single antenna wireless devices. in the high signal to noise ratio (snr) region, cooperation is rarely required; hence, the spectral efficiency of the cooperative protocol can be improved by applying a proper cooperation selection technique. in this paper, we present a simple \"cooperation selection\" technique based on instantaneous channel measurement to improve the spectral efficiency of cooperative protocols. we show that the same instantaneous channel measurement can also be used for relay selection. in this paper two protocols are proposed-proactive and reactive; the selection of one of these protocols depends on whether the decision of cooperation selection is made before or after the transmission of the source. these protocols can successfully select cooperation along with the best relay from a set of available m relays. if the instantaneous source to destination channel is strong enough to support the system requirements, then the source simply transmits to the destination as a noncooperative direct transmission; otherwise, a cooperative transmission with the help of the selected best relay is chosen by the system. analysis and simulation results show that these protocols can achieve higher order diversity with improved spectral efficiency, i.e., a higher diversity-multiplexing tradeoff in a slow-fading environment.",
    "present_kp": [
      "cooperative diversity",
      "diversity-multiplexing tradeoff",
      "relay selection",
      "spectral efficiency"
    ],
    "absent_kp": [
      "fading channel",
      "outage probability"
    ]
  },
  {
    "title": "a motion-tolerant dissolve detection algorithm.",
    "abstract": "gradual shot change detection is one of the most important research issues in the field of video indexing/retrieval. among the numerous types of gradual transitions, the dissolve-type gradual transition is considered the most common one, but it is also the most difficult one to detect. in most of the existing dissolve detection algorithms, the false/miss detection problem caused by motion is very serious. in this paper, we present a novel dissolve-type transition detection algorithm that can correctly distinguish dissolves from disturbance caused by motion. we carefully model a dissolve based on its nature and then use the model to filter out possible confusion caused by the effect of motion. experimental results show that the proposed algorithm is indeed powerful.",
    "present_kp": [
      "dissolve detection",
      "shot change detection"
    ],
    "absent_kp": [
      "fade detection"
    ]
  },
  {
    "title": "gmm-based evaluation of emotional style transformation in czech and slovak.",
    "abstract": "in the development of the voice conversion and the emotional speech style transformation in the text-to-speech systems, it is very important to obtain feedback information about the users opinion on the resulting synthetic speech quality. for this reason, the evaluations of the quality of the produced synthetic speech must often be performed for comparison. the main aim of the experiments described in this paper was to find out whether the classifier based on gaussian mixture models (gmms) could be applied for evaluation of male and female resynthesized speech that had been transformed from neutral to four emotional states (joy, surprise, sadness, and anger) spoken in czech and slovak languages. we suppose that it is possible to combine this gmm-based statistical evaluation with the classical one in the form of listening tests or it can replace them. for verification of our working hypothesis, a simple gmm emotional speech classifier with a one-level structure was realized. the next task of the performed experiment was to investigate the influence of different types and values (mean, median, standard deviation, relative maximum, etc.) of the used speech features (spectral and/or supra-segmental) on the gmm classification accuracy. the obtained gmm evaluation scores are compared with the results of the conventional listening tests based on the mean opinion scores. in addition, correctness of the gmm classification is analyzed with respect to the influence of the setting of the parameters during the gmm trainingthe number of mixture components and the types of speech features. the paper also describes the comparison experiment with the reference speech corpus taken from the berlin database of emotional speech in german language as the benchmark for the evaluation of the performance of our one-level gmm classifier. the obtained results confirm practical usability of the developed gmm classifier, so we will continue in this research with the aim to increase the classification accuracy and compare it with other approaches like the support vector machines.",
    "present_kp": [],
    "absent_kp": [
      "emotional speech transformation",
      "spectral and prosodic features of speech",
      "gmm-based emotion classification"
    ]
  },
  {
    "title": "collage of two-dimensional words.",
    "abstract": "we consider a new operation on one-dimensional (resp. two-dimensional) word languages, obtained by piling up, one on top of the other, words of a given recognizable language (resp. two-dimensional recognizable language) on a previously empty one-dimensional (resp. two-dimensional) array. the resulting language is the set of words \"seen from above\": a position in the array is labeled by the topmost letter. we show that in the one-dimensional case, the language is always recognizable. this is no longer true in the two-dimensional case which is shown by a counter-example, and we investigate in which particular cases the result may still hold.",
    "present_kp": [],
    "absent_kp": [
      "regular languages",
      "picture languages"
    ]
  },
  {
    "title": "inference management, trust and obfuscation principles for quality of information in emerging pervasive environments.",
    "abstract": "the emergence of large scale, distributed, sensor-enabled, machine-to-machine pervasive applications necessitates engaging with providers of information on demand to collect the information, of varying quality levels, to be used to infer about the state of the world and decide actions in response. in these highly fluid operational environments, involving information providers and consumers of various degrees of trust and intentions, information transformation, such as obfuscation, is used to manage the inferences that could be made to protect providers from misuses of the information they share, while still providing benefits to their information consumers. in this paper, we develop the initial principles for relating to inference management and the role that trust and obfuscation plays in it within the context of this emerging breed of applications. we start by extending the definitions of trust and obfuscation into this emerging application space. we, then, highlight their role as we move from the tightly-coupled to loosely-coupled sensory-inference systems and describe how quality, value and risk of information relate in collaborative and adversarial systems. next, we discuss quality distortion illustrated through a human activity recognition sensory system. we then present a system architecture to support an inference firewall capability in a publish/subscribe system for sensory information and conclude with a discussion and closing remarks.",
    "present_kp": [
      "quality of information",
      "risk of information",
      "obfuscation",
      "inference management"
    ],
    "absent_kp": [
      "value of information",
      "qoi",
      "voi",
      "roi"
    ]
  },
  {
    "title": "digging in the digg social news website.",
    "abstract": "the rise of social media aggregating websites provides platforms where users can actively publish, evaluate, and disseminate content in a collaborative way. in this paper, we present a large-scale empirical study about \"digg.com\", one of the biggest social media aggregating websites. our analysis is based on crawls of 1.5 million users and 10 million published stories on digg. we study the distinct network structure, the collaborative user characteristics, and the content dissemination process on digg. we empirically illustrate that friendship relations are used effectively in disseminating half of the content, although there exists a high overlap between the interests of friends. a successful content dissemination process can also be performed by random users who are browsing and digging stories. since 88% of the published content on digg is defined as news, it is important for the content to obtain sufficient votes in a short period of time before becoming obsolete. finally, we show that the synchronization of users' activities in time is the key to a successful content dissemination process. the dynamics between users' voting activities consequently decrease the efficiency of friendship relations during content dissemination. the results presented in this paper define basic observations and measurements to understand the underlying mechanism of disseminating content in current online social news aggregators. these findings are helpful to understand the influence of service interfaces and user behaviors on content dissemination.",
    "present_kp": [
      "content dissemination",
      "friendship relations",
      "user characteristics"
    ],
    "absent_kp": [
      "social media website"
    ]
  },
  {
    "title": "usage of agents in document management.",
    "abstract": "extensible java-based agent framework (xjaf) is a pluggable architecture of the hierarchical intelligent agents system with communication based on kqml. workers, inc. is a workflow management system implemented using mobile agents. it is especially suited for highly distributed and heterogeneous environments. the application of the above-mentioned systems will be considered in the area of document management systems.",
    "present_kp": [
      "mobile agents",
      "document management"
    ],
    "absent_kp": [
      "workflow management systems"
    ]
  },
  {
    "title": "ezpal: environment for composing constraint axioms by instantiating templates.",
    "abstract": "many ontology-development tools allow users to supplement frame-based representations with arbitrary logical sentences. however, few users actually take advantage of this opportunity. for example, in the ontolingua ontology library, only 20% of the ontologies have any user-defined axioms. we believe the difficulty of composing axioms primarily accounts for the lack of axioms in these knowledge bases: many domain experts cannot translate their thoughts into abstract and symbolic representations. we attempt to remedy the difficulties by identifying groups of axioms that manifest common patterns, creating templates that allow users to compose axioms by filling in the blanks. we studied axioms in two public ontology libraries, and derived 20 templates that cover 85% of all the user-defined axioms. we describe our methodology for identifying the templates and present examples. we constructed an interface that allows users to create constraints on knowledge bases by filling in blanks; our usability testing shows that users could use templates to encode axioms with a success rate similar to that of experts writing directly in an axiom language. our approach should foster the introduction of axioms and constraints that are currently missing in many ontologies.",
    "present_kp": [],
    "absent_kp": [
      "frame-based system",
      "knowledge acquisition",
      "knowledge representation"
    ]
  },
  {
    "title": "critical infrastructure dependencies: a holistic, dynamic and quantitative approach.",
    "abstract": "the proper functioning of critical infrastructures is crucial to societal well-being. however, critical infrastructures are not isolated, but instead are tightly coupled, creating a complex system of interconnected infrastructures. dependencies between critical infrastructures can cause a failure to propagate from one critical infrastructure to other critical infrastructures, aggravating and prolonging the societal impact. for this reason, critical infrastructure operators must understand the complexity of critical infrastructures and the effects of critical infrastructure dependencies. however, a major problem is posed by the fact that detailed information about critical infrastructure dependencies is highly sensitive and is usually not publicly available. moreover, except for a small number of holistic and dynamic research efforts, studies are limited to a few critical infrastructures and generally do not consider time-dependent behavior. this paper analyzes how a failed critical infrastructure that cannot deliver products and services impacts other critical infrastructures, and how a critical infrastructure is affected when another critical infrastructure fails. the approach involves a holistic analysis involving multiple critical infrastructures while incorporating a dynamic perspective based on the time period that a critical infrastructure is non-operational and how the impacts evolve over time. this holistic approach, which draws on the results of a survey of critical infrastructure experts from several countries, is intended to assist critical infrastructure operators in preparing for future crises.",
    "present_kp": [
      "critical infrastructure dependencies"
    ],
    "absent_kp": [
      "holistic treatment",
      "dynamic analysis",
      "quantitative analysis"
    ]
  },
  {
    "title": "traffic distribution for end-to-end qos routing with multicast multichannel services.",
    "abstract": "with the development of multimedia group applications and multicasting demands, the construction of multicast routing tree satisfying quality of service (qos) is more important. a multicast tree, which is constructed by existing multicast algorithms, suffers three major weaknesses: (1) it cannot be constructed by multichannel routing, transmitting a message using all available links, thus the data traffic cannot be preferably distributed; (2) it does not formulate duplication capacity; consequently, duplication capacity in each node cannot be optimally distributed; (3) it cannot change the number of links and nodes used optimally. in fact, it cannot employ and cover unused backup multichannel paths optimally. to overcome these weaknesses, this paper presents a polynomial time algorithm for distributed optimal multicast routing and quality of service (qos) guarantees in networks with multichannel paths which is called distributed optimal multicast multichannel routing algorithm (dommr). the aim of this algorithm is: (1) to minimize end-to-end delay across the multichannel paths, (2) to minimize consumption of bandwidth by using all available links, and (3) to maximize data rate by formulating network resources. dommr is based on the linear programming formulation (lpf) and presents an iterative optimal solution to obtain the best distributed routes for traffic demands between all edge nodes. computational experiments and numerical simulation results will show that the proposed algorithm is more efficient than the existing methods. the simulation results are obtained by applying network simulation tools such as qsb, opnet and matlb to some samples of network. we then introduce a generalized problem, called the delay-constrained multicast multichannel routing problem, and show that this generalized problem can be solved in polynomial time.",
    "present_kp": [
      "multicasting",
      "multichannel path",
      "traffic distribution",
      "linear programming"
    ],
    "absent_kp": [
      "optimized routing",
      "quality of services"
    ]
  },
  {
    "title": "improved error exponent for time-invariant and periodically time-variant convolutional codes.",
    "abstract": "an improved upper bound on the error probability (first error event) of time-invariant convolutional codes, and the resulting error exponent, is derived ill this paper. the improved error bound depends on both the delay of the code k and its width (the number of symbols that enter the delay line in parallel) b. determining the error exponent of time-invariant convolutional codes is an open problem. while the previously known bounds on the error probability of time-invariant codes led to the block-coding exponent, obtain a better error exponent (strictly better for b > 1). in the limit b --> infinity our error exponent equals the yudkin-viterbi exponent derived for time-variant convolutional codes. these results are also used to derive an improved error exponent for periodically time-variant codes.",
    "present_kp": [
      "convolutional codes",
      "error exponent",
      "error probability",
      "periodically time-variant codes",
      "time-invariant codes",
      "yudkin-viterbi exponent"
    ],
    "absent_kp": []
  },
  {
    "title": "online reputation management for improving marketing by using a hybrid mcdm model.",
    "abstract": "online reputation management (orm) has been considered as a significant tool of internet marketing. the purpose of this paper is to construct a decision model for evaluating performances and improving professional services of marketing. to investigate the interrelationship and influential weights among criteria, this study uses a hybrid mcdm model including decision-making trial and evaluation laboratory (dematel), dematel-based analytic network process (called danp). the empirical findings reveal that criteria have self-effect relationships based on dematel technique. according to the network relation map (nrm), the dimension that professional services of marketing should improve first when carrying out orm is online reputation. in the five criteria for evaluation, distributed reputation systems is the most important criterion impacting orm, followed by employees and social responsibility.",
    "present_kp": [
      "online reputation management ",
      "professional services of marketing",
      "dematel",
      "danp",
      "mcdm"
    ],
    "absent_kp": []
  },
  {
    "title": "feasibility of a primarily digital research library.",
    "abstract": "this position paper explores the issues related to the feasibility of having a primarily digital research library support the teaching and research needs of a university. the asian university for women (auw), a new university in chittagong, bangladesh, will open in september 2009. it must make a decision regarding the investment to be made in research resources to support the university. mass digitization efforts now make it possible to consider establishing a research library that consists primarily of digital resources rather than print. there are, however, many issues that make this consideration quite complex and far from certain. in this paper we explore the issues at a preliminary level. we focus on four broad perspectives in order to begin addressing the complex interactions that must be considered in transitioning to a primarily digital research environment: technical, economic, policy and social issues. the purpose of this paper is to begin to explore a research agenda for transitioning from a model for libraries where resources are primarily print to one that is predominantly digital. our research in this area is just beginning, so our purpose is to raise the issues rather than offer firm conclusions.",
    "present_kp": [
      "libraries",
      "digital research",
      "mass digitization"
    ],
    "absent_kp": [
      "digital libraries"
    ]
  },
  {
    "title": "targeting multiple myeloma cells and their bone marrow microenvironment.",
    "abstract": "although multiple myeloma (mm) is sensitive to chemotherapy and radiation therapy, long-term disease-free survival is rare, and mm remains incurable despite conventional and high-dose therapies. direct (cell-cell contact) and soluble (via cytokines) forms of interactions between mm cells and bone marrow stroma regulate growth, survival, and homing of mm cells. these interactions also play a critical role in angiogenesis and in myeloma bone disease. in recent years, several studies have established the biologic significance of cytokines in mm pathogenesis and delineated signaling cascades mediating their effects, providing the framework for related novel therapies targeting not only the mm cell, but also the bone marrow microenvironment.",
    "present_kp": [
      "multiple myeloma",
      "bone marrow microenvironment",
      "novel therapies"
    ],
    "absent_kp": []
  },
  {
    "title": "a network service curve approach for the stochastic analysis of networks.",
    "abstract": "the stochastic network calculus is an evolving new methodology for backlog and delay analysis of networks that can account for statistical multiplexing gain. this paper advances the stochastic network calculus by deriving a network service curve, which expresses the service given to a flow by the network as a whole in terms of a probabilistic bound. the presented network service curve permits the calculation of statistical end-to-end delay and backlog bounds for broad classes of arrival and service distributions. the benefits of the derived service curve are illustrated for the exponentially bounded burstiness (ebb) traffic model. it is shown that end-to-end performance measures computed with a network service curve are bounded by o ( h log h ), where h is the number of nodes traversed by a flow. using currently available techniques that compute end-to-end bounds by adding single node results, the corresponding performance measures are bounded by o ( h 3 ).",
    "present_kp": [
      "network service curve",
      "stochastic network calculus"
    ],
    "absent_kp": [
      "quality-of-service"
    ]
  },
  {
    "title": "mitigating kinematic locking in the material point method.",
    "abstract": "the material point method exhibits kinematic locking when traditional linear shape functions are used with a rectangular grid. the locking affects both the strain and the stress fields, which can lead to inaccurate results and nonphysical behavior. this paper presents a new anti-locking approach that mitigates the accumulation of fictitious strains and stresses, significantly improving the kinematic response and the quality of all field variables. the technique relies on the huwashizu multi-field variational principle, with separate approximations for the volumetric and the deviatoric portions of the strain and stress fields. the proposed approach is validated using a series of benchmark examples from both solid and fluid mechanics, demonstrating the broad range of modeling possibilities within the mpm framework when combined with appropriate anti-locking techniques and algorithms.",
    "present_kp": [
      "material point method",
      "locking"
    ],
    "absent_kp": [
      "meshfree methods",
      "particle methods"
    ]
  },
  {
    "title": "computational dialectic and rhetorical invention.",
    "abstract": "this paper has three dimensions, historical, theoretical and social. the historical dimension is to show how the ciceronian system of dialectical argumentation served as a precursor to computational models of argumentation schemes such as araucaria and carneades. the theoretical dimension is to show concretely how these argumentation schemes reveal the interdependency of rhetoric and logic, and so the interdependency of the normative with the empirical. it does this by identifying points of disagreement in a dialectical format through using argumentation schemes and critical questions. the social dimension is to show how the ciceronian dialectical viewpoint integrates with the use of computational tools that can be used to support the principle of reason-based deliberation and facilitate deliberative democracy.",
    "present_kp": [
      "argumentation schemes",
      "deliberative democracy"
    ],
    "absent_kp": [
      "informal logic",
      "ciceronian rhetoric",
      "carneades model",
      "fallacies",
      "persuasion"
    ]
  },
  {
    "title": "the roadmaker's algorithm for the discrete pulse transform.",
    "abstract": "the discrete pulse transform (dpt) is a decomposition of an observed signal into a sum of pulses, i.e., signals that are constant on a connected set and zero elsewhere. originally developed for 1-d signal processing, the dpt has recently been generalized to more dimensions. applications in image processing are currently being investigated. the time required to compute the dpt as originally defined via the successive application of lulu operators (members of a class of minimax filters studied by rohwer) has been a severe drawback to its applicability. this paper introduces a fast method for obtaining such a decomposition, called the roadmaker's algorithm because it involves filling pits and razing bumps. it acts selectively only on those features actually present in the signal, flattening them in order of increasing size by sub-tracing an appropriate positive or negative pulse, which is then appended to the decomposition. the implementation described here covers 1-d signal as well as two and 3-d image processing in a single framework. this is achieved by considering the signal or image as a function defined on a graph, with the geometry specified by the edges of the graph. whenever a feature is flattened, nodes in the graph are merged, until eventually only one node remains. at that stage, a new set of edges for the same nodes as the graph, forming a tree structure, defines the obtained decomposition. the roadmaker's algorithm is shown to be equivalent to the dpt in the sense of obtaining the same decomposition. however, its simpler operators are not in general equivalent to the lulu operators in situations where those operators are not applied successively. a by-product of the roadmaker's algorithm is that it yields a proof of the so-called highlight conjecture, stated as an open problem in 2006. we pay particular attention to algorithmic details and complexity, including a demonstration that in the 1-d case, and also in the case of a complete graph, the roadmaker's algorithm has optimal complexity: it runs in time o(m), where m is the number of arcs in the graph.",
    "present_kp": [],
    "absent_kp": [
      "clustering algorithms",
      "digital filters",
      "digital signal processing",
      "discrete transforms",
      "multidimensional signal processing",
      "nonlinear filters",
      "signal analysis",
      "tree graphs"
    ]
  },
  {
    "title": "implementation relations and test generation for systems with distributed interfaces.",
    "abstract": "some systems interact with their environment at physically distributed interfaces called ports and we separately observe sequences of inputs and outputs at each port. as a result we cannot reconstruct the global sequence that occurred and this reduces our ability to distinguish different systems in testing or in use. in this paper we explore notions of conformance for an input output transition system that has multiple ports, adapting the widely used ioco implementation relation to this situation. we consider two different scenarios. in the first scenario the agents at the different ports are entirely independent. alternatively, it may be feasible for some external agent to receive information from more than one of the agents at the ports of the system, these local behaviours potentially being brought together and here we require a stronger implementation relation. we define implementation relations for these scenarios and prove that in the case of a single-port system the new implementation relations are equivalent to ioco. in addition, we define what it means for a test case to be controllable and give an algorithm that decides whether this condition holds. we give a test generation algorithm to produce sound and complete test suites. finally, we study two implementation relations to deal with partially specified systems.",
    "present_kp": [],
    "absent_kp": [
      "formal approaches to testing",
      "systems with distributed ports",
      "formal methodologies to develop distributed software systems"
    ]
  },
  {
    "title": "dual-centers type-2 fuzzy clustering framework and its verification and validation indices.",
    "abstract": "the clustering model considers dual-centers rather than single centers. the dual-centers type-2 clustering model and algorithm are proposed. the relations among parameters of the proposed model are explained. the degrees of belonging to the clusters are defined by type-2 fuzzy numbers. the verification and verification indices are developed for model evaluation.",
    "present_kp": [],
    "absent_kp": [
      "dual-centers clustering",
      "interval type-2 fuzzy clustering",
      "pcm",
      "cluster center uncertainty",
      "validation index",
      "verification index"
    ]
  },
  {
    "title": "generalized sharing in survivable optical networks.",
    "abstract": "shared path protection has been demonstrated to be a very efficient survivability scheme for optical networking. in this scheme, multiple backup paths can share a given optical channel if their corresponding primary routes are not expected to fail simultaneously. the focus in this area has been the optimization of the total channels (i.e., bandwidth) provisioned in the network through the intelligent routing of primary and backup routes. in this work, we extend the current path protection sharing scheme and introduce the generalized sharing concept. in this concept, we allow for additional sharing of important node devices. these node devices (e.g., optical-electronic-optical regenerators (oeos), pure all-optical converters, etc.) constitute the dominant cost factor in an optical backbone network and the reduction of their number is of paramount importance. for demonstration purposes, we extend the concept of 1:n shared path protection to allow for the sharing of electronic regenerators needed for coping with optical transmission impairments. both design and control plane issues are discussed through numerical examples. considerable cost reductions in electronic budget are demonstrated.",
    "present_kp": [
      "optical networks"
    ],
    "absent_kp": [
      "shared protection"
    ]
  },
  {
    "title": "on the usefulness of knowledge of error variances in the consistent estimation of an unreplicated ultrastructural model.",
    "abstract": "this article considers an unreplicated ultrastructural model and discusses the asymptotic properties of three consistent estimators of slope parameter arising from the knowledge of measurement error variances. conditions are deduced when knowing the error variances associated with both the study and the explanatory variables is more/less beneficial than using a single error variance in the formulation of slope estimators.",
    "present_kp": [
      "error variance",
      "ultrastructural model"
    ],
    "absent_kp": [
      "measurement errors",
      "reliability ratio"
    ]
  },
  {
    "title": "exact solution of the heat equation with boundary condition of the fourth kind by hes variational iteration method.",
    "abstract": "in this paper, solutions of the heat equation with the boundary condition of the fourth kind are presented. the proposed solution is based on hes variational iteration method, after the application of which the exact solution of the problem is obtained.",
    "present_kp": [
      "heat equation",
      "variational iteration method"
    ],
    "absent_kp": []
  },
  {
    "title": "efficient neighborhood search for the one-machine earlinesstardiness scheduling problem.",
    "abstract": "this paper addresses the one-machine scheduling problem where the objective is to minimize a sum of costs such as earlinesstardiness costs. since the sequencing problem is np-hard, local search is very useful for finding good solutions. unlike scheduling problems with regular cost functions, the scheduling (or timing) problem is not trivial when the sequence is fixed. therefore, the local search approaches must deal with both job interchanges in the sequence and the timing of the sequenced jobs. we present a new approach that efficiently searches in a large neighborhood and always returns a solution for which the timing is optimal.",
    "present_kp": [
      "scheduling",
      "earlinesstardiness cost"
    ],
    "absent_kp": [
      "single machine",
      "neighborhoods",
      "search method"
    ]
  },
  {
    "title": "power characteristics of inductive interconnect.",
    "abstract": "the width of an interconnect line affects the total power consumed by a circuit. the effect of wire sizing on the power characteristics of an inductive interconnect line is presented in this paper. the matching condition between the driver and the load affects the power consumption since the short-circuit power dissipation may decrease and the dynamic power will increase with wider lines. a tradeoff, therefore, exists between short-circuit and dynamic power in inductive interconnects. the short-circuit power increases with wider linewidths only if the line is underdriven. the power characteristics of inductive interconnects therefore may have a great influence on wire sizing optimization techniques. an analytic solution of the transition time of a signal propagating along an inductive interconnect with an error of less than 15% is presented. the solution is useful in wire sizing synthesis techniques to decrease the overall power dissipation. the optimum linewidth that minimizes the total transient power dissipation is determined. an analytic solution for the optimum width with an error of less than 6% is presented. for a specific set of line parameters and resistivities, a reduction in power approaching 80% is achieved as compared to the minimum wire width. considering the driver size in the design process, the optimum wire and driver size that minimizes the total transient power is also determined.",
    "present_kp": [
      "dynamic power",
      "inductive interconnect",
      "short-circuit power",
      "transient power dissipation"
    ],
    "absent_kp": [
      "characteristic impedance",
      "underdamped systems"
    ]
  },
  {
    "title": "convergence acceleration of rungekutta schemes for solving the navierstokes equations.",
    "abstract": "the convergence of a rungekutta (rk) scheme with multigrid is accelerated by preconditioning with a fully implicit operator. with the extended stability of the rungekutta scheme, cfl numbers as high as 1000 can be used. the implicit preconditioner addresses the stiffness in the discrete equations associated with stretched meshes. this rk/implicit scheme is used as a smoother for multigrid. fourier analysis is applied to determine damping properties. numerical dissipation operators based on the roe scheme, a matrix dissipation, and the cusp scheme are considered in evaluating the rk/implicit scheme. in addition, the effect of the number of rk stages is examined. both the numerical and computational efficiency of the scheme with the different dissipation operators are discussed. the rk/implicit scheme is used to solve the two-dimensional (2-d) and three-dimensional (3-d) compressible, reynolds-averaged navierstokes equations. turbulent flows over an airfoil and wing at subsonic and transonic conditions are computed. the effects of the cell aspect ratio on convergence are investigated for reynolds numbers between 5.7106 5.7 10 6 and 100106 100 10 6 . it is demonstrated that the implicit preconditioner can reduce the computational time of a well-tuned standard rk scheme by a factor between 4 and 10.",
    "present_kp": [
      "fourier analysis",
      "multigrid"
    ],
    "absent_kp": [
      "navier\u2013stokes",
      "runge\u2013kutta",
      "implicit preconditioning"
    ]
  },
  {
    "title": "realizations of the game domination number.",
    "abstract": "domination game is a game on a finite graph which includes two players. first player, dominator, tries to dominate a graph in as few moves as possible; meanwhile the second player, staller, tries to hold him back and delay the end of the game as long as she can. in each move at least one additional vertex has to be dominated. the number of all moves in the game in which dominator makes the first move and both players play optimally is called the game domination number and is denoted by (gamma _g). the total number of moves in a staller-start game is denoted by (gamma _g^{prime }). it is known that (|gamma _g(g)-gamma _g^{prime }(g)|le 1) for any graph (g). graph (g) realizes a pair ((k,l)) if (gamma _g(g)=k) and (gamma _g^{prime }(g)=l). it is shown that pairs ((2k,2k-1)) for all (kge 2) can be realized by a family of 2-connected graphs. we also present 2-connected classes which realize pairs ((k,k)) and ((k,k+1)). exact game domination number for combs and 1-connected realization of the pair ((2k+1,2k)) are also given.",
    "present_kp": [
      "domination game",
      "game domination number",
      "realizations"
    ],
    "absent_kp": []
  },
  {
    "title": "lumiproxy: a hybrid representation of image-based models.",
    "abstract": "in this paper, we present a hybrid representation of image-based models combining the textured planes and the hierarchical points. taking a set of depth images as input, our method starts from classifying input pixels into two categories, indicating the planar and non-planar surfaces respectively. for the planar surfaces, the geometric coefficients are reconstructed to form the uniformly sampled textures. for nearly planar surfaces, some textured planes, called lumiproxies, are constructed to represent the equivalent visual appearance. the hough transform is used to find the positions of these textured planes, and optic flow measures are used to determine their textures. for remaining pixels corresponding to the non-planar geometries, the point primitive is applied, reorganized as the obb-tree structure. then, texture mapping and point splatting are employed together to render the novel views, with the hardware acceleration.",
    "present_kp": [
      "lumiproxy"
    ],
    "absent_kp": [
      "sampling",
      "surface fitting",
      "image-based rendering"
    ]
  },
  {
    "title": "constraint based methods for biological sequence analysis.",
    "abstract": "the need for processing biological information is rapidly growing, owing to the masses of new information in digital form being produced at this time. old methodologies for processing it can no longer keep up with this rate of growth. the methods of artificial intelligence (ai) in general and of language processing in particular can offer much towards solving this problem. however, interdisciplinary research between language processing and molecular biology is not yet widespread, partly because of the effort needed for each specialist to understand the other one's jargon. we argue that by looking at the problems of molecular biology from a language processing perspective, and using constraint based logic methodologies we can shorten the gap and make interdisciplinary collaborations more effective. we shall discuss several sequence analysis problems in terms of constraint based formalisms such concept formation rules, constraint handling rules (chr) and their grammatical counterpart, chrg. we postulate that genetic structure analysis can also benefit from these methods, for instance to reconstruct from a given rna secondary structure, a nucleotide sequence that folds into it. our proposed methodologies lend direct executability to high level descriptions of the problems at hand and thus contribute to rapid while efficient prototyping.",
    "present_kp": [
      "rna secondary structure",
      "concept formation",
      "constraint handling rules"
    ],
    "absent_kp": [
      "protein structure",
      "gene prediction",
      "constraint handling rule grammars"
    ]
  },
  {
    "title": "adaptive load balancing algorithm for multiple homing mobile nodes.",
    "abstract": "in places where mobile users can access multiple wireless networks simultaneously, a multipath scheduling algorithm can benefit the performance of wireless networks and improve the experience of mobile users. however, existing literature shows that it may not be the case, especially for tcp flows. according to early investigations, there are mainly two reasons that result in bad performance of tcp flows in wireless networks. one is the occurrence of out-of-order packets due to different delays in multiple paths. the other is the packet loss which is resulted from the limited bandwidth of wireless networks. to better exploit multipath scheduling for tcp flows, this paper presents a new scheduling algorithm named adaptive load balancing algorithm (albam) to split traffic across multiple wireless links within the isp infrastructure. targeting at solving the two adverse impacts on tcp flows, albam develops two techniques. firstly, albam takes advantage of the bursty nature of tcp flows and performs scheduling at the flowlet granularity where the packet interval is large enough to compensate for the different path delays. secondly, albam develops a packet number estimation algorithm (pnea) to predict the buffer usage in each path. with pnea, albam can prevent buffer overflow and schedule the tcp flow to a less congested path before it suffers packet loss. simulations show that albam can provide better performance to tcp connections than its other counterparts.",
    "present_kp": [],
    "absent_kp": [
      "multiple path scheduling",
      "multiple interface",
      "local domain"
    ]
  },
  {
    "title": "query by output.",
    "abstract": "it has recently been asserted that the usability of a database is as important as its capability. understanding the database schema, the hidden relationships among attributes in the data all play an important role in this context. subscribing to this viewpoint, in this paper, we present a novel data-driven approach, called query by output (qbo) , which can enhance the usability of database systems. the central goal of qbo is as follows: given the output of some query q on a database d , denoted by q ( d ), we wish to construct an alternative query q ? such that q ( d ) and q ? ( d ) are instance-equivalent. to generate instance-equivalent queries from q ( d ), we devise a novel data classification-based technique that can handle the at-least-one semantics that is inherent in the query derivation. in addition to the basic framework, we design several optimization techniques to reduce processing overhead and introduce a set of criteria to rank order output queries by various notions of utility. our framework is evaluated comprehensively on three real data sets and the results show that the instance-equivalent queries we obtain are interesting and that the approach is scalable and robust to queries of different selectivities.",
    "present_kp": [
      "query by output",
      "at-least-one semantics",
      "instance-equivalent queries"
    ],
    "absent_kp": []
  },
  {
    "title": "three-dimensional quantitative structureactivity relationships study on hiv-1 reverse transcriptase inhibitors in the class of dipyridodiazepinone derivatives, using comparative molecular field analysis1.",
    "abstract": "a three-dimensional quantitative structureactivity relationships (3d qsar) method, comparative molecular field analysis (comfa), was applied to a set of dipyridodiazepinone (nevirapine) derivatives active against wild-type (wt) and mutant-type (y181c) hiv-1 reverse transcriptase. the starting geometry of dipyridodiazepinone was taken from x-ray crystallographic data. all 75 derivatives, divided into a training set of 53 compounds and a test set of 22 molecules, were then constructed and full geometrical optimizations were performed, based on a semiempirical molecular orbital method (am1). comfa was used to discriminate between structural requirements for wt and y181c inhibitory activities. the resulting comfa models yield satisfactory predictive ability regarding wt and y181c inhibitions, with r2cv = 0.624 and 0.726, respectively. comfa contour maps reveal that steric and electrostatic interactions corresponding to the wt inhibition amount to 58.5% and 41.5%, respectively, while steric and electrostatic effects have approximately equal contributions for the explanation of inhibitory activities against y181c. the contour maps highlight different characteristics for different types of wild-type and mutant-type hiv-1 rt. in addition, these contour maps agree with experimental data for the binding topology. consequently, the results obtained provide information for a better understanding of the inhibitorreceptor interactions of dipyridodiazepinone analogs. 2000 elsevier science inc.",
    "present_kp": [
      "hiv-1 rt",
      "nevirapine",
      "comfa"
    ],
    "absent_kp": [
      "nnrti",
      "3d-qsar",
      "quantum chemical calculations",
      "molecular modeling"
    ]
  },
  {
    "title": "a shared-memory implementation of the hierarchical radiosity method.",
    "abstract": "the radiosity method is a simulation method from computer graphics to visualize the global illumination in scenes containing diffuse objects within an enclosure. a variety of realizations (including parallel approaches) were proposed to achieve a high efficiency while guaranteeing the same accuracy of the graphical representation. the hierarchical radiosity method reduces the computational costs considerably but results in a highly irregular algorithm which makes a parallel implementation more difficult. we investigate a task-oriented shared memory implementation and present optimizations with different behavior concerning locality and granularity. to be able to concentrate on load balancing and scalability issues, we use a shared-memory machine with uniform memory access time, the sb-pram.",
    "present_kp": [
      "hierarchical radiosity method",
      "shared memory implementation",
      "scalability",
      "granularity"
    ],
    "absent_kp": [
      "task-parallelism"
    ]
  },
  {
    "title": "a transaction mapping algorithm for frequent itemsets mining.",
    "abstract": "in this paper, we present a novel algorithm for mining complete frequent itemsets. this algorithm is referred to as the tm ( transaction mapping) algorithm from hereon. in this algorithm, transaction ids of each itemset are mapped and compressed to continuous transaction intervals in a different space and the counting of itemsets is performed by intersecting these interval lists in a depth-first order along the lexicographic tree. when the compression coefficient becomes smaller than the average number of comparisons for intervals intersection at a certain level, the algorithm switches to transaction id intersection. we have evaluated the algorithm against two popular frequent itemset mining algorithms, fp-growth and declat, using a variety of data sets with short and long frequent patterns. experimental data show that the tm algorithm outperforms these two algorithms.",
    "present_kp": [
      "algorithms",
      "frequent itemsets"
    ],
    "absent_kp": [
      "association rule mining",
      "data mining"
    ]
  },
  {
    "title": "semi-continuous network flow problems.",
    "abstract": "we consider semi-continuous network flow problems, that is, a class of network flow problems where some of the variables are restricted to be semi-continuous. we introduce the semi-continuous inflow set with variable upper bounds as a relaxation of general semi-continuous network flow problems. two particular cases of this set are considered, for which we present complete descriptions of the convex hull in terms of linear inequalities and extended formulations. we consider a class of semi-continuous transportation problems where inflow systems arise as substructures, for which we investigate complexity questions. finally, we study the computational efficacy of the developed polyhedral results in solving randomly generated instances of semi-continuous transportation problems.",
    "present_kp": [
      "network flow problems",
      ""
    ],
    "absent_kp": [
      "mixed-integer programming",
      "semi-continuous variables"
    ]
  },
  {
    "title": "construct message authentication code with one-way hash functions and block ciphers.",
    "abstract": "we suggest an mac scheme which combines a hash function and an block cipher in order. we strengthen this scheme to prevent the problem of leaking the intermediate hash value between the hash function and the block cipher by additional random bits. the requirements to the used hash function are loosely. security of the proposed scheme is heavily dependent on the underlying block cipher. this scheme is efficient on software implementation for processing long messages and has clear security properties.",
    "present_kp": [
      "mac",
      "one-way hash function",
      "block cipher"
    ],
    "absent_kp": [
      "cryptography"
    ]
  },
  {
    "title": "estimation of uncertainty in dynamic simulation results.",
    "abstract": "this paper presents a new approach for calculation of uncertainty in dynamic simulation results. the statistical moments (mean, variance, skewness etc.) of the simulation results are calculated using gaussian-quadrature with ''customized'' weight function. based on these moments, an approximating probability density function (pdf) is created by expansion into orthogonal polynomial series. the percentiles of the distribution can then be calculated. the method is computationally less demanding than monte-carlo simulation when the number of uncertain parameters are limited. a number of examples are used to illustrate the applicability of the proposed framework.",
    "present_kp": [
      "dynamic simulation"
    ],
    "absent_kp": [
      "uncertainty propagation",
      "stochastic simulation"
    ]
  },
  {
    "title": "a neural implementation of the jade algorithm (njade) using higher-order neurons.",
    "abstract": "a neural implementation of the jade algorithm, called njade, is developed which adaptively determines the mixing matrices to be jointly diagonalized with the jade algorithm. this alleviates the problem of algebraically determining these mixing matrices which becomes a very tedious if not impossible undertaking with high-dimensional data. the new learning rule uses higher-order neurons and generalizes oja's pca learning rule. as a test case the new njade algorithm is applied to high-dimensional natural image ensembles to learn appropriate edge filter structures. quantitative comparison concerning various filter characteristics is made with results obtained with a probabilistic ica algorithm with kernel-based source density estimation.",
    "present_kp": [
      "njade"
    ],
    "absent_kp": [
      "independent component analysis",
      "higher order neurons",
      "neural network",
      "natural images"
    ]
  },
  {
    "title": "jpeg 2000 encoding method for reducing tiling artifacts.",
    "abstract": "this paper proposes an effective jpeg 2000 encoding method for reducing tiling artifacts, which cause one of the biggest problems in jpeg 2000 encoders. symmetric pixel extension is generally thought to be the main factor in causing artifacts. however this paper shows that differences in quantization accuracy between tiles are a more significant reason for tiling artifacts at middle or low bit rates. this paper also proposes an algorithm that predicts whether tiling artifacts will occur at a tile boundary in the rate control process and that locally improves quantization accuracy by the original post quantization control. this paper further proposes a method for reducing processing time which is yet another serious problem in the jpeg 2000 encoder. the method works by predicting truncation points using the entropy of wavelet transform coefficients prior to the arithmetic coding. these encoding methods require no additional processing in the decoder. the experiments confirmed that tiling artifacts were greatly reduced and that the coding process was considerably accelerated.",
    "present_kp": [
      "jpeg 2000",
      "tiling artifacts",
      "rate control"
    ],
    "absent_kp": [
      "acceleration of coding process"
    ]
  },
  {
    "title": "laparoscopic myomectomy.",
    "abstract": "the appearance of uterine myomas has been linked to infertility. it has been suggested that surgical management of myomas by laparoscopic myomectomy improves fertility rates in these group of patients. in this paper we initially describe specific aspects of the surgical technique of laparoscopic myomectomy including the set-up, precise technique for hysteroromy, enucleation of the myoma, suturing of the uterus, and extraction of the myoma. we detail recent findings that demonstrate improved fertility rates in women undergoing laparoscopic myomectomy. we recommend that, when criteria for selection of patients is strictly adhered to and patients present with no other associated infertility, laparoscopic myomectomy be used to increase the implantation rate.",
    "present_kp": [
      "laparoscopic myomectomy",
      "uterine myoma",
      "fertility"
    ],
    "absent_kp": [
      "laparotomy"
    ]
  },
  {
    "title": "theoretical study on the antioxidant properties of 2'-hydroxychalcones: h-atom vs. electron transfer mechanism.",
    "abstract": "the free radical scavenging activity of six 2'-hydroxychalcones has been studied in gas phase and solvents using the density functional theory (dft) method. the three main working mechanisms, hydrogen atom transfer (hat), stepwise electron-transfer-proton-transfer (et-pt) and sequential-proton-loss-electron-transfer (splet) have been considered. the o-h bond dissociation enthalpy (bde), ionization potential (ip), proton affinity (pa) and electron transfer energy (ete) parameters have been computed in gas phase and solvents. the theoretical results confirmed the important role of the b ring in the antioxidant properties of hydroxychalcones. in addition, the calculated results matched well with experimental values. the results suggested that hat would be the most favorable mechanism for explaining the radical-scavenging activity of hydroxychalcone in gas phase, whereas splet mechanism is thermodynamically preferred pathway in aqueous solution.",
    "present_kp": [
      "dft",
      "hydrogen atom transfer",
      "hydroxychalcones",
      "radical scavenging",
      "sequential-proton-loss-electron-transfer",
      "stepwise electron-transfer-proton-transfer"
    ],
    "absent_kp": []
  },
  {
    "title": "program analysis for event-based distributed systems.",
    "abstract": "designing distributed applications around the idiom of events has several benefits including extensibility and scalability. to improve conciseness, safety, and efficiency of corresponding programs, several authors have recently proposed programming languages or language extensions with support for event-based programming. the presence of a dedicated programming language and compilation process offers avenues for program analyses to further improve simplicity, safety, and expressiveness of distributed event-based software. this paper presents three program analyses specifically designed for event-based programs: immutability analysis avoids costly cloning of events in the presence of co-located handlers for same events; guard analysis allows for simple yet expressive subscriptions which can be further simplified and handled efficiently; causality analysis determines causal dependencies among events which are related, allowing unrelated events to be transferred independently for efficiency. we convey the benefits of our approach by empirically evaluating their performance benefits.",
    "present_kp": [
      "language",
      "program analysis",
      "event",
      "distributed"
    ],
    "absent_kp": [
      "correlation"
    ]
  },
  {
    "title": "automatic determination of envelopes and other derived curves within a graphic environment.",
    "abstract": "dynamic geometry programs provide environments where accurate construction of geometric configurations can be done. nevertheless, intrinsic limitations in their standard development technology mostly produce objects that are equationally unknown and so can not be further used in constructions. in this paper, we pursue the development of a geometric system that uses in the background the symbolic capabilities of two computer algebra systems, cocoa and mathematica. the cooperation between the geometric and symbolic modules of the software is illustrated by the computation of plane envelopes and other derived curves. these curves are described both graphically and analytically. since the equations of these curves are known, the system allows the construction of new elements depending on them.",
    "present_kp": [
      "dynamic geometry",
      "envelopes"
    ],
    "absent_kp": [
      "symbolic computing",
      "symbolic-numeric interface",
      "groebner bases",
      "caustics",
      "pedals"
    ]
  },
  {
    "title": "a platform for okapi-based contextual information retrieval.",
    "abstract": "we present an extensible java-based platform for contextual retrieval based on the probabilistic information retrieval model. modules for dual indexes, relevance feedback with blind or machine learning approaches and query expansion with context are integrated into the okapi system to deal with the contextual information. this platform allows easy extension to include other types of contextual information.",
    "present_kp": [
      "contextual information retrieval"
    ],
    "absent_kp": [
      "probabilistic model"
    ]
  },
  {
    "title": "the ternary description language as a formalism for the parametric general systems theory: part iii.",
    "abstract": "this part is a continuation of the first and second parts of my article that were published in the international journal of general systems, vol. 28 (4-5), pp. 351-366; vol. 31 (2), pp. 131-151. in part iii, we deal with the construction of the axiomatic system of the ternary description language (tdl). axioms and rules of inference are formulated. on the basis of these axioms and rules some theorems of tdl are proved. several system-theoretical laws, which concern the values of systems parameters, are proved as theorems of tdl. thus the deductive construction of general systems theory is made.",
    "present_kp": [
      "axioms",
      "system-theoretical laws"
    ],
    "absent_kp": [
      "syntactical priority",
      "synonymy",
      "rules of substitution",
      "rules of replacement",
      "theorems of the tdl"
    ]
  },
  {
    "title": "definitions and approaches to model quality in model-based software development - a review of literature.",
    "abstract": "more attention is paid to the quality of models along with the growing importance of modelling in software development. we performed a systematic review of studies discussing model quality published since 2000 to identify what model quality means and how it can be improved. from forty studies covered in the review, six model quality goals were identified; i.e., correctness, completeness, consistency, comprehensibility, confinement and changeability. we further present six practices proposed for developing high-quality models together with examples of empirical evidence. the contributions of the article are identifying and classifying definitions of model quality and identifying gaps for future research.",
    "present_kp": [
      "systematic review",
      "modelling",
      "model quality"
    ],
    "absent_kp": [
      "model-driven development",
      "uml"
    ]
  },
  {
    "title": "animal identification: introduction and history.",
    "abstract": "in the beginning of the 1970 research institutes in different countries developed the first electronic animal identification systems. these systems were tested on experimental farms. the first systems were all built with the conventional components and attached to a collar around the cows neck. in the 1980s however special integrated circuits were developed minimising the size of the transponders. now in the 1990s, official organisations are testing systems for identification and registration of all animals to control movements from birth to slaughterhouse. this will enable farm livestock to be traced at the outbreak of diseases and residues in slaughter animals to be followed up. injectable transponders, electronic eartags and rumenal bolusses are being used.",
    "present_kp": [
      "transponder",
      "electronic eartag"
    ],
    "absent_kp": [
      "electronic identification",
      "ruminant bolus"
    ]
  },
  {
    "title": "a fractional variational iteration method for solving fractional nonlinear differential equations.",
    "abstract": "recently, fractional differential equations have been investigated by employing the famous variational iteration method. however, all the previous works avoid the fractional order term and only handle it as a restricted variation. a fractional variational iteration method was first proposed in wu and gave a generalized lagrange multiplier. in this paper, two fractional differential equations are approximately solved with the fractional variational iteration method.",
    "present_kp": [
      "fractional variational iteration method"
    ],
    "absent_kp": [
      "modified riemannliouville derivative",
      "fractional corrected functional"
    ]
  },
  {
    "title": "three-level privacy control for sensing-based real-world content digital diorama.",
    "abstract": "digital diorama, the sensing-based real-world content, can be constructed by integrating real-time information obtained from sensors monitoring the real world. in order to increase the benefits of viewers without violating the privacy of monitored persons, this paper proposes three-level privacy control over the monitored persons based on their agreement to the usage of their information obtained from sensors: privacy control with i) no agreement, ii) partial agreement. and iii) mutual agreement. i) presents only their positions to show where persons are. ii) additionally presents the information which can be automatically obtained from sensors such as age and gender to show what kinds of persons are where without disclosing the visual appearances. iii) presents their visual appearances based on their mutual agreement with specific viewers. our evaluation indicated that the representation simulating each privacy control presented information from sensors with acceptable privacy protection.",
    "present_kp": [
      "privacy protection",
      "sensing-based real-world content",
      "benefits of viewers"
    ],
    "absent_kp": []
  },
  {
    "title": "enhancing wireless video streaming using lightweight approximate authentication.",
    "abstract": "in this paper we propose a novel lightweight approximate authentication algorithm that provides efficient protection for wireless video streaming where bit errors are frequent. the benefits of the proposed algorithm over other algorithms are fast execution, due to its simplicity, and small message authentication code size. the algorithm is capable of detecting even a small number of bit errors in relatively small packets that are used in video streaming. these features have never previously been available at the same time. another benefit of the approximate authentication is that it supports error resilient video decoding by dropping packets with too many bit errors, thus improving the perceived quality of the video stream. the performance of the algorithm is demonstrated via simulations and measurements",
    "present_kp": [
      "approximate authentication",
      "wireless video streaming"
    ],
    "absent_kp": [
      "error resilient video coding",
      "security",
      "qos"
    ]
  },
  {
    "title": "distributed federative qos resource management.",
    "abstract": "in a distributed multimedia system qos resources have to be managed carefully to utilize the resource pool in a way that bottlenecks can be avoided. our key idea is to let the applications participate on the resource management. we propose a distributed architecture with a fine granulated, balanced resource management with explicit qos characteristics. the architecture is based on a distributed cooperative resource manager which combines both the adaption and reservation principle for guaranteeing qos. we have designed and implemented a prototype of our federative qos resource manager (fqrm) in the java environment.",
    "present_kp": [
      "qos resource management"
    ],
    "absent_kp": [
      "distributed resources",
      "cooperative resource sharing"
    ]
  },
  {
    "title": "algorithms for on-line order batching in an order picking warehouse.",
    "abstract": "in manual order picking systems, order pickers walk or ride through a distribution warehouse in order to collect items required by (internal or external) customers. order batching consists of combining these indivisible customer orders into picking orders. with respect to order batching, two problem types can be distinguished: in off-line (static) batching, all customer orders are known in advance; in on-line (dynamic) batching, customer orders become available dynamically over time. this paper considers an on-line order batching problem in which the maximum completion time of the customer orders arriving within a certain time period has to be minimized. the author shows how heuristic approaches for off-line order batching can be modified in order to deal with the on-line situation. in a competitive analysis, lower and upper bounds for the competitive ratios of the proposed algorithms are presented. the proposed algorithms are evaluated in a series of extensive numerical experiments. it is demonstrated that the choice of an appropriate batching method can lead to a substantial reduction of the maximum completion time.",
    "present_kp": [
      "order picking",
      "order batching"
    ],
    "absent_kp": [
      "warehouse management",
      "on-line optimization"
    ]
  },
  {
    "title": "passive and active reduction techniques for on-chip high-frequency digital power supply noise.",
    "abstract": "signal integrity has become a major problem in digital ic design. one cause is device scaling that results in a sharp reduction of supply voltage, creating stringent noise margin requirements to ensure functionality. this paper introduces both a novel on-chip decoupling capacitance methodology and active noise cancellation (anc) structure. the decoupling methodology focuses on quantification and location. the anc structure, with an area of 50 mu m x 55 mu m, uses decoupling capacitance to sense noise and inject a proportional current into v(ss) as a method of reduction. a chip has been designed and fabricated using tsmc's 90-nm technology. measurements show that the decoupling methodology improved the average voltage headroom loss by 17% while the anc structure improved the average voltage headroom loss by 18%.",
    "present_kp": [
      "active noise cancellation ",
      "decoupling capacitance",
      "power supply noise"
    ],
    "absent_kp": [
      "on-chip interconnect",
      "power distribution"
    ]
  },
  {
    "title": "on modal mu-calculus over finite graphs with small components or small tree width.",
    "abstract": "this paper is a continuation and correction of a paper presented by the same authors at the conference gandalf 2010. we consider the modal mu-calculus and some fragments of it. for every positive integer k we consider the class scck of all finite graphs whose strongly connected components have size at most k, and the class twk of all finite graphs of tree width at most k. as upper bounds, we show that for every k, the temporal logic ctl* collapses to alternation free mu-calculus in scck; and in tw1, the winning condition for parity games of any index n belongs to the level delta(2) of modal mu-calculus. as lower bounds, we show that buchi automata are not closed under complement in tw2 and cobuchi nondeterministic and alternating automata differ in tw1.",
    "present_kp": [
      "modal mu-calculus",
      "strongly connected component",
      "tree width"
    ],
    "absent_kp": []
  },
  {
    "title": "product line selection and pricing analysis: impact of genetic relaxations.",
    "abstract": "a model for the product line selection and pricing problem (plsp) is presented and three solution procedures based on a genetic algorithm are developed to analyze the results based on consumer preference patterns. since the plsp model is nonlinear and integer, two of the solution procedures use genetic encoding to \"relax\" the np hard model. the relaxations result in linear integer and shortest path models for the fitness evaluation which are solved using branch and bound and labeling algorithms, respectively. performance of the quality of solutions generated by the procedures is evaluated for various problem sizes and customer preference structures. the results show that the genetic relaxations provide efficient and effective solution methodologies for the problem, when compared to the pure artificial intelligence technique of genetic search. the impact of the preference structure on the product line and the managerial implications of the solution characteristics generated by the genetic relaxations are also discussed. the models can be used to explicitly consider tradeoffs between marketing and operations concerns in designing a product line.",
    "present_kp": [
      "product line",
      "pricing"
    ],
    "absent_kp": [
      "heuristics",
      "genetic algorithms"
    ]
  },
  {
    "title": "a decomposed-model predictive functional control approach to air-vehicle pitch-angle control.",
    "abstract": "the requirements for the pitch-angle control of an air vehicle are a very fast response with as few vibrations as possible. the vibrations can damage the equipment that is carried within the body of the vehicle. the main problem to deal with is the relatively fast and under damped dynamics of the vehicle and the slow actuators and sensors. we have solved the problem by using a predictive approach. the main idea of this approach is a process output prediction based on a decomposed process model. the decomposition enables the extension of the model-based approach to processes with integrative behavior such as in the case of a rocket's pitch-angle control. the proposed approach is not only useful in this case but it gives us a framework to design the control for a wide range of processes. we compared the predictive design methodology with the classical compensator control approach, known from aerospace system control. the advantage of the new approach is the reduced vibrations during the transient response.",
    "present_kp": [
      "vibrations"
    ],
    "absent_kp": [
      "compensation",
      "decomposition methods",
      "modelling",
      "predictive control"
    ]
  },
  {
    "title": "the impact of electronic medical record systems on outpatient workflows: a longitudinal evaluation of its workflow effects.",
    "abstract": "the promise of the electronic medical record (emr)1 lies in its ability to reduce the costs of health care delivery and improve the overall quality of care a promise that is realized through major changes in workflows within the health care organization. yet little systematic information exists about the workflow effects of emrs. moreover, some of the research to-date points to reduced satisfaction among physicians after implementation of the emr and increased time, i.e., negative workflow effects. a better understanding of the impact of the emr on workflows is, hence, vital to understanding what the technology really does offer that is new and unique. (i) to empirically develop a physician centric conceptual model of the workflow effects of emrs; (ii) to use the model to understand the antecedents to the physicians workflow expectation from the new emr; (iii) to track physicians satisfaction overtime, 3 months and 20 months after implementation of the emr; (iv) to explore the impact of technology learning curves on physicians reported satisfaction levels. the current research uses the mixed-method technique of concept mapping to empirically develop the conceptual model of an emr's workflow effects. the model is then used within a controlled study to track physician expectations from a new emr system as well as their assessments of the emr's performance 3 months and 20 months after implementation. the research tracks the actual implementation of a new emr within the outpatient clinics of a large northeastern research hospital. the pre-implementation survey netted 20 physician responses; post-implementation time 1 survey netted 22 responses, and time 2 survey netted 26 physician responses. the implementation of the actual emr served as the intervention. since the study was conducted within the same setting and tracked a homogenous group of respondents, the overall study design ensured against extraneous influences on the results. outcome measures were derived empirically from the conceptual model. they included 85 items that measured physician perceptions of the emr's workflow effect on the following eight issues: (1) administration, (2) efficiency in patient processing, (3) basic clinical processes, (4) documentation of patient encounter, (5) economic challenges and reimbursement, (6) technical issues, (7) patient safety and care, and (8) communication and confidentiality. the items were used to track expectations prior to implementation and they served as retrospective measures of satisfaction with the emr in post-implementation time 1 and time 2. the findings suggest that physicians conceptualize emrs as an incremental extension of older computerized provider order entries (cpoes) rather than as a new innovation. the emrs major functional advantages are seen to be very similar to, if not the same as, those of cpoes. technology learning curves play a statistically significant though minor role in shaping physician perceptions. the physicians expectations from the emr are based on their prior beliefs rather than on a rational evaluation of the emr's fit, functionality, or performance. their decision regarding the usefulness of the emr is made very early, within the first few months of use of the emr. these early perceptions then remain stable and become the lens through which subsequent experience with the emr is interpreted. the findings suggest a need for communication based interventions aimed at explaining the value, fit, and usefulness of emrs to physicians early in the pre- and immediate post-emr implementation stages.",
    "present_kp": [],
    "absent_kp": [
      "physicians adoption of emrs",
      "diffusion of technology",
      "health information technology",
      "work flow effects",
      "outpatient emr use"
    ]
  },
  {
    "title": "application of the lattice boltzmann method to flow in aneurysm with ring-shaped stent obstacles.",
    "abstract": "to resolve the characteristics of a highly complex flow, a lattice boltzmann method with an extrapolation boundary technique was used in aneurysms with and without transverse objects oil the upper wall, and results were compared with the non-stented aneurysm. the extrapolation boundary concept allows the use of cartesian grids even when the boundaries do not conform to cartesian coordinates. to case the code development and facilitate the incorporation of new physics, a new scientific programming strategy based on object-oriented concepts was developed. the reduced flow, smaller vorticity magnitude and wall shear stress, and smaller du/dy near the dome of the aneurysm were observed when the proposed stent obstacles were used. the height of the stent obstacles was more effective to reduce the vorticity near the dome of the aneurysm than the width of the stent. the rectangular stent with 20% height-of-vessel radius was observed to be optimal and decreased the magnitude of the vorticity by 21% near the dome of the aneurysm.",
    "present_kp": [
      "aneurysm",
      "lattice boltzmann method",
      "stent"
    ],
    "absent_kp": [
      "computational fluid dynamics",
      "object-oriented program"
    ]
  },
  {
    "title": "simplification rules for the coherent probability assessment problem.",
    "abstract": "in this paper we develop a procedure for checking the consistency (coherence) of a partial probability assessment. the general problem (called cpa) is np-complete, hence, to have a reasonable application some heuristic is needed. our proposal differs from others because it is based on a skilful use of the logical relations present among the events. in other approaches the consistency problem is reduced directly to the satisfiability of a system of linear constraints. here, thanks to the characterization of particular configurations and to the elimination of variables, an instance of the problem is reduced to smaller instances. to obtain such results, we introduce a procedure based on rules resembling those given by davis-putnam for the satisfiability of boolean formulas. at the end a particularized description of an actual implementation is given.",
    "present_kp": [
      "coherent probability assessment",
      "simplification rules"
    ],
    "absent_kp": [
      "probabilistic satisfiability"
    ]
  },
  {
    "title": "constructing special k-dominating sets using variations on the greedy algorithm.",
    "abstract": "this paper focuses on the efficient selection of a special type of subset of network nodes, which we call a k-spr set, for the purpose of coordinating the routing of messages through a network. such a set is a special k-hop-connected k-dominating set that has an additional property that promotes the regular occurrence of routers in all directions. the distributed algorithms introduced here for obtaining a k-spr set require that each node broadcast at most three messages to its k-hop neighbors. these transmissions can be made asynchronously. the time required to send these messages and the sizes of the resulting sets are compared by means of data collected from simulations. the main contribution is the adaptation of some variations of the distributed greedy algorithms to the problem of generating a small k-spr set. these variations are much faster than the standard distributed greedy algorithm. yet, when used with a sensible choice for a certain parameter, our empirical evidence strongly suggests that the resulting set size will generally be very close to the set size for the standard greedy algorithms.",
    "present_kp": [
      "distributed greedy algorithm",
      "routing",
      "dominating set"
    ],
    "absent_kp": [
      "ad hoc network"
    ]
  },
  {
    "title": "the convergence test of transformation performance of resource cities in china considering undesirable output.",
    "abstract": "the main challenge for sustainable development of resource cities is to work out a feasible strategy for transformation processes. this paper introduces a new approach for analysis of transformation performance. using the environmental production technology and a malmquist resource performance index (mrpi), we conduct sigma, absolute beta and conditional beta convergence tests for the transformation performance of 21 resource cities in china. the results show that mrpi does not follow the same trend as economic strength of three chinese regions. in addition, the transformation performance results exhibit a convergence trend for the 21 resource cities.",
    "present_kp": [
      "resource cities",
      "transformation performance",
      "convergence test"
    ],
    "absent_kp": []
  },
  {
    "title": "ant colony optimization based clustering methodology.",
    "abstract": "a novel aco based methodology (aco-c) is proposed for spatial clustering. it works in data sets with no a priori information. it includes solution evaluation, neighborhood construction and data set reduction. it has a multi-objective framework, and yields a set of non-dominated solutions. experimental results show that aco-c outperforms other competing approaches.",
    "present_kp": [
      "clustering",
      "ant colony optimization",
      "data set reduction"
    ],
    "absent_kp": [
      "multiple objectives"
    ]
  },
  {
    "title": "breaching euclidean distance-preserving data perturbation using few known inputs.",
    "abstract": "we examine euclidean distance-preserving data perturbation as a tool for privacy-preserving data mining. such perturbations allow many important data mining algorithms (e.g. hierarchical and k-means clustering), with only minor modification, to be applied to the perturbed data and produce exactly the same results as if applied to the original data. however, the issue of how well the privacy of the original data is preserved needs careful study. we engage in this study by assuming the role of an attacker armed with a small set of known original data tuples (inputs). little work has been done examining this kind of attack when the number of known original tuples is less than the number of data dimensions. we focus on this important case, develop and rigorously analyze an attack that utilizes any number of known original tuples. the approach allows the attacker to estimate the original data tuple associated with each perturbed tuple and calculate the probability that the estimation results in a privacy breach. on a real 16-dimensional dataset, we show that the attacker, with 4 known original tuples, can estimate an original unknown tuple with less than 7% error with probability exceeding 0.8.",
    "present_kp": [
      "euclidean distance",
      "privacy",
      "data mining",
      "data perturbation"
    ],
    "absent_kp": []
  },
  {
    "title": "consistent interactive augmentation of live camera images with correct near-field illumination.",
    "abstract": "inserting virtual objects in real camera images with correct lighting is an active area of research. current methods use a high dynamic range camera with a fish-eye lens to capture the incoming illumination. the main problem with this approach is the limitation to distant illumination. therefore, the focus of our work is a real-time description of both near - and far-field illumination for interactive movement of virtual objects in the camera image of a real room. the daylight, which is coming in through the windows, produces a spatially varying distribution of indirect light in the room; therefore a near-field description of incoming light is necessary. our approach is to measure the daylight from outside and to simulate the resulting indirect light in the room. to accomplish this, we develop a special dynamic form of the irradiance volume for real-time updates of indirect light in the room and combine this with importance sampling and shadow maps for light from outside. this separation allows object movements with interactive frame rates (10--17 fps). to verify the correctness of our approach, we compare images of synthetic objects with real objects.",
    "present_kp": [],
    "absent_kp": [
      "global illumination",
      "augmented image synthesis"
    ]
  },
  {
    "title": "dynamic programming based approximation algorithms for sequence alignment with constraints.",
    "abstract": "given two sequences x and y, the classical dynamic programming solution to the local alignment problem searches for two subsequences i subset of or equal to x and j subset of or equal to y with maximum similarity score under a given scoring scheme. in several applications, variants of this problem arise with different objectives and with length constraints on the subsequences i and j. this constraint can be explicit, such as requiring i + j greater than or equal to t, or j < t, or may be implicit such as in cyclic sequence comparison, or as in the maximization of length-normalized scores, and driven by practical considerations. we present a survey of approximation algorithms for various alignment problems with constraints, and several new approximation algorithms. these approximations are in two distinct senses: in one the constraints are satisfied but the score computed is within a prescribed tolerance of the optimum instead of the exact optimum. in another, the alignment returned is assured to have at least the optimum score with respect to the given constraints, but the length constraints are satisfied to within a prescribed tolerance from the required values. the algorithms proposed involve applications of techniques from fractional programming and dynamic programming.",
    "present_kp": [
      "local alignment",
      "cyclic sequence comparison",
      "approximation algorithm",
      "dynamic programming",
      "fractional programming"
    ],
    "absent_kp": [
      "normalized local alignment",
      "length-restricted local alignment",
      "ratio maximization"
    ]
  },
  {
    "title": "a model of multisecond timing behaviour under peak-interval procedures.",
    "abstract": "in this study, the authors developed a fundamental theory of interval timing behaviour, inspired by the learning-to-time (let) model and the scalar expectancy theory (set) model, and based on quantitative analyses of such timing behaviour. our experiments used the peak-interval procedure with rats. the proposed model of timing behaviour comprises clocks, a regulator, a mixer, a response, and memory. using our model, we calculated the basic clock speeds indicated by the subjects behaviour under such peak procedures. in this model, the scalar property can be defined as a kind of transposition, which can then be measured quantitatively. the akaike information criterion (aic) values indicated that the current model fit the data slightly better than did the set model. our model may therefore provide a useful addition to set for the analysis of timing behaviour.",
    "present_kp": [
      "akaike information criterion",
      "basic clock speed",
      "peak procedure",
      "scalar property",
      "scalar expectancy theory"
    ],
    "absent_kp": [
      "learning-to-time model"
    ]
  },
  {
    "title": "the support vector machine under test.",
    "abstract": "support vector machines (svms) are rarely benchmarked against other classification or regression methods. we compare a popular svm implementation (libsvm) to 16 classification methods and 9 regression methodsall accessible through the software rby the means of standard performance measures (classification error and mean squared error) which are also analyzed by the means of bias-variance decompositions. svms showed mostly good performances both on classification and regression tasks, but other methods proved to be very competitive.",
    "present_kp": [
      "benchmark",
      "support vector machines",
      "regression",
      "classification"
    ],
    "absent_kp": [
      "comparative study"
    ]
  },
  {
    "title": "using cascade method for table access on small devices.",
    "abstract": "users increasingly expect access to web data from a wide range of devices, both wired and wireless. the goal of our research is to inform the design of applications that support data access by providing reasonably seamless migration of web data among internet-compatible devices with minimal loss of effectiveness and efficiency. this study focuses on the tables of data on small mobile devices. in this paper we report on the results of a user study that compare effectiveness, efficiency and preference of two methods for the display and use of tables on small screens: column/row expansion and cascade, a cell based expansion method.",
    "present_kp": [
      "small screen",
      "tables"
    ],
    "absent_kp": [
      "handheld device",
      "pda",
      "auto-transformation",
      "focus + context"
    ]
  },
  {
    "title": "practical use of polynomials over the reals in proofs of termination.",
    "abstract": "nowadays, polynomial interpretations are an essential ingredient in the development of tools for proving termination. we have recently proven that polynomial interpretations over the reals are strictly better for proving polynomial termination of rewriting than those which only use integer coefficients. some essential aspects of their practical use, though, remain unexplored or underdeveloped. in this paper, we compare the two current frameworks for using polynomial intepretations over the reals and show that one of them is strictly better than the other, thus making a suitable choice for implementations. we also prove that the use of algebraic real co-efficients in the interpretations suffice for termination proofs. we also discuss the use of algorithms and techniques from tarski's first-order logic of the real closed fields for implementing their use in proofs of termination. we argue that more standard constraint-solving techniques are better suited for this. we propose an algorithm to solve the polynomial constraints which arise when specific finite subsets of rational (or even algebraic real) numbers are considered for giving value to the coefficients. we provide a preliminary experimental evaluation of the algorithm which has been implemented as part of the termination tool mu-term.",
    "present_kp": [
      "termination"
    ],
    "absent_kp": [
      "program analysis",
      "term rewriting",
      "polynomial orderings"
    ]
  },
  {
    "title": "detecting coherent energy.",
    "abstract": "we apply the mathematics of cognitive radio to a single receiver to obtain a new coherent energy metric. this allows us to derive the time correlation law separating gaussian colored noise from coherent signal energy.",
    "present_kp": [
      "cognitive radio",
      "coherent energy",
      "colored noise"
    ],
    "absent_kp": [
      "random matrix theory"
    ]
  },
  {
    "title": "optimal, quality-aware scheduling of data consumption in mobile ad hoc networks.",
    "abstract": "in this paper we study the delivery of quality contextual information in mobile ad-hoc networks. we consider that information has a certain quality level that fades over time. mobile context-aware applications receive and process disseminated information given that the corresponding quality is above the lowest level. the necessity for optimally scheduling information delivery arises from the dynamic nature of the network, e.g., probabilistic spreading, caching, deferred delivery, and mobility of nodes. we propose two policies for optimal scheduling information delivery consumption based on the optimal stopping theory. the mobile nodes delay the reporting of information to mobile context-aware applications in search for better quality. the proposed policies efficiently deal with the delivery of quality information in mobile ad-hoc networks.",
    "present_kp": [
      "mobile ad-hoc networks",
      "optimal stopping theory"
    ],
    "absent_kp": [
      "quality information delivery"
    ]
  },
  {
    "title": "partnering enhanced-nlp with semantic analysis in support of information extraction.",
    "abstract": "information extraction using natural language processing (nlp) tools focuses on extracting explicitly stated information from textual material. this includes named entity recognition (ner), which produces entities and some of the relationships that may exist among them. intelligent analysis requires examining the entities in the context of the entire document. while some of the relationships among the recognized entities may be preserved during extraction, the overall context of a document may not be preserved. in order to perform intelligent analysis on the extracted information, we provide an ontology, which describes the domain of the extracted information, in addition to rules that govern the classification and interpretation of added elements. the ontology is at the core of an interactive system that assists analysts with the collection, extraction, organization, analysis and retrieval of information, with the topic of \"terrorism financing\" as a case study. user interaction provides valuable assistance in assigning meaning to extracted information. the system is designed as a set of tools to provide the user with the flexibility and power to ensure accurate inference. this case study demonstrates the information extraction features as well as the inference power that is supported by the ontology.",
    "present_kp": [
      "semantic analysis",
      "information extraction",
      "nlp",
      "natural language processing",
      "intelligent analysis",
      "ontology"
    ],
    "absent_kp": [
      "modeling",
      "owl"
    ]
  },
  {
    "title": "on stabilization of gradient-based training strategies for computationally intelligent systems.",
    "abstract": "this paper develops a novel training methodology for computationally intelligent systems utilizing gradient information in parameter updating. the devised scheme uses the first-order dynamic model of the training procedure and applies the variable structure systems approach to control the training dynamics. this results in an optimal selection of the learning rate, which is continually updated as prescribed by the adopted strategy. the parameter update rule is then mixed with the conventional error backpropagation method in a weighted average. the paper presents an analysis of the imposed dynamics, which is the response of the training dynamics driven solely by the inputs designed by variable structure control approach. the analysis continues with the global stability proof of the mixed training methodology and the restrictions on the design parameters. the simulation studies presented are focused on the advantages of the proposed scheme with regards to the compensation of the adverse effects of the environmental disturbances and its capability to alleviate the inherently nonlinear behavior of the system under investigation. the performance of the scheme is compared with that of a conventional backpropagation, it is observed that the method presented is robust under noisy observations and time varying parameters due to the integration of gradient descent technique with variable structure systems methodology, in the application example studied, control of a two degrees of freedom direct-drive robotic manipulator is considered. a standard fuzzy system is chosen as the controller in which the adaptation is carried out only on the defuzzifier parameters.",
    "present_kp": [
      "gradient descent",
      "variable structure systems"
    ],
    "absent_kp": [
      "fuzzy control",
      "stable training"
    ]
  },
  {
    "title": "improving recruit distribution decisions in the us marine corps.",
    "abstract": "the united states marine corps (usmc) accomplishes its mission to put the right marine in the right place at the right time with the right skills and quality of life in various ways. one of these is a recruit distribution modeling (rdm) and information system that assigns new recruits to entry-level schools, thereby determining the entire career paths. this article proposes improvements to the existing marine corps decision processes and information systems for recruit distribution. the proposed system, recruit distribution decision support system (rddss), provides intuitive navigation through a hierarchy of switchboards, and promotes data integrity by eliminating manual data entry for data already available in the system. it incorporates four objective measures for understanding the quality of proposed distributions, and allows the user to generate and compare multiple solutions based on the trade-off between these objectives. it is a fully functional working prototype system that was installed into the usmc manpower environment, and demonstrated to provide several improvements over the current technology.",
    "present_kp": [
      "decision support system",
      "recruit distribution"
    ],
    "absent_kp": [
      "manpower modeling"
    ]
  },
  {
    "title": "boundary effects on the soil water characteristic curves obtained from lattice boltzmann simulations.",
    "abstract": "pore-scale simulations using a lattice boltzmann method (lbm)-based numerical model were conducted to examine how the capillary pressure (pc) ( p c ) and saturation (s ) evolve within a virtual porous medium subjected to drainage and imbibition cycles. the results show the presence of a sharp front (interface separating the wetting and non-wetting fluids) across the cell during the test, which expectably moves up and down as the controlling non-wetting fluid pressure at the upper boundary varies to simulate different pc p c levels over the drainage and imbibition cycle. this phenomenon, representing inhomogeneity at the simulated scale, is in conflict with the homogenization applied to the pressure cell for deriving the constitutive pcs p c s relationship. different boundary conditions, adopted to achieve more homogeneous states in the virtual soil, resulted in different pcs p c s curves. no unique relationship between pc p c and s , even with the interfacial area (anw) ( a nw ) included, could be found. this study shows dependence of the lbm-predicted pcs p c s relation on the chosen boundary conditions. this effect should be taken into account in future numerical studies of multiphase flow within porous media.",
    "present_kp": [],
    "absent_kp": [
      "lattice boltzmann methods",
      "unsaturated soil physics"
    ]
  },
  {
    "title": "a generic sampling framework for improving anomaly detection in the next generation network.",
    "abstract": "the heterogeneous nature of network traffic in next generation networks (ngns) may impose scalability issue to traffic monitoring applications. while this issue can be well addressed by existing sampling approaches, owing to their inherent 'lossy' characteristic and data reduction principle, traditional sampling techniques suffer from incomplete traffic statistics, which can lead to inaccurate inferences of the network traffic. by focusing on two distinct traffic monitoring applications, namely, anomaly detection and traffic measurement, we highlight the possibility of addressing the accuracy of both applications without having to sacrifice one for the sake of the other. in light of this, we propose a generic sampling framework, which is capable of providing creditable network traffic statistics for accurate anomaly detection in the non, while at the same time preserves the principal purpose of sampling (i.e., to sample dominant traffic flows for accurate traffic measurement), and thus addressing the accuracy of both applications concurrently. with the emphasize on the accuracy of anomaly detection and the scalability of monitoring devices, the performance evaluation over real network traces demonstrates the superiority of the proposed framework over traditional sampling techniques.",
    "present_kp": [
      "sampling framework",
      "accuracy",
      "scalability",
      "anomaly detection",
      "traffic measurement"
    ],
    "absent_kp": [
      "next generation network "
    ]
  },
  {
    "title": "the (k)-separator problem: polyhedra, complexity and approximation results.",
    "abstract": "given a vertex-weighted undirected graph (g=(v,e,w)) and a positive integer (k), we consider the (k)-separator problem: it consists in finding a minimum-weight subset of vertices whose removal leads to a graph where the size of each connected component is less than or equal to (k). we show that this problem can be solved in polynomial time for some graph classes including bounded treewidth, (m k_2)-free, ((g_1, g_2, g_3, p_6))-free, interval-filament, asteroidal triple-free, weakly chordal, interval and circular-arc graphs. polyhedral results with respect to the convex hull of the incidence vectors of (k)-separators are reported. approximation algorithms are also presented.",
    "present_kp": [
      "approximation algorithms",
      "polyhedra"
    ],
    "absent_kp": [
      "graph partitioning",
      "complexity theory",
      "optimization"
    ]
  },
  {
    "title": "structural and electronic properties of z isomers of (4 alpha -> 6 '',2 alpha -> o -> 1 '')-phenylflavans substituted with r=h, oh and och3 calculated in aqueous solution with pcm solvation model.",
    "abstract": "in the search for new antioxidants, flavan structures called our attention, as substructures of many important natural compounds, including catechins (flavan-3-ols), simple and dimeric proanthocyanidins, and condensed tannins. in this work the conformational space of the z-isomers of (4 alpha -> 6 '', 2 alpha -> o -> 1 '')-phenylflavans substituted with r=h, oh and och3 was scanned in aqueous solution, simulating the solvent by the polarizable continuum model (pcm). geometry optimizations were performed at b3lyp/6-31 g** level. electronic distributions were analyzed at a better calculation level, thus improving the basis set (6-311++g**). a topological study based on bader's theory (atoms in molecules) and natural bond orbital (nbo) framework was performed. furthermore, molecular electrostatic potential maps (meps) were obtained and thoroughly analyzed. the stereochemistry was discussed, and the effect of the solvent was addressed. moreover, intrinsic properties were identified, focusing on factors that may be related to their antioxidant properties. hyperconjugative and inductive effects were described. the coordinated nbo/aim analysis allowed us to rationalize the changes of meps in a polar solvent. to investigate the molecular and structural properties of these compounds in biological media, the polarizabilities and dipolar moments were predicted which were further used to enlighten stability and reactivity properties. all conformers were taken into account. relevant stereoelectronic aspects were described for understanding the stabilization and antioxidant function of these structures.",
    "present_kp": [
      "-phenylflavans",
      "antioxidants",
      "atoms in molecules"
    ],
    "absent_kp": [
      "aqueous solvent effect",
      "molecular dipole moment",
      "natural bond orbital analysis"
    ]
  },
  {
    "title": "re-thinking metaphor, experience and aesthetic awareness.",
    "abstract": "purpose - the purpose of this paper is to explore current questions about metaphor, experience and aesthetic awareness that persist through the variations of critical approaches and projective research in architectural theory and practice. design/methodology/approach - further considerations focus on the advanced technological possibilities which re-invest the relations between principles of cybernetics and architecture. findings - the current between art and architecture is more than ever manifested in fields related to the computer sciences and its conceptual background: cybernetic sciences. originality/value - the paper re-thinks the aesthetic value of architecture and architectural experience in this time of digital productivity.",
    "present_kp": [
      "cybernetics",
      "architecture",
      "experience",
      "metaphor"
    ],
    "absent_kp": [
      "aesthetics",
      "technology"
    ]
  },
  {
    "title": "agent technologies for sensor networks.",
    "abstract": "the development of agent technologies for sensor networks has received increasing research attention within both the sensor network and multi-agent systems research communities. the international workshops on agent technologies for sensor networks (atsn) held in 2007, 2008 and 2009 sought to bring these communities together, and this special issue of the computer journal presents extended versions of some of the papers that appeared at these workshops, along with new submissions specifically for this journal.",
    "present_kp": [
      "agent technologies",
      "sensor networks",
      "networks"
    ],
    "absent_kp": []
  },
  {
    "title": "optimal consensus of fuzzy opinions under group decision making environment.",
    "abstract": "the gist of this paper is to propose a new method for aggregating individual fuzzy opinions into an optimal group consensus. by optimality, we mean the sum of weighted dissimilarity among aggregated consensus and individual opinions is minimized. we propose an iterative procedure for approximating the optimal consensus of expert opinions. finally, the importance of each expert is taken into consideration in the process of aggregation.",
    "present_kp": [],
    "absent_kp": [
      "fuzzy individual opinions",
      "fuzzy opinions aggregation",
      "group consensus opinion",
      "fuzzy numbers",
      "multi-criteria decision making"
    ]
  },
  {
    "title": "multiscale bagging and its applications.",
    "abstract": "we propose multiscale bagging as a modification of the bagging procedure. in ordinary bagging, the bootstrap resampling is used for generating bootstrap samples. we replace it with the multiscale bootstrap algorithm. in multiscale bagging, the sample size in of bootstrap samples may be altered from the sample size n of learning dataset. for assessing the output of a classifier, we compute bootstrap probability of class label; the frequency of observing a specified class label in the outputs of classifiers learned from bootstrap samples. a scaling-law of bootstrap probability with respect to sigma(2) = n/m has been developed in connection with the geometrical theory. we consider two different ways for using multiscale bagging of classifiers. the first usage is to construct a confidence set of class labels, instead of a single label. the second usage is to find. inputs close to decision boundaries in the context of query by bagging for active learning. it turned out, interestingly, that an appropriate choice of m is m = -n, i.e., sigma(2) = -1, for the first usage, and m = infinity, i.e., sigma(2) = 0, for the second usage.",
    "present_kp": [
      "bagging",
      "active learning"
    ],
    "absent_kp": [
      "confidence level",
      "classification"
    ]
  },
  {
    "title": "estimation of lower and upper bounds on the power consumption from scheduled data flow graphs.",
    "abstract": "in this paper, we present an approach for the calculation of lower and upper bounds on the power consumption of data path resources like functional units, registers, i/o ports, and busses from scheduled data flow graphs executing a specified input data stream. the low power allocation and binding problem is formulated, first, it is shown that this problem without constraining the number of resources can be relaxed to the bipartite weighted matching problem which is solvable in o(n)(3). n is the number of arithmetic operations, variables, i/o-access or bus-access operations which have to be bound to data path resources, in a second step we demonstrate that the relaxation can be efficiently extended by including lagrange multipliers in the problem formulation to handle a resource constraint, the estimated bounds take into account the effects of resource sharing. the technique can be used, for example, to prune the design space in high-level synthesis for low power before the allocation and binding of the resources. the application of the technique on benchmarks with real application input data shows the tightness of the bounds.",
    "present_kp": [
      "high-level synthesis"
    ],
    "absent_kp": [
      "bound estimation",
      "low-power",
      "power dissipation",
      "power estimation",
      "switching activity"
    ]
  },
  {
    "title": "the effects of perceived risk and technology type on users acceptance of technologies.",
    "abstract": "previous studies on technology adoption disagree regarding the relative magnitude of the effects of perceived usefulness and perceived ease of use. however these studies did not consider moderating variables. we investigated four potential moderating variables perceived risk, technology type, user experience, and gender in users technology adoption. their moderating effects were tested in an empirical study of 161 subjects. results showed that perceived risk, technology type, and gender were significant moderating variables. however the effects of user experience were marginal after the variance of errors was removed.",
    "present_kp": [
      "perceived risk",
      "technology type",
      "user experience",
      "gender",
      "moderating variable"
    ],
    "absent_kp": [
      "technology acceptance",
      "utaut"
    ]
  },
  {
    "title": "estimation of process parameter variations in a pre-defined process window using a latin hypercube method.",
    "abstract": "the aim of this paper is to present a methodology that provides an analytical tool for estimation of robustness and response variation within a pre-defined process window. to exemplify the developed methodology, the stochastic simulation technique is used for a sheet-metal forming application. a sampling plan based on the latin hypercube sampling method for variation of design parameters is utilized, and the thickness reduction is specified as the response. moreover, the response surface methodology is applied for understanding the quantitative relationship between design parameters and response value. the conclusions of this study are that the applied method gives a possibility to illustrate and interpret the variation of the response versus a design parameter variation. consequently, it gives significant insights into the usefulness of individual design parameters. it has been shown that the method enables us to estimate the admissible design parameter variations and to predict the actual safe margin for given process parameters. furthermore, the dominating design parameters can be predicated using sensitivity analysis, and this in its turn clarifies how the reliability criteria are met. finally, the developed software can be used as an additional module for set-up of stochastic finite element simulations and to collect the numerical results from different solvers within different applications.",
    "present_kp": [
      "sheet-metal forming"
    ],
    "absent_kp": [
      "stochastical analysis",
      "sensitivity indicator",
      "admissible process parameter variation",
      "finite element method"
    ]
  },
  {
    "title": "architecture and applications of the fingermouse: a smart stereo camera for wearable computing hci.",
    "abstract": "in this paper we present a visual input hci system for wearable computers, the fingermouse. it is a fully integrated stereo camera and vision processing system, with a specifically designed asic performing stereo block matching at 5mpixel/s (e.g. qvga 320240at 30fps) and a disparity range of 47, consuming 187mw (78mw in the asic). it is button-sized (43mm18mm) and can be worn on the body, capturing the users hand and processing in real-time its coordinates as well as a 1-bit image of the hand segmented from the background. alternatively, the system serves as a smart depth camera, delivering foreground segmentation and tracking, depth maps and standard images, with a processing latency smaller than 1ms. this paper describes the fingermouse functionality and its applications, and how the specific architecture outperforms other systems in size, latency and power consumption.",
    "present_kp": [
      "wearable computing",
      "foreground segmentation",
      "hci"
    ],
    "absent_kp": [
      "stereo vision",
      "mobile embedded vision",
      "hand tracking"
    ]
  },
  {
    "title": "wireless sensor networking for rain-fed farming decision support.",
    "abstract": "wireless sensor networks (wsns) can be a valuable decision-support tool for farmers. this motivated our deployment of a wsn system to support rain-fed agriculture in india. we defined promising use cases and resolved technical challenges throughout a two-year deployment of our common-sense net system, which provided farmers with environment data. however, the direct use of this technology in the field did not foster the expected participation of the population. this made it difficult to develop the intended decision-support system. based on this experience, we take the following position in this paper: currently, the deployment of wsn technology in developing regions is more likely to be effective if it targets scientists and technical personnel as users, rather than the farmers themselves. we base this claim on the lessons learned from the common-sense system deployment and the results of an extensive user experiment with agriculture scientists, which we describe in this paper.",
    "present_kp": [
      "user experiment",
      "agriculture",
      "wireless sensor network"
    ],
    "absent_kp": [
      "developing country"
    ]
  },
  {
    "title": "on stable parametric finite element methods for the stefan problem and the mullinssekerka problem with applications to dendritic growth.",
    "abstract": "we introduce a parametric finite element approximation for the stefan problem with the gibbsthomson law and kinetic undercooling, which mimics the underlying energy structure of the problem. the proposed method is also applicable to certain quasi-stationary variants, such as the mullinssekerka problem. in addition, fully anisotropic energies are easily handled. the approximation has good mesh properties, leading to a well-conditioned discretization, even in three space dimensions. several numerical computations, including for dendritic growth and for snow crystal growth, are presented.",
    "present_kp": [
      "stefan problem",
      "mullinssekerka problem",
      "kinetic undercooling",
      "gibbsthomson law",
      "dendritic growth",
      "snow crystal growth"
    ],
    "absent_kp": [
      "surface tension",
      "anisotropy",
      "parametric finite elements"
    ]
  },
  {
    "title": "binomial moments of the distance distribution: bounds and applications.",
    "abstract": "we study a combinatorial invariant of codes which counts the number of ordered pairs of codewords in all subcodes of restricted support in a code. this invariant can be expressed as a linear form of the components of the distance distribution of the code with binomial numbers as coefficients. for this reason we call it a binomial moment of the distance distribution. binomial moments appear in the proof of the macwilliams identities and in many other problems of combinatorial coding theory. we introduce a linear programming problem for bounding these linear forms from below. it turns out that some known codes (1-error-correcting perfect codes, golay codes, nordstrom-robinson code, etc.) yield optimal solutions of this problem, i.e., have minimal possible binomial moments of the distance distribution. we derive several general feasible solutions of this problem, which give lower bounds on the binomial moments of codes with given parameters, and derive the corresponding asymptotic bounds. applications of these bounds include new lower bounds on the probability of undetected error for binary codes used over the binary-symmetric channel with crossover probability p and optimality of many codes for error detection. asymptotic analysis of the bounds enables us to extend the range of code rates in which the upper bound on the undetected error exponent is tight.",
    "present_kp": [
      "binomial moments",
      "distance distribution",
      "linear programming",
      "undetected error"
    ],
    "absent_kp": [
      "extremal codes",
      "rodemich theorem"
    ]
  },
  {
    "title": "characteristics of wap traffic.",
    "abstract": "this paper considers the characteristics of wireless application protocol (wap) traffic. we start by constructing a wap traffic model by analysing the behaviour of users accessing public wap sites via a monitoring system. a wide range of different traffic scenarios were considered, but most of these scenarios resolve to one of two basic types. the paper then uses this traffic model to consider the effects of large quantities of wap traffic on the core network. one traffic characteristic which is of particular interest in network dimensioning is the degree of self-similarity, so the paper looks at the characteristics of aggregated traffic with wap, web and packet speech components to estimate its self-similarity. the results indicate that, while wap traffic alone does not exhibit a significant degree of self-similarity, a combined load from various traffic sources retains almost the same degree of self-similarity as the most self-similar individual source.",
    "present_kp": [
      "wap",
      "self-similarity"
    ],
    "absent_kp": [
      "traffic modelling",
      "mobile data"
    ]
  },
  {
    "title": "multimodal interactions in typically and atypically developing children: natural versus artificial environments.",
    "abstract": "this review addresses the central role played by multimodal interactions in neurocognitive development. we first analyzed our studies of multimodal verbal and nonverbal cognition and emotional interactions within neuronal, that is, natural environments in typically developing children. we then tried to relate them to the topic of creating artificial environments using mobile toy robots to neurorehabilitate severely autistic children. by doing so, both neural/natural and artificial environments are considered as the basis of neuronal organization and reorganization. the common thread underlying the thinking behind this approach revolves around the brains intrinsic properties: neuroplasticity and the fact that the brain is neurodynamic. in our approach, neural organization and reorganization using natural or artificial environments aspires to bring computational perspectives into cognitive developmental neuroscience.",
    "present_kp": [
      "multimodal interactions",
      "mobile toy robot"
    ],
    "absent_kp": [
      "verbal/nonverbal development",
      "autism",
      "free game play",
      "positive emotion",
      "neural mediator"
    ]
  },
  {
    "title": "theoretical demonstration of symmetric iv curves in asymmetric molecular junction of monothiolate alkane.",
    "abstract": "a molecular junction of an asymmetric molecule generally demonstrates an asymmetric currentvoltage (iv) curve, due to the unequal voltage drops at the two moleculeelectrode contacts. however, for asymmetric s(ch2)nch3 molecules, symmetric iv curves are always obtained in the experimental measurements. here, we investigate the electronic transport of the aus(ch2)7ch3au molecular junction in order to reveal the mechanism of the symmetric iv curve with atk package, in which the density functional theory is combined with keldysh nonequilibrium green's function method to calculate the electronic and transport properties of nanoscale systems. and the symmetric iv curve can be interpreted by the curved surface model, which reproduces curved surface of the top electrode in the experiment.",
    "present_kp": [
      "iv curve"
    ],
    "absent_kp": [
      "first-principles",
      "symmetry",
      "thiolalkane monolayer junction"
    ]
  },
  {
    "title": "fuzzy reliability analysis of repairable industrial systems using soft-computing based hybridized techniques.",
    "abstract": "the present study analyzes the fuzzy reliability of a repairable industrial system utilizing uncertain data. one traditional (flt) and two soft-computing based hybridized techniques (gablt and ngablt) are used. some very important fuzzy reliability indices of a washing system in a paper plant have been computed. it is observed that gablt performs consistently well in comparison to other two techniques. the analysis may be helpful for improving the performance of the considered system.",
    "present_kp": [],
    "absent_kp": [
      "flt technique",
      "gablt technique",
      "ngablt technique",
      "nonlinear programming",
      "genetic algorithm",
      "artificial neural networks "
    ]
  },
  {
    "title": "demagnetization properties of ipm and spm motors used in the high demanding automotive application.",
    "abstract": "purpose - in order to reduce co2 emissions of new cars many hydraulic and mechanical systems like e.g.: water pump, oil pump, power steering, clime compressor have been exchanged with pure electromechanical systems, which are driven only on request. this helps to reduce fuel consumption. this trend requires of utilization of modern brushless electric motors, which are controlled from power electronic control unit - ecu. in today's car can be found between 30 to 150 electric motors. many of them are still simple brush type with ferrite magnets. also in this area, drift in the direction of brushless motors can bee seen, because of higher efficiency, longer lifetime, lower noise, better emc and more controllable torque vs speed characteristic. there are different technological solutions, which can been used in the area of brushless motors in order to reduce size and cost of single component. one major factor of bldc/ac motor is rear earth permanent magnet material used during production. a magnet material cost could be in the range from 30 percent (basis price 2010) up to 90 percent (basis price 2011) of total material motor cost, depends on actual rear earth material price level. in order to reduce magnet cost, the aim of this paper is to find the most robust motor design, which can be resistant against maximum temperature and phase current amplitude for the same magnet material properties, coercive force - hcj. this behaviour is called demagnetization property. design/methodology/approach - analysis was performed based on review of literature, own theoretical and practical research and experience in the area of electromechanical systems for automotive application. during motor analysis computer numerical simulation method, cad and experiment were used. findings - as a result, comparison of different motors' topologies with different properties of magnet materials is presented. the worked out methodology shows very good correlation between simulations and measurements. this work can be used in order to reduce test effort and reduce cost of design. practical implications - the presented methodology reduces for new designs test effort and development cost and gives an implication of robust motor topology for demagnetization effects. originality/value - it is the first paper where demagnetization effects have been studied theoretically and in laboratory in order to find the most robust design, reduce magnet cost by reduction of dysprosium content and develop simulation procedure for analysis of demagnetizations behaviours of interior and surface permanent magnet.",
    "present_kp": [
      "brushless motor",
      "coercive force",
      "automotive application",
      "dysprosium"
    ],
    "absent_kp": [
      "interior permanent magnet - ipm",
      "low weight",
      "mass production",
      "cost-effectiveness",
      "neodymium",
      "automotive components industry"
    ]
  },
  {
    "title": "a novel direct search approach for combined heat and power dispatch.",
    "abstract": "a novel approach based on the direct search method (dsm) is proposed for the solution of combined heat and power (chp) dispatch problem. to deal with the mutual dependency of multiple-demand and heatpower capacity of cogeneration units, the penalty functions should be considered in dsm to enforce the corresponding violated constraints from the infeasible region into the feasible region. many nonlinear characteristics of the generator can be handled properly in the direct search procedure. to increase the possibility of exploring the search space where the global optimal solution exists, another effective strategy based on a successive refinement search technique is also proposed to guarantee a possibly complete examination of the solution space. numerical experiments are included to demonstrate the proposed direct search approach can obtain a higher quality solution than many existing techniques.",
    "present_kp": [
      "cogeneration",
      "combined heat and power dispatch",
      "direct search method"
    ],
    "absent_kp": [
      "economic dispatch"
    ]
  },
  {
    "title": "fourth- and tenth-order compact finite difference solutions of perturbed circular vortex flows.",
    "abstract": "in this study, high-order compact finite difference calculations are reported for 2d unsteady incompressible circular vortex flow in primitive variable formulation. the fourth-order runge-kutta temporal discretization is used together with fourth- or tenth-order compact spatial discretization. dependent on the perturbation initially imposed, the solutions display a tripole, triangular or square vortex. the comparison of the predictions with the detailed spectral calculations of kloosterziel and carnevale (j. fluid mech. 1999; 388:217-257) shows that the vorticity fields are very well captured. the spectral resolution of the present method was quantified from the decomposition of the vorticity distribution in its azimuthal components and compared with reported spectral results. using identical grid resolution to the reference results yields negligible differences in the main features of the flow. the perturbation amplitude and its first harmonic are virtually identical to the reference results for both fourth- or tenth-order spatial discretization, as theoretically expected but seldom a posteriori verified. the differences between the two spatial discretizations appear only for coarser grids, favouring the tenth-order discretization.",
    "present_kp": [
      "high-order",
      "circular vortex"
    ],
    "absent_kp": [
      "compact finite differences",
      "incompressible navier stokes"
    ]
  },
  {
    "title": "simulation based study of wireless rf interconnects for practical cmos implementation.",
    "abstract": "an electromagnetic analysis for the practical implementation of on-chip antennas to be used as wireless ic interconnects is presented. the undesired electromagnetic signal coupling between the on-chip antennas and the metal interconnects is characterized under varying geometries and placement of the metal interconnects. the variations in the transmission gain between the antenna pair due to the typical complementary metal oxide semiconductor (cmos) manufacturing requirements are presented. using a 3-d finite element method (fem) based full wave electromagnetic solver, it is shown that the antenna characteristics are significantly impacted by the presence of the essential epitaxial layer and the required minimum metal utilization. it is also shown in a 250nm cmos technology that there can be a significant electromagnetic signal coupling between the on-chip transmitting antenna and the metal interconnects on a die (-12.09db for a 1.6mm long, 2?m wide interconnect at a distance of 1?m from the antenna). design considerations are presented for the metal interconnects in the presence of on-chip antennas in order to minimize the undesired electromagnetic signal coupling.",
    "present_kp": [
      "interconnects",
      "electromagnetic",
      "on-chip antennas"
    ],
    "absent_kp": [
      "vlsi"
    ]
  },
  {
    "title": "identification of tumor-immune system via recurrent neural network.",
    "abstract": "cancer immunotherapy is an emerging therapy for cancer disease treatment which stimulates immune systems to fight against tumor cells. in this paper, a back propagation neural network with some feedbacks from hidden layer is used as a method of identification for one validated mathematical model. since it is not possible to model complex system due to void of information and knowledge to model all complexity of complex system, identification methods are effective tools for modeling ill-defined system. afterward, it is possible to perform control methods on the estimated model to reach the clinical goals. the simulation results have shown the correctness of the identification process.",
    "present_kp": [
      "identification",
      "immune system"
    ],
    "absent_kp": [
      "multi-layer perceptron",
      "artificial neural network"
    ]
  },
  {
    "title": "an efficient iterative algorithm for the approximation of the fast and slow dynamics of stiff systems.",
    "abstract": "the relation between the iterative algorithms based on the computational singular perturbation (csp) and the invariance equation (ie) methods is examined. the success of the two methods is based on the appearance of fast and slow time scales in the dynamics of stiff systems. both methods can identify the low-dimensional surface in the phase space (slow invariant manifold, sim), where the state vector is attracted under the action of fast dynamics. it is shown that this equivalence of the two methods can be expressed by simple algebraic relations. csp can also construct the simplified non-stiff system that models the slow dynamics of the state vector on the sim. an extended version of ie is presented which can also perform this task. this new ie version is shown to be exactly similar to a modified version of csp, which results in a very efficient algorithm, especially in cases where the sim dimension is small, so that significant model simplifications are possible.",
    "present_kp": [],
    "absent_kp": [
      "invariant manifolds",
      "model reduction",
      "multiple time scales",
      "asymptotic analysis",
      "singular perturbation analysis"
    ]
  },
  {
    "title": "an adaptive comb filter with flexible notch gain.",
    "abstract": "this paper proposes an adaptive comb filter with flexible notch gain. it can appropriately remove a periodic noise from an observed signal. the proposed adaptive comb filter uses a simple lms algorithm to update the notch gain coefficient for removing the noise and preserving a desired signal, simultaneously. simulation results show the effectiveness of the proposed comb filter.",
    "present_kp": [
      "comb filter",
      "lms"
    ],
    "absent_kp": [
      "adaptive algorithm"
    ]
  },
  {
    "title": "does the use of structured reporting improve usability? a comparative evaluation of the usability of two approaches for findings reporting in a large-scale telecardiology context.",
    "abstract": "poor usability leads to a low adoption rate of telemedicine systems. mode of input, free-text or structured report, influences usability. usability and user satisfaction are higher for structured report interfaces in telecardiology.",
    "present_kp": [
      "telemedicine",
      "telecardiology"
    ],
    "absent_kp": [
      "system usability scale",
      "keystroke-level model",
      "heuristic usability evaluation",
      "dicom sr"
    ]
  },
  {
    "title": "fixed points of correspondences defined on cone metric spaces.",
    "abstract": "in the present note, we investigate the fixed points of correspondences defined on cone metric spaces satisfying a conditionally contractive condition.",
    "present_kp": [
      "fixed point",
      "cone metric space",
      "correspondence"
    ],
    "absent_kp": [
      "banach lattice"
    ]
  },
  {
    "title": "personal content management system: a semantic approach.",
    "abstract": "the amount of multimedia resources that is created and needs to be managed is increasing considerably. additionally, a significant increase of metadata, either structured (metadata fields of standardized metadata formats) or unstructured (free tagging or annotations) is noticed. this increasing amount of data and metadata, combined with the substantial diversity in terms of used metadata fields and constructs, results in severe problems to manage and retrieve these multimedia resources. standardized metadata schemes can be used but the plethora of these schemes results in interoperability issues. in this paper, we propose a metadata model suited for personal content management systems. we create a layered metadata service that implements the presented model as an upper layer and combines different metadata schemes in the lower layers. semantic web technologies are used to define and link formal representations of these schemes. specifically, we create an ontology for the dig35 metadata standard and elaborate on how it is used within this metadata service. to illustrate the service, we present a representative use case scenario consisting of the upload, annotation, and retrieval of multimedia content within a personal content management system.",
    "present_kp": [
      "personal content management",
      "semantic web",
      "metadata",
      "annotation",
      "ontology",
      "interoperability"
    ],
    "absent_kp": [
      "multimedia standards",
      "reasoning"
    ]
  },
  {
    "title": "a next generation multimedia call center for internet commerce: imc.",
    "abstract": "human assistance, as well as automated service, is necessary for providing more convenient services to customers on the internet-based commerce system. call centers have been typically human-based service systems. however, the services of existing public switched telephone network-based call centers are not enough to meet the needs of customers on the internet. most of them have been designed without considering the interaction involved in shopping on the internet in our research, we design a call center named imc (internet-based multimedia call center) that can be integrated with an internet shopping mall. it contains 2 parts: an internet multimedia dialogue system and a human agent assisting system. the internet multimedia dialogue system is an internet and multimedia version of the interactive voice response service of computer telephony integration-based call centers because it provides access to the multimedia web page along with the recorded voice explanation through the internet. the human agent assisting system aims to select the most appropriate human agents in the call center and support them in providing high-quality individualized information for each customer. imc is a real-time, human-embedded system that can provide high-quality services cost-effectively for internet commerce.",
    "present_kp": [
      "call center",
      "internet commerce",
      "computer telephony integration",
      "internet shopping mall",
      "human-embedded system"
    ],
    "absent_kp": [
      "electronic commerce"
    ]
  },
  {
    "title": "wind tunnel experiments of tracer dispersion downwind from a small-scale physical model of a landfill.",
    "abstract": "wind tunnel experiments have been carried out on a small-scale physical model of a municipal waste landfill (mwl) in the criaciv (research centre of building aerodynamics and wind engineering) environmental wind tunnel in prato (italy). the mwl model simulates a landfill whose surface is higher than the surrounding surface, applying a 1:200 scaling factor. modelling an area source such as landfill is a difficult task for numerical models due to turbulence phenomena that modifies the flow near the source increasing ground level concentration (glc). for the specific task, a new set-up of the wind tunnel has been developed, with respect to previous studies carried out on line and point sources physical models. the tracer used in the experiments was ethylene, suitable for non-buoyant plume conditions, typical for mwl emissions. a detailed result database has been obtained in terms of glc and concentration profiles as well as flow turbulence and velocity field characterisation.",
    "present_kp": [
      "wind tunnel",
      "concentration profiles",
      "landfill"
    ],
    "absent_kp": [
      "physical modelling",
      "experimental data"
    ]
  },
  {
    "title": "exclusively your's: dynamic individuate search by extending user profile.",
    "abstract": "a universal search engine is unable to provide a personal touch to a user query. to overcome the deficiency of a universal search engine, vertical search engines are used, which return search results from a specific domain. an alternate option is to use a personalized search system. in our endeavor to provide personalized search results, the proposed system, exclusively your's, observes a user browsing behavior and his actions. based on the observed user behavior, it dynamically constructs user profile which consists of some terms that are related to user's interest. the constructed profile is later used for query expansion. the goal of research work in this paper is not to provide all the relevant results, but a few high quality personalized search results at the top of ranked list, which in other words means high precision. we performed experiments by personalizing google, yahoo, and naver (widely used search engine in korea). the results show that using exclusively your's, a search engine yields significant improvement. we also compared the user profile constructed by the proposed approach with other similar personalization approaches; the results show a marginal increase in precision.",
    "present_kp": [
      "search engine",
      "personalization"
    ],
    "absent_kp": [
      "summarization",
      "weighted index",
      "anchor text",
      "hyperlink"
    ]
  },
  {
    "title": "constrained diffusion-limited aggregation in 3 dimensions.",
    "abstract": "diffusion-limited aggregation (dla) has usually been studied in 2 dimensions as a model of fractal growth processes such as river networks, plant branching, frost on glass, electro-deposition, lightning, mineral deposits, and coral. here, the basic principles are extended into 3 dimensions and used to create, among other things, believable models of root systems. an additional innovation is a means of constraining the growth of the 3d dla by a surface or containing it within a vessel.",
    "present_kp": [
      "fractal",
      "dla",
      "branching",
      "diffusion",
      "aggregation"
    ],
    "absent_kp": [
      "brownian motion"
    ]
  },
  {
    "title": "skill-specific spoken dialogs in a reading tutor that listens.",
    "abstract": "project listen's reading tutor listens to children read aloud. a controlled study indicates that the reading tutor helps children's reading comprehension. however, the results for word attack (decoding) skills and word identification skills were not statistically better than in the control condition. our thesis therefore proposes to develop skill-specific dialogs based on cognitive skill models and successful tutoring strategies. these dialogs will be dynamically assembled by the reading tutor and include text, speech, illustrations, and dialog parameters. we hypothesize that such dialogs will improve elementary students' reading abilities.",
    "present_kp": [
      "children",
      "spoken dialog",
      "reading"
    ],
    "absent_kp": [
      "intelligent tutoring systems",
      "speech recognition"
    ]
  },
  {
    "title": "a difference expansion oriented data hiding scheme for restoring the original host images.",
    "abstract": "this paper proposes a lossless data embedding scheme that exploits the difference expansion of the pixels to conceal large amount of message data in a digital image. the proposed scheme takes into consideration the correlation between the pixel and its surrounding pixels to determine the degree of the difference expansion for message data embedding. the performance has been evaluated in terms of image distortion, payload capacity, as well as embedding rate. the experimental results show that the scheme is capable of providing a great payload capacity, and the image quality of the embedded image is better than that of tians and celiks schemes for a gray-level image. what is more, for a color image, the proposed scheme outperforms alattars scheme at low psnr. in addition, the proposed scheme can completely restore the original image after data extraction.",
    "present_kp": [
      "lossless data embedding",
      "difference expansion"
    ],
    "absent_kp": [
      "information hiding",
      "reversible data hiding"
    ]
  },
  {
    "title": "bit-parallel random number generation for discrete uniform distributions.",
    "abstract": "when a die is cast, the outcome is one of the six sides, i.e. the outcome is discrete and uniformly distributed over the range r = {1, 2, 3, 4, 5, 6}. generating random numbers with such a distribution is very easy: obtain a random number w epsilon w, the domain of the random numbers, and take (w mod r) + 1. however, many uniform discrete distributions have a rather short range, e.g., r = 6 in a dice game, and r = 3 for the walking directions of a 2-dimensional nonreversal random walk. the number w is typically a machine word, i.e. log(2)(w) approximate to 32 in a 32-bit computer, so generating a log(2)(r)-bit random number has consumed about 32 random bits. when w much greater than r, it is wasteful and hence inefficient. this paper presents an efficient algorithm for generating random numbers for the distributions with r discrete uniform outcomes. the algorithm uses parallel bit-wise operations on machine words. the performance results of the algorithm are presented. the statistical quality of the random numbers generated from this algorithm is also discussed.",
    "present_kp": [
      "discrete uniform distribution"
    ],
    "absent_kp": [
      "random number generators",
      "monte carlo simulation",
      "random walks"
    ]
  },
  {
    "title": "a metaobject protocol for clforjava.",
    "abstract": "clforjava is a new implementation of common lisp that intertwines its architecture and operation with java. the authors describe a new architecture for a clos mop that supports transparent, bi-directional access between lisp and java. the access requires no special techniques nor syntactic mechanisms on the part of the programmer - being either java or lisp. the core of the new mop is a data structure that melds the fundamental structures of java instances (n-tuples) and clos instances (2-tuples) in such a way that the respective object systems can interact without cumbersome translations. methods from their respective object systems can interact freely. we discuss certain aspects of the respective mops that prevent a complete integration and replacement of one system by the other.",
    "present_kp": [
      "clos",
      "java",
      "lisp"
    ],
    "absent_kp": [
      "interoperation"
    ]
  },
  {
    "title": "a systematic evaluation of disk imaging in encase 6.8 and linen 6.1.",
    "abstract": "tools for disk imaging (or more generally speaking, digital acquisition) are a foundation for forensic examination of digital evidence. therefore it is crucial that such tools work as expected. the only way to determine whether this is the case or not is through systematic testing of each tool. in this paper we present such an evaluation of the disk imaging functions of encase 6.8 and linen 6.1, conducted on behalf of the swedish national laboratory of forensic science. although both tools performed as expected under most circumstances, we identified cases where flaws that can lead to inaccurate and incomplete acquisition results in linen 6.1 were exposed. we have also identified limitations in the tool that were not evident from its documentation. in addition summarizing the test results, we present our testing methodology, which has novel elements that we think can benefit other evaluation projects.",
    "present_kp": [
      "encase",
      "linen"
    ],
    "absent_kp": [
      "acquisition of digital data",
      "hard drive imaging",
      "testing forensic tools",
      "linux"
    ]
  },
  {
    "title": "parallel simulation of devs and cell-devs models on windows-based pc cluster systems.",
    "abstract": "the growing popularity of networks of workstations (now) in scientific computation has drawn increasing interest from the m&s community. this paper addresses the issue of parallel discrete-event simulation of devs and cell-devs models on a microsoft windows-based cluster system comprising interconnected general-purpose personal computers. we present the architecture and features of pcd++win, a parallel simulator that takes advantage of the multi-purpose graphical user interface of the deinompi middleware for construction of ad-hoc pc clusters and configuration of simulation environment. this environment significantly reduces the learning curve for general users and the cost of the simulation platform. pcd++win has been developed using a modular approach that promotes code reuse and allows for easy switching to other middleware technologies. the portability of the simulator is enhanced with multi-platform programming and compilation techniques. moreover, it leaves open the possibility of further extensions such as web-based distributed simulation and database-based model construction by leveraging the native support of microsoft visual studio. the experiments demonstrate the capability of the new simulator, making it an ideal m&s toolkit for tapping the computational power of general-purpose desktop computers.",
    "present_kp": [
      "devs",
      "cell-devs",
      "parallel simulation",
      "cluster systems"
    ],
    "absent_kp": [
      "discrete event simulation"
    ]
  },
  {
    "title": "a computable version of the daniell-stone theorem on integration and linear functionals.",
    "abstract": "for every measure mu, the integral i: f bar right arrow integral f d mu is a linear functional on the set of real measurable functions. by the daniell-stone theorem, for every abstract integral lambda: f --> r on a stone vector lattice f of real functions f : omega --> r there is a measure mu such that integral f d mu = lambda(f) for all f is an element of f. in this paper we prove a computable version of this theorem.",
    "present_kp": [
      "daniell-stone theorem"
    ],
    "absent_kp": [
      "computability",
      "computable analysis",
      "measure theory"
    ]
  },
  {
    "title": "factors influencing intention to use e-government services among citizens in malaysia.",
    "abstract": "this study is an exploratory study on the e-government in malaysia. with the liberalization and globalization, internet has been used as a medium of transaction in almost all aspects of human living. this study investigates the factors that influencing the intention to use e-government service among malaysians. this study integrates constructs from the models of technology acceptance model (tam), diffusion of innovation (doi) which been moderated by culture factor and trust model with five dimensions. the study was conducted by surveying a broad diversity of citizens in malaysia community. a structured questionnaire was used to collect data from 195 respondents but only 150 of the respondents with complete answers participating in the study. the result of the analysis showed that trust, perceived usefulness, perceived relative advantage and perceived image, respectively, has a direct positive significant relationship towards intention to use e-government service and perceived complexity has a significant negative relationship towards intention to use e-government service. while perceived strength of online privacy and perceived strength of non-repudiation have a positive impact on a citizen's trust to use e-government service. however, the uncertainty avoidance (moderating factor) used in the study has no significant effect on the relationship between the innovation factors (complexity, relative advantage and image) and intention to use e-government service. finally in comparing the explanatory power of the entire intention based model (tam, doi and trust) with the studied model, it has been found that the doi model has a better explanatory power.",
    "present_kp": [
      "e-government",
      "innovation"
    ],
    "absent_kp": [
      "adoptions",
      "malaysian services"
    ]
  },
  {
    "title": "reducing the energy dissipation of the issue queue by exploiting narrow immediate operands.",
    "abstract": "in contemporary superscalar microprocessors, issue queue is a considerable energy dissipating component due its complex scheduling logic. in addition to the energy dissipated for scheduling activities, read and write lines of the issue queue entries are also high energy consuming pieces of the issue queue. when these lines are used for reading and writing unnecessary information bits, such as the immediate operand part of an instruction that does not use the immediate field or the insignificant higher order bits of an immediate operand that are in fact not needed, significant amount of energy is wasted. in this paper, we propose two techniques to reduce the energy dissipation of the issue queue by exploiting the immediate operand files of the stored instructions: firstly by storing immediate operands in separate immediate operand files rather than storing them inside the issue queue entries and secondly by issue queue partitioning based on widths of immediate operands of instructions. we present our performance results and energy savings using a cycle accurate simulator and testing the design with spec2k benchmarks and 90 nm cmos (umc) technology.",
    "present_kp": [
      "issue queue",
      "immediate operands"
    ],
    "absent_kp": [
      "encoding",
      "energy consumption",
      "low power"
    ]
  },
  {
    "title": "study of speed-dependent packet error rate for wireless sensor on rotating mechanical structures.",
    "abstract": "wireless sensors on rotating mechanical structures have rich and fast changing multipath that cannot be easily predicted by conventional regression approaches in time for effective transmission coding or power control, resulting in deteriorated transmission quality. this study aims to study the speed-dependent packet error rate (per) of wireless sensor radios on rotating mechanical structures. a series of rotating ieee 802.15.4 sensor radio transmission experiments and vector network analyzer measurements have been conducted to derive and validate a predictive per model for a fast rotating sensor radio channel based on channel impulse response measurements. the proposed predictive per model, including power attenuation, bit error rate (ber) and per sub-models, captures the channel property of rotating sensors based on the received signal strength and the radio receiving sensitivity. the per model has accurately predicted the per profile of sensors on a rotating machine tool spindle as well as a rotating plate of a prototype rotation system. the analysis provides an in-depth understanding of how multipath propagation causes the fast power variation and the resulting speed-dependent per for wireless sensors on rotating mechanical structures.",
    "present_kp": [
      "packet error rate",
      "rotating mechanical structure",
      "wireless sensor"
    ],
    "absent_kp": [
      "transmission performance"
    ]
  },
  {
    "title": "crosstalk in vlsi interconnections.",
    "abstract": "we address the problem of crosstalk computation and reduction using circuit and layout techniques in this paper, we provide easily computable expressions for crosstalk amplitude and pulse width in resistive, capacitively coupled lines, the expressions hold for nets with arbitrary number of pins and of arbitrary topology under any specified input excitation. experimental results show that the average error is about 10% and the maximum error is less than 20%. the expressions are used to motivate circuit techniques, such as transistor sizing, and layout techniques, such as wire ordering and wire width optimization to reduce crosstalk.",
    "present_kp": [],
    "absent_kp": [
      "coupled noise",
      "signal integrity",
      "timing optimization"
    ]
  },
  {
    "title": "a hybrid genetic algorithm for the energy-efficient virtual machine placement problem in data centers.",
    "abstract": "server consolidation using virtualization technology has become an important technology to improve the energy efficiency of data centers. virtual machine placement is the key in the server consolidation technology. in the past few years, many approaches to the virtual machine placement have been proposed. however, existing virtual machine placement approaches consider the energy consumption by physical machines only, but do not consider the energy consumption in communication network, in a data center. however, the energy consumption in the communication network in a data center is not trivial, and therefore should be considered in the virtual machine placement. in our preliminary research, we have proposed a genetic algorithm for a new virtual machine placement problem that considers the energy consumption in both physical machines and the communication network in a data center. aiming at improving the performance and efficiency of the genetic algorithm, this paper presents a hybrid genetic algorithm for the energy-efficient virtual machine placement problem. experimental results show that the hybrid genetic algorithm significantly outperforms the original genetic algorithm, and that the hybrid genetic algorithm is scalable.",
    "present_kp": [
      "virtual machine placement",
      "server consolidation",
      "data center",
      "hybrid genetic algorithm"
    ],
    "absent_kp": [
      "cloud computing"
    ]
  },
  {
    "title": "sensitivity of tapered optical fiber surface plasmon resonance sensors.",
    "abstract": "the effect of tapered profiles on the sensitivity of spr sensor is studied. it is observed that as the taper ratio decreases the sensitivity of proposed sensor for each profile increases up to certain taper ratio where the plasmonic condition is satisfied. in all considered cases, the maximum sensitivity is obtained for sinusoidal tapered profile.",
    "present_kp": [
      "sensitivity",
      "tapered profiles",
      "taper ratio",
      "sinusoidal tapered profile",
      "surface plasmon resonance"
    ],
    "absent_kp": []
  },
  {
    "title": "an image contrast enhancement method based on genetic algorithm.",
    "abstract": "contrast enhancement plays a fundamental role in image/video processing. histogram equalization (he) is one of the most commonly used methods for image contrast enhancement. however, he and most other contrast enhancement methods may produce un-natural looking images and the images obtained by these methods are not desirable in applications such as consumer electronic products where brightness preservation is necessary to avoid annoying artifacts. to solve such problems, we proposed an efficient contrast enhancement method based on genetic algorithm in this paper. the proposed method uses a simple and novel chromosome representation together with corresponding operators. experimental results showed that this method makes natural looking images especially when the dynamic range of input image is high. also, it has been shown by simulation results that the proposed genetic method had better results than related ones in terms of contrast and detail enhancement and the resulted images were suitable for consumer electronic products.",
    "present_kp": [
      "contrast enhancement",
      "genetic algorithm",
      "natural looking images"
    ],
    "absent_kp": []
  },
  {
    "title": "limits of a conjecture on a leakage-resilient cryptosystem.",
    "abstract": "we introduce the hidden shares number problem, a variant of the hidden number problem. we give a leakage-resilience bound for elgamal cryptosystem with stateful decryption. we have implemented our attack and give some details about our implementation.",
    "present_kp": [
      "elgamal",
      "hidden number problem"
    ],
    "absent_kp": [
      "cryptography",
      "leakage-resilient cryptography",
      "lattice-based attacks"
    ]
  },
  {
    "title": "robust reconstruction of low-resolution document images by exploiting repetitive character behaviour.",
    "abstract": "in this paper, we present a new approach for reconstructing low-resolution document images. unlike other conventional reconstruction methods, the unknown pixel values are not estimated based on their local surrounding neighbourhood, but on the whole image. in particular, we exploit the multiple occurrence of characters in the scanned document. in order to take advantage of this repetitive behaviour, we divide the image into character segments and match similar character segments to filter relevant information before the reconstruction. a great advantage of our proposed approach over conventional approaches is that we have more information at our disposal, which leads to a better reconstruction of the high-resolution (hr) image. experimental results confirm the effectiveness of our proposed method, which is expressed in a better optical character recognition (ocr) accuracy and visual superiority to other traditional interpolation and restoration methods.",
    "present_kp": [
      "restoration",
      "interpolation",
      "ocr"
    ],
    "absent_kp": [
      "repetition",
      "character segmentation",
      "bimodal distribution"
    ]
  },
  {
    "title": "complexity of inflammatory responses in endothelial cells and vascular smooth muscle cells determined by microarray analysis.",
    "abstract": "to better understand the molecular basis of vascular cell system behavior in inflammation, we used gene expression microarrays to analyze the expression of 7,075 genes and their response to il-1? and tnf? in cultures of coronary artery endothelium and smooth muscle derived from a single coronary artery. the most noticeable difference between the cell types was the considerably greater magnitude and complexity of the transcriptional response in the endothelial cells. two hundred and nine genes were regulated in the endothelium and only 39 in vascular smooth muscle. among the 209 regulated genes in the endothelium, 99 have not been previously associated with endothelial cell activation and many implicate the endothelium in unconventional roles. for example, the induced genes include several that have only been associated with leukocyte function (e.g., il-7 receptor, ebi-3 receptor) and others related to antiviral and antibacterial defense (e.g., oligoadenylate synthetase, lmp7, toll-like receptor 4, complement component 3). in addition, 43 genes likely to participate in signal transduction (eg. il-18 receptor, stk2 kinase, staf50, anp receptor, vip receptor, rac3, ifp35) were regulated providing evidence that a major effect of tnf? and il-1? is to alter the potential of the endothelial cell to respond to various other external stimuli.",
    "present_kp": [
      "inflammation"
    ],
    "absent_kp": [
      "tnf-?",
      "interleukin-1 beta",
      "gene expression profiling",
      "gene expression regulation"
    ]
  },
  {
    "title": "partial x-ray photoelectron spectroscopy to constructing neural network model of plasma etching surface.",
    "abstract": "a new model to control plasma processes was constructed by combining a backpropagation neural network (bpnn) with x-ray photoelectron spectroscopy (xps). this technique was evaluated with the data collected during the etching of silicon carbide films at nf3 inductively coupled plasma. the etching characteristics modeled were the etch rate and surface roughness measured by scanning electron microscope and atomic force microscopy, respectively. for systematic modeling, the etching was characterized by means of 24 full factorial experiment plus one center point. the bpnn was trained by the training data composed of xps spectra corresponding to five major peaks. prediction performance of trained bpnn model was tested with a test data set, not belonging to the training data. in modeling surface roughness, pure xps model yielded an improvement of about 24% over pca-xps (99% data variance) model. for the etch rate data, the improvement was more than 40% irrespective of the data variances. these results indicate that non-reduced xps spectra are more effective in constructing a prediction model. xps models can be utilized to diagnose or control plasma processes.",
    "present_kp": [
      "x-ray photoelectron spectroscopy",
      "surface roughness",
      "neural network",
      "atomic force microscopy",
      "plasma etching",
      "model"
    ],
    "absent_kp": []
  },
  {
    "title": "aggregate profit-based caching replacement algorithms for streaming media transcoding proxy systems.",
    "abstract": "this work derives a generalized video object profit function from the extended weighted transcoding graph to calculate the individual cache profit of certain versions of a video object, and the aggregate profit from caching multiple versions of the same video object. this proposed function takes into account the popularity of certain versions of an object, the transcoding delay among versions, and the average duration of access of each version. based on the profit function, cache-replacement algorithms are proposed to reduce the startup delay and network traffic by efficiently caching video objects with the most profits. two kinds of simulations were conducted to evaluate the performance of the proposed algorithms. these simulations exploit partial viewing traces and complete viewing traces, separately. the results demonstrate that the proposed algorithms outperform the competing algorithms by 15%-39% in delay saving ratio and 5%-10% in byte-hit ratio.",
    "present_kp": [],
    "absent_kp": [
      "computer networks",
      "multimedia communication",
      "multimedia streaming",
      "prefix caching",
      "proxy caching",
      "streaming media distribution"
    ]
  },
  {
    "title": "refining and reasoning about nonfunctional requirements.",
    "abstract": "nonfunctional requirements (nfr) must be addressed early in the software development cycle to avoid the cost of revisiting those requirements or re-factoring at the later stages of the development cycle. methods and frameworks that identify and incorporate nfr at each stage of development cycle reduce this cost. the methodology used in this work for refining and reasoning about nfr is based on the nfr framework. this work identifies four nfr types and provides the methodology for developing domain specific nfr by using techniques for converting the requirements into design artifacts per nfr type. the contribution is four nfr types: functionally restrictive, additive restrictive, policy restrictive, and architecture restrictive and the software engineering process that provides specific refinements that result in unique architectural and design artifacts. by applying the same functional requirement focus to the different nfr domains it enhances the development process and promotes software quality attributes such as composability, maintainability, evolvability, and traceability.",
    "present_kp": [
      "software engineering",
      "nonfunctional requirements"
    ],
    "absent_kp": [
      "aspect-oriented programming"
    ]
  },
  {
    "title": "model-updated image guidance: initial clinical experiences with gravity-induced brain deformation.",
    "abstract": "image-guided neurosurgery relies on accurate registration of the patient, the preoperative image series, and the surgical instruments in the same coordinate space. recent clinical reports have documented the magnitude of gravity-induced brain deformation in the operating room and suggest these levels of tissue motion may compromise the integrity of such systems, we are investigating a model-based strategy which exploits the wealth of readily-available preoperative information in conjunction with intraoperatively acquired data to construct and drive a three dimensional (3-d) computational model which estimates volumetric displacements in order to update the neuronavigational image set. using model calculations, the preoperative image database can be deformed to generate a more accurate representation of the surgical focus during an operation, in this paper, we present a preliminary study of four patients that experienced substantial brain deformation from gravity and correlate cortical shift measurements with model predictions, additionally, me illustrate our image deforming algorithm and demonstrate that preoperative image resolution is maintained. results over the four cases show that the brain shifted, on average, 5.7 mm in the direction of gravity and that model predictions could reduce this misregistration error to an average of 1.2 mm.",
    "present_kp": [
      "brain shift",
      "image guidance"
    ],
    "absent_kp": [
      "brain deformation model",
      "consolidation",
      "finite element model",
      "porous media"
    ]
  },
  {
    "title": "communication structure and collective actions in social media.",
    "abstract": "in this paper i present results a study of different types of social media communication and networking channels that allow for collective action (ca): twitter, jaiku / qaiku, ning and facebook. my preliminary findings indicate, that the visual outlining and the structure of communication create different kinds of collectivity and collective actions. a status stream is effective for simple and fast repetitive mass actions and for individual mass broadcasting, while channels and threads are needed as a backchannel for more complicated, coordinated and iterative tasks and support a sense of community. when planning collective action on the internet, ranging from citizen participation to marketing campaigns, it is essential to note that different social media tools support different forms of collective action and feelings of collectiveness.",
    "present_kp": [
      "backchannel",
      "collective action",
      "social media"
    ],
    "absent_kp": [
      "crowdsourcing",
      "collective intelligence",
      "real-time web",
      "micro channel"
    ]
  },
  {
    "title": "better reporting of randomized trials in biomedical journal and conference abstracts.",
    "abstract": "well reported research published in conference and journal abstracts is important as individuals reading these reports often base their initial assessment of a study based on information reported in the abstract. however, there is growing concern about the reliability and quality of information published in these reports. this article provides an overview of research evidence underpinning the need for better reporting of abstracts reported in conference proceedings and abstracts of journal articles; with a particular focus in the area of health care. where available we highlight evidence which refers specifically to abstracts reporting randomized trials. we seek to identify current initiatives aimed at improving the reporting of these reports and recommend that an extension of the consort statement (consolidated standards of reporting trials), consort for abstracts, be developed. this checklist would include a list of essential items to be reported in any conference or journal abstract reporting the results of a randomized trial.",
    "present_kp": [
      "conference proceedings"
    ],
    "absent_kp": [
      "randomized controlled trial",
      "methodological quality",
      "structured abstracts",
      "checklists"
    ]
  },
  {
    "title": "continuous testing in eclipse.",
    "abstract": "continuous testing uses excess cycles on a developer's workstation to continuously run regression tests in the background, providing rapid feedback about test failures as code is edited. it reduces the time and energy required to keep code well-tested, and it prevents regression errors from persisting uncaught for long periods of time.",
    "present_kp": [
      "continuous testing",
      "testing"
    ],
    "absent_kp": [
      "development environments"
    ]
  },
  {
    "title": "bayesian analysis of the logit model and comparison of two metropolishastings strategies.",
    "abstract": "we examine some markov chain monte carlo (mcmc) methods for a generalized non-linear regression model, the logit model. it is first shown that mcmc algorithms may be used since the posterior is proper under the choice of non-informative priors. then two non-standard mcmc methods are compared: a metropolishastings algorithm with a bivariate normal proposal resulting from an approximation, and a metropolishastings algorithm with an adaptive proposal. the results presented here are illustrated by simulations, and show the good behavior of both methods, and superior performances of the method with an adaptive proposal in terms of convergence to the stationary distribution and exploration of the posterior distribution surface.",
    "present_kp": [
      "markov chain monte carlo",
      "metropolishastings algorithm"
    ],
    "absent_kp": [
      "bayesian statistic",
      "adaptive algorithm",
      "stationarity",
      "convergence assessment"
    ]
  },
  {
    "title": "aggregate features and adaboost for music classification.",
    "abstract": "we present an algorithm that predicts musical genre and artist from an audio waveform. our method uses the ensemble learner adaboost to select from a set of audio features that have been extracted from segmented audio and then aggregated. our classifier proved to be the most effective method for genre classification at the recent mirex 2005 international contests in music information extraction, and the second-best method for recognizing artists. this paper describes our method in detail, from feature extraction to song classification, and presents an evaluation of our method on three genre databases and two artist-recognition databases. furthermore, we present evidence collected from a variety of popular features and classifiers that the technique of classifying features aggregated over segments of audio is better than classifying either entire songs or individual short-timescale features.",
    "present_kp": [
      "genre classification",
      "mirex"
    ],
    "absent_kp": [
      "artist recognition",
      "audio feature aggregation",
      "multiclass adaboost"
    ]
  },
  {
    "title": "weak limits and their calculation in analog signal theory.",
    "abstract": "purpose - this paper aims to improve the mathematical justification of certain analog signal theory concepts and offer a rigorous framework for it. design/methodology/approach - the framework relies on functional analysis, namely theory of distributions and the concept of weak limit. its notation is adjusted to resemble the notation usually used in engineering signal theory. it can be used to prove in a rigorous manner already established results in signal theory, but also to establish new ones. findings - examples have shown the lack of rigour caused by using ordinary calculus in proving fundamental signal theoretic results. on that basis, concepts of limit, fourier transform and derivative are revisited in the spirit of functional analysis. a new useful formula for weak limit computation is proved. originality/value - functional analysis is efficiently used in signal theory in a manner approachable by engineers. an original and efficient formula for weak limit computation is presented and proved.",
    "present_kp": [
      "analog signal theory",
      "functional analysis",
      "weak limit"
    ],
    "absent_kp": [
      "schwartz distribution",
      "fourier transforms"
    ]
  },
  {
    "title": "the social information infrastructure.",
    "abstract": "the division of social, behavioral, and economic research of the national science foundation has explored aggressively the potential involvement of the social sciences in the national information infrastructure. we invision the nii as a global network of computer communications, which will evolve out of the internet, linking all social scientists to massive digital libraries and to myriad smaller distributed data sources containing information of every imaginable sort. five workshops have charted applications of high-performance computing in the social and behavioral sciences: cognitive science, computational geography computational economics, artificial social intelligence, and electronic networks. a survey of seer programs revealed that many are helping to create the information infrastructure, and substantial investment in six ''flagship'' digital library projects will develop the systems necessary for the nii of the 21st century.",
    "present_kp": [
      "information infrastructure",
      "digital library",
      "cognitive science",
      "computational geography",
      "computational economics",
      "internet"
    ],
    "absent_kp": [
      "artificial intelligence"
    ]
  },
  {
    "title": "web-based public participation geographical information systems: an aid to local environmental decision-making.",
    "abstract": "current research examining the potential of the world-wide web as a means of increasing public participation in local environmental decision making in the uk is discussed. the paper considers traditional methods of public participation and argues that new internet-based technologies have the potential to widen participation in the uk planning system. evidence is provided of the potential and actual benefits of online spatial decision support systems in the uk through a real environmental decision support problem in a village in northern england. the paper identifies key themes developing in this area of web-based geographical information systems (gis) and provides a case-study example of an online public participation gis from inception to the final phase in a public participation process. it is shown that in certain uk planning problems and policy formulation processes, participatory online systems are a useful means of informing and engaging the public and can potentially bring the public closer to a participatory planning system.",
    "present_kp": [
      "gis",
      "web",
      "public participation"
    ],
    "absent_kp": [
      "planning for real"
    ]
  },
  {
    "title": "medical image analysis for cancer management in natural computing framework.",
    "abstract": "natural computing, through its repertoire of nature-inspired strategies, is playing a major role in the development of intelligent decision-making systems. the objective is to provide flexible, application-oriented solutions to current medical image analysis problems. it encompasses fuzzy sets, neural networks, genetic algorithms, rough sets, swarm intelligence, and a host of other paradigms, mimicking biological and physical processes from nature. radiographic imaging modalities, like computed tomography (ct), positron emission tomography (pet), and magnetic resonance imaging (mri), help in providing improved diagnosis, prognosis and treatment planning for cancer. this survey highlights the role of natural computing, in efficiently analyzing radiographic medical images, for improved tumor management. we also provide a categorization of the segmentation, feature extraction and selection methods, based on different natural computing technologies, with reference to the application involving malignancy of the brain, breast, prostate, skin, lung, and liver.",
    "present_kp": [
      "segmentation"
    ],
    "absent_kp": [
      "quantitative imaging",
      "feature selection",
      "radiomics",
      "evolutionary algorithms",
      "biomedical imaging"
    ]
  },
  {
    "title": "parallelisation of the lagrangian model in a mixed eulerian-lagrangian cfd algorithm.",
    "abstract": "this manuscript presents an algorithm implemented in a commercial computational fluid dynamics (cfd) code for parallelisation of the lagrangian particle tracking model in a mixed eulerian-lagrangian cfd algorithm. the algorithm is based on the domain decomposition parallelisation strategy and asynchronous message passing protocol. the methodology is tested on two industrial cfd test cases and the parallelisation results are presented. further, it is discussed how the parallel efficiency of the runs can be improved by adopting the domain decomposition scattering technique.",
    "present_kp": [],
    "absent_kp": [
      "parallel eulerian-lagrangian cfd",
      "parallel particle tracking",
      "parallelisation of the spray model"
    ]
  },
  {
    "title": "sequential and parallel triangulating algorithms for elimination game and new insights on minimum degree.",
    "abstract": "elimination game is a well-known algorithm that simulates gaussian elimination of matrices on graphs, and it computes a triangulation of the input graph. the number of fill edges in the computed triangulation is highly dependent on the order in which elimination game processes the vertices, and in general the produced triangulations are neither minimum nor minimal. in order to obtain a triangulation which is close to minimum, the minimum degree heuristic is widely used in practice, but until now little was known on the theoretical mechanisms involved. in this paper we show some interesting properties of elimination came; in particular that it is able to compute a partial minimal triangulation of the input graph regardless of the order in which the vertices are processed. this results in a new algorithm to compute minimal triangulations that are sandwiched between the input graph and the triangulation resulting from elimination came. one of the strengths of the new approach is that it is easily parallelizable, and thus we are able to present the first parallel algorithm to compute such sandwiched minimal triangulations. in addition, the insight that we gain through elimination game is used to partly explain the good behavior of the minimum degree algorithm. we also give a new algorithm for producing minimal triangulations that is able to use the minimum degree idea to a wider extent.",
    "present_kp": [
      "minimum degree",
      "minimal triangulation"
    ],
    "absent_kp": [
      "chordal graphs",
      "parallel and sequential algorithms"
    ]
  },
  {
    "title": "quality evaluation of e-government digital services.",
    "abstract": "in this paper we present a \"quality estimation model\" for digital e-government services suitable for quality evaluation, monitoring, discovery, selection and composition.",
    "present_kp": [
      "e-government"
    ],
    "absent_kp": [
      "quality model",
      "quality of service"
    ]
  },
  {
    "title": "throughput improvement of incremental redundancy ldpc coded mimo v-blast system.",
    "abstract": "in this paper, we present ensembles of incremental redundancy low-density parity-check (ir-ldpc) codes to improve the throughput performance of hybrid forward error correction (fec)/automatic repeat request (arq) schemes in a vertical bell lab layered space-time (v-blast) system. these ensembles are designed to have good error rate performance at short block lengths, which result in higher throughput performance. the throughput simulations in various fading conditions show that these ensembles outperform a conventional random punctured ensemble by 3 db eb/n0 at a throughput region of 0.8. to reduce the traffic of feedback channels, we consider using an adaptive code selection algorithm. in these adaptive hybrid fec/arq schemes, the number of negative acknowledgement signals for retransmission is greatly reduced at operating snr ranges without any significant throughput loss.",
    "present_kp": [
      "hybrid fec/arq scheme"
    ],
    "absent_kp": [
      "mimo system",
      "ldpc codes"
    ]
  },
  {
    "title": "benefits of averaging lateration estimates obtained using overlapped subgroups of sensor data.",
    "abstract": "in this paper, we suggest averaging lateration estimates obtained using overlapped subgroups of distance measurements as opposed to obtaining a single lateration estimate from all of the measurements directly if a redundant number of measurements are available. least squares based closed form equations are used in the lateration. in the case of gaussian measurement noise the performances are similar in general and for some subgroup sizes marginal gains are attained. averaging laterations method becomes especially beneficial if the lateration estimates are classified as useful or not in the presence of outlier measurements whose distributions are modeled by a mixture of gaussians (mog) pdf. a new modified trimmed mean robust averager helps to regain the performance loss caused by the outliers. if the measurement noise is gaussian, large subgroup sizes are preferable. on the contrary, in robust averaging small subgroup sizes are more effective for eliminating measurements highly contaminated with mog noise. the effect of high-variance noise was almost totally eliminated when robust averaging of estimates is applied to qr decomposition based location estimator. the performance of this estimator is just 1 cm worse in root mean square error compared to the cramrrao lower bound (crlb) on the variance both for gaussian and mog noise cases. theoretical crlbs in the case of mog noise are derived both for time of arrival and time difference of arrival measurement data.",
    "present_kp": [
      "averaging",
      "lateration",
      "robust averaging"
    ],
    "absent_kp": [
      "least squares time of arrival location estimator",
      "least squares time difference of arrival location estimator",
      "robust estimator"
    ]
  },
  {
    "title": "em-based iterative receiver for coded mimo systems in unknown spatially correlated noise.",
    "abstract": "we present iterative channel estimation and decoding schemes for multi-input multi-output (mimo) rayleigh block fading channels in spatially correlated noise. an expectation-maximization (em) algorithm is utilized to find the maximum likelihood (ml) estimates of the channel and spatial noise covariance matrices, and to compute soft information of coded symbols which is sent to an error-control decoder. the extrinsic information produced by the decoder is then used to refine channel estimation. several iterations are performed between the above channel estimation and decoding steps. we derive modified cramer-rao bound (mcrb) for the unknown channel and noise parameters, and show that the proposed em-based channel estimation scheme achieves the mcrb at medium and high snrs. for a bit error rate of 10(-6) and long frame length, there is negligible performance difference between the proposed scheme and the ideal coherent detector that utilizes the true channel and noise covariance matrices.",
    "present_kp": [
      "mimo",
      "channel estimation",
      "expectation-maximization",
      "crb"
    ],
    "absent_kp": [
      "turbo code",
      "ber"
    ]
  },
  {
    "title": "parameter exploration in science and engineering using many-task computing.",
    "abstract": "robust scientific methods require the exploration of the parameter space of a system (some of which can be run in parallel on distributed resources), and may involve complete state space exploration, experimental design, or numerical optimization techniques. many-task computing (mtc) provides a framework for performing robust design, because it supports the execution of a large number of otherwise independent processes. further, scientific workflow engines facilitate the specification and execution of complex software pipelines, such as those found in real science and engineering design problems. however, most existing workflow engines do not support a wide range of experimentation techniques, nor do they support a large number of independent tasks. in this paper, we discuss nimrod/k-a set of add in components and a new run time machine for a general workflow engine, kepler. nimrod/k provides an execution architecture based on the tagged dataflow concepts, developed in 1980s for highly parallel machines. this is embodied in a new kepler \"director\" that supports many-task computing by orchestrating execution of tasks on on clusters, grids, and clouds. further, nimrod/k provides a set of \"actors\" that facilitate the various modes of parameter exploration discussed above. we demonstrate the power of nimrod/k to solve real problems in cardiac science.",
    "present_kp": [
      "parameter exploration",
      "kepler",
      "many-task computing"
    ],
    "absent_kp": [
      "scientific workflows"
    ]
  },
  {
    "title": "group context-based adaptations for recommendation.",
    "abstract": "in groupware or community based applications the user interface is usually static or tailored to the individual user's needs. newer developments try to adapt the user interface automatically in regard to user contexts. even though these techniques are proven useful, there exists no contextadaptive system taking the current context of a group or community in regard. in this paper, we briefly discuss the problems of defining context and present our understanding of context as a subset of the current information state. we provide an exemplary scenario to present different approaches how to compute group contexts based on semantic models and user contexts, and the consequences for the adaptation goals - in the interface or through changes at system functionalities or tools. we additionally discuss the problems occurring at evaluating adaptations and the value of group context for collaborative work.",
    "present_kp": [
      "user context",
      "semantic models",
      "group context",
      "group context-based adaptation",
      "context"
    ],
    "absent_kp": [
      "content recommendation"
    ]
  },
  {
    "title": "issues of trust and control on agent autonomy.",
    "abstract": "the relationship between trust and control is quite relevant both for the very notion of trust and for modelling and implementing trust-control relations with autonomous systems. we claim that control is antagonistic of the strict form of trust: 'trust in y'; but also that it completes and complements it for arriving to a global trust. in other words, putting control and guaranties is trust-building; it produces a sufficient trust, when trust in y's autonomous willingness and competence would not be enough. we also argue that control requires new forms of trust: trust in the control itself or in the controller, trust in y as for being monitored and controlled, trust in possible authorities, etc. finally, we show that paradoxically control could not be antagonistic of strict trust in y, but it can even create, increase it by making y more willing or more effective. in conclusion, depending on the circumstances, control makes y more reliable or less reliable; control can either decrease or increase trust. a good theory of trust cannot be complete without a theory of control.",
    "present_kp": [
      "trust",
      "control"
    ],
    "absent_kp": [
      "autonomous agents",
      "cooperation",
      "adjustable autonomy"
    ]
  },
  {
    "title": "youubi: open software for ubiquitous learning.",
    "abstract": "we propose a reference architecture for u-learning environments. we propose a method development and validation of u-learning environments. we present a u-learning environment that combines playful aspects with learning strategies.",
    "present_kp": [
      "ubiquitous learning"
    ],
    "absent_kp": [
      "ubiquitous computing",
      "software engineering",
      "design interactive",
      "gamification"
    ]
  },
  {
    "title": "single-symbol ml decodable distributed stbcs for cooperative networks.",
    "abstract": "in this correspondence, the distributed orthogonal space-time block codes (dostbcs), which achieve the single-symbol maximum likelihood (ml) decodability and full diversity order, are first considered. however, systematic construction of the dostbcs is very hard, since the noise covariance matrix is not diagonal in general. thus, some special dostbcs, which have diagonal noise covariance matrices at the destination terminal, are investigated. these codes are referred to as the row-monomial dostbcs. an upper bound of the data-rate of the row-monomial dostbc is derived and it is approximately twice higher than that of the repetition-based cooperative strategy. furthermore, systematic construction methods of the row-monomial dostbcs achieving the upper bound of the data-rate are developed when the number of relays and/or the number of information-bearing symbols are even.",
    "present_kp": [
      "cooperative networks",
      "diversity"
    ],
    "absent_kp": [
      "distributed space-time block codes",
      "single-symbol maximum likelihood  decoding"
    ]
  },
  {
    "title": "ontology-based data mining approach implemented for sport marketing.",
    "abstract": "since sport marketing is a commercial activity, precise customer and marketing segmentation must be investigated frequently and it would help to know the sport market after a specific customer profile, segmentation, or pattern come with marketing activities has found. such knowledge would not only help sport firms, but would also contribute to the broader field of sport customer behavior and marketing. this paper proposes using the apriori algorithm of association rules, and clustering analysis based on an ontology-based data mining approach, for mining customer knowledge from the database. knowledge extracted from data mining results is illustrated as knowledge patterns, rules, and maps in order to propose suggestions and solutions to the case firm, taiwan adidas, for possible product promotion and sport marketing.",
    "present_kp": [
      "sport marketing",
      "ontology",
      "data mining",
      "apriori algorithm",
      "clustering analysis"
    ],
    "absent_kp": [
      "endorser",
      "media"
    ]
  },
  {
    "title": "layered acting for character animation.",
    "abstract": "we introduce an acting-based animation system for creating and editing character animation at interactive speeds. our system requires minimal training, typically under an hour, and is well suited for rapidly prototyping and creating expressive motion. a real-time motion-capture framework records the user's motions for simultaneous analysis and playback on a large screen. the animator's real-world, expressive motions are mapped into the character's virtual world. visual feedback maintains a tight coupling between the animator and character. complex motion is created by layering multiple passes of acting. we also introduce a novel motion-editing technique, which derives implicit relationships between the animator and character. the animator mimics some aspect of the character motion, and the system infers the association between features of the animator's motion and those of the character. the animator modifies the mimic by acting again, and the system maps the changes onto the character. we demonstrate our system with several examples and present the results from informal user studies with expert and novice animators.",
    "present_kp": [
      " framework ",
      "examples",
      "association",
      "maps",
      "analysis",
      "express",
      "novice",
      "real-time",
      "informal",
      "motion",
      "user",
      "minimal",
      "change",
      "virtual world",
      "records",
      "training",
      "character",
      "aspect",
      "relationships",
      "motion-capture",
      "character animation",
      "edit",
      "layer",
      "demonstrate",
      "feedback",
      "feature",
      "animation",
      "user studies"
    ],
    "absent_kp": [
      "statistical analysis",
      "3d user interfaces",
      "interaction",
      "motion transformation",
      "visualization",
      "systems",
      "motion editing",
      "couples",
      "prototype",
      "complexity"
    ]
  },
  {
    "title": "an extensible architecture-based framework for coordination languages.",
    "abstract": "the dynamic and heterogeneous nature of distributed systems makes the development of distributed applications a difficult task. various tools, such as middleware systems, component systems, and coordination languages, offer support the application developer at different levels. there are several coordination systems that integrate such tools into a complete environment to build applications from heterogeneous components. to achieve extensibility they usually have a layered architecture: an application is first mapped to a middle layer and then to a target system. but this approach hides the specific features of a target system from the developer, as they are not represented in the middle layer, and often induces additional run-time overhead. in this paper, we introduce the extensible coordination framework ecf that allows developers to build efficient distributed applications which exploit the specific features of the target systems. support for target systems and application domains are encapsulated by extension modules. modules can be built on top of other modules to support refined functionality.",
    "present_kp": [
      "coordination language",
      "distributed systems"
    ],
    "absent_kp": [
      "component technology",
      "developer framework",
      "software architectures"
    ]
  },
  {
    "title": "novel approaches to the measurement of arterial blood flow from dynamic digital x-ray images.",
    "abstract": "we have developed two new algorithms for the measurement of blood flow from dynamic x-ray angiographic images. both algorithms aim to improve on existing techniques. first, a model-based (mb) algorithm is used to constrain the concentration-distance curve matching approach. second, a weighted optical flow algorithm (op) is used to improve on point-based optical flow methods by averaging velocity estimates along a vessel with weighting based on the magnitude of the spatial derivative. the op algorithm was validated using a computer simulation of pulsatile blood flow. both the op and the mb algorithms were validated using a physiological blood flow circuit. dynamic biplane digital x-ray images were acquired following injection of iodine contrast medium into a variety of simulated arterial vessels. the image data were analyzed using our integrated angiographic analysis software sara to give blood how waveforms using the nib and op algorithms. these waveforms were compared to flow measured using an electromagnetic flow meter (emf). in total 4935 instantaneous measurements of flow were made and compared to the emf recordings. it was found that the new algorithms showed low measurement bias and narrow limits of agreement and also outperformed the concentration-distance curve matching algorithm (org) and a modification of this algorithm (pa) in all studies.",
    "present_kp": [],
    "absent_kp": [
      "blood flow measurement",
      "x-ray angiography",
      "x-ray measurements"
    ]
  },
  {
    "title": "his-monitor: an approach to assess the quality of information processing in hospitals.",
    "abstract": "hospital information systems (his) are a substantial quality and cost factor for hospitals. systematic monitoring of his quality is an important task; however, this task is often seen to be insufficiently supported. to support systematic his monitoring, we developed his-monitor, comprising about 107 questions, focusing on how a hospital information system does efficiently support clinical and administrative tasks. the structure of his-monitor consists of a matrix, crossing his quality criteria on one axis with a list of process steps within patient care on the other axis. his-monitor was developed based on several pretests and was now tested in a larger feasibility study with 102 participants. his-monitor intends to describe strengths and weaknesses of information processing in a hospital. results of the feasibility study show that his-monitor was able to highlight certain his problems such as insufficiently supported cross-departmental communication, legibility of drug orders and other paper-based documents, and overall time needed for documentation. we discuss feasibility of his-monitor and the reliability and validity of the results. further refinement and more formal validation of his-monitor are planned.",
    "present_kp": [
      "quality",
      "hospital information systems"
    ],
    "absent_kp": [
      "healthcare information systems",
      "information management",
      "evaluation",
      "questionnaire"
    ]
  },
  {
    "title": "recursive channel estimation based on finite parameter model using reduced-complexity maximum likelihood equalizer for ofdm over doubly-selective channels.",
    "abstract": "to take intercarrier interference (ici) attributed to time variations of the channel into consideration, the time- and frequency-selective (doubly-selective) channel is parameterized by a finite parameter model. by capitalizing on the finite parameter model to approximate the doubly-selective channel, a kalman filter is developed for channel estimation. the ici suppressing, reduced-complexity viterbi-type maximum likelihood (rml) equalizer is incorporated into the kalman filter for recursive channel tracking and equalization to improve the system performance. an enhancement in the channel tracking ability is validated by theoretical analysis, and a significant improvement in ber performance using the channel estimates obtained by the recursive channel estimation method is verified by monte-carlo simulations.",
    "present_kp": [],
    "absent_kp": [
      "polynomial model",
      "oversampled basis expansion model",
      "recursive kalman",
      "reduced-complexity ml equalizer"
    ]
  },
  {
    "title": "a web-service agent-based decision support system for securities exception management.",
    "abstract": "with rising trading volumes and increasing risks in securities transactions, the securities industry is making an effort to shorten the trade lifecycle and minimize transaction risks. while attempting to achieve this, exception management is crucial to pass trade information within the trade lifecycle in a timely and accurate fashion. for a competitive solution to exception management, a web-service-agent-based decision support system is developed in this paper. agent technology is applied to deal with the dynamic, complex, and distributed processes in exception management; web services techniques are proposed for more scalability and interoperability in network-based business environment. by integrating agent technology with web services to make use of the advantages from both, this approach leads to more intelligence, flexibility and collaboration in business exception management. the effectiveness of this approach is evaluated through a use case and demonstration feedback.",
    "present_kp": [
      "web services",
      "exception management"
    ],
    "absent_kp": [
      "intelligent agents",
      "decision support systems",
      "securities trading"
    ]
  },
  {
    "title": "measuring energy consumption using eml (energy measurement library).",
    "abstract": "energy consumption and efficiency is a main issue in high performance computing systems in order to reach exascale computing. researchers in the field are focusing their effort in reducing the first and increasing the latter while there is no current standard for energy measurement. current energy measurement tools are specific and architectural dependent and this has to be addressed. by creating a standard tool, it is possible to generate independence between the experiments and the hardware, and thus, researchers effort can be focused in energy, by maximizing the portability of the code used for experimentation with the multiple architectures we have access nowadays. we present the energy measurement library (eml) library, a software library that eases the access to the energy measurement tools and can be easily extended to add new measurement systems. using eml, it is viable to obtain architectural and algorithmic parameters that affect energy consumption and efficiency. the use of this library is tested in the field of the analytic modeling of the energy consumed by parallel programs.",
    "present_kp": [
      "energy measurement library",
      "eml"
    ],
    "absent_kp": [
      "energy-aware computing",
      "energy efficiency"
    ]
  },
  {
    "title": "methylation-sensitive representational difference analysis and its application to cancer research.",
    "abstract": "methylation-sensitive representational difference analysis (ms-rda) was previously established to detect differences in the methylation status of two genomes. this method uses the digestion of genomic dna with a methylation-sensitive restriction enzyme, hpaii, and pcr to prepare hpaii-amplicons, followed by rda. an hpaii-amplicon prepared using betaine and reverse electrophoresis was enriched 3.6-fold (compared with the hpaii-amplicon prepared by the original method) with dna fragments originating from cpg islands (cgis). as for the specificity of ms-rda, it was shown that dna fragments that are unmethylated in the tester and almost completely methylated in the driver are efficiently isolated. this indicated that genes that are in biallelic methylation or in monoallelic methylation with loss of the other allele are efficiently isolated. further, by use of two additional methylation-sensitive six-base recognition restriction enzymes, sacii and nari, more dna fragments were isolated from cgis in the 5? regions of genes. after analysis of human lung, gastric, and breast cancers, 12 genes were seen to be silenced and additional genes seen to show decreased expression in association with methylation of genomic regions outside cgis in the 5? regions of genes. ms-rda is effective in identifying silenced genes in various cancers.",
    "present_kp": [
      "cpg island",
      "cancer"
    ],
    "absent_kp": [
      "dna methylation",
      "gene silencing",
      "genome scanning"
    ]
  },
  {
    "title": "simple ptass for families of graphs excluding a minor.",
    "abstract": "we show that very simple algorithms based on local search are polynomial-time approximation schemes for maximum independent set, minimum vertex cover and minimum dominating set, when the input graphs have a fixed forbidden minor.",
    "present_kp": [
      "polynomial-time approximation scheme",
      "forbidden minor",
      "maximum independent set",
      "minimum vertex cover",
      "minimum dominating set"
    ],
    "absent_kp": [
      "separator",
      "local optimization"
    ]
  },
  {
    "title": "a modification of the index of liou and wang for ranking fuzzy number.",
    "abstract": "different methods have been proposed for ranking fuzzy numbers. these include methods based on distances, centroid point, coefficient of variation, and weighted mean value. however, there is still no method that can always give a satisfactory result to every situation; some are counterintuitive and not discriminating. this paper presents an approach for ranking fuzzy numbers with integral value that is an extension of the index of liou and wang. this method, that is independent of the type of membership function used, can rank more than two fuzzy numbers simultaneously. this ranking method use an index of optimism to reflect the decision maker's optimistic attitude, but rather it also contains an index of modality that represents the neutrality of the decision maker. the approach is illustrated with numerical examples.",
    "present_kp": [
      "ranking fuzzy numbers",
      "integral value",
      "index of optimism",
      "index of modality"
    ],
    "absent_kp": []
  },
  {
    "title": "a mobility-based load control scheme in hierarchical mobile ipv6 networks.",
    "abstract": "by introducing a mobility anchor point (map), hierarchical mobile ipv6 (hmipv6) reduces the signaling overhead and handoff latency associated with mobile ipv6. in this paper, we propose a mobility-based load control (mlc) scheme, which mitigates the burden of the map in fully distributed and adaptive manners. the mlc scheme combines two algorithms: a threshold-based admission control algorithm and a session-to-mobility ratio (smr)-based replacement algorithm. the threshold-based admission control algorithm gives higher priority to ongoing mobile nodes (mns) than new mns, by blocking new mns when the number of mns being serviced by the map is greater than a predetermined threshold. on the other hand, the smr-based replacement algorithm achieves efficient map load distribution by considering mns traffic and mobility patterns. we analyze the mlc scheme using the continuous time markov chain in terms of the new mn blocking probability, ongoing mn dropping probability, and binding update cost. also, the map processing latency is evaluated based on the m/g/1 queueing model. analytical and simulation results demonstrate that the mlc scheme outperforms other schemes and thus it is a viable solution for scalable hmipv6 networks.",
    "present_kp": [
      "hierarchical mobile ipv6",
      "mobility-based load control",
      "mobility anchor point",
      "admission control algorithm",
      "session-to-mobility ratio"
    ],
    "absent_kp": []
  },
  {
    "title": "randomized diffusion for indivisible loads.",
    "abstract": "presentation of new algorithm for balancing discrete loads. algorithm is very natural and avoids nodes having negative loads. quality measure is the gap between maximum and minimum load, called discrepancy. upper bounds on discrepancy after algorithm has run as long as its continuous counterpart.",
    "present_kp": [
      "diffusion"
    ],
    "absent_kp": [
      "distributed computing",
      "load balancing",
      "randomized algorithms",
      "random walk"
    ]
  },
  {
    "title": "automatic image segmentation by dynamic region merging.",
    "abstract": "this paper addresses the automatic image segmentation problem in a region merging style. with an initially oversegmented image, in which many regions (or superpixels) with homogeneous color are detected, an image segmentation is performed by iteratively merging the regions according to a statistical test. there are two essential issues in a region-merging algorithm: order of merging and the stopping criterion. in the proposed algorithm, these two issues are solved by a novel predicate, which is defined by the sequential probability ratio test and the minimal cost criterion. starting from an oversegmented image, neighboring regions are progressively merged if there is an evidence for merging according to this predicate. we show that the merging order follows the principle of dynamic programming. this formulates the image segmentation as an inference problem, where the final segmentation is established based on the observed image. we also prove that the produced segmentation satisfies certain global properties. in addition, a faster algorithm is developed to accelerate the region-merging process, which maintains a nearest neighbor graph in each iteration. experiments on real natural images are conducted to demonstrate the performance of the proposed dynamic region-merging algorithm.",
    "present_kp": [
      "image segmentation",
      "region merging"
    ],
    "absent_kp": [
      "dynamic programming ",
      "wald's sequential probability ratio test "
    ]
  },
  {
    "title": "boundary properties of the inconsistency of pairwise comparisons in group decisions.",
    "abstract": "we study the inconsistency of pairwise comparisons in group decision making. we study the effect of the aggregation of pairwise comparisons on their consistency. we derive general results and provide a complete study for well-known inconsistency indices. we start a discussion on the meaning of inconsistency of aggregated preferences.",
    "present_kp": [
      "inconsistency indices",
      "boundary properties",
      "group decision making"
    ],
    "absent_kp": [
      "pairwise comparison matrix",
      "analytic hierarchy process"
    ]
  },
  {
    "title": "sensitivity of optimal prices to system parameters in a steady-state service facility.",
    "abstract": "we consider the problem of maximizing the long-run average reward in a service facility with dynamic pricing. we investigate sensitivity of optimal pricing policies to the parameters of the service facility which is modelled as an m/m/s/k m / m / s / k queueing system. arrival process to the facility is poisson with arrival rate a decreasing function of the price currently being charged by the facility. we prove structural results on the optimal pricing policies when the parameters in the facility change. namely, we show that optimal prices decrease when the capacity of the facility or the number of servers in the facility increase. under a reasonable assumption, we also show that optimal prices increase as the overall demand for the service provided by the facility increases or when the service rate of the facility decreases. we illustrate how these structural results simplify the required computational effort while finding the optimal policy.",
    "present_kp": [
      "queueing",
      "pricing"
    ],
    "absent_kp": [
      "stochastic programming",
      "markov decision processes",
      "robustness and sensitivity analysis"
    ]
  },
  {
    "title": "a self-organizing p2p system with multi-dimensional structure.",
    "abstract": "this paper presents and analyzes self-can, a self-organizing p2p system that, while relying on the multi-dimensional structured organization of peers provided by can, exploits the operations of ant-based mobile agents to sort the resource keys and distribute them to peers. the benefits of the self-organization approach are remarkable, starting from increased flexibility and robustness, to better load balancing characteristics. most notably, peer indexes and resource keys can be defined on different and independent spaces, which overcomes the main limitation of standard structured p2p systems, i.e., the necessity of assigning each key to a peer having a specified index. this decoupling opens the possibility of giving a semantic meaning to resource keys and enables the efficient execution of multi-dimensional range queries, which are essential in some types of distributed systems, for example in grids.",
    "present_kp": [
      "self-organizing"
    ],
    "absent_kp": [
      "bio-inspired",
      "peer-to-peer",
      "key-value storage",
      "resource discovery"
    ]
  },
  {
    "title": "self-organized traffic control.",
    "abstract": "in this paper we propose and present preliminary results on the migration of traffic lights as roadside-based infrastructures to in-vehicle virtual signs supported only by vehicle- to-vehicle communications. we design a virtual traffic light protocol that can dynamically optimize the flow of traffic in road intersections without requiring any roadside infrastructure. elected vehicles act as temporary road junction infrastructures and broadcast traffic light messages that are shown to drivers through in-vehicle displays. this approach renders signalized control of intersections truly ubiquitous, which significantly increases the overall traffic flow. we pro- vide compelling evidence that our proposal is a scalable and cost-effective solution to urban traffic control.",
    "present_kp": [
      "self-organized traffic",
      "traffic lights"
    ],
    "absent_kp": [
      "v2v communication"
    ]
  },
  {
    "title": "search strategies on a new health information retrieval system.",
    "abstract": "purpose - the goals of this study are: to evaluate the merits of a newly developed health information retrieval system; to investigate users' search strategies when using the new search system; and to study the relationships between users' search strategies and their prior topic knowledge. design/methodology/approach - the paper developed a new health information retrieval system called meshmed. a term browser and a tree browser are included in the new system in addition to the traditional search box. the term browser allows a user to search medical subject heading (mesh) terms using natural language. the tree browser presents a hierarchical tree structure of related mesh terms. a user study with 30 participants was conducted to evaluate the benefits of meshmed. findings - the paper found that meshmed provides a user with more choices to select an appropriate searching component and form more effective search strategies. based on the time a participant spent using different meshmed components, the paper identified three different search styles: the traditional style, the novel style, and the balanced style, which falls in between. meshmed was particularly helpful for users with low topic knowledge. originality/value - a new health information retrieval system (meshmed) was designed and developed (and is currently available at <url>9/meshmed). this is the first study to explore users' search strategies on such a system. the study results can inform the design of future clinical-oriented health information retrieval systems.",
    "present_kp": [
      "information retrieval"
    ],
    "absent_kp": [
      "knowledge management",
      "hospitals",
      "information systems"
    ]
  },
  {
    "title": "anfis approach to the scour depth prediction at a bridge abutment.",
    "abstract": "an accurate estimation of the maximum possible scour depth at bridge abutments is of paramount importance in decision-making for the safe abutment foundation depth and also for the degree of scour counter-measure to be implemented against excessive scouring. despite analysis of innumerable prototype and hydraulic model studies in the past, the scour depth prediction at the bridge abutments has remained inconclusive. this paper presents an alternative to the conventional regression model (rm) in the form of an adaptive network-based fuzzy inference system (anfis) modelling. the performance of anfis over rm and artificial neural networks (anns) is assessed here. it was found that the anfis model performed best among of these methods. the causative variables in raw form result in a more accurate prediction of the scour depth than that of their grouped form.",
    "present_kp": [
      "bridge abutments",
      "neural network"
    ],
    "absent_kp": [
      "anfis and regression analysis",
      "local scour"
    ]
  },
  {
    "title": "an improved strength pareto evolutionary algorithm 2 with application to the optimization of distributed generations.",
    "abstract": "this paper presents an improved strength pareto evolutionary algorithm 2 (ispea2), which introduces a penalty factor in objective function constraints, uses adaptive crossover and a mutation operator in the evolutionary process, and combines simulated annealing iterative process over spea2. the testing result of ispea2 by authoritative testing functions meets the requirement of petro-optimum fronts. the case study result shows that the proposed algorithm provides a rapid convergence in obtaining pareto-optimal solutions during the calculation process of evolution. based on the fuzzy set theory, ispea2 is able to solve the multi-objective problems in the ieee 33-bus system, and its validity and practicality are demonstrated by the utilization on dgs economic dispatch and optimal operation in the field of power industry.",
    "present_kp": [
      "improved strength pareto evolutionary algorithm",
      "simulated annealing",
      "distributed generation"
    ],
    "absent_kp": [
      "distribution power system",
      "coordinative optimization"
    ]
  },
  {
    "title": "a distributed instrument for performance analysis of real-time ethernet networks.",
    "abstract": "ethernet technology is widely used in real-time industrial automation. thanks to real-time ethernet (rte) protocols, defined in iec61784-2 standard, new top-performance automation solutions can be created. such systems may have communication cycle time down to tens of mu s and cycle jitter less than 1 mu s, making network testing and debugging very critical. existing network and protocol analyzers can perform detailed local analysis, but characterization of high-performance rte systems requires measurement of transmission delays and these instruments cannot be adequately synchronized among them to realize a distributed measurement network. this paper introduces a new low-cost distributed measurement instrument to measure timing characteristics of rte nodes (end-to-end delays, synchronization, etc.). the proposed instrument has multiple fpga-based probes that allow for simultaneous/synchronized logging on different place of the target rte network. a pc-based \"monitor station\" stores all the data, ready for further elaboration. architecture details are discussed, a prototype has been realized, and some experimental results are presented. for instance, synchronization accuracy between probes is below 100 ns.",
    "present_kp": [
      "ethernet network",
      "performance analysis"
    ],
    "absent_kp": [
      "fieldbus",
      "network synchronization",
      "real time"
    ]
  },
  {
    "title": "documentation standards for beginning students.",
    "abstract": "the importance of writing programs that are readable has finally gained preeminence in the struggle with such competing and contradictory goals as cuteness and optimization of code. as a result, a much greater stress on documentation standards is found in computer science education these days. industry and government standards for documentation are being more widely adhered to and certain points of agreement have emerged. some excellent books have been written that cover the subject (van tassel, 1974; ledgard, 1975; kernighan & plauger, 1974); however it is safe to say that both the exhaustive treatment of the subject in such publications and the extremely high standards proposed probably preclude wholesale adoption by instructors of beginning level programming courses. what is proposed here is a set of common sense, scaled down documentation standards for the student in a first programming course in, say, fortran, pl/i, algol, or basic. the following represents an amalgam of documentation requirements achieved as a result of teaching introductory programming to college students for nine years. the actual sources have been the literature, colleagues, and last but not least, experience. they are not intended to represent an only or best approach; the author has recently encountered other efforts in this direction that must surely be as reasonable and effective. it does represent one educator's approach; it is sufficiently scaled down so that one might reasonably expect to use it as a standard for beginning students; and it may be most useful as a contributor of components to be integrated into a more effective set of standards. the basics of documentation and readable programming include comments, meaningful variable names, labelled output, flowcharts, and clear program flow. the major components of and basic rules for each of these categories will be presented in the context of the needs and limitations of the beginning student.",
    "present_kp": [
      "requirements",
      "point",
      "use",
      "goals",
      "computer science education",
      "context",
      "teaching",
      "direct",
      "experience",
      "writing",
      "student",
      "component",
      "program",
      "introductory programming",
      "author",
      "public",
      "code",
      "flow",
      "adopt",
      " stress ",
      "documentation",
      "effect",
      "rules"
    ],
    "absent_kp": [
      "standardization",
      "variability",
      "industrial",
      "governance",
      "integrability"
    ]
  },
  {
    "title": "differential space-time modulation for ds-cdma systems.",
    "abstract": "differential space-time modulation (dstm) schemes were recently proposed to fully exploit the transmit and receive antenna diversities without the need for channel state information. dstm is attractive in fast flat fading channels since accurate channel estimation is difficult to achieve. in this paper. we propose a new modulation scheme to improve the performance of ds-cdma systems in fast time-dispersive fading channels. this scheme is referred to as the differential space-time modulation for ds-cdma (dst-cdma) systems. the new modulation and demodulation schemes are especially studied for the fast fading down-link transmission in ds-cdma systems employing multiple transmit antennas and one receive antenna. we present three demodulation schemes. referred to as the differential space-time rake (dstr) receiver, differential space-time deterministic (dstd) receiver, and differential space-time deterministic de-prefix (dstdd) receiver, respectively. the dstd receiver exploits the known information of the spreading sequences and their delayed paths deterministically besides the rake-type combination: consequently, it can outperform the dstr receiver. which employs the rake-type combination only, especially for moderate-to-high snr. the dstdd receiver avoids the effect of intersymbol interference and hence can offer better performance than the dstd receiver.",
    "present_kp": [
      "ds-cdma"
    ],
    "absent_kp": [
      "wireless communications",
      "space-time coding",
      "smart antennas",
      "spread spectrum",
      "rake receiver"
    ]
  },
  {
    "title": "on the normalization of interval and fuzzy weights.",
    "abstract": "the normalization of interval and fuzzy weights is often necessary in multiple criteria decision analysis (mcda) under uncertainty, especially in analytic hierarchy process (ahp) with interval or fuzzy judgements. the existing normalization methods based on interval arithmetic and fuzzy arithmetic are found flawed and need to be revised. this paper presents the correct normalization methods for interval and fuzzy weights and offers relevant theorems in support of them. numerical examples are examined to show the correctness of the proposed normalization methods and their differences from those existing normalization methods.",
    "present_kp": [
      "fuzzy weights",
      "normalization"
    ],
    "absent_kp": [
      "interval weights",
      "multiple criteria decision making"
    ]
  },
  {
    "title": "a queue with semi-markovian batch plus poisson arrivals with application to the mpeg frame sequence.",
    "abstract": "we consider a queueing system with a single server having a mixture of a semi-markov process (smp) and a poisson process as the arrival process, where each smp arrival contains a batch of customers. the service times are exponentially distributed. we derive the distributions of the queue length of both smp and poisson customers when the sojourn time distributions of the smp have rational laplace-stieltjes transforms. we prove that the number of unknown constants contained in the generating function for the queue length distribution equals the number of zeros of the denominator of this generating function in the case where the sojourn times of the smp follow exponential distributions. the linear independence of the equations generated by those zeros is discussed for the same case with additional assumption. the necessary and sufficient condition for the stability of the system is also analyzed. the distributions of the waiting times of both smp and poisson customers are derived. the results are applied to the case in which the smp arrivals correspond to the exact sequence of motion picture experts group ( mpeg) frames. poisson arrivals are regarded as interfering traffic. in the numerical examples, the mean and variance of the waiting time of the atm cells generated from the mpeg frames of real video data are evaluated.",
    "present_kp": [
      "semi-markov process",
      "queue",
      "waiting time",
      "mpeg"
    ],
    "absent_kp": [
      "batch arrival",
      "group of pictures "
    ]
  },
  {
    "title": "fractal dimension as a descriptor of urban growth dynamics.",
    "abstract": "the objective of this paper is to examine the development of the urban form of the city of olomouc since the 1920s in terms of fractal dimension, and to link the observation with two other descriptors of shape - area and perimeter. the fractal dimension of built-up areas and fractal dimension of the boundary of the city are calculated employing the box-counting method; the possibilities of their interpretation and usage in urban planning are discussed. the process of urban growth is observed with respect to its fractality and perspectives of this approach are discussed. an interesting dependence between area and its fractal dimension is derived.",
    "present_kp": [
      "urban growth",
      "fractal dimension",
      "box-counting method"
    ],
    "absent_kp": [
      "area/perimeter relation"
    ]
  },
  {
    "title": "adaptive critics for dynamic optimization.",
    "abstract": "a novel action-dependent adaptive critic design (acd) is developed for dynamic optimization. the proposed combination of a particle swarm optimization-based actor and a neural network critic is demonstrated through dynamic sleep scheduling of wireless sensor motes for wildlife monitoring. the objective of the sleep scheduler is to dynamically adapt the sleep duration to nodes battery capacity and movement pattern of animals in its environment in order to obtain snapshots of the animal on its trajectory uniformly. simulation results show that the sleep time of the node determined by the actor critic yields superior quality of sensory data acquisition and enhanced node longevity.",
    "present_kp": [
      "adaptive critic design",
      "sleep scheduling",
      "wildlife monitoring"
    ],
    "absent_kp": [
      "energy efficiency",
      "wireless sensor networks"
    ]
  },
  {
    "title": "linearity and recursion in a typed lambda-calculus.",
    "abstract": "we show that the full pcf language can be encoded in l _rec, a syntactically linear ?-calculus extended with numbers, pairs, and an unbounded recursor that preserves the syntactic linearity of the calculus. we give call-by-name and call-by-value evaluation strategies and discuss implementation techniques for l _rec, exploiting its linearity.",
    "present_kp": [
      "recursion",
      "pcf"
    ],
    "absent_kp": [
      "linear lambda calculus"
    ]
  },
  {
    "title": "power aware page allocation.",
    "abstract": "one of the major challenges of post-pc computing is the need to reduce energy consumption, thereby extending the lifetime of the batteries that power these mobile devices. memory is a particularly important target for efforts to improve energy efficiency. memory technology is becoming available that offers power management features such as the ability to put individual chips in any one of several different power modes. in this paper we explore the interaction of page placement with static and dynamic hardware policies to exploit these emerging hardware features. in particular, we consider page allocation policies that can be employed by an informed operating system to complement the hardware power management strategies. we perform experiments using two complementary simulation environments: a trace-driven simulator with workload traces that are representative of mobile computing and an execution-driven simulator with a detailed processor/memory model and a more memory-intensive set of benchmarks (spec2000). our results make a compelling case for a cooperative hardware/software approach for exploiting power-aware memory, with down to as little as 45% of the energy delay for the best static policy and 1% to 20% of the energy delay for a traditional full-power memory.",
    "present_kp": [
      "challenge",
      "lifetime",
      "traces",
      "policy",
      "simulation",
      "benchmark",
      "memory model",
      "batteries",
      "delay",
      "case",
      "paper",
      "power-aware",
      "processor",
      "interaction",
      "software",
      "strategies",
      "environments",
      "energy consumption",
      "mobile device",
      "workload",
      "exploit",
      "dynamic",
      "energy",
      "operating system",
      "placement",
      "power",
      "hardware",
      "mobile computing",
      "feature",
      "energy efficiency",
      "allocation"
    ],
    "absent_kp": [
      "paging",
      "computation",
      "experience",
      "informal",
      "exploration",
      "technologies",
      "cooperation",
      "memorialized",
      "power-management"
    ]
  },
  {
    "title": "change rules for hierarchical beliefs.",
    "abstract": "the paper builds a belief hierarchy as a framework common to all uncertainty measures expressing that an actor is ambiguous about his uncertain beliefs. the belief hierarchy is further interpreted by distinguishing physical and psychical worlds, associated to objective and subjective probabilities. various rules of transformation of a belief hierarchy are introduced, especially changing subjective beliefs into objective ones. these principles are applied in order to relate different contexts of belief change, revising, updating and even focusing. the numerous belief change rules already proposed in the literature receive epistemic justifications by associating them to specific belief hierarchies and change contexts. as a result, it is shown that the resiliency of probability judgments may have some limits and be reconciled with the possibility of learning from factual messages.",
    "present_kp": [
      "belief change",
      "focusing",
      "hierarchical belief",
      "revising",
      "updating"
    ],
    "absent_kp": [
      "objective probability",
      "subjective probability"
    ]
  },
  {
    "title": "a characterization of the two-commodity network design problem.",
    "abstract": "we study the uncapacitated version of the two-commodity network design problem. we characterize optimal solutions and show that when flow costs are zero there is an optimal solution with at most one shared path. using this characterization, we solve the problem on a transformed graph with o(n) nodes and o(m) arcs based on a shortest path algorithm. next, we describe a linear programming reformulation of the problem using o(m) variables and o(n) constraints and show that it always has an integer optimal solution. we also interpret the dual constraints and variables as generalizations of the are constraints and node potentials for the shortest path problem. we show that the polyhedron described by the constraints of the reformulation always has an integer optimal solution for a more general two-commodity problem with flow costs and an additional condition on the cost function.",
    "present_kp": [
      "network design",
      "integer optima"
    ],
    "absent_kp": [
      "two commodity"
    ]
  },
  {
    "title": "on the minimum number of negations leading to super-polynomial savings.",
    "abstract": "we show that an explicit sequence of monotone functions f(n): {0, 1}(n) --> {0, 1}(m) (m less than or equal to n) can be computed by boolean circuits with polynomial (in n) number of and, or and not gates, but every such circuit must use at least log n - o(log log n) not gates. this is almost optimal because results of markov and fisher imply that, with only small increase of the total number of gates, any circuit in n variables can be simulated by a circuit with at most [log (n + 1)] not gates.",
    "present_kp": [],
    "absent_kp": [
      "computational complexity",
      "negation-limited circuits"
    ]
  },
  {
    "title": "differential log-domain wave active filters.",
    "abstract": "in this paper, the design of differential log-domain wave filters is outlined which is based on the log-domain wave technique. the differential configuration is achieved by introducing the differential log-domain wave equivalent. the resulting filters inherit the good characteristics of the wave active filters, are very modular and easy to design. the method is demonstrated by a design example and its validity is verified through simulations results.",
    "present_kp": [
      "wave active filters"
    ],
    "absent_kp": [
      "analogue filters",
      "differential filters",
      "log-domain filters"
    ]
  },
  {
    "title": "a hybrid method based on linear programming and tabu search for routing of logging trucks.",
    "abstract": "in this paper, we consider an operational routing problem to decide the daily routes of logging trucks in forestry. this industrial problem is difficult and includes aspects such as pickup and delivery with split pickups, multiple products, time windows, several time periods, multiple depots, driver changes and a heterogeneous truck fleet. in addition, the problem size is large and the solution time limited. we describe a two-phase solution approach which transforms the problem into a standard vehicle routing problem with time windows. in the first phase, we solve an lp problem in order to find a destination of flow from supply points to demand points. based on this solution, we create transport nodes which each defines the origin(s) and destination for a full truckload. in phase two, we make use of a standard tabu search method to combine these transport nodes, which can be considered to be customers in vehicle routing problems, into actual routes. the tabu search method is extended to consider some new features. the solution approach is tested on a set of industrial cases from major forest companies in sweden.",
    "present_kp": [
      "forestry",
      "routing",
      "tabu search",
      "linear programming"
    ],
    "absent_kp": [
      "or in practice"
    ]
  },
  {
    "title": "the definition of assembly line balancing difficulty and evaluation of balance solution quality.",
    "abstract": "assembly line balancing is a classic ill-structured problem where total enumeration is infeasible and optimal solutions uncertain for industrial problems. a quantitative approach to classifying problem difficulty and solution quality is therefore important. two existing measures of difficulty, order strength and west ratio are compared to a new compound expression of difficulty, project index. project index is based on individual assessment of precedence (precedence index) and task time (task time index). the current working definition of project index is given. early criteria for judging assembly lines use balance delay and smoothness index, both are flawed as criteria. line and balance efficiency are developed as more appropriate. project index, line and balance efficiency will be illustrated for a published test-case examined by the a?line balancing package. the potential for a learning approach, selecting models to suit problems using the measures of difficulty, will form part of the conclusions within this paper.",
    "present_kp": [
      "balancing",
      "evaluation"
    ],
    "absent_kp": [
      "assembly-line"
    ]
  },
  {
    "title": "a class-oriented feature selection approach for multi-class imbalanced network traffic datasets based on local and global metrics fusion.",
    "abstract": "feature selection is often used as a pre-processing step for machine learning based network traffic classification. many feature selection techniques have been developed to find an optimal subset of relevant features and to improve overall classification accuracy. but such techniques ignore the class imbalance problem encountered in network traffic classification. the selected feature subset may bias towards the traffic class that occupies the majority of traffic flows on the internet. to address this issue, this paper proposes a new approach, called class-oriented feature selection (cofs), to identify a relevant feature subset for every class. it combines the proposed local metric and the existing global metric to yield a potentially optimal feature subset for each class, and then removes the redundant features in each feature subset based on the weighted symmetric uncertainty. additionally, to enhance the generalization on network traffic data, an ensemble learning based scheme is presented with cofs to overcome the negative impacts of the data drift on a traffic classifier. experiments on real-world network traffic data show that cofs outperforms existing feature selection techniques in most cases. moreover, our approach achieves >96% flow accuracy and >93% byte accuracy on average.",
    "present_kp": [
      "feature selection",
      "multi-class imbalance",
      "data drift",
      "network traffic"
    ],
    "absent_kp": [
      "local metrics"
    ]
  },
  {
    "title": "identifying and quantifying structural nonlinearities in engineering applications from measured frequency response functions.",
    "abstract": "engineering structures seldom behave linearly and, as a result, linearity checks are common practice in the testing of critical structures exposed to dynamic loading to define the boundary of validity of the linear regime. however, in large scale industrial applications, there is no general methodology for dynamicists to extract nonlinear parameters from measured vibration data so that these can be then included in the associated numerical models. in this paper, a simple method based on the information contained in the frequency response function (frf) properties of a structure is studied. this technique falls within the category of single-degree-of-freedom (sdof) modal analysis methods. the principle upon which it is based is effectively a linearisation whereby it is assumed that at given amplitude of displacement response the system responds at the same frequency as the excitation and that stiffness and damping are constants. in so doing, by extracting this information at different amplitudes of vibration response, it is possible to estimate the amplitude-dependent natural frequency and modal loss factor. because of its mathematical simplicity and practical implementation during standard vibration testing, this method is particularly suitable for practical applications. in this paper, the method is illustrated and new analyses are carried out to validate its performance on numerical simulations before applying it to data measured on a complex aerospace test structure as well as a full-scale helicopter.",
    "present_kp": [],
    "absent_kp": [
      "nonlinear identification",
      "nonlinear modal testing",
      "nonlinear modal analysis",
      "experimental",
      "nonlinear modal analysis"
    ]
  },
  {
    "title": "on the formal specification and verification of multi-agent systems.",
    "abstract": "this article describes first steps towards the formal specification and verification of multiagent systems, through the use of temporal belief logics. the article first describes concurrent metatem, a multi-agent programming language, and then develops a logic that may be used to reason about concurrent metatem systems. the utility of this logic for specifying and verifying concurrent metatem systems is demonstrated through a number of examples. the article concludes with a brief discussion on the wider implications of the work, and in particular on the use of similar logics for reasoning about multi-agent systems in general.",
    "present_kp": [
      "formal specification and verification",
      "multi-agent systems"
    ],
    "absent_kp": []
  },
  {
    "title": "a contribution to multimedia document modeling and querying.",
    "abstract": "metadata on multimedia documents may help to describe their content and make their processing easier, for example by identifying events in temporal media, as well as carrying descriptive information for the overall resource. metadata is essentially static and may be associated with, or embedded in, the multimedia contents. the aim of this paper is to present a proposal for multimedia documents annotation, based on modeling and unifying features elicited from content and structure mining. our approach relies on the availability of annotated metadata representing segment content and structure as well as segment transcripts. temporal and spatial operators are also taken into account when annotating documents. any feature is identified into a descriptor called \"meta-document\". these meta-documents are the basis of querying by adapted query languages.",
    "present_kp": [
      "metadata",
      "annotation",
      "querying"
    ],
    "absent_kp": [
      "spatiotemporal operators"
    ]
  },
  {
    "title": "a comparison of pressure and tilt input techniques for cursor control.",
    "abstract": "three experiments were conducted in this study to investigate the human ability to control pen pressure and pen tilt input, by coupling this control with cursor position, angle and scale. comparisons between pen pressure input and pen tilt input have been made in the three experiments. experimental results show that decreasing pressure input resulted in very poor performance and was not a good input technique for any of the three experiments. in \"experiment 1-coupling to cursor position\", the tilt input technique performed relatively better than the increasing pressure input technique in terms of time. even though the tilt technique had a slightly higher error rate. in \"experiment 2-coupling to cursor angle\", the tilt input performed a little better than the increasing pressure input in terms of time, but the gap between them is not so apparent as experiment 1. in \"experiment 3-coupling to cursor scale\", tilt input performed a little better than increasing pressure input in terms of adjustment time. based on the results of our experiments, we have inferred several design implications and guidelines.",
    "present_kp": [
      "pressure input",
      "tilt input"
    ],
    "absent_kp": [
      "target selection tasks",
      "pen-based interfaces"
    ]
  },
  {
    "title": "a framework for analyzing the cognitive complexity of computer-assisted clinical ordering.",
    "abstract": "computer-assisted provider order entry is a technology that is designed to expedite medical ordering and to reduce the frequency of preventable errors. this paper presents a multifaceted cognitive methodology for the characterization of cognitive demands of a medical information system. our investigation was informed by the distributed resources (dr) model, a novel approach designed to describe the dimensions of user interfaces that introduce unnecessary cognitive complexity. this method evaluates the relative distribution of external (system) and internal (user) representations embodied in system interaction. we conducted an expert walkthrough evaluation of a commercial order entry system, followed by a simulated clinical ordering task performed by seven clinicians. the dr model was employed to explain variation in user performance and to characterize the relationship of resource distribution and ordering errors. the analysis revealed that the configuration of resources in this ordering application placed unnecessarily heavy cognitive demands on the user, especially on those who lacked a robust conceptual model of the system. the resources model also provided some insight into clinicians interactive strategies and patterns of associated errors. implications for user training and interface design based on the principles of humancomputer interaction in the medical domain are discussed.",
    "present_kp": [
      "provider order entry"
    ],
    "absent_kp": [
      "medical errors",
      "cognitive evaluation",
      "distributed cognition",
      "information systems"
    ]
  },
  {
    "title": "crack propagation analysis in composite materials by using moving mesh and multiscale techniques.",
    "abstract": "a novel multiscale method for crack propagation analysis in composites is proposed. an adaptive model refinement is used during crack propagation to improve efficiency. competition between different damage mechanisms is handled during crack simulation. matrix cracking is modeled by a novel optimization strategy based on moving meshes. the proposed approach is validated by original comparisons with existing methods.",
    "present_kp": [
      "composite materials",
      "crack propagation",
      "moving mesh"
    ],
    "absent_kp": [
      "concurrent multiscale methods",
      "micromechanics",
      "interface debonding"
    ]
  },
  {
    "title": "motivations in virtual health communities and their relationship to community, connectedness and stress.",
    "abstract": "this study explores the relationships between motivations for joining virtual health communities, online behaviors, and psycho-social outcomes. a sample of 144 women from two virtual health communities focusing on infertility completed survey measures assessing motivations, posting and receiving support, connectedness, community, and stress. our results indicate that socio-emotional support motivations for joining the community were associated with posting support within the virtual community, while informational motivations were related to receiving support. further, receiving support was associated with greater sense of virtual community as well as more general feelings of connectedness, which was related to less stress. implications for virtual health community research are discussed.",
    "present_kp": [
      "virtual health communities",
      "motivations",
      "connectedness",
      "stress",
      "sense of virtual community",
      "infertility"
    ],
    "absent_kp": []
  },
  {
    "title": "enterprise resource planning: implementation procedures and critical success factors.",
    "abstract": "enterprise resource planning (erp) systems are highly complex information systems. the implementation of these systems is a difficult and high cost proposition that places tremendous demands on corporate time and resources. many erp implementations have been classified as failures because they did not achieve predetermined corporate goals. this article identifies success factors, software selection steps, and implementation procedures critical to a successful implementation. a case study of a largely successful erp implementation is presented and discussed in terms of these key factors.",
    "present_kp": [
      "enterprise resource planning",
      "critical success factors",
      "implementation procedures"
    ],
    "absent_kp": [
      "business process reengineering",
      "project management"
    ]
  },
  {
    "title": "dflowz: a free program to evaluate the area potentially inundated by a debris flow.",
    "abstract": "debris flow inundated area can be estimated using scaling relationships. we provide a free, open-source program to evaluate debris flow hazard. the model considers the uncertainties in scaling relationships and input data. a graphical user interface facilitate the process of susceptibility mapping.",
    "present_kp": [
      "debris flow",
      "scaling relationships"
    ],
    "absent_kp": [
      "flooding",
      "hazard assessment"
    ]
  },
  {
    "title": "a novel approach to identify optimal access point and capacity of multiple dgs in a small, medium and large scale radial distribution systems.",
    "abstract": "distributed generation (dg) sources are predicated to play major role in distribution systems due to the demand growth for electrical energy. location and sizing of dg sources found to be important on the system losses and voltage stability in a distribution network. in this paper an efficient technique is presented for optimal placement and sizing of dgs in a large scale radial distribution system. the main objective is to minimize network power losses and to improve the voltage stability. a detailed performance analysis is carried out on 33-bus, 69-bus and 118-bus large scale radial distribution systems to demonstrate the effectiveness of the proposed technique. performing multiple power flow analysis on 118-bus system, the effect of dg sources on the most sensitive buses to voltage collapse is also carried out.",
    "present_kp": [
      "distributed generation",
      "large scale radial distribution system",
      "voltage"
    ],
    "absent_kp": [
      "simulated annealing",
      "stability index"
    ]
  },
  {
    "title": "robust discrete control of nonlinear processes: application to chemical reactors.",
    "abstract": "trajectory tracking or rejecting persistent disturbances with digital controllers in nonlinear processes is a class of problems where classical control methods breakdown since it is very difficult to describe the dynamic behavior over the entire trajectory. in this paper, a model-based robust control scheme is proposed as a potential solution approach for these systems. the proposed control algorithm is a robust error feedback controller that allows us to track predetermined operation profiles while attenuating the disturbances and maintaining the stability conditions of the nonlinear processes. various numerical simulation examples demonstrate the effectiveness of this robust scheme. two examples deal with effective trajectory tracking in chemical reactors over a wide range of operating conditions. the third example analyses the attenuation of periodic load in a biological reactor. all examples illustrate the ability of the robust control scheme to provide good control in the face of parameter uncertainties and load disturbances.",
    "present_kp": [
      "robust discrete control"
    ],
    "absent_kp": [
      "tracking control",
      "reactors control"
    ]
  },
  {
    "title": "wavelet-based statistical approach for speckle reduction in medical ultrasound images.",
    "abstract": "a novel speckle-reduction method is introduced, based on soft thresholding of the wavelet coefficients of a logarithmically transformed medical ultrasound image. the method is based on the generalised gaussian distributed (ggd) modelling of sub-band coefficients. the method used was a variant of the recently published bayesshrink method by chang and vetterli, derived in the bayesian framework for denoising natural images. it was scale adaptive, because the parameters required for estimating the threshold depend on scale and sub-band data. the threshold was computed by ksigma(2)/sigma(x), where sigma and sigma(x) were the standard deviation of the noise and the sub-band data of the noise-free image, respectively, and k was a scale parameter. experimental results showed that the proposed method outperformed the median filter and the homomorphic wiener filter by 29% in terms of the coefficient of correlation and 4% in terms of the edge preservation parameter. the numerical values of these quantitative parameters indicated the good feature preservation performance of the algorithm, as desired for better diagnosis in medical image processing.",
    "present_kp": [
      "speckle reduction",
      "soft thresholding",
      "wiener filter",
      "median filter",
      "bayesshrink"
    ],
    "absent_kp": [
      "discrete wavelet transform"
    ]
  },
  {
    "title": "computer-based imaging and interventional mri: applications for neurosurgery.",
    "abstract": "advances in computer technology and the development of open mri systems definitely enhanced intraoperative image-guidance in neurosurgery. based upon the integration of previously acquired and processed 3d information and the corresponding anatomy of the patient, this requires computerized image-processing methods (segmentation, registration, and display) and fast image integration techniques. open mr systems equipped with instrument tracking systems, provide an interactive environment in which biopsies and minimally invasive interventions or open surgeries can be performed. enhanced by the integration of multimodal imaging these techniques significantly improve the available treatment options and can change the prognosis for patients with surgically treatable diseases.",
    "present_kp": [
      "interventional mri",
      "image-guidance",
      "neurosurgery"
    ],
    "absent_kp": [
      "minimally invasive therapy",
      "surgical planning"
    ]
  },
  {
    "title": "dynamic response of a pile embedded in a porous medium subjected to plane sh waves.",
    "abstract": "in this paper, the frequency domain dynamic response of a pile embedded in a porous medium subjected to sh seismic waves is investigated. the surrounding porous medium of the pile is described by biots poro-elastic theory, while the pile embedded in the porous medium is treated as a beam and described by a beam vibration theory. using the hankel transformation method, the fundamental solution for a half-space porous medium subjected to a horizontal circular patch load is established. according to the fictitious pile methodology, the second kind of fredholm integral equation for the pile is established in terms of the obtained fundamental solution and free wave field. the solution of the integral equation yields the dynamic response of the pile to plane sh waves. numerical results indicate that the parameters of the porous medium, the pile and incident waves have considerable influences on the dynamic response of the pile and the porous medium.",
    "present_kp": [
      "pile",
      "sh waves",
      "fredholm integral equation"
    ],
    "absent_kp": [
      "biots theory",
      "porous media"
    ]
  },
  {
    "title": "towards computational models of animal cognition, an introduction for computer scientists.",
    "abstract": "the last few years of the twentieth century witnessed the emerging convergence of biology and computer science and this trend has been accelerating since then. the study of animal behavior or behavior biology has been one of the major contributors for this convergence. behavior is fascinating because it is the response of an organism to internal and external signals and it is controlled by complex interactions among nerves, the sensory and the motor systems. to some extent, behavior is similar to the output (or response) of a computer system or a network node if we consider an animal brain as a computer node. this paper is the first in a two-part series in which i review the state-of-the-art research in behavior biology inspired computing and communication, with the first part focusing on animal cognition and the second part on animal communication ( ma, 2014). the present article also assumes the task of presenting a general introduction on behavior biology literature, which sets a foundation for synthesizing both parts of the series but the synthesis will be performed in the second part of the series. i sets three objectives in this cognition part: (i) to present a brief overview on the literature of behavior biology for computer scientists; (ii) to summarize the state-of-the-art studies in several cognitive aspects of animal behavior: focusing on emerging research in cognitive ecology, social learning and innovation, as well as animal logics; (iii) to review some important existing studies inspired by animal behavior and further present a perspective on the future research. these cognition-related topics offer insights for research fields such as machine learning, human computer interactions (hci), brain computer interfaces (bcis), evolutionary computing, pervasive computing, etc. in perspective, i suggest that the interaction between behavioral biology and computer science should be bidirectional, and a new subject, behavioral informatics, or more general computational behavior biology, should be developed by the cooperative efforts between biologists and computer scientists.",
    "present_kp": [
      "animal cognition",
      "cognitive ecology",
      "social learning",
      "behavioral informatics",
      "computational behavior biology"
    ],
    "absent_kp": [
      "bioinspired computing and communication"
    ]
  },
  {
    "title": "crossing boundaries in facebook: students framing of language learning activities as extended spaces.",
    "abstract": "young peoples interaction online is rapidly increasing, which enables new spaces for communication; the impact on learning, however, is not yet acknowledged in education. the aim of this exploratory case study is to scrutinize how students frame their interaction in social networking sites (sns) in school practices and what that implies for educational language teaching and learning practices. analytically, the study departs from a sociocultural perspective on learning, and adopts conceptual distinctions of frame analysis. the results based on ethnographic data from a facebook group in english-learning classes, with 60 students aged between 13 and 16 from colombia, finland, sweden and taiwan indicate that there is a possibility for boundary crossing, which could generate extended spaces for collaborative language-learning activities in educational contexts where students combine their school subject of learning language and their communicative use of language in their everyday life. such extended spaces are, however, difficult to maintain and have to be recurrently negotiated. to take advantage of young peoples various dynamic communicative uses of language in their everyday life in social media, the implementation of such media for educational purposes has to be deliberately, collaboratively and dynamically negotiated by educators and students to form a new language-learning space with its own potentials and constraints.",
    "present_kp": [
      "sns",
      "boundary crossing",
      "extended spaces",
      "language-learning activities",
      "facebook",
      "framing"
    ],
    "absent_kp": [
      "computer-supported collaborative learning"
    ]
  },
  {
    "title": "a study of the stabilizing process of unstable structures by dynamic relaxation method.",
    "abstract": "in this paper, the stabilizing process of unstable structures is studied by dynamic relaxation method. the process of applying the internal force to unstable structures is called as stabilizing process of unstable structure. the initial behavior of unstable structures such as cables, pneumatic structures or cable domes, are unstable state because of no initial internal stiffness. the dynamic relaxation method is the energy minimization technique that searches the static equilibrium state by simple vector iteration method. because the dynamic relaxation method does not need to assemble the tangential stiffness matrix of structure during each iteration of the stabilizing process, the computational effort and cpu run-time can be reduced. the finite difference integration technique is used to integrate the dynamic equilibrium equation for static equilibrium state. some numerical examples are presented to confirm the efficiency and applicability of dynamic relaxation method.",
    "present_kp": [
      "unstable structures",
      "stabilizing process",
      "dynamic relaxation method"
    ],
    "absent_kp": []
  },
  {
    "title": "a novel parallelization approach for hierarchical clustering.",
    "abstract": "identification of groups of genes that manifest similar expression patters is a key step in the analysis of gene expression data. hierarchical clustering is developed for that purpose. a fundamental problem with the previous implementations of this clustering method is its limitation to handle large data sets within a reasonable time and memory resources. in this paper, we present a parallel approach for solving this problem. implementation of the parallel algorithm is illustrated on data from high dimensional microarray experiments related to the gene expression in cancerous disease and arabidopsis seedling growth. they show considerable reduction in computational time and inter-node communication overhead, especially for large data sets.",
    "present_kp": [
      "clustering",
      "parallelization",
      "gene expression"
    ],
    "absent_kp": []
  },
  {
    "title": "surjective multidimensional cellular automata are non-wandering: a combinatorial proof.",
    "abstract": "a combinatorial proof that surjective d-dimensional ca are non-wandering is given. this answers an old open question stated in blanchard and tisseur (2000) . moreover, an explicit upper bound for the return time is given.",
    "present_kp": [
      "multidimensional cellular automata"
    ],
    "absent_kp": [
      "combinatorial problems",
      "symbolic dynamics",
      "discrete dynamical systems"
    ]
  },
  {
    "title": "event-based application of ws-security policy on soap messages.",
    "abstract": "ws-security and ws-security policy are the common standards for ensuring integrity and confidentiality for web service messages. on the one hand they allow very flexible definition of security requirements. on the other hand they lead to complex security administration and low performance message processing. in this paper, we present our solution for a security gateway, which uses complete event-based xml and ws-security processing to create policy conforming soap messages. the evaluation of our implementation shows that the event-based approach leads to a much better performance than tree-based ws-security implementations. further, we discuss some problematical issues of ws-security policy processing, such as determination of digital identities.",
    "present_kp": [
      "ws-security"
    ],
    "absent_kp": [
      "event-based processing",
      "web services"
    ]
  },
  {
    "title": "a scheduling problem with three competing agents.",
    "abstract": "scheduling with multiple agents has become a popular topic in recent years. however, most of the research focused on problems with two competing agents. in this article, we consider a single-machine scheduling problem with three agents. the objective is to minimize the total weighted completion time of jobs from the first agent given that the maximum completion time of jobs from the second agent does not exceed an upper bound and the maintenance activity from the third agent must be performed within a specified period of time. a lower bound based on job division and several propositions are developed for the branch-and-bound algorithm, and a genetic algorithm with a local search is constructed to obtain near-optimal solutions. in addition, computational experiments are conducted to test the performance of the algorithms.",
    "present_kp": [
      "scheduling",
      "total weighted completion time",
      "single-machine",
      "maintenance activity"
    ],
    "absent_kp": [
      "makespan",
      "multiple-agent"
    ]
  },
  {
    "title": "tsa: tree-seed algorithm for continuous optimization.",
    "abstract": "this paper presents a new optimizer to solve continuous optimization problems. new optimizer is proposed by considering relations between trees and their seeds. the new optimizer is applied to solve 24 benchmark functions. the results of the proposed method are compared with state-of-arts methods. the proposed method is a useful optimizer for continuous optimization.",
    "present_kp": [],
    "absent_kp": [
      "heuristic search",
      "tree and seed",
      "numeric optimization",
      "multilevel thresholding"
    ]
  },
  {
    "title": "solving fuzzy multidimensional multiple-choice knapsack problems: the multi-start partial bound enumeration method versus the efficient epsilon-constraint method.",
    "abstract": "in this paper a new fuzzy multidimensional multiple-choice knapsack problem (mmkp) is proposed. in the proposed fuzzy mmkp, each item may belong to several groups according to a predefined fuzzy membership value. the total profit and the total cost of the knapsack problem are considered as two conflicting objectives. a mathematical approach and a heuristic algorithm are proposed to solve the fuzzy mmkp. one method is an improved version of a well-known exact multi-objective mathematical programming technique, called the efficient ?-constraint method. the second method is a heuristic algorithm called multi-start partial-bound enumeration (pbe). both methods are used to comparatively generate a set of non-dominated solutions for the fuzzy mmkp. the performance of the two methods is statistically compared with respect to a set of simulated benchmark cases using different diversity and accuracy metrics.",
    "present_kp": [
      "fuzzy multidimensional multiple-choice knapsack problem",
      "efficient epsilon-constraint method",
      "multi-start partial bound enumeration method"
    ],
    "absent_kp": []
  },
  {
    "title": "a trace transformation technique for communication refinement.",
    "abstract": "models of computation like kahn and dataflow process networks provide convenient means for modeling signal processing applications. this is partly due to the abstract primitives that these models offer for communication between concurrent processes. however, when mapping an application model onto an architecture, these primitives need to be mapped onto architecture level communication primitives. we present a trace transformation technique that supports a system architect in performing this communication refinement. we discuss the implementation of this technique in a tool for architecture exploration named spade and present examples.",
    "present_kp": [
      "communication",
      "network",
      "applications",
      "examples",
      "architecture",
      "computation",
      "implementation",
      "tool",
      "process",
      "map",
      "model",
      "refine",
      "dataflow",
      "signal processing",
      "exploration"
    ],
    "absent_kp": [
      "traces",
      "systems",
      "abstraction",
      "concurrency"
    ]
  },
  {
    "title": "a regularity condition of the information matrix of a multilayer perceptron network.",
    "abstract": "the fisher information matrix of a multi-layer perceptron network can be singular at certain parameters, and in such cases many statistical techniques based on asymptotic theory cannot be applied properly. in this paper, we prove rigorously that the fisher information matrix of a three-layer perceptron network is positive definite if and only if the network is irreducible; that is, if there is no hidden unit that makes no contribution to the output and there is no pair of hidden units that could be collapsed to a single unit without altering the input-output map. this implies that a network that has a singular fisher information matrix can be reduced to a network with a positive definite fisher information matrix by eliminating redundant hidden units.",
    "present_kp": [
      "multilayer perceptron",
      "information matrix"
    ],
    "absent_kp": [
      "parametric estimation",
      "irreducibility",
      "minimality",
      "sigmoidal function"
    ]
  },
  {
    "title": "how hci design influences web security decisions.",
    "abstract": "even though security protocols are designed to make computer communication secure, it is widely known that there is potential for security breakdowns at the human-machine interface. this paper reports on a diary study conducted in order to investigate what people identify as security decisions that they make while using the web. the study aimed to uncover how security is perceived in the individual's context of use. from this data, themes were drawn, with a focus on addressing security goals such as confidentiality and authentication. this study is the first study investigating users' web usage focusing on their self-documented perceptions of security and the security choices they made in their own environment.",
    "present_kp": [
      "security",
      "hci",
      "design",
      "diary study"
    ],
    "absent_kp": [
      "participation",
      "retail",
      "phishing",
      "online",
      "trust"
    ]
  },
  {
    "title": "the complexity of finding arc-disjoint branching flows.",
    "abstract": "the concept of arc-disjoint flows in networks was recently introduced in bang-jensen and bessy (2014). this is a very general framework within which many well-known and important problems can be formulated. in particular, the existence of arc-disjoint branching flows, that is, flows which send one unit of flow from a given source s to all other vertices, generalizes the concept of arc-disjoint out-branchings (spanning out-trees) in a digraph. a pair of out-branchings b s , 1 + , b s , 2 + from a root s in a digraph d=(v,a) on n vertices corresponds to arc-disjoint branching flows x1,x2 (the arcs carrying flow in xi are those used in b s , i + , i=1,2) in the network that we obtain from d by giving all arcs capacity n?1 . it is then a natural question to ask how much we can lower the capacities on the arcs and still have, say, two arc-disjoint branching flows from the given root s . we prove that for every fixed integer k?2 it is an np-complete problem to decide whether a network n=(v,a,u) where uij=k for every arc ij has two arc-disjoint branching flows rooted at s . a polynomial problem to decide whether a network n=(v,a,u) on n vertices and uij=n?k for every arc ij has two arc-disjoint branching flows rooted at s .",
    "present_kp": [
      "branching flow",
      "np-complete"
    ],
    "absent_kp": [
      "disjoint branchings",
      "polynomial algorithm"
    ]
  },
  {
    "title": "double-ended queues with impatience.",
    "abstract": "the effect of impatient behaviour is studied primarily in the context of double-ended queues where each demands service from the other, typically taxis and passengers. related models, single queue, and double, with a variety of mechanisms are considered. impatience is to be understood in a wider context than simply becoming tired of waiting: it can arise because the customer, for some reason, runs out of time (inventory and organ transplantation), or because an alternative service becomes available (communication applications). the emphasis in this paper is theoretical but a brief numerical assessment of operational consequences is given. the double-ended (or synchronization) queue is a model for a variety of service demanding/providing systems. in an orderly taxi rank at a railway station or airport, on one side a queue is formed by the arrival of stream of passengers who wait for taxis to their destinations while on the other side a queue of taxis waiting for passengers. obviously, the two queues can never coexist. the concept of impatience enters when a taxi or passenger leaves the queue before receiving service. this concept of reneging is widely applicable. in health care, for example, organs are stored for transplantation for needful patients. both the organs and the demands for them have limited lifetime. a similar scenario applies to perishable inventory systems. in a similar manner, the real-time communication networks admit impatient behaviour. a typical example is a processor-shared queue in data networks with random time-out periods or deadlines. the paper sets out the basics in a variety of theoretical model settings with the common feature of exponential arrival, service and impatience mechanisms. a brief discussion based on numerical calculation is given of some operational features of the models but the thrust is on the theoretical techniques needed to make meaningful operational assessments.",
    "present_kp": [
      "double-ended queues",
      "impatience"
    ],
    "absent_kp": [
      "randomised random walk",
      "birth\u2013death models"
    ]
  },
  {
    "title": "su-8 ridge-waveguide with holographic grating embedded in nanoimprinted groove.",
    "abstract": "fabrication of su-8 slab- and ridge-waveguides with holographic grating for dfb laser, effectively utilizing nanoimprint technology (nil), is presented. rhodamine-6g-doped su-8 slab- and ridge-waveguides were embedded in grooves defined by nil in uv curable resin. utilization of nil made it easier to form such a three-dimensional micro structure consisting of ridge stripe and fine corrugation grating. te-polarized 587nm laser and te-polarized 594nm light emissions were observed from the slab- and ridge-waveguides, respectively, when the waveguides were irradiated by 532nm pulsed nd:yag laser.",
    "present_kp": [
      "nanoimprint",
      "su-8",
      "waveguide",
      "holographic grating"
    ],
    "absent_kp": [
      "polymer optics",
      "polymer dfb laser"
    ]
  },
  {
    "title": "optical method for characterization of nanoplates in lyosol.",
    "abstract": "in the recent years, two-dimensional nanoparticles (the nanoplates) have become a subject of intense scientific researches and industrial applications. the liquid exfoliation of layered crystals becomes the most simple and efficient manufacturing method of graphene and layered compounds nanosheets. the primary product of the liquid exfoliation is a lyosol a colloidal suspension of desired nanoparticles in a fluid. the shape of the produced nanoparticles may be determined with the use of an electron microscopy (sem). in the case of spherical particles the concentration and the particles size can be assessed by the light absorption and the dynamic laser light scattering (dls) measurements respectively. we propose an optical method for fast and comprehensive characterization of shape and size of nanoplates in a lyosol. the system makes use of an optical goniometer. it allows to measure angular distribution of intensity of the light scattered on the lyosol sample. the distribution of the intensity (indicatrix) depends on the shape and size of the nanoparticles. we developed a method of theoretical indicatrix calculation of the nanoplates with the use of amsterdam discrete dipole approximation method. the experimentally measured indicatrix is compared with the theoretical calculations (standardized indicatrices). we observed that the nanoplate has a dual character of light scattering. it can scatter light as a small or big particle according to the direction of the incident light beam. we have applied the proposed method of characterization for a water suspension of graphene nanosheets. results of the measurements are in good agreement with particles dimensions assessed with the sem images and do not fit with the graphene dimensions estimated with the dls method. it leads to a conclusion that the two-dimensional particles size estimation with the dls is highly inaccurate.",
    "present_kp": [
      "graphene",
      "nanoplates",
      "discrete dipole approximation"
    ],
    "absent_kp": [
      "light scattering measurement"
    ]
  },
  {
    "title": "what you see is what you code: a live algorithm development and visualization environment for novice learners.",
    "abstract": "pedagogical algorithm visualization (av) systems produce graphical representations that aim to assist learners in understanding the dynamic behavior of computer algorithms. in order to foster active learning, computer science educators have developed av systems that empower learners to construct their own visualizations of algorithms under study. notably, these systems support a similar development model in which coding an algorithm is temporally distinct from viewing and interacting with the resulting visualization. given that they are known to have problems both with formulating syntactically correct code, and with understanding how code executes, novice learners would appear likely to benefit from a more live development model that narrows the gap between coding an algorithm and viewing its visualization. in order to explore this possibility, we have implemented what you see is what you code, an algorithm development and visualization model geared toward novices first learning to program under the imperative paradigm. in the model, the line of algorithm code currently being edited is reevaluated on every edit, leading to immediate syntactic feedback, along with immediate semantic feedback in the form of an av. analysis of usability and field studies involving introductory computer science students suggests that the immediacy of the model's feedback can help novices to quickly identify and correct programming errors, and ultimately to develop semantically correct code.",
    "present_kp": [
      "algorithm visualization",
      "field studies"
    ],
    "absent_kp": [
      "novice programming environments",
      "live programming environments",
      "usability studies"
    ]
  },
  {
    "title": "inver2dbasea program to compute basement depths of density interfaces above which the density contrast varies with depth.",
    "abstract": "a computer program to invert the gravity anomalies of density interfaces above which the density contrast varies with depth, is presented. the sedimentbasement interface is approximated by an n-sided polygon. a function subprogram gr2dpol and two subroutine subprograms zor2dpol and simeq support the main program. subroutine zor2dpol calculates the initial depth estimates of a density interface at all anomaly points on the principal profile using the infinite slab approximation. function subprogram gr2dpol computes the theoretical gravity anomaly of a density interface at each anomaly point on the profile and returns to the main program. subroutine simeq is used to solve n incremental depth parts of the vertices of a density interface. partial derivatives are calculated by a simple finite-difference method. normal equations are constructed and solved by marquardt's algorithm. the proposed inversion scheme is independent on the equal station spacing criteria. the validity of the algorithm is demonstrated by calculating the basement depths of the tucson basin, southern arizona.",
    "present_kp": [
      "sedimentbasement interface",
      "n-sided polygon",
      "gravity anomaly",
      "inversion"
    ],
    "absent_kp": [
      "variable density contrast"
    ]
  },
  {
    "title": "graph based construction of textured large field of view mosaics for bladder cancer diagnosis.",
    "abstract": "large field-of-view panoramic images greatly facilitate bladder cancer diagnosis and follow-up. such 2d mosaics can be obtained by registering the images of a video-sequence acquired during cystoscopic examinations. the scientific challenge in the registration process lies in the strong inter- and intra-patient texture variability of the images, from which primitives cannot be robustly extracted. state-of-the-art registration methods are not at the same time robust and accurate, especially for image pairs with a small amount of overlap (less than 90%) or strong perspective transformations. moreover, no previous contribution to cystoscopy mosaicing presents panoramic images created from multiple overlapping sequences (e.g. zigzags or loop trajectories). we show how such overlapping sections can be automatically detected and present a novel registration algorithm that robustly superimposes non-consecutive image pairs, which are related by stronger perspective transformations and share less overlap than consecutive images (less than 50%). globally coherent panoramic images are constructed using a non-linear optimization and a novel contrast-enhancing stitching method. results on both phantom and patient data are obtained using constant algorithm parameters, which demonstrate the robustness of the proposed method. while the methods presented in this contribution are specifically designed for cystoscopy mosaicing, they can also be applied to more general mosaicing problems. we demonstrate this on a traditional stitching application, where a set of pictures of a building are stitched into a seamless, globally coherent panoramic image.",
    "present_kp": [
      "bladder cancer"
    ],
    "absent_kp": [
      "image mosaicing",
      "seamless panoramic stitching",
      "image registration",
      "endoscopy",
      "graph cuts",
      "higher order terms",
      "non-linear refinement"
    ]
  },
  {
    "title": "an heuristic set for evaluation in information visualization.",
    "abstract": "evaluation is a key research challenge within the international information visualization (infovis) community, and heuristic evaluation is one recognized method. various sets of heuristics have been proposed but there remains no consensus as to which heuristics are most useful for addressing aspects specific to the complex interactive visual displays used in modern infovis systems. this paper presents a first effort to empirically determine a new set of such general heuristics tailored for heuristic evaluation of common and important usability problems in infovis techniques. participants in the study rated how well a total of 63 heuristics from 6 earlier published heuristic sets could explain a collection of 74 usability problems derived from earlier infovis evaluations. the results were used to synthesize 10 heuristics that, as a set, provided the highest explanatory coverage. the paper also stresses the challenges for future research to validate and further improve upon this set.",
    "present_kp": [
      "heuristics",
      "heuristic evaluation",
      "information visualization"
    ],
    "absent_kp": []
  },
  {
    "title": "a dependable infrastructure for cooperative web services coordination.",
    "abstract": "a current trend in the web services community is to define coordination mechanisms to execute collaborative tasks involving multiple organizations. following this tendency, in this paper the authors present a dependable (i.e., intrusion-tolerant) infrastructure for cooperative web services coordination that is based on the tuple space coordination model. this infrastructure provides decoupled communication and implements several security mechanisms that allow dependable coordination even in presence of malicious components. this work also investigates the costs related to the use of this infrastructure and possible web service applications that can benefit from it.",
    "present_kp": [
      "coordination mechanisms",
      "web service applications",
      "web services coordination"
    ],
    "absent_kp": [
      "dependability",
      "tuple spaces"
    ]
  },
  {
    "title": "performance evaluation of an ieee 802.15.4 sensor network with a star topology.",
    "abstract": "one class of applications envisaged for the ieee 802.15.4 lr-wpan (low data rate-wireless personal area network) standard is wireless sensor networks for monitoring and control applications. in this paper we provide an analytical performance model for a network in which the sensors are at the tips of a star topology, and the sensors need to transmit their measurements to the hub node so that certain objectives for packet delay and packet discard are met. we first carry out a saturation throughput analysis of the system; i.e., it is assumed that each sensor has an infinite backlog of packets and the throughput of the system is sought. after a careful analysis of the csma/ca mac that is employed in the standard, and after making a certain decoupling approximation, we identify an embedded markov renewal process, whose analysis yields a fixed point equation, from whose solution the saturation throughput can be calculated. we validate our model against ns2 simulations (using an ieee 802.15.4 module developed by zheng ). we find that with the default back-off parameters the saturation throughput decreases sharply with increasing number of nodes. we use our analytical model to study the problem and we propose alternative back-off parameters that prevent the drop in throughput. we then show how the saturation analysis can be used to obtain an analytical model for the finite arrival rate case. this finite load model captures very well the qualitative behavior of the system, and also provides a good approximation to the packet discard probability, and the throughput. for the default parameters, the finite load throughput is found to first increase and then decrease with increasing load. we find that for typical performance objectives (mean delay and packet discard) the packet discard probability would constrain the system capacity. finally, we show how to derive a node lifetime analysis using various rates and probabilities obtained from our performance analysis model.",
    "present_kp": [
      "wireless sensor networks",
      "performance analysis"
    ],
    "absent_kp": [
      "lr-wpans"
    ]
  },
  {
    "title": "navigation models for a flexible, multi-mode vr navigation framework.",
    "abstract": "navigation is a key issue for virtual reality (vr) applications because it forms an integral part of the feeling of presence, which should be conveyed by vr applications. this paper presents several vr navigation modes which are useful for orientation and interaction in virtual environments (ves). due to the existence of different kinds of applications several navigation modes are required.",
    "present_kp": [
      "navigation models"
    ],
    "absent_kp": [
      "virtual reality navigation",
      "navigation techniques"
    ]
  },
  {
    "title": "multiscale simulations: application to the heat transfer simulation of sliding solids.",
    "abstract": "molecular dynamics is a powerful tool allowing the simulation of matter behaviour at the atomic scale. due to computation time, it is clearly not possible to use molecular dynamics to simulate a forming process. however, atomistic simulations can be used to study and understand the physical phenomena that occur during matter deformation. as an example, heat transfer between the contacting solids in forming processes is one of the important physics phenomena that have to be taken into account in order to do realistic simulations. a multiscale analysis of heat transfer is presented. this analysis leads to two kinds of models: a macroscopic model which can be used for the simulation of the process itself and a microscopic model that is used to determine the parameters of the macroscopic model. in this microscopic model, the friction heat generation phenomena has to be described quite accurately. friction heat is mainly due to plastic and elastic deformation and adhesion. thus, to understand the underlying friction heat generation phenomena, atomistic simulations using molecular dynamics are carried out. it is shown that friction heat is the transformation of mechanical work given to the system at the macroscopic scale into potential energy during elastic deformation. this potential energy which is stored in the system is finally transformed into atomic kinetic energy (friction heat) during plastic transformation.",
    "present_kp": [
      "molecular dynamics",
      "friction",
      "atomic scale",
      "heat transfer"
    ],
    "absent_kp": [
      "sliding contact"
    ]
  },
  {
    "title": "l-p-nested symmetric distributions.",
    "abstract": "in this paper, we introduce a new family of probability densities called l-p-nested symmetric distributions. the common property, shared by all members of the new class, is the same functional form rho(x)= (rho) over tilde (f(x)), where f is a nested cascade of l-p-norms parallel to x parallel to(p) = (sigma vertical bar x(i)vertical bar(p))(1/p). l-p-nested symmetric distributions thereby are a special case of nu-spherical distributions for which f is only required to be positively homogeneous of degree one. while both, nu-spherical and l-p-nested symmetric distributions, contain many widely used families of probability models such as the gaussian, spherically and elliptically symmetric distributions, l-p-spherically symmetric distributions, and certain types of independent component analysis (ica) and independent subspace analysis (isa) models, nu-spherical distributions are usually computationally intractable. here we demonstrate that l-p-nested symmetric distributions are still computationally feasible by deriving an analytic expression for its normalization constant, gradients for maximum likelihood estimation, analytic expressions for certain types of marginals, as well as an exact and efficient sampling algorithm. we discuss the tight links of l-p-nested symmetric distributions to well known machine learning methods such as ica, isa and mixed norm regularizers, and introduce the nested radial factorization algorithm (nrf), which is a form of non-linear ica that transforms any linearly mixed, non-factorial l-p-nested symmetric source into statistically independent signals. as a corollary, we also introduce the uniform distribution on the l-p-nested unit sphere.",
    "present_kp": [
      "symmetric distribution",
      "nu-spherical distributions",
      "independent subspace analysis",
      "nested radial factorization"
    ],
    "absent_kp": [
      "parametric density model",
      "non-linear independent component analysis",
      "robust bayesian inference",
      "mixed norm density model",
      "uniform distributions on mixed norm spheres"
    ]
  },
  {
    "title": "data delivery in fragmented wireless sensor networks using mobile agents.",
    "abstract": "due to the wide range of applications in sensors and wireless sensor networks (wsn), research in this area has recently received increasing attention. wsns rely on network connectivity to deliver data to a base station through multihop communication. however, connectivity may not be always achievable for a number of reasons. in this paper, we study the problem of data delivery in disconnected wsns. a special class of disconnected sensor networks called \"fragmented wireless sensor networks (fwsn)\" is considered. a fwsn consists of several groups of connected sensors that we call \"fragments\". to achieve connectivity between these fragments, mobile agents move in the network and act as data relays between fragments, in order to eventually deliver data to the base station. the main contribution of this paper is the modeling of the movement of these mobile relay nodes as a closed queueing network to obtain steady state results of the distribution of the mobile relays in the network. building on these results, we derive the distributions of the fragment-to-fragment, and fragment-to-sink delays. comparing these analytical results to results from the tossim simulator, it is shown that this model accurately captures the system behavior, and can be used to predict data delivery delays.",
    "present_kp": [
      "wireless sensor networks",
      "mobile agents"
    ],
    "absent_kp": [
      "closed queueing networks"
    ]
  },
  {
    "title": "implementation of conditional simulation by successive residuals.",
    "abstract": "conditional simulation of ergodic and stationary gaussian random fields using successive residuals is a new approach used to overcome the size limitations of the lu decomposition algorithm as well as provide fast updating of existing simulated realizations with new data. this paper discusses two different implementations of this approach. the implementations differ in the use of the new information available; in the first implementation new information is partially used to generate updated realizations; however, in the second implementation, the realizations are updated using all the new information available. the implementations are validated using the walker lake data set, and compared through a case study at a stockwork gold deposit.",
    "present_kp": [
      "lu decomposition",
      "successive residuals"
    ],
    "absent_kp": [
      "generalised sequential gaussian simulation"
    ]
  },
  {
    "title": "completion of overdetermined parabolic pdes.",
    "abstract": "in this paper we apply methods of commutative algebra to analysis of systems of pdes. more precisely, we show that systems which are parabolic in a generalized sense are equivalent to certain completed systems which are parabolic in the standard sense. we also propose a constructive method for getting this completion, and grobner basis methods, via symbol modules of the systems, play a central role in practical computations. moreover, we can easily construct systems which are not parabolic in the generalized sense but nevertheless become parabolic when completed.",
    "present_kp": [
      "completion"
    ],
    "absent_kp": [
      "overdetermined system",
      "partial differential equation",
      "parabolic system",
      "graded module",
      "free resolution"
    ]
  },
  {
    "title": "steady-state analysis of a discrete-time batch arrival queue with working vacations.",
    "abstract": "this paper analyzes a discrete-time batch arrival queue with working vacations. in a geo x / g / 1 system, the server works at a lower speed during the vacation period which becomes a lower speed operation period. this model is more appropriate for the communication systems with the transmit units arrived in batches. we formulate the system as an embedded markov chain at the departure epoch and by the m/g/1-type matrix analytic approach, we derive the probability generating function (pgf) of the stationary queue length. then, we obtain the distribution for the number of the customers at the busy period initiation epoch, and use the stochastic decomposition technique to present another equivalent pgf of the queue length. we also develop a variety of stationary performance measures for this system. some special models and numerical results are presented. finally, a real-world example in an ethernet passive optical network (epon) is provided.",
    "present_kp": [
      "batch arrival",
      "working vacations",
      "m/g/1-type matrix",
      "stochastic decomposition",
      "ethernet passive optical network "
    ],
    "absent_kp": [
      "discrete-time queue"
    ]
  },
  {
    "title": "bimql an open query language for building information models.",
    "abstract": "in this paper we present the on-going development of a framework for a domain specific, open query language for building information models. the proposed query language is intended for selecting, updating and deleting of data stored in industry foundation classes models. even though some partial solutions already have been suggested, none of them are open source, domain specific, platform independent and implemented at the same time. this paper provides an overview of existing approaches, conceptual sketches of the language in development and documents the current state of implementation as a prototype plugin developed for the open source model server platform bimserver.org. we report on the execution of example test-cases to show the general feasibility of the approach chosen.",
    "present_kp": [
      "bimql",
      "query language",
      "bim"
    ],
    "absent_kp": [
      "domain specific language",
      "ifc",
      "building information model server"
    ]
  },
  {
    "title": "a prototype for an agent-based secure electronic marketplace including reputation-tracking mechanisms.",
    "abstract": "software agents will play a crucial role in the coming digital economy, but their reliability and honesty cannot be guaranteed by technical security mechanisms, such as encryption of messages and digital signing of documents, and entities that can sanction fraudulent behavior in open networks are still in a rudimentary stage. this article describes a reputation mechanism that records previous cooperation behavior of participants in agent-based markets and conveys this information to other software agents, thereby influencing the future behavior of participants. the mechanism has been prototypically implemented in the avalanche multiagent system. the deployment of this reputation mechanism will help to exclude fraudulent software agents from market participation.",
    "present_kp": [
      "software agents"
    ],
    "absent_kp": [
      "agent-mediated electronic commerce",
      "electronic marketplaces",
      "reputation tracking"
    ]
  },
  {
    "title": "an advanced hydro-mechanical constitutive model for unsaturated soils with different initial densities.",
    "abstract": "this paper presents an advanced constitutive model for unsaturated soils, using bishops effective stress ( ) and the effective degree of saturation (se) as two fundamental constitutive variables in the proposed constitutive model. a sub-loading surface and a unified hardening parameter (h) are introduced into the se modelling framework to interpret the effects of initial density on coupled hydro-mechanical behaviour of compacted soils. compared with existing models in the literature, the main advantage of the proposed model that it is capable of modelling hydro-mechanical behaviour of unsaturated soils compacted to different initial densities, such as the dependence of loadingcollapse volume on initial void ratio and density effect on the shearing-induced saturation change. the proposed model requires 13 material parameters, all of which can be calibrated through conventional laboratory tests. numerical studies are conducted to assess the performance of the model for a hypothetical soil under two typical hydro-mechanical loading scenarios. the proposed advanced unsaturated soil model is then validated against a number of experimental results for both isotropic and triaxial conditions reported in the literature.",
    "present_kp": [
      "unsaturated soils",
      "compacted soils",
      "bishops effective stress",
      "degree of saturation",
      "initial densities"
    ],
    "absent_kp": [
      "hydro-mechanical interaction"
    ]
  },
  {
    "title": "understanding the value of software engineering technologies.",
    "abstract": "when ai search methods are applied to software process models, then appropriate technologies can be discovered for a software project. we show that those recommendations are greatly affected by the business context of its use. for example, the automatic defect reduction tools explored by the ase community are only relevant to a subset of software projects, and only according to certain value criteria. therefore, when arguing for the value of a particular technology, that argument should include a description of the value function of the target user community.",
    "present_kp": [],
    "absent_kp": [
      "artificial intelligence",
      "software economics"
    ]
  },
  {
    "title": "the unfinished history of usage rights for spectrum.",
    "abstract": "the key task in the next stage of spectrum management is to adapt regulation to the prospect of widespread sharing, on a much more sophisticated basis than sharing is used today. there is a role for the regulator to take steps to expand the area of choice within which public and private sector users can operate. this is best done in general by enhancing the flexibility of usage rights, which itself is best achieved by enhancing the freedom to trade them in the dimensions of time, space, level of interference and priority of access, by subdividing, re-aggregating, etc. however, there are considerable transactions cost impediments to trading where unlicensed users are involved. this creates a role for the regulator pro-actively to investigate different allocations, to make provisions for the most promising to occur and to incorporate both in refarming exercises and in primary assignments based on auctions configurations of usage rights, which might favour promising avenues of shared spectrum use.",
    "present_kp": [
      "spectrum",
      "usage rights",
      "unlicensed",
      "refarming"
    ],
    "absent_kp": [
      "surs"
    ]
  },
  {
    "title": "a discrete scheme of laplacebeltrami operator and its convergence over quadrilateral meshes.",
    "abstract": "laplacebeltrami operator and its discretization play a central role in the fields of image processing, computer graphics, computer aided geometric design and so on. in this paper, a discrete scheme for laplacebeltrami operator over quadrilateral meshes is constructed based on a bilinear interpolation of the quadrilateral. convergence results for the proposed discrete scheme are established under some conditions. numerical results which justify the theoretical analysis are also given.",
    "present_kp": [
      "laplacebeltrami operator",
      "quadrilateral meshes",
      "discretization",
      "convergence",
      "bilinear interpolation"
    ],
    "absent_kp": [
      "mean curvature"
    ]
  },
  {
    "title": "energy and time efficient algorithm for cloud offloading using dynamic profiling.",
    "abstract": "with the advent of computationally intensive application for mobile devices there is need of time and energy efficient component offloading algorithm which involves execution of resource intensive components of an application on remote machine. traditional solution includes offloading of entire application (no partition), offloading predetermined components (static partition) or making offloading decision at runtime for each component (01 ilp). our proposed solution of dynamic profiling uses depth-first search (topological sorting) to calculate the offloading point at runtime. the subsequent nodes are offloaded with high probability. experimental result demonstrates that proposed algorithm is better than 01 ilp in time domain while outperforming no-partitioning and static-partitioning in energy domain.",
    "present_kp": [
      "depth-first search",
      "topological sorting"
    ],
    "absent_kp": [
      "mobile computing"
    ]
  },
  {
    "title": "adaptive critic design-based robust neural network control for nonlinear distributed parameter systems with unknown dynamics.",
    "abstract": "in this paper, an adaptive critic design (acd)-based robust on-line neural network control design is developed for a class of parabolic partial differential equation (pde) systems with unknown nonlinear dynamics. first, the galerkin method is applied to the parabolic pde system to derive a finite-dimensional slow one and an infinite-dimensional stable fast subsystem. the obtained slow system is an ordinary differential equation (ode) system with unknown nonlinearities, which accurately describes the dynamics of the slow modes of the pde system. then, a novel acd-based robust optimal control scheme is proposed for the resulting nonlinear slow system with unknown dynamics. an action neural network (nn) is employed to approximate all the derived unknown nonlinear terms and a robust control term is further developed to attenuate the nn reconstruction errors and disturbances. especially, by developing novel critic signals and lyapunov function candidate, together with the adaptive bounding technique, no a prior knowledge for the bounds of the disturbance term, the nn ideal weights of action nn and critic nn and the nn reconstruction errors is required. finally, simulation results demonstrate the effectiveness of the proposed robust optimal control scheme.",
    "present_kp": [
      "partial differential equation"
    ],
    "absent_kp": [
      "adaptive critic designs",
      "adaptive dynamic programming",
      "learning control",
      "neural networks",
      "uniformly ultimate boundedness"
    ]
  },
  {
    "title": "web services with generic simulation models for discrete event simulation.",
    "abstract": "today the internet and the world wide web (www) are on the cusp of a paradigm shift. up to now most actions in the www are sorts of human-computer interaction, but the introduction of the extensible markup language (xml) changed the perception. the internet will be seen as a great space of information and with the use of xml and following technologies like web services, grid computing and semantic web the difference between human-machine interaction and machine-machine interaction vanishes. this work investigates the usefulness of xml in the simulation domain and uses web service technology to build the simasp framework for discrete event simulation (des).",
    "present_kp": [
      "web service",
      "xml",
      "discrete event simulation"
    ],
    "absent_kp": [
      "application service providing"
    ]
  },
  {
    "title": "the impact of sample reduction on pca-based feature extraction for supervised learning.",
    "abstract": "\"the curse of dimensionality\" is pertinent to many learning algorithms, and it denotes the drastic raise of computational complexity and classification error in high dimensions. in this paper, different feature extraction (fe) techniques are analyzed as means of dimensionality reduction, and constructive induction with respect to the performance of nave bayes classifier. when a data set contains a large number of instances, some sampling approach is applied to address the computational complexity of fe and classification processes. the main goal of this paper is to show the impact of sample reduction on the process of fe for supervised learning. in our study we analyzed the conventional pca and two eigenvector-based approaches that take into account class information. the first class-conditional approach is parametric and optimizes the ratio of between-class variance to the within-class variance of the transformed data. the second approach is a nonparametric modification of the first one based on the local calculation of the between-class covariance matrix. the experiments are conducted on ten uci data sets, using four different strategies to select samples: (1) random sampling, (2) stratified random sampling, (3) kd -tree based selective sampling, and (4) stratified sampling with kd -tree based selection. our experiments show that if the sample size for fe model construction is small then it is important to take into account both class information and data distribution. further, for supervised learning the nonparametric fe approach needs much less instances to produce a new representation space that result in the same or higher classification accuracy than the other fe approaches.",
    "present_kp": [
      "supervised learning",
      "feature extraction",
      "sample reduction"
    ],
    "absent_kp": []
  },
  {
    "title": "a topology preserving level set method for geometric deformable models.",
    "abstract": "active contour and surface models, also known as deformable models, are powerful image segmentation techniques. geometric deformable models implemented using level set methods have advantages over parametric models due to their intrinsic behavior, parameterization independence, and ease of implementation. however, a long claimed advantage of geometric deformable models-the ability to automatically handle topology changes-turns out to be a liability in applications where the object to be segmented has a known topology that must be preserved. in this paper, we present a new class of geometric deformable models designed using a novel topology-preserving level set method, which achieves topology preservation by applying the simple point concept from digital topology. these new models maintain the other advantages of standard geometric deformable models including subpixel accuracy and production of nonintersecting curves or surfaces. moreover, since the topology-preserving constraint is enforced efficiently through local computations, the resulting algorithm incurs only nominal computational overhead over standard geometric deformable models. several experiments on simulated and real data are provided to demonstrate the performance of this new deformable model algorithm.",
    "present_kp": [
      "geometric deformable model",
      "topology preservation",
      "level set method",
      "digital topology"
    ],
    "absent_kp": [
      "topological constraint",
      "simple points",
      "active contours"
    ]
  },
  {
    "title": "elastic geodesic paths in shape space of parameterized surfaces.",
    "abstract": "this paper presents a novel riemannian framework for shape analysis of parameterized surfaces. in particular, it provides efficient algorithms for computing geodesic paths which, in turn, are important for comparing, matching, and deforming surfaces. the novelty of this framework is that geodesics are invariant to the parameterizations of surfaces and other shape-preserving transformations of surfaces. the basic idea is to formulate a space of embedded surfaces (surfaces seen as embeddings of a unit sphere in r-3) and impose a riemannian metric on it in such a way that the reparameterization group acts on this space by isometries. under this framework, we solve two optimization problems. one, given any two surfaces at arbitrary rotations and parameterizations, we use a path-straightening approach to find a geodesic path between them under the chosen metric. second, by modifying a technique presented in [25], we solve for the optimal rotation and parameterization (registration) between surfaces. their combined solution provides an efficient mechanism for computing geodesic paths in shape spaces of parameterized surfaces. we illustrate these ideas using examples from shape analysis of anatomical structures and other general surfaces.",
    "present_kp": [
      "shape analysis",
      "path-straightening",
      "geodesics"
    ],
    "absent_kp": [
      "riemannian distance",
      "parameterization invariance"
    ]
  },
  {
    "title": "on the coupling of the homotopy perturbation method and laplace transformation.",
    "abstract": "in this paper, a laplace homotopy perturbation method is employed for solving one-dimensional non-homogeneous partial differential equations with a variable coefficient. this method is a combination of the laplace transform and the homotopy perturbation method (lhpm). lhpm presents an accurate methodology to solve non-homogeneous partial differential equations with a variable coefficient. the aim of using the laplace transform is to overcome the deficiency that is mainly caused by unsatisfied conditions in other semi-analytical methods such as hpm, vim, and adm. the approximate solutions obtained by means of lhpm in a wide range of the problem's domain were compared with those results obtained from the actual solutions, the homotopy perturbation method (hpm) and the finite element method. the comparison shows a precise agreement between the results, and introduces this new method as an applicable one which it needs fewer computations and is much easier and more convenient than others, so it can be widely used in engineering too.",
    "present_kp": [
      "laplace homotopy perturbation method ",
      "homotopy perturbation method ",
      "non-homogeneous partial differential equation"
    ],
    "absent_kp": []
  },
  {
    "title": "central limit theorems for super ornstein-uhlenbeck processes.",
    "abstract": "suppose that x={x t :t?0} is a supercritical super ornstein-uhlenbeck process, that is, a superprocess with an ornstein-uhlenbeck process on (mathbb{r}^{d}) corresponding to (l=frac{1}{2}sigma^{2}delta-b xcdotnabla) as its underlying spatial motion and with branching mechanism ?(?)= + 2+?(0,+?)(e x ?1+?x)n(dx), where ?= (0+)>0, 0, and n is a measure on (0,?) such that ?(0,+?) x 2 n(dx)<+?. let (mathbb{p} _{mu}) be the law of x with initial measure ?. then the process w t =e t ?x t ? is a positive (mathbb{p} _{mu})-martingale. therefore there is w ? such that w t ?w ?, (mathbb{p} _{mu})-a.s. as t . in this paper we establish some spatial central limit theorems for x.",
    "present_kp": [
      "central limit theorem",
      "superprocess",
      "super ornstein-uhlenbeck process",
      "ornstein-uhlenbeck process",
      ""
    ],
    "absent_kp": [
      "backbone decomposition",
      "branching process",
      "branching ornstein-uhlenbeck process"
    ]
  },
  {
    "title": "e-learning recommender system for a group of learners based on the unified learner profile approach.",
    "abstract": "in the age of information explosion, e-learning recommender systems (el_rss) have emerged as effective information filtering techniques that attempt to provide the most appropriate learning resources for learners while using e-learning systems. these learners are differentiated on the basis of their learning styles, goals, knowledge levels and others. several attempts have been made in the past to design el_rss to recommend resources to individuals; however, an investigation of recommendations to a group of learners in e-learning is still in its infancy. in this paper, we focus on the problem of recommending resources to a group of learners rather than to an individual. the major challenge in group recommendation is how to merge the individual preferences of different learners that form a group and extract a pseudo unified learner profile (ulp) that closely reflects the preferences of all learners. firstly, we propose a profile merging scheme for the ulp by utilizing learning styles, knowledge levels and ratings of learners in a group. thereafter, a collaborative approach is proposed based on the ulp for effective group recommendations. experimental results are presented to demonstrate the effectiveness of the proposed group recommendation strategy for e-learning.",
    "present_kp": [
      "e-learning",
      "recommender systems",
      "learning styles",
      "knowledge levels"
    ],
    "absent_kp": [
      "group learning"
    ]
  },
  {
    "title": "sms-based human-hosted interactive tv in finland.",
    "abstract": "interactive tv entertainment has brought to life a new kind of tv game show host culture in finland. a qualitative study of sms-to-tv human-hosted interactive tv games, specifically, tv-mobile games and call quizzes, was conducted by recording sample interactive tv programs and corresponding discussion forums on the internet and analyzing the content. the role of the human host in these programs was analyzed and discussed to answer these questions: why is this interactive entertainment popular? what different dimensions can be found? how could this field be used more effectively and what are the aspects developers should pay attention to while designing itv entertainment? this research is important beyond finland since finland tends to pioneer interactive entertainment that later spreads out to other countries",
    "present_kp": [
      "tv-mobile games"
    ],
    "absent_kp": [
      "tv quizzes",
      "ethnography"
    ]
  },
  {
    "title": "artificial channel aided lmmse estimation for timefrequency selective channels in ofdm context.",
    "abstract": "this paper proposes a linear minimum mean square error-based (lmmse) channel estimation method, which allows avoiding the necessary knowledge of the channel covariance matrix or its estimation. to do so, a perfectly tunable filter acting like an artificial channel is added at the receiver side. we show that an lmmse estimation of the sum of this artificial channel and the physical channel only needs the covariance matrix of the artificial channel, and the channel estimation is finally obtained by subtracting the frequency coefficients of the added filter. we call this method artificial channel aided-lmmse (aca-lmmse). theoretical developments and simulations prove that its performance is close to theoretical lmmse, and we show that this method reduces the computational complexity, compared to usual lmmse, due to the covariance matrix used for aca-lmmse is computed only once throughout the transmission duration. we put the conditions on the artificial channel parameters to get the expected mask effect. simulations display the performance of the proposed method, in terms of mmse and bit error rate (ber). indeed, the difference of ber between our method and the theoretical lmmse is less than 2db.",
    "present_kp": [
      "ofdm",
      "channel estimation"
    ],
    "absent_kp": [
      "mean square error methods",
      "digital radio mondiale"
    ]
  },
  {
    "title": "prediction of automobile tire cornering force characteristics by finite element modeling and analysis.",
    "abstract": "in this study, a detailed finite element model of a radial automobile tire is constructed for the prediction of cornering force characteristics during the design stage. the nonlinear stressstrain relationship of rubber as well as a linear elastic approximation, reinforcement, large displacements, and frictional ground contact are modeled. validity of various simplifications is checked. the cornering force characteristics obtained by the finite element tire model are verified on the experimental setup constructed for this purpose.",
    "present_kp": [
      "cornering force characteristics"
    ],
    "absent_kp": [
      "pneumatic tires",
      "nonlinear finite element analysis",
      "tire testing"
    ]
  },
  {
    "title": "design and implementation of a mppt circuit for a solar uav.",
    "abstract": "this paper presents a maximum power point tracking (mppt) circuit for an unmanned air vehicle. the design of the mppt is proposed utilizing a boost-converter topology. the power of the photovoltaic cells is monitored by a closed-loop microcontroller based control system, and the pwm signal of the boost converter continuously adjusted to extract maximum power. the mppt is used to charge the lithium-ion polymer battery and feed the electrical load of the unmanned aircraft.",
    "present_kp": [
      "mppt",
      "photovoltaic cells",
      "uav"
    ],
    "absent_kp": []
  },
  {
    "title": "technical assessment and evaluation of environmental models and software: letter to the editor.",
    "abstract": "this letter details the collective views of a number of independent researchers on the technical assessment and evaluation of environmental models and software. the purpose is to stimulate debate and initiate action that leads to an improved quality of model development and evaluation, so increasing the capacity for models to have positive outcomes from their use. as such, we emphasize the relationship between the model evaluation process and credibility with stakeholders (including funding agencies) with a view to ensure continued support for modelling efforts. many journals, including em&s, publish the results of environmental modelling studies and must judge the work and the submitted papers based solely on the material that the authors have chosen to present and on how they present it. there is considerable variation in how this is done with the consequent risk of considerable variation in the quality and usefulness of the resulting publication. part of the problem is that the review process is reactive, responding to the submitted manuscript. in this letter, we attempt to be proactive and give guidelines for researchers, authors and reviewers as to what constitutes best practice in presenting environmental modelling results. this is a unique contribution to the organisation and practice of model-based research and the communication of its results that will benefit the entire environmental modelling community. for a start, our view is that the community of environmental modellers should have a common vision of minimum standards that an environmental model must meet. a common vision of what a good model should be is expressed in various guidelines on good modelling practice. the guidelines prompt modellers to codify their practice and to be more rigorous in their model testing. our statement within this letter deals with another aspect of the issue it prompts professional journals to codify the peer-review process. introducing a more formalized approach to peer-review may discourage reviewers from accepting invitations to review given the additional time and labour requirements. the burden of proving model credibility is thus shifted to the authors. here we discuss how to reduce this burden by selecting realistic evaluation criteria and conclude by advocating the use of standardized evaluation tools as this is a key issue that needs to be tackled.",
    "present_kp": [
      "model evaluation",
      "model credibility"
    ],
    "absent_kp": [
      "software verification",
      "environmental assessment"
    ]
  },
  {
    "title": "on the wadge reducibility of k-partitions.",
    "abstract": "we establish some results on the wadge degrees and on the boolean hierarchy of k-partitions of some spaces, where k is a natural number. the main attention is paid to the baire space, baire domain and their close relatives. for the case of delta(0)(2)-measurable k-partitions the structures of wadge degrees are characterized completely. for many degree structures, undecidability of the first-order theories is shown, for any k >= 3.",
    "present_kp": [
      "baire space",
      "baire domain",
      "wadge reducibility",
      "k-partition"
    ],
    "absent_kp": [
      "discrete weak semilattice",
      "forest",
      "homomorphic preorder"
    ]
  },
  {
    "title": "simulation of axonal excitability using a spreadsheet template created in microsoft excel.",
    "abstract": "the objective of this present study was to implement an established simulation protocol (a.m. brown, a methodology for simulating biological systems using microsoft excel, comp. methods prog. biomed. 58 (1999) 18190) to model axonal excitability. the simulation protocol involves the use of in-cell formulas directly typed into a spreadsheet and does not require any programming skills or use of the macro language. once the initial spreadsheet template has been set up the simulations described in this paper can be executed with a few simple keystrokes. the model axon contained voltage-gated ion channels that were modeled using hodgkin huxley style kinetics. the basic properties of axonal excitability modeled were: (1) threshold of action potential firing, demonstrating that not only are the stimulus amplitude and duration critical in the generation of an action potential, but also the resting membrane potential; (2) refractoriness, the phenomenon of reduced excitability immediately following an action potential. the difference between the absolute refractory period, when no amount of stimulus will elicit an action potential, and relative refractory period, when an action potential may be generated by applying increased stimulus, was demonstrated with regard to the underlying state of the na+ and k+ channels; (3) temporal summation, a process by which two sub-threshold stimuli can unite to elicit an action potential was shown to be due to conductance changes outlasting the first stimulus and summing with the second stimulus-induced conductance changes to drive the membrane potential past threshold; (4) anode break excitation, where membrane hyperpolarization was shown to produce an action potential by removing na+ channel inactivation that is present at resting membrane potential. the simulations described in this paper provide insights into mechanisms of axonal excitation that can be carried out by following an easily understood protocol.",
    "present_kp": [
      "axon",
      "hodgkin huxley",
      "ion channel",
      "microsoft excel",
      "simulation",
      "spreadsheet"
    ],
    "absent_kp": [
      "modeling"
    ]
  },
  {
    "title": "trabecular bone remodelling under pathological conditions based on biochemical and mechanical processes involved in bmu activity.",
    "abstract": "in adulthood, bone tissue is continuously renewed by processes governed by basic multicellular units composed of osteocytes, osteoclasts and osteoblasts, which are subjected to local mechanical loads. osteocytes are known to be integrated mechanosensors that regulate the activation of the osteoclasts and osteoblasts involved in bone resorption and apposition processes, respectively. after collagen tissue apposition, a process of collagen mineralisation takes place, gradually increasing the effective stiffness of bone. this study presents a new model based on physicochemical parameters involved in spongy bone remodelling under pathological conditions. our model simulates the transient evolution of both geometry and effective young's modulus of the trabeculae, also taking turnover into account. various loads were applied on a trabecula in order to determine the evolution of bone volume fraction under pathological conditions. a parametric study performed on the model showed that one key parameter here is the kinetic constant of hydroxyapatite crystallisation. we subsequently tested our model on a pathological case approaching osteoporosis, involving a decrease in the number of viable osteocytes present in bone. the model converges to a lower value (-5%) for bone volume fraction than with a normal quantity of osteocytes. this useful tool offers new perspectives for predicting bone remodelling deficits on a local scale in patients with pathological conditions such as osteoporosis and in bedridden patients, as well as for astronauts subjected to weightlessness in space.",
    "present_kp": [
      "osteocytes",
      "osteoclast",
      "osteoblast",
      "mineralisation"
    ],
    "absent_kp": [
      "bone pathology"
    ]
  },
  {
    "title": "robust and imperceptible dual watermarking for telemedicine applications.",
    "abstract": "in this paper, the effects of different error correction codes on the robustness and imperceptibility of discrete wavelet transform and singular value decomposition based dual watermarking scheme is investigated. text and image watermarks are embedded into cover radiological image for their potential application in secure and compact medical data transmission. four different error correcting codes such as hamming, the bose, ray-chaudhuri, hocquenghem (bch), the reedsolomon and hybrid error correcting (bch and repetition code) codes are considered for encoding of text watermark in order to achieve additional robustness for sensitive text data such as patient identification code. performance of the proposed algorithm is evaluated against number of signal processing attacks by varying the strength of watermarking and covers image modalities. the experimental results demonstrate that this algorithm provides better robustness without affecting the quality of watermarked image.this algorithm combines the advantages and removes the disadvantages of the two transform techniques. out of the three error correcting codes tested, it has been found that reedsolomon shows the best performance. further, a hybrid model of two of the error correcting codes (bch and repetition code) is concatenated and implemented. it is found that the hybrid code achieves better results in terms of robustness. this paper provides a detailed analysis of the obtained experimental results.",
    "present_kp": [
      "singular value decomposition",
      "error correcting codes"
    ],
    "absent_kp": [
      "image watermarking",
      "steganography",
      "discrete wavelet transforms"
    ]
  },
  {
    "title": "robust estimation of dimension reduction space.",
    "abstract": "most dimension reduction methods based on nonparametric smoothing are highly sensitive to outliers and to data coming from heavy-tailed distributions. two recently proposed methods, minimum average variance estimation and outer product of gradients, can be and are made robust in such a way that preserves all advantages of the original approach. their extension based on the local one-step m-estimators is sufficiently robust to outliers and data from heavy-tailed distributions, it is relatively easy to implement, and surprisingly, it performs as well as the original methods when applied to normally distributed data.",
    "present_kp": [
      "dimension reduction"
    ],
    "absent_kp": [
      "l- and m-estimation",
      "nonparametric regression"
    ]
  },
  {
    "title": "complexity of deciding sense of direction.",
    "abstract": "in this paper we prove that deciding whether a distributed system (represented as a colored digraph with n nodes) has weak sense of direction is in ac(1) (using n(6) processors). moreover, we show that deciding sense of direction is in p. our algorithms can also be used to decide in ac(1) whether a colored graph is a cayley color graph.",
    "present_kp": [
      "sense of direction"
    ],
    "absent_kp": [
      "distributed systems",
      "computational complexity",
      "cayley graphs"
    ]
  },
  {
    "title": "probabilistic fuzzy image fusion approach for radar through wall sensing.",
    "abstract": "this paper addresses the problem of combining multiple radar images of the same scene to produce a more informative composite image. the proposed approach for probabilistic fuzzy logic-based image fusion automatically forms fuzzy membership functions using the gaussian-rayleigh mixture distribution. it fuses the input pixel values directly without requiring fuzzification and defuzzification, thereby removing the subjective nature of the existing fuzzy logic methods. in this paper, the proposed approach is applied to through-the-wall radar imaging in urban sensing and evaluated on real multi-view and polarimetric data. experimental results show that the proposed approach yields improved image contrast and enhances target detection.",
    "present_kp": [
      "fuzzy logic",
      "image fusion",
      "through-the-wall radar imaging"
    ],
    "absent_kp": []
  },
  {
    "title": "inductive time-space lower bounds for sat and related problems.",
    "abstract": "we improve upon indirect diagonalization arguments for lower bounds on explicit problems within the polynomial hierarchy. our contributions are summarized as follows. 1. we present a technique that uniformly improves upon most known nonlinear time lower bounds for nondeterminism and alternating computation, on both subpolynomial (n(o(1))) space rams and sequential one-tape machines with random access to the input. we obtain improved lower bounds for boolean satisfiability (sat), as well as all np-complete problems that have efficient reductions from sat, and sigma(k)-sat, for constant k >= 2. for example, sat cannot be solved by random access machines using n(root 3) time and subpolynomial space. 2. we show how indirect diagonalization leads to time-space lower bounds for computation with bounded nondeterminism. for both the random access and multitape turing machine models, we prove that for all k >= 1, there is a constant c(k) > 1 such that linear time with n(1/k) nondeterministic bits is not contained in deterministic n(ck) time with subpolynomial space. this is used to prove that satisfiability of boolean circuits with n inputs and n(k) size cannot be solved by deterministic multitape turing machines running in n(k.ck) time and subpolynomial space.",
    "present_kp": [
      "lower bounds",
      "satisfiability",
      "diagonalization",
      "bounded nondeterminism"
    ],
    "absent_kp": [
      "time-space tradeoffs",
      "polynomial-time hierarchy"
    ]
  },
  {
    "title": "general form of lattice-valued fuzzy sets under the cutworthy approach.",
    "abstract": "in this note a new solution of problem of synthesis of fuzzy sets is presented. in other words, necessary and sufficient conditions are formulated, under which for a given family of subsets f of a set x and a fixed complete lattice l there is a fuzzy set it : x -> l, such that the collection of cuts of it coincides with f. moreover, it is proved that the general form of lattice-valued fuzzy sets (considering families of cuts) are the type of fuzzy sets having the codomain (0, 1}",
    "present_kp": [
      "lattice-valued fuzzy sets",
      "cuts"
    ],
    "absent_kp": []
  },
  {
    "title": "mechanotransduction in cardiac myocytes.",
    "abstract": "cardiac myocytes react to diverse mechanical demands with a multitude of transient and long-term responses to normalize the cellular mechanical environment. several stretch-activated signaling pathways have been identified, most prominently guanine nucleotide binding proteins (g-proteins), mitogen-activated protein kinases (mapk), janus-associated kinase/signal transducers and activators of transcription (jak/stat), protein kinase c (pkc), calcineurin, intracellular calcium regulation, and several autocrine and paracrine factors. multiple levels of crosstalk exist between pathways. the cellular response to changes in the mechanical environment can lead to cardiac myocyte hypertrophy, cellular growth that can be accompanied by pathological myocyte dysfunction, and tissue fibrosis. several candidates for the primary mechanosensor in cardiac myocytes have been identified, ranging from stretch-activated ion channels in the membrane to yet-unknown mechanosensitive mechanisms in the nucleus. new and refined experimental techniques will exploit advances in molecular biology and biological imaging to study mechanotransduction in isolated cells and genetically engineered mice to explore the function of individual proteins.",
    "present_kp": [
      "mechanotransduction",
      "cardiac myocytes"
    ],
    "absent_kp": [
      "cardiac hypertrophy"
    ]
  },
  {
    "title": "design of a sliding window scheme for detecting high packet-rate flows via random packet sampling.",
    "abstract": "we discuss the design of a sliding window scheme for detecting high packet-rate flows via random packet sampling. we determine the values of control parameters, such as the sampling rate and window length, to minimize the false positive ratio, while keeping the false negative ratio sufficiently low and making the on-line processing possible. under mild assumptions, we formulate this problem as a nonlinear program and provide its numerically feasible global optimal solution. we then conduct sampling experiments with public trace data and discuss the fundamental characteristics of the sliding window scheme with random packet sampling.",
    "present_kp": [
      "random packet sampling",
      "sliding window scheme",
      "high packet-rate flows"
    ],
    "absent_kp": []
  },
  {
    "title": "the fuzzy metric-truth reasoning approach to decision making in soft computing milieux.",
    "abstract": "this article considers fuzzy approximate reasoning utilizing the metric-ruth approach. this approach assesses the truth of a sentence on the basis of its distance from the respective true one. the author has previously examined this subject matter from the logico-methodological point of view. this article focuses on the aspects typical of fuzzy if-then rules within control and decision making.",
    "present_kp": [
      "soft computing",
      "decision making"
    ],
    "absent_kp": [
      "fuzzy reasoning"
    ]
  },
  {
    "title": "querying a summary of database.",
    "abstract": "for some years, data summarization techniques have been developed to handle the growth of databases. however these techniques are usually not provided with tools for end-users to efficiently use the produced summaries. this paper presents a first attempt to develop a querying tool for the saintetiq summarization model. the proposed search algorithm takes advantage of the hierarchical structure of the saintetiq summaries to efficiently answer questions such as \"how are, on some attributes, the tuples which have specific characteristics?\" moreover, this algorithm can be seen both as a boolean querying mechanism over a hierarchy of summaries, and as a flexible querying mechanism over the underlying relational tuples.",
    "present_kp": [
      "data summarization",
      "flexible querying"
    ],
    "absent_kp": [
      "linguistic summaries",
      "summary querying",
      "relational database",
      "fuzzy labels"
    ]
  },
  {
    "title": "subband domain coding of binary textual images for document archiving.",
    "abstract": "in this work, a subband domain textual image compression method is developed. the document image is first decomposed into subimages using binary subband decompositions. next, the character locations in the subbands and the symbol library consisting of the character images are encoded, the method is suitable for keyword search in the compressed data. it is observed that very high compression ratios are obtained with this method. simulation studies are presented.",
    "present_kp": [
      "binary subband decomposition",
      "textual image compression"
    ],
    "absent_kp": [
      "binary image coding",
      "document retrieval"
    ]
  },
  {
    "title": "on-site volume rendering with gpu-enabled devices.",
    "abstract": "now that high-performance computing systems can rely more on a cloud based infrastructure, it becomes much more important to have ubiquitous data processing and visualization capability. this will allow data sharing among numerous clients using shared data repositories through a secure web server. thanks to the wide availability of gpu support in todays mobile devices such as smart phones and tablets, as well as the recently published webgl standard, pervasive computing for high-quality and real-time volume rendering may be realized on such high-performance platforms. we have invented two high-performance volume renderers, namely, single-pass gpu ray caster and fast 3d texture slicer, for both mobile and desktop platforms. rigorous experiments and performance assessments reveal that the proposed mobile 3d image rendering system outperforms the existing approaches in the literature.",
    "present_kp": [
      "pervasive computing",
      "webgl",
      "gpu"
    ],
    "absent_kp": [
      "3d rendering",
      "mobile computing"
    ]
  },
  {
    "title": "multipartite priority queues.",
    "abstract": "we introduce a framework for reducing the number of element comparisons performed in priority-queue operations. in particular, we give a priority queue which guarantees the worst-case cost of o(1) per minimum finding and insertion, and the worst-case cost of o(log n) with at most log n + o(1) element comparisons per deletion, improving the bound of 2 log n + o(1) known for binomial queues. here, n denotes the number of elements stored in the data structure prior to the operation in question, and log n equals log(2)(max {2, n}). as an immediate application of the priority queue developed, we obtain a sorting algorithm that is optimally adaptive with respect to the inversion measure of disorder, and that sorts a sequence having n elements and i inversions with at most n log(i/n) + o(n) element comparisons.",
    "present_kp": [
      "priority queues"
    ],
    "absent_kp": [
      "heaps",
      "meticulous analysis",
      "constant factors"
    ]
  },
  {
    "title": "bounds on codes derived by counting components in varshamov graphs.",
    "abstract": "we are interested in improving the varshamov bound for finite values of length n and minimum distance d. we employ a counting lemma to this end which we find particularly useful in relation to varshamov graphs. since a varshamov graph consists of components corresponding to low weight vectors in the cosets of a code it is a useful tool when trying to improve the estimates involved in the varshamov bound. we consider how the graph can be iteratively constructed and using our observations are able to achieve a reduction in the over-counting which occurs. this tightens the lower bound for any choice of parameters n, k, d or q and is not dependent on information such as the weight distribution of a code.",
    "present_kp": [
      "varshamov bound",
      "varshamov graph"
    ],
    "absent_kp": [
      "greedy codes"
    ]
  },
  {
    "title": "forecasting of circuit-breaker behaviour in high-voltage electrical power systems: necessity for future maintenance management.",
    "abstract": "two research projects were started in order to investigate new methods of maintenance management. the first project was finished in april 1999, dealing with the problem of relating the information of individual devices with their importance in the complete system. the combination of both sets of information is the aim of reliability centred maintenance (rcm). more important devices can be maintained more frequently than those of less importance, leading to reduced maintenance costs but retaining a high level of reliability of the system. the question of how new methods of maintenance influence behaviour in future, cannot be answered right now. the forecasting of the behaviour of circuit-breakers will now be investigated in a further project in the field of maintenance management. a software model will be developed in strong co-ordination with partners of some electrical power system utilities. the model shall include the simulation of the behaviour of circuit-breakers in the future regarding maintenance activities as well as the operational stresses of the present. this paper will give an overview of the actual activities and aims of the project. main activities have been the definition of investigated circuit-breaker types and the methodology for the starting steps of the project.",
    "present_kp": [
      "maintenance management"
    ],
    "absent_kp": [
      "state forecasting"
    ]
  },
  {
    "title": "quality as empowerment: going around in circles.",
    "abstract": "the article introduces a new international educational community based on students quality circles, in which industry and education have learned to collaborate for mutual benefit. in each country represented in this special issue, there have been distinctive bottom-up initiatives, informed by the experience of collaboration. we emphasise quality as empowerment.",
    "present_kp": [
      "bottom-up",
      "empowerment",
      "students quality circles"
    ],
    "absent_kp": [
      "collaborative advantage",
      "compliance",
      "continuous improvement",
      "top-down"
    ]
  },
  {
    "title": "a computer-vision based precision seed drill guidance assistance.",
    "abstract": "this paper presents a control mechanism aiming to position seed drills relative to the previous lines, while sowing. the position was measured by a machine vision system and used in a feedback control loop. an articulated mechanism was used to ensure the lateral displacement of the drill relative to the tractor. the behaviour of the whole outfit was studied during several field tests. the standard deviation of the error, measured as the difference between the observed inter-row distance and its set value, was 23mm and its range was less than 100mm, which was sufficient to fulfil the requirements of the application. sources of systematic errors were also identified as linked to the geometric considerations. their correction requires an accurate mounting of the camera, which may be possible for a serial montage.",
    "present_kp": [
      "seed drill",
      "machine vision"
    ],
    "absent_kp": [
      "automatic guidance",
      "hough transform"
    ]
  },
  {
    "title": "rulebased regulatory and metabolic model for quorum sensing in p. aeruginosa.",
    "abstract": "in the pathogen p. aeruginosa, the formation of virulence factors is regulated via quorum sensing signaling pathways. due to the increasing number of strains that are resistant to antibiotics, there is a high interest to develop novel antiinfectives. in the combat of resistant bacteria, selective blockade of the bacterial celltocell communication (quorum sensing) has gained special interest as antivirulence strategy. here, we modeled the las, rhl, and pqs quorum sensing systems by a multilevel logical approach to analyze how enzyme inhibitors and receptor antagonists effect the formation of autoinducers and virulence factors.",
    "present_kp": [
      "quorum sensing",
      "multilevel logical approach",
      "inhibitor"
    ],
    "absent_kp": [
      "boolean network",
      "generegulatory network",
      "pseudomonas aeruginosa",
      "pqs system"
    ]
  },
  {
    "title": "strong and ultra separation axioms on fuzzy bitopological spaces.",
    "abstract": "given a fuzzy bitopological space (x, tau(1), tau(2)), we introduce a new notion of fuzzy pairwise separation axioms by using the family of its level bitopologies l alpha(tau(1)), l alpha(tau(2)), alpha is an element of [0,1). we prove that these concepts are good extension and we compare them with its corresponding fpti (kandil and el-shafee, 1991) and fpt*(i) (abu safiya et al., 1994) (i = 0, 1,2, 3, 4), respectively. we show that these notions are not equivalent and we give a number of examples which illustrate this fact.",
    "present_kp": [],
    "absent_kp": [
      "fuzzy bitopologicai spaces",
      "alpha-level bitopological spaces",
      "strong fuzzy pairwise separation axioms",
      "alpha-bitopologically generated"
    ]
  },
  {
    "title": "secure and scalable mobility management scheme for the internet of things integration in the future internet architecture.",
    "abstract": "internet of things is becoming a reality with the rapid development of communication technologies. this evolution presents an enrichment of the users' experiences, but also challenges regarding network scalability, security, privacy vulnerabilities, and mobility support. mobility support for the future internet is focused on id/locator split architectures since the limitations of the current internet. this work analyses the security challenges for the himalis (heterogeneity inclusion and mobility adaptation through locator id separation) architecture for the particularities from the internet of things and the id/locator management messages vulnerable to attacks. this work proposes a secure and scalable mobility management scheme that considers the constraints from the internet of things, solving the possible security and privacy vulnerabilities of the himalis architecture. the proposed scheme supports scalable inter-domain authentication and secure location update and binding transfer for the mobility process. the proposed scheme has been verified and evaluated successfully with the avispa framework.",
    "present_kp": [
      "internet of things",
      "future internet architecture",
      "security",
      "privacy",
      "mobility"
    ],
    "absent_kp": []
  },
  {
    "title": "influence of different shoulder-elbow configurations on steering precision and steering velocity in automotive context.",
    "abstract": "influence of posture on driving precision and steering velocity was investigated. arm posture influences steering precision and steering velocity. steering precision and velocity are significantly increased in mid-positions. driver safety can be enhanced by implementing these data in the design process. subjective comfort rating confirmed experimental results.",
    "present_kp": [
      "steering precision",
      "steering velocity"
    ],
    "absent_kp": [
      "optimum driving posture"
    ]
  },
  {
    "title": "modelling procedures for directed network of data blocks.",
    "abstract": "here are presented procedures for modelling data in a network. the methods are extensions of pca or pls regression to a forward network of data blocks. it is assumed that the data blocks are organised in a network such that one data block leads to one or more other data blocks. the procedures are stepwise ones. at each step a passage through the network is carried out. from the input weight vectors of the input or starting blocks, the score and loading vectors of all data blocks are computed. it is investigated if some score/loading vectors are not significant. if some are, they are deleted and revised estimation of the input weights are carried out. when one step is finished, all data matrices are adjusted for score and loading vectors found. a new passage through the network is carried out on the reduced matrices. if no significant loading/score vectors are found for a given set of input weights, the modelling stops. in case of one data block, the algorithm reduces to pca. in case of two data blocks it reduces to pls regression. most methods used in pca or pls regression can be applied to this procedure, e.g., cross-validation and re-sampling procedures. it is pointed out, how these methods can be used to extend other regression methods than pca and pls regression to a network regression.",
    "present_kp": [
      "pls",
      "forward network"
    ],
    "absent_kp": [
      "linear regression",
      "path models",
      "multi-block data"
    ]
  },
  {
    "title": "a novel joint-processing adaptive nonlinear equalizer using a modular recurrent neural network for chaotic communication systems.",
    "abstract": "to eliminate nonlinear channel distortion in chaotic communication systems, a novel joint-processing adaptive nonlinear equalizer based on a pipelined recurrent neural network (jprnn) is proposed, using a modified real-time recurrent learning (rtrl) algorithm. furthermore, an adaptive amplitude rtrl algorithm is adopted to overcome the deteriorating effect introduced by the nesting process. computer simulations illustrate that the proposed equalizer outperforms the pipelined recurrent neural network (prnn) and recurrent neural network (rnn) equalizers.",
    "present_kp": [
      "recurrent neural network"
    ],
    "absent_kp": [
      "pipelined architecture",
      "channel equalizer",
      "chaotic signal"
    ]
  },
  {
    "title": "dynamics of connected vehicle systems with delayed acceleration feedback.",
    "abstract": "acceleration-based connected cruise control (ccc) is implemented for heterogeneous platoons. the ad-hoc nature of wireless vehicle-to-vehicle (v2v) communication is exploited. the design is robust against variation of human parameters and is scalable for large systems. delays are used as design parameters in order to ensure string stability. it is demonstrated that acceleration feedback shall be used in a selective manner.",
    "present_kp": [
      "connected cruise control (ccc)",
      "acceleration feedback"
    ],
    "absent_kp": [
      "head-to-tail string stability",
      "time delay",
      "robustness",
      "dsrc"
    ]
  },
  {
    "title": "exact solution for nonlinear schrodinger equation by he's frequency formulation.",
    "abstract": "in this work, we apply he's frequency formulation to search for the solution to nonlinear schrodinger equation. three examples are given and the solutions obtained are in good accordance with wazwaz's solution . it is shown that he's frequency formulation is of utter straightforward and effective.",
    "present_kp": [
      "nonlinear schrodinger equation",
      "exact solution",
      "he's frequency formulation"
    ],
    "absent_kp": []
  },
  {
    "title": "on the role of trust in collaborative web search.",
    "abstract": "recommender systems combine ideas from information retrieval, user modelling, and artificial intelligence to focus on the provision of more intelligent and proactive information services. as such, recommender systems play an important role when it comes to assisting the user during both routine and specialised information retrieval tasks. like any good assistant it is important that users can trust in the ability of a recommender system to respond with timely and relevant suggestions. in this paper, we will look at a collaborative recommendation system operating in the domain of web search. we will show how explicit models of trust can help to inform more reliable recommendations that translate into more relevant search results. moreover, we demonstrate how the availability of this trust-model facilitates important interface enhancements that provide a means to declare the provenance of result recommendations in a way that will allow searchers to evaluate their likely relevance based on the reputation and trustworthiness of the recommendation partners behind these suggestions.",
    "present_kp": [
      "trust",
      "user modelling",
      "collaborative web search"
    ],
    "absent_kp": []
  },
  {
    "title": "an intelligent learning diagnosis system for web-based thematic learning platform.",
    "abstract": "this work proposes an intelligent learning diagnosis system that supports a web-based thematic learning model, which aims to cultivate learners' ability of knowledge integration by giving the learners the opportunities to select the learning topics that they are interested, and gain knowledge on the specific topics by surfing on the internet to search related learning courseware and discussing what they have learned with their colleagues. based on the log files that record the learners' past online learning behavior, an intelligent diagnosis system is used to give appropriate learning guidance to assist the learners in improving their study behaviors and grade online class participation for the instructor. the achievement of the learners' final reports can also be predicted by the diagnosis system accurately. our experimental results reveal that the proposed learning diagnosis system can efficiently help learners to expand their knowledge while surfing in cyberspace web-based \"theme-based learning\" model.",
    "present_kp": [
      "theme-based learning",
      "learning diagnosis"
    ],
    "absent_kp": [
      "web-based learning",
      "fuzzy expert system",
      "k-nearest neighbor",
      "naive bayesian classifier",
      "support vector machines"
    ]
  },
  {
    "title": "the longest common extension problem revisited and applications to approximate string searching.",
    "abstract": "the longest common extension (lce) problem considers a string s and computes, for each pair (i,j) ( i , j ) , the longest substring of s that starts at both i and j. it appears as a subproblem in many fundamental string problems and can be solved by linear-time preprocessing of the string that allows (worst-case) constant-time computation for each pair. the two known approaches use powerful algorithms: either constant-time computation of the lowest common ancestor in trees or constant-time computation of range minimum queries in arrays. we show here that, from practical point of view, such complicated approaches are not needed. we give two very simple algorithms for this problem that require no preprocessing. the first is 5 times faster than the best previous algorithms on the average whereas the second is faster on virtually all inputs. as an application, we modify the landauvishkin algorithm for approximate matching to use our simplest lce algorithm. the obtained algorithm is 13 to 20 times faster than the original. we compare it with the more widely used ukkonen's cutoff algorithm and show that it behaves better for a significant range of error thresholds.",
    "present_kp": [
      "string",
      "algorithm",
      "longest common extension",
      "approximate string search"
    ],
    "absent_kp": []
  },
  {
    "title": "lumbar spine segmentation using a statistical multi-vertebrae anatomical shape plus pose model.",
    "abstract": "segmentation of the spinal column from computed tomography (ct) images is a preprocessing step for a range of image-guided interventions. one intervention that would benefit from accurate segmentation is spinal needle injection. previous spinal segmentation techniques have primarily focused on identification and separate segmentation of each vertebra. recently, statistical multi-object shape models have been introduced to extract common statistical characteristics between several anatomies. these models can be used for segmentation purposes because they are robust, accurate, and computationally tractable. in this paper, we develop a statistical multi-vertebrae shape+pose model and propose a novel registration-based technique to segment the ct images of spine. the multi-vertebrae statistical model captures the variations in shape and pose simultaneously, which reduces the number of registration parameters. we validate our technique in terms of accuracy and robustness of multi-vertebrae segmentation of ct images acquired from lumbar vertebrae of 32 subjects. the mean error of the proposed technique is below 2 mm, which is sufficient for many spinal needle injection procedures, such as facet joint injections.",
    "present_kp": [
      "computed tomography ",
      "registration",
      "segmentation"
    ],
    "absent_kp": [
      "multi-vertebrae anatomical model",
      "spinal intervention",
      "statistical shape plus pose model"
    ]
  },
  {
    "title": "relative blocking in posets.",
    "abstract": "poset-theoretic generalizations of set-theoretic committee constructions are presented. the structure of the corresponding subposets is described. sequences of irreducible fractions associated to the principal order ideals of finite bounded posets are considered and those related to the boolean lattices are explored; it is shown that such sequences inherit all the familiar properties of the farey sequences.",
    "present_kp": [
      "committee",
      "farey sequence",
      "lattice",
      "poset"
    ],
    "absent_kp": [
      "antichain",
      "blocker",
      "blocker map",
      "clutter"
    ]
  },
  {
    "title": "mixed finite elements for numerical weather prediction.",
    "abstract": "we show how mixed finite element methods that satisfy the conditions of finite element exterior calculus can be used for the horizontal discretisation of dynamical cores for numerical weather prediction on pseudo-uniform grids. this family of mixed finite element methods can be thought of in the numerical weather prediction context as a generalisation of the popular polygonal c-grid finite difference methods. there are a few major advantages: the mixed finite element methods do not require an orthogonal grid, and they allow a degree of flexibility that can be exploited to ensure an appropriate ratio between the velocity and pressure degrees of freedom so as to avoid spurious mode branches in the numerical dispersion relation. these methods preserve several properties of the c-grid method when applied to linear barotropic wave propagation, namely: (a) energy conservation, (b) mass conservation, (c) no spurious pressure modes, and (d) steady geostrophic modes on the f-plane. we explain how these properties are preserved, and describe two examples that can be used on pseudo-uniform grids: the recently-developed modified rtk-q(k-1) element pairs on quadrilaterals and the bdfm1-p1dg p 1 dg element pair on triangles. all of these mixed finite element methods have an exact 2:1 ratio of velocity degrees of freedom to pressure degrees of freedom. finally we illustrate the properties with some numerical examples.",
    "present_kp": [
      "mixed finite elements",
      "numerical weather prediction"
    ],
    "absent_kp": [
      "stability",
      "steady geostrophic states",
      "geophysical fluid dynamics"
    ]
  },
  {
    "title": "dc offset control with application in a zero-if 0.18 mu m cmos bluetooth receiver chain.",
    "abstract": "a compact dc offset correction circuit based on the intrinsic properties of quasi-floating gate (qfg) transistors is presented. the proposed scheme uses a tuning mechanism to make its initial response faster improving the traditional large settling time of these circuits. a zero-if baseband receiver chain suitable for bluetooth that includes the proposed dc offset correction has been designed in a 0.18 mu m cmos technology at 1.2 v supply voltage.",
    "present_kp": [
      "dc offset"
    ],
    "absent_kp": [
      "direct-conversion receivers",
      "low power and low-voltage circuits",
      "qfg transistors"
    ]
  },
  {
    "title": "combining accuracy and success-rate to improve the performance of extended classifier system (xcs) for data-mining and control applications.",
    "abstract": "the emergence of extended classifier systems (xcs) raised the bar for learning classifier systems by incorporating the accuracies of the rules in the lcs's traditional reinforcement mechanism. however, neither xcs nor its extensions take into account the nature of a classifier's experience of attending the action set. we introduce an experienceevaluation mechanism that, once added to the traditional xcs, would assigns to each member of the action set a success rate indicating how effectively the classifier has contributed to the correct responding of the system to the environment's queries. application of the augmented system (called srxcs) to several benchmark problems shows that the proposed mechanism enhances xcs' classification capability and its rate of convergence at the same time. application results indicate that srxcs performs notably better on both pattern association and pattern recognition tasks. the applicability and efficiency of the proposed mechanism is further demonstrated through solving a fairly complex path planning problem for an autonomous mobile robot in a dynamic environment.",
    "present_kp": [
      "classifier systems",
      "xcs"
    ],
    "absent_kp": [
      "rule experience",
      "rule elimination",
      "reinforcement policy"
    ]
  },
  {
    "title": "an experimental study of field dependency in altered gz environments.",
    "abstract": "failure to address extreme environments constraints at the human-computer interaction level may lead to the commission of critical and potentially fatal errors. this experimental study addresses gaps in our current theoretical understanding of the impact of gz accelerations and field dependency independency on task performance in human-computer interaction. it investigates the effects of gz accelerations and field dependency independency on human performance in the completion of perceptual-motor tasks on a personal digital assistant (pda). we report the results of a controlled experiment, conducted in an aerobatic aircraft under multiple gz conditions, showing that cognitive style significantly impacts latency and accuracy in target acquisition for perceptual-motor tasks in altered gz environments and propose design guidelines as countermeasures. based on the results, we argue that developing design requirements taking into account cognitive differences in extreme environments will allow users to execute perceptual-motor tasks efficiently without unnecessarily increasing cognitive load and the probability of critical errors.",
    "present_kp": [
      "extreme environments",
      "target acquisition"
    ],
    "absent_kp": [
      "mobile devices",
      "perceptual style",
      "comparative informatics"
    ]
  },
  {
    "title": "locality discriminating indexing for document classification.",
    "abstract": "this paper introduces a locality discriminating indexing (ldi) algorithm for document classification. based on the hypothesis that samples from different classes reside in class-specific manifold structures, ldi seeks for a projection which best preserves the within-class local structures while suppresses the between-class overlap. comparative experiments show that the proposed method isable to derives compact discriminating document representations for classification.",
    "present_kp": [
      "document classification"
    ],
    "absent_kp": [
      "document indexing",
      "manifold analysis"
    ]
  },
  {
    "title": "accelespell, a gestural interactive game to learn and practice finger spelling.",
    "abstract": "in this paper, an interactive computer game for learning and practicing continuous fingerspelling is described. the game is controlled by an instrumented glove known as acceleglove and a recognition algorithm based on decision trees. the graphical user interface is designed to allow beginners to remember the correct hand shapes and start finger spelling words sooner than traditional methods of learning.",
    "present_kp": [
      "finger spelling"
    ],
    "absent_kp": [
      "interactive games",
      "instrumented gloves"
    ]
  },
  {
    "title": "a systematic optimization approach for assembly sequence planning using taguchi method, doe, and bpnn.",
    "abstract": "research in assembly planning can be categorised into three types of approach: graph-based, knowledge-based and artificial intelligence approaches. the main drawbacks of the above approaches are as follows: the first is time-consuming; in the second approach it is difficult to find the optimal solution; and the third approach requires a high computing efficiency. to tackle these problems, this study develops a novel approach integrated with some graph-based heuristic working rules, robust back-propagation neural network (bpnn) engines via taguchi method and design of experiment (doe), and a knowledge-based engineering (kbe) system to assist the assembly engineers in promptly predicting a near-optimal assembly sequence. three real-world examples are dedicated to evaluating the feasibility of the proposed model in terms of the differences in assembly sequences. the results show that the proposed model can efficiently generate bpnn engines, facilitate assembly sequence optimisation and allow the designers to recognise the contact relationships, assembly difficulties and assembly constraints of three-dimensional (3d) components in a virtual environment type.",
    "present_kp": [
      "assembly sequence planning",
      "design of experiment",
      "taguchi method"
    ],
    "absent_kp": [
      "assembly precedence diagrams",
      "neural networks"
    ]
  },
  {
    "title": "a fuzzy neural network controller with adaptive learning rates for nonlinear slider-crank mechanism.",
    "abstract": "a fuzzy neural network (fnn) controller with adaptive learning rates is proposed to control a nonlinear mechanism system in this study. first, the network structure and the on-line learning algorithm of the fnn is described. to guarantee the convergence of the tracking error, analytical methods based on a discrete-type lyapunov function are proposed to determine the adaptive learning rates of the fnn. next, a slider-crank mechanism, which is driven by a permanent magnet (pm) synchronous motor, is studied as an example to demonstrate the effectiveness of the proposed control technique; the fnn controller is implemented to control the slider position of the motor-slider-crank nonlinear mechanism. the robust control performance and learning ability of the proposed fnn controller with adaptive learning rates is demonstrated by simulation and experimental results.",
    "present_kp": [
      "fuzzy neural network",
      "adaptive learning rates",
      "synchronous motor",
      "slider-crank mechanism"
    ],
    "absent_kp": [
      "position control"
    ]
  },
  {
    "title": "simulated analysis for ingap/gaas heterostructure-emitter bipolar transistor with ingaas/gaas superlattice-base structure.",
    "abstract": "a novel ingap/gaas heterostructure-emitter bipolar transistor (hebt) with ingaas/gaas superlattice-base structure is proposed and demonstrated by two-dimensional analysis. as compared with the traditional hebt, the studied superlattice-base device exhibits a higher collector current, a higher current gain of 246, and a lower baseemitter (be) turn-on voltage of 0.966v at a current level of 1?a, attributed to the increased charge storage of minority carriers in the ingaas/gaas superlattice-base region by tunneling behavior. the low turn-on voltage can reduce the operating voltage and collectoremitter offset voltage for low power consumption in circuit applications.",
    "present_kp": [
      "ingap/gaas",
      "turn-on voltage",
      "offset voltage"
    ],
    "absent_kp": [
      "heterostructure emitter",
      "superlattice base"
    ]
  },
  {
    "title": "why it has become more difficult to predict nobel prize winners: a bibliometric analysis of nominees and winners of the chemistry and physics prizes (<phone>).",
    "abstract": "we propose a comprehensive bibliometric study of the profile of nobel prize winners in chemistry and physics from 1901 to 2007, based on citation data available over the same period. the data allows us to observe the evolution of the profiles of winners in the years leading up toand followingnominations and awarding of the nobel prize. the degree centrality and citation rankings in these fields confirm that the prize is awarded at the peak of the winners citation history, despite a brief halo effect observable in the years following the attribution of the prize. changes in the size and organization of the two fields result in a rapid decline of predictive power of bibliometric data over the century. this can be explained not only by the growing size and fragmentation of the two disciplines, but also, at least in the case of physics, by an implicit hierarchy in the most legitimate topics within the discipline, as well as among the scientists selected for the nobel prize. furthermore, the lack of readily-identifiable dominant contemporary physicists suggests that there are few new paradigm shifts within the field, as perceived by the scientific community as a whole.",
    "present_kp": [
      "nobel prize",
      "citation",
      "centrality"
    ],
    "absent_kp": [
      "scientific disciplines"
    ]
  },
  {
    "title": "an interactive documentation system.",
    "abstract": "most chronic users of time sharing computer systems are familiar with programs that allow the creation and manipulation of text files. less often they have at their disposal programs that will format the document described by a text file, generating output such as a typist might produce. rarely is there any mechanism by which graphics can be integrated with text. lawrence livermore laboratory has a powerful, flexible and interactive computer-based documentation system that will format a source document description according to user specifications and incorporate illustrations to produce online documents, offset reproduction masters, 35mm color slides, movie titles, or viewgraphs. the flexibility of the system is greatly enhanced by the use of a device independent graphics library. text may be plotted using the hardware characters specific to a device (when possible), or may be drawn as hershey characters or polygonally outlined symbols. illustrations may be defined in a simple 2d graphics language, and graphical output from application programs may also be incorporated directly into a document.",
    "present_kp": [
      "color",
      "use",
      "flexibility",
      "graphics",
      "user",
      "online",
      "sharing",
      "program",
      "character",
      "systems",
      "users",
      "language",
      "device",
      "text",
      "hardware",
      "manipulation",
      "documentation",
      "laboratory"
    ],
    "absent_kp": [
      "applications",
      "computation",
      "text processing",
      "timing",
      "libraries",
      "interaction",
      "printing",
      "documentation graphics",
      "color graphics",
      "integrability"
    ]
  },
  {
    "title": "robust time-varying filtering and separation of some nonstationary signals in low snr environments.",
    "abstract": "the proposed algorithm improves filtering performance for monocomponent signals. the proposed algorithm separates multicomponent signals into individual components. the requirement of high sampling rates is significantly relaxed. the proposed algorithm is implemented with low complexity.",
    "present_kp": [
      "time-varying filtering"
    ],
    "absent_kp": [
      "multi-component separation",
      "instantaneous frequency estimation",
      "sinusoidal timefrequency distribution"
    ]
  },
  {
    "title": "towards a general neural controller for quadrupedal locomotion.",
    "abstract": "our study aims at the design and implementation of a general controller for quadruped locomotion, allowing the robot to use the whole range of quadrupedal gaits (i.e.from low speed walking to fast running). a general legged locomotion controller must integrate both posture control and rhythmic motion control and have the ability to shift continuously from one control method to the other according to locomotion speed. we are developing such a general quadrupedal locomotion controller by using a neural model involving a cpg (central pattern generator) utilizing ground reaction force sensory feedback. we used a biologically faithful musculoskeletal model with a spine and hind legs, and computationally simulated stable stepping motion at various speeds using the neuro-mechanical system combining the neural controller and the musculoskeletal model. we compared the changes of the most important locomotion characteristics (stepping period, duty ratio and support length) according to speed in our simulations with the data on real cat walking. we found similar tendencies for all of them. in particular, the swing period was approximately constant while the stance period decreased with speed, resulting in a decreasing stepping period and duty ratio. moreover, the support length increased with speed due to the posterior extreme position that shifted progressively caudally, while the anterior extreme position was approximately constant. this indicates that we succeeded in reproducing to some extent the motion of a cat from the kinematical point of view, even though we used a 2d bipedal model. we expect that such computational models will become essential tools for legged locomotion neuroscience in the future.",
    "present_kp": [
      "quadruped",
      "neural controller",
      "cpg",
      "rhythmic motion",
      "posture"
    ],
    "absent_kp": [
      "computational simulation"
    ]
  },
  {
    "title": "on the superlinear local convergence of a filter-sqp method.",
    "abstract": "transition to superlinear local convergence is shown for a modified version of the trust-region filter-sqp method for nonlinear programming introduced by fletcher, leyffer, and toint . hereby, the original trust-region sqp-steps can be used without an additional second order correction. the main modification consists in using the lagrangian function value instead of the objective function value in the filter together with an appropriate infeasibility measure. moreover, it is shown that the modified trust-region filter-sqp method has the same global convergence properties as the original algorithm in [8].",
    "present_kp": [
      "nonlinear programming",
      "global convergence",
      "filter",
      "sqp"
    ],
    "absent_kp": [
      "superlinear convergence"
    ]
  },
  {
    "title": "multiway covariates regression models.",
    "abstract": "an abundance of methods exist to regress a y variable on a set of x variables collected in a matrix x. in the chemical sciences a growing number of problems translate into arrays of measurements x and y, where x and y are three-way arrays or multiway arrays. in this paper a general model is described for regressing such a multiway y on a multiway x, while taking into account three-way structures in x and y. a global least squares optimization problem is formulated to estimate the parameters of the model. the model is described and illustrated with a real industrial example from batch process operation. an algorithm is given in an appendix.",
    "present_kp": [],
    "absent_kp": [
      "principal covariates regression",
      "partial least squares",
      "multilinear pls",
      "three-way methods",
      "multiway methods"
    ]
  },
  {
    "title": "a note on the iterative object symmetry transform.",
    "abstract": "this paper introduces a new operator named the iterated object transform that is computed by combining the object symmetry transform with the morphological operator erosion. this new operator has been applied on both binary and gray levels images showing the ability to grasp the internal structure of a digital object. we present also some experiments on artificial and real images and potential applications.",
    "present_kp": [],
    "absent_kp": [
      "symmetry transforms",
      "mathematical morphology",
      "image classification",
      "feature extraction"
    ]
  },
  {
    "title": "an anelastic allspeed projection method for gravitationally stratified flows.",
    "abstract": "this paper looks at gravitationally stratified atmospheric flows at low mach and fronde numbers and proposes a new algorithm to solve the compressible euler equations, in which the asymptotic limits are recovered numerically and the boundary conditions for block-structured local refinement methods are well-posed. the model is non-hydrostatic and the numerical algorithm uses a splitting to separate the fast acoustic dynamics from the slower anelastic dynamics. the acoustic waves are treated implicitly while the anelastic dynamics is treated semi-implicitly and an embedded-boundary method is used to represent orography. we present an example that verifies our asymptotic analysis and a set of results that compares very well with the classical gravity wave results presented by durran.",
    "present_kp": [
      "embedded-boundary method",
      "projection method"
    ],
    "absent_kp": [
      "non-hydrostatic atmospheric model",
      "gravity waves"
    ]
  },
  {
    "title": "improvement of cardiac ct reconstruction using local motion vector fields.",
    "abstract": "the motion of the heart is a major challenge for cardiac imaging using ct. a novel approach to decrease motion blur and to improve the signal to noise ratio is motion compensated reconstruction which takes motion vector fields into account in order to correct motion. the presented work deals with the determination of local motion vector fields from high contrast objects and their utilization within motion compensated filtered back projection reconstruction. image registration is applied during the quiescent cardiac phases. temporal interpolation in parameter space is used in order to estimate motion during strong motion phases. the resulting motion vector fields are during image reconstruction. the method is assessed using a software phantom and several clinical cases for calcium scoring. as a criterion for reconstruction quality, calcium volume scores were derived from both, gated cardiac reconstruction and motion compensated reconstruction throughout the cardiac phases using low pitch helical cone beam ct acquisitions. the presented technique is a robust method to determine and utilize local motion vector fields. motion compensated reconstruction using the derived motion vector fields leads to superior image quality compared to gated reconstruction. as a result, the gating window can be enlarged significantly, resulting in increased snr, while reliable hounsfield units are achieved due to the reduced level of motion artefacts. the enlargement of the gating window can be translated into reduced dose requirements.",
    "present_kp": [
      "cardiac ct",
      "motion compensated reconstruction",
      "calcium scoring"
    ],
    "absent_kp": [
      "motion model"
    ]
  },
  {
    "title": "sharing the costs of maintaining environmental resources: a comparison of different programmes.",
    "abstract": "suppose state a controls some resource such as a rainforest and there are some other agents in the international system that wish to see this resource preserved. these agents are prepared to make a contribution towards sharing the costs of maintaining the resource. what would be the long term trajectory of the resource level under different programmes? what type of cost sharing programme would maintain the highest level of the resource? which programme would give the best value for money for the contributing player? this paper attempts to answer these questions. this is done by examining a dynamic model with an infinite time horizon.",
    "present_kp": [
      "environment"
    ],
    "absent_kp": [
      "international environmental agreements",
      "rainforests",
      "dynamic models",
      "optimal control",
      "resource preservation",
      "debt for equity"
    ]
  },
  {
    "title": "regression based d-optimality experimental design for sparse kernel density estimation.",
    "abstract": "this paper derives an efficient algorithm for constructing sparse kernel density (skd) estimates. the algorithm first selects a very small subset of significant kernels using an orthogonal forward regression (ofr) procedure based on the d-optimality experimental design criterion. the weights of the resulting sparse kernel model are then calculated using a modified multiplicative nonnegative quadratic programming algorithm. unlike most of the skd estimators, the proposed d-optimality regression approach is an unsupervised construction algorithm and it does not require an empirical desired response for the kernel selection task. the strength of the d-optimality ofr is owing to the fact that the algorithm automatically selects a small subset of the most significant kernels related to the largest eigenvalues of the kernel design matrix, which counts for the most energy of the kernel training data, and this also guarantees the most accurate kernel weight estimate. the proposed method is also computationally attractive, in comparison with many existing skd construction algorithms. extensive numerical investigation demonstrates the ability of this regression-based approach to efficiently construct a very sparse kernel density estimate with excellent test accuracy, and our results show that the proposed method compares favourably with other existing sparse methods, in terms of test accuracy, model sparsity and complexity, for constructing kernel density estimates.",
    "present_kp": [
      "orthogonal forward regression",
      "d-optimality"
    ],
    "absent_kp": [
      "probability density function",
      "parzen window estimate",
      "sparse kernel modelling",
      "optimal experimental design"
    ]
  },
  {
    "title": "pulmonary nodule registration in serial ct scans using global rib matching and nodule template matching.",
    "abstract": "we propose an automatic nodule registration method between baseline and follow-up chest ct scans. initial alignment using the center of the lung volume corrects the gross translational mismatch, and rigid registration using coronal and sagittal maximum intensity projection images effectively refines the rigid motion of the lungs. nodule correspondences are established by finding the most similar region in terms of density as well as the geometrical constraint. the proposed nodule registration method increased the nodule hit rate (the ratio of the number of successfully matched nodules to total nodule number) from 26% to 100%.",
    "present_kp": [
      "pulmonary nodule registration",
      "rib matching",
      "nodule template matching"
    ],
    "absent_kp": [
      "computed tomography",
      "follow-up ct study",
      "geometrical constraint using log-polar image"
    ]
  },
  {
    "title": "robotics in special needs education.",
    "abstract": "the purpose of this study is to explore the potential of robotics as an educational tool in special needs education. qualitative case studies are used to increase knowledge about programmable lego nxt and topobo robotics constructions kits in special needs education, and about the social robot and topobo that are used in early childhood education when possible learning disabilities have not yet been diagnosed. this study aims to provide suggestions about how robotics might be used to recognize disabilities at an early stage of education and to compensate for them in learning.",
    "present_kp": [
      "robotics",
      "social robot",
      "special needs education"
    ],
    "absent_kp": [
      "educational technology",
      "programmable construction kit"
    ]
  },
  {
    "title": "applying fisher's filter to select kdd connections' features and using neural networks to classify and detect attacks.",
    "abstract": "most of the neural networks-based intrusion detection systems (ids) examine all data features to detect intrusion or misuse patterns. some of the features may be redundant or contribute little (if anything) to the detection process. that is why the purpose of this study is to identify important kdd features which will be used to train a neural network (nn), in order to best classify and detect attacks. four nns were studied: modular, recurrent, principal component analysis (pca), and time-lag recurrent (tlr) nns. we investigated the performance of combining the fisher's filter used as a feature selection technique, with one of the previously cited nns. our simulations show that using fisher's filter improves largely the performance of the four considered nns in terms of detection rate, attack classification, and computational time.",
    "present_kp": [
      "intrusion detection systems",
      "neural networks"
    ],
    "absent_kp": [
      "misuse detection",
      "fisher's anova ranking",
      "knowledge discovery and data mining  dataset",
      "kdd feature reduction"
    ]
  },
  {
    "title": "tight upper bounds on the minimum precision required of the divisor and the partial remainder in high-radix division.",
    "abstract": "digit-recurrence binary dividers are sped up via two complementary methods: keeping the partial remainder in redundant form and selecting the quotient digits in a radix higher than 2. use of a redundant partial remainder replaces the standard addition in each cycle by a carry-free addition, thus making the cycles shorter. deriving the quotient in high radix reduces the number of cycles (by a factor of about h for radix 2(h)). to make the redundant partial remainder scheme work, quotient digits must be chosen from a redundant set, such as [-2, 2] in radix 4. the redundancy provides some tolerance to imprecision so that the quotient digits can be selected based on examining truncated versions of the partial remainder and divisor. no closed form formula for the required precision in the partial remainder and divisor, as a function of the quotient digit set and the range of the partial remainders is known. in this paper, we establish tight upper bounds on the required precision for the partial remainder and divisor. the bounds are tight in the sense that each is only one bit over a well-known simple lower bound. we also discuss the implications of these bounds for the quotient digit selection process.",
    "present_kp": [
      "high-radix division",
      "quotient digit selection"
    ],
    "absent_kp": [
      "digit-recurrence division",
      "digit-selector pla",
      "p-d plot",
      "srt division"
    ]
  },
  {
    "title": "the effect of viewing angle on wrist posture estimation from photographic images using novice raters.",
    "abstract": "observational assessment of wrist posture using photographic methods is theoretically affected by camera view angle. a study was conducted to investigate whether wrist flexion/extension and radial/ulnar deviation postures were estimated differently by raters depending on the viewing angle and compared to predictions using a quantitative 2d model of parallax. novice raters (n=26) estimated joint angles from images of wrist postures photographed from ten different viewing angles. results indicated that ideal views, orthogonal to the plane of motion, produced more accurate estimates of posture compared to non-ideal views. the neutral (0) posture was estimated the most accurately even at different viewing angles. raters were more accurate than model predictions. findings demonstrate a need for more systematic methods for collecting and analyzing photographic data for observational studies of posture. renewed caution in interpreting existing studies of wrist posture where viewing angle was not controlled is advised.",
    "present_kp": [
      "parallax",
      "wrist",
      "posture",
      "viewing angle"
    ],
    "absent_kp": []
  },
  {
    "title": "new proposals for the design of steel beam-columns in case of fire, including a new approach for the lateraltorsional buckling.",
    "abstract": "the possibility of having, in parts 1-1 and 1-2 of eurocode 3, the same approach for the design of beam-columns and for lateraltorsional buckling, was investigated by the authors in previous papers using a numerical approach, where it was concluded that those assumptions could be made. in the present paper, a new approach for lateraltorsional buckling has been used with the formulae for the design of beam-columns at elevated temperature based on pren 1993-1-1 combined with the formulae from pren 1993-1-2. in both cases the results obtained are much better than the current design expressions, when compared with those obtained in the numerical calculations.",
    "present_kp": [
      "steel",
      "beam-column",
      "lateraltorsional buckling",
      "fire",
      "eurocode 3"
    ],
    "absent_kp": [
      "numerical modelling"
    ]
  },
  {
    "title": "a note on the article \"fuzzy less strongly semiopen sets and fuzzy less strong semicontinuity\".",
    "abstract": "in this note we show that some results in the article by fang jing ming are incorrect.",
    "present_kp": [],
    "absent_kp": [
      "fuzzy topology",
      "fuzzy strongly semiopen set"
    ]
  },
  {
    "title": "differential fault analysis on camellia.",
    "abstract": "camellia is a 128-bit block cipher published by ntt and mitsubishi in 2000. on the basis of the byte-oriented model and the differential analysis principle, we propose a differential fault attack on the camellia algorithm. mathematical analysis and simulating experiments show that our attack can recover its 128-bit, 192-bit or 256-bit secret key by introducing 30 faulty ciphertexts. thus our result in this study describes that camellia is vulnerable to differential fault analysis. this work provides a new reference to the fault analysis of other block ciphers.",
    "present_kp": [
      "differential fault analysis",
      "block ciphers",
      "camellia"
    ],
    "absent_kp": [
      "side channel attacks"
    ]
  },
  {
    "title": "the l(2,1) -labeling of unigraphs.",
    "abstract": "the l(2,1)-labeling problem consists of assigning colors from the integer set 0,,? to the nodes of a graph g in such a way that nodes at a distance of at most two get different colors, while adjacent nodes get colors which are at least two apart. the aim of this problem is to minimize and it is in general np-complete. in this paper the problem of l(2,1)-labeling unigraphs, i.e. graphs uniquely determined by their own degree sequence up to isomorphism, is addressed and a 3/2-approximate algorithm for l(2,1)-labeling unigraphs is designed. this algorithm runs in o(n) time, improving the time of the algorithm based on the greedy technique, requiring o(m) time, that may be near to ?(n2) for unigraphs.",
    "present_kp": [
      "unigraphs"
    ],
    "absent_kp": [
      "l -labeling",
      "frequency assignment"
    ]
  },
  {
    "title": "ground control station embedded mission planning for uas.",
    "abstract": "as the unmanned aerial system (uas) level of automation increases, mission planning relevance raises. a mission plan can be defined as all the information needed to reach the assigned goals, and it is composed by several sub-plans. in particular, the mission plan core is represented by the routes. since the route creation process is very complex, the introduction of route creation and verification algorithms is required. these algorithms enhance the crew replan performances during the mission execution, and permit to implement autonomous on-board replanning. furthermore, planning/replanning processes could also have a key role in the integration of uas in the civil airspace. according to these considerations, a mission planner embedded in the alenia aermacchi uas ground control station (gcs) has been developed, comprised of advanced planning algorithms.",
    "present_kp": [
      "mission plan"
    ],
    "absent_kp": [
      "route creation/validation algorithms",
      "stanag 4586"
    ]
  },
  {
    "title": "automatic frechet differentiation for the numerical solution of boundary-value problems.",
    "abstract": "a new solver for nonlinear boundary-value problems (bvps) in matlab is presented, based on the chebfun software system for representing functions and operators automatically as numerical objects. the solver implements newton's method in function space, where instead of the usual jacobian matrices, the derivatives involved are frechet derivatives. a major novelty of this approach is the application of automatic differentiation (ad) techniques to compute the operator-valued frechet derivatives in the continuous context. other novelties include the use of anonymous functions and numbering of each variable to enable a recursive, delayed evaluation of derivatives with forward mode ad. the ad techniques are applied within a new chebfun class called chebop which allows users to set up and solve nonlinear bvps, both scalar and systems of coupled equations, in a few lines of code, using the \"nonlinear backslash\" operator (). this framework enables one to study the behaviour of newton's method in function space.",
    "present_kp": [
      "chebfun",
      "newton's method in function space"
    ],
    "absent_kp": [
      "algorithms",
      "design",
      "performance",
      "linearization of boundary-value problems",
      "object-oriented matlab"
    ]
  },
  {
    "title": "asynchronous parallel finite automaton: a new mechanism for deep packet inspection in cloud computing.",
    "abstract": "security is quite an important issue in cloud computing. the general security mechanisms applied in the cloud are always passive defense methods such as encryption. besides these, it's necessary to utilize real-time active monitoring, detection and defense technologies. according to the published researches, deep packets inspection (dpi) is the most effective technology to realize active inspection and defense. however, most of the works on dpi focus on its performance in general application scenarios and make improvement for space reduction, which could not meet the demands of high speed and stability in the cloud. therefore it is meaningful to improve the common mechanisms of dpi, making it more suitable for cloud computing. in this paper, an asynchronous parallel finite automaton (fa) is proposed. the applying of asynchronous parallelization and heuristic forecast mechanism decreases the time consumed in matching significantly, while still reduces the memory required. moreover, it is immune to overlapping problem, also enhancing the stability. the final evaluation results show that asynchronous parallel fa has higher stability, better performance on both time and memory, and is more suitable for cloud computing.",
    "present_kp": [
      "cloud computing",
      "deep packet inspection",
      "asynchronous parallel finite automaton"
    ],
    "absent_kp": [
      "lock-free fifo"
    ]
  },
  {
    "title": "dynamic of a non-autonomous predatorprey system with infinite delay and diffusion.",
    "abstract": "in the present paper, a nonlinear non-autonomous predatorprey dispersion model with continuous delay is studied. sufficient conditions which guarantee the existence of a periodic positive solution are obtained by using gaines and mawhins continuation theorem of coincidence degree theory. moreover, globally asymptotically stability of the system is also obtained by means of a suitable lyapunov functional. the applications show that these criteria are easily verified.",
    "present_kp": [
      "dispersion"
    ],
    "absent_kp": [
      "time delay",
      "periodic solution",
      "persistent",
      "globally asymptotically stable"
    ]
  },
  {
    "title": "secure and lightweight network admission and transmission protocol for body sensor networks.",
    "abstract": "a body sensor network (bsn) is a wireless network of biosensors and a local processing unit, which is commonly referred to as the personal wireless hub (pwh). personal health information (phi) is collected by biosensors and delivered to the pwh before it is forwarded to the remote healthcare center for further processing. in a bsn, it is critical to only admit eligible biosensors and pwh into the network. also, securing the transmission from each biosensor to pwh is essential not only for ensuring safety of phi delivery, but also for preserving the privacy of phi. in this paper, we present the design, implementation, and evaluation of a secure network admission and transmission subsystem based on a polynomial-based authentication scheme. the procedures in this subsystem to establish keys for each biosensor are communication efficient and energy efficient. moreover, based on the observation that an adversary eavesdropping in a bsn faces inevitable channel errors, we propose to exploit the adversary's uncertainty regarding the phi transmission to update the individual key dynamically and improve key secrecy. in addition to the theoretical analysis that demonstrates the security properties of our system, this paper also reports the experimental results of the proposed protocol on resource-limited sensor platforms, which show the efficiency of our system in practice.",
    "present_kp": [
      "efficiency",
      "network admission and transmission",
      "security"
    ],
    "absent_kp": [
      "body sensor networks ",
      "key update"
    ]
  },
  {
    "title": "time-domain orthogonal finite-element reduction-recovery method for electromagnetics-based analysis of large-scale integrated circuit and package problems.",
    "abstract": "a time-domain orthogonal finite-element reduction-recovery method is developed to overcome the large problem sizes encountered in the simulation of large-scale integrated-circuit and package problems. in this method, a set of orthogonal prism vector basis functions is developed. based on this set of bases, an arbitrary 3-d multilayered system such as a combined package and die is reduced to a single-layer system with negligible computational cost. more importantly, the reduced single-layer system is diagonal and, hence, can be solved readily. from the solution of the reduced system, the solution of the other unknowns is recovered in linear complexity. the method entails no theoretical approximation. it applies to any arbitrarily shaped multilayer structure involving inhomogeneous materials or any structure that can be geometrically modeled by triangular prism elements. in addition, it permits nonlinear device modeling and broadband simulation within one run. numerical and experimental results have demonstrated its accuracy and high capacity in simulating on-chip, package, and die-package interface problems.",
    "present_kp": [
      "on-chip",
      "package"
    ],
    "absent_kp": [
      "die-package cosimulation",
      "electromagnetic simulation",
      "finite-element methods",
      "large scale",
      "time domain"
    ]
  },
  {
    "title": "the role of social network sites in romantic relationships: effects on jealousy and relationship happiness.",
    "abstract": "on social network sites (sns), information about one's romantic partner is readily available and public for friends. the paper focuses on the negative (sns jealousy) and positive (sns relationship happiness) consequences of sns use for romantic relationships. we examined whether relationship satisfaction, trait jealousy, sns use and need for popularity predicted these emotional consequences of sns use and tested the moderating role of self-esteem. for low self-esteem individuals, need for popularity predicted jealousy and relationship happiness. for high-self-esteem individuals, sns use for grooming was the main predictor. low-self-esteem individuals try to compensate their low self-esteem by creating an idealized picture. undesirable information threatens this picture, and especially individuals with a high need for popularity react with sns jealousy.",
    "present_kp": [
      "relationship"
    ],
    "absent_kp": [
      "networks",
      "interpersonal",
      "psychological factors"
    ]
  },
  {
    "title": "nonlinear data projection on non-euclidean manifolds with controlled trade-off between trustworthiness and continuity.",
    "abstract": "this paper presents a framework for nonlinear dimensionality reduction methods aimed at projecting data on a non-euclidean manifold, when their structure is too complex to be embedded in an euclidean space. the methodology proposes an optimization procedure on manifolds to minimize a pairwise distance criterion that implements a control of the trade-off between trustworthiness and continuity, two criteria that, respectively, represent the risks of flattening and tearing the projection. the methodology is presented as general as possible and is illustrated in the specific case of the sphere.",
    "present_kp": [
      "nonlinear dimensionality reduction",
      "trade-off between trustworthiness and continuity"
    ],
    "absent_kp": [
      "distance-based data projection method",
      "optimization on manifolds"
    ]
  },
  {
    "title": "quantum computation for action selection using reinforcement learning.",
    "abstract": "this paper proposes a novel action selection method based on quantum computation and reinforcement learning (rl). inspired by the advantages of quantum computation, the state/action in a rl system is represented with quantum superposition state. the probability of action eigenvalue is denoted by probability amplitude, which is updated according to rewards. and the action selection is carried out by observing quantum state according to collapse postulate of quantum measurement. the results of simulated experiments show that quantum computation can be effectively used to action selection and decision making through speeding up learning. this method also makes a good tradeoff between exploration and exploitation for rl using probability characteristics of quantum theory.",
    "present_kp": [
      "quantum computation",
      "action selection",
      "reinforcement learning"
    ],
    "absent_kp": [
      "grover iteration"
    ]
  },
  {
    "title": "principal agent theory and its application to analyze outsourcing of software development.",
    "abstract": "much has been written on process models, project management or tool support to increase the return on investment in software through higher quality of the development process and the resulting software or system. yet, we lack understanding in the underlying economic principles; e.g., an external firm paid to develop software for someone else tries to maximize their own profit instead of the contractor's. these divergences of interests result in projects that consume more time and money and meet fewer requirements than expected. in this paper, we try to fill the gap by providing an insight into the theory and presenting applicable suggestions how to diminish or avoid the problems that arise when selecting the 'best' contractor and during the project. basic advises on the formulation of contracts can be derived.",
    "present_kp": [
      "outsourcing",
      "principal agent theory"
    ],
    "absent_kp": []
  },
  {
    "title": "saccular projections in the human cerebral cortex.",
    "abstract": "abstract: the cerebral cortical areas processing saccular information were investigated in human subjects using the fmri method and loud clicks, which selectively activate the saccule. the results were compared with previous vestibular evoked potential (vep) studies in anesthetized patients following vestibular nerve stimulation. nine normal subjects participated in fmri studies. by comparing the cortical areas activated by a click at 85 db (auditory activation) with those activated by 102 db (auditory plus saccular activation), the following cortical areas were selectively activated by saccular stimulation: intraparietal sulcus, frontal eye fields, prefrontal cortex, and postcentral gyrus, in addition to insula, supplementary motor area, and anterior and posterior cingulate cortex. previous vep studies also revealed similar activation areas by vestibular nerve stimulation with latencies at 6 ms, suggesting that the shortest pathways for activation of cerebral cortical neurons from the labyrinth are trisynaptic, with a relay in the thalamus. the activated areas are also consistent with results in previous studies using caloric stimulation, which primarily activates horizontal semicircular canals. these results suggest that canal and otolith information is processed largely by similar cortical areas in humans. multiple cortical areas activated by these studies suggest that these areas are involved in different aspects of processing vestibular information. the saccular projections to the prefrontal and frontal cortex suggest that these areas are involved in planning motor synergies to counteract loss of equilibrium.",
    "present_kp": [
      "fmri",
      "prefrontal",
      "human"
    ],
    "absent_kp": [
      "vestibular evoked potentials"
    ]
  },
  {
    "title": "state-density functions over dbm domains in the analysis of non-markovian models.",
    "abstract": "quantitative evaluation of models with generally distributed transitions requires the analysis of non-markovian processes that may be not isomorphic to their underlying untimed models and may include any number of concurrent nonexponential timers. the analysis of stochastic time petri nets (stpns) copes with the problem by covering the state space with stochastic classes, which extend the theory of difference bounds matrix (dbm) with a state probability density function. as a core step, the analysis process requires symbolic manipulation of density functions supported over dbm domains. we characterize and engineer the critical steps of this derivation. we first show that the state-density function accepts a continuous piecewise representation over a partition in dbm-shaped subdomains. we then develop a closed-form symbolic calculus of state-density functions under the assumption that transitions in the stpn model have expolynomial distributions over possibly bounded intervals. the calculus shows that within each subdomain, the state-density function is a multivariate expolynomial function, and it makes explicit the way in which this form evolves and grows in complexity as the state accumulates memory through subsequent transitions. this enables an efficient implementation of the analysis process and provides the formal basis that supports the introduction of an imprecise analysis based on the approximation of state-density functions through bernstein polynomials. the approximation attacks practical and theoretical limits in the applicability of stochastic state classes and devises a new approach to the analysis of non-markovian models, relying on approximations in the state space rather than in the structure of the model.",
    "present_kp": [
      "quantitative evaluation",
      "stochastic time petri nets",
      "difference bounds matrix",
      "bernstein polynomials"
    ],
    "absent_kp": [
      "correctness verification",
      "performance and dependability",
      "dense-time state-space analysis",
      "markov renewal theory",
      "approximate state-space representation",
      "density function approximation"
    ]
  },
  {
    "title": "reanalysis and sensitivity reanalysis by combined approximations.",
    "abstract": "one of the main obstacles in the solution of structural optimization problems is the need to repeat solutions of the analysis and sensitivity analysis equations. in large-scale structures, having complex analysis models, the computational effort may become prohibitive. to alleviate this difficulty a general approach for repeated analysis and repeated sensitivity analysis, called combined approximations, was developed during the last 15 years. the solution is based on the integration of several algorithms and methods. as a result, accurate results can be achieved efficiently. in previous studies, solution procedures for various particular problems were developed. this article summarizes the various formulations and solution procedures for reanalysis and sensitivity reanalysis of linear, nonlinear, static and dynamic systems. it is shown that the various solution procedures are based on applications of similar basic algorithms. numerical examples demonstrate the efficiency of the calculations and the accuracy of the results.",
    "present_kp": [
      "sensitivity reanalysis"
    ],
    "absent_kp": [
      "approximate reanalysis",
      "combined approximations ",
      "efficient reanalysis"
    ]
  },
  {
    "title": "making on-line logistics training sustainable through e-learning.",
    "abstract": "the purpose of this study is to investigate the possibility of using an online logistics certification learning environment as a training tool to equip future logisticians with required logistics skills. this study incorporates an online logistics certification website that was constructed for college students to familiarize themselves with the certification. in addition, this study also performed comparison tests on students before and after their interaction with the web-based learning environment system to ascertain the systems effectiveness. our findings suggest that such a system might motivate students to familiarize themselves with logistics-related certification information and can enhance students professional capabilities. in addition, the web-based learning environment might possibly motivate students to join logistics related industries in the future.",
    "present_kp": [
      "e-learning",
      "training",
      "logistics"
    ],
    "absent_kp": [
      "self-learning",
      "capability",
      "motivation"
    ]
  },
  {
    "title": "panorama weaving: fast and flexible seam processing.",
    "abstract": "a fundamental step in stitching several pictures to form a larger mosaic is the computation of boundary seams that minimize the visual artifacts in the transition between images. current seam computation algorithms use optimization methods that may be slow, sequential, memory intensive, and prone to finding suboptimal solutions related to local minima of the chosen energy function. moreover, even when these techniques perform well, their solution may not be perceptually ideal (or even good). such an inflexible approach does not allow the possibility of user-based improvement. this paper introduces the panorama weaving technique for seam creation and editing in an image mosaic. first, panorama weaving provides a procedure to create boundaries for panoramas that is fast, has low memory requirements and is easy to parallelize. this technique often produces seams with lower energy than the competing global technique. second, it provides the first interactive technique for the exploration of the seam solution space. this powerful editing capability allows the user to automatically extract energy minimizing seams given a sparse set of constraints. with a variety of empirical results, we show how panorama weaving allows the computation and editing of a wide range of digital panoramas including unstructured configurations.",
    "present_kp": [
      "digital panoramas"
    ],
    "absent_kp": [
      "panorama editing",
      "panorama seams",
      "interactive image boundaries"
    ]
  },
  {
    "title": "mining service abstractions (nier track).",
    "abstract": "several lines of research rely on the concept of service abstractions to enable the organization, the composition and the adaptation of services. however, what is still missing, is a systematic approach for extracting service abstractions out of the vast amount of services that are available all over the web. to deal with this issue, we propose an approach for mining service abstractions, based on an agglomerative clustering algorithm. our experimental findings suggest that the approach is promising and can serve as a basis for future research.",
    "present_kp": [
      "services",
      "agglomerative clustering"
    ],
    "absent_kp": [
      "abstraction recovery"
    ]
  },
  {
    "title": "average-voice-based speech synthesis using hsmm-based speaker adaptation and adaptive training.",
    "abstract": "in speaker adaptation for speech synthesis, it is desirable to convert both voice characteristics and prosodic features such as f0 and phone duration. for simultaneous adaptation of spectrum, f0 and phone duration within the hmm framework, we need to transform not only the state output distributions corresponding to spectrum and f0 but also the duration distributions corresponding to phone duration. however, it is not straightforward to adapt the state duration because the original hmm does not have explicit duration distributions. therefore, we utilize the framework of the hidden semi-markov model (hsmm), which is an hmm having explicit state duration distributions, and we apply an hsmm-based model adaptation algorithm to simultaneously transform both the state output and state duration distributions. furthermore, we propose an hsmm-based adaptive training algorithm to simultaneously normalize the state output and state duration distributions of the average voice model. we incorporate these techniques into our hsmm-based speech synthesis system, and show their effectiveness from the results of subjective and objective evaluation tests.",
    "present_kp": [
      "speaker adaptation",
      "hidden semi-markov model "
    ],
    "absent_kp": [
      "hmm-based speech synthesis",
      "speaker adaptive training ",
      "maximum likelihood linear regression ",
      "voice conversion"
    ]
  },
  {
    "title": "a two-parameter continuation algorithm for vortex pinning in rotating boseeinstein condensates.",
    "abstract": "we describe an efficient two-parameter continuation algorithm combined with spectral collocation methods for computing the ground state and central vortex state solutions of rotating boseeinstein condensates in optical lattices, where the first kind and second kind chebyshev polynomials are used as the basis functions for the trial function space. by treating the chemical potential and angular velocity as the continuation parameters simultaneously under the additional constraint of normalization condition, the proposed algorithm can effectively compute numerical solutions for a rich variety of physical phenomena observed in physical experiments with very little cost. comparisons with various numerical methods on some sample test problems are reported.",
    "present_kp": [
      "spectral collocation methods"
    ],
    "absent_kp": [
      "grosspitaevskii equation",
      "ground state solution"
    ]
  },
  {
    "title": "complexity and endogeneity in economic modeling.",
    "abstract": "purpose - the concepts of complexity, endogeneity and circular causation - myrdal's term was cumulative causation - are shown to be interrelated ones in configuring an economic model in the framework of systemic embedding and its empirical application. design/methodology/approach - the ensuing framework of economic modeling with complexity provides a controllable and predictable overarching worldview. anomie in the economic universe and its embedded world-system are analytically rejected. this consequence is due to the epistemic nature of modeling that combines complexity, endogeneity, and circular causation for attaining predictability and controllability, even in the face of complex systemic perturbations. the epistemology of unity of knowledge contrasted with rationalism is treated as the foundational worldview. an illustrative empirical work is given to convey the conceptual model and its applied viability. findings - both the theoretical and empirical results point out how the induced effects of knowledge flows in reference to the epistemology of unity of knowledge continuously improves the complementary relationships of the evolutionary learning fields, and rejects marginalism as being logically non-sequiter in such epistemic systems. research limitations/implications - more variables and data would increase the explanation of the continuous simulation in the evolutionary learning world-system model. practical implications - more data would increase the versatility of the empirical exercise. social implications - the study is based on the idea of social and economic interface in extending the scope of economic modeling. originality/value - the paper is very original in the area of heterodox economics that questions orthodox economic postulates and presents the complex methodology by circular causation method instead.",
    "present_kp": [
      "complexity",
      "economics"
    ],
    "absent_kp": [
      "causality",
      "consciousness",
      "cybernetics",
      "creativity",
      "modelling"
    ]
  },
  {
    "title": "dyfram: dynamic fragmentation and replica management in distributed database systems.",
    "abstract": "in distributed database systems, tables are frequently fragmented and replicated over a number of sites in order to reduce network communication costs. how to fragment, when to replicate and how to allocate the fragments to the sites are challenging problems that has previously been solved either by static fragmentation, replication and allocation, or based on a priori query analysis. many emerging applications of distributed database systems generate very dynamic workloads with frequent changes in access patterns from different sites. in such contexts, continuous refragmentation and reallocation can significantly improve performance. in this paper we present dyfram, a decentralized approach for dynamic table fragmentation and allocation in distributed database systems based on observation of the access patterns of sites to tables. the approach performs fragmentation, replication, and reallocation based on recent access history, aiming at maximizing the number of local accesses compared to accesses from remote sites. we show through simulations and experiments on the dascosa distributed database system that the approach significantly reduces communication costs for typical access patterns, thus demonstrating the feasibility of our approach.",
    "present_kp": [
      "fragmentation",
      "replication"
    ],
    "absent_kp": [
      "distributed dbms",
      "physical database design"
    ]
  },
  {
    "title": "the anatomy of decision support during inpatient care provider order entry (cpoe): empirical observations from a decade of cpoe experience at vanderbilt.",
    "abstract": "the authors describe a pragmatic approach to the introduction of clinical decision support at the point of care, based on a decade of experience in developing and evolving vanderbilts inpatient wizorder care provider order entry (cpoe) system. the inpatient care setting provides a unique opportunity to interject cpoe-based decision support features that restructure clinical workflows, deliver focused relevant educational materials, and influence how care is delivered to patients. from their empirical observations, the authors have developed a generic model for decision support within inpatient cpoe systems. they believe that the models utility extends beyond vanderbilt, because it is based on characteristics of end-user workflows and on decision support considerations that are common to a variety of inpatient settings and cpoe systems. the specific approach to implementing a given clinical decision support feature within a cpoe system should involve evaluation along three axes: what type of intervention to create (for which the authors describe 4 general categories); when to introduce the intervention into the users workflow (for which the authors present 7 categories), and how disruptive, during use of the system, the intervention might be to end-users workflows (for which the authors describe 6 categories). framing decision support in this manner may help both developers and clinical end-users plan future alterations to their systems when needs for new decision support features arise.",
    "present_kp": [
      "cpoe",
      "clinical decision support"
    ],
    "absent_kp": []
  },
  {
    "title": "the method of approximate fundamental solutions for axisymmetric problems with laplace operator.",
    "abstract": "the paper presents a new numerical technique for solving axisymmetric problems with laplace operator. it is similar to the method of fundamental solutions but it is based on the use of special basis functions which satisfy the majority of the boundary conditions of the problem considered. this reduces the number of unknowns and the size of the collocation matrix considerably. as it is shown in the paper, this technique can also be applied successfully in the cases when the solution domain has infinite boundaries in z or r directions. numerical examples justifying the method are presented.",
    "present_kp": [
      "axisymmetric problems",
      "fundamental solutions"
    ],
    "absent_kp": [
      "laplace equation",
      "infinite domain"
    ]
  },
  {
    "title": "bayesian ordinal and binary regression models with a parametric family of mixture links.",
    "abstract": "an ordinal and binary regression model with parametric link is introduced. the link is a member of a one-parameter family of mixture links, a family that comprises smooth mixtures of the extreme minimum-value, extreme maximum-value, and logistic distributions. a bayesian version of this flexible model serves as a vehicle for introducing a priori information regarding the choice of link. owing to non-conjugacy, posterior and predictive distributions are approximated using markov chain monte carlo simulation methods. link-independent, bayesian interpretations of covariate effects are described. the method is illustrated through the analyses of several data sets.",
    "present_kp": [
      "markov chain monte carlo"
    ],
    "absent_kp": [
      "categorical data",
      "cumulative link model",
      "ld50",
      "logistic regression",
      "mixture distribution",
      "metropolishastings algorithm",
      "ordinal data",
      "predictive posterior distribution",
      "tolerance distribution"
    ]
  },
  {
    "title": "a generic implementation of replica exchange with solute tempering (rest2) algorithm in namd for complex biophysical simulations.",
    "abstract": "replica exchange with solute tempering (rest2) is a powerful sampling enhancement algorithm of molecular dynamics (md) in that it needs significantly smaller number of replicas but achieves higher sampling efficiency relative to standard temperature exchange algorithm. in this paper, we extend the applicability of rest2 for quantitative biophysical simulations through a robust and generic implementation in greatly scalable md software namd. the rescaling procedure of force field parameters controlling rest2 hot region is implemented into namd at the source code level. a user can conveniently select hot region through vmd and write the selection information into a pdb file. the rescaling keyword/parameter is written in namd tcl script interface that enables an on-the-fly simulation parameter change. our implementation of rest2 is within communication-enabled tcl script built on top of charm++, thus communication overhead of an exchange attempt is vanishingly small. such a generic implementation facilitates seamless cooperation between rest2 and other modules of namd to provide enhanced sampling for complex biomolecular simulations. three challenging applications including native rest2 simulation for peptide foldingunfolding transition, free energy perturbation/rest2 for absolute binding affinity of proteinligand complex and umbrella sampling/rest2 hamiltonian exchange for free energy landscape calculation were carried out on ibm blue gene/q supercomputer to demonstrate efficacy of rest2 based on the present implementation. program title: rest2-namd catalogue identifier: aexx_v1_0 program summary url:<url> program obtainable from: cpc program library, queens university, belfast, n. ireland licensing provisions: standard cpc licence, <url> no. of lines in distributed program, including test data, etc.: 240886 no. of bytes in distributed program, including test data, etc.: <phone> distribution format: tar.gz programming language: c/c++, tcl8.5. computer: not computer specific. operating system: any. has the code been vectorized or parallelized?: yes, mpi and/or pami parallelized depending on machine system software; 8192 cores used on ibm blue gene/q classification: 3. external routines: namd 2.10 (<url>) nature of problem: a generic implementation providing user-friendly api including input file preparation and performing replica exchange, and high frequency exchange attempt frequency with minimal communication overhead. solution method: the rescaling procedure of force field parameters controlling rest2 is implemented into namd at the source code level. a user can conveniently select hot region through vmd and write the selection information into a pdb file. the rescaling keyword/parameter is written in namd tcl script interface that enables an on-the-fly simulation parameter change. the implementation of rest2 is within communication-enabled tcl script built on top of charm++, thus communication overhead of an exchange attempt is vanishingly small. running time: 30 min60 min",
    "present_kp": [
      "rest2",
      "namd",
      "tcl"
    ],
    "absent_kp": [
      "free energy calculation"
    ]
  },
  {
    "title": "physically-based modeling, simulation and rendering of fire for computer animation.",
    "abstract": "we give an up-to-date survey on techniques and methods for fire simulation in computer graphics. physically-based method prevails over traditional non-physical methods for realistic visual effect. in this paper, we explore visual simulation of fire-related phenomena in terms of physically modeling, numerical simulation and visual rendering. firstly, we introduce a physical and chemical coupled mathematical model to explain fire behavior and motion. several assumptions and constrains are put forward to simplify their implementations in computer graphics. we then give an overview of present methods to solve the most complicated processes in numerical simulation: velocity advection and pressure projection. in addition, comparisons of these methods are also presented respectively. since fire is a participating medium as well as a visual radiator, we discuss techniques and problems of these issues as well. we conclude by addressing several open challenges and possible future research directions in fire simulation.",
    "present_kp": [
      "fire"
    ],
    "absent_kp": [
      "physically-based simulation",
      "navierstokes equations",
      "chemicalreaction",
      "blackbody radiation",
      "visual adaption"
    ]
  },
  {
    "title": "evaluating the success of an emergency response medical information system.",
    "abstract": "statpack is an information system used to aid in the diagnosis of pathogens in hospitals and state public health laboratories. statpack is used as a communication and telemedicine diagnosis tool during emergencies. this paper explores the success of this emergency response medical information system (ermis) using a well-known framework of information systems success developed by delone and mclean. using an online survey, the entire population of statpack users evaluated the success of the information system by considering system quality, information quality, system use, intention to use, user satisfaction, individual impact, and organizational impact. the results indicate that the overall quality of this ermis (i.e., system quality, information quality, and service quality) has a positive impact on both user satisfaction and intention to use the system. however, given the nature of ermis, overall quality does not necessarily predict use of the system. moreover, the user's satisfaction with the information system positively affected the intention to use the system. user satisfaction, intention to use, and system use had a positive influence on the system's impact on the individual. finally, the organizational impacts of the system were positively influenced by use of the system and the system's individual impact on the user. the results of the study demonstrate how to evaluate the success of an ermis as well as introduce potential changes in how one applies the delone and mclean success model in an emergency response medical information system context.",
    "present_kp": [
      "emergencies"
    ],
    "absent_kp": [
      "clinical laboratory information systems",
      "program evaluation"
    ]
  },
  {
    "title": "easy cases of probabilistic satisfiability.",
    "abstract": "the probabilistic satisfiability problem (psat) can be considered as a probabilistic counterpart of the classical sat problem. in a psat instance, each clause in a cnf formula is assigned a probability of being true; the problem consists in checking the consistency of the assigned probabilities. actually, psat turns out to be computationally much harder than sat, e.g., it remains difficult for some classes of formulas where sat can be solved in polynomial time. a column generation approach has been proposed in the literature, where the pricing sub-problem reduces to a weighted max-sat problem on the original formula. here we consider some easy cases of psat, where it is possible to give a compact representation of the set of consistent probability assignments. we follow two different approaches, based on two different representations of cnf formulas. first we consider a representation based on directed hypergraphs. by extending a well-known integer programming formulation of sat and max-sat, we solve the case in which the hypergraph does not contain cycles; a linear time algorithm is provided for this case. then we consider the co-occurrence graph associated with a formula. we provide a solution method for the case in which the co-occurrence graph is a partial 2-tree, and we show how to extend this result to partial k-trees with k >2.",
    "present_kp": [
      "probabilistic satisfiability",
      "cnf formulas",
      "directed hypergraphs",
      "partial k-trees"
    ],
    "absent_kp": [
      "balanced matrices"
    ]
  },
  {
    "title": "face description with local binary patterns: application to face recognition.",
    "abstract": "this paper presents a novel and efficient facial image representation based on local binary pattern (lbp) texture features. the face image is divided into several regions from which the lbp feature distributions are extracted and concatenated into an enhanced feature vector to be used as a face descriptor. the performance of the proposed method is assessed in the face recognition problem under different challenges. other applications and several extensions are also discussed.",
    "present_kp": [
      "facial image representation",
      "local binary pattern",
      "texture features"
    ],
    "absent_kp": [
      "component-based face recognition",
      "face misalignment"
    ]
  },
  {
    "title": "low-complexity adaptive decision-feedback equalization of mimo channels.",
    "abstract": "a new adaptive mimo channel equalizer is proposed based on adaptive generalized decision-feedback equalization and ordered-successive interference cancellation. the proposed equalizer comprises equal-length subequalizers, enabling any adaptive filtering algorithm to be employed for coefficient updates. a recently proposed computationally efficient recursive least squares algorithm based on dichotomous coordinate descents is utilized to solve the normal equations associated with the adaptation of the new equalizer. convergence of the proposed algorithm is examined analytically and simulations show that the proposed equalizer is superior to the previously proposed adaptive mimo channel equalizers by providing both enhanced bit error rate performance and reduced computational complexity. furthermore, the proposed algorithm exhibits stable numerical behavior and can deliver a trade-off between performance and complexity.",
    "present_kp": [
      "adaptive generalized decision-feedback equalization"
    ],
    "absent_kp": [
      "mimo systems",
      "ordered-successive interference cancelation",
      "rlsdcd algorithm",
      "v-blast"
    ]
  },
  {
    "title": "performance of hsr and qpp-based interleavers for turbo coding on power line communication systems.",
    "abstract": "in this paper, the performance of different type and length interleavers for turbo codes is analyzed in the context of power line communication systems. this system typically operates in very noisy environments; the noise, in this channel, is a combination of colored, narrow band and impulsive noises; it has also strong amplitude attenuations. the digital modulation frequently employed in power line communication to counteract the channels noise effects is the orthogonal frequency division multiplexing due to its high spectral efficiency and robustness in multipath fading environments; hence, it is also considered in our experimentation. we report the performance of turbo codes with the two types of interleavers: the high-spread random and the based quadratic permutation polynomial. the constituent codes are part of the 3gpp standard. finally, it is used a punctured matrix in order to achieve a coding rate of 1/2. the performance is evaluated in terms of bit error rate, through the way of simulations.",
    "present_kp": [
      "turbo codes",
      "interleavers",
      "punctured matrix"
    ],
    "absent_kp": [
      "plc",
      "ofdm"
    ]
  },
  {
    "title": "a numerical method for a model of two-phase flow in a coupled free flow and porous media system.",
    "abstract": "in this article, we study two-phase fluid flow in coupled free flow and porous media regions. the model consists of coupled cahnhilliard and navierstokes equations in the free fluid region and the two-phase darcy law in the porous medium region. we propose a robinrobin domain decomposition method for the coupled navierstokes and darcy system with the generalized beaversjosephsaffman condition on the interface between the free flow and the porous media regions. numerical examples are presented to illustrate the effectiveness of this method.",
    "present_kp": [
      "porous media",
      "robinrobin domain decomposition"
    ],
    "absent_kp": [
      "two phase flow",
      "darcy's law"
    ]
  },
  {
    "title": "circular-elm for the reduced-reference assessment of perceived image quality.",
    "abstract": "providing a satisfactory visual experience is one of the main goals for present-day electronic multimedia devices. all the enabling technologies for storage, transmission, compression, rendering should preserve, and possibly enhance, the quality of the video signal; to do so, quality control mechanisms are required. these mechanisms rely on systems that can assess the visual quality of the incoming signal consistently with human perception. computational intelligence (ci) paradigms represent a suitable technology to tackle this challenging problem. the present research introduces an augmented version of the basic extreme learning machine (elm), the circular-elm (c-elm), which proves effective in addressing the visual quality assessment problem. the c-elm model derives from the original circular backpropagation (cbp) architecture, in which the input vector of a conventional multilayer perceptron (mlp) is augmented by one additional dimension, the circular input; this paper shows that c-elm can actually benefit from the enhancement provided by the circular input without losing any of the fruitful properties that characterize the basic elm framework. in the proposed framework, c-elm handles the actual mapping of visual signals into quality scores, successfully reproducing perceptual mechanisms. its effectiveness is proved on recognized benchmarks and for four different types of distortions.",
    "present_kp": [
      "extreme learning machine",
      "circular backpropagation"
    ],
    "absent_kp": [
      "image quality assessment"
    ]
  },
  {
    "title": "distributed reinforcement learning control for batch sequencing and sizing in just-in-time manufacturing systems.",
    "abstract": "this paper presents an approach that is suitable for just-in-time (jit) production for multi-objective scheduling problem in dynamically changing shop floor environment. the proposed distributed learning and control (dlc) approach integrates part-driven distributed arrival time control (datc) and machine-driven distributed reinforcement learning based control. with datc, part controllers adjust their associated parts' arrival time to minimize due-date deviation. within the restricted pattern of arrivals, machine controllers are concurrently searching for optimal dispatching policies. the machine control problem is modeled as semi markov decision process ( smdp) and solved using q-learning. the dlc algorithms are evaluated using simulation for two types of manufacturing systems: family scheduling and dynamic batch sizing. results show that dlc algorithms achieve significant performance improvement over usual dispatching rules in complex real-time shop floor control problems for jit production.",
    "present_kp": [
      "scheduling"
    ],
    "absent_kp": [
      "machine learning",
      "just-in-time production"
    ]
  },
  {
    "title": "algorithm for calculating the noncentral chi-square distribution.",
    "abstract": "this correspondence presents a new algorithm for evaluating the noncentral chi-square distribution based on parl's method of neumann series expansion, it is applicable to both even and odd degrees of freedom, unlike most prior work, which has been directed at the even eases. convergence tests and procedures for detection of loss of precision are given. the overall method is extremely simple to program, accurate to many decimal places where applicable, and efficient over a wide range of parameters. the method is reliable provided the proper expansion is chosen based on the parameters.",
    "present_kp": [
      "algorithm",
      "chi-square distribution",
      "neumann series"
    ],
    "absent_kp": [
      "q-function"
    ]
  },
  {
    "title": "new hierarchical architecture for ubiquitous wireless sensing and access with improved coverage using cwdm-rof links.",
    "abstract": "a novel hierarchical architecture for hybrid wireless sensor and access networks has been proposed based on cost-effective radio-over-fiber (rof) links with the coarse wavelength-division-multiplexing (cwdm) technique. wireless fidelity (wifi) signals are distributed to the remote radio units transparently over optical fibers in a star-shaped network topology. the wireless access traffic together with the perceiving usage scenarios including video monitoring and temperature sensing has been successfully demonstrated in the hybrid ieee 802.11 and 802.15.4 networks. the transmission performance of the cwdm-rof links is evaluated in terms of the error-vector magnitude (evm) and data throughput for both uplinks and downlinks. the results show that the wifi signals are successfully delivered through the cwdm-rof links including a 4.5 km fiber and a 7 m wireless channel with a 3% evm penalty. this cwdm-rof technology can expand the application range of wireless sensor networks with the advantages of better capacity, larger coverage area, and lower investment on wired infrastructure.",
    "present_kp": [
      "coarse wavelength-division-multiplexing",
      "radio-over-fiber",
      "wireless sensor network"
    ],
    "absent_kp": [
      "wireless local area network"
    ]
  },
  {
    "title": "bifurcation analysis of an inductorless chaos generator using 1d piecewise smooth map.",
    "abstract": "in this work we investigate the dynamics of a one-dimensional piecewise smooth map, which represents the model of a chaos generator circuit. in a particular (symmetric) case analytic results can be given showing that the chaotic region is wide and robust. in the general model only the border collision bifurcation can be analytically determined. however, the dynamics behave in a similar way, leading effectively to robust chaos. on behalf of imacs.",
    "present_kp": [
      "chaos generator"
    ],
    "absent_kp": [
      "border-collision",
      "piecewise smooth continuous map"
    ]
  },
  {
    "title": "a hierarchical approach for energy-efficient scheduling of large workloads in multicore distributed systems.",
    "abstract": "definition of a novel multi-objective problem for energy-efficient scheduling in distributed data-centers. design of a hierarchical two-level scheduler that allows dividing the problem into simpler and smaller sub-problems. evaluation and comparison of 16 different variants of the scheduler on large sets of workflows. accurate solutions found by the best performing schedulers, achieving important improvements over classical strategies.",
    "present_kp": [
      "workflows",
      "multicore"
    ],
    "absent_kp": [
      "energy efficiency",
      "scheduling heuristics"
    ]
  },
  {
    "title": "collaborative feature-based design via operations with a fine-grain product database.",
    "abstract": "this paper reports a collaborative product design framework and a prototype system that supports multiple cad systems. the key contribution is an 'operation'-based, multi-application oriented, and near real-time collaboration mechanism which can significantly reduce collaboration communication load over the network. the mechanism is discussed and demonstrated with examples. to support the proposed multi-application collaboration system, a fine-grain feature-oriented product database is used. this research is a continued effort based on a shared common product modeling scheme, which covers fundamental issues of generic feature, feature level interoperability, engineering intent and operation definitions.",
    "present_kp": [
      "fine-grain product database"
    ],
    "absent_kp": [
      "collaborative engineering",
      "feature-based engineering"
    ]
  },
  {
    "title": "unified architecture for reed-solomon decoder combined with burst-error correction.",
    "abstract": "reed-solomon (rs) codes are widely used as forward correction codes (fec) in digital communication and storage systems. correcting random errors of rs codes have been extensively studied in both academia and industry. however, for burst-error correction, the research is still quite limited due to its ultra high computation complexity. in this brief, starting from a recent theoretical work, a low-complexity reformulated inversionless burst-error correcting (ribc) algorithm is developed for practical applications. then, based on the proposed algorithm, a unified vlsi architecture that is capable of correcting burst errors, as well as random errors and erasures, is firstly presented for multi-mode decoding requirements. this new architecture is denoted as unified hybrid decoding (uhd) architecture. it will be shown that, being the first rs decoder owning enhanced burst-error correcting capability, it can achieve significantly improved error correcting capability than traditional hard-decision decoding (hdd) design.",
    "present_kp": [
      "burst errors",
      "unified architecture",
      "vlsi"
    ],
    "absent_kp": [
      "reed-solomon  codes"
    ]
  },
  {
    "title": "implementation aspects of 3d lattice-bgk: boundaries, accuracy, and a new fast relaxation method.",
    "abstract": "in many realistic fluid-dynamical simulations the specification of the boundary conditions, the error sources, and the number of time steps to reach a steady state are important practical considerations. in this paper we study these issues in the case of the lattice-bgk model. the objective is to present a comprehensive overview of some pitfalls and shortcomings of the lattice-bgk method and to introduce some new ideas useful in practical simulations. we begin with an evaluation of the widely used bounce-back boundary condition in staircase geometries by simulating flow in an inclined tube. it is shown that the bounce-back scheme is first-order accurate in space when the location of the non-slip wall is assumed to be at the boundary nodes. moreover, for a specific inclination angle of 45 degrees, the scheme is found to be second-order accurate when the location of the non-slip velocity is fitted halfway between the last fluid nodes and the first solid nodes. the error as a function of the relaxation parameter is in that case qualitatively similar to that of flat walls. next, a comparison of simulations of fluid flow by means of pressure boundaries and by means of body force is presented. a good agreement between these two boundary conditions has been found in the creeping-flow regime. for higher reynolds numbers differences have been found that are probably caused by problems associated with the pressure boundaries. furthermore, two widely used 3d models, namely d(3)q(15) and d(3)q(19), are analysed. it is shown that the d(3)q(15) model may induce artificial checkerboard invariants due to the connectivity of the lattice. finally, a new iterative method, which significantly reduces the saturation time, is presented and validated on different benchmark problems.",
    "present_kp": [
      "lattice-bgk model",
      "accuracy",
      "boundary conditions"
    ],
    "absent_kp": []
  },
  {
    "title": "partition refinement of component interaction automata.",
    "abstract": "component interaction automata provide a fitting model to capture and analyze the temporal facets of hierarchically-structured component-oriented software systems. however, the rules governing composition typically suffer from combinatorial state explosion, an effect that can impede modeling languages, like component interaction automata, from being successful in real-world scenarios. we must, therefore, find some appropriate ways to counteract state explosion, one of which is partition refinement through bisimulation, in particular, weak bisimulation. while this technique can yield the desired state space reduction, it does not consider synchronization cliques, that is, groups of states that are interconnected solely by internal synchronization transitions. synchronization cliques give rise to action prefixes, local states that encapsulate preconditions for a component's ability to interact with the environment. furthermore, both the existence and the size of synchronization cliques can be used as an indicator for the success of partition refinement. in particular, the more frequent synchronization cliques are and the more states they entail, the more likely it is that partition refinement can reduce the state space. but, there may be other factors that impact the refinement process. for this reason, we study, in this paper, how partition refinement behaves under weak bisimulation, how synchronization cliques emerge when using weak bisimulation, how we make state space reduction through partition refinement aware of the existence of synchronization cliques, and what other attributes of component interaction automata specifications can provides us with additional cues to forecast the possible outcome of the partition refinement process.",
    "present_kp": [
      "partition refinement"
    ],
    "absent_kp": [
      "automata-based specification",
      "emergent properties"
    ]
  },
  {
    "title": "an on-line replication strategy to increase availability in data grids.",
    "abstract": "data is typically replicated in a data grid to improve the job response time and data availability. strategies for data replication in a data grid have previously been proposed, but they typically assume unlimited storage for replicas. in this paper, we address the system-wide data availability problem assuming limited replica storage. we describe two new metrics to evaluate the reliability of the system, and propose an on-line optimizer algorithm that can minimize the data missing rate (mindmr) in order to maximize the data availability. based on mindmr, we develop four optimizers associated with four different file access prediction functions. simulation results utilizing the optorsim show our mindmr strategies achieve better performance overall than other strategies in terms of the goal of data availability using the two new metrics.",
    "present_kp": [
      "data availability",
      "data grid",
      "data missing rate",
      "limited storage"
    ],
    "absent_kp": [
      "replica strategy"
    ]
  },
  {
    "title": "power assignment for k-connectivity in wireless ad hoc networks.",
    "abstract": "the problem min-power k-connectivity seeks a power assignment to the nodes in a given wireless ad hoc network such that the produced network topology is k-connected and the total power is the lowest. in this paper, we present several approximation algorithms for this problem. specifically, we propose a 3k-approximation algorithm for any k 3, a ( k + 12h( k))-approximation algorithm for k(2k - 1) n where n is the network size, a ( k + 2 [( k + 1)/ 2])-approximation algorithm for 2 k 7, a 6-approximation algorithm for k = 3, and a 9-approximation algorithm for k = 4.",
    "present_kp": [
      "k-connectivity",
      "power assignment"
    ],
    "absent_kp": [
      "wireless ad hoc sensor networks"
    ]
  },
  {
    "title": "comparison of anova-f and anom tests with regard to type i error rate and test power.",
    "abstract": "a monte carlo simulation was conducted to compare the type i error rate and test power of the analysis of means (anom) test to the one-way analysis of variance f-test (anova-f). simulation results showed that as long as the homogeneity of the variance assumption was satisfied, regardless of the shape of the distribution, number of group and the combination of observations, both anova-f and anom test have displayed similar type i error rates. however, both tests have been negatively affected from the heterogeneity of the variances. this case became more obvious when the variance ratios increased. the test power values of both tests changed with respect to the effect size (), variance ratio and sample size combinations. as long as the variances are homogeneous, anova-f and anom test have similar powers except unbalanced cases. under unbalanced conditions, the anova-f was observed to be powerful than the anom-test. on the other hand, an increase in total number of observations caused the power values of anova-f and anom test approach to each other. the relations between effect size () and the variance ratios affected the test power, especially when the sample sizes are not equal. as anova-f has become to be superior in some of the experimental conditions being considered, anom is superior in the others. however, generally, when the populations with large mean have larger variances as well, anom test has been seen to be superior. on the other hand, when the populations with large mean have small variances, generally, anova-f has observed to be superior. the situation became clearer when the number of the groups is 4 or 5.",
    "present_kp": [
      "analysis of variance",
      "anom",
      "type i error",
      "test power",
      "simulation"
    ],
    "absent_kp": []
  },
  {
    "title": "on mining multi-time-interval sequential patterns.",
    "abstract": "sequential pattern mining is essential in many applications, including computational biology, consumer behavior analysis, web log analysis, etc. although sequential patterns can tell us what items are frequently to be purchased together and in what order, they cannot provide information about the time span between items for decision support. previous studies dealing with this problem either set time constraints to restrict the patterns discovered or define time-intervals between two successive items to provide time information. accordingly, the first approach falls short in providing clear time-interval information while the second cannot discover time-interval information between two non-successive items in a sequential pattern. to provide more time-related knowledge, we define a new variant of time-interval sequential patterns, called multi-time-interval sequential patterns, which can reveal the time-intervals between all pairs of items in a pattern. accordingly, we develop two efficient algorithms, called the mi-apriori and mi-prefixspan algorithms, to solve this problem. the experimental results show that the mi-prefixspan algorithm is faster than the mi-apriori algorithm, but the mi-apriori algorithm has better scalability in long sequence data.",
    "present_kp": [
      "sequential pattern",
      "time-interval",
      "multi-time-interval"
    ],
    "absent_kp": [
      "data mining",
      "knowledge discovery"
    ]
  },
  {
    "title": "fpga implementation of a wavelet neural network with particle swarm optimization learning.",
    "abstract": "this paper introduces implementation of a wavelet neural network (wnn) with learning ability on field programmable gate array (fpga). a learning algorithm using gradient descent method is not easy to implement in an electronic circuit and has local minimum. a more suitable method is the particle swarm optimization (pso) that is a population-based optimization algorithm. the pso is similar to the ga, but it has no evolution operators such as crossover and mutation. in the approximation of a nonlinear activation function, we use a taylor series and a look-up table (lut) to achieve a more accurate approximation. the results of the two experiments demonstrate the successful hardware implementation of the wavelet neural networks with the pso algorithm using fpga. from the results of the experiment, it can be seen that the performance of the pso is better than that of the simultaneous perturbation algorithm at sufficient particle sizes.",
    "present_kp": [
      "wavelet neural networks ",
      "field programmable gate array ",
      "particle swarm optimization "
    ],
    "absent_kp": [
      "prediction",
      "identification"
    ]
  },
  {
    "title": "interface synthesis for heterogeneous multi-core systems from transaction level models.",
    "abstract": "this paper presents a tool for automatic synthesis of rtl interfaces for heterogeneous mpsoc from transaction level models (tlms). the tool captures the communication parameters in the platform and generates interface modules called universal bridges between buses in the design. the design and configuration of the bridges depend on several platform components including heterogeneity of the components, traffic on the bus, size of messages and so on. we define these parameters and show how the synthesizable rtl code for the bridge can be automatically derived based on these parameters. we use industrial strength design drivers such as an mp3 decoder to test our automatically generated bridges for a variety of platforms and compare them to manually designed bridges on different quality metrics. our experimental results show that performance of automatically generated bridges are within 5% of manual design for simple platforms but surpasses them for more complex platforms. the area and rtl code size is consistently better than manual design while giving 5 orders of improvement in development time.",
    "present_kp": [
      "interface synthesis",
      "universal bridge",
      "transaction level model",
      "performance",
      "design"
    ],
    "absent_kp": [
      "hw-sw co-design",
      "communication synthesis",
      "channel",
      "reliability",
      "experimentation",
      "verification"
    ]
  },
  {
    "title": "efficient implementations of construction heuristics for the rectilinear block packing problem.",
    "abstract": "the rectilinear block packing problem is a problem of packing a set of rectilinear blocks into a larger rectangular container, where a rectilinear block is a polygonal block whose interior angle is either 90 or 270. there exist many applications of this problem, such as vlsi design, timber/glass cutting, and newspaper layout. in this paper, we design efficient implementations of two construction heuristics for rectilinear block packing. the proposed algorithms are tested on a series of instances, which are generated from nine benchmark instances. the computational results show that the proposed algorithms are especially efficient for large instances with repeated shapes.",
    "present_kp": [
      "rectilinear blocks",
      "construction heuristics",
      "efficient implementation"
    ],
    "absent_kp": [
      "strip packing"
    ]
  },
  {
    "title": "the effect of finite lattice-size in lattice boltzmann model.",
    "abstract": "in this paper, numerical results on two-dimensional vapor-liquid equilibrium calculated by lattice boltzmann method have been presented. artefacts resulted by the finite lattice-size have been reviewed. a set of criteria for minimal lattice-size to avoid lattice artefacts is given.",
    "present_kp": [
      "lattice boltzmann method"
    ],
    "absent_kp": [
      "phase transition",
      "finite-size effect"
    ]
  },
  {
    "title": "estimating vignetting function from a single image for image authentication.",
    "abstract": "vignetting is the phenomenon of reduced brightness in an image at the peripheral region compared to the central region. as patterns of vignetting are characteristics of lens models, they can be used to authenticate digital images for forensic analysis. in this paper, we describe a new method for model based single image vignetting estimation and correction. we use the statistical properties of natural images in the discrete derivative domains and formulate the vignetting estimation problem as a maximum likelihood estimation. we further provide a simple and efficient procedure for better initialization of the numerical optimization. empirical evaluations of the proposed method using synthesized and real vignetted images show significant gain in both performance and running efficiency in correcting vignetting from digital images, and the estimated vignetting functions are shown to be effective in classifying different lens models.",
    "present_kp": [],
    "absent_kp": [
      "vignetting function estimation",
      "camera identification"
    ]
  },
  {
    "title": "effective utility mining with the measure of average utility.",
    "abstract": "frequent-itemset mining only considers the frequency of occurrence of the items but does not reflect any other factors, such as price or profit. utility mining is an extension of frequent-itemset mining, considering cost, profit or other measures from user preference. traditionally, the utility of an itemset is the summation of the utilities of the itemset in all the transactions regardless of its length. the average utility measure is thus adopted in this paper to reveal a better utility effect of combining several items than the original utility measure. it is defined as the total utility of an itemset divided by its number of items within it. the average-utility itemsets, as well as the original utility itemsets, does not have the \"downward-closure\" property. a mining algorithm is then proposed to efficiently find the high average-utility itemsets. it uses the summation of the maximal utility among the items in each transaction with the target itemset as the upper bound to overestimate the actual average utilities of the itemset and processes it in two phases. as expected, the mined high average-utility itemsets in the proposed way will be fewer than the high utility itemsets under the same threshold. the proposed approach can thus be executed under a larger threshold than the original, thus with a more significant and relevant criterion. experimental results also show the performance of the proposed algorithm.",
    "present_kp": [
      "utility mining",
      "average utility"
    ],
    "absent_kp": [
      "two-phase mining",
      "downward closure"
    ]
  },
  {
    "title": "surgical workflow management schemata for cataract procedures process model-based design and validation of workflow schemata.",
    "abstract": "objective: workflow guidance of surgical activities is a challenging task. because of variations in patient properties and applied surgical techniques, surgical processes have a high variability. the objective of this study was the design and implementation of a surgical workflow management system (swfms) that can provide a robust guidance for surgical activities. we investigated how many surgical process models are needed to develop a swfms that can guide cataract surgeries robustly. methods: we used 100 cases of cataract surgeries and acquired patient-individual surgical process models (ispms) from them. of these, randomized subsets ispms were selected as learning sets to create a generic surgical process model (gspm). these gspms were mapped onto workflow nets as workflow schemata to define the behavior of the swfms. finally, 10 ispms from the disjoint set were simulated to validate the workflow schema for the surgical processes. the measurement was the successful guidance of an ispm. results: we demonstrated that a swfms with a workflow schema that was generated from a subset of 10 ispms is sufficient to guide approximately 65% of all surgical processes in the total set, and that a subset of 50 ispms is sufficient to guide approx. 80% of all processes. conclusion: we designed a swfms that is able to guide surgical activities on a detailed level. the study demonstrated that the high inter-patient variability of surgical processes can be considered by our approach.",
    "present_kp": [
      "workflow",
      "surgical process model"
    ],
    "absent_kp": [
      "operative surgical procedures",
      "computer-assisted decision making",
      "computer-assisted surgery"
    ]
  },
  {
    "title": "temspol: a matlab thermal model for deep subduction zones including major phase transformations.",
    "abstract": "temspol is an open matlab code suitable for calculating temperature and lateral anomaly of density distributions in deep subduction zones, taking into account the olivine to spinel phase transformation in a self-consistent manner. the code solves, by means of a finite difference scheme, the heat transfer equation including adiabatic heating, radioactive heat generation, latent heat associated with phase changes and frictional heating. we show, with a few simulations, that temspol can be a useful tool for researchers studying seismic velocity, stress and seismicity distribution in deep subduction zones. deep earthquakes in subducting slabs are thought to be caused by shear instabilities associated with the olivine to spinel phase transition in metastable olivine wedges. we investigate the kinematic and thermal conditions of the subducting plate that lead to the formation of metastable olivine wedges. moreover, temspol calculates lateral anomalies of density within subducting slabs, which can be used to evaluate buoyancy forces that determine the dynamics of subduction and the stress distribution within the slab. we use temspol to evaluate the effects of heat sources such as shear heating and latent heat release, which are neglected in commonly used thermal models of subduction. we show that neglecting these heat sources can lead to significant overestimation of the depth reached by the metastable olivine wedge.",
    "present_kp": [
      "temperature",
      "subduction",
      "olivine",
      "spinel"
    ],
    "absent_kp": [
      "phase transitions",
      "density anomaly"
    ]
  },
  {
    "title": "online social advertising via influential endorsers.",
    "abstract": "in recent years, many web-based services such as facebook and myspace have been making great progress and creating new opportunities. because online advertising is the main business model for social networking sites, in this paper we propose a social endorser-based advertising system formulated on network influence and user preference analyses. by utilizing the social network and user preference analysis techniques, the theories of dynamic social influence and celebrity endorsement are realized in the proposed advertising approach. experiments show that our mechanism significantly improves advertising effectiveness and efficiency and outperforms other advertising approaches.",
    "present_kp": [
      "social network"
    ],
    "absent_kp": [
      "endorser advertising",
      "ewom",
      "influence model",
      "word of mouth"
    ]
  },
  {
    "title": "similarity measures between type-2 fuzzy sets.",
    "abstract": "in this paper, we give similarity measures between type-2 fuzzy sets and provide the axiom definition and properties of these measures. for practical use, we show how to compute the similarities between gaussian type-2 fuzzy sets. yang and shih's algorithm, a clustering method based on fuzzy relations by beginning with a similarity matrix, is applied to these gaussian type-2 fuzzy sets by beginning with these similarities. the clustering results are reasonable consisting of a hierarchical tree according to different levels.",
    "present_kp": [
      "gaussian type-2 fuzzy sets",
      "similarity measure",
      "type-2 fuzzy sets"
    ],
    "absent_kp": [
      "hausdorff distance"
    ]
  },
  {
    "title": "a mechanization of unity in pc-nqthm-92.",
    "abstract": "this paper presents in detail how the unity logic for reasoning about concurrent programs was formalized within the mechanized theorem prover pc-nqthm-92. most of unity's proof rules were formalized in the unquantified logic of nqthm, and the proof system has been used to mechanically verify several concurrent programs. the mechanized proof system is sound by construction, since unity's proof rules were proved about an operational semantics of concurrency, also presented here. skolem functions are used instead of quantifiers, and the paper describes how proof rules containing skolem function are used instead of unity's quantified proof rules when verifying concurrent programs. this formalization includes several natural extensions to unity, including nondeterministic statements. the paper concludes with a discussion of the cost and value of mechanization.",
    "present_kp": [
      "unity",
      "pc-nqthm",
      "concurrency"
    ],
    "absent_kp": [
      "theorem proving",
      "parallelism"
    ]
  },
  {
    "title": "reconciling while tolerating disagreement in collaborative data sharing.",
    "abstract": "in many data sharing settings, such as within the biological and biomedical communities, global data consistency is not always attainable: different sites' data may be dirty, uncertain, or even controversial. collaborators are willing to share their data, and in many cases they also want to selectively import data from others --- but must occasionally diverge when they disagree about uncertain or controversial facts or values. for this reason, traditional data sharing and data integration approaches are not applicable, since they require a globally consistent data instance. additionally, many of these approaches do not allow participants to make updates; if they do, concurrency control algorithms or inconsistency repair techniques must be used to ensure a consistent view of the data for all users.in this paper, we develop and present a fully decentralized model of collaborative data sharing , in which participants publish their data on an ad hoc basis and simultaneously reconcile updates with those published by others. individual updates are associated with provenance information, and each participant accepts only updates with a sufficient authority ranking, meaning that each participant may have a different (though conceptually overlapping) data instance. we define a consistency semantics for database instances under this model of disagreement, present algorithms that perform reconciliation for distributed clusters of participants, and demonstrate their ability to handle typical update and conflict loads in settings involving the sharing of curated data.",
    "present_kp": [
      "updates",
      "communities",
      "conflict",
      "paper",
      "model",
      "participant",
      "sharing",
      "data sharing",
      "author",
      "values",
      "users",
      "semantic",
      "mean",
      "data",
      "proven",
      "demonstrate",
      "publish",
      "consistency",
      "distributed",
      "algorithm",
      "collaborative data sharing",
      "database",
      "reconciliation",
      "data integration",
      "ranking",
      "global",
      "cluster"
    ],
    "absent_kp": [
      "association",
      "transactions",
      "informal",
      "collaboration",
      "peer-to-peer",
      "decentralization",
      "ad-hoc"
    ]
  },
  {
    "title": "application of evolutionary strategies for 3d graphical model categorization and retrieval.",
    "abstract": "in multimedia information processing, while the previous focus was on image/video retrieval, content-based categorization and retrieval of 3d computer graphics model is becoming increasingly important. this is due to the increased adoption of 3d graphics representations in multimedia applications and the resulting need for rapid virtual scene assembly from a repository of 3d models. motivated by these requirements, the main focus of this paper is on the content-based classification and retrieval of 3d computer graphics models based on a histogram feature representation, and the search for an adaptive transformation of this representation such that the resulting classification and retrieval accuracies are optimized. observing that a histogram is basically an approximation of the probability density function of an underlying random variable, and that a suitable transformation, when applied to the random variable, will allow the classifier to attain better accuracy based on this new representation, we propose an evolutionary optimization approach to search for this set of optimal transformations due to the large size of the search space. in particular, we consider the special class of transformations that take the form of a piecewise continuous mapping. in this case, the transformed variable is a mixed random variable, with both discrete and continuous components, which provides added flexibility for modeling a number of more diverse random variable types. with a suitably defined fitness function for evolutionary strategies (es) that measures the capability of the transformed histogram representation to induce the correct class structure, our proposed approach is capable of improving the head model classification performance, which in turn allows, in the case of content-based retrieval, the correct preassignment of a query object to its correct class for more efficient search, even in those cases where the query is ambiguous and difficult to characterize.",
    "present_kp": [
      "evolutionary strategies"
    ],
    "absent_kp": [
      "pattern classification",
      "3d head model",
      "evolutionary computation",
      "multiple classifier system"
    ]
  },
  {
    "title": "face recognition based on a novel linear discriminant criterion.",
    "abstract": "as an effective technique for feature extraction and pattern classification fisher linear discriminant (fld) has been successfully applied in many fields. however, for a task with very high-dimensional data such as face images, conventional fld technique encounters a fundamental difficulty caused by singular within-class scatter matrix. to avoid the trouble, many improvements on the feature extraction aspect of fld have been proposed. in contrast, studies on the pattern classification aspect of fld are quiet few. in this paper, we will focus our attention on the possible improvement on the pattern classification aspect of fld by presenting a novel linear discriminant criterion called maximum scatter difference (msd). theoretical analysis demonstrates that msd criterion is a generalization of fisher discriminant criterion, and is the asymptotic form of discriminant criterion: large margin linear projection. the performance of msd classifier is tested in face recognition. experiments performed on the orl, yale, feret and ar databases show that msd classifier can compete with top-performance linear classifiers such as linear support vector machines, and is better than or equivalent to combinations of well known facial feature extraction methods, such as eigenfaces, fisherfaces, orthogonal complementary space, nullspace, direct linear discriminant analysis, and the nearest neighbor classifier.",
    "present_kp": [
      "fisher linear discriminant",
      "pattern classification",
      "face recognition"
    ],
    "absent_kp": [
      "small sample size problem",
      "multi-objective programming",
      "binary linear classifier"
    ]
  },
  {
    "title": "multi-channel sampling on shift-invariant spaces with frame generators.",
    "abstract": "let phi be a continuous function in l-2(r) such that the sequence {phi(t - n)}(n is an element of z) is a frame sequence in l-2(r) and assume that the shift-invariant space v (phi) generated by phi has a multi-banded spectrum sigma(v). the main aim in this paper is to derive a multi-channel sampling theory for the shift-invariant space v (phi). by using a type of fourier duality between the spaces v (phi) and l-2[ 0, 2 pi] we find necessary and sufficient conditions allowing us to obtain stable multi-channel sampling expansions in v (phi).",
    "present_kp": [
      "shift-invariant spaces",
      "multi-channel sampling"
    ],
    "absent_kp": [
      "frames"
    ]
  },
  {
    "title": "focus of b-to-b e-commerce initiatives and related benefits in manufacturing small- and medium-sized enterprises.",
    "abstract": "empirical research into business-to-business e-commerce issues involving manufacturing small- and medium-sized enterprises (smes) is still embryonic. in an attempt to partially fill this gap, this paper presents empirical data from an electronic survey conducted among 96 manufacturing smes to investigate e-commerce initiatives and their related benefits. e-commerce initiatives are assessed using a set of 36 business processes that can be conducted electronically. these processes were classified according to their focus: customer (downstream), supplier (upstream) or in-house. the research findings point to four main profiles of manufacturing smes with different e-commerce focuses. the first group seems to lack any focus or may still be exploring e-commerce opportunities. the second and third groups are supplier- and customer-focused, respectively. the fourth group consists of the more involved smes that have leveraged their e-commerce initiatives with both their customers and their suppliers. results also suggest the existence of a close alignment between e-commerce focus and related benefits.",
    "present_kp": [
      "b-to-b e-commerce",
      "business processes",
      "smes"
    ],
    "absent_kp": []
  },
  {
    "title": "incomplete information-based decentralized cooperative control strategy for distributed energy resources of vsi-based microgrids.",
    "abstract": "this paper presents an effective method to control distributed energy resources (ders) installed in a microgrid (mg) to guarantee its stability after islanding occurrence. considering voltage and frequency variations after islanding occurrence and based on stability criteria, mg pre-islanding conditions are divided into secure and insecure classes. it is shown that insecure mg can become secure, if appropriate preventive control is applied on the ders in different operating conditions of the mg. to select the most important variables of mg, which can estimate proper values of output power set points of ders, a feature selection procedure known as symmetrical uncertainty is used in this paper. among all the mg variables, critical ones are selected to calculate the appropriate output power of different ders for different conditions of the mg. the values of selected features are transmitted by the communication system to the control unit installed on each der to control its output power set point. in order to decrease the communication system cost, previous researchers have used local variables to control the set point of different ders. this approach decreases the accuracy of the controller because the controller uses incomplete information. in this paper, multi-objective approach is used in order to decrease the cost of the communication system, while keeping the accuracy of the preventive control strategy in an allowable margin. the results demonstrate the effectiveness of the proposed method in comparison with other methods.",
    "present_kp": [
      "incomplete information",
      "cooperative control",
      "distributed energy resources"
    ],
    "absent_kp": [
      "decentralized control",
      "ann-based control"
    ]
  },
  {
    "title": "modeling and reasoning with paraconsistent rough sets.",
    "abstract": "we present a language for defining paraconsistent rough sets and reasoning about them. our framework relates and brings together two major fields: rough sets and paraconsistent logic programming . to model inconsistent and incomplete information we use a four-valued logic. the language discussed in this paper is based on ideas of our previous work developing a four-valued framework for rough sets. in this approach membership function, set containment and set operations are four-valued, where logical values are t (true), f (false), i (inconsistent) and u (unknown). we investigate properties of paraconsistent rough sets as well as develop a paraconsistent rule language, providing basic computational machinery for our approach.",
    "present_kp": [
      "rough sets"
    ],
    "absent_kp": [
      "approximate reasoning",
      "paraconsistent reasoning",
      "four-valued logics"
    ]
  },
  {
    "title": "a dynamical tikhonov regularization for solving ill-posed linear algebraic systems.",
    "abstract": "the tikhonov method is a famous technique for regularizing ill-posed linear problems, wherein a regularization parameter needs to be determined. this article, based on an invariant-manifold method, presents an adaptive tikhonov method to solve ill-posed linear algebraic problems. the new method consists in building a numerical minimizing vector sequence that remains on an invariant manifold, and then the tikhonov parameter can be optimally computed at each iteration by minimizing a proper merit function. in the optimal vector method (ovm) three concepts of optimal vector, slow manifold and hopf bifurcation are introduced. numerical illustrations on well known ill-posed linear problems point out the computational efficiency and accuracy of the present ovm as compared with classical ones.",
    "present_kp": [
      "tikhonov regularization",
      "adaptive tikhonov method",
      "dynamical tikhonov regularization",
      "optimal vector method ",
      ""
    ],
    "absent_kp": [
      "ill-posed linear system",
      "steepest descent method ",
      "conjugate gradient method ",
      "barzilai-borwein method "
    ]
  },
  {
    "title": "mapping transit-based access: integrating gis, routes and schedules.",
    "abstract": "accessibility is a concept that is not entirely easy to define. gould (1969) once stated that it is a 'slippery notion ... one of those common terms that everyone uses until faced with the problem of defining and measuring it'. considerable research over the last 40 years has been devoted to defining and measuring accessibility, ranging from access to jobs within an hour's travel time to the ease at which given places can be reached. this article is concerned with the measurement of access provided by transit. it includes a review of past work on measuring accessibility in general and with respect to transit services in particular. from this overview of the literature, it can be seen that current methods fall short in measuring transit service access in several meaningful aspects. based on this review and critique, we propose new refinements that can be used to help overcome some of these shortcomings. as a part of this, we define an extended gis data structure to handle temporal elements of transit service. to demonstrate the value of these new measures, examples are presented with respect to mapping accessibility of transit services in santa barbara, california. finally, we show how these measures can be used to develop a framework for supporting transit service analysis and planning.",
    "present_kp": [
      "accessibility"
    ],
    "absent_kp": [
      "public transit",
      "schedule and route information",
      "geographic information systems",
      "urban applications"
    ]
  },
  {
    "title": "the complexity of compressing subsegments of images described by finite automata.",
    "abstract": "we investigate how the compression size of the compressed version of a two-dimensional image changes when we cut off a part of it, e.g. extract a photo of one person from a photo of a group of people, when compression is considered in terms of finite automata. denote by c(t) the compression size of a square image t in terms of deterministic automata, it is the smallest size of a deterministic acyclic automaton a describing t. the corresponding alphabet of a has only four letters, corresponding to four quadrants. we consider an independent useful combinatorial interpretation of c(t) in terms of regular subsquares of t. denote by ?(n) the largest compression size c(r) of a square subsegment r of the image t such that c(t)=n. we show that there is a constant c>0 such that: we also show how to construct efficiently (in linear time w.r.t. the total size of the input and the produced output) the compressed representation of subsegments given the compressed representation of the whole image.",
    "present_kp": [
      "finite automata",
      "complexity"
    ],
    "absent_kp": [
      "square images"
    ]
  },
  {
    "title": "accomplishing universal access through system reachabilitya management perspective.",
    "abstract": "the aim of this paper is to describe the need of a method by which we can estimate the return on accessibility investments in information technology (it) systems. this paper reveals some of the reasons why accessibility still is a secondhand criterion when designing digital services. it also describes the authors experiences regarding the concept of accessibility and how it must develop in order to obtain the status of a basic business criterion for the benefit of disabled people who are currently excluded from public services and labour markets. the paper also questions the need of a separate accessibility standard. additionally, we discuss some of the hindering in the market and limiting perspectives that are blocking further development. one of the problems in the market seems to be that accessibility as a concept has been more of an issue about creating equal opportunities and therefore probably does not have the quality of a business criterion. in order to bridge that gap, we argue for replacing accessibility with reachability, which is a concept based on a measure used by media when estimating the reached percentage of a population or target group.",
    "present_kp": [
      "accessibility",
      "management"
    ],
    "absent_kp": [
      "guidelines",
      "standards",
      "usability"
    ]
  },
  {
    "title": "pothmf: a program for computing potential curves and matrix elements of the coupled adiabatic radial equations for a hydrogen-like atom in a homogeneous magnetic field.",
    "abstract": "a fortran 77 program is presented which calculates with the relative machine precision potential curves and matrix elements of the coupled adiabatic radial equations for a hydrogen-like atom in a homogeneous magnetic field. the potential curves are eigenvalues corresponding to the angular oblate spheroidal functions that compose adiabatic basis which depends on the radial variable as a parameter. the matrix elements of radial coupling are integrals in angular variables of the following two types: product of angular functions and the first derivative of angular functions in parameter, and product of the first derivatives of angular functions in parameter, respectively. the program calculates also the angular part of the dipole transition matrix elements (in the length form) expressed as integrals in angular variables involving product of a dipole operator and angular functions. moreover, the program calculates asymptotic regular and irregular matrix solutions of the coupled adiabatic radial equations at the end of interval in radial variable needed for solving a multi-channel scattering problem by the generalized r-matrix method. potential curves and radial matrix elements computed by the pothmf program can be used for solving the bound state and multichannel scattering problems. as a test desk, the program is applied to the calculation of the energy values, a short-range reaction matrix and corresponding wave functions with the help of the kantbp program. benchmark calculations for the known photoionization cross-sections are presented.",
    "present_kp": [],
    "absent_kp": [
      "eigenvalue and multi-channel scattering problems",
      "kantorovich method",
      "finite element method",
      "r-matrix calculation",
      "multi-channel adiabatic approximation",
      "ordinary differential equations",
      "high-order accuracy approximation"
    ]
  },
  {
    "title": "completely lazy learning.",
    "abstract": "local classifiers are sometimes called lazy learners because they do not train a classifier until presented with a test sample. however, such methods are generally not completely lazy because the neighborhood size k (or other locality parameter) is usually chosen by cross validation on the training set, which can require significant preprocessing and risks overfitting. we propose a simple alternative to cross validation of the neighborhood size that requires no preprocessing: instead of committing to one neighborhood size, average the discriminants for multiple neighborhoods. we show that this forms an expected estimated posterior that minimizes the expected bregman loss with respect to the uncertainty about the neighborhood choice. we analyze this approach for six standard and state-of-the-art local classifiers, including discriminative adaptive metric knn (dann), a local support vector machine (svm-knn), hyperplane distance nearest neighbor (hknn), and a new local bayesian quadratic discriminant analysis (local bda). the empirical effectiveness of this technique versus cross validation is confirmed with experiments on seven benchmark data sets, showing that similar classification performance can be attained without any training.",
    "present_kp": [
      "lazy learning",
      "cross validation",
      "quadratic discriminant analysis"
    ],
    "absent_kp": [
      "bayesian estimation",
      "local learning"
    ]
  },
  {
    "title": "a tabu search approach for scheduling hazmat shipments.",
    "abstract": "vehicle routing and scheduling are two main issues in the hazardous material (hazmat) transportation problem. in this paper, we study the problem of managing a set of hazmat transportation requests in terms of hazmat shipment route selection and actual departure time definition. for each hazmat shipment, a set of minimum and equitable risk alternative routes from origin to destination points and a preferred departure time are given. the aim is to assign a route to each hazmat shipment and schedule these shipments on the assigned routes in order to minimize the total shipment delay, while equitably spreading the risk spatially and preventing the risk induced by vehicles traveling too close to each other. we model this hazmat shipment scheduling problem as a job-shop scheduling problem with alternative routes. no-wait constraints arise in the scheduling model as well, since, supposing that no safe area is available, when a hazmat vehicle starts traveling from the given origin it cannot stop until it arrives at the given destination. a tabu search algorithm is proposed for the problem, which is experimentally evaluated on a set of realistic test problems over a regional area, evaluating the provided solutions also with respect to the total route risk and length.",
    "present_kp": [
      "job-shop scheduling",
      "tabu search algorithm"
    ],
    "absent_kp": [
      "hazmat transportation problem"
    ]
  },
  {
    "title": "a comparative study of direct-forcing immersed boundary-lattice boltzmann methods for stationary complex boundaries.",
    "abstract": "in this study, we assess several interface schemes for stationary complex boundary flows under the direct-forcing immersed boundary-lattice boltzmann methods (ib-lbm) based on a split-forcing lattice boltzmann equation (lbe). our strategy is to couple various interface schemes, which were adopted in the previous direct-forcing immersed boundary methods (ibm), with the split-forcing lbe, which enables us to directly use the direct-forcing concept in the lattice boltzmann calculation algorithm with a second-order accuracy without involving the navier-stokes equation. in this study, we investigate not only common diffuse interface schemes but also a sharp interface scheme. for the diffuse interface scheme, we consider explicit and implicit interface schemes. in the calculation of velocity interpolation and force distribution, we use the 2- and 4-point discrete delta functions, which give the second-order approximation. for the sharp interface scheme, we deal with the exterior sharp interface scheme, where we impose the force density on exterior (solid) nodes nearest to the boundary. all tested schemes show a second-order overall accuracy when the simulation results of the taylor-green decaying vortex are compared with the analytical solutions. it is also confirmed that for stationary complex boundary flows, the sharper the interface scheme, the more accurate the results are. in the simulation of flows past a circular cylinder, the results from each interface scheme are comparable to those from other corresponding numerical schemes.",
    "present_kp": [
      "immersed boundary-lattice boltzmann method",
      "split-forcing lattice boltzmann equation",
      "interface scheme",
      "stationary complex boundary",
      "taylor-green decaying vortex"
    ],
    "absent_kp": [
      "direct-forcing method",
      "flow past a circular cylinder"
    ]
  },
  {
    "title": "precise euclidean distance transforms in 3d from voxel coverage representation.",
    "abstract": "we propose a method for computing euclidean distance transform (edt) in 3d images. the method utilizes voxel coverage information to increase precision of edt. the method can be used with any vector propagation based edt in 3d. synthetic tests confirm significant improvement in achieved precision. both the related binary and the existing coverage based methods are outperformed.",
    "present_kp": [
      "distance transform",
      "precision",
      "coverage representation"
    ],
    "absent_kp": [
      "vector propagation dt algorithm",
      "sub-voxel accuracy"
    ]
  },
  {
    "title": "can we trust digital image forensics.",
    "abstract": "compared to the prominent role digital images play in nowadays multimedia society, research in the field of image authenticity is still in its infancy. only recently, research on digital image forensics has gained attention by addressing tamper detection and image source identification. however, most publications in this emerging field still lack rigorous discussions of robustness against strategic counterfeiters, who anticipate the existence of forensic techniques. as a result, the question of trustworthiness of digital image forensics arises. this work will take a closer look at two state-of-the-art forensic methods and proposes two counter-techniques; one to perform resampling operations undetectably and another one to forge traces of image origin. implications for future image forensic systems will be discussed.",
    "present_kp": [
      "digital image forensics",
      "image source identification",
      "tamper detection"
    ],
    "absent_kp": [
      "tamper hiding"
    ]
  },
  {
    "title": "max-optimal and sum-optimal labelings of graphs.",
    "abstract": "given a graph g, a function f: v (g) -> {1, 2, ..., k} is a k-ranking of g if f (u) = f (v) implies that every u - v path contains a vertex w such that f (w) > f (u). a k-ranking is minimal if the reduction of any label greater than 1 violates the described ranking property. we consider two norms for minimal rankings. the max-optimal norm parallel to f(g)parallel to(infinity) is the smallest k for which g has a minimal k-ranking. this value is also referred to as the rank number chi(r)(g). in this paper we introduce the sum-optimal norm parallel to f(g)parallel to(1) which is the minimum sum of all labels over all minimal rankings. we investigate similarities and differences between the two norms. in particular we show rankings for paths and cycles that are sum-optimal are also max-optimal.",
    "present_kp": [
      "rank number"
    ],
    "absent_kp": [
      "graph algorithms",
      "vertex coloring"
    ]
  },
  {
    "title": "2-((v,k,1)) designs with a point-primitive rank 3 automorphism group of affine type.",
    "abstract": "2-((v,k,1)) designs with a point-primitive rank 3 automorphism group of affine type are investigated and several new examples are provided.",
    "present_kp": [
      ""
    ],
    "absent_kp": [
      "2-) designs",
      "rank 3 group",
      "orbit",
      "segre variety"
    ]
  },
  {
    "title": "increase in the releasable pool of synaptic vesicles underlies facilitation.",
    "abstract": "facilitation is the ability of presynaptic terminals to release neurotransmitter more efficiently following repetitive stimulation. we demonstrated that facilitation can be explained by ca2+-dependent vesicles priming and the increase in the number of synaptic vesicles activated for release. employing the model with two ca2+ sensors, we computed ca2+ concentration at the sites of priming and release, the size of the releasable pool of vesicles, and the rate of transmitter release during repetitive nerve stimulation. the calculated rates of vesicle release and the increase in the releasable pool during facilitation were in agreement with the results of electrophysiology experiments.",
    "present_kp": [],
    "absent_kp": [
      "lobster neuromuscular junction",
      "synaptic plasticity",
      "neurosecretion",
      "calcium"
    ]
  },
  {
    "title": "viscoelastic fracture of multiple cracks in functionally graded materials.",
    "abstract": "in this paper, the viscoelastic fracture of multiple cracks in a functionally graded strip is studied. the solution of linear elastic crack tip field is investigated at first, using the finite element method. both applied stress load and applied strain load are taken into account. the effects of the crack length, crack spacing, material gradient index and the loading condition on the crack tip field intensity factor are plotted and discussed. according to the correspondence principle, the viscoelastic crack tip field under applied strain is obtained from the linear elastic results. variation of stress intensity factor of the viscoelastic functionally graded strip is analyzed. some useful information for the design of functionally graded materials is provided.",
    "present_kp": [
      "stress intensity factor",
      "correspondence principle"
    ],
    "absent_kp": [
      "linear elasticity",
      "viscoelasticity"
    ]
  },
  {
    "title": "power distribution system optimization by an algorithm for capacitated steiner tree problems with complex-flows and arbitrary cost functions.",
    "abstract": "an algorithm called genetic shortest-path algorithm is presented to solve capacitated minimal steiner tree problems in graphs with complex flows and arbitrary arc cost functions, but without negative cycles. voltage constraint can also been taken into consideration by the algorithm. hence, it can solve various power distribution system optimization problems with detailed mathematical models. in the proposed algorithm, a local optimization method based on shortest-path algorithm and heuristics is used to find the local optimums, in which the minimum cost objective and all constraints are considered and the specialties of the problems are made good use of. genetic operations are only used to search the global optimum from the local optimums. therefore, this algorithm overcomes the disadvantage of general genetic algorithm in local searching. an example for distribution system planning problem with large scale is given to demonstrate the power of the algorithm.",
    "present_kp": [
      "steiner tree problem",
      "genetic algorithm"
    ],
    "absent_kp": [
      "power distribution system planning",
      "power distribution system reconfiguration"
    ]
  },
  {
    "title": "qmbr(i): inverse quantization of minimum bounding rectangles for spatial data compression.",
    "abstract": "in this paper, we propose qmbr(i), the inverse representation of the quantized minimum bounding rectangles (mbrs) scheme, which compresses a minimum bounding rectangle key into one byte for spatial-data compression. qmbr(i) is a novel spatial-data compression scheme that is based on inverse quantization and overcomes the shortcomings of conventional relative coordination or quantization schemes. if a spatial data is far from the starting point of the search region, the relative coordination scheme does not guarantee compression. in a quantization scheme, since the mbrs are expanded, the overlapping of mbrs is increased and the search performance is reduced. the proposed scheme overcomes these shortcomings, and simulation results suggest that it performs better than other schemes.",
    "present_kp": [
      "spatial data",
      "spatial-data compression",
      "mbr",
      "qmbr"
    ],
    "absent_kp": [
      "rmbr",
      "hmbr"
    ]
  },
  {
    "title": "two-dimensional model of base force element method (bfem) on complementary energy principle for geometrically nonlinear problems.",
    "abstract": "based on the concept of the base forces by gao, a new finite element methodthe base force element method (bfem) on complementary energy principle for two-dimensional geometrically nonlinear problems is presented using arbitrary meshes. an arbitrary convex polygonal element model of the bfem for geometrically nonlinear problem is derived by assuming that the stress is uniformly distributed on each edges of a plane element. the explicit formulations of the control equations for the bfem are derived using the modified complementary energy principle. the bfem is naturally universal for small displacement and large displacement problems. a number of example problems are solved using the bfem and the results are compared with corresponding analytical solutions. a good agreement of the results using the arbitrary convex polygonal element model of bfem in the large displacement and large rotation calculations, are observed.",
    "present_kp": [
      "finite element",
      "complementary energy",
      "geometrically nonlinear",
      "two-dimensional",
      "base forces"
    ],
    "absent_kp": []
  },
  {
    "title": "theoretical study of [xn5](-) (x=o, s, se, te) systems.",
    "abstract": "a series of [xn5](-) (x=o, s, se, te) compounds has been examined with ab initio and density functional theory (dft) methods. the five-membered nitrogen ring series of structures are global minima and may exist or be characterized due to their significant dissociation barriers (29.7-32.7 kcal mol(-1)). nucleus-independent chemical shifts (nics) criteria and the presence of (4n+2) pi-electrons confirmed that the five-membered nitrogen ring in their structures exhibits characteristics of aromaticity. thus, the strong stability of the five-membered nitrogen ring structures may be attributed partially to their aromaticity.",
    "present_kp": [
      "ab initio",
      "[xn5]",
      "aromaticity",
      "nics"
    ],
    "absent_kp": [
      "hedms"
    ]
  },
  {
    "title": "incremental fault diagnosis.",
    "abstract": "fault diagnosis is important in improving the circuit-design process and the manufacturing yield. diagnosis of today's complex defects is a challenging problem due to the explosion of the underlying solution space with the increasing number of fault locations and fault models. to tackle this complexity, an incremental diagnosis method is proposed. this method captures faulty lines one at a time using the novel linear-time single-fault diagnosis algorithms. to capture complex fault effects, a model-free incremental diagnosis algorithm is outlined, which alleviates the need for an explicit fault model. to demonstrate the applicability of the proposed method, experiments on multiple stuck-at faults, open-interconnects and bridging faults are performed. extensive results on combinational and full-scan sequential benchmark circuits confirm its resolution and performance.",
    "present_kp": [
      "fault diagnosis",
      "open-interconnect"
    ],
    "absent_kp": [
      "circuit simulation",
      "very large scale integration "
    ]
  },
  {
    "title": "dim, a portable, light weight package for information publishing, data transfer and inter-process communication.",
    "abstract": "the real-time systems of hep experiments are presently highly distributed, possibly on heterogeneous cpus. in many applications, there is an important need to make information available to a large number of other processes in a transparent way. for this purpose the \"rpc-like\" systems are not suitable, since most of them rely on polling from the client and one-to-one connections. dim is a very powerful alternative to those systems. it provides a named space for processes to publish information (publishers) and a very simple api for processes willing to use this information (subscribers). it fully handles error recovery at the publisher and subscriber level, without additional software in the application, dim is available on a large variety of platforms and operating systems with c and c++ bindings. it is presently used in several hep experiments, while it was developed in the delphi experiment and is maintained at cern. we shall present its capabilities and examples of its use in hep experiments in domains ranging from simple data publishing to event transfer, process control or communication layer for an experiment control package (smi++). we shall also present prospectives for using it as communications layer for future experiment's control systems.",
    "present_kp": [],
    "absent_kp": [
      "asynchronous communications",
      "heterogeneous distributed systems"
    ]
  },
  {
    "title": "roswel workflow language: a declarative, resource-oriented approach.",
    "abstract": "well defined business processes are a crucial success factor for deploying soa/soku architectures. in this paper, the declarative business process description language-roswel-which supports applications compatible with roa, is discussed. roswel provides a declarative, reliable and semi-automatic composition of restful web services, enriched by the knowledge representation. the paper discusses benefits of roswel, and presents an example of a simple workow that captures essential roswel features.",
    "present_kp": [
      "soku",
      "rest",
      "business process"
    ],
    "absent_kp": [
      "declarative workflow language"
    ]
  },
  {
    "title": "the (1+3)-dimensional burgers equation and its comparative solutions.",
    "abstract": "in this paper, we will carry out an analytic comparative study between the adomian decomposition method and the differential transformation method. this is achieved by handling the (1 + 3)-dimensional burgers equation. two numerical simulations have also been carried out to validate and demonstrate efficiency of the two methods.",
    "present_kp": [
      "adomian decomposition method",
      "differential transformation method"
    ],
    "absent_kp": [
      "the -dimensional burgers equation"
    ]
  },
  {
    "title": "modeling the development of goal-specificity in mirror neurons.",
    "abstract": "neurophysiological studies have shown that parietal mirror neurons encode not only actions but also the goal of these actions. although some mirror neurons will fire whenever a certain action is perceived (goal-independently), most will only fire if the motion is perceived as part of an action with a specific goal. this result is important for the action-understanding hypothesis as it provides a potential neurological basis for such a cognitive ability. it is also relevant for the design of artificial cognitive systems, in particular robotic systems that rely on computational models of the mirror system in their interaction with other agents. yet, to date, no computational model has explicitly addressed the mechanisms that give rise to both goal-specific and goal-independent parietal mirror neurons. in the present paper, we present a computational model based on a self-organizing map, which receives artificial inputs representing information about both the observed or executed actions and the context in which they were executed. we show that the map develops a biologically plausible organization in which goal-specific mirror neurons emerge. we further show that the fundamental cause for both the appearance and the number of goal-specific neurons can be found in geometric relationships between the different inputs to the map. the results are important to the action-understanding hypothesis as they provide a mechanism for the emergence of goal-specific parietal mirror neurons and lead to a number of predictions: (1) learning of new goals may mostly reassign existing goal-specific neurons rather than recruit new ones; (2) input differences between executed and observed actions can explain observed corresponding differences in the number of goal-specific neurons; and (3) the percentage of goal-specific neurons may differ between motion primitives.",
    "present_kp": [
      "mirror neurons",
      "action-understanding hypothesis",
      "computational model",
      "self-organizing map"
    ],
    "absent_kp": [
      "neural activation patterns"
    ]
  },
  {
    "title": "a framework for designing and implementing the ada standard container library.",
    "abstract": "an open issue of the ada language is the definition of a standard container library. containers in this library (e.g., sets, maps and lists) shall offer some core functionalities that characterise their behaviour (i.e., different strategies for managing the elements stored therein) as well as other general functionalities. among these general functionalities, we are interested in alternative ways for accessing the containers, namely direct access by position and traversals using iterators. in this paper, we present the shortcut-based framework (sbf), a framework aimed at providing suitable, uniform, accurate and secure access by position and iterators, while keeping other nice properties such as comprehensibility and changeability. the sbf should be considered as a baseline upon which the ada standard container library can be built. we assess the feasibility of our proposal defining a quality model for container libraries and evaluating the sbf using some metrics defined with the goal-question-metric approach.",
    "present_kp": [
      "iterators",
      "access by position",
      "container libraries"
    ],
    "absent_kp": [
      "quality models"
    ]
  },
  {
    "title": "stability of block lu factorization for block tridiagonal matrices.",
    "abstract": "it is showed that if a is i-block diagonally dominant (ii-block diagonally dominant), then the reduced matrix s preserves the same property. we also give a sufficient condition for the reduced matrix s also to be a block h-matrix when a is a block h-matrix, and some properties on the comparison matrices mu(i)(a(k)), mu(ii)(a(k)), mu(i)(l), mu(i)(u) are obtained. finally, error analysis of block lu factroization for block tridiagonal matrix is presented.",
    "present_kp": [
      "block lu factorization",
      "block tridiagonal matrix",
      "stability"
    ],
    "absent_kp": [
      "block h-matrices",
      "i-block diagonally dominant matrices",
      "ii-block diagonally dominant matrices"
    ]
  },
  {
    "title": "russian-dutch double-degree masters programme in computational science in the age of global education.",
    "abstract": "we present a new double-degree graduate (masters) programme in computational science launched in 2012 by the itmo university, russia and university of amsterdam, the netherlands. we discuss the global aspects of integration of different educational systems and list some funding opportunities. we describe our double-degree program curriculum, suggest the timeline of enrollment and studies, and give some examples of student research topics. finally, we discuss the issues of joint programs with russia and suggest possible solutions, analyze the results of the first three student intakes and reflect on the lessons learnt, and share our thoughts and experiences that could be of interest to the international community expanding the educational markets to the vast countries like russia, china or india. the paper is written for education professionals and contains useful information for potential students.",
    "present_kp": [
      "computational science",
      "masters programme",
      "curriculum",
      "enrollment",
      "student research",
      "funding opportunities"
    ],
    "absent_kp": [
      "graduate program",
      "double degree"
    ]
  },
  {
    "title": "techno-economic analysis of epon and wimax for future fiber-wireless (fiwi) networks.",
    "abstract": "hybrid fiber-wireless (fiwi) networks become rapidly mature and represent a promising candidate for reducing power consumption, costs, and bandwidth bottlenecks of next-generation broadband access networks. two key fiwi technologies with similar design goals are ethernet passive optical network (epon) and wimax. in this paper, we develop a powerful and flexible techno-economic analysis to compare the two technologies, taking into account not only equipment and installation costs but also oam related costs such as power consumption and repairing costs for a wide range of different network failure scenarios, terrain types, and wireless channel conditions. the presented results give insight into the cost-performance trade-offs of current and next-generation epon and wimax networks.",
    "present_kp": [
      "epon",
      "fiwi",
      "power consumption",
      "wimax"
    ],
    "absent_kp": [
      "cost modeling",
      "resiliency"
    ]
  },
  {
    "title": "insight into goal-directed movement strategies.",
    "abstract": "the current paper proposes a novel method of analyzing goal-directed movements by dividing them into distinct movement intervals. we demonstrate how the description of the first and second most prominent movement intervals in terms of duration and length can provide insight into the applied movement strategies under different conditions. this method, although demonstrated for goal-directed movements, has the potential to be generalized to other types of movements, such as steering movements.",
    "present_kp": [],
    "absent_kp": [
      "computer input devices",
      "movement analysis",
      "pointing tasks"
    ]
  },
  {
    "title": "mobius-invariant curve and surface energies and their applications.",
    "abstract": "curvature-based surface energies are frequently used in mathematics, physics, thin plate and shell engineering, and membrane chemistry and biology studies. invariance under rotations and shifts makes curvature-based energies very attractive for modeling various phenomena. in computer-aided geometric design, the willmore surfaces and the so-called minimum variation surfaces (mvs) are widely used for shape modeling purposes. the willmore surfaces are invariant w.r.t conformal transformations (mobius or conformal invariance), and studied thoroughly in differential geometry and related disciplines. in contrast, the minimum variation surfaces are not conformal invariant. in this paper, we suggest a simple modification of the minimum variation energy and demonstrate that the resulting modified mvs enjoy mobius invariance (so we call them conformal-invariant mvs or, shortly, ci-mvs). we also study connections of ci-mvs with the cyclides of dupin. in addition, we consider several other conformal-invariant curve and surface energies involving curvatures and curvature derivatives. in particular, we show how filtering with a conformal-invariant curve energy can be used for detecting salient subsets of the principal curvature extremum curves used by hosaka and co-workers for shape quality inspection purposes.",
    "present_kp": [
      "minimum variation surfaces"
    ],
    "absent_kp": [
      "willmore energy",
      "dupin's cyclides",
      "mobius/conformal invariance"
    ]
  },
  {
    "title": "calculation of delay characteristics for multiserver queues with constant service times.",
    "abstract": "we consider a discrete-time infinite-capacity queueing system with a general uncorrelated arrival process, constant-length service times of multiple slots, multiple servers and a first-come-first-served queueing discipline. under the assumption that the queueing system can reach a steady state, we first establish a relationship between the steady-state probability distributions of the system content and the customer delay. next, by means of this relationship, an explicit expression for the probability generating function of the customer delay is obtained from the known generating function of the system content, derived in previous work. in addition, several characteristics of the customer delay, namely the mean value, the variance and the tail distribution of the delay, are derived through some mathematical manipulations. the analysis is illustrated by means of some numerical examples.",
    "present_kp": [
      "queueing",
      "multiple servers",
      "constant service times"
    ],
    "absent_kp": [
      "discrete time",
      "delay analysis"
    ]
  },
  {
    "title": "converting computer-integrated manufacturing into an intelligent information system by combining cim with concurrent engineering and knowledge management.",
    "abstract": "some industrial organizations using computer-integrated manufacturing (cim) for managing intelligent product and process data during a concurrent processing are facing acute implementation difficulties. some of the difficulties are due to the fact that cim - in the current form - is not able to adequately address knowledge management and concurrent engineering (ce) issues. also, with cim, it is not possible to solve problems related to decision and control even though there has been an increasing interest in artificial intelligence (ai), knowledge based systems (kbs), expert systems, etc. in order to improve the productivity gain through cim, eds focused its information technology (it) vision on the combined potential of concurrent engineering (ce), knowledge management (km) and computer-integrated manufacturing (cim) technologies. eds-through a number of it and cim implementations - realized that ce, km and cim do go hand in hand. the three together provide a formidable base, which is called intelligent information system (iis) in this paper. describes the rationales used for creating an iis framework at eds, its usefulness to our clients and a make-up of this emerging iis framework for integrated product development.",
    "present_kp": [
      "computer-integrated manufacturing",
      "knowledge management",
      "information technology",
      "product development"
    ],
    "absent_kp": [
      "information systems",
      "simultaneous engineering"
    ]
  },
  {
    "title": "extending fre and zelenyuk (2003).",
    "abstract": "in our recent work, fre and zelenyuk (2003) , we have proposed a way of aggregating farrell-type efficiency scores with weights (and aggregation function) derived from economic-type optimization behaviour. in this comment we correct a mathematical error present in that work as well as generalize the price-independent weights we have proposed earlier.",
    "present_kp": [
      "efficiency",
      "aggregation"
    ],
    "absent_kp": [
      "distance function",
      "duality"
    ]
  },
  {
    "title": "policy oscillation is overshooting.",
    "abstract": "a majority of approximate dynamic programming approaches to the reinforcement learning problem can be categorized into greedy value function methods and value-based policy gradient methods. the former approach, although fast, is well known to be susceptible to the policy oscillation phenomenon. we take a fresh view to this phenomenon by casting, within the context of non-optimistic policy iteration, a considerable subset of the former approach as a limiting special case of the latter. we explain the phenomenon in terms of this view and illustrate the underlying mechanism with artificial examples. we also use it to derive the constrained natural actor-critic algorithm that can interpolate between the aforementioned approaches. in addition, it has been suggested in the literature that the oscillation phenomenon might be subtly connected to the grossly suboptimal performance in the tetris benchmark problem of all attempted approximate dynamic programming methods. based on empirical findings, we offer a hypothesis that might explain the inferior performance levels and the associated policy degradation phenomenon, and which would partially support the suggested connection. finally, we report scores in the tetris problem that improve on existing dynamic programming based results by an order of magnitude.",
    "present_kp": [
      "reinforcement learning",
      "approximate dynamic programming",
      "policy gradient",
      "policy oscillation"
    ],
    "absent_kp": [
      "natural gradient",
      "policy chattering"
    ]
  },
  {
    "title": "configuration and dynamic reconfiguration of components using the coordination paradigm.",
    "abstract": "one of the most promising approaches in developing component-based (possibly distributed) systems is that of coordination models and languages. coordination programming enjoys a number of advantages such as the ability to express different software architectures and abstract interaction protocols, support for multi-linguality, reusability and programming-in-the-large, etc. configuration programming is another promising approach in developing large scale, component-based systems, with the increasing need for supporting the dynamic evolution of components. in this paper we explore and exploit the relationship between the notions of coordination and (dynamic) configuration and we illustrate the potential of control- or event-driven coordination languages to be used as languages for expressing dynamically reconfigurable software architectures. we argue that control-driven coordination has similar goals and aims with the notion of dynamic configuration and we illustrate how the former can achieve the functionality required by the latter.",
    "present_kp": [
      "dynamic reconfiguration",
      "component-based systems"
    ],
    "absent_kp": [
      "coordination languages and models",
      "software engineering for distributed and parallel systems",
      "modelling software architectures"
    ]
  },
  {
    "title": "upper-level scheduling supporting multimedia traffic in cellular data networks.",
    "abstract": "wireless data networks such as cdma2000 1x ev-do and umts hsdpa use downlink scheduling that exploits channel fading to increase the system throughput. as future wireless networks will eventually support multimedia and data traffic together, we need a proper criterion for scheduling that can count various service requirements such as delay and packet loss. although some previous approaches proposed opportunistic schedulers at the lower layer, it has not been investigated well whether they are able to meet explicit qos defined at the upper layer. hence, in this paper, we develop a hierarchical scheduling model that considers qos provisioning and the time-varying channel feature separately. we focus on the upper-level qos scheduling that supports various traffic classes in a unified manner. supposing that a user gets some satisfaction or utility when served, we introduce a novel concept of opportunity cost, which is defined as the maximum utility loss among users incurred by serving a particular user at the current turn. we obtain each user's net profit by subtracting the opportunity cost from its expected utility, and then select a user with the maximum profit for service. simulation results reveal that our scheme supports various qos classes well that are represented by delay and packet loss under various traffic loadings.",
    "present_kp": [
      "scheduling",
      "utility",
      "opportunity cost"
    ],
    "absent_kp": [
      "quality-of-service"
    ]
  },
  {
    "title": "repcidn: a reputation-based collaborative intrusion detection network to lessen the impact of malicious alarms.",
    "abstract": "distributed and coordinated attacks in computer networks are causing considerable economic losses worldwide in recent years. this is mainly due to the transition of attackers operational patterns towards a more sophisticated and more global behavior. this fact is leading current intrusion detection systems to be more likely to generate false alarms. in this context, this paper describes the design of a collaborative intrusion detection network (cidn) that is capable of building and sharing collective knowledge about isolated alarms in order to efficiently and accurately detect distributed attacks. it has been also strengthened with a reputation mechanism aimed to improve the detection coverage by dropping false or bogus alarms that arise from malicious or misbehaving nodes. this model will enable a cidn to detect malicious behaviors according to the trustworthiness of the alarm issuers, calculated from previous interactions with the system. experimental results will finally demonstrate how entities are gradually isolated as their behavior worsens throughout the time.",
    "present_kp": [
      "intrusion detection systems"
    ],
    "absent_kp": [
      "security",
      "trust management",
      "reputation systems",
      "collaboration networks",
      "group reputation"
    ]
  },
  {
    "title": "differential evolution for system identification of self-excited vibrations.",
    "abstract": "a competitive version of the differential evolution algorithm is applied to regression on second-order ordinary differential equations from only the original time signal. specific attention is devoted to application aspects of self-excited vibrations in physics and engineering. extensive numerical experiments reveal the factors that influence test errors in free optimization. two novel approaches for a constrained optimization treatment of this inverse problem are proposed. this enables accurate identification of the target coefficients of the dynamical system.",
    "present_kp": [
      "constrained optimization",
      "system identification",
      "self-excited vibrations",
      "differential evolution",
      "ordinary differential equations"
    ],
    "absent_kp": [
      "evolutionary computing",
      "monte carlo"
    ]
  },
  {
    "title": "mean curvature mapping for detection of corneal shape abnormality.",
    "abstract": "corneal topography is used to measure the anterior surface of the cornea. it is conventionally represented as radial slope, radial curvature, and elevation. in this paper, we introduce the application of mean curvature mapping as an alternative representation of the corneal topography. the purpose is to improve the detection of keratoconus and other diseases characterized by local increase in corneal curvature. both simulated keratoconic cornea and real keratoconus; data exported from the corneal topography system were analyzed. four representations of corneal topography were generated and compared. it was found that mean curvature mapping provided the most precise cone location in simulated keratoconus. in both actual and simulated keratoconus cases, the appearance of the cone-like distortion is more consistent on mean curvature maps. mean curvature mapping may improve the detection and localization of corneal shape abnormalities.",
    "present_kp": [
      "cornea",
      "corneal topography",
      "keratoconus",
      "mean curvature"
    ],
    "absent_kp": []
  },
  {
    "title": "client-led information system creation (clic): navigating the gap.",
    "abstract": "abstract.? this paper offers a new framework to facilitate an interpretive approach to client-led information system development, referred to as clic (client-led information system creation). the challenge of moving seamlessly through a process of information systems (is) design is still the subject of much research in the is field. attempts to address the difficulties of bridging the gap between a client's business needs and an information system definition have hitherto not provided a coherent and practical approach. rather than attempting to bridge the gap, this paper describes an approach to managing this gap by facilitating the clients navigating through the information system design process (or inquiry process) in a coherent manner. the framework has been developed through practice, and the paper provides an example of navigating through the design phase taken from an action research field study in a major uk bank.",
    "present_kp": [
      "information systems",
      "action research"
    ],
    "absent_kp": [
      "systems theory",
      "modelling participation"
    ]
  },
  {
    "title": "markov process based reliability model for laser diodes in space radiation environment.",
    "abstract": "reliability model of irradiated laser diodes in space environment. degradation process is separated into discrete states. degradation of laser diodes is described as a markov process. reliability characteristics of laser diodes are simulated over 100,000h.",
    "present_kp": [
      "laser diodes",
      "space radiation environment",
      "reliability",
      "markov process"
    ],
    "absent_kp": []
  },
  {
    "title": "novartis malaria initiative: best practice example of pharmaceutical industry's engagement in the fight against malaria.",
    "abstract": "despite considerable advances in the treatment and prevention of malaria, plasmodium falciparum is still a threat to millions of people across the world, particularly in sub-saharan africa, with infants and young children bearing the greatest burden in terms of morbidity and mortality. since 1999, the artemisinin-based combination therapy artemether-lumefantrine (al; coartem) has been made available. a wealth of evidence supports consistently high efficacy of al, and a favorable safety and tolerability profile has been demonstrated. the child-friendly dispersible formulation of al has proven to be as effective and well tolerated as the standard tablets, and will encourage ease of administration and improved adherence to the drug regimen. this article reviews the significant impact made by al on the progress in malaria control and describes the way forward for the novartis malaria initiative in leading the fight against malaria.",
    "present_kp": [
      "artemisinin-based combination therapy",
      "artemether-lumefantrine",
      "dispersible",
      "malaria"
    ],
    "absent_kp": []
  },
  {
    "title": "scan architecture with align-encode.",
    "abstract": "scan architectures that provide compression capabilities have become mandatory due to the unbearable test costs imposed by high test data volume and prolonged test application. to alleviate these test costs, a stimulus decompressor and a response compactor block are inserted between the tester channels and the scan chains. as a result, a few tester channels drive a larger number of scan chains. in such an architecture, whether a particular test pattern can be delivered depends on the care bit distribution of that pattern. in this paper, we introduce a hardware block to be utilized in conjunction with a combinational stimulus decompressor block. this block., namely, align-encode, provides a deterministic per pattern control over care bit distribution of test vectors, improving pattern deliverability, and thus, the effectiveness of the particular stimulus decompressor. align-encode is reconfigured on a per pattern basis to delay the shift-in operations in selected scan chains. the number of cycles that a chain may be delayed can be between zero and the maximum allowable value, in order to align the scan slices in such a way that originally undeliverable test vectors become encodable. the reconfigurability of align-encode provides a test pattern independent solution, wherein any, given set of test vectors can be analyzed to compute the proper delay information. we present efficient techniques for computing the scan chain delay values that lead to pattern encodability. experimental results also justify the test pattern encodability enhancements that align-encode delivers, enabling significant test quality improvements and/or test cost reductions.",
    "present_kp": [
      "align-encode",
      "stimulus decompressor"
    ],
    "absent_kp": [
      "test data compression",
      "test data encoding"
    ]
  },
  {
    "title": "efficient discovery of highly interrelated users in one-way communications.",
    "abstract": "in this paper, we introduce a new sequential pattern, the interactive user sequence pattern (iusp). this pattern is useful for grouping highly interrelated users in one-way communications such as e-mail, sms, etc., especially when the communications include many spam users. also, we propose an efficient algorithm for discovering iusps from massive one-way communication logs containing only the following information: senders, receivers, and dates and times. even though there is a difficulty in that our new sequential pattern violates the apriori property, the proposed algorithm shows excellent processing performance and low storage cost in experiments on a real dataset.",
    "present_kp": [
      "sequential pattern"
    ],
    "absent_kp": [
      "data mining",
      "apriori property violation"
    ]
  },
  {
    "title": "a pc cluster system employing ieee 1394.",
    "abstract": "in this paper, we describe the design and evaluation of a pc cluster system in which ieee 1394 is applied. networks for parallel cluster computing require low latency and high bandwidth. it is also important that the networks be commercially available at low cost. few network devices satisfy all of the above requirements. however, the ieee 1394 standard provides a good compromise for fulfilling these requirements. we have used ieee 1394 devices, which support a 400 mbps data transfer rate, to connect the nodes of a pc cluster system which we have designed and implemented. we have implemented two communication libraries. one is a fast communication library called cf for ieee 1394. the other is a mpi layer library on the cf library. experimental results show that cf achieves a 17.2 microsecond round-trip time. on application benchmarks, the system was considerably faster than tcp/ip over fast ethernet. even though the system was constructed at very low cost, it provides good performance. using the ieee 1394 standard is thus a good solution for low-cost cluster systems.",
    "present_kp": [
      "cluster computing",
      "ieee 1394",
      "communication libraries"
    ],
    "absent_kp": [
      "performance evaluation"
    ]
  },
  {
    "title": "does habituation affect fingerprint quality.",
    "abstract": "interest in the environmental factors that affect biometric image quality is increasing as biometric technologies are currently being implemented in various business applications. this study aims to determine, through repeated trials, the effects of various external factors on the image quality and usability of prints collected by an electronic reader. these factors include age and gender but also the absence or presence of immediate feedback. a key factor in biometric systems that will be used daily or routinely is habituation. the user's behavior could potentially change as a result of acclimatization; one's input might increase in quality as one learns how to use the system better, or decrease in quality since comfort with the system could translate into carelessness.",
    "present_kp": [
      "habituation",
      "feedback"
    ],
    "absent_kp": [
      "biometrics",
      "fingerprint image quality"
    ]
  },
  {
    "title": "on the relocation problem with a second working crew for resource recycling.",
    "abstract": "in this paper, we introduce a variant of the relocation problem, which was formulated from a public house redevelopment project in boston. in the problem of interest, given some initial resources in a common pool there is a set of jobs to be processed on a two-machine flowshop. each job acquires a specific number of resources to start its processing and will return a number of resources to the pool at its completion. the resource consumption and resource recycle processes are performed on machine one and machine two, respectively, in a two-machine flowshop style. abiding by the resource constraints, the problem seeks to find a feasible schedule whose makespan is minimized. in this paper, we first present np-hardness proofs for some special cases. three heuristic algorithms are designed to compose approximate schedules. two lower bounds are developed and then used to test the performance of our proposed heuristics. numerical results from computational experiments suggest that the proposed heuristics can produce quality solutions in a reasonable time.",
    "present_kp": [
      "relocation problem",
      "flowshop",
      "makespan",
      "np-hard",
      "heuristic algorithms",
      "lower bound"
    ],
    "absent_kp": []
  },
  {
    "title": "equivariant pieri rule for the homology of the affine grassmannian.",
    "abstract": "an explicit rule is given for the product of the degree two class with an arbitrary schubert class in the torus-equivariant homology of the affine grassmannian. in addition a pieri rule (the schubert expansion of the product of a special schubert class with an arbitrary one) is established for the equivariant homology of the affine grassmannians of sl n and a similar formula is conjectured for sp 2n and so 2n+1. for sl n the formula is explicit and positive. by a theorem of peterson these compute certain products of schubert classes in the torus-equivariant quantum cohomology of flag varieties. the sl n pieri rule is used in our recent definition of k-double schur functions and affine double schur functions.",
    "present_kp": [
      "affine grassmannian",
      "pieri rule",
      "quantum cohomology"
    ],
    "absent_kp": [
      "schubert calculus"
    ]
  },
  {
    "title": "dual-rail asynchronous logic multi-level implementation.",
    "abstract": "a synthesis flow oriented on producing the delay-insensitive dual-rail asynchronous logic is proposed. within this flow, the existing synchronous logic synthesis tools are exploited to design technology independent single-rail synchronous boolean network of complex (and-or) nodes. next, the transformation into a dual-rail boolean network is done. each node is minimized under the formulated constraint to ensure hazard-free implementation. then the technology dependent mapping procedure is applied. the mcnc and iscas benchmark sets are processed and the area overhead with respect to the synchronous implementation is evaluated. the implementations of the asynchronous logic obtained using the proposed (with and-or nodes) and the state-of-the-art (nodes are designed based on dims, direct logic and ncl) network structures are compared. a method, where nodes are designed as simple (nand, nor, etc.) gates is chosen for a detailed comparison. in our approach, the number of completion detection logic inputs is reduced significantly, since the number of nodes that should be supplied with the completion detection is less than in the case of the network structure that is based on simple gates. as a result, the improvement in sense of the total complexity and performance is obtained.",
    "present_kp": [
      "asynchronous logic",
      "multi-level implementation",
      "boolean network",
      "node"
    ],
    "absent_kp": [
      "decomposition"
    ]
  },
  {
    "title": "off-line signature verification and forgery detection using fuzzy modeling.",
    "abstract": "automatic signature verification is a well-established and an active area of research with numerous applications such as bank check verification, atm access, etc. this paper proposes a novel approach to the problem of automatic off-line signature verification and forgery detection. the proposed approach is based on fuzzy modeling that employs the takagisugeno (ts) model. signature verification and forgery detection are carried out using angle features extracted from box approach. each feature corresponds to a fuzzy set. the features are fuzzified by an exponential membership function involved in the ts model, which is modified to include structural parameters. the structural parameters are devised to take account of possible variations due to handwriting styles and to reflect moods. the membership functions constitute weights in the ts model. the optimization of the output of the ts model with respect to the structural parameters yields the solution for the parameters. we have also derived two ts models by considering a rule for each input feature in the first formulation (multiple rules) and by considering a single rule for all input features in the second formulation. in this work, we have found that ts model with multiple rules is better than ts model with single rule for detecting three types of forgeries; random, skilled and unskilled from a large database of sample signatures in addition to verifying genuine signatures. we have also devised three approaches, viz., an innovative approach and two intuitive approaches using the ts model with multiple rules for improved performance.",
    "present_kp": [
      "off-line signature verification",
      "forgery detection",
      "structural parameters",
      "ts model"
    ],
    "absent_kp": [
      "fuzzy logic",
      "bank check recognition"
    ]
  },
  {
    "title": "investigation of si/sige/si heterostructure implanted by h ion and annealed in vacuum and dry o2 ambient.",
    "abstract": "the 20-nm-thick si cap layer/74-nm-thick si0.72ge0.28 epilayer/si heterostructures implanted by 25kev h+ ion to a dose of 11016cm?2 were annealed in ultra-high vacuum ambient and dry o2 ambient at the temperature of 800c for 30min, respectively. rutherford backscattering/ion channeling (rbs/c), raman spectra, high-resolution x-ray diffraction (hrxrd) and atomic force microscopy (afm) were used to characterize the structural characteristics of the si0.72ge0.28 layer. investigations by rbs/c demonstrated that the crystal quality of the si/si0.72ge0.28/si heterostructure sample implanted by 25kev h+ in conjunction with subsequent annealing in dry o2 ambient is superior to that of identical sample annealing in ultra-high vacuum ambient. the less strain relaxation of sige layer of the si/si0.72ge0.28/si heterostructures implanted by h ion and annealed in dry o2 ambient at the temperature of 800c for 30min could be doublechecked by raman spectra as well as hrxrd, which was compared with that in an identical sample annealed in ultra-high vacuum ambient for identical thermal budget. in addition, the sige layer of the h-implanted si/sige/si heterostructural sample annealed in dry o2 ambient accompanied by better crystal quality and less strain relaxation made its surface morphology superior to that of the sample annealed in ultra-high vacuum ambient at the temperature of 800c for 30min, which was also verified by afm images.",
    "present_kp": [
      "strain relaxation",
      "sige"
    ],
    "absent_kp": [
      "ion implantation"
    ]
  },
  {
    "title": "energy-neutral scheduling and forwarding in environmentally-powered wireless sensor networks.",
    "abstract": "in environmentally-powered wireless sensor networks (epwsns), low latency wakeup scheduling and packet forwarding is challenging due to dynamic duty cycling, posing time-varying sleep latencies and necessitating the use of dynamic wakeup schedules. we show that the variance of the intervals between receiving wakeup slots affects the expected sleep latency: when the variance of the intervals is low (high), the expected latency is low (high). we therefore propose a novel scheduling scheme that uses the bit-reversal permutation sequence (brps) a finite integer sequence that positions receiving wakeup slots as evenly as possible to reduce the expected sleep latency. at the same time, the sequence serves as a compact representation of wakeup schedules thereby reducing storage and communication overhead. but while low latency wakeup schedule can reduce per-hop delay in ideal conditions, it does not necessarily lead to low latency end-to-end paths because wireless link quality also plays a significant role in the performance of packet forwarding. we therefore formulate expected transmission delay (etd), a metric that simultaneously considers sleep latency and wireless link quality. we show that the metric is left-monotonic and left-isotonic, proving that its use in distributed algorithms such as the distributed bellmanford yields consistent, loop-free and optimal paths. we perform extensive simulations using real-world energy harvesting traces to evaluate the performance of the scheduling and forwarding scheme.",
    "present_kp": [
      "wireless sensor network",
      "dynamic duty cycling",
      "sleep latency"
    ],
    "absent_kp": [
      "energy-harvesting",
      "dynamic wakeup scheduling",
      "routing"
    ]
  },
  {
    "title": "minimizing downtime in seamless migrations of mobile applications.",
    "abstract": "application migration is a key enabling technology component of mobile computing that allows rich semantics involving location awareness, trust and timeliness of information processing by moving the application where the data is. seamlessness is one of the key properties of mobile computing and downtime must be eliminated/minimized during the migration to achieve seamlessness. but migration involves large overheads, dominant of which are the overheads due to serialization and de-serialization . to achieve seamless migration, an application state could be pre-serialized during the program's execution, and upon migration, the serialized data could be transmitted and de-serialized to get the execution started. previous approach to this problem removed dead state but still suffered from large migration overheads due to serialization on-demand that could lead to an unacceptable downtime.in this work, we develop a static compiler analysis plus runtime assisted framework to decrease the migration overhead to almost zero while minimizing the degradation in the program's performance. we achieve such a goal by deciding which data to be pre-serialized through analysis, and pre-serializing the state in the program. a safe state is kept that would allow immediate migration upon the arrival of an interrupt while minimizing frequent pre-serialization. when the migration interrupt comes in, the serialized data can be transmitted directly to the destination machine. this allows an application to resume its execution at the destination machine with almost no interruption (only a small amount of non-serialized data needs to be serialized during migration). the optimization serializes the data in such a way that a maximal number of functions can execute without interruption after migration. our experiments with multimedia applications show that the migration latency is significantly reduced leading to a small downtime. thus, the contribution of the paper is to provide an efficient methodology to perform seamless migration while limiting the overhead.",
    "present_kp": [
      "mobile computing",
      "compiler",
      "seamless migration"
    ],
    "absent_kp": []
  },
  {
    "title": "an efficient algorithm based on the differential quadrature method for solving navier-stokes equations.",
    "abstract": "in this paper, an approach to improve the application of the differential quadrature method for the solution of navierstokes equations is presented. in using the conventional differential quadrature method for solving navierstokes equations, difficulties such as boundary conditions' implementation, generation of an ill conditioned set of linear equations, large memory storage requirement to store data, and matrix coefficients, are usually encountered. also, the solution of the generated set of equations takes a long running time and needs high computational efforts. an approach based on the point pressurevelocity iteration method, which is a variant of the newtonraphson relaxation technique, is presented to overcome these problems without losing accuracy. to verify its performance, four cases of two-dimensional flows in single and staggered double lid-driven cavity and flows past backward facing step and square cylinder, which have been often solved by researchers as benchmark solution, are simulated for different reynolds numbers. the results are compared with existing solutions in the open literature. very good agreement with low computational efforts of the approach is shown. it has been concluded that the method can be applied easily and is very time efficient.",
    "present_kp": [
      "differential quadrature method"
    ],
    "absent_kp": [
      "ns equations",
      "lid-driven cavity flow",
      "simple strategy"
    ]
  },
  {
    "title": "optimal transmission schemes for parallel and fading gaussian broadcast channels with an energy harvesting rechargeable transmitter.",
    "abstract": "we consider an energy harvesting transmitter sending messages to two users over parallel and fading gaussian broadcast channels. energy required for communication arrives (is harvested) at the transmitter and a finite-capacity battery stores it before being consumed for transmission. under off-line knowledge of energy arrival and channel fading variations, we obtain the trade-off between the performances of the users by characterizing the maximum departure region in a given interval. we first analyze the transmission with an energy harvesting transmitter over parallel broadcast channels. we show that the optimal total transmit power policy that achieves the boundary of the maximum departure region is the same as the optimal policy for the non-fading broadcast channel, which does not depend on the priorities of the users, and therefore is the same as the optimal policy for the non-fading scalar single-user channel. the optimal total transmit power can be found by a directional water-filling algorithm. the optimal splitting of the power among the parallel channels is performed in each epoch separately. next, we consider fading broadcast channels and obtain the transmission policies that achieve the boundary of the maximum departure region. the optimal total transmit power allocation policy is found using a specific directional water-filling algorithm for fading broadcast channels. the optimal power allocation depends on the priorities of the users unlike in the case of parallel broadcast channels. finally, we provide numerical illustrations of the optimal policies and maximum departure regions for both parallel and fading broadcast channels.",
    "present_kp": [
      "energy harvesting",
      "finite-capacity battery"
    ],
    "absent_kp": [
      "rechargeable wireless networks",
      "off-line scheduling",
      "throughput maximization"
    ]
  },
  {
    "title": "efficient detection in hyperspectral imagery.",
    "abstract": "hyperspectral sensors collect hundreds of narrow and contiguously spaced spectral bands of data. such sensors provide fully registered high resolution spatial and spectral images that are invaluable in discriminating between man-made objects and natural clutter backgrounds. the price paid for this high resolution data is extremely large data sets, several hundred of mbytes for a single scene, that make storage and transmission difficult, thus requiring fast onboard processing techniques to reduce the data being transmitted. attempts to apply traditional maximum likelihood detection techniques for in-flight processing of these massive amounts of hyperspectral data suffer from two limitations: first, they neglect the spatial correlation of the clutter by treating it as spatially white noise; second, their computational cost renders them prohibitive without significant data reduction like by grouping the spectral bands into clusters, with a consequent loss of spectral resolution. this paper presents a maximum likelihood detector that successfully confronts both problems: rather than ignoring the spatial and spectral correlations, our detector exploits them to its advantage; and it is computationally expedient, its complexity increasing only linearly with the number of spectral bands available. our approach is based on a gauss-markov random held (gmrf) modeling of the clutter, which has the advantage of providing a direct parameterization of the inverse of the clutter covariance, the quantity of interest in the test statistic. we discuss in detail two alternative gmrf detectors: one based on a binary hypothesis approach, and the other on a 'single' hypothesis formulation. we analyze extensively with real hyperspectral imagery data (hydice and sebass) the performance of the detectors, comparing them to a benchmark detector, the rx-algorithm. our results show that the gmrf 'single' hypothesis detector outperforms significantly in computational cost the rx-algorithm, while delivering noticeable detection performance improvement.",
    "present_kp": [],
    "absent_kp": [
      "gauss-markov random field",
      "hyperspectral sensor imagery",
      "maximum-likelihood detection",
      "'single' hypothesis test"
    ]
  },
  {
    "title": "two-way eye contact between humans and robots.",
    "abstract": "eye contact is an effective means of controlling human communication, such as in starting communication. it seems that we can make eye contact if we simply look at each other. however, this alone does not establish eye contact. both parties also need to be aware of being watched by the other. we propose a method of two-way eye contact for human-robot communication. when a human wants to start communication with a robot, he/she watches the robot. if it finds a human looking at it, the robot turns to him/her, changing its facial expressions to let him/her know its awareness of his/her gaze. when the robot wants to initiate communication with a particular person, it moves its body and face toward him/her and changes its facial expressions to make the person notice its gaze. we show several experimental results to prove the effectiveness of this method. moreover, we present a robot that can recognize hand gestures after making eye contact with the human to show the usefulness of eye contact as a means of controlling communication.",
    "present_kp": [
      "eye contact",
      "gaze"
    ],
    "absent_kp": [
      "embodied agent",
      "human-robot interface",
      "nonverbal behavior",
      "gesture recognition"
    ]
  },
  {
    "title": "program representations for testing wireless sensor network applications.",
    "abstract": "because of the growing complexity of wireless sensor network applications (wsns), traditional software development tools are being developed that are specifically designed for their special characteristics. however, testing tools have yet to be proposed. one problem in developing testing tools is the need for a program representation that expresses the execution behavior. due to characteristics of wsn applications that use a concurrent, event-based execution model, a representation is challenging to develop. in this paper, we present novel representations for wsns applications that express the execution behavior of event and tasks, the major components of a wsn application. our representations include a task posting graph, an event graph and finally an application graph that expresses the relationships among events and tasks as well as both timing and environmental interrupts. these representations are the first step in developing testing tools for wsn applications. based on the graphs, traditional and event-based coverage criteria can be evaluated. when combined with individual control flow graphs(cfgs) of events and tasks, the graphs' paths can be used as a criterion for evaluating the completeness of the test cases.",
    "present_kp": [
      "program representation"
    ],
    "absent_kp": [
      "wireless sensor networks",
      "test criteria"
    ]
  },
  {
    "title": "equalising stamp and substrate deformations in solid parallel-plate uv-based nanoimprint lithography.",
    "abstract": "we deal with solid parallel-plate uv-based nanoimprint lithography (uv-nil) using rigid quartz stamps and spin coated substrates. achieving a conformal contact of stamp and substrate in a parallel-plate setup is challenging, since the solid stamp and the substrate usually are not perfectly flat. finite element simulations and experimental results show that a correctly designed compliant layer underneath the substrate guarantees a conformal contact of stamp and substrate and therefore imprints of a high quality with a homogeneous and thin residual layer down to 10nm can be achieved.",
    "present_kp": [
      "uv-nil",
      "deformation",
      "compliant layer",
      "finite element simulation"
    ],
    "absent_kp": []
  },
  {
    "title": "optimal inventory policies for profit maximizing eoq models under various cost functions.",
    "abstract": "in this paper, we establish and analyze three eoq based inventory models under profit maximization via geometric programming (gp) techniques. through gp, we find optimal order quantity and price for each of these models considering production (lot sizing) as well as marketing (pricing) decisions. we also investigate the effects on the changes in the optimal solutions when different parameters are changed. in addition, a comparative analysis between the profit maximization models is conducted. by investigating the error in the optimal price, order quantity, and profit of these models, several interesting economic implications and insights can be observed.",
    "present_kp": [
      "inventory",
      "geometric programming",
      "eoq"
    ],
    "absent_kp": []
  },
  {
    "title": "augmented state gm-phd filter with registration errors for multi-target tracking by doppler radars.",
    "abstract": "we build the linear gaussian dynamics and measurement model of the augmented state. we derive related equations for the augmented state gm-phd filter with sensor biases. to effectively utilize the doppler, we propose the sequential processing method. the proposed gm-phd-r-d is compared with the gm-phd-r for varying clutter rates.",
    "present_kp": [
      "augmented state",
      "registration errors",
      "doppler"
    ],
    "absent_kp": [
      "multi-sensor multi-target tracking",
      "gaussian mixture probability hypothesis density filter"
    ]
  },
  {
    "title": "restoring solvability of the electric network equations: an approach based on the augmented lagrangean algorithm.",
    "abstract": "this paper presents and discusses a new, robust approach for restoring solvability of the electric network equations in power systems. the unsolvable power flow is modelled as a constrained optimization problem. the cost function is the squared sum of the real and reactive power mismatches at the electric system buses which are subject to suffer load shedding. the equality constraints are the real and reactive power mismatches at null injection buses and/or at buses whose power demands must be integrality supplied due to technical and/or economical criteria. the mathematical model is solved using an algorithm based on augmented lagrangean function method which takes into account the special structure of the proposed problem. the inner iterations of the proposed methodology are solved using the levenberg-marquardt (lm) algorithm. numerical results for both ieee test systems and a real equivalent electric system corresponding to brazil south-southeast region are presented in order to analyze and test the performance of the proposed methodology.",
    "present_kp": [
      "unsolvable power flow",
      "restoring solvability of the electric network equations"
    ],
    "absent_kp": [
      "augmented lagrangean method",
      "levenberg-marquardt algorithm"
    ]
  },
  {
    "title": "combination of independent component analysis and support vector machines for intelligent faults diagnosis of induction motors.",
    "abstract": "this paper studies the application of independent component analysis (ica) and support vector machines (svms) to detect and diagnose of induction motor faults. the ica is used for feature extraction and data reduction from original features. the principal components analysis is also applied in feature extraction process for comparison with ica does. in this paper, the training of the svms is carried out using the sequential minimal optimization algorithm and the strategy of multi-class svms-based classification is applied to perform the faults identification. also, the performance of classification process due to the choice of kernel function is presented to show the excellent of characteristic of kernel function. various scenarios are examined using data sets of vibration and stator current signals from experiments, and the results are compared to get the best performance of classification process.",
    "present_kp": [
      "independent component analysis",
      "support vector machines",
      "feature extraction",
      "induction motor",
      "current signal"
    ],
    "absent_kp": [
      "fault diagnosis",
      "principal component analysis",
      "vibration signal"
    ]
  },
  {
    "title": "context analysis to support development of virtual reality applications.",
    "abstract": "to develop a usable virtual reality system, the prospective context of use of such a system may need to be considered in order to make sure it meets the requirements and restrictions of that context. in this paper, a contextual analysis is described for a virtual reality system to aid medical diagnosis and treatment planning of vascular disorders. semi-structured interviews were coupled with observations in an ethnographic approach to requirements gathering in the daily work environment of (interventional) radiologists and vascular surgeons. the identified potential usability problems of a fully immersive prototype, coupled with the needs, requirements and real-life environment of the end-users lead to guidelines for the development of a vr application on a semi-immersive desktop environment. the findings lead us to believe that contextual analysis can be a powerful way to inform the design of a vr application by offering an understanding of the context of use and to inform developers of the most appropriate degree of immersiveness of the vr environment.",
    "present_kp": [
      "usability"
    ],
    "absent_kp": [
      "desktop virtual reality",
      "contextual design",
      "ethnography"
    ]
  },
  {
    "title": "efficient reconfigurable manchester adders for low-power media processing.",
    "abstract": "a new highly reconfigurable manchester adder for low-power media signal processing is presented. the proposed circuit can be run-time partitioned. its 64-bit version performs one 64-, two 32-, four 16-, or eight 8-bit additions. when the ams 0.35mm 2-poly 3-metal 3.3v cmos (csd) process is used to produce a layout, an energy dissipation of only 78 pj and a worst propagation delay of about 10.2 ns are obtained. the novelty demonstrated in this letter is that the introduction of dummy bit positions along the carry-path can be avoided using on-purpose dynamic logic stages.",
    "present_kp": [
      "adders"
    ],
    "absent_kp": [
      "cmos circuit",
      "multimedia processing",
      "low-power arithmetic"
    ]
  },
  {
    "title": "numerical convergence and physical fidelity analysis for maxwells equations in metamaterials.",
    "abstract": "in this paper, we develop a leap-frog mixed finite element method for solving maxwells equations resulting from metamaterials. our scheme is similar to the popular yees fdtd scheme used in electrical engineering community, and is preferable for three dimensional large scale modeling since no storage of the large coefficient matrix is needed. our scheme is proved to obey the gausss law automatically if the initial fields satisfy that. furthermore, the conditional stability and optimal error estimate for the proposed scheme are proved. to our best knowledge, we are unaware of any other publications devoted to the convergence analysis of this leap-frog explicit scheme for maxwells equations even in a simple medium, while our results for metamaterials automatically reduce to the standard maxwells equations in vacuum by dropping some terms resulting from the constitutive equations. numerical results confirming our analysis are presented.",
    "present_kp": [
      "mixed finite element method"
    ],
    "absent_kp": [
      "maxwell\u2019s equations",
      "double negative metamaterials"
    ]
  },
  {
    "title": "weak convergence of finite element approximations of linear stochastic evolution equations with additive noise.",
    "abstract": "a unified approach is given for the analysis of the weak error of spatially semidiscrete finite element methods for linear stochastic partial differential equations driven by additive noise. an error representation formula is found in an abstract setting based on the semigroup formulation of stochastic evolution equations. this is then applied to the stochastic heat, linearized cahn-hilliard, and wave equations. in all cases it is found that the rate of weak convergence is twice the rate of strong convergence, sometimes up to a logarithmic factor, under the same or, essentially the same, regularity requirements.",
    "present_kp": [
      "finite element",
      "stochastic",
      "wave equation",
      "additive noise",
      "weak convergence"
    ],
    "absent_kp": [
      "parabolic equation",
      "hyperbolic equation",
      "heat equation",
      "cahn-hilliard-cook equation",
      "wiener process",
      "error estimate"
    ]
  },
  {
    "title": "numerical modeling of magnetic induction tomography using the impedance method.",
    "abstract": "this article discusses the impedance method in the forward calculation in magnetic induction tomography (mit). magnetic field and eddy current distributions were obtained numerically for a sphere in the field of a coil and were compared with an analytical model. additionally, numerical and experimental results for phase sensitivity in mit were obtained and compared for a cylindrical object in a planar array of sensors. the results showed that the impedance method provides results that agree very well with reality in the frequency range from 100khz to 20mhz and for low conductivity objects (10s/m or less). this opens the possibility of using this numerical approach in image reconstruction in mit.",
    "present_kp": [
      "tomography",
      "impedance method",
      "magnetic field",
      "numerical modeling"
    ],
    "absent_kp": [
      "eddy currents"
    ]
  },
  {
    "title": "how to help seismic analysts to verify the french seismic bulletin.",
    "abstract": "in this paper, classifiers based on multi-layer perceptrons and support vector machines are used in order to classify seismic events that occurred in metropolitan france. the results are exploited in the software ramses to help the seismic analysts to conduct efficiently the revision of the weekly french seismic bulletin. with 96.5% of good classification, and less than 7% of the events emphasized for verification, ramses strikingly improves the speed of the revision.",
    "present_kp": [
      "support vector machines"
    ],
    "absent_kp": [
      "neural networks",
      "seismic event classification"
    ]
  },
  {
    "title": "robust stability of uncertain fuzzy cohen-grossberg bam neural networks with time-varying delays.",
    "abstract": "in this paper, the takagi-sugeno (ts) fuzzy model representation is extended to the stability analysis for uncertain cohen-grossberg type bidirectional associative memory (bam) neural networks with time-varying delays using linear matrix inequality (lmi) theory. a novel lmi-based stability criterion is obtained by using lmi optimization algorithms to guarantee the asymptotic stability of uncertain cohen-grossberg bam neural networks with time varying delays which are represented by ts fuzzy models. finally, the proposed stability conditions are demonstrated with numerical examples.",
    "present_kp": [
      "fuzzy cohen-grossberg bam neural networks ",
      "linear matrix inequality",
      "time-varying delays"
    ],
    "absent_kp": [
      "global asymptotic stability",
      "lyapunov functional"
    ]
  },
  {
    "title": "fluid-structure interaction problems in free surface flows: application to boat dynamics.",
    "abstract": "in this paper, we present some recent studies on fluid-structure interaction problems in the presence of free surface flow. we consider the dynamics of boats simulated as rigid bodies. several hydrodynamic models are presented, ranging from full reynolds averaged navier-stokes equations to reduced models based on potential flow theory.",
    "present_kp": [
      "fluid-structure interaction"
    ],
    "absent_kp": [
      "computational fluid dynamics",
      "dynamics of rowing"
    ]
  },
  {
    "title": "information-theoretic approaches to branching in search.",
    "abstract": "deciding what to branch on at each node is a key element of search algorithms. we present four families of methods for selecting what question to branch on. they are all information-theoretically motivated to reduce uncertainty in remaining subproblems . in the first family, a good variable to branch on is selected based on lookahead. in real-world procurement optimization, this entropic branching method outperforms default cplex and strong branching. the second family combines this idea with strong branching. the third family does not use lookahead, but instead exploits features of the underlying structure of the problem. experiments show that this family significantly outperforms the state-of-the-art branching strategy when the problem includes indicator variables as the key driver of complexity. the fourth family is about branching using carefully constructed linear inequality constraints over sets of variables.",
    "present_kp": [
      "entropic branching",
      "search"
    ],
    "absent_kp": [
      "branching heuristics",
      "mixed-integer programming"
    ]
  },
  {
    "title": "efficient solution to the 3d problem of automatic wall paintings reassembly.",
    "abstract": "this paper introduces a new approach for the automated reconstruction- reassembly of fragmented objects having one surface near to plane, on the basis of the 3d representation of their constituent fragments. the whole process starts by 3d scanning of the available fragments. the obtained representations are properly processed so that they can be tested for possible matches. next, four novel criteria are introduced, that lead to the determination of pairs of matching fragments. these criteria have been chosen so as the whole process imitates the instinctive reassembling method dedicated scholars apply. the first criterion exploits the volume of the gap between two properly placed fragments. the second one considers the fragments overlapping in each possible matching position. criteria 3,4 employ principles from calculus of variations to obtain bounds for the area and the mean curvature of the contact surfaces and the length of contact curves, which must hold if the two fragments match. the method has been applied, with great success, both in the reconstruction of objects artificially broken by the authors and, most importantly, in the virtual reassembling of parts of wall paintings belonging to the mycenaic civilization (c.1300 bc.), excavated in a highly fragmented condition in tyrins, greece",
    "present_kp": [
      "calculus of variations"
    ],
    "absent_kp": [
      "fragmented objects reassembly",
      "wall paintings reconstruction",
      "pattern matching",
      "3d pattern analysis",
      "geometry"
    ]
  },
  {
    "title": "resolution enhancement of nondestructive testing from b-scans.",
    "abstract": "this article presents an approach to extend our previous work of the minimum-weighted norm method in computerized tomography. in particular concentrating on applications of ultrasonic nondestructive testing, the resolution enhancement in the image reconstruction from b-scans is achieved. to combat the degradation problem due to physical focus of finite-sized ultrasonic transducer and incompleteness of b-scan data, a profile-oriented prior knowledge about the object being detected is incorporated in the image reconstruction, in the form of weighted summation of specific basis functions. each basis function is characterized by an image of coherent illumination pattern associated with a specific measuring time and a specific measuring position. from the demonstrations with both simulated and experimental data values, this technique proves a great potential in improving the image quality.",
    "present_kp": [
      "minimum-weighted norm",
      "ultrasonic nondestructive testing",
      "resolution enhancement",
      "b-scans",
      "profile-oriented prior knowledge"
    ],
    "absent_kp": []
  },
  {
    "title": "user centric cloud service model in public sectors: policy implications of cloud services.",
    "abstract": "this study examines the acceptance of cloud computing services in government agencies by focusing on the key characteristics that affect behavioral intent. the study expanded upon the technology acceptance model by incorporating contextual factors such as availability, access, security, and reliability. the research model was empirically verified by investigating the perception of users working in public institutions. modeling results showed that user intentions and behaviors were largely influenced by the perceived features of cloud services. also these features were found to be the significant antecedents of cloud computing usefulness and ease of use. the findings should guide governments' promotion of cloud public services to increase user awareness by enhancing usability and appeal and ensuring security.",
    "present_kp": [
      "cloud computing",
      "public sector",
      "technology acceptance model",
      "security",
      "reliability",
      "access"
    ],
    "absent_kp": [
      "cloud policy"
    ]
  },
  {
    "title": "a characterization of concordance relations.",
    "abstract": "the notion of concordance is central to many multiple criteria techniques relying on ordinal information, e.g. outranking methods. it leads to compare alternatives by pairs on the basis of a comparison of coalitions of attributes in terms of importance. this paper proposes a characterization of the binary relations that can be obtained using such comparisons within a general framework for conjoint measurement that allows for intransitive preferences. we show that such relations are mainly characterized by the very rough differentiation of preference differences that they induce on each attribute.",
    "present_kp": [
      "concordance",
      "outranking methods",
      "conjoint measurement"
    ],
    "absent_kp": [
      "multiple criteria analysis",
      "nontransitive preferences"
    ]
  },
  {
    "title": "impact of sourcing flexibility on the outsourcing of services under demand uncertainty.",
    "abstract": "this paper investigates the relationship between market conditions and the value and use of sourcing flexibility for service processes. we develop and analyze a series of models, and we derive expressions for the optimal switching decision, the value of the option to outsource, the value of the option to backsource, and the probability and timing of switches between the alternative sources. one contribution is the models and associated derivations, which are largely new to the literature and may serve as a tool to support service sourcing plans and decisions. the second contribution is a series of results with managerial implications: (1) the probability of outsourcing is generally increasing in volatility for high-skill process and decreasing in volatility for low-skill processes. earlier work has found that the hysteresis band is increasing in volatility, which is interpreted as an indicator of increasing organizational inertia. we also find that the hysteresis band is increasing in volatility, but interestingly for the case of high-skill processes, organizational inertia tends to be decreasing in volatility. (2) the option to backsource is generally more valuable for high-skill processes than for low-skill processes. this result suggests that investments to make it easier to backsource should have a higher priority for high-skill processes. (3) the value of the option to backsource a high-skill service process can be decreasing in volatility. the result suggests that a rather nuanced consideration of volatility is in order when considering investments in the flexibility to backsource a high-skill process.",
    "present_kp": [
      "demand uncertainty"
    ],
    "absent_kp": [
      "services outsourcing",
      "backsourcing",
      "real options"
    ]
  },
  {
    "title": "a robotic model of reaching and grasping development.",
    "abstract": "we present a neurorobotic model that develops reaching and grasping skills analogous to those displayed by infants during their early developmental stages. the learning process is realized in an incremental manner, taking into account the reflex behaviors initially possessed by infants and the neurophysiological and cognitive maturation occurring during the relevant developmental period. the behavioral skills acquired by the robots closely match those displayed by children. the comparison between incremental and nonincremental experiments demonstrates how some of the limitations characterizing the initial developmental phase channel the learning process toward better solutions.",
    "present_kp": [
      "grasping",
      "reaching"
    ],
    "absent_kp": [
      "developmental robotics",
      "humanoid",
      "incremental learning"
    ]
  },
  {
    "title": "performance analyses of notch fourier transform (nft) and constrained notch fourier transform (cnft).",
    "abstract": "fourier analysis of sinusoidal and/or quasi-periodic signals in additive noise has been used in various fields. so far, many analysis algorithms including the well-known dft have been developed. in particular, many adaptive algorithms have been proposed to handle non-stationary signals whose discrete fourier coefficient (dfcs) are time-varying. notch fourier transform (nft) and constrained notch fourier transform(cnft) proposed by tadokoro et al. and kilani et al., respectively, are two of them, which are implemented by filter banks and estimate the dfcs via simple sliding algorithms of their own. this paper presents, for the first time, statistical performance analyses of the nft: and the cnft. estimation biases and mean square errors (mses) of their sliding algorithms will be derived in closed form. as a result, it is revealed that both algorithms are unbiased, and their estimation mses are related to the signal frequencies, the additive noise variance and orders of comb filters used in their filter banks. extensive simulations are performed to confirm the analytical findings.",
    "present_kp": [
      "fourier analysis",
      "notch fourier transform "
    ],
    "absent_kp": [
      "constrained nft",
      "performance analysis",
      "mean square error "
    ]
  },
  {
    "title": "complementary cycles in almost regular multipartite tournaments, where one cycle has length four.",
    "abstract": "let d be a digraph with vertex set v(d) v ( d ) and independence number ?(d) ? ( d ) . if x?v(d) x ? v ( d ) , then the numbers d+(x) d + ( x ) and d?(x) d ? ( x ) are the outdegree and indegree of x , respectively. the global irregularity of a digraph d is defined by in 1999, yeo conjectured that each regular c-partite tournament d with c?4 c ? 4 and |v(d)|?8 | v ( d ) | ? 8 contains a pair of vertex-disjoint directed cycles of lengths 4 and |v(d)|?4 | v ( d ) | ? 4 . in 2004, volkmann confirmed this conjecture for c?5 c ? 5 and c=4 c = 4 and ?(d)?4 ? ( d ) ? 4 . as a supplement to this result, we prove in this paper the following theorem. let d be an almost regular c-partite tournament with |v(d)|?8 | v ( d ) | ? 8 such that all partite sets have the same cardinality r . if c?5 c ? 5 or c=4 c = 4 and r?6 r ? 6 , then d contains a pair of vertex-disjoint directed cycles of lengths 4 and |v(d)|?4 | v ( d ) | ? 4 .",
    "present_kp": [
      "multipartite tournaments",
      "complementary cycles",
      "almost regular multipartite tournaments"
    ],
    "absent_kp": []
  },
  {
    "title": "data assimilation framework: linking an open data assimilation library (openda) to a widely adopted model interface (openmi).",
    "abstract": "a generic framework for data assimilation is presented. it bridges together two open source projects: openda and openmi. openmi compliant models can easily get access to data assimilation algorithms. tested on a catchment in denmark assimilating hydraulic head.",
    "present_kp": [
      "openda",
      "openmi",
      "data assimilation"
    ],
    "absent_kp": [
      "hydrological modeling",
      "kalman filter",
      "uncertainty"
    ]
  },
  {
    "title": "realistic scalability of noise in dynamic circuits.",
    "abstract": "the usage of noise-sensitive dynamic circuits has become commonplace due to speed and area requirements, making the noise issue even more prominent. this paper focuses on the trends of coupling and its effects on dynamic circuits. it presents closed form analytical solutions for noise, as well as noise tolerance metrics for dynamic circuits. these solutions are within 5% of dynamic simulations. it is shown that not all scaling trends are negative for noise, and that the scaling down of supply voltage and increasing frequency, help improve certain aspects of the noise immunity of dynamic circuits. most of the works treated the noise immunity and the noise content separately. this paper introduces an analysis of noise seatability by looking at the noise immunity and the noise content simultaneously.",
    "present_kp": [
      "dynamic circuits",
      "noise content",
      "noise immunity",
      "noise tolerance"
    ],
    "absent_kp": [
      "coupling noise",
      "noise scalability"
    ]
  },
  {
    "title": "envy, truth, and profit.",
    "abstract": "we consider profit maximizing (incentive compatible) mechanism design in general environments that include, e.g., position auctions (for selling advertisements on internet search engines) and single-minded combinatorial auctions. we analyze optimal envy-free pricings in these settings, and give economic justification for using the optimal revenue of envy-free pricings as a benchmark for prior-free mechanism design and analysis. moreover, we show that envy-free pricing has a simple nice structure and a strong connection to incentive compatible mechanism design, and we exploit this connection to design prior-free mechanisms with strong approximation guarantees.",
    "present_kp": [
      "prior-free",
      "envy-free pricing"
    ],
    "absent_kp": [
      "revenue maximization",
      "optimal auction",
      "truthful mechanisms"
    ]
  },
  {
    "title": "model checking one-dimensional cellular automata.",
    "abstract": "we show that the first order theory of a one-dimensional cellular automaton, construed as a structure with the global map and equality, is decidable. the argument employs bi-infinite versions of buchi automata that can also be used to demonstrate that the spectra of cellular automata oil finite grids are regular. for existential properties our method can be used to produce witnesses.",
    "present_kp": [
      "cellular automaton",
      "model checking"
    ],
    "absent_kp": [
      "first order logic",
      "decidability",
      "first order spectra"
    ]
  },
  {
    "title": "comparison of visualization of optimal clustering using self-organizing map and growing hierarchical self-organizing map in cellular manufacturing system.",
    "abstract": "we model visual clustering of machine-part cell formation using ghsom model. we examine the optimal ghsom map that helps manager to visualize optimum cell formation. we compare som and ghsom models based on the network architecture and goodness of cell formation to find the efficacy of the performance on a set of 15 benchmarked problems. ghsom algorithm concludes as the best model as it improves the gte performance measure for 75% of the cell formation problems than the som model and the other best models from the literature.",
    "present_kp": [
      "cellular manufacturing system",
      "visual clustering",
      "self-organizing map",
      "growing hierarchical self-organizing map"
    ],
    "absent_kp": [
      "operation sequence",
      "group technology efficiency"
    ]
  },
  {
    "title": "detecting non-ergodic simulation models of logistics networks.",
    "abstract": "simulation is a frequently applied method when analysing logistics networks. also within the collaborative research center 559 \"modelling of large logistics networks\" simulation is broadly applied and process chains are used as a mutual basis for model development and description. previous research activities exposed non-ergodicity of models as one of the typical application-specific problems which are difficult to discover by simulation. in order to detect non-ergodic models the problem has been reduced to its core employing the more analysis oriented modelling formalism of petri nets. with the help of the petri net formalism we developed an efficient method for the detection of non-ergodic models. since petri nets is not the common modelling paradigm for logisticians, this method had to be made available in the process chain modelling world of the logistics area, additionally supported by an appropriate tool. this paper describes our corresponding approach and also demonstrates the process of identifying a problem class in an application area, reducing it to its core, establishing a solution in an analysis-oriented formalism and making corresponding techniques available in the application-oriented modelling world and thus also available for the end-user.",
    "present_kp": [
      "logistics",
      "process chains",
      "ergodicity",
      "simulation",
      "petri nets"
    ],
    "absent_kp": []
  },
  {
    "title": "simd, smp and mimd-dm approaches for real-time 2d image stabilization.",
    "abstract": "we present a real-time image stabilization method, based on a 2d motion model, and exploiting different levels of parallelism in its implementation. this stabilization method is decomposed into three parts. first, the image matching is determined by a feature-based technique. in the second part, the motion between consecutive frames is estimated and filtered to extract the unwanted motion component. finally, these component is used to correct (warp) the images, resulting in a stable sequence. to validate our stabilization approach in a real-time on-board system context, the algorithm was implemented and tested over different hardware platforms, allowing a performance evaluation in function of the adopted architecture. in this paper, we present some results concerning the parallel implementation of the algorithm, using the simd altivec instructions set, a symmetric multi-processor architecture (smp) and a mimd-dm architecture.",
    "present_kp": [
      "2d image stabilization",
      "smp",
      "mimd-dm"
    ],
    "absent_kp": [
      "real-time application",
      "simd instructions"
    ]
  },
  {
    "title": "simulation modeling decision support through belief networks.",
    "abstract": "this paper presents an automated approach aimed at optimizing simulation model performance by means of intelligent agents and belief networks. the method described embeds intelligent agents in simulation models to conduct simulation real-time model evaluation. these agents can be considered intelligent observers placed within a model that inspect and make real-time decisions regarding the overall model performance. the knowledge encapsulation for the agents is provided via belief networks, which allow the agents to record their observations and make inferences. to exemplify this approach, a general-purpose resource allocation model describing an earthmoving operation is provided.",
    "present_kp": [
      "simulation modeling",
      "resource allocation",
      "belief network",
      "intelligent agents"
    ],
    "absent_kp": [
      "planning"
    ]
  },
  {
    "title": "a fast and stable algorithm for downdating the singular value decomposition.",
    "abstract": "in this paper, we modify a classical downdating svd algorithm and reduce its complexity significantly. we use a structured low-rank approximation algorithm to compute an hierarchically semiseparable (hss) matrix approximation to the eigenvector matrix of a diagonal matrix plus rank-one modification. the complexity of our downdating algorithm is analyzed. we further show that the structured low-rank approximation algorithm is backward stable. numerous experiments have been done to show the efficiency of our algorithm. for some matrices with large dimensions, our algorithm can be much faster than that using plain matrixmatrix multiplication routine in intel mkl in both sequential and parallel cases.",
    "present_kp": [
      "downdating svd"
    ],
    "absent_kp": [
      "hss matrices",
      "cauchy-like matrices",
      "numerical stability"
    ]
  },
  {
    "title": "augmented reality vehicle system: left-turn maneuver study.",
    "abstract": "augmented reality ar is a promising paradigm that can offer users with real-time, high-quality visualization of a wide variety of information. in ar, virtual objects are added to the real-world view in real time. the ar technology can offer a very realistic environment for enhancing drivers performance on the road and testing drivers ability to react to different road design and traffic operations scenarios. this can be achieved by adding virtual objects (people, vehicles, hazards, and other objects) to the normal view while driving an actual vehicle in a real environment. this paper explores a new augmented reality vehicle arv system and attempts to apply this new concept to a selected traffic engineering application namely the left-turn maneuver at two-way stop-controlled twsc intersection. this twsc intersection experiment, in addition to testing the feasibility of the application, tries to quantify the size of gaps accepted by different drivers characteristics (age and gender). the arv system can be installed in any vehicle where the driver can see the surrounding environment through a head mounted display hmd and virtual objects are generated through a computer and added to the scene. these different environments are generated using a well defined set of scenarios. the results from this study supported the feasibility and validity of the proposed arv system and they showed promise for this system to be used in the field-testing for the safety and operation aspects of transportation research. results of the left-turn maneuver study revealed that participants accepted gaps in the range of 4.09.0s. this finding implies that all gaps below 4s are rejected and all gaps above 9s are likely to be accepted. the mean value of the left-turn time was 4.67s which is a little bit higher than reported values in the literature (4.04.3s). older drivers were found to select larger gaps to make left turns than younger drivers. the conservative driving attitude of older drivers indicates the potential presence of reduced driving ability of elderly. drivers characteristics (age and gender) did not significantly affect the left-turn time. based on the survey questions that were handed to participants, most participants indicated good level of comfort with none or small level of risk while driving the vehicle with the arv system. none of the participants felt any kind of motion sickness and the participants answers indicated a good visibility and realism of the scene with overall good system fidelity.",
    "present_kp": [
      "augmented reality"
    ],
    "absent_kp": [
      "intelligent transportation system",
      "left turn maneuver",
      "gap acceptance"
    ]
  },
  {
    "title": "granulometric analysis of corneal endothelium specular images by using a germgrain model.",
    "abstract": "specular microscopy is widely used to study the human corneal endothelium status in vivo. in this paper, the corneal endothelium is represented as a binary image composed of the cell inscribed circles. the granulometric distribution function of the complement of this image is used as a functional descriptor, which provides information about the shape, size and spatial arrangement of cells. experimental evaluation using bootstrap techniques shows its ability to discriminate between controls and pathological cases. it represents a reliable and graphical alternative to the classical indices (cell density, hexagonality and coefficient of variation of cell areas), which behave poorly when detecting subtle abnormalities.",
    "present_kp": [
      "human corneal endothelium",
      "germgrain model"
    ],
    "absent_kp": [
      "granulometry",
      "shape analysis",
      "texture analysis"
    ]
  },
  {
    "title": "maximum studentized score tests for the detection of outliers in time series regression models.",
    "abstract": "efficient score tests exist among others, for testing the presence of additive and/or innovative outliers that are the result of the shifted mean of the error process under the regression model. a sample influence function of autocorrelation-based diagnostic technique also exists for the detection of outliers that are the result of the shifted autocorrelations. the later diagnostic technique is however not useful if the outlying observation does not affect the autocorrelation structure but is generated due to an inflation in the variance of the error process under the regression model. in this paper, we develop a unified maximum studentized type test which is applicable for testing the additive and innovative outliers as well as variance shifted outliers that may or may not affect the autocorrelation structure of the outlier free time series observations. since the computation of the p-values for the maximum studentized type test is not easy in general, we propose a satterthwaite type approximation based on suitable doubly non-central f-distributions for finding such p-values . the approximations are evaluated through a simulation study, for example, for the detection of additive and innovative outliers as well as variance shifted outliers that do not affect the autocorrelation structure of the outlier free time series observations. some simulation results on model misspecification effects on outlier detection are also provided.",
    "present_kp": [
      "outliers",
      "sample influence function",
      "score test"
    ],
    "absent_kp": [
      "autocorrelated errors",
      "shifted normal mean and variance",
      "p-value of the test"
    ]
  },
  {
    "title": "cursor movement control development by using anfis algorithm.",
    "abstract": "our non-invasive brain computer interface uses eeg signals and beta frequency bands over sensorimotor cortex to control cursor movement horizontally (i.e., one-dimension). the main goal of this study is to help people with sever motor disabilities (i.e., spinal cord injuries) and pro vide them a new way of communication and control options by which they can move the cursor in one dimension. in this study, offline analysis of the data collected was used to make the user able controlling the movement of the cursor horizontally (i.e., one dimension). the data was collected during a session in which the user selected among two targets by thinking and moving either the right hand little finger or the left hand little finger. the adaptive-network based fuzzy inference system algorithm was examined for the classification method with some parameters. in the offline analysis, the method used showed a significant performance in the classification accuracy level and it gave an accuracy level of more than 80%. this result suggests that using the adoptive-network based fuzzy inferences system algorithm will improve online operation of the current bci system.",
    "present_kp": [
      "anfis algorithm"
    ],
    "absent_kp": [
      "brain-computer interface",
      "fuzzy logic",
      "electroencephalogram"
    ]
  },
  {
    "title": "images of health technology in national and local strategies.",
    "abstract": "objectives: this paper examines the potential of various models relating technology to society and institutional structures to inform health policy. among the models discussed are various versions of technological determinism, social constructivism, actor network theory and critical theory. methods: the paper considers recent developments in policy and strategy that aim to shape the way the uk's national health service (nhs) integrates information and communication technologies (icts) into health care and considers what these alternative models highlight or emphasise, and how they might influence the activities of setting local implementation strategies. results and conclusions: contemporary icts are often presented as having a particular relevance and power in reforming or transforming the delivery of health care. understanding how such technologies might be conceived of, implemented and become an integral part of some future health care system is an important and challenging task that requires innovative theoretical treatments.",
    "present_kp": [
      "health policy",
      "local implementation strategies"
    ],
    "absent_kp": [
      "social shaping of technology",
      "actor-network theory",
      "nhs information technology policy"
    ]
  },
  {
    "title": "random heuristic search: applications to gas and functions of unitation.",
    "abstract": "describing a wide range of search methods at various levels of detail, the theory of random heuristic search speaks of their qualitative and quantitative aspects. this paper begins by outlining the theory, reviewing some of the more basic principles and results, and then goes on to illustrate its application by presenting both fine-grained and coarse-grained models for a genetic algorithm applied to functions of unitation. particular emphasis is given to the interrelationships between the models.",
    "present_kp": [
      "random heuristic search",
      "functions of unitation"
    ],
    "absent_kp": [
      "modeling evolutionary algorithms"
    ]
  },
  {
    "title": "evolving model trees for mining data sets with continuous-valued classes.",
    "abstract": "this paper presents a genetic programming (gp) approach to extract symbolic rules from data sets with continuous-valued classes, called gpmcc. the gpmcc makes use of a genetic algorithm (ga) to evolve multi-variate non-linear models at the terminal nodes of the gp. several mechanisms have been developed to optimise the gp, including a fragment pool of candidate non-linear models, k-means clustering of the training data to facilitate the use of stratified sampling methods, and specialized mutation and crossover operators to evolve structurally optimal and accurate models. it is shown that the gpmcc is insensitive to control parameter values. experimental results show that the accuracy of the gpmcc is comparable to that of neurolinear and cubist, while producing significantly less rules with less complex antecedents.",
    "present_kp": [
      "continuous-valued classes",
      "genetic programming",
      "model trees"
    ],
    "absent_kp": [
      "data mining"
    ]
  },
  {
    "title": "key-dependency for a wavelet-based blind watermarking algorithm.",
    "abstract": "when a host image is watermarked multiple times by the same algorithm collisions can occur. this makes it difficult for an image to host multiple watermarks. but this hosting is necessary for an image distribution chain, where several persons all watermark the same image. wavelet domain transformations provide several possibilities to customize the transformation process. we discuss the applicability of the methods of wavelet filter parametrization and wavelet packet decomposition for secret watermark embedding on the algorithm of dugad et al. we conclude that filter parametrization is not suited while wavelet packet decomposition shows good results.",
    "present_kp": [
      "blind watermarking"
    ],
    "absent_kp": [
      "wavelet packets",
      "parameterized wavelet filters",
      "multiple watermarking"
    ]
  },
  {
    "title": "finitely additive extensions of distribution functions and moment sequences: the coherent lower prevision approach.",
    "abstract": "we study the information that a distribution function provides about the finitely additive probability measure inducing it. we show that in general there is an infinite number of finitely additive probabilities associated with the same distribution function. secondly, we investigate the relationship between a distribution function and its given sequence of moments. we provide formulae for the sets of distribution functions, and finitely additive probabilities, associated with some moment sequence, and determine under which conditions the moments determine the distribution function uniquely. we show that all these problems can be addressed efficiently using the theory of coherent lower previsions.",
    "present_kp": [
      "coherent lower prevision",
      "moment sequence"
    ],
    "absent_kp": [
      "lower distribution function",
      "lower riemann-stieltjes integral",
      "complete monotonicity"
    ]
  },
  {
    "title": "an application of heterogeneous agents to fabricate large, realistic corporate transaction data sets for data mining tool testing and evaluation.",
    "abstract": "we describe methods used to specify and instantiate hundreds of heterogeneous agents and their use in a simulation of national trade and shipping. the agents, representing synthetic corporate entities, interacted to produce hundreds of thousands of trade transaction documents. the goal for the system was for the corpus of documents to evidence diverse, but realistic linkage patterns of corporate entities engaged in emergent shipping behaviors. we then used the documents to test and evaluate a data mining tool that purported to be able to detect these types of behavioral patterns. our contributions include a design algorithm for a heterogeneous mas that produces multi-featured outcomes, and a method for instantiating realistic heritages and preferences of agents that extends recent work in heterogeneous random utility modeling.",
    "present_kp": [],
    "absent_kp": [
      "lifelike and believable qualities",
      "synthetic agents: human-like",
      "multi-agent simulation and modeling",
      "agents and complex systems"
    ]
  },
  {
    "title": "comparative study of approximation algorithms and heuristics for sinr scheduling with power control.",
    "abstract": "various recent theoretical studies have achieved considerable progress in understanding combined link scheduling and power control in wireless networks with sinr constraints. these analyses were mainly focused on designing and analyzing approximation algorithms with provable approximation guarantees. while these studies revealed interesting effects from a theoretical perspective, so far there has not been a systematic evaluation of the theoretical results in simulations. in this paper, we examine the performance of various approximation algorithms and heuristics for the common scheduling problems on instances generated by different random network models, e.g., taking clustering effects into account. using (mixed) integer linear programming, we are able to compute the theoretical optima for some of these instances such that the performance of the different algorithms can be compared with these optima. the simulations support the practical relevance of the theoretical findings. for example, setting transmission powers by a square-root power assignment, the network's capacity increases significantly in comparison to uniform power assignments. furthermore, the developed approximation algorithms are able to exploit this gap providing in general a better performance than any algorithm using uniform transmission powers, even with unlimited computational power. the obtained results are robust against changes in parameters and network generation models.",
    "present_kp": [
      "sinr",
      "link scheduling",
      "simulations",
      "approximation algorithms",
      "power control"
    ],
    "absent_kp": []
  },
  {
    "title": "robust design optimization by polynomial dimensional decomposition.",
    "abstract": "this paper introduces four new methods for robust design optimization (rdo) of complex engineering systems. the methods involve polynomial dimensional decomposition (pdd) of a high-dimensional stochastic response for statistical moment analysis, a novel integration of pdd and score functions for calculating the second-moment sensitivities with respect to the design variables, and standard gradient-based optimization algorithms. new closed-form formulae are presented for the design sensitivities that are simultaneously determined along with the moments. the methods depend on how statistical moment and sensitivity analyses are dovetailed with an optimization algorithm, encompassing direct, single-step, sequential, and multi-point single-step design processes. numerical results indicate that the proposed methods provide accurate and computationally efficient optimal solutions of rdo problems, including an industrial-scale lever arm design.",
    "present_kp": [
      "score functions",
      "optimization"
    ],
    "absent_kp": [
      "design under uncertainty",
      "anova dimensional decomposition",
      "orthogonal polynomials"
    ]
  },
  {
    "title": "circadian rhythm changes in heart rate variability during chronic sound stress.",
    "abstract": "to study the circadian rhythm changes of the heart rate variability (hrv) during chronic sound stress, wistar rats were implanted with telemetry transmitters and exposed to chronic ultrasound stress for 14 days. the heart rate, mean r-r intervals (mean r-r) and body temperature were monitored hourly. the spectra of five-minute heart rate variability were plotted on a log-log scale of frequency versus power spectral density, and the spectral exponent beta of the regression line of this plot was calculated. the exponent beta, heart rate (hr) and body temperature recorded hourly were plotted and fitted to sine curves to observe the circadian rhythm of these parameters. the correlation coefficient of the fitted sine curves in beta decreased from 0.644 in the control period to 0.105 in the stress period, indicating that the rhythm of beta deceased during stress. this did not occur in the other two parameters, demonstrating that the hrv exponent can assess chronic stress.",
    "present_kp": [
      "heart rate variability",
      "chronic sound stress"
    ],
    "absent_kp": [
      "1/f fluctuations",
      "telemetry system"
    ]
  },
  {
    "title": "editable polycube map for gpu-based subdivision surfaces.",
    "abstract": "in this paper we propose an editable polycube mapping method that, given an arbitrary high-resolution polygonal mesh and a simple polycube representation plus optional sketched features indicating relevant correspondences between the two, provides a uniform, regular and artist-controllable quads-only mesh with a parameterized subdivision scheme. the method introduces a global parameterization, based on a divide and conquer strategy, which allows to create polycube-maps with a much smaller number of patches, and gives much more control over the quality of the induced subdivision surface. all this makes it practical for real-time rendering on modern hardware (e.g. ogl 4.1 and d3d11 tessellation hardware). by sketching these correspondence features, processing large-scale models with complex geometry and topology is now feasible. this is crucial for obtaining watertight displaced catmull-clark subdivision surfaces and high-quality texturing on real-time applications.",
    "present_kp": [
      "polycube map"
    ],
    "absent_kp": [
      "gpu subdivision surface",
      "digital geometry processing",
      "surface parameterization"
    ]
  },
  {
    "title": "beyond schema evolution to database reorganization.",
    "abstract": "while the contents of databases can be easily changed, their organization is typically extremely rigid. some databases relax the rigidity of database organization somewhat by supporting simple changes to individual schemas. as described in this paper, otgen supports not only more complex schema changes, but also database reorganization. a database administrator uses a declarative notation to describe mappings between objects created with old versions of schemas and their corresponding representations using new versions. otgen generates a transformer that applies the mappings to update the database to the new definitions, thus facilitating improvements in performance, functionality, and usability of the database. 1",
    "present_kp": [
      "organization",
      "usability",
      "evolution",
      "functional",
      "performance",
      "definition",
      "object",
      "map",
      "schema",
      "version",
      "paper",
      "database",
      "update",
      "change"
    ],
    "absent_kp": [
      "contention",
      "complexity"
    ]
  },
  {
    "title": "motor pattern selection by combinatorial code of interneuronal pathways.",
    "abstract": "we use a modeling approach to examine ideas derived from physiological network analyses, pertaining to the switch of a motor control network between two opposite control modes. we studied the femurtibia joint control system of the insect leg, and its switch between resistance reflex in posture control and active reaction in walking, both elicited by the same sensory input. the femurtibia network was modeled by fitting the responses of model neurons to those obtained in animals. the strengths of 16 interneuronal pathways that integrate sensory input were then assigned three different values and varied independently, generating a database of more than 43 million network variants. we demonstrate that the same neural network can produce the two different behaviors, depending on the combinatorial code of interneuronal pathways. that is, a switch between behaviors, such as standing to walking, can be brought about by altering the strengths of selected sensory integration pathways.",
    "present_kp": [
      "posture control",
      "combinatorial code"
    ],
    "absent_kp": [
      "physiological simulations",
      "exhaustive search",
      "stick insect",
      "movement control",
      "neuronal network",
      "sensory feedback"
    ]
  },
  {
    "title": "animal models got you puzzled?: think pig.",
    "abstract": "swine are an excellent large animal model for human health and disease because their size and physiology are similar to humans, in particular, with respect to the skin, heart, gastrointestinal tract, and kidneys. in addition, the pig has many emerging technologies that will only enhance the development of the pig as the nonrodent biomedical model of choice.",
    "present_kp": [
      "pig"
    ],
    "absent_kp": [
      "genetically modified",
      "genome",
      "toxicology",
      "vinclozolin"
    ]
  },
  {
    "title": "multimodal neuroimaging computing: a review of the applications in neuropsychiatric disorders.",
    "abstract": "multimodal neuroimaging is increasingly used in neuroscience research, as it overcomes the limitations of individual modalities. one of the most important applications of multimodal neuroimaging is the provision of vital diagnostic data for neuropsychiatric disorders. multimodal neuroimaging computing enables the visualization and quantitative analysis of the alterations in brain structure and function, and has reshaped how neuroscience research is carried out. research in this area is growing exponentially, and so it is an appropriate time to review the current and future development of this emerging area. hence, in this paper, we review the recent advances in multimodal neuroimaging (mri, pet) and electrophysiological (eeg, meg) technologies, and their applications to the neuropsychiatric disorders. we also outline some future directions for multimodal neuroimaging where researchers will design more advanced methods and models for neuropsychiatric research.",
    "present_kp": [
      "multimodal",
      "neuroimaging",
      "neuropsychiatric"
    ],
    "absent_kp": []
  },
  {
    "title": "an ic manufacturing yield model considering intra-die variations.",
    "abstract": "in deep submicron feature sizes continue to shrink aggressively beyond the natural capabilities of the 193 nm lithography used to produce those features thanks to all the innovations in the field of resolution enhancement techniques (ret). with reduced feature sizes and tighter pitches die level variations become an increasingly dominant factor in determining manufacturing yield. thus a prediction of design-specific features that impact intra-die variability and correspondingly its yield is extremely valuable as it allows for altering such features in a manner that reduces intra-die variability and improves yield. in this paper, a manufacturing yield model which takes into account both physical layout features and manufacturing fluctuations is proposed. the intra-die systematic variations are evaluated using a physics-based model as a function of a design's physical layout. the random variations and their across-die spatial correlations are obtained from data harvested from manufactured test structures. an efficient algorithm is proposed to reduce the order of the numerical integration in the yield model. the model can be used to (i) predict manufacturing yields at the design stage and (ii) enhance the layout of a design for higher manufacturing yield.",
    "present_kp": [
      "random variation",
      "manufacturing yield",
      "systematic variation",
      "spatial correlation"
    ],
    "absent_kp": [
      "cmp"
    ]
  },
  {
    "title": "on selecting an optimal wavelet for detecting singularities in traffic and vehicular data.",
    "abstract": "serving as a powerful tool for extracting localized variations in non-stationary signals, applications of wavelet transforms (wts) in traffic engineering have been introduced; however, lacking in some important theoretical fundamentals. in particular, there is little guidance provided on selecting an appropriate wt across potential transport applications. this research described in this paper contributes uniquely to the literature by first describing a numerical experiment to demonstrate the shortcomings of commonly-used data processing techniques in traffic engineering (i.e., averaging, moving averaging, second-order difference, oblique cumulative curve, and short-time fourier transform). it then mathematically describes wts ability to detect singularities in traffic data. next, selecting a suitable wt for a particular research topic in traffic engineering is discussed in detail by objectively and quantitatively comparing candidate wavelets performances using a numerical experiment. finally, based on several case studies using both loop detector data and vehicle trajectories, it is shown that selecting a suitable wavelet largely depends on the specific research topic, and that the mexican hat wavelet generally gives a satisfactory performance in detecting singularities in traffic and vehicular data.",
    "present_kp": [
      "wavelet transform",
      "the mexican hat wavelet",
      "short-time fourier transform",
      "oblique cumulative curve"
    ],
    "absent_kp": [
      "singularity detection",
      "traffic data analysis"
    ]
  },
  {
    "title": "haskell program coverage.",
    "abstract": "we describe the design, implementation and use of hpc, a tool-kit to record and display haskell program coverage. hpc includes tools that instrument haskell programs to record program coverage, run instrumented programs, and display information derived from coverage data in various ways.",
    "present_kp": [
      "haskell"
    ],
    "absent_kp": [
      "software engineering",
      "code coverage"
    ]
  },
  {
    "title": "the zeta-image, illuminant estimation, and specularity manipulation.",
    "abstract": "a novel log-chromaticity illumination constraint we call the zeta-image. the new feature is equivalent to a novel application of the kullbackleibler divergence. for illumination estimation the method outperforms other unsupervised methods. post-processing any color constancy method, that methods accuracy is improved. using the zeta-image we are able to manipulate specular content in the image.",
    "present_kp": [
      "specular",
      "illuminant estimation",
      "color constancy",
      "zeta-image"
    ],
    "absent_kp": []
  },
  {
    "title": "binary synthesis.",
    "abstract": "recent high-level synthesis approaches and c-based hardware description languages attempt to improve the hardware design process by allowing developers to capture desired hardware functionality in a well-known high-level source language. however, these approaches have yet to achieve wide commercial success due in part to the difficulty of incorporating such approaches into software tool flows. the requirement of using a specific language, compiler, or development environment may cause many software developers to resist such approaches due to the difficulty and possible instability of changing well-established robust tool flows. thus, in the past several years, synthesis from binaries has been introduced, both in research and in commercial tools, as a means of better integrating with tool flows by supporting all high-level languages and software compilers. binary synthesis can be more easily integrated into a software development tool-flow by only requiring an additional backend tool, and it even enables completely transparent dynamic translation of executing binaries to configurable hardware circuits. in this article, we survey the key technologies underlying the important emerging field of binary synthesis. we compare binary synthesis to several related areas of research, and we then describe the key technologies required for effective binary synthesis: decompilation techniques necessary for binary synthesis to achieve results competitive with source-level synthesis, hardware/software partitioning methods necessary to find critical binary regions suitable for synthesis, synthesis methods for converting regions to custom circuits, and binary update methods that enable replacement of critical binary regions by circuits.",
    "present_kp": [
      "design",
      "binary synthesis",
      "hardware/software partitioning"
    ],
    "absent_kp": [
      "performance",
      "synthesis from software binaries",
      "hardware/software codesign",
      "fpga",
      "configurable logic",
      "warp processors"
    ]
  },
  {
    "title": "a note on the existence and uniqueness of mild solutions to neutral stochastic partial functional differential equations with non-lipschitz coefficients.",
    "abstract": "in this note, we study the existence and uniqueness of mild solutions to neutral stochastic partial functional differential equations under some carathodory-type conditions on the coefficients by means of the successive approximation. in particular, we generalize and improve the results that appeared in govindan and bao and hou .",
    "present_kp": [
      "mild solution",
      "neutral stochastic partial functional differential equations"
    ],
    "absent_kp": [
      "carathodory condition",
      "non-lipschitz condition"
    ]
  },
  {
    "title": "patinformatics: tasks to tools.",
    "abstract": "this article starts with an overview of the field of patinformaticsthe science of analyzing patent information to discover relationships and trends. this is followed by a survey of many common analysis tasks in this field, and many of the software tools available to tackle these tasks. the survey is set out under the tasks of list cleanup and grouping of concepts; list generation; co-occurrency matrices and circle graphs; clustering of structured data; clustering of unstructured data; mapping document clusters; adding temporal component to cluster map; citation analysis; subject/action/object functions. the author concludes that patinformatics has developed very rapidly over the last few years, and provides continuing challenges and opportunities in making optimal use of the resources available to achieve reliable and meaningful results. useful tables summarizing aspects of this survey are included.",
    "present_kp": [
      "patinformatics",
      "list cleanup",
      "list generation",
      "co-occurrency matrices",
      "circle graphs",
      "mapping document clusters",
      "citation analysis"
    ],
    "absent_kp": [
      "patent information analysis",
      "software analysis tools",
      "patent intelligence",
      "concept grouping",
      "data clustering"
    ]
  },
  {
    "title": "modular static scheduling of synchronous data-flow networks.",
    "abstract": "this paper addresses the question of producing modular sequential imperative code from synchronous data-flow networks. precisely, given a system with several input and output flows, how to decompose it into a minimal number of classes executed atomically and statically scheduled without restricting possible feedback loops between input and output? though this question has been identified by raymond in the early years of lustre, it has almost been left aside until the recent work of lublinerman, szegedy and tripakis. the problem is proven to be intractable, in the sense that it belongs to the family of optimization problems where the corresponding decision problem-there exists a solution with size c-is np-complete. then, the authors derive an iterative algorithm looking for solutions for c=1,2,aeuro broken vertical bar where each step is encoded as a satisfiability (sat) problem. despite the apparent intractability of the problem, our experience is that real programs do not exhibit such a complexity. based on earlier work by raymond, the current paper presents a new encoding of the problem in terms of input/output relations. this encoding simplifies the problem, in the sense that it rejects some solutions, while keeping all the optimal ones. it allows, in polynomial time, (1) to identify nodes for which several schedules are feasible and thus are possible sources of combinatorial explosion; (2) to obtain solutions which in some cases are already optimal; (3) otherwise, to get a non trivial lower bound for c to start an iterative combinatorial search. the method has been validated on several industrial examples. the solution applies to a large class of block-diagram formalisms based on atomic computations and a delay operator, ranging from synchronous languages such as lustre or scade to modeling tools such as simulink.",
    "present_kp": [
      "synchronous languages"
    ],
    "absent_kp": [
      "real-time systems",
      "block-diagrams",
      "compilation",
      "np-completeness",
      "partial orders",
      "preorders"
    ]
  },
  {
    "title": "exact and approximate construction of offset polygons.",
    "abstract": "the minkowski sum of two sets a,b?r2 a , b ? r 2 , denoted a?b a ? b , is defined as {a+b?a?a,b?b} { a + b ? a ? a , b ? b } . we describe an efficient and robust implementation of the construction of the minkowski sum of a polygon in r2 r 2 with a disc, an operation known as offsetting the polygon. our software package includes a procedure for computing the exact offset of a straight-edge polygon, based on the arrangement of conic arcs computed using exact algebraic number-types. we also present a conservative approximation algorithm for offset computation that uses only rational arithmetic and decreases the running times by an order of magnitude in some cases, while having a guarantee on the quality of the result. the package will be included in the next public release of the computational geometry algorithms library, cgalversion 3.3. it also integrates well with other cgalpackages; in particular, it is possible to perform regularized boolean set-operations on the polygons the offset procedures generate.",
    "present_kp": [
      "offset polygons"
    ],
    "absent_kp": [
      "minkowski sums",
      "exact geometric computation",
      "robustness"
    ]
  },
  {
    "title": "ontology based affective context representation.",
    "abstract": "in this paper we propose an ontology based representation of the affective states for context aware applications that allows expressing the complex relations that are among the affective states and between these and the other context elements. this representation is open to map different affective spaces; basic and secondary states relation (using fuzzy logic), the relation between these states and other context elements as location, time, person, activity etc. the proposed affective context model is encoded in owl. due to difficulties in direct detection of the secondary affective states we propose a method to infer the characteristic values of these states from other context elements values. the deduces states are used here to improve the behavior of a context aware museum guide in order to react more intuitively and more intelligent by taking into account the users affective states.",
    "present_kp": [
      "ontology"
    ],
    "absent_kp": [
      "affective computing",
      "context awareness",
      "logical inference"
    ]
  },
  {
    "title": "biased mutation operators for subgraph-selection problems.",
    "abstract": "many graph problems seek subgraphs of minimum weight that satisfy a set of constraints. examples include the minimum spanning tree problem (mstp), the degree-constrained minimum spanning tree problem (d-mstp), and the traveling salesman problem (tsp). low-weight edges predominate in optimum solutions to such problems, and the performance of evolutionary algorithms (eas) is often improved by biasing variation operators to favor these edges. we investigate the impact of biased edge-exchange mutation. in a large-scale empirical investigation on euclidean and uniform random instances, we describe the distributions of edges in optimum solutions of the mstp, the d-mstp, and the tsp in terms of the edges' weight-based ranks. we approximate these distributions by exponential functions and derive approximately optimal probabilities for selecting edges to be incorporated into candidate solutions during mutation. a theoretical analysis of the expected running time of a (1 + 1)-ea on nondegenerate instances of the mstp shows that when using the derived probabilities for edge selection in mutation, the (1 + 1)-ea is asymptotically as fast as a classical implementation of kruskal's minimum spanning tree algorithm. in experiments on the mstp, d-mstp, and the tsp, we compare the new edge-selection strategy to four alternative methods. the results of a (1 + 1)-ea on instances of the mstp support the theory and indicate that the new strategy is superior to the other methods in practice. on instances of the d-mstp, a more sophisticated ea with a larger population and unbiased recombination performs better with the new biased mutation than with alternate mutations. on the tsp, the advantages of weight-biased mutation are generally smaller, because the insertion of a specific new edge into a tour requires the insertion of a second dependent edge as well. although we considered euclidean and uniform random instances only, we conjecture that the same biasing toward low-weight edges also works well on other instance classes structured in different ways.",
    "present_kp": [
      "graph problems",
      "minimum spanning tree problem ",
      "mutation",
      "traveling salesman problem "
    ],
    "absent_kp": [
      "biased operators"
    ]
  },
  {
    "title": "discrete approximations to real-valued leaf sequencing problems in radiation therapy.",
    "abstract": "for a given mn m n nonnegative real matrix a , a segmentation with 1-norm relative error e is a set of pairs (?,s)={(?1,s1),(?2,s2),,(?k,sk)} ( ? , s ) = { ( ? 1 , s 1 ) , ( ? 2 , s 2 ) , , ( ? k , s k ) } , where each ?i ? i is a positive number and si s i is an mn m n binary matrix, and e = | a i = 1 k ? i s i | 1 / | a | 1 , where |a|1 | a | 1 is the 1-norm of a vector which consists of all the entries of the matrix a . in certain radiation therapy applications, given a and positive scalars ?, , ? , we consider the optimization problem of finding a segmentation (?,s) ( ? , s ) that minimizes z = ? i = 1 k ? i + ? k + ? e subject to certain constraints on si s i . this problem poses a major challenge in preparing a clinically acceptable treatment plan for intensity modulated radiation therapy (imrt) and is known to be np-hard. known discrete imrt algorithms use alternative objectives for this problem and an l-level entrywise approximation a ? (i.e.each entry in a is approximated by the closest entry in a set of l equally-spaced integers), and produce a segmentation that satisfies a ? = ? i = 1 k i s i . in this paper we present two algorithms that focus on the original non-discretized intensity matrix and consider measures of delivery quality and complexity ( i+?k) ( i + ? k ) as well as approximation error e . the first algorithm uses a set partitioning approach to approximate a by a matrix a ? that leads to segmentations with smaller k for a given e . the second algorithm uses a constrained least square approach to post-process a segmentation { ( i , s i ) } of a ? to replace i with real-valued ?i ? i in order to reduce k and e .",
    "present_kp": [
      "leaf sequencing problems",
      "imrt",
      "radiation"
    ],
    "absent_kp": []
  },
  {
    "title": "gpu-based computation of discrete periodic centroidal voronoi tessellation in hyperbolic space.",
    "abstract": "periodic centroidal voronoi tessellation (cvt) in hyperbolic space provides a nice theoretical framework for computing the constrained cvt on high-genus (genus>1) surfaces. this paper addresses two computational issues related to such a hyperbolic cvt framework: (1) efficient reduction of unnecessary site copies in neighbor domains on the universal covering space, based on two special rules; (2) gpu-based parallel algorithms to compute a discrete version of the hyperbolic cvt. our experiments show that with the dramatically reduced number of unnecessary site copies in neighbor domains and the gpu-based parallel algorithms, we significantly speed up the computation of cvt for high-genus surfaces. the proposed discrete hyperbolic cvt guarantees to converge and produces high-quality results.",
    "present_kp": [
      "centroidal voronoi tessellation",
      "universal covering space",
      "hyperbolic space"
    ],
    "absent_kp": [
      "gpu algorithm"
    ]
  },
  {
    "title": "efficient transient analysis of markovian models using a block reduction approach.",
    "abstract": "one of the most widely used techniques to obtain transient measures is the uniformization method. however, although uniformization has many advantages, the computational cost required to calculate transient probabilities is very large for stiff models. we study efficient solutions that can be applied to an approximate method developed for calculating transient state probabilities of markov models and cumulative expected reward measures over a finite interval. our work is based on a method that approximates the state probabilities at time t by the state probabilities calculated at a random time with erlangian distribution. the original method requires an inversion of a matrix obtained from the state transition rate matrix that destroys special structures such as sparseness and banded matrices. this precludes the use of the technique for large models. in our work we propose efficient solutions that can take advantage of special structures. finally, we present examples that show that the proposed technique is computationally very efficient for stiff models when compared with uniformization.",
    "present_kp": [
      "transient analysis",
      "uniformization"
    ],
    "absent_kp": [
      "performability",
      "reward models",
      "performance models"
    ]
  },
  {
    "title": "feature selection for improved 3d facial expression recognition.",
    "abstract": "an entropy based feature selection process for 3d facial expression recognition is proposed. mpeg-4 facial definition parameters are used as a base for feature selection. two-level svm classifier system is employed to classify six basic expressions of the face. tests are performed on bu-3dfe database and the system achieves 88% average recognition rate.",
    "present_kp": [
      "facial expression recognition"
    ],
    "absent_kp": [
      "facial expression analysis",
      "facial feature selection",
      "faces biometrics"
    ]
  },
  {
    "title": "a data-driven stochastic approach to model and analyze test data on fatigue response.",
    "abstract": "a stochastic approach to model and analyze test data on the fatigue response of materials and laminated composites is developed. the developed approach is data-driven in nature. it has been customary to describe the fatigue response of metallic and laminated composite materials using a suitable parameter that can serve as the indicator and descriptor of damage accumulation. in the present methodology, the fatigue response of the material is quantified by interpreting the corresponding material parameter to be an embedded markov process. the true probability distributions of the fatigue response parameter are extracted from sample test data based on an analytical approach, and they are used in the formulation. to this end, the maximum entropy method is incorporated into the formulation. a recursive stochastic matrix equation is developed based on the test data using the theory of reliability and fokkerplankkolmogorov equation. application of the methodology to a composite laminate is demonstrated.",
    "present_kp": [
      "laminated composites",
      "reliability"
    ],
    "absent_kp": [
      "fatigue behavior",
      "failure of materials",
      "stochastic process modeling",
      "markov chains"
    ]
  },
  {
    "title": "analyzing requirements - evolution in engineering design using the method of problem-reduction.",
    "abstract": "traditional requirements definition activities begin with the engineer or design team performing a needs-analysis to identify user requirements . while recent studies have focused on conceptual design activities, research into the requirements-definition pro cess has for the most part been lacking. needs-analysis is generally subjective, and varies according to the composition and experience of the design team. systematic procedures for defining and ranking requirements could consolidate the foundation on which the design process is predicated and enhance its outcome by providing the designer with a consistent, reliable approach to product development . before such systematic procedures could be developed, it would be necessary to establish an understanding of the existing process by which requirements evolve. and to create a model for evaluating this process. therefore, a pilot study was conducted at stanford university using empirical data from an actual spaceflight experiment, sponsored by nasa ames research center (arc), and flown aboard the space shuttle. a large body of empirical evidence was examined, and on the basis of this evidence, the method of problem-reduction using and/or graphs proved to be an effective framework for analyzing requirements-evolution.",
    "present_kp": [
      "requirements",
      "needs-analysis",
      "problem-reduction"
    ],
    "absent_kp": [
      "design theory",
      "product design",
      "information modelling"
    ]
  },
  {
    "title": "choosing the optimal set of instruments from large instrument sets.",
    "abstract": "it is well known that instrumental variables (iv) estimation is sensitive to the choice of instruments both in small samples and asymptotically. recently, a simple method has been suggested in the literature for choosing the instrument set. the method involves minimising the approximate mean square error (mse) of a given iv estimator where the mse is obtained using refined asymptotic theory. an issue with this method is the fact that when considering large sets of valid instruments, it is not clear how to order the instruments in order to choose which ones ought to be included in the estimation. a possible solution to the problem using nonstandard optimisation algorithms is provided. the properties of the algorithms are discussed. a monte carlo study illustrates the potential of the new method.",
    "present_kp": [
      "instrumental variables",
      "mse"
    ],
    "absent_kp": [
      "simulated annealing",
      "genetic algorithms"
    ]
  },
  {
    "title": "a hybrid mac scheme to improve the transmission performance in body sensor networks.",
    "abstract": "wireless body sensor networks (wbsns) constitute a key technology for closing the loop between patients and healthcare providers, as wbsns provide sensing ability, as well as mobility and portability, essential characteristics for wide acceptance of wireless healthcare technology. however, one important and difficult aspect of wbsns is to provide data transmissions with quality of service, among other factors due to the antennas being small size and placed close to the body. such transmissions cannot be fully provided without the assumption of a mac protocol that solves the problems of the medium sharing. a vast number of mac protocols conceived for wireless networks are based on random or scheduled schemes. this paper studies firstly the suitability of two mac protocols, one using csma and the other tdma, to transmit directly to the base station the signals collected continuously from multiple sensor nodes placed on the human body. tests in a real scenario show that the beaconed tdma mac protocol presents an average packet loss ratio lower than csma. however, the average packet loss ratio is above 1.0%. to improve this performance, which is of vital importance in areas such as e-health and ambient assisted living, a hybrid tdma/csma scheme is proposed and tested in a real scenario with two wbsns and four sensor nodes per wbsn. an average packet loss ratio lower than 0.2% was obtained with the hybrid scheme. to achieve this significant improvement, the hybrid scheme uses a lightweight algorithm to control dynamically the start of the superframes. scalability and traffic rate variation tests show that this strategy allows approximately ten wbsns operating simultaneously without significant performance degradation.",
    "present_kp": [
      "wireless body sensor network",
      "mac protocol"
    ],
    "absent_kp": [
      "real performance tests"
    ]
  },
  {
    "title": "playing the language-games of design and use-on skill and participation.",
    "abstract": "this paper deals with computers and cooperative work. focus in not on applications for cooperative work, but on the cooperative process of designing such and other computer applications. focus is on the role of skill and participation in design as a creative and communicative process. the paper suggests a need to go beyond the cartesian philosophical assumptions of rationalistic reasoning as epistemology and dualism as ontology, so strongly embedded in traditional design methods. there are many philosophical candidates for such a reinterpretation. in this paper i have chosen to elaborate on language-games and the ordinary language philosophy of ludwig wittgenstein. hence, focus is on the shift in design from language as description towards language as action. some consequences of such a shift is illustrated with reflections on examples from utopia (a research and development project for skill enhancing computer based tools for graphic workers), and with design ideas on an application simulator from a new research programme on cooperative design and communication.",
    "present_kp": [
      "communication",
      "applications",
      "use",
      "examples",
      "cooperative design",
      "design",
      "project",
      "games",
      "creative",
      "paper",
      "role",
      "play",
      "reasoning",
      "research",
      "language",
      "cooperative work",
      "process",
      "skill",
      "design method",
      "action",
      "tools"
    ],
    "absent_kp": [
      "simulation",
      "developer",
      "computation",
      "graphics",
      "participant",
      "ontologies",
      "cooperation",
      "reflectance",
      "embedding",
      "programmer"
    ]
  },
  {
    "title": "co-sizing of an electromechanical device by using optimisation process.",
    "abstract": "purpose - this paper deals with the collaborative design of electromagnetic devices over the internet network. the design is made by both mechanical and electrical engineers. so, the paper tries to show the importance but also constraints to size such a system using a collaborative optimisation process. design/methodology/approach - the paper compares two approaches in order to size an electromechanical actuator between mechanical and electrical engineers. in the first one, each profession designs its part, and only common constrained are negotiated. this can result in a design process with many iterations. in the second one, electrical and mechanical engineers built together a common model of the structure and a common list of specifications: this allows a global optimisation that is more efficient. findings - the main result of the paper is that the second approach in which a global model is built between electrical and mechanical engineers is more efficient. originality/value - the originality of the paper is to explore the problems and difficulties of an optimisation of an electromechanical device between engineers of different culture working together over the internet network.",
    "present_kp": [
      "design"
    ],
    "absent_kp": [
      "optimization techniques",
      "electromagnetism",
      "electrically operated devices"
    ]
  },
  {
    "title": "an algorithm for trading off quantization error with hardware resources for matlab-based fpga design.",
    "abstract": "most practical fpga designs of digital signal processing (dsp) applications are limited to fixed-point arithmetic owing to the cost and complexity of floating-point hardware. while mapping dsp applications onto fpgas, a dsp algorithm designer must determine the dynamic range and desired precision of input, intermediate, and output signals in a design implementation. the first step in a matlab-based hardware design flow is the conversion of the floating-point matlab code into a fixed-point version using \"quantizers\" from the filter design and analysis (fda) toolbox for matlab. this paper describes an approach to automate the conversion of floating-point matlab programs into fixed-point matlab programs, for mapping to fpgas by profiling the expected inputs to estimate errors. our algorithm attempts to minimize the hardware resources while constraining the quantization error within a specified limit. experimental results on five matlab benchmarks are reported for xilinx virtex ii fpgas.",
    "present_kp": [
      "fixed-point arithmetic",
      "quantization"
    ],
    "absent_kp": [
      "automation",
      "field programmable gate arrays",
      "floating-point arithmetic"
    ]
  },
  {
    "title": "an aig-based qbf-solver using sat for preprocessing.",
    "abstract": "in this paper we present a solver for quantified boolean formulas (qbfs) which is based on and-inverter graphs (aigs). we use a new quantifier elimination method for aigs, which heuristically combines cofactor-based quantifier elimination with quantification using bdds and thus benefits from the strengths of both data structures. moreover, we present a novel sat-based method for preprocessing qbfs that is able to efficiently detect variables with forced truth assignments, allowing for an elimination of these variables from the input formula. we describe the used algorithm which heavily relies on the incremental features of modern sat-solvers. experimental results demonstrate that our preprocessing method can significantly improve the performance of qbf preprocessing and thus is able to accelerate the overall solving process when used in combination with state-of-the-art qbf-solvers. in particular, we integrated the preprocessing technique as well as the quantifier elimination method into the qbf-solver aigsolve , allowing it to outperform state-of-the-art solvers.",
    "present_kp": [
      "quantified boolean formulas"
    ],
    "absent_kp": [
      "boolean satisfiability"
    ]
  },
  {
    "title": "a proactive wireless self-protection system.",
    "abstract": "though mobile computing systems constitute the core of the next generation ubiquitous pervasive services, they still have many flaws in their security. this paper describes a novel framework for wireless anomaly based intrusion detection and response system, which is capable of detecting complex malicious attacks. this framework is based on multi-channel online monitoring and analysis of wireless network features with respect to multiple observation time windows. these features are related to data link layer frame behaviors and the mobility of stations. a general purpose wireless self-protection system (wsps) is presented. wsps has the following modules: wireless network probes, wireless features filtration and generation module, wireless network flow generator, behavior analysis module, and action module. wsps self protects against attacks by online monitoring and analyzing anomalies and misuses in the network features, and utilizes the low false alerts of the analysis module. the validation and effectiveness of this framework is carried out by experimenting with more than 20 different types of wireless attacks using wireless lans (wlans). our experimental results show that our approach can protect from wireless network attacks with average false-positive rate of 2.234%, and average detection rate of 99.13% for all the experimented attacks.",
    "present_kp": [
      "wireless",
      "security",
      "wsps",
      "mobile",
      "network"
    ],
    "absent_kp": [
      "wnetflow"
    ]
  },
  {
    "title": "data visualization optimization via computational modeling of perception.",
    "abstract": "we present a method for automatically evaluating and optimizing visualizations using a computational model of human vision. the method relies on a neural network simulation of early perceptual processing in the retina and primary visual cortex. the neural activity resulting from viewing flow visualizations is simulated and evaluated to produce a metric of visualization effectiveness. visualization optimization is achieved by applying this effectiveness metric as the utility function in a hill-climbing algorithm. we apply this method to the evaluation and optimization of 2d flow visualizations, using two visualization parameterizations: streaklet-based and pixel-based. an emergent property of the streaklet-based optimization is head-to-tail streaklet alignment. it had been previously hypothesized the effectiveness of head-to-tail alignment results from the perceptual processing of the visual system, but this theory had not been computationally modeled. a second optimization using a pixel-based parameterization resulted in a lic-like result. the implications in terms of the selection of primitives is discussed. we argue that computational models can be used for optimizing complex visualizations. in addition, we argue that they can provide a means of computationally evaluating perceptual theories of visualization, and as a method for quality control of display methods.",
    "present_kp": [
      "perception",
      "optimization"
    ],
    "absent_kp": [
      "information visualization",
      "human information processing",
      "neural nets"
    ]
  },
  {
    "title": "robust dissipative control for internet-based switching systems.",
    "abstract": "a class of hybrid multi-rate control models with time-delay and switching controllers are formulated based on combined remote control and local control strategies. the problem of robust dissipative control for this discrete system is investigated. an improved lyapunovkrasovskii functional is constructed and the subsequent analysis provides some new sufficient conditions in the form of lmis for both nominal and uncertain representations. several special cases of practical interests are derived. a numerical simulation example is given to illustrate the effectiveness of the theoretical result.",
    "present_kp": [
      "internet-based switching systems",
      "dissipative control",
      "switching controllers",
      "multi-rate control models",
      "lmis"
    ],
    "absent_kp": []
  },
  {
    "title": "estimating receptive fields in the presence of spike-time jitter.",
    "abstract": "neurons in sensory systems are commonly characterized by their receptive fields. these are experimentally often obtained by reverse-correlation analyses, for example, by calculating the spike-triggered average. the reverse-correlation approach, however, generally assumes a fixed temporal relation between spike-generating stimulus features and measured spikes. temporal jitter of spikes will therefore distort the estimated receptive fields. here, a novel extension of widely used reverse-correlation techniques (spike-triggered average as well as spike-triggered covariance) is presented that allows accurate measurements of receptive fields even in the presence of considerable spike-time jitter. it is shown that the method correctly recovers the receptive fields from simulated spike trains. when applied to recordings from auditory receptor cells of locusts, a considerable sharpening of receptive fields as compared to standard spike-triggered averages is observed. in addition, the multiple filters that are obtained from a conventional spike-triggered covariance analysis of these data can be collapsed into a single component if spike jitter is accounted for. finally, it is shown how further effects on spike timing, such as systematic shifts in spike latency, can be included in the approach.",
    "present_kp": [
      "receptive field",
      "spike timing",
      "jitter"
    ],
    "absent_kp": [
      "reverse correlation",
      "spike-triggered analysis"
    ]
  },
  {
    "title": "topological conceptual model of geological relative time scale for geoinformation systems.",
    "abstract": "in geological information systems, the methods of encoding geological age, both relative and absolute, play an important role. interoperable exchange of information between these systems requires application of solutions based upon recognized international standards. the iso 19108 standard can serve as a basis for elaboration of a conceptual model for geological data, as well as its implementation, for instance in extensible markup language (xml). the nature of the relative geological time scale requires application of topological elements in this model, and the model presented here is a complex construction of such elements. as a result, operations of the interface used for objects belonging to classes derived from the timetopologicalordinalera class may return a value in the form of topological time relations such as: before, after, during and overlappedby.",
    "present_kp": [],
    "absent_kp": [
      "geographic information system",
      "ordinal temporal reference system",
      "uml language",
      "xml language",
      "topological relation"
    ]
  },
  {
    "title": "the involvement of thyroid hormone metabolism in gilthead sea bream (sparus auratus) osmoregulation.",
    "abstract": "abstract: we have investigated the effect of adaptation to low salinity water on the thyroid status of the euryhaline teleost, sparus auratus. we show that, following low salinity adaptation, the plasma t4 concentration increases and branchial deiodination activities of t4, t3, and rt3 decrease. moreover, branchial and hepatic enzyme activities that are putatively involved in thyroid hormone metabolism respond differentially in low salinity conditions. our results indicate the involvement of thyroid hormones in sparus auratus osmoregulation. moreover, the gills appear well equipped to play an important role in the modulation of plasma thyroid hormone titers.",
    "present_kp": [
      "thyroid hormone",
      "osmoregulation",
      "sparus auratus"
    ],
    "absent_kp": [
      "deiodinase"
    ]
  },
  {
    "title": "a lower-bound formulation for the geometry and topology optimization of truss structures under multiple loading.",
    "abstract": "in this contribution, we propose an effective formulation to address the stress-based minimum volume problem of truss structures. starting from the lower-bound formulation in topology optimization, the problem is further expanded to geometry optimization and multiple loading scenarios, and systematically reformulated to alleviate numerical difficulties related to the melting node effect and stress singularities. the subsequent simultaneous analysis and design (sand) formulation is well suited for a direct treatment by introducing a barrier function. using exact second derivatives, this difficult class of problem is solved by sequential quadratic programming with trust regions. these building blocks result into an integrated design process. two examples-including a large-scale application-illustrate the robustness of the proposed formulation.",
    "present_kp": [
      "truss structure",
      "geometry optimization",
      "topology optimization",
      "multiple loading",
      "melting node effect",
      "simultaneous analysis and design"
    ],
    "absent_kp": []
  },
  {
    "title": "flexible and stretchable micro-electrodes for in vitro and in vivo neural interfaces.",
    "abstract": "microelectrode arrays (meas) are designed to monitor and/or stimulate extracellularly neuronal activity. however, the biomechanical and structural mismatch between current meas and neural tissues remains a challenge for neural interfaces. this article describes a material strategy to prepare neural electrodes with improved mechanical compliance that relies on thin metal film electrodes embedded in polymeric substrates. the electrode impedance of micro-electrodes on polymer is comparable to that of mea on glass substrates. furthermore, meas on plastic can be flexed and rolled offering improved structural interface with brain and nerves in vivo. meas on elastomer can be stretched reversibly and provide in vitro unique platforms to simultaneously investigate the electrophysiological of neural cells and tissues to mechanical stimulation. adding mechanical compliance to meas is a promising vehicle for robust and reliable neural interfaces.",
    "present_kp": [
      "micro-electrodes",
      "compliance"
    ],
    "absent_kp": [
      "polymers",
      "action potentials",
      "field potentials",
      "peripheral nerve",
      "slice culture"
    ]
  },
  {
    "title": "study of temporal stationarity and spatial consistency of fmri noise using independent component analysis.",
    "abstract": "spatial independent component analysis (ica) was used to study the temporal stationarity and spatial consistency of structured functional mri (fmri) noise. spatial correlations have been used in the past to generate filters for the removal of structured noise for each time-course in an fmri dataset. it would be beneficial to produce a multivariate filter based on the same principles. ica is examined to determine if it has properties that are beneficial for this type of filtering. six fmri baseline datasets were decomposed via spatial ica. the time-courses associated with each component were tested for wide-sense stationarity using the wide sense stationarity quotient (wss). each dataset was divided into three subsets and each subset was decomposed. the components of first and third subset were matched by the strength of their correlation. the components produced by ica were found to have largely nonstationary time-courses. despite the temporal nonstationarity in the data, ica was found to produce consistent spatial components. the degree of correlation among components differed depending on the amount of dimension reduction performed on the data. it was found that a relatively small number of dimensions produced components that are potentially useful for generating a spatial fmri filter.",
    "present_kp": [
      "functional mri ",
      "independent component analysis "
    ],
    "absent_kp": [
      "stationarity of structured noise"
    ]
  },
  {
    "title": "petri net-based ftl architecture for parametric wcet estimation via ftl operation sequence derivation.",
    "abstract": "a flash translation layer (ftl) provides file systems with transparent access to nand flash memory. although many applications running on it require real-time guarantees, it is difficult to provide tight worst case execution time (wcet) bounds with conventional static wcet analysis since an ftl exhibits a large variance in execution time depending on its runtime state. parametric wcet analysis could be an effective alternative but it is also challenging to formulate a parametric wcet function for an ftl program because traditional ftl architecture does not properly model the runtime availability of flash resources in its code structure. to overcome such a limitation, we propose petri net-based ftl architecture where a petri net explicitly specifies dependencies between ftl operations and the runtime resource availability. it comes with an ftl operation sequencer that derives at runtime the shortest sequence of ftl operations for servicing an incoming ftl request under the current resource availability. the sequencer computes the wcet of the request by merely summing the wcets of only those ftl operations in the sequence. our experimental results show the effectiveness of our ftl architecture. it allowed for tight wcet estimation that yielded wcets shorter by a factor of 54 than statically analyzed ones.",
    "present_kp": [],
    "absent_kp": [
      "real-time and embedded systems",
      "software architectures",
      "performance modeling and prediction"
    ]
  },
  {
    "title": "students participation intention in an online discussion forum: why is computer-mediated interaction attractive.",
    "abstract": "anecdotal evidence indicates that an online discussion forum may not be utilized to its full potential in enhancing the effectiveness and efficiency of teaching due to a lower than expected student participation rate. this paper seeks to identify the motivational behavioral factors influencing students intention to participate in an online discussion forums (odf). drawing on the literature on social psychology and applying the theory of reasoned action, we develop a conceptual model of intention to participate in an online discussion forum and empirically test the hypotheses in a cross-sectional quantitative survey. the findings indicate that expectancy on hedonic outcome and utilitarian outcome and peer pressure positively influence the participation intention of students. also, the perceived importance of learning positively moderates the relationship between utilitarian outcome expectancy and participation intention. theoretical and practical implications of the findings are discussed.",
    "present_kp": [
      "online discussion forum",
      "theory of reasoned action"
    ],
    "absent_kp": [
      "collaborative learning"
    ]
  },
  {
    "title": "transient response of a plateliquid system under an aerial detonation : simulations and experiments.",
    "abstract": "the unsteady flow equations for air blasts were solved with a home house cfd code. the simulation of the plateliquid system under this blast load has been validated. space and time scales of the cfd and structural models are made compatible. experimental strains and pressures show good agreement with the simulations.",
    "present_kp": [
      "plateliquid system"
    ],
    "absent_kp": [
      "blast wave",
      "fluidstructure interaction",
      "cartesian methods",
      "reduced scale experiments"
    ]
  },
  {
    "title": "on equations for union-free regular languages.",
    "abstract": "in this paper we consider the variety uf, generated by all algebras of binary relations equipped with the operations of composition, reflexive-transitive closure, and the empty set and the identity relation as constants. this variety coincides with the variety generated by the union-free reducts of kleene algebras of languages and its free objects are formed by union-free regular languages, that is, regular languages represented by regular expressions having no occurrence of +. we show that the variety uf; is not finitely based. the situation does not change if we consider the variety ufv generated by the above algebras of binary relations equipped with the conversion operation.",
    "present_kp": [
      "regular language",
      "kleene algebra"
    ],
    "absent_kp": [
      "equational logic"
    ]
  },
  {
    "title": "a reappraisal on advanced planning and scheduling systems.",
    "abstract": "purpose - this paper sets out to present a reappraisal on advanced planning and scheduling (aps) systems in industrial settings and propose an effective approach for aps implementation. design/methodology/approach - a case study approach is adopted, and a research framework comprising human-, technological-, and organizational-dimensions is developed to analyze the evidence database which includes business flows, system design documents, archival records, post-system assessment, participant-observation and semi-structured interviews. findings - the findings indicate that real-world production planning problems are ill-defined, complex and dynamic. a post-implementation evaluation reveals major pitfalls in the technology-dominant approach, whose negative ramifications are usually overlooked. besides, these aps implementation pitfalls are found to be attributable to the real-world context, human factors and organizational aspects. research limitations/implications - despite advances in information technology (it) and computer modeling techniques, humans still play critical roles in the production-planning processes especially in a complex and dynamic manufacturing environment where incomplete, ambiguous, inconsistent and untimely data make automatic planning unrealistic. a rational human-computer collaboration scheme under an effective organizational structure would be in a better position to take advantage of the it. originality/value - this paper presents a humans-technology-organization-framework of real planning systems, which is employed to analyze a case of aps implementation. practical insights are extracted as a result of this field research, and a realist approach is proposed to cope with the problems and pitfalls of aps implementation in industrial settings.",
    "present_kp": [
      "production planning"
    ],
    "absent_kp": [
      "supply chain management",
      "semiconductors",
      "production scheduling",
      "human resource accounting"
    ]
  },
  {
    "title": "dominant parameter selection in the marginally identifiable case.",
    "abstract": "often a rather limited set of experimental data is available for the identification of a dynamic model, which contains many parameters. this is, e.g. the usual case for crop growth models. in this situation, only some parameter values can be estimated. based on an analysis of the fisher information matrix, a method for a reasonable selection of parameters is suggested here. the method chooses the most sensitive parameters, i.e. those to which the model under the considered experimental conditions is most sensitive, and excludes both coupled parameters and those that exhibit multiplecorrelation. a comparison with different ridge regression methods is made. the methodology is illustrated with a simple lettuce growth model.",
    "present_kp": [
      "crop growth models"
    ],
    "absent_kp": [
      "model calibration",
      "system identification",
      "fisher matrix"
    ]
  },
  {
    "title": "parallels between the analytic hierarchy and network processes (ahp/anp) and fractal geometry.",
    "abstract": "the aim of this work is to show the parallelisms and analogies that exist in modeling and measuring of dependence and feedback processes, in physical and in decision making processes, that is, to compare among the scales of measurement of the physical world (geometry) and the scales of measurement of the human being's internal decision process, in other words, the brain's internal generation of a relative measure scale.",
    "present_kp": [
      "anp",
      "parallelisms"
    ],
    "absent_kp": [
      "fractals",
      "system representations",
      "history"
    ]
  },
  {
    "title": "development of a hybrid methodology for dimensionality reduction in mahalanobis-taguchi system using mahalanobis distance and binary particle swarm optimization.",
    "abstract": "mahalanobis-taguchi system (mts) is a pattern recognition method applied to classify data into categories - \"healthy\" and \"unhealthy\" or \"acceptable\" and \"unacceptable\". mts has found applications in a wide range of problem domains. dimensionality reduction of the input set of attributes forms an important step in mts. the current practice is to apply taguchi's design of experiments (doe) and orthogonal array (oa) method to achieve this end. maximization of signal-to-noise (sin) ratio forms the basis for selection of the optimal combination of variables. however the doe-oa method has been reviewed to be inadequate for the purpose in this research study, we propose a dimensionality reduction method by addressing the problem as feature selection exercise. the optimal combination of attributes minimizes a weighted sum of total fractional misclassification and the percentage of the total number of variables employed to obtain the misclassification. mahalanobis distances (mds) of \"healthy\" and \"unhealthy\" conditions are used to compute i he misclassification. a mathematical model formulates the feature selection approach and it is solved by binary particle swarm optimization (pso). data from an indian foundry shop is adopted to test the mathematical model and the swarm heuristic. results are compared with that of doe-oa method of mts.",
    "present_kp": [
      "dimensionality reduction",
      "feature selection",
      "mahalanobis-taguchi system",
      "mahalanobis distance",
      "orthogonal array",
      "binary particle swarm optimization"
    ],
    "absent_kp": []
  },
  {
    "title": "real time design and animation of fractal plants and trees.",
    "abstract": "the goal of science is to understand why things are the way they are. by emulating the logic of nature, computer simulation programs capture the essence of natural objects, thereby serving as a tool of science. when these programs express this essence visually, they serve as an instrument of art as well.this paper presents a fractal computer model of branching objects. this program generates pictures of simple orderly plants, complex gnarled trees, leaves, vein systems, as well as inorganic structures such as river deltas, snow flakes, etc. the geometry and topology of the model are controlled by numerical parameters which are analogous to the organism's dna. by manipulating the genetic parameters, one can modify the geometry of the object in real time, using tree based graphics hardware. the random effects of the environment are taken into account, to produce greater diversity and realism. increasing the number of significant parameters yields more complex and evolved species.the program provides a study in the structure of branching objects that is both scientific and artistic. the results suggest that organisms and computers deal with complexity in similar ways.",
    "present_kp": [
      "geometry",
      "design",
      "fractal",
      "tool",
      "object",
      "trees",
      "account",
      "art",
      "instrument",
      "paper",
      "model",
      "graphics hardware",
      "program",
      "science",
      "structure",
      "systems",
      "diversity",
      "computer simulation",
      "logic",
      "complexity",
      "animation",
      "effect"
    ],
    "absent_kp": [
      "computation",
      "topologies",
      "real-time",
      "randomization",
      "organization",
      "environments",
      "computer modeling"
    ]
  },
  {
    "title": "the fully implicit stochastic- method for stiff stochastic differential equations.",
    "abstract": "a fully implicit integration method for stochastic differential equations with significant multiplicative noise and stiffness in both the drift and diffusion coefficients has been constructed, analyzed and illustrated with numerical examples in this work. the method has strong order 1.0 consistency and has user-selectable parameters that allow the user to expand the stability region of the method to cover almost the entire drift-diffusion stability plane. the large stability region enables the method to take computationally efficient time steps. a system of chemical langevin equations simulated with the method illustrates its computational efficiency.",
    "present_kp": [
      "stiff stochastic differential equation",
      "multiplicative noise",
      "chemical langevin equations"
    ],
    "absent_kp": [
      "fully implicit numerical method"
    ]
  },
  {
    "title": "looking beyond learning: notes towards the critical study of educational technology.",
    "abstract": "this paper makes a case for academic research and writing that looks beyond the learning potential of technology and, instead, seeks to develop social scientific accounts of the often compromised and constrained realities of education technology use on the ground. the paper discusses how this critical approach differs from the ways that educational technology scholarship has tended to be pursued to date. these differences include viewing technology as being socially constructed and negotiated rather than imbued with pre-determined characteristics; developing objective and realistic accounts of technology use in situ; and producing context rich analyses of the social conflicts and politics that underpin the use of technology in educational settings. the paper concludes by encouraging academic researchers and writers to show greater interest in the issues of democracy and social justice that surround educational technology.",
    "present_kp": [
      "education",
      "research",
      "technology"
    ],
    "absent_kp": [
      "sociology",
      "social theory"
    ]
  },
  {
    "title": "a soft computing method to predict sludge volume index based on a recurrent self-organizing neural network.",
    "abstract": "the structure of rsonn can be self-organized based on the contributions of each hidden node, which uses not only the past states but also the current states. the appropriately adjusted learning rates of rsonn is derived based on the lyapunov stability theorem. moreover, the convergence of the proposed rsonn is discussed. an experimental hardware, including the proposed soft computing method is set up. the experimental results have confirmed that the soft computing method exhibits satisfactory predicting performance for svi.",
    "present_kp": [
      "soft computing method",
      "recurrent self-organizing neural network",
      "sludge volume index"
    ],
    "absent_kp": [
      "prediction",
      "sensitivity analysis"
    ]
  },
  {
    "title": "varieties of helmholtz machine.",
    "abstract": "the helmholtz machine is a new unsupervised learning architecture that uses top-down connections to build probability density models of input and bottom-up connections to build inverses to those models. the wake-sleep learning algorithm for the machine involves just the purely local delta rule. this paper suggests a number of different varieties of helmholtz machines, each with its own strengths and weaknesses, and relates them to cortical information processing.",
    "present_kp": [
      "unsupervised learning"
    ],
    "absent_kp": [
      "expectation-maximization",
      "feedback connections"
    ]
  },
  {
    "title": "cyclophosphamides as hypoxia-activated diffusible cytotoxins: a theoretical study.",
    "abstract": "cyclophosphamides have been in clinical use as anti-cancer drugs for a long time and much research has been directed towards reducing their side effects. here we have performed a theoretical investigation into the possibility of designing bioreductive analogues of cyclophosphamides. our calculations have employed semiempirical molecular orbital am1-sm2 and pm3-sm3 calculations, as implemented in mopac 93, which include a modified born method for the treatment of solvation. we have investigated the effect of bioreductive activation on the beta-elimination reaction that is central to the activation of cyclophosphamides. the approach was tested on two known bioreductive agents, including cb1954, and gave results in agreement with experiment. non-local density functional calculations on cb1954 and its metabolites, including the radical anion, were in agreement with the semiempirical calculations. the calculations have identified a number of potentially novel bioreductive cyclophosphamides. in particular, our calculations identified compounds in which the initial one-electron reduction was not activating. such compounds are likely to be more effective bioreductive agents, as the beta-elimination will not compete under oxic conditions with the important re-oxidation required for the protection of oxic tissue.",
    "present_kp": [
      "bioreductive",
      "cyclophosphamides",
      "molecular orbital"
    ],
    "absent_kp": [
      "nitroimidazole",
      "tirapazamine"
    ]
  },
  {
    "title": "power-aware operand delivery.",
    "abstract": "based on operand delivery, existing microprocessors can be categorized into architected register file ( arf ) or physical register file ( prf ) machines, both with or without payload ram ( pl ). though many previous generation microprocessors use a prf without pl , the trend of newer microprocessors targeting lower power environments seem to be moving towards arf with pl . we quantitatively analyze power consumption of different machine styles: arf with pl , arf without pl , prf with pl , and prf only machine. our result shows that prf without pl consumes the least amount of power and is fundamentally the best approach for building power-aware out-of-order microprocessors.",
    "present_kp": [
      "power"
    ],
    "absent_kp": [
      "microarchitecture",
      "renaming"
    ]
  },
  {
    "title": "nanomechanical strength mechanisms of hierarchical biological materials and tissues.",
    "abstract": "biological protein materials (bpms), intriguing hierarchical structures formed by assembly of chemical building blocks, are crucial for critical functions of life. the structural details of bpms are fascinating: they represent a combination of universally found motifs such as -helices or -sheets with highly adapted protein structures such as cytoskeletal networks or spider silk nanocomposites. bpms combine properties like strength and robustness, self-healing ability, adaptability, changeability, evolvability and others into multi-functional materials at a level unmatched in synthetic materials. the ability to achieve these properties depends critically on the particular traits of these materials, first and foremost their hierarchical architecture and seamless integration of material and structure, from nano to macro. here, we provide a brief review of this field and outline new research directions, along with a review of recent research results in the development of structure-property relationships of biological protein materials exemplified in a study of vimentin intermediate filaments.",
    "present_kp": [
      "hierarchical",
      "biological protein materials",
      "vimentin"
    ],
    "absent_kp": [
      "nanomechanics",
      "fracture",
      "deformation",
      "experiment",
      "simulation",
      "cytoskeleton"
    ]
  },
  {
    "title": "dense crystalline dimer packings of regular tetrahedra.",
    "abstract": "we present the densest known packing of regular tetrahedra with density phi = 4000/4671 = 0.856341 .... like the recently discovered packings of kallus et al. and torquato-jiao, our packing is crystalline with a unit cell of four tetrahedra forming two triangular dipyramids (dimer clusters). we show that our packing has maximal density within a three-parameter family of dimer packings. numerical compressions starting from random configurations suggest that the packing may be optimal at least for small cells with up to 16 tetrahedra and periodic boundaries.",
    "present_kp": [
      "packing"
    ],
    "absent_kp": [
      "crystallography",
      "regular solid",
      "hilbert problem"
    ]
  },
  {
    "title": "a generic architecture to synchronise design models issued from heterogeneous business tools: towards more interoperability between design expertises.",
    "abstract": "product development involves many experts collaborating to the same design goal. every expert has his own formalisms and tools leading to a high heterogeneity of information systems supporting design activities. interoperability became a major challenge to avoid information incompatibility along the product life cycle. to synchronise heterogeneous representations of product will be a major step to integrate expert activities. in this paper, the authors propose a meta-model framework to connect together heterogeneous design models. this meta-model framework is used to formalise possible interactions between heterogeneous representations. interaction formalisation is considered as a key point to synchronise heterogeneous models and to provide more interoperability between various computer assisted systems. the synchronisation loop is also presented as a major sequence of activities to manage collaborative design. tools to support synchronisation are proposed. however, through a basic case study, authors highlight what can be automated and where human intervention is still expected.",
    "present_kp": [
      "collaborative design",
      "interoperability",
      "heterogeneous models",
      "synchronisation"
    ],
    "absent_kp": [
      "multi-representation"
    ]
  },
  {
    "title": "oh behave! agent-based behavioral representations in problem solving environments.",
    "abstract": "the development of deregulated electricity systems around the world has produced the need for simulation systems that are capable of addressing the complexities that arise in the new markets. agent-based models allow the use of complex adaptive systems approaches that are capable of producing tools or problem solving environments that can address the behavior of each of the participants within the electricity market. the agents in the tools are allowed to establish their own objectives and apply their own decision rules. they can be developed to learn from their previous experiences and change their behavior when future opportunities arise. in this paper, we will argue that the same type of agent-based technology that is used to produce realistic agent behavior in agent-based simulation tools at argonne national laboratory can also be used to embed these tools in problem solving environments.",
    "present_kp": [
      "problem solving environments",
      "behavioral representations"
    ],
    "absent_kp": [
      "agent-based modelling"
    ]
  },
  {
    "title": "keypoint recognition using randomized trees.",
    "abstract": "in many 3d object-detection and pose-estimation problems, runtime performance is of critical importance. however, there usually is time to train the system, which we will show to be very useful. assuming that several registered images of the target object are available, we developed a keypoint-based approach that is effective in this context by formulating wide-baseline matching of keypoints extracted from the input images to those found in the model images as a classification problem. this shifts much of the computational burden to a training phase, without sacrificing recognition performance. as a results, the resulting algorithm is robust, accurate, and fast-enough for frame-rate performance. this reduction in runtime computational complexity is our first contribution. our second contribution is to show that, in this context, a simple and fast keypoint detector suffices to support detection and tracking even under large perspective and scale variations. while earlier methods require a detector that can be expected to produce very repeatable results, in general, which usually is very time-consuming, we simply find the most repeatable object keypoints for the specific target object during the training phase. we have incorporated these ideas into a real-time system that detects planar, nonplanar, and deformable objects. it then estimates the pose of the rigid ones and the deformations of the others.",
    "present_kp": [
      "tracking"
    ],
    "absent_kp": [
      "image processing and computer vision",
      "object recognition",
      "statistical",
      "classifier design and evaluation",
      "edge and feature detection"
    ]
  },
  {
    "title": "driver behaviour at roadworks.",
    "abstract": "road networks around the world are reaching a critical stage in their lifecycle. transport authorities are planning significant maintenance activities with associated roadworks and traffic management. traffic microsimulation is used to plan these roadworks but modelled drivers are not behaving in the same way as real drivers. a range of psychological explanations for this difference are reviewed. guidance for incorporating these psychological factors into future models is proposed.",
    "present_kp": [
      "roadworks",
      "driver behaviour",
      "microsimulation"
    ],
    "absent_kp": []
  },
  {
    "title": "sally: a tool for embedding strings in vector spaces.",
    "abstract": "strings and sequences are ubiquitous in many areas of data analysis. however, only few learning methods can be directly applied to this form of data. we present sally, a tool for embedding strings in vector spaces that allows for applying a wide range of learning methods to string data. sally implements a generalized form of the bag-of-words model, where strings are mapped to a vector space that is spanned by a set of string features, such as words or n-grams of words. the implementation of sally builds on efficient string algorithms and enables processing millions of strings and features. the tool supports several data formats and is capable of interfacing with common learning environments, such as weka, shogun, matlab, or pylab. sally has been successfully applied for learning with natural language text, dna sequences and monitored program behavior.",
    "present_kp": [],
    "absent_kp": [
      "string embedding",
      "bag-of-words models",
      "learning with sequential data"
    ]
  },
  {
    "title": "photon splatting for participating media.",
    "abstract": "since the beginning of image synthesis, much research have been done on global illumination simulation. however, simulation in participating media is still an open problem as far as computing time is concerned. recently some methods, like photon mapping, proposed an optimization of the resolution of global illumination in participating media. nevertheless, the computing costs of these methods remain very expensive.in this paper, we present a method which takes advantage of density estimation to efficiently reconstruct volume irradiance from the photon map. our idea is to separate the computation of emission, absorption and out-scattering from the computation of in-scattering. then we use a dual approach of density estimation to optimize this last part as it is the most computational expensive. our method extends photon splatting, which optimizes the computation time of photon mapping for surface rendering, to participating media, and then considerably reduce participating media rendering times. even though our method is faster than photon mapping for equal quality, we also propose a gpu based optimization of our algorithm.",
    "present_kp": [
      "density estimation",
      "participating media",
      "photon splatting",
      "global illumination"
    ],
    "absent_kp": []
  },
  {
    "title": "a genetic algorithm based approach for simultaneously balancing and sequencing of mixed-model u-lines with parallel workstations and zoning constraints.",
    "abstract": "this paper presents a priority-based genetic algorithm (pga) based method for the simultaneously tackling of the mixed-model u-shape assembly line (mmul) line balancing/model sequencing problems (mmul/bs) with parallel workstations and zoning constraints and allows the decision maker to control the process to create parallel workstations and to work in different scenarios. in the presented method, simulated annealing based fitness evaluation approach (sabfea) is developed to be able to make fitness function calculations easily and effectively. a new fitness function is adapted to mmuls for aiming at minimizing the number of workstations as primary goal and smoothing the workload between-within workstations by taking all cycles into consideration. a numerical example to clarify the solution methodology is presented. performance of the proposed approach is tested through sets of test problem with randomly generated minimum part sets. the results of the computational experiments indicate that sabfea works with pga very concordantly; and it is an effective method in solving mmul/bs with parallel workstations and zoning constraints.",
    "present_kp": [
      "genetic algorithm",
      "zoning constraint"
    ],
    "absent_kp": [
      "mixed-model u-shape balancing/sequencing problem",
      "simulated annealing algorithm",
      "fitness evaluation-relaxation",
      "parallel workstation assignment"
    ]
  },
  {
    "title": "ursa: a system for uniform reduction to sat.",
    "abstract": "there are a huge number of problems, from various areas, being solved by reducing them to sat. however, for many applications, translation into sat is performed by specialized, problem-specific tools. in this paper we describe a new system for uniform solving of a wide class of problems by reducing them to sat. the system uses a new specification language ursa that combines imperative and declarative programming paradigms. the reduction to sat is defined precisely by the semantics of the specification language. the domain of the approach is wide (e.g., many np-complete problems can be simply specified and then solved by the system) and there are problems easily solvable by the proposed system, while they can be hardly solved by using other programming languages or constraint programming systems. so, the system can be seen not only as a tool for solving problems by reducing them to sat, but also as a general-purpose constraint solving system (for finite domains). in this paper, we also describe an open-source implementation of the described approach. the performed experiments suggest that the system is competitive to state-of-the-art related modelling systems.",
    "present_kp": [
      "constraint solving"
    ],
    "absent_kp": [
      "sat problem",
      "specification languages"
    ]
  },
  {
    "title": "transient analysis of zero attracting nlms algorithm without gaussian inputs assumption.",
    "abstract": "we present the individual weight error variance analysis (iwv) of the za-nlms algorithm without gaussian inputs assumption. our iwv analysis is based on exact individual weight error relation. we use iwv analysis to derive the transient and steady-state behavior of the za-nlms algorithm without restricting the input to being gaussian or white. we use iwv analysis to deduce the mean square convergence condition on step-size for the za-nlms algorithm.",
    "present_kp": [
      "zero attracting",
      "individual weight error variance",
      "transient analysis"
    ],
    "absent_kp": [
      "steady-state analysis"
    ]
  },
  {
    "title": "an entropy-based query expansion approach for learning researchers dynamic information needs.",
    "abstract": "proposing an entropy-based query expansion with a reweighting (e_qe) approach. the e_qe used to learn the researchers evolving information needs at different levels of topic change. adopting a simulation pseudo-relevance feedback process to evaluate the proposed approach. the results show that the proposed e_qe approach can achieve better search results than the tfidf.",
    "present_kp": [
      "query expansion",
      "pseudo-relevance feedback",
      "topic change",
      "entropy"
    ],
    "absent_kp": [
      "term weighting"
    ]
  },
  {
    "title": "laplacian spectrum characterization of extensions of vertices of wheel graphs and multi-fan graphs.",
    "abstract": "the graph cn1?kk is the product of a circuit cn1 and a clique kk. in this paper, we will prove that it is determined by their laplacian spectrum except when n1=6. if n1=6 , there are several counterexamples. we also prove that the product of s vertexdisjoint paths and a clique (pn1?pn2 pns)?kk is also determined by the laplacian spectrum.",
    "present_kp": [
      "laplacian spectrum",
      "cn1?kk",
      "(pn1?pn2 pns)?kk"
    ],
    "absent_kp": [
      "cospectral graphs"
    ]
  },
  {
    "title": "a travel-efficient driving assistance scheme in vanets by providing recommended speed.",
    "abstract": "vehicles' speed is one of the key factors in vehicle travel efficiency, as speed is related to vehicle travel time, travel safety, fuel consumption, and exhaust gas emissions (e.g., co2 emissions). therefore, to improve the travel efficiency, a recommended speed calculation scheme is proposed to assist driving in vehicle ad hoc networks (vanets) circumstances. in the proposed scheme, vehicles' current speed and space headway are obtained by vehicle-to-roadside unit (v2r) communication and vehicle-to-vehicle (v2v) communication. based on the vehicles' current speed and adjacent vehicles' space headway, a recommended speed is calculated by on-board units installed in the vehicles, and then this recommended speed is provided to drivers. the drivers can change their speed to the recommended speed. at the recommended speed, vehicle travel efficiency can be improved: vehicles can arrive at destinations in a shorter travel time with fewer stop times, lower fuel consumption, and less co2 emission. in particular, when approaching intersections, vehicles can pass through the intersections with less red light waiting time and a higher nonstop passing rate.",
    "present_kp": [
      "travel efficiency",
      "vanets",
      "recommended speed",
      "co2 emissions"
    ],
    "absent_kp": [
      "intelligent transportation systems "
    ]
  },
  {
    "title": "open access to scholarly full-text documents.",
    "abstract": "purpose - the purpose of this article is to discuss open access to scholarly full-text documents. design/methodology/approach - discusses open access to scholarly full-text documents. findings - the paper shows that while open access archives are good for the majority, for publishers, editors and authors, open access articles can substantially increase their impact, and the impact factor for the source journals. originality/value - the paper offers insights into scholarly full-text documents.",
    "present_kp": [
      "publishers"
    ],
    "absent_kp": [
      "digital storage",
      "universities"
    ]
  },
  {
    "title": "the numerical methods for oscillating singularities in elliptic boundary value problems.",
    "abstract": "the singularities near the crack tips of homogeneous materials are monotone of type r(alpha) and r(alpha)log(delta)r, (depending on the boundary conditions along nonsmooth domains). however. the singularities around the interfacial cracks of the heterogeneous bimaterials are oscillatory of type, r(alpha) sin(epsilon logr). the method of auxiliary mapping (mam). introduced by babuska and oh, was proven to be successful in dealing with, r(alpha) type singularities. however, the effectiveness of mam is reduced in handling oscillating singularities. this paper deals with oscillating singularities as well as the monotone singularities by extending mam through introducing the power auxiliary mapping and the exponential auxiliary mapping.",
    "present_kp": [
      "interfacial cracks",
      "elliptic boundary value problems",
      "method",
      "method of auxiliary mapping"
    ],
    "absent_kp": [
      "monotone singularity",
      "oscillating singularity",
      "the p-version  of finite element",
      "the power  auxiliary mapping"
    ]
  },
  {
    "title": "hope: a genetic algorithm for the unequal area facility layout problem.",
    "abstract": "the paper discusses the application of an evolutionary computation technique for the design of efficient facilities. genetic algorithms (ga) have been applied to heuristically solve a number of combinatorial problems such as scheduling, the traveling salesman problem and the quadratic assignment problem. we apply ga to the layout problem which arises frequently in the design of manufacturing and service organizations to find good solutions. in this paper we outline a ga based algorithm for solving the single-floor facility layout problem. we consider departments of both equal and unequal sizes. the gas performance is evaluated using several test problems available in the literature. the results indicate that ga may provide a better alternative in a realistic environment where the objective is to find a number of reasonably good layouts. the implementation also provides the flexibility of having fixed departments and to interactively modify the layouts produced.",
    "present_kp": [
      "genetic algorithm",
      "quadratic assignment problem",
      "facility layout problem"
    ],
    "absent_kp": [
      "heuristics"
    ]
  },
  {
    "title": "a multi-society-based intelligent association discovery and selection for ambient intelligence environments.",
    "abstract": "this article presents a novel intelligent embedded agent approach for reducing the number of associations and interconnections between various agents operating within ad hoc multiagent societies of an ambient intelligent environment (aie) in order to reduce the processing latency and overheads. the main goal of the proposed fuzzy-based intelligent embedded agents (f-ias) includes learning the overall network configuration and adapting to the system functionality to personalize themselves to the user needs based on monitoring the user in a lifelong nonintrusive mode. in addition, the f-ias agents aim to reduce the agent interconnections to the most relevant set of agents in order to reduce the processing overheads and thus implicitly improving the system overall efficiency. we employ embedded ambassador agents, namely embassadors, which are designated f-ias agents utilized with additional novel characteristics to not only act as a gateway filtering the number of messages multicast across societies but also discover, recommend, and establish associations between agents residing in separate societies. in order to validate the efficiency of the proposed methods for multiagent and society-based intelligent association discovery and learning of f-ias agents/embassadors we will present two sets of unique experiments. the first experiment describes the obtained results carried out within the intelligent dormitory (idorm) which is a real-world testbed for aie research. here we specifically demonstrate the utilization of the f-ias agents and discuss that by optimizing the set of associations the agents increase efficiency and performance. the second set of experiments is based on emulating an idorm-like large-scale multi-society-based aie environment. the results illustrate how embassadors discover strongly correlated agent pairs and cause them to form associations so that relevant agents of separate societies can start interacting with each other.",
    "present_kp": [
      "ambient intelligence"
    ],
    "absent_kp": [
      "algorithms",
      "experimentation",
      "fuzzy control",
      "computing methodologies"
    ]
  },
  {
    "title": "sorting in column stores.",
    "abstract": "in recent years, we have seen a number of new database architectures based on the idea of vertical fragmentation of relations. these architectures target the analysis of huge amounts of relational data, because vertical fragmentation facilitates column scans which are common in analytic applications at the expense of single-tuple operations. although sorting is a common operation for analytics, few is known about sorting vertically fragmented relations. this paper compares various possibilities to apply (external) merge sort to vertically fragmented relations on different layers of the memory hierarchy and gives hints on when to apply which one. we propose a greedy algorithm to find the optimum mixture of steps that leads to a sorted version of a given relation which is stored column-wise.",
    "present_kp": [
      "column stores",
      "sorting"
    ],
    "absent_kp": [
      "main-memory databases"
    ]
  },
  {
    "title": "an integrated model for site selection and space determination of warehouses.",
    "abstract": "in this paper we present an integrated model for site selection and space determination for warehouses in a two-stage network in which products are shipped from part suppliers to warehouses, where they are stored for an uncertain length of time and then delivered to assembly plants. the objective is to minimize the total inbound and outbound transportation costs and the total warehouse operation costs, which include the fixed costs related to their locations and the variable costs related to their space requirements for given service levels. each warehouse is modeled as an m/g/c m / g / c queueing system in which each storage slot acts as a server. we formulate this problem as a nonlinear mixed integer program with a probabilistic constraint. two cases are considered. for the continuous unbounded size case, we find an approximate formula for the overflow probability and reformulate the problem into a set-covering problem. for the discrete size option case, we reformulate the problem into a capacitated connection location problem with discrete size options. computational experiments are performed and the results show that the continuous model is appropriate for the small and median size problems and the discrete model is a good choice for the large size problems.",
    "present_kp": [
      "warehouse",
      "location"
    ],
    "absent_kp": [
      "sizing",
      "concave minimization"
    ]
  },
  {
    "title": "fast spectral solution of the generalized enskog equation for dense gases.",
    "abstract": "we propose a fast spectral method for solving the generalized enskog equation for dense gases. for elastic collisions, the method solves the enskog collision operator with a computational cost of o(md?1ndlog?n) o ( m d ? 1 n d log ? n ) , where d is the dimension of the velocity space, and md?1 m d ? 1 and nd n d are the number of solid angle and velocity space discretizations, respectively. for inelastic collisions, the cost is n times higher. the accuracy of this fast spectral method is assessed by comparing our numerical results with analytical solutions of the spatially-homogeneous relaxation of heated granular gases. we also compare our results for force-driven poiseuille flow and fourier flow with those from molecular dynamics and monte carlo simulations. although it is phenomenological, the generalized enskog equation is capable of capturing the flow dynamics of dense granular gases, and the fast spectral method is accurate and efficient. as example applications, fourier and couette flows of a dense granular gas are investigated. in addition to the temperature profile, both the density and the high-energy tails in the velocity distribution functions are found to be strongly influenced by the restitution coefficient.",
    "present_kp": [
      "enskog equation",
      "dense granular gas",
      "fast spectral method"
    ],
    "absent_kp": [
      "rarefied gas dynamics"
    ]
  },
  {
    "title": "a new approach to chaotic image encryption based on quantum chaotic system, exploiting color spaces.",
    "abstract": "a new color image encryption based on quantum chaotic system is proposed. it comprises efficient confusion and diffusion properties for image cipher. it achieves brilliant characteristics for color image encryption applications. experimental results and analysis show better performance than other schemes.",
    "present_kp": [
      "image encryption",
      "quantum chaotic system"
    ],
    "absent_kp": [
      "toral automorphism",
      "lifting-wavelet transform "
    ]
  },
  {
    "title": "a circular visualization of people?s activities in distributed teams.",
    "abstract": "a visualization technique that shows daily patterns of people activities. visualization of activities of several individuals at once. the goal of the visualization is to foster interpersonal awareness. the tool implementing the technique is also described. the technique can be generalized and used in different contexts.",
    "present_kp": [],
    "absent_kp": [
      "circular boxplot",
      "circular histograms",
      "activity traces"
    ]
  },
  {
    "title": "simulation of geometrical and electronic structure of quasi-two-dimensional layer consisting of fullerenes d6h-c36.",
    "abstract": "this article describes a computer simulation of the geometrical and electronic structure of a quasi-two-dimensional carbon layer with a trigonal lattice consisting of fullerenes c36 (1) with topological symmetry d6h. every polyhedral cluster 1 of this polymeric layer (2) is surrounded by six similar fullerenes and connected with every such a fullerene by two covalent bonds. atomic coordinates of the repeating unit are estimated on the basis of mndo/pm3 calculations of hydrocarbon molecule (d6h)-c132h48 (3). the carbon skeleton of 3 coincides with a sufficiently large fragment of the polymeric layer 2. the electronic spectrum of the quasi-two-dimensional layer 2 is calculated by the crystalline orbital method in the eht approximation. the band gap in the electronic spectrum of 2 was found to be equal to 1.5 ev. the geometric and electronic structure of some oligomers of cluster c36, quasi-linear macromolecule [c36]n, and hypergraphite layer is also discussed.",
    "present_kp": [
      "fullerenes",
      "computer simulation",
      "eht"
    ],
    "absent_kp": [
      "quantum-chemical calculations",
      "mndo/pm3 method",
      "crystals of c36",
      "hydrocarbons"
    ]
  },
  {
    "title": "generating chordal graphs included in given graphs.",
    "abstract": "a chordal graph is a graph which contains no chordless cycle of at least four edges as an induced subgraph. the class of chordal graphs contains many famous graph classes such as trees, interval graphs, and split graphs, and is also a subclass of perfect graphs. in this paper, we address the problem of enumerating all labeled chordal graphs included in a given graph. we think of some variations of this problem. first we introduce an algorithm to enumerate all connected labeled chordal graphs in a complete graph of n vertices. next, we extend the algorithm to an algorithm to enumerate all labeled chordal graphs in a n-vertices complete graph. then, we show that we can use, with small changes, these algorithms to generate all (connected or not necessarily connected) labeled chordal graphs in arbitrary graph. all our algorithms are based on reverse search method, and time complexities to generate a chordal graph are o(l), and also o(l) delay. additionally, we present an algorithm to generate every clique of a given chordal graph in constant time. using these algorithms we obtain combinatorial gray code like sequences for these graph structures in which the differences between two consecutive graphs are bounded by a constant size.",
    "present_kp": [
      "chordal graph",
      "constant time"
    ],
    "absent_kp": [
      "enumeration"
    ]
  },
  {
    "title": "optimal camera placement for automated surveillance tasks.",
    "abstract": "camera placement has an enormous impact on the performance of vision systems, but the best placement to maximize performance depends on the purpose of the system. as a result, this paper focuses largely on the problem of task-specific camera placement. we propose a new camera placement method that optimizes views to provide the highest resolution images of objects and motions in the scene that are critical for the performance of some specified task (e.g. motion recognition, visual metrology, part identification, etc.). a general analytical formulation of the observation problem is developed in terms of motion statistics of a scene and resolution of observed actions resulting in an aggregate observability measure. the goal of this system is to optimize across multiple cameras the aggregate observability of the set of actions performed in a defined area. the method considers dynamic and unpredictable environments, where the subject of interest changes in time. it does not attempt to measure or reconstruct surfaces or objects, and does not use an internal model of the subjects for reference. as a result, this method differs significantly in its core formulation from camera placement solutions applied to problems such as inspection, reconstruction or the art gallery class of problems. we present tests of the system's optimized camera placement solutions using real-world data in both indoor and outdoor situations and robot-based experimentation using an all terrain robot vehicle-jr robot in an indoor setting.",
    "present_kp": [
      "observability"
    ],
    "absent_kp": [
      "camera networks",
      "robot/camera placement",
      "optimization",
      "sensor networks",
      "vision-based robotics"
    ]
  },
  {
    "title": "on some geometric problems of color-spanning sets.",
    "abstract": "in this paper we study several geometric problems of color-spanning sets: given n points with m colors in the plane, selecting m points with m distinct colors such that some geometric properties of the m selected points are minimized or maximized. the geometric properties studied in this paper are the maximum diameter, the largest closest pair, the planar smallest minimum spanning tree, the planar largest minimum spanning tree and the planar smallest perimeter convex hull. we propose an o(n 1+? ) time algorithm for the maximum diameter color-spanning set problem where ? could be an arbitrarily small positive constant. then, we present hardness proofs for the other problems and propose two efficient constant factor approximation algorithms for the planar smallest perimeter color-spanning convex hull problem.",
    "present_kp": [
      "color-spanning set"
    ],
    "absent_kp": [
      "computational geometry",
      "np complete"
    ]
  },
  {
    "title": "feature-guided convolution for pencil rendering.",
    "abstract": "we re-render a photographic image as a simulated pencil drawing using two independent line integral convolution (lic) algorithms that express tone and feature lines. the lic for tone is then applied in the same direction across the image, while the lic for features is applied in pixels close to each feature line in the direction of that line. features are extracted using the coherent line scheme. changing the direction and range of the lics allows a wide range of pencil drawing style to be mimicked. we tested our algorithm on diverse images and obtained encouraging results.",
    "present_kp": [
      "pencil drawing",
      "convolution",
      "feature"
    ],
    "absent_kp": [
      "non-photorealistic rendering",
      "coherent lines"
    ]
  },
  {
    "title": "a fuzzy model for human fall detection in infrared video.",
    "abstract": "fall detection, especially for elderly people, is a challenging problem which demands new products and technologies. in this paper a fuzzy model for fall detection and inactivity monitoring in infrared video is presented. the classification features proposed include geometric and kinematic parameters associated with more or less sudden changes in the tracked human-related regions of interest. a complete segmentation and tracking algorithm for infrared video as well as a fuzzy fall detection and confirmation algorithm are introduced. the proposed system is capable of identifying true and false falls, enhanced with inactivity monitoring aimed at confirming the need for medical assistance and/or care. the fall indicators used as well as their fuzzy model is explained in detail. the fuzzy model has been tested for a wide number of static and dynamic falls, demonstrating exciting initial results.",
    "present_kp": [
      "fall detection",
      "infrared video"
    ],
    "absent_kp": [
      "video segmentation and tracking",
      "fuzzy system"
    ]
  },
  {
    "title": "multiple hypothesis tracking for data association in vehicular networks.",
    "abstract": "the introduction of vehicle to vehicle (v2v) and vehicle to infrastructure (v2i) communications in intelligent transportation systems of the future brings new opportunities and new challenges into the automotive scene. vehicular communications broaden the information spectrum that is available to each vehicle, allowing the enhancement of existing applications and the introduction of new ones. undoubtedly, the impact of this new technology in transportation safety, efficiency and infotainment is expected to be very important. a significant part of research in vehicular networks (vanets) is dedicated to networking issues like routing and safety. however, perception systems which until now were based on onboard sensors only, need to incorporate the wirelessly received information in order to extend the situation awareness of the vehicle and the driver. this paper presents an algorithm for associating targets tracked from an onboard radar sensor with the position and motion data received from the vanet. the core of the algorithm is a track oriented multiple hypothesis tracker that is modified for incorporating information included in vanet messages. the algorithm is tested in real scenarios using two experimental vehicles and then compared with two other algorithmic approaches. one is using a simpler single hypothesis algorithm for association of vanet messages and the second is using only the onboard sensors for environment perception. as a result, the advantages of the multiple hypothesis algorithm regarding association performance and the added value of wireless information in the perception system are highlighted.",
    "present_kp": [
      "vehicular networks",
      "data association"
    ],
    "absent_kp": [
      "sensor data fusion",
      "track oriented multiple hypothesis tracking"
    ]
  },
  {
    "title": "basic competitive neural networks as adaptive mechanisms for non-stationary colour quantisation.",
    "abstract": "in this paper we consider the application of two basic competitive neural networks (cnn) to the adaptive computation of colour representatives on image sequences that show non-stationary distributions of pixel colours. the tested algorithms are the simple competitive learning (scl) algorithm and the frequency-sensitive competitive learning (fscl) algorithm. both, scl and fcsl are the simplest adaptive methods based, respectively, on minimising the distortion and on the search for a uniform quantisation. the aim of this paper is to study several computational properties of these methods when applied to non-stationary clustering as adaptive vector quantisation algorithms. nonstationary colour quantisation is, therefore, representative of the more general class of non-stationary clustering problems. we expect our results to be meaningful for other algorithms that involve either the minimisation of the distortion or the search for uniform quantisers. we study experimentally the effect of the size of the image sample employed in the one-pass adaptation, their robustness to initial conditions, and the effect of local versus global scheduling of the learning rate.",
    "present_kp": [
      "competitive learning",
      "non-stationary clustering",
      "simple competitive learning"
    ],
    "absent_kp": [
      "colour quantisation frequency-sensitive"
    ]
  },
  {
    "title": "an efficient pd data mining method for power transformer defect models using som technique.",
    "abstract": "laboratory pd measurement set-up is developed. hfct is employed to pick up pd current pulses. texture feature extraction method is applied on time-domain pd data. capability in different pd sources discrimination (using texture features) is assessed by som.",
    "present_kp": [],
    "absent_kp": [
      "bmu best match unit",
      "ch channel",
      "ddf dielectric dissipation factor",
      "dga dissolved gas analysis",
      "dwt discrete wavelet transform",
      "fra frequency response analysis",
      "glcm gray level covariance matrix",
      "gldv gray level difference vector",
      "gs/s giga sample per second",
      "hfct high frequency current transformer",
      "hv high voltage",
      "ir insulation resistance",
      "lab laboratory",
      "mpts mega points",
      "ms/s mega sample per second",
      "ng gray level value",
      "p.f. power frequency",
      "pc principal component",
      "pca principal component analysis",
      "pd partial discharge",
      "pdc polarization and depolarization current",
      "prpd phase resolved partial discharge",
      "rvm recovery voltage measurement",
      "som self-organizing map",
      "svm support vector machine",
      "sne stochastic neighbor embedding"
    ]
  },
  {
    "title": "an expeditious cum efficient algorithm for salt-and-pepper noise removal and edge-detail preservation using cardinal spline interpolation.",
    "abstract": "salt-and-pepper impulse noise removal for noise densities up to 95%. edge-detail preservation with psnr for noise densities up to 95%. the variation of the parameters ? and ? to maximize the psnr. the comparative analysis and time complexity with existing algorithms.",
    "present_kp": [
      "salt-and-pepper impulse noise",
      "cardinal spline interpolation"
    ],
    "absent_kp": [
      "edge-preserving regularization",
      "cardinal splines",
      "expeditious algorithm",
      "image de-noising",
      "edge preservation",
      "two-phase noise removal"
    ]
  },
  {
    "title": "semi-supervised learning by disagreement.",
    "abstract": "in many real-world tasks, there are abundant unlabeled examples but the number of labeled training examples is limited, because labeling the examples requires human efforts and expertise. so, semi-supervised learning which tries to exploit unlabeled examples to improve learning performance has become a hot topic. disagreement-based semi-supervised learning is an interesting paradigm, where multiple learners are trained for the task and the disagreements among the learners are exploited during the semi-supervised learning process. this survey article provides an introduction to research advances in this paradigm.",
    "present_kp": [
      "semi-supervised learning",
      "disagreement-based semi-supervised learning"
    ],
    "absent_kp": [
      "machine learning",
      "data mining"
    ]
  },
  {
    "title": "applying hierarchical grey relation clustering analysis to geographical information systems - a case study of the hospitals in taipei city.",
    "abstract": "deng proposed grey clustering analysis (gca) in 1987. later, jin presented a new method in 1993, called grey relational clustering (grc) method that combined grey relational analysis with clustering. however, the crc method cannot use a tree diagram to make appropriate classification decisions without re-computation. this study thus attempts to combine crc and hierarchical clustering analysis. given the existence of an excess of medical resources in the taipei area, this study attempts to understand the degree of concentration of medical resources in this area. specifically, this study applies a geographical information system (gis) to present the geographical distribution of hospitals in taipei. additionally, a new-type of cluster analysis, known as hierarchical grey relation clustering analysis, is used to analyze the distribution of hospitals and understand how they compete with one another. the analytical results demonstrate that hierarchical grey relation clustering analysis is a suitable method of analyzing geographical position. tree diagrams can help policymakers make appropriate classification decisions without re-computation. the study results can inform hospitals of their competitors and help them to develop appropriate responses. additionally, the analytical results can also provide a reference to government or hospital policymakers to help them position hospitals in areas, thus achieving a better distribution of medical resources in taipei.",
    "present_kp": [
      "grey relational analysis",
      "grey clustering analysis",
      "hierarchical clustering analysis",
      "geographical information system",
      "medical resource"
    ],
    "absent_kp": []
  },
  {
    "title": "optimization of test power and data volume in bist scheme based on scan slice overlapping.",
    "abstract": "in order to further reduce test data storage and test power of deterministic bist based on scan slice overlapping, this paper proposes a novel optimization approach. firstly, a san cell grouping method considering layout constraint is introduced to shorten the scan chain. secondly, a novel scan cell ordering approach considering layout constraint is proposed to optimize the order of scan chain. lastly, the authors propose an improved test pattern partition algorithm which selects the scan slice with the most specified bits as the first scan slice of the current overlapping block. experimental results indicate that the proposed optimization approach significantly reduces the scan-in transitions and test data storage by 73%93% and 60%87%, respectively.",
    "present_kp": [
      "layout constraint",
      "scan slice overlapping"
    ],
    "absent_kp": [
      "low-power testing",
      "scan-based design"
    ]
  },
  {
    "title": "relational databases as a massive information source for defeasible argumentation.",
    "abstract": "argumentation provides a sophisticated yet powerful mechanism for the formalization of commonsense reasoning in knowledge-based systems, with application in many areas of artificial intelligence. nowadays, most argumentation systems build their arguments on the basis of a single, fixed knowledge base, often under the form of a logic program as in defeasible logic programming or in assumption-based argumentation. currently, adding new information to such programs requires a manual encoding, which is not feasible for many real-world environments which involve large amounts of data, usually conceptualized as relational databases. this paper presents a novel approach to compute arguments from premises obtained from relational databases, identifying several relevant aspects. in our setting, different databases can be updated by external, independent applications, leading to changes in the spectrum of available arguments. we present algorithms for integrating a database management system with an argument-based inference engine. empirical results and running-time analysis associated with our approach show that it provides a powerful alternative for efficiently achieving massive argumentation, taking advantage of modern dbms technologies. we contend that our proposal is significant for developing new architectures for knowledge-based applications, such as decision support systems and recommender systems, using argumentation as the underlying inference model.",
    "present_kp": [
      "knowledge-based systems",
      "defeasible argumentation",
      "relational databases",
      "massive argumentation"
    ],
    "absent_kp": [
      "argument-supporting data retrieval"
    ]
  },
  {
    "title": "iterative inversion of fuzzified neural networks.",
    "abstract": "the inversion of a neural network is a process of computing inputs that produce a given target when fed into the neural network. the inversion algorithm of crisp neural networks is based on the gradient descent search in which a candidate inverse is iteratively refined to decrease the error between its output and the target. in this paper, me derive an inversion algorithm of fuzzified neural networks from that of crisp neural networks. first, we present a framework of learning algorithms of fuzzified neural networks and introduce the idea of adjusting schemes for fuzzy variables. next, we derive the inversion algorithm of fuzzified neural networks by applying the adjusting scheme for fuzzy variables to total inputs in the input layer, finally, we make three experiments on the parity-three problem; we examine the effect of the size of training sets on the inversion and investigate how the fuzziness of inputs and targets of training sets affects the inversion.",
    "present_kp": [
      "fuzzified neural network",
      "inversion algorithm",
      "learning algorithm"
    ],
    "absent_kp": []
  },
  {
    "title": "upper bounds on overloaded ldpcoptimum combinied system over rayleigh fading channel.",
    "abstract": "in this paper a closed form expression for the bit error rate of ldpcoc system in the presence of interferers is derived over an i.i.d rayleigh fading channel using message passing algorithm. all interferers are assumed to have equal power. the analysis is done for the case when the number of interferes is more than the number of receive antenna elements. in this paper, analytical results showed that for a ber of 10?2, the ldpcoc system provides an additional gain of 6.3db over oc system alone. both the systems provide identical diversity gain of 1.2db when the number of receive antennas are increased from 5 to 6.",
    "present_kp": [
      "message passing algorithm"
    ],
    "absent_kp": [
      "optimum combining",
      "irregular low-density parity check codes",
      "gaussian approximation approach"
    ]
  },
  {
    "title": "accelerating boolean matching using bloom filter.",
    "abstract": "boolean matching is a fundamental problem in fpga synthesis, but existing boolean matchers are not scalable to complex plbs (programmable logic blocks) and large circuits. this paper proposes a filter-based boolean matching method, f-bm, which accelerates boolean matching using lookup tables implemented by bloom filters storing pre-calculated matching results. to show the effectiveness of the proposed f-bm, a post-mapping re-synthesis minimizing area which employs boolean matching as the kernel has been implemented. tested on a broad selection of benchmarks, the re-synthesizer using f-bm is 80x faster with 0.5% more area, compared with the one using a sat-based boolean matcher.",
    "present_kp": [
      "fpga",
      "boolean matching",
      "bloom filter",
      "sat",
      "re-synthesis"
    ],
    "absent_kp": []
  },
  {
    "title": "a binary method for fast computation of inter and intra cluster similarities for combining multiple clusterings.",
    "abstract": "in this paper, we introduce a novel binary method for fast computation of an objective function to measure inter and intra class similarities, which is used for combining multiple clusterings. our method has the advantages of using less memory and cpu time. moreover, compared with the conventional technique, we reduce the time complexity of the problem considerably. experimental test results demonstrate the effectiveness of our new method.",
    "present_kp": [
      "combining multiple clusterings",
      "clustering"
    ],
    "absent_kp": [
      "binary techniques",
      "clustering algorithms"
    ]
  },
  {
    "title": "creating animation with personal photo collections and map for storytelling.",
    "abstract": "the goal of our research is to support the sharing of stories with digital photographs. some map sites are now collecting stories associated with peoples' relationships to places. users are mapping collections of places including intangible emotional associations of places along with photographs, videos. though this framework of mapping stories is important for accelerating individual creation and transmission of map content, it is not expressive enough to communicate stories narratively. for example, especially when the number of the mapped collections of places is large, it is not easy for viewers to read the map, and it is not easy for creators to express stories as series of events in the real world. that is because one narrative story in a form of a sequence of text narrations, a sequence of photographs, a movie, and an audio like podcasting, etc. is mapped to just one point. as a result, it's up to the viewers which point on the map to read, and in what order. the common framework is rather suitable for mapping and expressing fragments or snapshots of a whole story, and not suitable for expressing a whole story narratively by using a whole area of a map as the setting of the story. we therefore propose a new framework for mapping personal photo collections and constructing them as stories such as route guidances, sightseeing guidances, historical topics, fieldwork records, personal diaries, and so on. we named this framework as spatial slideshow. a spatial slideshow is a fusion of personal photo mapping and photo storytelling. each story of a spatial slideshow is made of a sequence of mapped photographs, and presented as synchronized animations of a map and an enhanced photo slideshow. the main technical suggestion of this paper is a method for creating a three-dimensional animation of photo image which has a visual effect of moving from photo to photo. we have developed a personal photo album software and released it on the web, which works as an editor and a browser of spatial slideshows. we assume the proposed framework has a significance of helping with grassroots development of spatial content driven by visual communications about places or events in the real world.",
    "present_kp": [
      "photo mapping",
      "photo storytelling",
      "visual communication",
      "animation"
    ],
    "absent_kp": [
      "photo sharing"
    ]
  },
  {
    "title": "key issues in data center security: an investigation of government audit reports.",
    "abstract": "the rising volume of electronic data, the growth in cloud computing and the need for secure and affordable large-scale data storage all contribute to the increasing reliance on data centers in society. this paper provides an overview of security issues relevant to data centers. we offer an aggregation and exploratory analysis of four audit reports of government data centers operating in the united states. using the information security common body of knowledge to categorize audit findings, we identify the key issues from the reoccurring findings in the reports, particularly in regards to operations security, data center management, physical security, and disaster planning. the security of data centers has become a paramount concern for both government and the information technology industry. both practitioners and academics can benefit from our research results because it provides insight into the key security issues facing modern data centers.",
    "present_kp": [
      "data center",
      "information security",
      "audit reports",
      "common body of knowledge",
      "operations security",
      "physical security"
    ],
    "absent_kp": [
      "disaster preparedness"
    ]
  },
  {
    "title": "charged partial surface area (cpsa) descriptors qsar applications.",
    "abstract": "the charged partial surface area, or cpsa descriptors were originally designed for use in structure-physical relationship studies to capture information about the features of molecules responsible for polar intermolecular interactions. since their development, they have found applications in abroad variety of both structure-property and structure-activity relationship studies. in the present work, the cpsa descriptors are examined in more detail, evaluating their characteristics with regard to conformational dependence, sources of partial atomic charges, utility of whole molecule and substructure varieties, and the inclusion or exclusion of explicit hydrogens. additionally, an examination of the physical interpretation that can be derived from structure-activity relationships that incorporate the cpsa descriptors is made. most recently, the cpsa descriptors have been found to be practically useful in the study of acute aquatic toxicity where they appear to provide an alternative to lumo energy level measures for describing global and local electrophilicity in cases of non-covalent molecular interactions. a second example illustrates the ability of the cpsa descriptors to discriminate agonists and antagonists among compounds that bind strongly at the estrogen receptor. while measures of global and local nucleophilicity and interatomic distances are required to explain receptor binding, volumetric parameters, such as cpsas, were found to be necessary to provide separation between reactivity patterns for agonists and antagonists, all having high binding affinity to estrogen receptor.",
    "present_kp": [
      "charged partial surface"
    ],
    "absent_kp": [
      "acute toxicity",
      "estrogenicity",
      "agonists/antagonists"
    ]
  },
  {
    "title": "the structural and electronic properties of cumagn (m+n=6) clusters.",
    "abstract": "the structural and electronic properties of cumagn (m+n=6) clusters have been investigated through density functional theory. our results show that the triangular shape (a) clusters and capped shape (b) clusters are generally stabler than w-shaped (c) clusters. it is shown that the homo (highest occupied molecular orbital)lumo (lowest unoccupied molecular orbital) gaps vary inversely to the average bond length. we also investigated spectra of cumagn (m+n=6) clusters, which shows that the dominant peaks near 3.5ev are contributed from the electrons of cu3d, cu4s and ag5s .",
    "present_kp": [
      "cumagn"
    ],
    "absent_kp": [
      "binary clusters",
      "absorption spectra"
    ]
  },
  {
    "title": "a tv agent system that integrates knowledge and answers users' questions.",
    "abstract": "aiming to close the digital divide in the television viewing environment, we are developing a tv system with an agent that controls the tv and peripherals on behalf of the user and provides information to the user. we propose a tv system function that answers viewers' questions about tv programs by calling upon multiple question-answering agents that search for relevant information.",
    "present_kp": [
      "tv",
      "agent"
    ],
    "absent_kp": [
      "question answering"
    ]
  },
  {
    "title": "new current-controlled current-mode sinusoidal quadrature oscillators using cdtas.",
    "abstract": "this article presents new current-mode oscillator circuits using cdtas which is designed from block diagram. the proposed circuits consist of three cdtas and two grounded capacitors. the condition of oscillation can be adjusted electronically/orthogonally from the oscillation frequency by adjusting bias current of the cdtas. the proposed quadrature oscillators have high output impedance and use only grounded capacitors without any external resistor which is very appropriate for future development into an integrated circuit. the pspice simulation and experimental results are corresponding to the theoretical analysis.",
    "present_kp": [
      "quadrature oscillator",
      "current-mode",
      "cdta"
    ],
    "absent_kp": []
  },
  {
    "title": "identification of unknown pure component spectra by indirect hard modeling.",
    "abstract": "indirect hard modeling (ihm) is a physically motivated spectral analysis principle. it utilizes nonlinear spectral hard models generated by peak fitting of the pure spectra. this approach allows the consideration of various nonlinear effects such as peak variations or spectral shifts. compared to established methods, less calibration samples are required and basic calibration transfer is performed inherently. to extend the applicability of ihm, which currently requires knowledge of the pure component spectra, two methods for the identification of pure spectra are presented in this work. these methods work automatically on a mathematically objective basis and do thus not depend on the expertise of the user. as ihm relies on an underlying physical picture of the spectra, the relevant information in the input data is exploited very efficiently especially for selective spectra, and nonideal spectral behavior is captured throughout the identification process. compared to established smcr methods the number of required spectra is reduced. the first method, complemental hard modeling (chm), is introduced for the case that a single pure spectrum is unknown. the method is based on a deconvolution approach and only requires a single mixture spectrum as input data. the second method, hard modeling factor analysis (hmfa), is conceptually related to smcr methods. it allows the identification of all pure spectra in a completely unknown mixture from a limited set of mixture spectra. as shown in this work, even highly collinear data can be employed.",
    "present_kp": [
      "indirect hard modeling ",
      "peak fitting",
      "hard modeling",
      "factor analysis",
      "identification"
    ],
    "absent_kp": [
      "spectroscopic analysis",
      "self modeling curve resolution ",
      "nonlinear optimization",
      "collinearity"
    ]
  },
  {
    "title": "a multi-parameter regularization model for image restoration.",
    "abstract": "based on total variation and wavelet frame, a multi-parameter regularization model for image restoration is proposed. an effective algorithm based on admm is given for solving the new model, i.e., tvframe. convergence analysis of the new algorithm is given. numerical experiments show that tvframe outperforms several state-of-the-art image restoration approaches.",
    "present_kp": [
      "tv",
      "multi-parameter regularization"
    ],
    "absent_kp": [
      "framelet",
      "denoising",
      "deblurring"
    ]
  },
  {
    "title": "distributed version control in the classroom.",
    "abstract": "modern distributed version control systems offer compelling advantages for teaching students professional software development practices and skills. in this paper, we explore the potential for incorporating mercurial into introductory, intermediate, and advanced computing courses. by incorporating version control into the entire cs curriculum, instructors create unique opportunities to engage students in collaborative, real-world projects and activities, giving them critical early exposure to the expectations and assumptions prevalent in the software development community. early introduction to version control provides students with an important foundation in both personal and collaborative development excellence, offering them a competitive edge in the marketplace and a superior understanding of software development best practice.",
    "present_kp": [
      "distributed version control",
      "mercurial"
    ],
    "absent_kp": [
      "software development process",
      "education",
      "software tools"
    ]
  },
  {
    "title": "a statistical model for synthesis of detailed facial geometry.",
    "abstract": "detailed surface geometry contributes greatly to the visual realism of 3d face models. however, acquiring high-resolution face geometry is often tedious and expensive. consequently, most face models used in games, virtual reality, or computer vision look unrealistically smooth. in this paper, we introduce a new statistical technique for the analysis and synthesis of small three-dimensional facial features, such as wrinkles and pores. we acquire high-resolution face geometry for people across a wide range of ages, genders, and races. for each scan, we separate the skin surface details from a smooth base mesh using displaced subdivision surfaces. then, we analyze the resulting displacement maps using the texture analysis/synthesis framework of heeger and bergen, adapted to capture statistics that vary spatially across a face. finally, we use the extracted statistics to synthesize plausible detail on face meshes of arbitrary subjects. we demonstrate the effectiveness of this method in several applications, including analysis of facial texture in subjects with different ages and genders, interpolation between high-resolution face scans, adding detail to low-resolution face scans, and adjusting the apparent age of faces. in all cases, we are able to re-produce fine geometric details consistent with those observed in high resolution scans.",
    "present_kp": [],
    "absent_kp": [
      "face modeling",
      "texture synthesis"
    ]
  },
  {
    "title": "development of an autonomous distributed control system for optical packet and circuit integrated networks.",
    "abstract": "in this paper, we describe an autonomous distributed control system that we have been developing for an optical packet and circuit integrated network, and we experimentally evaluate its performance. colored (i.e., multi-wavelength) optical packet-switched links transfer both control signals for circuit switching (e.g., signaling and routing) and best-effort packet data. we successfully transmitted high-definition uncompressed real-time video signals on two lightpaths established by our control system without degradation of video quality, simultaneously with other optical packet data transferred on the same optical fibers. our developed control system not only achieved autonomous distributed signaling and routing but also has a function that can adjust wavelength resources for optical packet and circuit switching autonomously in each link at each node. controllers achieved lightpath establishment within approximately 360 ms and dynamic resource adjustment within approximately 454 ms, in the best possible case in our experimental setup.",
    "present_kp": [
      "networks",
      "switching",
      "circuit",
      "switching",
      "packet"
    ],
    "absent_kp": [
      "fiber optics communications",
      "multiplexing",
      "optical communications"
    ]
  },
  {
    "title": "fun and games: player profiles.",
    "abstract": "this paper presents findings about player preferences regarding characteristics of gaming experiences. it describes a quantitative investigation employing a survey with 27 items as a basis to categorise players in profiles, and it shows how the fun factors represented by the survey items relate to each other. the survey was applied to 634 students in florianpolis (brazil). a factorial analysis method was used to understand how the fun factors are associated with one other in the players opinions, and it generated six dimensions of fun, every one of which can be described as a way to have fun: (1) improvement, (2) distinction, (3) immersion, (4) decoration, (5) empathy, and (6) grotesque. a cluster analysis method was used to divide players into eight profiles, every one of which describes the preferences of a group of players who declared to have fun with similar aspects. the profiles were named as: (1) competitive and enthusiastic; (2) competitive and selfish; (3) competitive and overcoming; (4) immersed in the beauty; (5) immersed and selfish; (6) distracted and uninterested; (7) quitters; and, (8) friendly and overcoming. the results are a useful source to reflect on game design and fun, and to consider topics such as violence, verisimilitude, distraction, socialization, as well as the multiplicity of modes of engagement with digital game.",
    "present_kp": [
      "fun",
      "player profiles"
    ],
    "absent_kp": [
      "digital games",
      "motivation"
    ]
  },
  {
    "title": "analyzing operational risk-reward trade-offs for start-ups.",
    "abstract": "we model the advertising and inventory decisions of a start-up. we develop a novel methodology to track variance as well as expected rewards of the operational decisions. we call this methodology variance retentive stochastic dynamic programming. we develop a heuristic (risktrackr) to construct risk-reward efficient frontiers.",
    "present_kp": [
      "risk-reward",
      "efficient frontiers",
      "heuristic"
    ],
    "absent_kp": [
      "start-up operations",
      "variance retentive stochastic programming"
    ]
  },
  {
    "title": "identity-based deniable authentication for ad hoc networks.",
    "abstract": "deniable authentication is an important security requirement for ad hoc networks. however, all known identity-based deniable authentication (ibda) protocols are lack of formal security proof which is very important for cryptographic protocol design. in this paper, we propose a non-interactive ibda protocol using bilinear pairings. our protocol admits formal security proof in the random oracle model under the bilinear diffie-hellman assumption. our protocol is faster than all known ibda protocols of its type. in addition, our protocol supports batch verification that can speed up the verification of authenticators. this characteristic makes our protocol useful in ad hoc networks.",
    "present_kp": [
      "ad hoc networks",
      "deniable authentication",
      "bilinear pairings",
      "random oracle model",
      ""
    ],
    "absent_kp": [
      "identity-based cryptography"
    ]
  },
  {
    "title": "lossless trace compression.",
    "abstract": "the tremendous storage space required for a useful data base of program traces has prompted a search for trace reduction techniques. in this paper, we discuss a range of information-lossless address and instruction trace compression schemes that can reduce both storage space and access time by an order of magnitude or more, without discarding either references or interreference timing information from the original trace. the pdats family of trace compression techniques achieves trace coding densities of about six references per byte. this family of techniques is now in use as the standard in the nmsu tracebase, an extensive trace archive that has been established for use by the international research and teaching community.",
    "present_kp": [
      "trace reduction",
      "trace compression"
    ],
    "absent_kp": [
      "lossless coding",
      "trace-driven simulation"
    ]
  },
  {
    "title": "diffusion tensor-based fast marching for modeling human brain connectivity network.",
    "abstract": "diffusion tensor imaging (dti) is an effective modality in studying the connectivity of the brain. to eliminate possible biases caused by fiber extraction approaches due to low spatial resolution of dti and the number of fibers obtained, the fast marching (fm) algorithm based on the whole diffusion tensor information is proposed to model and study the brain connectivity network. our observation is that the connectivity extracted from the whole tensor field would be more robust and reliable for constructing brain connectivity network using dti data. to construct the connectivity network, in this paper, the arrival time map and the velocity map generated by the fm algorithm are combined to define the connectivity strength among different brain regions. the conventional fiber tracking-based and the proposed tensor-based fm connectivity methods are compared, and the results indicate that the connectivity features obtained using the fm-based method agree better with the neuromorphical studies of the human brain.",
    "present_kp": [
      "diffusion tensor imaging",
      "fast marching",
      "fiber tracking"
    ],
    "absent_kp": [
      "brain connectivity analysis",
      "tractography"
    ]
  },
  {
    "title": "vertical handover management scheme using multiple tcp connections for heterogeneous networks.",
    "abstract": "although mobile nodes will handle vertical handovers to achieve transparent mobility in the heterogeneous network, the communication quality will be drastically degraded. we proposed vertical handover management scheme that solves all of the issues. in this paper, to verify the effectiveness of the scheme in a real environment, we implement it and show a demonstration.",
    "present_kp": [
      "vertical handover management",
      "multiple tcp connections"
    ],
    "absent_kp": [
      "cross-layer",
      "seamless and efficient handover"
    ]
  },
  {
    "title": "multimedia applications and security in mapreduce: opportunities and challenges.",
    "abstract": "cloud computing has recently attracted great attention, both commercially and academically. mapreduce is a popular programming model for distributed storage and computation in the cloud. in this paper, we survey cloud-based multimedia applications, identifying the open issues and challenges which arise when mapreduce is used for cloud computing.",
    "present_kp": [
      "cloud computing",
      "multimedia",
      "mapreduce",
      "security"
    ],
    "absent_kp": []
  },
  {
    "title": "internal states on equality algebras.",
    "abstract": "this paper investigates properties of equality algebras introduced by jenei as a possible algebraic semantic for fuzzy type theory. we define and study the pointed equality algebras and its subclass of compatible pointed equality algebras. we introduce and investigate the internal states and the state-morphism operators on equality algebras and on their corresponding bck-meet-semilattices. we prove that any internal state (state-morphism) on an equality algebra is also an internal state (state-morphism) on its corresponding bck-meet-semilattice, and we prove the converse for the case of linearly ordered equality algebras. another main result consists of proving that any state-morphism on a linearly ordered equality algebra is an internal state on it. we show that any internal state on a linearly ordered bck-meet-semilattice satisfying the distributivity condition is also an internal state on its corresponding equality algebra and a state-morphism on a bck-meet-semilattice satisfying the distributivity condition is also a state-morphism on its corresponding equality algebra.",
    "present_kp": [
      "equality algebra",
      "meet-semilattice",
      "pointed equality algebra",
      "internal state",
      "state-morphism operator"
    ],
    "absent_kp": [
      "bck-algebra",
      "compatible equality algebra"
    ]
  },
  {
    "title": "distributed data replenishment.",
    "abstract": "we propose a distributed data replenishment mechanism for some distributed peer-to-peer-based storage systems that automates the process of maintaining a sufficient level of data redundancy to ensure the availability of data in presence of peer departures and failures. the dynamics of peers entering and leaving the network are modeled as a stochastic process. a novel analytical time-backward technique is proposed to bound the expected time for a piece of data to remain in p2p systems. both theoretical and simulation results are in agreement, indicating that the data replenishment via random linear network coding (rlnc) outperforms other popular strategies. specifically, we show that the expected time for a piece of data to remain in a p2p system, the longer the better, is exponential in the number of peers used to store the data for the rlnc-based strategy, while they are quadratic for other strategies.",
    "present_kp": [
      "stochastic process",
      "network coding"
    ],
    "absent_kp": [
      "absorption time",
      "distributed storage"
    ]
  },
  {
    "title": "knowledge management for context-aware, policy-based ubiquitous computing systems.",
    "abstract": "ubiquitous computing systems depend on a distributed intelligence that relates the context of the task being performed to the available system resources and services. currently, this is hampered by the problems inherent in heterogeneous devices and technologies used to manage network resources and services, and the associated lack of a network language lingua franca that all systems can use and understand. this paper describes a new approach for representing, using and managing knowledge for ubiquitous computing systems, based on a novel combination of extracting knowledge from models and ontologies to form such a lingua franca. an extensible context model is used to select applicable policies to govern system behaviour; as context changes, policies change, which in turn causes system behaviour to change accordingly.",
    "present_kp": [
      "ubiquitous computing",
      "knowledge management"
    ],
    "absent_kp": [
      "context awareness",
      "policy-based management"
    ]
  },
  {
    "title": "path space regularization for holistic and robust light transport.",
    "abstract": "we propose a simple yet powerful regularization framework for robust light transport simulation. it builds on top of existing unbiased methods and resorts to a consistent estimation using regularization only for paths which cannot be sampled in an unbiased way. to introduce as little bias as possible, we selectively regularize individual interactions along paths, and also derive the regularization consistency conditions. our approach is compatible with the majority of unbiased methods, e. g. (bidirectional) path tracing and metropolis light transport (mlt), and only a simple modification is required to adapt existing renderers. we compare to recent unbiased and consistent methods and show examples of scenes with difficult light paths, where regularization is required to account for all illumination features. when coupled with mlt we are able to sample all phenomena, like recent consistent methods, while achieving superior convergence.",
    "present_kp": [
      "light transport",
      "regularization"
    ],
    "absent_kp": [
      "global illumination",
      "markov chain monte-carlo"
    ]
  },
  {
    "title": "integrating generations with advanced reference counting garbage collectors.",
    "abstract": "we propose the use of generations with modern reference counting. a reference counting collector is well suited to collect the old generation, containing a large fraction of live objects that are modified infrequently. such a collector can be combined with a tracing collector to collect the young generation, typically containing a small fraction of live objects. we have designed such a collector appropriate for running on a multiprocessor. as our building blocks, we used the sliding-views on-the-fly collectors. we have implemented the new collector on the jikes research java virtual machine (jikes rvm) and compared it with the concurrent reference counting collector supplied with the jikes rvm package. our measurements demonstrate short pause times, retaining those of the original on-the-fly collectors and a gain in application throughput time. it turns out that a modern reference counting collector may benefit from the use of generations.",
    "present_kp": [],
    "absent_kp": [
      "runtime systems",
      "memory management",
      "garbage collection",
      "generational garbage collection"
    ]
  },
  {
    "title": "on the use of visual motion in particle filter tracking.",
    "abstract": "particle filtering is now established as one of the most popular methods for visual tracking. within this framework, a basic assumption is that the data are temporally independent given the sequence of object states. in this paper, we argue that in general the data are correlated, and that modeling such dependency should improve tracking robustness. besides, the choice of using the transition prior as proposal distribution is also often made. thus, the current observation data is not taken into account in the generation of the new samples, requesting the noise process of the prior to be large enough to handle abrupt trajectory changes between the previous image data and the new one. therefore, many particles are either wasted in low likelihood area, resulting in a low efficiency of the sampling, or, more importantly, propagated on near distractor regions of the image, resulting in tracking failures. in this paper, we propose to handle both issues using motion. explicit motion measurements are used to drive the sampling process towards the new interesting regions of the image, while implicit motion measurements are introduced in the likelihood evaluation to model the data correlation term. the proposed model allows to handle abrupt motion changes and to filter out visual distractors when tracking objects with generic models based on shape or color distribution representations. experimental results compared against the condensation algorithm have demonstrated superior tracking performance.",
    "present_kp": [
      "sequence",
      "visual motion",
      "particle filter",
      "condensation"
    ],
    "absent_kp": [
      "object tracking",
      "estimation",
      "sequential monte-carlo"
    ]
  },
  {
    "title": "numerical model for two-bolted joints subjected to compressive loading.",
    "abstract": "this paper deals with the dimensioning of two-bolted assemblies made up from joint prismatic subassemblies subjected to fatigue compressive loads coplanar with the screw axis. the presented study is complementary to a previous study relative to tension loading . however, one should note that the two loading cases are not similar since an additional influential corner contact problem occurs in the case of compressive loading. the main development in this paper relative to the previous study is taking into consideration this complex corner contact problem. in this framework, the local deformation between the subassemblies and the corner of the supporting structure is formulated and a non-linear expression of a constrained displacement is established. moreover, the evolution of the contact zone under the compressive loading, which is not similar in the case of tension loading, is taken into consideration. consequently, a new extended numerical model for compressive loading is established from unidirectional finite elements and validated by 3d finite element simulations. an algorithm which updates the contact stiffness matrix and sets out forces and displacements at each node of the subassembly is developed using c language program. finally, a statistical software method is used as in the case of tensile loading. it is important to note that in the case of compressive loading, this statistical software method is not only used to establish the effect of the joint parameters, but also to identify and tune up parameters relative to the complex problem of the corner contact.",
    "present_kp": [
      "bolted joints",
      "finite elements",
      "fatigue"
    ],
    "absent_kp": [
      "structures modelling",
      "contact non-linearity"
    ]
  },
  {
    "title": "comment on support vector machine for classification based on fuzzy training data by a.-b. ji, j.-h. pang, h.-j. qiu.",
    "abstract": "this paper comments on the recently published work dealing with support vector machine for classification based on fuzzy data . the authors have claimed that their proposed program is a classical convex quadratic program. but, we show that their proposed program is neither convex nor classical quadratic. then, we propose a convex program by using the similar strategy proposed by ji et al. (2010).",
    "present_kp": [
      "support vector machine",
      "convex program"
    ],
    "absent_kp": [
      "fuzzy number"
    ]
  },
  {
    "title": "performance of the ensemble kalman filter outside of existing wells for a channelized reservoir.",
    "abstract": "the ensemble kalman filter (enkf) appears to give good results for matching production data at existing wells. however, the predictive power of these models outside of the existing wells is much more uncertain. in this paper, for a channelized reservoir for five different cases with different levels of information the production history is matched using the enkf. the predictive power of the resulting model is tested for the existing wells and for new wells. the results show a consistent improvement for the predictions at the existing wells after assimilation of the production data, but not for prediction of production at new well locations. the latter depended on the settings of the problem and prior information used. the results also showed that the fit during the history match was not always a good predictor for predictive capabilities of the history match model. this suggests that some form of validation outside of observed wells is essential.",
    "present_kp": [
      "ensemble kalman filter",
      "channelized reservoir"
    ],
    "absent_kp": [
      "assisted history matching",
      "uncertainty quantification",
      "multiple point statistics",
      "fluvial deposits"
    ]
  },
  {
    "title": "the dynamics of an elastic hopping hoop.",
    "abstract": "this paper discusses the motion of a hoop which is loaded with a heavy particle fixed to the rim and which is rolling in a vertical plane. in contrast to previous analyses, the hoop is not rigid; the elasticity in the system produces results that are in agreement with previously reported observations of hopping hula-hoops. the main result of this analysis is the identification of the conditions that are required for hopping to occur after a rotation through less than 90degrees after starting with the particle at the highest point.",
    "present_kp": [
      "elastic",
      "hopping hoop"
    ],
    "absent_kp": [
      "classical mechanics"
    ]
  },
  {
    "title": "opportunities for actuated tangible interfaces to improve protein study.",
    "abstract": "we outline strategies for actuated tangible user interfaces (tuis) to improve the study of proteins. current protein study tools miss fundamental biology concepts because graphical and symbolic interfaces do not allow users to intuitively manipulate complex physical forms. actuated, tangible tools may enhance understanding at all levels of protein study. to advance tui awareness of protein study, we present an overview of protein concepts and current protein study tools. thirty-six protein researchers, engineers, professors and students recommend design guidelines for tangible interfaces in protein study, and we outline research directions for tuis to improve protein study at all educational levels.",
    "present_kp": [
      "tangible user interface",
      "education"
    ],
    "absent_kp": [
      "actuation"
    ]
  },
  {
    "title": "point rendering of non-manifold surfaces with features.",
    "abstract": "we are concerned with producing high quality images of implicit surfaces, in particular those with non-manifold features. in this work we present a point based technique that improves the rendering of non-manifold implicit surfaces by using point and gradient information to prune plotting nodes resulting from using octree spatial subdivision based on the natural interval extension of the surface's function. the use of intervals guarantees that no parts of the surfaces are missed in the view volume and the combination of point and gradient sampling preserves this feature while greatly enhacing the quality of point based rendering of implicit surfaces. we also sucessfully render non-manifold features of implicit surfaces such as rays and thin sections. we illustrate the technique with a number of example surfaces.",
    "present_kp": [
      "rays",
      "point rendering",
      "non-manifold surface",
      "intervals"
    ],
    "absent_kp": [
      "octrees"
    ]
  },
  {
    "title": "a novel reordering write buffer to improve write performance of log-structured file systems.",
    "abstract": "this paper presents a novel reordering write buffer which improves the performance of log-structured file systems (lfs). while lfs has a good write performance, high garbage-collection overhead degrades its performance under high disk space utilization. previous research concentrated on how to improve the efficiency of the garbage collector after data is written to disk. we propose a new method that reduces the amount of work the garbage collector would do before data reaches disk. by classifying active and inactive data in memory into different segment buffers and then writing them to different disk segments, we force the disk segments to form a bimodal distribution. most data blocks in active segments are quickly invalidated, while inactive segments remain mostly intact. simulation results based on a wide range of both real-world and synthetic traces show that our method significantly reduces the garbage collection overhead, slashing the overall write cost of lfs by up to 53 percent, improving the write performance of lfs by up to 26 percent, and the overall system performance by up to 21 percent.",
    "present_kp": [
      "log-structured file systems",
      "file systems",
      "write performance"
    ],
    "absent_kp": [
      "storage systems"
    ]
  },
  {
    "title": "doppler ultrasound wall removal based on the spatial correlation of wavelet coefficients.",
    "abstract": "in medical doppler ultrasound systems, a high-pass filter is commonly used to reject echoes from the vessel wall. however, this leads to the loss of the information from the low velocity blood flow. here a spatially selective noise filtration algorithm cooperating with a threshold denoising based on wavelets coefficients is applied to estimate the wall clutter. then the blood flow signal is extracted by subtracting the wall clutter from the mixed signal. experiments on computer simulated signals with various clutter-to-blood power ratios indicate that this method achieves a lower mean relative error of spectrum than the high-pass filtering and other two previously published separation methods based on the recursive principle component analysis and the irregular sampling and iterative reconstruction, respectively. the method also performs well when applied to in vivo carotid signals. all results suggest that this approach can be implemented as a clutter rejection filter in doppler ultrasound instruments.",
    "present_kp": [
      "doppler ultrasound",
      "wall removal",
      "threshold denoising",
      "spatial correlation"
    ],
    "absent_kp": [
      "translation invariant"
    ]
  },
  {
    "title": "evaluation of probabilistic queries over imprecise data in constantly-evolving environments.",
    "abstract": "sensors are often employed to monitor continuously changing entities like locations of moving objects and temperature. the sensor readings are reported to a database system, and are subsequently used to answer queries. due to continuous changes in these values and limited resources (e.g., network bandwidth and battery power), the database may not be able to keep track of the actual values of the entities. queries that use these old values may produce incorrect answers. however, if the degree of uncertainty between the actual data value and the database value is limited, one can place more confidence in the answers to the queries. more generally, query answers can be augmented with probabilistic guarantees of the validity of the answers. in this paper, we study probabilistic query evaluation based on uncertain data. a classification of queries is made based upon the nature of the result set. for each class, we develop algorithms for computing probabilistic answers, and provide efficient indexing and numeric solutions. we address the important issue of measuring the quality of the answers to these queries, and provide algorithms for efficiently pulling data from relevant sensors or moving objects in order to improve the quality of the executing queries. extensive experiments are performed to examine the effectiveness of several data update policies.",
    "present_kp": [
      "constantly-evolving environments",
      "probabilistic queries"
    ],
    "absent_kp": [
      "data uncertainty",
      "query quality",
      "entropy",
      "data caching"
    ]
  },
  {
    "title": "understanding and modeling pedestrian mobility of train-station scenarios.",
    "abstract": "this work presents the observations of pedestrian mobility characteristics based on the traces collected in a train station; provides a mobility model using these observations.",
    "present_kp": [
      "mobility model"
    ],
    "absent_kp": []
  },
  {
    "title": "an anytime assignment algorithm: from local task swapping to global optimality.",
    "abstract": "the assignment problem arises in multi-robot task-allocation scenarios. inspired by existing techniques that employ task exchanges between robots, this paper introduces an algorithm for solving the assignment problem that has several appealing features for online, distributed robotics applications. the method may start with any initial matching and incrementally improve the current solution to reach the global optimum, producing valid assignments at any intermediate point. it is an any-time algorithm with a performance profile that is attractive: quality improves linearly with stages (or time). additionally, the algorithm is comparatively straightforward to implement and is efficient both theoretically (complexity of (o(n^3lg n)) is better than many widely used solvers) and practically (comparable to the fastest implementation, for up to hundreds of robots/tasks). the algorithm generalizes swap primitives used by existing task exchange methods already used in the robotics community but, uniquely, is able to obtain global optimality via communication with only a subset of robots during each stage. we present a centralized version of the algorithm and two decentralized variants that trade between computational and communication complexity. the centralized version turns out to be a computational improvement and reinterpretation of the little-known method of balinskigomory proposed half a century ago. thus, deeper understanding of the relationship between approximate swap-based techniquesdeveloped by roboticistsand combinatorial optimization techniques, e.g., the hungarian and auction algorithmsdeveloped by operations researchers but used extensively by roboticistsis uncovered.",
    "present_kp": [
      "task swapping"
    ],
    "absent_kp": [
      "multi-robot task allocation",
      "decentralized assignment",
      "anytime algorithms"
    ]
  },
  {
    "title": "on minimum reload cost paths, tours, and flows.",
    "abstract": "the concept of reload cost, that is of a cost incurred when two consecutive arcs along a path are of different types, naturally arises in a variety of applications related to transportation, telecommunication, and energy networks. previous work on reload costs is devoted to the problem of finding a spanning tree of minimum reload cost diameter (with no arc costs) or of minimum reload cost. in this article, we investigate the complexity and approximability of the problems of finding optimum paths, tours, and flows under a general cost model including reload costs as well as regular arc costs. some of these problems, such as shortest paths and minimum cost flows, turn out to be polynomially solvable while others, such as minimum shortest path tree and minimum unsplittable multicommodity flows, are np-hard to approximate within any polynomial-time computable function.",
    "present_kp": [],
    "absent_kp": [
      "network optimization",
      "extended reload cost model",
      "computational complexity"
    ]
  },
  {
    "title": "natural scene text detection with multi-layer segmentation and higher order conditional random field based analysis.",
    "abstract": "the contrasts in rgb channels are integrated to segment image into multi layers. the multi-layer segmentation is implemented with a graph cuts based model. a higher order crf based connected component analysis is used.",
    "present_kp": [
      "scene text detection",
      "multi-layer segmentation",
      "graph cuts",
      "higher order crf"
    ],
    "absent_kp": []
  },
  {
    "title": "operations research practice on logistics management in taiwan: an academic view.",
    "abstract": "the opinions of logistics educators in taiwan on or practices in the domestic logistics industry are explored in this research. in this study, questionnaires were given to 42 pre-screened educators at 10 logistics departments and graduate institutes. according to the 37 valid returned questionnaires, 70% of the responding educators believe the major source of or techniques in taiwans logistics industry originates from individual employee training, while 92% believe or is not widely used primarily because companies are unfamiliar with or techniques. or techniques were considered helpful in solving problems by 73%. generally speaking, familiarity with or techniques is not associated with implementation of or techniques by educators. additionally, logistics educators express concern about insufficient training for logistics students in taiwan. however, they are optimistic about the logistics industrys willingness to more actively adopt or techniques in taiwan in the next two years.",
    "present_kp": [
      "logistics"
    ],
    "absent_kp": [
      "education",
      "or applications"
    ]
  },
  {
    "title": "interference mitigation through limited receiver cooperation.",
    "abstract": "interference is a major issue limiting the performance in wireless networks. cooperation among receivers can help mitigate interference by forming distributed mimo systems. the rate at which receivers cooperate, however, is limited in most scenarios. how much interference can one bit of receiver cooperation mitigate? in this paper, we study the two-user gaussian interference channel with conferencing decoders to answer this question in a simple setting. we identify two regions regarding the gain from receiver cooperation: linear and saturation regions. in the linear region, receiver cooperation is efficient and provides a degrees-of-freedom gain, which is either one cooperation bit buys one over-the-air bit or two cooperation bits buy one over-the-air bit. in the saturation region, receiver cooperation is inefficient and provides a power gain, which is bounded regardless of the rate at which receivers cooperate. the conclusion is drawn from the characterization of capacity region to within two bits/s/hz, regardless of channel parameters. the proposed strategy consists of two parts: 1) the transmission scheme, where superposition encoding with a simple power split is employed and 2) the cooperative protocol, where one receiver quantize-bin-and-forwards its received signal and the other after receiving the side information decode-bin-and-forwards its received signal.",
    "present_kp": [
      "distributed mimo system",
      "receiver cooperation"
    ],
    "absent_kp": [
      "capacity to within a bounded gap",
      "interference management"
    ]
  },
  {
    "title": "a note on permutations and rank aggregation.",
    "abstract": "in this brief note we consider rank aggregation, a popular method in voting theory, social choice, business decisions, etc. mathematically the problem is to find a permutation-viewed as a vector-that minimizes the sum of the l(1)-distances to a given family of permutations. the problem may be solved as an assignment problem and we establish several properties of optimal solutions in this problem.",
    "present_kp": [
      "voting",
      "permutations",
      "assignment problem"
    ],
    "absent_kp": [
      "ranking"
    ]
  },
  {
    "title": "mapping search relevance to social networks.",
    "abstract": "this paper explores how information contained in the structure of the social graph can improve search result relevance on social networking websites. traditional approaches to search include scoring documents for relevance based on a set of keywords or using the link structure across documents to infer quality and relevance. these approaches attempt to optimally match keywords to documents with little or no information about the searcher and no information about his network. this study analyzes 3.8m profile search queries from a large social networking site in conjunction with the tie structure of a 21m member social graph. the key finding is that a measure of social distance, when used in conjunction with standard search relevance methods, improves the ordering of profiles in search results.",
    "present_kp": [
      "search relevance"
    ],
    "absent_kp": [
      "social search",
      "community analysis",
      "social network analysis"
    ]
  },
  {
    "title": "some refinements of rough k-means clustering.",
    "abstract": "lingras et at. proposed a rough cluster algorithm and successfully applied it to web mining. in this paper we analyze their algorithm with respect to its objective function, numerical stability, the stability of the clusters and others. based on this analysis a refined rough cluster algorithm is presented. the refined algorithm is applied to synthetic, forest and microarray gene expression data.",
    "present_kp": [
      "rough k-means"
    ],
    "absent_kp": [
      "cluster algorithms",
      "soft computing",
      "data analysis",
      "forest data",
      "bioinformatics data"
    ]
  },
  {
    "title": "taming hardware event samples for precise and versatile feedback directed optimizations.",
    "abstract": "feedback-directed optimization (fdo) is effective in improving application runtime performance, but has not been widely adopted due to the tedious dual-compilation model, the difficulties in generating representative training data sets, and the high runtime overhead of profile collection. the use of hardware-event sampling overcomes these drawbacks by providing a lightweight approach to collect execution profiles in the production environment, which naturally consumes representative input. yet, hardware event samples are typically not precise at the instruction or basic-block granularity. these inaccuracies lead to missed performance when compared to instrumentation-based fdo. in this paper, we use performance monitoring unit (pmu)-based sampling to collect the instruction frequency profiles. by collecting profiles using multiple events, and applying heuristics to predict the accuracy, we improve the accuracy of the profile. we also show how emerging techniques can be used to further improve the accuracy of the sample-based profile. additionally, these emerging techniques are used to collect value profiles, as well as to assist a lightweight interprocedural optimizer. all these profiles are represented in a portable form, thus they can be used across different platforms. we demonstrate that sampling-based fdo can achieve an average of 92 percent of the performance gains obtained using instrumentation-based exact profiles for both spec cint2000 and cint2006 benchmarks. the overhead of collection is only 0.93 percent on average, while compiler-based instrumentation incurs 2.0-351.5 percent overhead (and 10x overhead on an industrial web search application).",
    "present_kp": [
      "feedback directed optimization"
    ],
    "absent_kp": [
      "sample profile",
      "performance counter",
      "last branch record"
    ]
  },
  {
    "title": "discrete particle swarm optimization for high-order graph matching.",
    "abstract": "high-order graph matching aims at establishing correspondences between two sets of feature points using high-order constraints. it is usually formulated as an np-hard problem of maximizing an objective function. this paper introduces a discrete particle swarm optimization algorithm for resolving high-order graph matching problems, which incorporates several re-defined operations, a problem-specific initialization method based on heuristic information, and a problem-specific local search procedure. the proposed algorithm is evaluated on both synthetic and real-world datasets. its outstanding performance is validated in comparison with three state-of-the-art approaches.",
    "present_kp": [
      "high-order graph matching",
      "particle swarm optimization"
    ],
    "absent_kp": [
      "evolutionary algorithm"
    ]
  },
  {
    "title": "the structure of continuous uni-norms.",
    "abstract": "the concept of uni-norm. aggregation operators (uni-norms) was introduced by yager and rybalov to unify and generalize the t-norms and t-conorms. considering that uni-norms continuous on [0, 1](2) must be t-norms or t-conorms, we concentrate our attention on uni-norms continuous in (0, 1)(2) in this paper. we mainly investigate their properties, representation and structure.",
    "present_kp": [
      "operators",
      "uni-norm",
      "t-norm",
      "t-conorm"
    ],
    "absent_kp": []
  },
  {
    "title": "undergraduate research experiences in data mining.",
    "abstract": "the new interdisciplinary field of data mining emerged in the early 1990s as a response to the profusion of digital data generated in numerous fields such as biology, chemistry, astronomy, advertising, banking and finance, retail market, stock market, and the www. in this paper, i describe an undergraduate course in data mining offered at the college of saint benedict and saint john's university in spring of 2007 as a csci-317-upper-division \"topics in computer science\"- course, entitled \"data intelligence.\" one of the main objectives of the course was to engage students in experimental computing research through a number of carefully planned research activities resulting in better understanding of the course contents and deeper insights into the challenges faced by the data mining community.",
    "present_kp": [
      "data mining"
    ],
    "absent_kp": [
      "pattern recognition",
      "machine learning",
      "undergraduate course design"
    ]
  },
  {
    "title": "a single tri-axial accelerometer-based real-time personal life log system capable of human activity recognition and exercise information generation.",
    "abstract": "recording a personal life log (pll) of daily activities in a ubiquitous environment is an emerging application of information technology. in this work, we present a single tri-axial accelerometer-based pll system capable of human activity recognition and exercise information generation. our pll system exhibits two main functions: activity recognition and exercise information generation. for activity recognition, the system first recognizes a state of daily activities based on the statistical and spectral features of the accelerometer signals. an activity within the recognized state is then recognized using a set of augmented features, including autoregressive coefficients, signal magnitude area, and tilt angle, via linear discriminant analysis and hierarchical artificial neural networks. upon the recognition of each activity, the system further estimates exercise information that includes energy expenditure based on metabolic equivalents, stride length, step count, walking distance, and walking speed. our pll system operates in real-time, and the life log information it generates is archived in a daily log database. we have validated our pll system for six daily activities (i.e., lying, standing, walking, going-upstairs, going-downstairs, and driving) via subject-independent and subject-dependent recognition on a total of twenty subjects, achieving an average recognition accuracy of 94.43 and 96.61%, respectively. our results demonstrate the feasibility of a portable real-time pll system that could be used for u-lifecare and u-healthcare services in the near future.",
    "present_kp": [
      "personal life log",
      "activity recognition",
      "exercise information",
      "accelerometer"
    ],
    "absent_kp": []
  },
  {
    "title": "simulation optimization with qualitative variables and structural model changes: a genetic algorithm approach.",
    "abstract": "in many common simulation optimization methods the structure of the system stays the same and only the set of values for certain parameters of the system such as the number of machines in a station or the in-process inventory is varied from one evaluation to the next. the methodology described in this paper is a simulation-optimization process where the qualitative variables and the structure of the system are the subjects of optimization. here, the optimum response sought is a function of design and operation characteristics of the system such as the type of machines to use, dispatching rules, sequence of processing operations, etc. in the methodology developed here simulation models are automatically generated through an object-oriented process and are evaluated for various candidate configurations of the system. these candidates are suggested by a genetic algorithm (ga) that automatically guides the system towards better solutions. after simulating the alternatives, the results are returned to the ga to be utilized in selection of the next generation of configurations to be evaluated. this process continues until a satisfactory solution is obtained for the system.",
    "present_kp": [
      "optimization",
      "simulation",
      "qualitative variables"
    ],
    "absent_kp": [
      "genetic algorithms",
      "object oriented"
    ]
  },
  {
    "title": "clinical decision-support for diagnosing stress-related disorders by applying psychophysiological medical knowledge to an instance-based learning system.",
    "abstract": "an important procedure in diagnosing stress-related disorders caused by dysfunction in the interaction of the heart with breathing, i.e., respiratory sinus arrhythmia (rsa), is to analyse the breathing first and then the heart rate. analysing these measurements is a time-consuming task for the diagnosing clinician. a decision-support system in this area would reduce the analysis task of the clinician and enable him/her to give more attention to the patient. we have created a decision-support system which contains a signal classifier and a pattern identifier. the system performs an analysis of the physiological time series concerned which would otherwise be performed manually by the clinician. the signal-classifier, hr3modul, classifies heart-rate patterns by analysing both cardio- and pulmonary signals, i.e., physiological time series. hr3modul uses case-based reasoning (cbr), using a wavelet-based method for retrieving features from the signals. the system searches for familiar shapes in the signals by comparing them with shapes already stored. we have applied a best fit scheme for handling signals of different lengths, as the length of a breath is highly dynamic. we also apply automatic weighting to the features to obtain a more autonomous system. the classified heart signals indicate if a patient may be suffering from a stress-related disorder and the nature of the disorder. these classified signals are thereafter sent to the second subsystem, the pattern-identifier. the pattern-identifier analyses the classified signals and searches for familiar patterns by identifying sequences in the classified signals. the identified sequences give clinicians a more complete analysis of the measurements, providing them with a better basis for diagnosis. we have shown that a case-based classifier with a wavelet feature extractor and automatic weighting is a viable option for building a decision-support system for the psychophysiological domain, as it is at par, or even outperforms other retrieval techniques and is less complex.",
    "present_kp": [
      "respiratory sinus arrhythmia"
    ],
    "absent_kp": [
      "decision support",
      "case-based classification",
      "wavelet retrieval",
      "knowledge discovery"
    ]
  },
  {
    "title": "the case for research in game engine architecture.",
    "abstract": "this paper is a call for research in the field of game engine architecture and design, a more comprehensive and thorough understanding of which we consider to be essential for its development. we present a number of key aspects that may help to define the problem space and provide a catalogue of questions that we believe identify areas of interest for future investigation.",
    "present_kp": [
      "game engine",
      "game engine architecture"
    ],
    "absent_kp": [
      "entertainment systems"
    ]
  },
  {
    "title": "a geometry-based soft shadow volume algorithm using graphics hardware.",
    "abstract": "most previous soft shadow algorithms have either suffered from aliasing, been too slow, or could only use a limited set of shadow casters and/or receivers. therefore, we present a strengthened soft shadow volume algorithm that deals with these problems. our critical improvements include robust penumbra wedge construction, geometry-based visibility computation, and also simplified computation through a four-dimensional texture lookup. this enables us to implement the algorithm using programmable graphics hardware, and it results in images that most often are indistinguishable from images created as the average of 1024 hard shadow images. furthermore, our algorithm can use both arbitrary shadow casters and receivers. also, one version of our algorithm completely avoids sampling artifacts which is rare for soft shadow algorithms. as a bonus, the four-dimensional texture lookup allows for small textured light sources, and, even video textures can be used as light sources. our algorithm has been implemented in pure software, and also using the geforce fx emulator with pixel shaders. our software implementation renders soft shadows at 0.5--5 frames per second for the images in this paper. with actual hardware, we expect that our algorithm will render soft shadows in real time. an important performance measure is bandwidth usage. for the same image quality, an algorithm using the accumulated hard shadow images uses almost two orders of magnitude more bandwidth than our algorithm.",
    "present_kp": [
      "graphics hardware",
      "pixel shaders",
      "soft shadows"
    ],
    "absent_kp": []
  },
  {
    "title": "management and analysis of unstructured construction data types.",
    "abstract": "compared with structured data sources that are usually stored and analyzed in spreadsheets, relational databases, and single data tables, unstructured construction data sources such as text documents, site images, web pages, and project schedules have been less intensively studied due to additional challenges in data preparation, representation, and analysis. in this paper, our vision for data management and mining addressing such challenges are presented, together with related research results from previous work, as well as our recent developments of data mining on text-based, web-based, image-based, and network-based construction databases.",
    "present_kp": [
      "data mining",
      "construction"
    ],
    "absent_kp": [
      "information management"
    ]
  },
  {
    "title": "nonlinear multiresolution signal decomposition schemes - part ii: morphological wavelets.",
    "abstract": "in its original form, the wavelet transform is a linear tool. however, it has been increasingly recognized that nonlinear extensions are possible. a major impulse to the development of nonlinear wavelet transforms has been given by the introduction of the lifting scheme by sweldens, the aim of this paper, which is a sequel of a previous paper devoted exclusively to the pyramid transform, is to present an axiomatic framework encompassing most existing linear and nonlinear wavelet decompositions. furthermore, it introduces some, thus far unknown, wavelets based on mathematical morphology, such as the morphological haar wavelet, both in one and two dimensions. a general and flexible approach for the construction of nonlinear (morphological) wavelets is provided by the lifting scheme. this paper briefly discusses one example, the max-lifting scheme, which has the intriguing property that preserves local maxima in a signal over a range of scales, depending on how local or global these maxima are.",
    "present_kp": [
      "lifting scheme",
      "mathematical morphology",
      "max-lifting",
      "multiresolution signal decomposition",
      "nonlinear wavelet transform"
    ],
    "absent_kp": [
      "coupled and uncoupled wavelet decomposition",
      "morphological operators"
    ]
  },
  {
    "title": "a learner-centered approach to teaching ethics in computing.",
    "abstract": "this paper presents an approach to teaching computer ethics that blends the use of contemporary media, subscriptions to digests of current technology news, and reflective writing in a learner-centered strategy. this approach is designed to make use of activities and assignments that take advantage of (1) student interest in contemporary media (video and film) to provide motivation and context beyond historical case studies, (2) breaking news about technology and technology use in education to provide current real world context, and (3) reflective writing to stimulate thinking critically about the course content outside the classroom context. digests published three times weekly provide a constant flow of current real-world issues that can be used for focused reflective writing. contemporary media productions are viewed and then a writing assignment in a structured learning log is used to focus on ethical issues raised by the film. we present an example using a feature length film and subsequent learning log assignment.",
    "present_kp": [
      "media",
      "ethics",
      "writing"
    ],
    "absent_kp": []
  },
  {
    "title": "a pattern-theoretic characterization of biological growth.",
    "abstract": "mathematical and statistical modeling of biological growth is an important problem in medical diagnostics. here, we seek tools to analyze changes in anatomical parts using images collected over time. we introduce a structured model, called growth by random iterated diffeomorphisms (grid), that treats a cumulative growth deformation as a composition of several elementary deformations. each elementary deformation applies to a small region by capturing deformation local to that region and is characterized by a seed and a radial deformation pattern around that seed. these grid variables-seed locations and radial deformation patterns-are estimated from observed images in two steps: 1) estimate a cumulative deformation over an observation interval; 2) estimate grid variables using maximum-likelihood criterion from this estimated cumulative deformation. we demonstrate this framework using an mri image data of a rat's brain growth. for future statistical analysis, we propose a time-varying poisson process for the seed placements and a random drawing from a predetermined catalog of deformations for the radial deformation patterns.",
    "present_kp": [],
    "absent_kp": [
      "growth dynamics",
      "growth models",
      "growth patterns",
      "random diffeomorphism"
    ]
  },
  {
    "title": "low-power low-voltage class-ab linear ota for hf filters with a large tuning range.",
    "abstract": "this letter presents a new low-voltage class-ab differential linear ota. the proposed transconductor uses a novel scheme based on two cross-coupled class-ab pseudo-differential pairs biased by a flipped voltage follower . the transconductor has been designed using a 0.8 mum cmos technology to operate at 2 v supply voltage with only 260 muw of quiescent power consumption. simulation results show 90 mhz bandwidth with more than two decades of transconductance tuning range.",
    "present_kp": [
      "linear ota",
      "transconductor"
    ],
    "absent_kp": [
      "class ab"
    ]
  },
  {
    "title": "on (q+t,t)-arcs of type (0,2,t).",
    "abstract": "in this paper we construct an infinite series of (q + t, t)-arcs of type ( 0, 2, t). we show that this construction includes the korchmaros-mazzocca arcs, and we gain new infinite series of examples, too.",
    "present_kp": [
      "arcs"
    ],
    "absent_kp": [
      "sets without tangents",
      "polynomials",
      "power sums"
    ]
  },
  {
    "title": "semidiscrete formulations for transient transport at small time steps.",
    "abstract": "solutions of direct time-integration schemes for transient advection-diffusion-reaction problems that converge in time to conventional semidiscrete formulations may be polluted at small time steps by spurious spatial oscillations. this degradation is not an artifact of the time-marching scheme, but rather a property of the solution of the semidiscrete galerkin approximation itself. an analogy to steady advection-diffusion-reaction problems with a modified reaction coefficient by the rothe method of discretizing in time prior to spatial discretization provides an upper bound on the time step for the onset of spatial instability. spatial stabilization removes this pathology, leading to stabilized implicit time-integration schemes that are free of spurious oscillations at small time steps.",
    "present_kp": [
      "rothe method",
      "spatial stabilization"
    ],
    "absent_kp": [
      "semidiscrete advection-diffusion-reaction",
      "small time step oscillation"
    ]
  },
  {
    "title": "adaptive security management of real-time storage applications over nand based storage systems.",
    "abstract": "establishing the model of security-critical storage applications based on security protection of nand flash systems.. deriving a dynamic model that captures the vulnerability and utilization constraints in nand-based storage systems. designing a feedback control loop to guarantee security performance and soft real-time requirements. employing two pi controllers to achieve the utilization and vulnerability control in one framework.",
    "present_kp": [
      "storage application",
      "security-critical",
      "real-time",
      "feedback control"
    ],
    "absent_kp": [
      "nand flash memory",
      "scheduling"
    ]
  },
  {
    "title": "sensor placement and coordination via distributed multi-agent cooperative control.",
    "abstract": "this paper examines the problem of sensor placement and coordination to maximize the sensor utilization when monitoring different types of environments. our assumption is that the sensors are mobile and each sensor can have more than one type of sensing capabilities which can be active or not at each specific moment. the goal is to maximize the amount of information collected from the environment, given the limited amount of resources that the total of the available sensors can provide, and at the same time to be fault tolerant in failures of individual sensors by using a decentralized approach that re-organizes their placement in case of failures. we tackle this problem by employing a decentralized multi-agent coordination framework using message passing and the max-sum algorithm for building and maintaining a common picture of the area to be monitored. we show that by representing each sensor as an independent agent which can take decisions individually and at the same time can affect the decisions of its neighbouring sensor-agents we can provide a robust and efficient system for the monitoring of life-critical environments such as assistive environments or governmental infrastructures.",
    "present_kp": [
      "max-sum algorithm",
      "sensor placement and coordination"
    ],
    "absent_kp": [
      "multi-agent systems",
      "mobile sensors"
    ]
  },
  {
    "title": "implementation and evaluation of the msc course in health informatics in greece.",
    "abstract": "objectives: health informatics is a well established and important multi-disciplinary and inter-disciplinary field that not only involves informatics but also medicine, nursing, engineering, biology and other-related subjects. the program has been organized on the basis of an inter-university approach with the participation of five greek universities. the paper aims at providing a current description of the academic program and a preliminary evaluation of the implementation phase. methods: the paper presents a case study of a curriculum implementation from the phase of curriculum development to the phase of implementation and evaluation. due to the interdisciplinary character of the course appropriate procedures were undertaken to ensure that mixed backgrounds can assimilate the broad spectrum of the teaching material taught. in the first stages of the implementation international students mainly from europe attended the course. in addition, local graduates provided an extra dimension to the multi-layered difficulties and challenges of such a course implementation. results: the students registered in the course were from different backgrounds and disciplines. they were mainly from health sciences and engineering schools. the interdisciplinary arrangement of the course facilitated the proper exchange of thoughts, skills, and knowledge among and between students and teachers. conclusions: the postgraduate course in health informatics at the university of athens has now been running for more than fifteen consecutive years and is one of the first and longest standing courses in europe. continuous evaluation and adaptation is required to fit within the changing and evolving amazing field of biomedical and health informatics.",
    "present_kp": [
      "health informatics"
    ],
    "absent_kp": [
      "inter-univeisity academic programs",
      "health and medical informatics education"
    ]
  },
  {
    "title": "a novel fpga architecture and an integrated framework of cad tools for implementing applications.",
    "abstract": "a complete system for the implementation of digital logic in a field-programmable gate array (fpga) platform is introduced. the novel power-efficient fpga architecture was designed and simulated in stm 0.18 mu m cmos technology. the detailed design and circuit characteristics of the configurable logic block, the interconnection network, the switch box and the connection box were determined and evaluated in terms of energy, delay and area. a number of circuit-level low-power techniques were employed because power consumption was the primary concern. additionally, a complete tool framework for the implementation of digital logic circuits in fpga platforms is introduced. having as input vhdl description of an application, the framework derives the reconfiguration bitstream of fpga. the framework consists of: i) non-modified academic tools, ii) modified academic tools and iii) new tools. furthermore, the framework can support a variety of fpga architectures. qualitative and quantitative comparisons with existing academic and commercial architectures and tools are provided, yielding promising results.",
    "present_kp": [
      "fpga",
      "cad tools",
      "configuration bitstream"
    ],
    "absent_kp": [
      "circuit design",
      "rtl design"
    ]
  },
  {
    "title": "ode: a tool for distributing object-oriented applications.",
    "abstract": "object-oriented applications are increasingly being deployed in distributed computing environments. technologies, such as java rmi, and architectures, such as corba, dcom, and enterprise java beans, are facilitating and enhancing this trend. the performance and eventual success of these applications is dependent on distribution decisions made by the application designer. this decision is a complex one, involving a large number of alternatives and multiple conflicting criteria. rigorous approaches for effective distribution of object-oriented applications are still lacking. this paper describes the implementation of a practical and effective approach for distributing object-oriented applications. a prototype decision support systemobject distribution environment (ode)that implements the approach in the form of a user-friendly tool for the design of distributed object-oriented applications is described. ode has been successfully used in the distribution of a real world distributed object-oriented system.",
    "present_kp": [
      "object distribution",
      "decision support system"
    ],
    "absent_kp": [
      "distributed systems",
      "linear programming",
      "multiple criteria optimization"
    ]
  },
  {
    "title": "fault tolerance of hypercubes and folded hypercubes.",
    "abstract": "let (g = (v,e)) be a connected graph. the conditional edge connectivity (lambda _delta ^k(g)) is the cardinality of the minimum edge cuts, if any, whose deletion disconnects (g) and each component of (g - f) has (delta ge k). we assume that (f subseteq e) is an edge set, (f) is called edge extra-cut, if (g - f) is not connected and each component of (g - f) has more than (k) vertices. the edge extraconnectivity (lambda _mathrm{e}^k(g)) is the cardinality of the minimum edge extra-cuts. in this paper, we study the conditional edge connectivity and edge extraconnectivity of hypercubes and folded hypercubes.",
    "present_kp": [
      "conditional edge connectivity",
      "edge extraconnectivity"
    ],
    "absent_kp": [
      "interconnection networks",
      "fault-tolerance"
    ]
  },
  {
    "title": "reuse of manufacturing knowledge to facilitate platform-based product realization.",
    "abstract": "product platforming is a technique for exploiting commonality across a family of products. while utilizing a common platform can have many advantages when developing and manufacturing products, the approach places greater demands on collaboration, in particular the sharing and reuse of knowledge and information. repositories are intended to facilitate information sharing across organizational groups and geographically distributed collaborators. a particular challenge in utilizing repositories is culling a search for the most appropriate information for the problem at hand. the reuse existing unit for shape and efficiency (r.e.u.s.e.) method facilitates the search of information in a repository through three stages that consider similarity, efficiency, and configuration. automated search and filter techniques are implemented with interaction with the user to effectively obtain the desired results. the similarity stud), uses thresholds to clarify different opportunities for reuse. the user can then select alternatives for further examination based on efficiency of satisfaction of desired characteristics. the degree of modification of the similar alternatives is reported to assist in the configuration of the new design. this method contributes to the field by (a) accounting for the variety of the product family during the reuse of existing process design information; (b) integrating an efficiency assessment for retrieval by considering characteristics beyond cost; and (c) addressing the search with a multicriteria method. the implementation of the r.e.u.s.e method is supported with an example of assembly line design for an air conditioner module in automobile production.",
    "present_kp": [
      "reuse",
      "similarity",
      "knowledge",
      "manufacturing",
      "product platform",
      "assembly line"
    ],
    "absent_kp": []
  },
  {
    "title": "precise rates in the law of the logarithm for negatively associated random variables.",
    "abstract": "let {xn;n?1} be a strictly stationary sequence of negatively associated random variables with mean zero and finite variance. set s n = ? k = 1 n x k , mn=maxk?n|sk|,n?1 m n = max k ? n | s k | , n ? 1 . suppose ? 2 = e x 1 2 + 2 ? k = 2 ? e x 1 x k . we study the precise rates of a kind of weighted infinite series of p { m n n log n } and p { | s n | n log n } as 0 0 , and p { m n 2 n 8 log n } as . the results are related to the convergence rates of the law of the logarithm and the chung type law of the logarithm.",
    "present_kp": [
      "the law of the logarithm",
      "chung type law of the logarithm",
      "negatively associated random variables"
    ],
    "absent_kp": [
      "l2 convergence",
      "a.s. convergence"
    ]
  },
  {
    "title": "antivirus security: naked during updates.",
    "abstract": "the security of modern computer systems heavily depends on security tools, especially on antivirus software solutions. in the anti-malware research community, development of techniques for evading detection by antivirus software is an active research area. this has led to malware that can bypass or subvert antivirus software. the common strategies deployed include the use of obfuscated code and staged malware whose first instance (usually installer such as dropper and downloader) is not detected by the antivirus software. increasingly, most of the modern malware are staged ones in order for them to be not detected by antivirus solutions at the early stage of intrusion. the installers then determine the method for further intrusion including antivirus bypassing techniques. some malware target boot and/or shutdown time when antivirus software may be inactive so that they can perform their malicious activities. however, there can be another time frame where antivirus solutions may be inactive, namely, during the time of update. all antivirus software share a unique characteristic that they must be updated at a very high frequency to provide up-to-date protection of their system. in this paper, we suggest a novel attack vector that targets antivirus updates and show practical examples of how a system and antivirus software itself can be compromised during the update of antivirus software. local privilege escalation using this vulnerability is also described. we have investigated this design vulnerability with several of the major antivirus software products such as avira, avg, mcafee, microsoft, and symantec and found that they are vulnerable to this new attack vector. the paper also discusses possible solutions that can be used to mitigate the attack in the existing versions of the antivirus software as well as in the future ones.",
    "present_kp": [
      "security",
      "antivirus",
      "malware",
      "vulnerability",
      "local privilege escalation"
    ],
    "absent_kp": [
      "code execution",
      "denial of service"
    ]
  },
  {
    "title": "collaboratory use by peripheral scientists.",
    "abstract": "recent years have seen an increasing use of collaboratories in scientific work. it is hypothesized that by enabling scientists to reach remotely located data, instruments and experts, collaboratories will benefit peripheral scientists (e.g., scientists from developing countries and scientists from minority colleges in the u.s.) more than core scientists. however, previous studies on computer network use have shown mixed results regarding peripherality effects. adopting a qualitative approach, this study intends to investigate cultural, political, and technical factors that influence collaboratory use by peripheral scientists.",
    "present_kp": [
      "peripheral scientists",
      "collaboratory"
    ],
    "absent_kp": [
      "scientific collaboration",
      "interviews"
    ]
  },
  {
    "title": "government records and records management: law on the right to information in turkey.",
    "abstract": "operating of the laws on right to information is related to effective management of government records and information having social value. this article contains the relation of government records and records management considering the role of records management at institutions in execution of law on right to information in turkey, and the evaluation of turkish law on the right to information that came into force in 2004 in view of records management and archival approaches.",
    "present_kp": [
      "government records",
      "records management",
      "turkish law on the right to information"
    ],
    "absent_kp": []
  },
  {
    "title": "a random number.",
    "abstract": "this pedagogical tip presents a physical means for generating a random number - that turns out to be not so random after all.",
    "present_kp": [],
    "absent_kp": [
      "cs1/2",
      "random numbers",
      "computational science",
      "algorithms"
    ]
  },
  {
    "title": "an efficient resource allocation scheme using particle swarm optimization.",
    "abstract": "developing techniques for optimal allocation of limited resources to a set of activities has received increasing attention in recent years. in this paper, an efficient resource allocation scheme based on particle swarm optimization (pso) is developed. different from many existing evolutionary algorithms for solving resource allocation problems (raps), this pso algorithm incorporates a novel representation of each particle in the population and a comprehensive learning strategy for the pso search process. the novelty of this representation lies in that the position of each particle is represented by a pair of points, one on each side of the constraint hyper-plane in the problem space. the line joining these two points intersects the constraint hyper-plane and their intersection point indicates a feasible solution. with the evaluation value of the feasible solution used as the fitness value of the particle, such a representation provides an effective way to ensure the equality resource constraints in raps are met. without the distraction of infeasible solutions, the particle thus searches the space smoothly. in addition, particles search for optimal solutions by learning from themselves and their neighborhood using the comprehensive learning strategy, helping prevent premature convergence and improve the solution quality for multimodal problems. this new algorithm is shown to be applicable to both single-objective and multiobjective raps, with performance validated by a number of benchmarks and by a real-world bed capacity planning problem. experimental results verify the effectiveness and efficiency of the proposed algorithm.",
    "present_kp": [
      "bed capacity planning",
      "particle swarm optimization "
    ],
    "absent_kp": [
      "multiobjective resource allocation problem ",
      "resource allocation problem "
    ]
  },
  {
    "title": "enhancing integrated earthquake simulation with high performance computing.",
    "abstract": "integrated earthquake simulation (ies) is a seamless simulation of the three earthquake processes, namely, the earthquake hazard process, the earthquake disaster process and the anti-disaster action process. high performance computing (hpc) is essential if ies, or particularly, the simulation of the earthquake disaster process is applied to an urban area in which 104?6 structures are located. ies is enhanced with parallel computation, and its performance is examined, so that virtual earthquake disaster simulation will be made for a model of an actual city by inputting observed strong ground motion. it is shown that parallel ies has fairly good scalability even when advanced non-linear seismic structure analysis is used.",
    "present_kp": [
      "high performance computing"
    ],
    "absent_kp": [
      "earthquake engineering",
      "numerical simulation",
      "software integration",
      "software engineering",
      "object oriented design"
    ]
  },
  {
    "title": "event-based modeling and processing of digital media.",
    "abstract": "capture, processing, and assimilation of digital media-based information such as video, images, or audio requires a unified framework within which signal processing techniques and data modeling and retrieval approaches can act and interact. in this paper we present the rudiments of such a framework based on the notion of \"events\". this framework serves the dual roles of a conceptual data model as well as a prescriptive model that defines the requirements for appropriate signal processing. amongst the key advantages of this framework, lies the fact that it fundamentally brings together the traditionally diverse disciplines of databases and (various areas of) digital signal processing. in addition to the conceptual event-based framework, we present a physical implementation of the event model. our implementation specifically targets the problem of processing, storage, and querying of multimedia information related to indoor group- oriented activities such as meetings. such multimedia information may comprise of video, image, audio, and text-based data. we use this application context to illustrate many of the practical challenges that are encountered in this area, our solutions to them, and the open problems that require research across databases, computer vision, audio processing, and multimedia.",
    "present_kp": [
      "requirements",
      "challenge",
      " framework ",
      "video",
      "use",
      "digital media",
      "event",
      "context",
      "model",
      "paper",
      "computer vision",
      "practical",
      "audio",
      "research",
      "digital signal processing",
      "meeting",
      "storage",
      "implementation",
      "multimedia",
      "text",
      "retrieval",
      "data",
      "process",
      "image",
      "physical",
      "signal processing",
      "database",
      "roles"
    ],
    "absent_kp": [
      "activation",
      "applications",
      "imaging",
      "informal",
      "data models",
      "queries"
    ]
  },
  {
    "title": "a blocked qr-decomposition for the parallel symmetric eigenvalue problem.",
    "abstract": "new parallel algorithm for the qr-decomposition of tall and skinny matrices (based on choleskyqr). reduced synchronization requirements, compared to the classic householder qr-decomposition. adaptive blocking during householder vector generation, to guarantee numerical stability. considerable speedups were achieved on a bluegene/p and power6 system.",
    "present_kp": [
      "qr-decomposition"
    ],
    "absent_kp": [
      "eigenvalue and eigenvector computation",
      "two-step tridiagonalization",
      "parallelization"
    ]
  },
  {
    "title": "compiling cryptographic protocols for deployment on the web.",
    "abstract": "cryptographic protocols are useful for trust engineering in web transactions. the cryptographic protocol programming language (cppl) provides a model wherein trust management annotations are attached to protocol actions, and are used to constrain the behavior of a protocol participant to be compatible with its own trust policy. the first implementation of cppl generated stand-alone, single-session servers, making it unsuitable for deploying protocols on the web. we describe a new compiler that uses a constraint-based analysis to produce multi-session server programs. the resulting programs run without persistent tcp connections for deployment on traditional web servers. most importantly, the compiler preserves existing proofs about the protocols. we present an enhanced version of the cppl language, discuss the generation and use of constraints, show their use in the compiler, formalize the preservation of properties, present subtleties, and outline implementation details.",
    "present_kp": [
      "cryptographic protocols",
      "cppl"
    ],
    "absent_kp": [
      "sessions",
      "http"
    ]
  },
  {
    "title": "association of pre-pregnancy weight and weight gain with perinatal mortality.",
    "abstract": "reducing infant mortality is one of the primary millennium development goals 2015. a lot of effort has been made to reduce infant mortality but it remains high in most of the developing countries and the underdeveloped world. perinatal mortality is a cause of great emotional pain and social unrest. the main cause of pregnancy failure in the developed world is obesity but in the under-developed world the main cause remains malnutrition. however, their are a mix of factors that affect pregnancy failure in the developing countries. pakistan has a very high infant mortality rate which stands at 78 deaths per 1000 births. the reasons for this are many including lack of proper healthcare. this is because of a severe shortage of healthcare professionals and specialists in pakistan. the gap in healthcare may be overcome by leveraging it to provide automated healthcare. in this paper, we show how machine learning may be used to predict perinatal failure. we examine the relationship between pre-pregnancy weight, weight gain during pregnancy and the body mass index (bmi) to investigate how they relate to foetal failure. we employ the k nearest neighbor (k-nn) technique to automatically differentiate between successful and failed pregnancies. our method is able to predict the the outcome of a pregnancy with about 95% accuracy.",
    "present_kp": [
      "machine learning",
      "perinatal mortality"
    ],
    "absent_kp": [
      "automatic diagnosis"
    ]
  },
  {
    "title": "a linearly implicit conservative scheme for the coupled nonlinear schrodinger equation.",
    "abstract": "the coupled nonlinear schrodinger equation models several intersting physical phenomena. it presents a model equation for optical fiber with linear birefringence. in this paper, we present a linearly implicit conservative method to solve this equation. this method is second order accurate in space and time and conserves the energy exactly. many numerical experiments have been conducted and have shown that this method is quite accurate and describe the interaction picture clearly.",
    "present_kp": [],
    "absent_kp": [
      "coupled nonlinear shrodinger equation",
      "linearly implicit scheme",
      "finite difference method",
      "solitons"
    ]
  },
  {
    "title": "self-adaptive multimethod search for global optimization in real-parameter spaces.",
    "abstract": "many different algorithms have been developed in the last few decades for solving complex real-world search and optimization problems. the main focus in this research has been on the development of a single universal genetic operator for population evolution that is always efficient for a diverse set of optimization problems. in this paper, we argue that significant advances to the field of evolutionary computation can be made if we embrace a concept of self-adaptive multimethod optimization in which multiple different search algorithms are run concurrently, and learn from each other through information, exchange using a common population of points. we present an evolutionary algorithm, entitled a multialgorithm genetically adaptive method for single objective optimization (amalgam-so), that implements this concept of self adaptive multimethod search. this method simultaneously merges the strengths of the covariance matrix adaptation (cma) evolution strategy, genetic algorithm (ga), and particle swarm optimizer (pso) for population evolution and implements a self-adaptive learning strategy to automatically tune the number of offspring these three individual algorithms are allowed to contribute during each generation. benchmark results in 10, 30, and 50 dimensions using synthetic functions from the special session on real-parameter optimization of cec 2005 show that amalgam-so obtains similar efficiencies as existing algorithms on relatively simple unimodal problems, but is superior for more complex higher dimensional multimodal optimization problems. the new search method scales well with increasing number of dimensions, converges in the close proximity of the global minimum for functions with noise induced multimodality, and is designed to take full advantage of the power of distributed computer networks.",
    "present_kp": [
      "optimization"
    ],
    "absent_kp": [
      "adaptive estimation",
      "elitism",
      "genetic algorithms",
      "nonlinear estimation"
    ]
  },
  {
    "title": "pilot study of the development of a theory-based instrument to evaluate the communication process during multidisciplinary team conferences in rheumatology.",
    "abstract": "coordinated teams with multidisciplinary team conferences are generally seen as a solution to the management of complex health conditions. however, problems regarding the process of communication during team conferences are reported, such as the absence of a common language or viewpoint and the exchange of irrelevant or repeated information. to determine the outcome of interventions aimed at improving communication during team conferences, a reliable and valid assessment method is needed. to investigate the feasibility of a theory-based measurement instrument for assessing the process of the communication during multidisciplinary team conferences in rheumatology. an observation instrument was developed based on communication theory. the instrument distinguishes three types of communication: (i) grounding activities, (ii) coordination of non-team activities, and (iii) coordination of team activities. to assess the process of communication during team conferences in a rheumatology clinic with inpatient and day patient facilities, team conferences were videotaped. to determine the inter-rater reliability, in 20 conferences concerning 10 patients with rheumatoid arthritis admitted to the inpatient unit, the instrument was applied by two investigators independently. content validity was determined by analysing and comparing the results of initial and follow-up team conferences of 25 consecutive patients with rheumatoid arthritis admitted to the day patient unit (wilcoxon signed rank test). the inter-rater reliability was excellent with the intra-class correlation coefficients being >0.98 for both types i and iii communications in 10 initial and 10 follow-up conferences (type ii was not observed). an analysis of an additional 25 initial and 86 follow-up team conferences showed that time spent on grounding (type i) made up the greater part of the contents of communication (87% s.d. 14 and 60% s.d. 29 in initial and follow-up conferences, respectively), which is significantly more compared to time spent on co-ordination (p<0.001 and 0.02 for categories ii and iii, respectively). moreover, significantly less time spent was spent on grounding in follow-up as compared to initial team conferences, whereas the time spent on coordination (type iii) increased (both p-values <0.001). this theory-based measurement instrument for describing and evaluating the communication process during team conferences proved to be reliable and valid in this pilot study. its usefulness to detect changes in the communication process, e.g. after implementing systems for re-structuring team conferences mediated by ict applications, should be further examined.",
    "present_kp": [
      "team conferences",
      "communication process",
      "theory",
      "measurement instrument"
    ],
    "absent_kp": [
      "information and communication technology ",
      "rehabilitation"
    ]
  },
  {
    "title": "traffic aware cross-site virtual machine migration in future mobile cloud computing.",
    "abstract": "by moving virtual machines(vms) to the sites closest to their users, cross-site vm migration is promising to improve users experience. however, when multiple vms are required to be migrated, an arbitrary migration sequence will possibly congest the inter-site links. to avoid such congestion and also maximize the number of successful migrations, in this paper, we formulate vm migration sequence planning problem as a mixed integer linear programming(milp) problem, which considers both inter-vm communication traffic and migration traffic. due to the high computational complexity to get the optimal results, we further propose a heuristic algorithm referred as minuti-o to approximate the optimal results with low complexity. the extensive simulation results show that the success ratio achieved by minuti-o is close to the optimal results with less than 5 % gap under different topology with variation of migration requests and network conditions.",
    "present_kp": [
      "vm migration",
      "mobile cloud computing",
      "sequence planning"
    ],
    "absent_kp": []
  },
  {
    "title": "efficient and accurate numerical methods for the klein-gordon-schrodinger equations.",
    "abstract": "in this paper, we present efficient, unconditionally stable and accurate numerical methods for approximations of the klein-gordon-schrodinger (kgs) equations with/without damping terms. the key features of our methods are based on: (i) the application of a time-splitting spectral discretization for a schrodinger-type equation in kgs, (ii) the utilization of fourier pseudospectral discretization for spatial derivatives in the klein-gordon equation in kgs, (iii) the adoption of solving the ordinary differential equations (odes) in phase space analytically under appropriate chosen transmission conditions between different time intervals or applying crank-nicolson/leap-frog for linear/nonlinear terms for time derivatives. the numerical methods are either explicit or implicit but can be solved explicitly, unconditionally stable, and of spectral accuracy in space and second-order accuracy in time. moreover, they are time reversible and time transverse invariant when there is no damping terms in kgs, conserve (or keep the same decay rate of) the wave energy as that in kgs without (or with a linear) damping term, keep the same dynamics of the mean value of the meson field, and give exact results for the plane-wave solution. extensive numerical tests are presented to confirm the above properties of our numerical methods for kgs. finally, the methods are applied to study solitary-wave collisions in one dimension (id), as well as dynamics of a 2d problem in kgs.",
    "present_kp": [
      "klein-gordon-schrodinger equations",
      "klein-gordon equation",
      "wave energy",
      "unconditionally stable"
    ],
    "absent_kp": [
      "nonlinear schrodinger equation",
      "time splitting",
      "plane wave",
      "solitary wave",
      "schrodinger-yukawa equations"
    ]
  },
  {
    "title": "openmp versus mpi for pde solvers based on regular sparse numerical operators.",
    "abstract": "two parallel programming models represented by openmp and mpi are compared for pde solvers based on regular sparse numerical operators. as a typical representative of such an operator, a finite difference approximation of the euler equations for fluid flow is considered. the comparison of programming models is made with regard to uniform memory access (uma), non-uniform memory access (numa), and self-optimizing numa (numa-opt) computer architectures. by numa-opt, we mean numa systems extended with self-optimization algorithms, in order to reduce the non-uniformity of the memory access time. the main conclusions of the study are: (1) that openmp is a viable alternative to mpi on uma and numa-opt architectures; (2) that openmp is not competitive on numa platforms, unless special care is taken to get an initial data placement that matches the algorithm; (3) that for openmp to be competitive in the numa-opt case, it is not necessary to extend the openmp model with additional data distribution directives, nor to include user-level access to the page migration library.",
    "present_kp": [
      "openmp",
      "mpi",
      "uma",
      "numa",
      "optimization",
      "pde",
      "euler"
    ],
    "absent_kp": [
      "stencil"
    ]
  },
  {
    "title": "sentiment analysis of movie reviews on discussion boards using a linguistic approach.",
    "abstract": "we propose a linguistic approach for sentiment analysis of message posts on discussion boards. a sentence often contains independent clauses which can represent different opinions on the multiple aspects of a target object. therefore, the proposed system provides clause-level sentiment analysis of opinionated texts. for each sentence in a message post, it generates a dependency tree, and splits the sentence into clauses. then it determines the contextual sentiment score for each clause utilizing grammatical dependencies of words and the prior sentiment scores of the words derived from sentiwordnet and domain specific lexicons. negation is also delicately handled in this study, for instance, the term \"not superb\" is assigned a lower negative sentiment score than the term \"not good\" . we have experimented with a dataset of movie review sentences, and the experimental results show the effectiveness of the proposed approach.",
    "present_kp": [
      "dependency tree",
      "sentiment analysis",
      "movie reviews",
      "discussion board"
    ],
    "absent_kp": []
  },
  {
    "title": "design guidelines for the integration of geiger-mode avalanche diodes in standard cmos technologies.",
    "abstract": "the goal of this paper is to provide some useful design guidelines at the device level regarding the main challenges to be typically faced in the design and integration of geiger-mode avalanche diodes in a standard cmos process. different techniques are found in literature in order to avoid premature edge breakdown with the aim of limiting the electric field at the edges to be weaker than in the multiplication region. in this article, the use of such techniques, the conditions where they can effectively work and above all their limitations are studied by means of tcad simulations for various diode architectures. additionally, the noise performance is discussed by focusing on the band-to-band tunneling and shallow trench isolation enhanced dark count rates. geiger-mode bias techniques as well as a synthesis on the pros and cons of the various avalanche diode architectures are finally presented aiming at facilitating future design choices.",
    "present_kp": [
      "avalanche diode",
      "premature edge breakdown",
      "dark count rate",
      "geiger-mode",
      "band-to-band tunneling"
    ],
    "absent_kp": [
      "peb",
      "guard-ring",
      "deep sub-micrometer technology"
    ]
  },
  {
    "title": "dummy fill aware buffer insertion after layer assignment based on an effective estimation model.",
    "abstract": "this paper studies the impact of dummy fill for chemical mechanical polishing (cmp)-induced capacitance variation on buffer insertion based on a virtual cmp fill estimation model. compared with existing methods, our algorithm is more feasible by performing buffer insertion not in post-process but during early physical design. our contributions are threefold. first, we introduce an improved fast dummy fill amount estimation algorithm based on [4], and use some speedup techniques (tile merging, fill factor and amount assigning) for early estimation. second, based on some reasonable assumptions, we present an optimum virtual dummy fill method to estimate dummy position and the effect on the interconnect capacitance. then the dummy fill estimation model was verified by our experiments. third, we use this model in early buffer insertion after layer assignment considering the effects of dummy fill. experimental results verified the necessity of early dummy fill estimation and the validity of our algorithm. buffer insertion considering dummy fill during early physical design is necessary and our algorithm is promising.",
    "present_kp": [
      "buffer insertion",
      "physical design",
      "dummy fill"
    ],
    "absent_kp": [
      "vlsi",
      "dfm"
    ]
  },
  {
    "title": "reconsidering assessment in online/hybrid courses: knowing versus learning.",
    "abstract": "this study explores the influence of assessment on students' online written discussions. a two-by-two design was used to understand students' expression of knowledge and of learning in the contexts of \"regular\" online discussions versus \"final test\" online discussions. findings suggested that assessment had an impact on how students interacted online and in their use of rhetorical moves; and that knowing and learning are related but distinct constructs, correlated within each writing context, dissociated across contexts, and performing differentially as a function of students' perceptions of academic demands. we discuss the limitations of traditional assessment, offer an alternative approach, and conclude with practical suggestions for online/hybrid course instructors.",
    "present_kp": [
      "assessment"
    ],
    "absent_kp": [
      "online course",
      "discourse analysis",
      "social learning",
      "asynchronous written discussion"
    ]
  },
  {
    "title": "energy flow prediction in built-up structures through a hybrid finite element/wave and finite element approach.",
    "abstract": "numerical tool for the energy flow evaluation in a periodic substructure from the near-field to the far-field domain. the near-field part is then modeled by finite element method (fem). the periodic structure and the far-field part are regarded as waveguides and modeled by an enhanced wave finite element method (wfem) where a modal reduction technique is employed to accelerate the calculation.. a multi-scale strategy is used such that the final matrices dimensions of the built-up structure are largely reduced disorders. an application is presented, a structural dynamical system coupled with periodic resistive piezoelectric shunts is discussed.",
    "present_kp": [
      "modal reduction",
      "energy flow"
    ],
    "absent_kp": [
      "multi-scale modeling",
      "wave and finite element method",
      "forced response",
      "periodic waveguide"
    ]
  },
  {
    "title": "structural optimization of 3d masonry buildings.",
    "abstract": "in the design of buildings, structural analysis is traditionally performed after the aesthetic design has been determined and has little influence on the overall form. in contrast, this paper presents an approach to guide the form towards a shape that is more structurally sound. our work is centered on the study of how variations of the geometry might improve structural stability. we define a new measure of structural soundness for masonry buildings as well as cables, and derive its closed-form derivative with respect to the displacement of all the vertices describing the geometry. we start with a gradient descent tool which displaces each vertex along the gradient. we then introduce displacement operators, imposing constraints such as the preservation of orientation or thickness; or setting additional objectives such as volume minimization.",
    "present_kp": [
      "structural stability",
      "optimization"
    ],
    "absent_kp": [
      "statics",
      "architecture"
    ]
  },
  {
    "title": "a neuro-dynamic programming-based optimal controller for tomato seedling growth in greenhouse systems.",
    "abstract": "this work proposes a neuro-dynamic programming-based optimal controller to guide the growth of tomato seedling crops by manipulating its environmental conditions in a greenhouse. the neurocontroller manages the growth development of the crop, while minimizing a predefined cost function that considers the operative costs and the final state errors under physical constraints on process variables and actuator signals. the aim is to guide the growth of tomato seedlings by controlling the microclimate of the greenhouse. the design process of the neurocontroller considers the nonlinear dynamic behavior of the crop-greenhouse system model and the real climate data. simulations of the proposed approach allow for contrasting its performance against those of other strategies for tomato seedling crop development subject to various climatic conditions.",
    "present_kp": [
      "neuro-dynamic programming",
      "optimal control"
    ],
    "absent_kp": [
      "greenhouse control systems",
      "intelligent agriculture",
      "neurocontrollers"
    ]
  },
  {
    "title": "on the capacity of bicm with qam constellations.",
    "abstract": "in this tutorial paper we analyze the capacity of bit-interleaved coded modulation (bicm) with quadrature amplitude modulation (qam) constellations, and we pay special attention to different bit-to-symbol labeling strategies. the relation between the bicm capacity and the capacity of other cm schemes such as trellis coded modulation (tcm) and multilevel codes (mlc) is analyzed. motivated by the fact that for bicm with some particular labelings, the same e b / n 0 maps to more than one bicm capacity value, we study the relation between the capacity and e b / n 0 . in particular, we present some analytical results on this relation, and we also give an intuitive explanation for the somehow contradictory behavior of these curves.",
    "present_kp": [
      "mlc",
      "quadrature amplitude modulation",
      "tcm",
      "bicm",
      "coded modulation"
    ],
    "absent_kp": [
      "channel capacity",
      "gray code",
      "binary labeling"
    ]
  },
  {
    "title": "sparse communication networks and efficient routing in the plane (extended abstract).",
    "abstract": "traditional approaches to network design separate the issues of designing the network itself and designing its management and control subsystems. this paper proposes an approach termed routing-oriented network design , which is based on designing the network topology and its routing scheme together, attempting to optimize some of the relevant parameters of both simultaneously. this approach is explored by considering the design of communication networks supporting efficient routing in the special case of points located in the euclidean plane. the desirable network parameters considered include low degree and small number of communication links. the desirable routing parameters considered include small routing tables, small number of hops and low routing stretch. two rather different schemes are presented, one based on direct navigation in the plane and the other based on efficient hierarchical tree covers. on a collection of n sites with diameter d , these methods yield networks with maximum degree o (log d ) (hence a total of o ( n log d ) communication links), coupled with routing schemes with constant routing stretch, o (log n log d ) memory bits per vertex and routes with at most log n or log d hops.",
    "present_kp": [
      "network",
      "communication",
      "tree",
      "point",
      "log",
      "collect",
      "design",
      "tables",
      "direct",
      "links",
      "case",
      "network design",
      "paper",
      "navigation",
      "control",
      "management",
      "yield",
      "hierarchic",
      "method",
      "communication networks",
      "routing",
      "scheme"
    ],
    "absent_kp": [
      "efficiency",
      "relevance",
      "abstraction",
      "locatability",
      "optimality",
      "memorialized",
      "network topologies"
    ]
  },
  {
    "title": "evolutionary repair of faulty software.",
    "abstract": "testing and fault localization are very expensive software engineering tasks that have been tried to be automated. although many successful techniques have been designed, the actual change of the code for fixing the discovered faults is still a human-only task. even in the ideal case in which automated tools could tell us exactly where the location of a fault is, it is not always trivial how to fix the code. in this paper we analyse the possibility of automating the complex task of fixing faults. we propose to model this task as a search problem, and hence to use for example evolutionary algorithms to solve it. we then discuss the potential of this approach and how its current limitations can be addressed in the future. this task is extremely challenging and mainly unexplored in the literature. hence, this paper only covers an initial investigation and gives directions for future work. a research prototype called jaff and a case study are presented to give first validation of this approach.",
    "present_kp": [
      "repair",
      "fault localization"
    ],
    "absent_kp": [
      "automated debugging",
      "genetic programming",
      "search based software engineering",
      "coevolution"
    ]
  },
  {
    "title": "fuzzy unidirectional force control of constrained robotic manipulators.",
    "abstract": "the end effector of a robotic arm is required to keep a contact on the contour of the constraint surface in tasks such as deburring and grinding. being different from contacts resulted from general mechanical pairs, such a contact is unidirectional, or equivalently, the contact force can only act along the outward normal of the constraint surface at the contact point. how to achieve this specification was not addressed explicitly in many position/force control schemes developed so far, instead it was assumed in the development of controllers. in this paper, the unidirectionality of the contact force is explicitly included in modeling and control of constrained robot system. a fuzzy tuning mechanism is developed to generate the impedance model resulted from the continuous contact made by the end effector of the robotic manipulator on the constraint surface while it is in motion. a controller is then developed based on the fuzzy rule bases and the nonlinear feedback technique. the simulation is carried out to verify the effectiveness of the approach.",
    "present_kp": [
      "force control"
    ],
    "absent_kp": [
      "fuzzy relations",
      "impedance control"
    ]
  },
  {
    "title": "a quadratic construction for zielonka automata with acyclic communication structure.",
    "abstract": "asynchronous automata are parallel compositions of finite-state processes synchronizing over shared variables. a deep theorem due to zielonka says that every regular trace language can be recognized by a deterministic asynchronous automaton. the construction is rather involved and the most efficient variant produces automata which are exponential in the number of processes and polynomial in the size of the dfa. in this paper we show a simple, quadratic construction in the case where the synchronization actions are binary and define an acyclic communication graph.",
    "present_kp": [
      "asynchronous automata"
    ],
    "absent_kp": [
      "distributed synthesis"
    ]
  },
  {
    "title": "palmprint verification based on 2d gabor wavelet and pulse-coupled neural network.",
    "abstract": "to alleviate the limitation that the recent texture based algorithms for palmprint recognition yield unsatisfactory robustness to the variations of orientation, position and illumination in capturing palmprint images, this paper describes a novel texture based algorithm for palmprint recognition combining 2d gabor wavelets and pulse coupled neural network (pcnn). in the proposed algorithm, palmprint images are decomposed by 2d gabor wavelets, and then pcnn is employed to imitate the creatural vision perceptive process and decompose each gabor subband into a series of binary images. entropies for these binary images are calculated and regarded as features. a support vector machine-based classifier is employed to implement classification. experimental results show that the proposed approach yields a better performance in terms of the correct classification percentages and relatively high robustness to the variations of orientation, position and illumination compared with the recent texture based approaches.",
    "present_kp": [
      "gabor wavelet",
      "pulse-coupled neural network",
      "palmprint recognition"
    ],
    "absent_kp": [
      "support vector machine ",
      "entropy"
    ]
  },
  {
    "title": "a distributed activity scheduling algorithm for wireless sensor networks with partial coverage.",
    "abstract": "one of the most important design objectives in wireless sensor networks (wsn) is minimizing the energy consumption since these networks are expected to operate in harsh conditions where the recharging of batteries is impractical, if not impossible. the sleep scheduling mechanism allows sensors to sleep intermittently in order to reduce energy consumption and extend network lifetime. in applications where 100% coverage of the network field is not crucial, allowing the coverage to drop below full coverage while keeping above a predetermined threshold, i.e., partial coverage, can further increase the network lifetime. in this paper, we develop the distributed adaptive sleep scheduling algorithm (dassa) for wsns with partial coverage. dassa does not require location information of sensors while maintaining connectivity and satisfying a user defined coverage target. in dassa, nodes use the residual energy levels and feedback from the sink for scheduling the activity of their neighbors. this feedback mechanism reduces the randomness in scheduling that would otherwise occur due to the absence of location information. the performance of dassa is compared with an integer linear programming (ilp) based centralized sleep scheduling algorithm (cssa), which is devised to find the maximum number of rounds the network can survive assuming that the location information of all sensors is available. dassa is also compared with the decentralized dgt algorithm. dassa attains network lifetimes up to 92% of the centralized solution and it achieves significantly longer lifetimes compared with the dgt algorithm.",
    "present_kp": [
      "wireless sensor networks",
      "partial coverage"
    ],
    "absent_kp": [
      "energy efficiency",
      "sleep/activity scheduling"
    ]
  },
  {
    "title": "the enhanced quality function deployment for developing virtual items in massive multiplayer online role playing games.",
    "abstract": "because of the huge potential profit, the development of virtual items in massive multiplayer online role playing games (mmorpgs) has lately begun receiving attention. as a successful means for developing new products, the quality function deployment (qfd) has been widely used in devising virtual items. in traditional qfd, information about the customers, needs and their priorities can be gained through some marketing methods. however, these approaches heavily rely on the subjective results and cannot identify the demands of each customer because of bewildering amount of information. thus, we adopt the genetic chaotic neural network (gcnn) technique to identify each customer's needs and their priorities and propose the enhanced qualify function deployment (eqfd). however, in most of the existing literature, the equations to describe chaos dynamics are fixed and rigid corresponding to different nonlinear dynamic systems. in fact, for many chaotic systems in applications, it is often difficult to obtain accurate and faithful mathematical models, regarding their physically complex structures and hidden parameters. therefore, gcnn is proposed in this paper, where ga is embedded into the chaotic neural network to generate and refine the equations of chaotic systems. by experimenting our methods with several benchmark methods, the proposed gcnn is found to demonstrate a clear advantage over other identifying methods, and eqfd is proven to be a feasible technique for developing the virtual items in mmorpgs.",
    "present_kp": [
      "chaotic neural network",
      "qfd",
      "mmorpg",
      "virtual item"
    ],
    "absent_kp": [
      "genetic algorithm"
    ]
  },
  {
    "title": "an agent-based approach to the two-dimensional guillotine bin packing problem.",
    "abstract": "the two-dimensional guillotine bin packing problem consists of packing, without overlap, small rectangular items into the smallest number of large rectangular bins where items are obtained via guillotine cuts. this problem is solved using a new guillotine bottom left (gbl) constructive heuristic and its agent-based (ab) implementation. gbl, which is sequential, successively packs items into a bin and creates a new bin every time it can no longer fit any unpacked item into the current one. ab, which is pseudo-parallel, uses the simplest system of artificial life. this system consists of active agents dynamically interacting in real time to jointly fill the bins while each agent is driven by its own parameters, decision process, and fitness assessment. ab is particularly fast and yields near-optimal solutions. its modularity makes it easily adaptable to knapsack related problems.",
    "present_kp": [],
    "absent_kp": [
      "heuristics",
      "cutting and packing",
      "two-dimensional bin packing",
      "agent-based systems",
      "artificial intelligence"
    ]
  },
  {
    "title": "identification and characterization of thymosin ?-4 in chicken macrophages using whole cell maldi-tof.",
    "abstract": "the aim of the study was to determine chicken monocyte- and granulocyte-associated peptides and proteins using whole cell matrix-assisted laser desorption/ionization-timeofflight mass spectrometry (maldi-tof ms) and to characterize the peptides based on their abundance. the mass spectra showed a prominent peak at m/z 4963 in monocytes/macrophages but not in the granulocytes. subsequent purification and characterization of the m/z 4963 peptide from an avian macrophage cell line htc, revealed it to be thymosin ?-4 (t?-4), an actin-modulating peptide. htc cells when treated with bacterial lipopolysaccharide and peptidoglycan to determine the modulation of t?-4 gene expression or its secretion, showed no changes",
    "present_kp": [
      "chicken",
      "thymosin ?-4",
      "macrophages",
      "maldi-tof"
    ],
    "absent_kp": []
  },
  {
    "title": "explaining and predicting the adoption intention of mobile data services: a value-based approach.",
    "abstract": "this study comes to examine the adoption intention of mobile data services (mds) in jordan. this study develops a value-based approach where value is used as a multidimensional construct. our results show that utilitarian value is according to previous studies an important adoption factor. our results show that economic factor is also significant as cost of using the mobile service comes into play. in the context of this study, hedonic, uniqueness, and epistemic value dimensions are not as important for the use of mds.",
    "present_kp": [
      "adoption intention",
      "mobile data services",
      "value dimensions",
      "mds",
      "value-based approach",
      "jordan"
    ],
    "absent_kp": []
  },
  {
    "title": "a bridge across the bosphorus returned migrants, their internet and media use and social capital.",
    "abstract": "relatively few studies have focused on migrants who returned to the country of their origin or their parents' origin. still fewer have examined the communication patterns of call center workers who live in one country but conduct all of their business in the language and culture of another country. drawing on work by portes and bourdieu, this study treats the use of traditional media and the internet and its relation to the bridging and bonding behavior of a group of turks who returned to istanbul from the netherlands and who are now employed by a dutch call center company. based on a survey and three focus groups of employee participants, this research finds that more recent and younger returnees primarily bond with family and friends in holland through use of dutch media and internet use, whereas longer term returnees connect more with turkish media. some bridging was occurring through interpersonal communication with turks by the younger and more recent arrivals.",
    "present_kp": [
      "social capital",
      "media",
      "migrants",
      "returnees"
    ],
    "absent_kp": [
      "ethnic minorities",
      "call centers"
    ]
  },
  {
    "title": "extracting user web browsing patterns from non-content network traces: the online advertising case study.",
    "abstract": "online advertising is a rapidly growing industry currently dominated by the search engine 'giant' google. in an attempt to tap into this huge market, internet service providers (isps) started deploying deep packet inspection techniques to track and collect user browsing behavior. however, these providers have the fear that such techniques violate wiretap laws that explicitly prevent intercepting the contents of communication without gaining consent from consumers. in this paper, we explore how it is possible for isps to extract user browsing patterns without inspecting contents of communication. our contributions are threefold. first, we develop a methodology and implement a system that is capable of extracting web browsing features from stored non-content based network traces, which could be legally shared. when such browsing features are correlated with information collected by independently crawling the web, it becomes possible to recover the actual web pages accessed by clients. second, we evaluate our system on the internet and check that it can successfully recover user browsing patterns with high accuracy.",
    "present_kp": [
      "online advertising"
    ],
    "absent_kp": [
      "web navigation",
      "web fingerprinting"
    ]
  },
  {
    "title": "cloud enabled fractal based ecg compression in wireless body sensor networks.",
    "abstract": "we proposed a cloud efficient compression technique suitable for a wireless body sensor network. the compression ratio achieved is 40 with percentage residual difference (prd) of less than 1%. the decompression technique is designed to support partial retrieval of ecg data which make it suitable for cloud solutions. better compression ratios compared with other available compression techniques.",
    "present_kp": [
      "ecg",
      "compression",
      "fractal",
      "prd"
    ],
    "absent_kp": [
      "cloud computing",
      "body sensors"
    ]
  },
  {
    "title": "temporal reasoning and bayesian networks.",
    "abstract": "this work examines important issues in probabilistic temporal representation and reasoning using bayesian networks (also known as belief networks). the representation proposed here utilizes temporal (or dynamic) probabilities to represent facts, events, and the effects of events. the architecture of a belief network may change with time to indicate a different causal context. probability variations with time capture temporal properties such as persistence and causation. they also capture event interaction, and when the interaction between events follows known models such as the competing risks model, the additive model, or the dominating event model, the net effect of many interacting events on the temporal probabilities can be calculated efficiently. this representation of reasoning also exploits the notion of temporal degeneration of relevance due to information obsolescence to improve the efficiency.",
    "present_kp": [
      "temporal representation and reasoning"
    ],
    "absent_kp": [
      "uncertain reasoning",
      "bayesian  networks",
      "models of interaction"
    ]
  },
  {
    "title": "vertex-transitive cubic graphs of square-free order.",
    "abstract": "a classification of connected vertex-transitive cubic graphs of square-free order is provided. it is shown that such graphs are well-characterized metacirculants (including dihedrants, generalized petersen graphs, mbius bands), or tutte's 8-cage, or graphs arisen from simple groups psl(2, p).",
    "present_kp": [
      "metacirculant",
      "tutte's 8-cage"
    ],
    "absent_kp": [
      "vertex-transitive graph",
      "arc-transitive graph",
      "coset graph"
    ]
  },
  {
    "title": "the two-weighted inequalities for sublinear operators generated by b singular integrals in weighted lebesgue spaces.",
    "abstract": "in this paper, the authors establish several general theorems for the boundedness of sublinear operators (b sublinear operators) satisfies the condition (1.2), generated by b singular integrals on a weighted lebesgue spaces (l_{p,omega,gamma}(mathbb{r}_{k,+}^{n})), where (b=sum_{i=1}^{k} (frac{partial^{2}}{partial x_{k}^{2}} + frac{gamma_{i}}{x_{i}}frac{partial}{partial x_{i}} )). the condition (1.2) are satisfied by many important operators in analysis, including b maximal operator and b singular integral operators. sufficient conditions on weighted functions ? and ? 1 are given so that b sublinear operators satisfies the condition (1.2) are bounded from (l_{p,omega,gamma}(mathbb{r}_{k,+}^{n})) to (l_{p,omega_{1},gamma}(mathbb{r}_{k,+}^{n})).",
    "present_kp": [
      "weighted lebesgue space",
      "b sublinear operator",
      "b maximal operator",
      ""
    ],
    "absent_kp": [
      "bsingular integral operator",
      "two-weighted inequality"
    ]
  },
  {
    "title": "the union of congruent cubes in three dimensions.",
    "abstract": "a {em dihedral (trihedral) wedge} is the intersection of two (resp. t hree) half-spaces in $reals^3$. it is called {em $alpha$-fat} if the angle (resp., solid angle) determined by these half-spaces is at least $alpha>0$. if, in addition, the sum of the three face angles of a trihedral wedge is at least $gamma >4pi/3$, then it is called {em $(gamma,alpha)$-substantially fat}. we prove that, for any fixed $gamma>4pi/3, alpha>0$, the combinatorial complexity of the union of $n$ (a) $alpha$-fat dihedral wedges, (b) $(gamma,alpha)$-substantially fat trihedral wedges is at most $o(n^{2+eps})$, for any $eps>0$, where the constants of proportionality depend on $eps$, $alpha$ (and $gamma$). we obtain as a corollary that the same upper bound holds for the combinatorial complexity of the union of $n$ (nearly) congruent cubes in $reals^3$. these bounds are not far from being optimal.",
    "present_kp": [
      "intersection",
      "complexity",
      "space"
    ],
    "absent_kp": [
      "optimality"
    ]
  },
  {
    "title": "read, write, and present for acm siguccs conferences.",
    "abstract": "the association of computing machinery special interest group in university and college computing services (acm siguccs) is made up of professionals who support and manage of information technology services at higher education institutions. siguccs sponsors an annual conference that is drawn together by volunteers. the conference program takes the form of paper authors presenting their findings in 30 minute talks, as part of a panel, or in a poster session. papers are presented on a variety of tracks such as management, technology, customer support, documentation and training, or instructional technology. the track titles can change over time. attending and contributing to the siguccs conference program is an opportunity for professional development. this paper seeks to demystify the process of contributing to the siguccs conference program as a reader, author, and presenter and thus make the opportunity to obtain professional development through contributing to the siguccs conference program easier.",
    "present_kp": [
      "siguccs",
      "program",
      "conference",
      "presenting"
    ],
    "absent_kp": [
      "authoring",
      "template",
      "reading"
    ]
  },
  {
    "title": "web-based information content and its application to concept-based video retrieval.",
    "abstract": "semantic similarity between words or phrases is frequently used to find matching correlations between search queries and documents when straightforward matching of terms fails. this is particularly important for searching in visual databases, where pictures or video clips have been automatically tagged with a small set of semantic concepts based on analysis and classification of the visual content. here, the textual description of documents is very limited, and semantic similarity based on wordnet's cognitive synonym structure, along with information content derived from term frequencies, can help to bridge the gap between an arbitrary textual query and a limited vocabulary of visual concepts. this approach, termed concept-based retrieval, has received significant attention over the last few years, and its success is highly dependent on the quality of the similarity measure used to map textual query terms to visual concepts. in this paper, we consider some issues of semantic similarity measures based on information content (ic), and propose a way to improve them. in particular, we note that most ic-based similarity measures are derived from a small and relatively outdated corpus (the brown corpus), which does not adequately capture the usage pattern of many contemporary terms: for example, out of more than 150,000 wordnet terms, only about 36,000 are represented. this shortcoming reflects very negatively on the coverage of typical search query terms. we therefore suggest using alternative ic corpora that are larger and better aligned with the usage of modern vocabulary. we experimentally derive two such corpora using the www google search engine, and show that they provide better coverage of vocabulary, while showing comparable frequencies for brown corpus terms. finally, we evaluate the two proposed ic corpora in the context of a concept-based video retrieval application using the trecvid 2005, 2006, and 2007 datasets, and we show that they increase average precision results by up to 200%.",
    "present_kp": [
      "semantic similarity",
      "wordnet",
      "brown corpus",
      "trecvid",
      "information content"
    ],
    "absent_kp": [
      "lscom"
    ]
  },
  {
    "title": "comparison between two muscle models under dynamic conditions.",
    "abstract": "one fundamental problem when trying to calculate the force developed by one muscle during a motor task is the muscle model. usually, one control signal is juxtaposed to one musclotendon unit. the question is how is this signal connected to the activation of the motor units (mus) that compose the muscle and fire differently. the aim of the paper is to compare a hill-type muscle model to a model composed of mus. a fast elbow flexion performed by only one muscle is considered. the activation necessary for performing the motion and the corresponding frequencies are calculated for cases of fast and slow muscles using hill-type model. then the muscle is modelled as a mixture of with uniformly distributed twitch parameters. using motco software the moments of impulsation of all mus and their mechanical responses are predicted. the activation characteristics obtained by the two muscle models are compared. it is concluded that there are two essential parameters for proper muscle modelling: the lead-time and the mus composition.",
    "present_kp": [
      "muscle model",
      "motor units"
    ],
    "absent_kp": [
      "motor control",
      "hill-model",
      "muscle forces",
      "genetic algorithm",
      "neural control"
    ]
  },
  {
    "title": "local joint entropy based non-rigid multimodality image registration.",
    "abstract": "we present a new variational model for image registration. the local joint entropy is used to measure the similarity of the images to be aligned. the weighted horn-type regularization term is used to protect displacement fields from over-smoothing. the proposed model has the advantage of aligning local edges of images well. a fast iteration algorithm is designed to solve our model.",
    "present_kp": [
      "image registration",
      "non-rigid"
    ],
    "absent_kp": [
      "weighted horn regularization",
      "variational differential",
      "alternative minimization",
      "aos algorithm"
    ]
  },
  {
    "title": "survey of experimental evaluation studies for wireless mesh network deployments in urban areas towards ubiquitous internet.",
    "abstract": "establishing wireless networks in urban areas that can provide ubiquitous internet access to end-users is a central part of the efforts towards defining the internet of the future. in recent years, wireless mesh network (wmn) backbone infrastructures are proposed as a cost effective technology to provide city-wide internet access. studies that evaluate the performance of city-wide mesh network deployments via experiments provide essential information on various challenges of building them. in this survey, we particularly focus on such studies and provide brief conclusions on the problems, benefits, and future research directions of city-wide wmns.",
    "present_kp": [
      "deployment",
      "experiments"
    ],
    "absent_kp": [
      "city-wide wireless mesh networks"
    ]
  },
  {
    "title": "extremal sizes of subspace partitions.",
    "abstract": "a subspace partition i of v = v(n, q) is a collection of subspaces of v such that each 1-dimensional subspace of v is in exactly one subspace of i . the size of i is the number of its subspaces. let sigma (q) (n, t) denote the minimum size of a subspace partition of v in which the largest subspace has dimension t, and let rho (q) (n, t) denote the maximum size of a subspace partition of v in which the smallest subspace has dimension t. in this article, we determine the values of sigma (q) (n, t) and rho (q) (n, t) for all positive integers n and t. furthermore, we prove that if n a parts per thousand yen 2t, then the minimum size of a maximal partial t-spread in v(n + t-1, q) is sigma (q) (n, t).",
    "present_kp": [
      "subspace partition"
    ],
    "absent_kp": [
      "vector space partitions",
      "partial t-spreads"
    ]
  },
  {
    "title": "visual trustworthy monitoring system (v-tms) for behavior of trusted computing.",
    "abstract": "as the platform mobility increases, it becomes increasingly susceptible to theft stolen data is often regarded as being more valuable than the notebook hardware itself thus, the need to protect user data and secrets is underscored in a mobile computing environment the trusted platform module (tpm) is defined as a hardware instantiation which has been proposed by the tcg (trusted computing group) for trust computing tpm offers facilities for the secure generation of cryptographic keys, and limitations on their use, in addition to a hardware pseudo-random number generator it also includes capabilities such as remote attestation and sealed storage a tpm can be used to authenticate hardware devices since each tpm chip has a unique and secret rsa key burned in during production, it is capable of performing platform authentication the tpb (trusted platform board) is an expansion of the tpm for enhancing the efficiency and usability of the tpm chip in addition to the tpb functions supporting high-standard trust environments within the hardware standard of the system in this paper, we develop the v-tms (visual trustworthy monitoring system) that provides visualization of real-time monitoring for the behavior of system resources (process, memory, network, users, etc) with tpb and system software for hardening of os and applications moreover, v-tms is not only a web-based computing environment for system resources but also a real-time monitoring system for a trust computing environment",
    "present_kp": [
      "trusted computing",
      "trusted platform module",
      "trusted computing group"
    ],
    "absent_kp": [
      "trustworthiness monitoring"
    ]
  },
  {
    "title": "a homotopy-based approach for computing defocus blur and affine transform simultaneously.",
    "abstract": "this paper presents a homotopy-based algorithm for a simultaneous recovery of defocus blur and the affine parameters of apparent shifts between planar patches of two pictures. these parameters are recovered from two images of the same scene acquired by a camera evolving in time and/or space and for which the intrinsic parameters are known. using limited taylor's expansion one of the images (and its partial derivatives) is expressed as a function of the partial derivatives of the two images, the blur difference, the affine parameters and a continuous parameter derived from homotopy methods. all of these unknowns can thus be directly computed by resolving a system of equations at a single scale. the proposed algorithm is tested using synthetic and real images. the results confirm that dense and accurate estimation of the previously mentioned parameters can be obtained.",
    "present_kp": [
      "defocus blur",
      "homotopy method"
    ],
    "absent_kp": [
      "computer vision",
      "unified model",
      "affine matching",
      "generalized moment expansion"
    ]
  },
  {
    "title": "architectures for functional imagination.",
    "abstract": "imagination can be defined broadly as the manipulation of information that is not directly available to an agent's sensors. however, the topic of imagination raises representational, physiological, and phenomenological issues that cannot be tackled easily without using the body as a reference point. within this framework, we define functional imagination as the mechanism that allows an embodied agent to simulate its own actions and their sensory consequences internally, and to extract behavioural benefits from doing so. in this paper, we present five necessary and sufficient requirements for the implementation of functional imagination, as well as a minimal architecture that meets all these criteria. we also present a taxonomy for categorising possible architectures according to their main attributes. finally, we describe experiments with some simple architectures designed using these principles and implemented on simulated and real robots, including an extremely complex anthropomimetic humanoid.",
    "present_kp": [
      "imagination"
    ],
    "absent_kp": [
      "cognitive architectures",
      "antropomorphic robots",
      "internal models"
    ]
  },
  {
    "title": "quantifying patterns of agentenvironment interaction.",
    "abstract": "this article explores the assumption that a deeper (quantitative) understanding of the information-theoretic implications of sensorymotor coordination can help endow robots not only with better sensory morphologies, but also with better exploration strategies. specifically, we investigate by means of statistical and information-theoretic measures to what extent sensorymotor coordinated activity can generate and structure information in the sensory channels of a simulated agent interacting with its surrounding environment. the results show how the usage of correlation, entropy, and mutual information can be employed (a) to segment an observed behavior into distinct behavioral states; (b) to analyze the informational relationship between the different components of the sensorymotor apparatus; and (c) to identify patterns (or fingerprints) in the sensorymotor interaction between the agent and its local environment.",
    "present_kp": [],
    "absent_kp": [
      "self-structuring of information",
      "sensory-motor coordination",
      "fingerprinting agentenvironment interaction"
    ]
  },
  {
    "title": "an inventory model for deteriorating items with stock-dependent consumption rate and shortages under inflation and time discounting.",
    "abstract": "this paper derives an inventory model for deteriorating items with stock-dependent consumption rate and shortages under inflation and time discounting over a finite planning horizon. we show that the total cost function is convex. with the convexity, a simple solution algorithm is presented to determine the optimal order quantity and the optimal interval of the total cost function. the results are discussed with a numerical example and particular cases of the model are discussed in brief. a sensitivity analysis of the optimal solution with respect to the parameters of the system is carried out.",
    "present_kp": [
      "inventory",
      "stock-dependent consumption rate",
      "deteriorating",
      "inflation",
      "shortages"
    ],
    "absent_kp": []
  },
  {
    "title": "significance of q fever serologic diagnosis in clinically suspect patients.",
    "abstract": "q fever is caused by c. burnetii, an intracellular obligate bacterium. for clinical confirmation of q fever, diagnosis of interstitial pneumonia is of significance. the acute disease varies in severity from minor to fatal, with the possibility of serious complications. chronic endocarditis is a well-known outcome. symptoms of q fever can vary; fixing diagnosis is done by serology with the phase i and the phase ii antibody. we tested 44 sera of 31 clinically suspect patients. from these, 22 patients were taken to the infection clinic, 8 to the pulmonary clinic, and one to the general hospital. from the 31 patients, 21 patients had one serum, 7 patients, 2 sera, and 3 patients, 3 sera. blood samples were collected by vein puncture, and serum samples were kept at ?20c until testing. all sera were processed by indirect imunofluorescent assay (ifa) q fever igm and igg. of 44 processed sera, 21 were seropositive. specific igm antibody was found in sera of 6 patients (19.4%), and specific igg antibody in sera of 16 patients (51.2%). in sera of 15 clinically suspect patients (48.3%), no specific anticoxiella antibody was found. from these results we can confirm the importance of serology in laboratory diagnosis and clinical affirmation of suspect q fever. indirect imunofluorescent assay (ifa) is reliable and appropriate for daily, routine diagnosis of human q fever.",
    "present_kp": [
      "q fever",
      "clinically suspect patients"
    ],
    "absent_kp": [
      "sarajevo",
      "coxiella burnetii",
      "serology diagnosis"
    ]
  },
  {
    "title": "residual stress estimation in damascene copper interconnects using embedded sensors.",
    "abstract": "mechanical stress in damascene copper/low-k interconnects has been studied by means of micro-rotating sensors embedded in chips and directly integrated in cmos process flow. a new hinge sensor design has been elaborated and a new analytical model of the mechanical equilibrium of sensors is validated. these sensors allow the study of the average residual stress as a function of the line width in a range from few hundred nanometers to several microns. it was found that the residual stress increases from 290 to 850mpa in, respectively, 2 and 0.25?m wide lines. this trend shows a yield stress increase with the line width reduction. copper grains microstructure change between large and narrow lines is probably one of the reasons for yield stress and so residual stress increase. this microstructure change has been observed by means of transmission electron microscopy (tem) observations.",
    "present_kp": [
      "mechanical stress",
      "embedded sensor",
      "interconnects",
      "copper"
    ],
    "absent_kp": []
  },
  {
    "title": "visually localizing design problems with disharmony maps.",
    "abstract": "assessing the quality of software design is difficult, as \"design\" is expressed through guidelines and heuristics, not rigorous rules. one successful approach to assess design quality is based on detection strategies, which are metrics-based composed logical conditions, by which design fragments with specific properties are detected in the source code. such detection strategies, when executed on large software systems usually return large sets of artifacts, which potentially exhibit one or more \"design disharmonies\", which are then inspected manually, a cumbersome activity. in this article we present disharmony maps, a visualization-based approach to locate such flawed software artifacts in large systems. we display the whole system using a 3d visualization technique based on a city metaphor. we enrich such visualizations with the results returned by a number of detection strategies, and thus render both the static structure and the design problems that affect a subject system. we evaluate our approach on a number of open-source java systems and report on our findings.",
    "present_kp": [],
    "absent_kp": [
      "software visualization",
      "software design anomalies"
    ]
  },
  {
    "title": "optimal database search: waves and catalysis.",
    "abstract": "grover's database search algorithm, although discovered in the context of quantum computation, can be implemented using any system that allows superposition of states. a physical realization of this algorithm is described using coupled simple harmonic oscillators, which can be exactly solved in both classical and quantum domains. classical wave algorithms are far more stable against decoherence compared to their quantum counterparts. in addition to providing convenient demonstration models, they may have a role in practical situations, such as catalysis.",
    "present_kp": [
      "database search",
      "harmonic oscillator",
      "catalysis"
    ],
    "absent_kp": []
  },
  {
    "title": "on numerical methods for highly oscillatory problems in circuit simulation.",
    "abstract": "purpose - the purpose of this paper is to analyse a novel technique for an efficient numerical approximation of systems of highly oscillatory ordinary differential equations (odes) that arise in electronic systems subject to modulated signals. design/methodology/approach - the paper combines a filon-type method with waveform relaxation techniques for nonlinear systems of odes. findings - the analysis includes numerical examples to compare with traditional methods such as the trapezoidal rule and runge-kutta methods. this comparison shows that the proposed approach can be very effective when dealing with systems of highly oscillatory differential equations. research limitations/implications - the present paper constitutes a preliminary study of filon-type methods applied to highly oscillatory odes in the context of electronic systems, and it is a starting point for future research that will address more general cases. originality/value - the proposed method makes use of novel and recent techniques in the area of highly oscillatory problems, and it proves to be particularly useful in cases where standard methods become expensive to implement.",
    "present_kp": [
      "differential equations"
    ],
    "absent_kp": [
      "electronic engineering",
      "oscillators",
      "circuits"
    ]
  },
  {
    "title": "type inference and semi-unification.",
    "abstract": "the milner calculus is the typed &lgr;-calculus underlying the type system for the programming language ml and several other strongly typed polymorphic functional languages such as miranda and sps . mycroft extended the problematical typing rule for recursive definitions and proved that the resulting calculus, termed milner-mycroft calculus here, is sound with respect to milner's semantics and that it preserves the principal typing property of the milner calculus. the extension is of practical significance in typed logic programming languages and, more generally, in any language with (mutually) recursive definitions. mycroft didn't solve the decidability problem for typings in this calculus, though. this was an open problem independently raised also by meertens . the decidability question was answered in the affirmative just recently by kfoury et al. in . we show that the type inference problems in the milner and the milner-mycroft calculi can be reduced to solving equations and inequations between first-order terms, a problem we have termed semi-unification . we show that semi-unification problems have most general solutions in analogy to unification problems which translates into principal typing properties for the underlying calculi. in contrast to the (essentially) nonconstructive methods of we present functional specifications, which we prove partially correct, for computing the most general solution of semi-unification problems, and we devise a concrete nondeterministic algorithm on a graph-theoretic representation for computing these most general solutions. finally, we point out some erroneous statements about the efficiency of polymorphic type checking that have persisted throughout the literature including an incorrect claim, submitted by ourselves, of polynomial time type checking in the milner-mycroft calculus.",
    "present_kp": [
      "unification",
      " ml ",
      "point",
      "order",
      "efficiency",
      "polynomial",
      "decidability",
      "definition",
      "general",
      "representation",
      "practical",
      "analog",
      "type system",
      "sound",
      "method",
      "semantic",
      "language",
      "type inference",
      "functional",
      "polymorphic",
      "logic program",
      "graph",
      "algorithm",
      "functional languages",
      "programming language"
    ],
    "absent_kp": [
      "recursion",
      "computation",
      "extensibility",
      "timing"
    ]
  },
  {
    "title": "srtm resample with short distance-low nugget kriging.",
    "abstract": "the shuttle radar topography mission (srtm), was flow on the space shuttle endeavour in february 2000, with the objective of acquiring a digital elevation model of all land between 60 degrees north latitude and 56 degrees south latitude, using interferometric synthetic aperture radar (insar) techniques. the srtm data are distributed at horizontal resolution of 1 arc-second (similar to 30m) for areas within the usa and at 3 arc-second (similar to 90m) resolution for the rest of the world. a resolution of 90m can be considered suitable for the small or medium-scale analysis, but it is too coarse for more detailed purposes. one alternative is to interpolate the srtm data at a finer resolution; it will not increase the level of detail of the original digital elevation model (dem), but it will lead to a surface where there is the coherence of angular properties (i.e. slope, aspect) between neighbouring pixels, which is an important characteristic when dealing with terrain analysis. this work intents to show how the proper adjustment of variogram and kriging parameters, namely the nugget effect and the maximum distance within which values are used in interpolation, can be set to achieve quality results on resampling srtm data from 3\" to 1\". we present for a test area in western usa, which includes different adjustment schemes (changes in nugget effect value and in the interpolation radius) and comparisons with the original 1\" model of the area, with the national elevation dataset (ned) dems, and with other interpolation methods (splines and inverse distance weighted (idw)). the basic concepts for using kriging to resample terrain data are: (i) working only with the immediate neighbourhood of the predicted point, due to the high spatial correlation of the topographic surface and omnidirectional behaviour of variogram in short distances; (ii) adding a very small random variation to the coordinates of the points prior to interpolation, to avoid punctual artifacts generated by predicted points with the same location than original data points and; (iii) using a small value of nugget effect, to avoid smoothing that can obliterate terrain features. drainages derived from the surfaces interpolated by kriging and by splines have a good agreement with streams derived from the 1\" ned, with correct identification of watersheds, even though a few differences occur in the positions of some rivers in flat areas. although the 1\" surfaces resampled by kriging and splines are very similar, we consider the results produced by kriging as superior, since the spline-interpolated surface still presented some noise and linear artifacts, which were removed by kriging.",
    "present_kp": [
      "rtm",
      "dem",
      "kriging",
      "variogram",
      "interpolation"
    ],
    "absent_kp": [
      "geostatistics",
      "nugget value"
    ]
  },
  {
    "title": "one set of pliers for more tasks in installation work: the effects on (dis)comfort and productivity.",
    "abstract": "in installation work, the physical workload is high. awkward postures, heavy lifting and repetitive movements are often seen. to improve aspects of the work situation, frequently used pliers were redesigned to make them suitable for more cutting tasks. in this study these multitask pliers are evaluated in comparison to the originally used pliers in a field study and a laboratory study. for the field study 26 subjects participated divided into two groups according to their type of work. ten subjects participated in the laboratory study. the multitask plier appeared to result in more comfort during working, more relaxed working and more satisfaction. no differences in productivity were found. in conclusion, the multitask pliers can replace the originally used pliers and are suitable for more tasks than the original pliers. the installation workers have to carry less pliers by using the multitask pliers.",
    "present_kp": [
      "productivity"
    ],
    "absent_kp": [
      "hand tools",
      "user comfort"
    ]
  },
  {
    "title": "simulated unbound structures for benchmarking of protein docking in the dockground resource.",
    "abstract": "proteins play an important role in biological processes in living organisms. many protein functions are based on interaction with other proteins. the structural information is important for adequate description of these interactions. sets of protein structures determined in both bound and unbound states are essential for benchmarking of the docking procedures. however, the number of such proteins in pdb is relatively small. a radical expansion of such sets is possible if the unbound structures are computationally simulated.",
    "present_kp": [
      "protein docking"
    ],
    "absent_kp": [
      "protein interactions",
      "molecular recognition",
      "conformational analysis"
    ]
  },
  {
    "title": "threshold-based admission control for a multimedia grid: analysis and performance evaluation.",
    "abstract": "in a grid-based services system facing a large number of requests with different services and profits significance, there is always a trade-off between the system profits and the quality of service (qos). in such systems, admission control plays an important role: the system has to employ a proper strategy to make admission control decisions and reserve resources for the coming requests thus to achieve greater profits without violating the qos of the requests already admitted. in this paper, we introduce three essential admission control strategies with threshold on resource reservation and a newly proposed strategy with layered threshold. through comprehensive theoretical analyses and extensive simulations, we demonstrate that the strategy with layered threshold is more efficient and flexible than the existing strategies for grid-based multimedia services systems.",
    "present_kp": [
      "multimedia grid",
      "admission control",
      "threshold"
    ],
    "absent_kp": [
      "reward/penalty"
    ]
  },
  {
    "title": "diffusion kernels on statistical manifolds.",
    "abstract": "a family of kernels for statistical learning is introduced that exploits the geometric structure of statistical models. the kernels are based on the heat equation on the riemannian manifold defined by the fisher information metric associated with a statistical family, and generalize the gaussian kernel of euclidean space. as an important special case, kernels based on the geometry of multinomial families are derived, leading to kernel-based learning algorithms that apply naturally to discrete data. bounds on covering numbers and rademacher averages for the kernels are proved using bounds on the eigenvalues of the laplacian on riemannian manifolds. experimental results are presented for document classification, for which the use of multinomial geometry is natural and well motivated, and improvements are obtained over the standard use of gaussian or linear kernels, which have been the standard for text classification.",
    "present_kp": [
      "kernels",
      "heat equation",
      "diffusion",
      "text classification"
    ],
    "absent_kp": [
      "information geometry"
    ]
  },
  {
    "title": "self-monitoring for sensor networks.",
    "abstract": "local monitoring is an effective mechanism for the security of wireless sensor networks (wsns). existing schemes assume the existence of sufficient number of active nodes to carry out monitoring operations. such an assumption, however, is often difficult for a large scale sensor network. in this work, we focus on designing an efficient scheme integrated with good self-monitoring capability as well as providing an infrastructure for various security protocols using local monitoring. to the best of our knowledge, we are the first to present the formal study on finding optimized self-monitoring topology for wsns. we show the problem is np-complete even under the unit disk graph (udg) model, and give the upper bound on the approximation ratio. we further propose two distributed polynomial algorithms with provable approximation ratio to address this issue. through comprehensive simulations, we evaluate the effectiveness of this design.",
    "present_kp": [
      "security",
      "wireless sensor network",
      "np-complete",
      "self-monitoring"
    ],
    "absent_kp": []
  },
  {
    "title": "hci and business practices in a collaborative method for augmented reality systems.",
    "abstract": "every interactive system is composed of a functional core and a user interface. however, the software engineering (se) and humancomputer interaction (hci) communities do not share the same methods, models or tools. this usually induces a large work overhead when specialists from the two domains try to connect their applicative studies, especially when developing augmented reality systems that feature complex interaction cores. we present in this paper the essential activities and concepts of a development method integrating the se and hci development practices, from the specifications down to the design, as well as their application on a case study. the efficiency of the method was tested in a qualitative study involving four pairs of se and hci experts in the design of an application for which an augmented reality interaction would provide better user performance than a classic interactive system. the effectivity of the method was evaluated in a qualitative study comparing the quality of three implementations of the same application fragment (based on the same analysis model), using software engineering metrics. the first evaluation confirmed the ease of use of our method and the relevance of our tools for guiding the design process, but raised concerns on the handling of conflicting collaborative activities. the second evaluation gave indications that the structure of the analysis model facilitates the implementation of quality software (in terms of coupling, stability and complexity). it is concluded that our method enables design teams with different backgrounds in application development to collaborate for integrating augmented reality applications with information systems. areas of improvement are also described.",
    "present_kp": [
      "information systems",
      "software engineering",
      "humancomputer interaction",
      "augmented reality"
    ],
    "absent_kp": [
      "collaborative design",
      "evolution"
    ]
  },
  {
    "title": "dimensions of emotion in expressive musical performance.",
    "abstract": "abstract: this paper explores the dimensions of emotion conveyed by music. participants rated emotion terms after seeing and/or hearing recordings of clarinet performances that varied in expressive content. a factor analysis revealed four independent dimensions of emotion. changes to the clarinetists' expressive intentions did not significantly affect emotions conveyed by sound. it was largely through the visual modality that expressive intentions influenced the experience for observers.",
    "present_kp": [
      "emotion",
      "performance"
    ],
    "absent_kp": [
      "crossmodal interactions",
      "music cognition"
    ]
  },
  {
    "title": "study of a plane-free jet exhausting from a channel by vortex-in-cell method.",
    "abstract": "a two-dimensional simulation of a plane jet exhausting from a channel has been performed using the vortex in cell algorithm in the reynolds number range of 100-900. the vorticity is generated on the wall of the entrance channel whose length has been fixed in order to obtain a fully developed velocity profile at the entry of the jet. the transient behaviour of the velocity field starting from rest has been observed until reaching a quasi steady regime. the mean value of the velocity field is compared with the results of a finite volume computation on the same mesh. the velocity fluctuations obtained using this method are analysed. their effect on the mean flow is estimated to be smaller than the viscous effect.",
    "present_kp": [
      "plane jet",
      "velocity field"
    ],
    "absent_kp": [
      "vortex method",
      "numerical simulation",
      "vortex flow",
      "lagrangian method"
    ]
  },
  {
    "title": "an improved relaxed complex scheme for receptor flexibility in computer-aided drug design.",
    "abstract": "the interactions among associating (macro)molecules are dynamic, which adds to the complexity of molecular recognition. while ligand flexibility is well accounted for in computational drug design, the effective inclusion of receptor flexibility remains an important challenge. the relaxed complex scheme (rcs) is a promising computational methodology that combines the advantages of docking algorithms with dynamic structural information provided by molecular dynamics (md) simulations, therefore explicitly accounting for the flexibility of both the receptor and the docked ligands. here, we briefly review the rcs and discuss new extensions and improvements of this methodology in the context of ligand binding to two example targets: kinetoplastid rna editing ligase 1 and the w191g cavity mutant of cytochrome c peroxidase. the rcs improvements include its extension to virtual screening, more rigorous characterization of local and global binding effects, and methods to improve its computational efficiency by reducing the receptor ensemble to a representative set of configurations. the choice of receptor ensemble, its influence on the predictive power of rcs, and the current limitations for an accurate treatment of the solvent contributions are also briefly discussed. finally, we outline potential methodological improvements that we anticipate will assist future development.",
    "present_kp": [
      "docking",
      "kinetoplastid rna editing ligase 1",
      "molecular dynamics",
      "virtual screening"
    ],
    "absent_kp": [
      "clustering",
      "ensemble-based docking",
      "non-redundant ensemble",
      "protein-ligand binding",
      "relaxed complex method",
      "representative ensemble",
      "w191g cytochrome c peroxidase"
    ]
  },
  {
    "title": "twin support vector machine with universum data.",
    "abstract": "the universum, which is defined as the sample not belonging to either class of the classification problem of interest, has been proved to be helpful in supervised learning. in this work, we designed a new twin support vector machine with universum (called u-tsvm), which can utilize universum data to improve the classification performance of tsvm. unlike u-svm, in u-tsvm, universum data are located in a nonparallel insensitive loss tube by using two hinge loss functions, which can exploit these prior knowledge embedded in universum data more flexible. empirical experiments demonstrate that u-tsvm can directly improve the classification accuracy of standard tsvm that use the labeled data alone and is superior to u-svm in most cases.",
    "present_kp": [
      "classification",
      "twin support vector machine",
      "universum"
    ],
    "absent_kp": []
  },
  {
    "title": "compiler analysis of irregular memory accesses.",
    "abstract": "irregular array accesses are array accesses whose array subscripts do not have closed-form expressions in terms of loop indices. traditional array analysis and loop transformation techniques cannot handle irregular array accesses. in this paper, we study two kinds of simple and common cases of irregular array accesses: single-indexed access and indirect array access. we present techniques to analyze these two cases at compile-time, and we provide experimental results showing the effectiveness of these techniques in finding more implicit loop parallelism at compile-time and improved speedups.",
    "present_kp": [
      "speedup",
      "access",
      "paper",
      "effect",
      "parallel",
      "analysis",
      "express"
    ],
    "absent_kp": [
      "arrays",
      "memorialized",
      "compilation",
      "timing",
      "experimentation"
    ]
  },
  {
    "title": "relying on time synchronization for security in ad hoc networks.",
    "abstract": "mobile ad hoc networks are networks composed of mobile computing devices with wireless connections and no fixed infrastructure. the unique attributes of these networks cause new security problems. some security protocols, such as the timed, efcient, streaming, loss-tolerant authentication protocol, tesla , rely on loose time synchronization for security. these protocols have not fully analyzed the implications of relying on time synchronization in this environment. this paper looks into some of the difficulties in time synchronization and points out further areas for research.",
    "present_kp": [
      "ad hoc"
    ],
    "absent_kp": [
      "clock synchronization",
      "drift"
    ]
  },
  {
    "title": "model execution: an approach based on extending domain-specific modeling with action reports.",
    "abstract": "in this paper, we present an approach to development and application of domain-specific modeling (dsm) tools in the model-based management of business processes. the level of model-to-text (m2t) transformations in the standard architecture for domain-specific modeling solutions is extended with action reports, which allow synchronization between models, generated code, and target interpreters. the basic idea behind the approach is to use m2t transformation languages to construct submodels, client application components, and operations on target interpreters. in this manner, m2t transformations may be employed to support not only generation of target platform code from domain-specific graphical language (dsgl) models but also straightforward use of models and appropriate dsm tools as client applications. the applicability of action reports is demonstrated by examples from document engineering, and measurement and control systems.",
    "present_kp": [
      "domain-specific modeling",
      "document engineering"
    ],
    "absent_kp": [
      "model-driven development",
      "model transformations",
      "modeling tools"
    ]
  },
  {
    "title": "fast and scalable parallel processing of scalar multiplication in elliptic curve cryptosystems.",
    "abstract": "to secure parallel systems in communication networks, in this paper, we propose a fast and scalable parallel scalar multiplication method over generic elliptic curves for elliptic curve cryptosystems, by means of our proposed scalar folding and unfolding techniques. in contrast to previous parallel scalar multiplication methods, our method can be implemented into scalable parallel computers. the optimal time complexity is k point doublings (d) plus log k point additions (a), denoted as kd+(logk)a, where k is the bit length of the scalar. if our method is applied to koblitz curves, the optimal time complexity can be reduced to (logk)a. furthermore, previous simple side-channel-protected scalar multiplication methods can be integrated into our method for resisting against simple side-channel attacks.",
    "present_kp": [
      "scalar multiplication",
      "elliptic curve cryptosystems",
      "side-channel attacks"
    ],
    "absent_kp": [
      "parallel computing"
    ]
  },
  {
    "title": "construction of dual ovsf codes with lower correlations.",
    "abstract": "in wide-band code division multiple access (wcdma), orthogonal variable spreading factors (ovsf) codes are assigned to different users to preserve the orthogonality between users' physical channels. in this letter, we present the dual ovsf code, which can transmit the variable data rates by suing two different modulated signals without loss of the orthogonality. the bit error rate (ber) performance under a multi-user environment suffering the additive white gaussian noise (awgn) channel and correlations of those codes are evaluated. the results demonstrate that the proposed dual ovsf scheme could provide flexible rates and lower correlation values with a slight increase in complexity.",
    "present_kp": [
      "wcdma",
      "ovsf",
      "ber",
      "awgn",
      "multi-user environment"
    ],
    "absent_kp": []
  },
  {
    "title": "layout optimization of cmos functional cells.",
    "abstract": "an optimal non-exhaustive method of minimizing the layout area of complementary series-parallel cmos functional cells in the standard-cell style is presented. this generalizes earlier work of uehara and van cleemput which is heuristic and nonoptimal. a complete graph-theoretical framework for cmos cell layout is developed and illustrated. the approach demonstrates a new class of graph-based algebras which characterize this layout problem.",
    "present_kp": [
      "layout",
      " framework ",
      "graph",
      "method",
      "algebra",
      "parallel",
      "class",
      "functional"
    ],
    "absent_kp": [
      "heuristics",
      "standard cell",
      "optimality",
      "completeness"
    ]
  },
  {
    "title": "some theoretical aspects of algorithmic routing.",
    "abstract": "in this paper we describe a router which has the capabilities of automatic rip-up and rerouting. the system employs a look-ahead feature for determining if as yet unrouted nets are unroutable. backtrack programming is used to implement an implicit enumeration scheme which sequentially produces alternative paths for a given net. the relationship between this type of router and the problem of ordering nets is discussed. by varying parameters in our router which deal with back tracking and path-length deviations, we can obtain a hierarchy of routing systems. one extreme case corresponds to a classical lee-type router; another to a perfect router which guarantees finding a solution if one exists.",
    "present_kp": [
      "program",
      "order",
      "aspect",
      "capabilities",
      "systems",
      "router",
      "routing",
      "case",
      "tracking",
      "paper",
      "feature",
      "scheme"
    ],
    "absent_kp": [
      "relationships",
      "hierarchies"
    ]
  },
  {
    "title": "classifier subset selection to construct multi-classifiers by means of estimation of distribution algorithms.",
    "abstract": "this paper proposes a novel approach to select the individual classifiers to take part in a multiple-classifier system. individual classifier selection is a key step in the development of multi-classifiers. several works have shown the benefits of fusing complementary classifiers. nevertheless, the selection of the base classifiers to be used is still an open question, and different approaches have been proposed in the literature. this work is based on the selection of the appropriate single classifiers by means of an evolutionary algorithm. different base classifiers, which have been chosen from different classifier families, are used as candidates in order to obtain variability in the classifications given. experimental results carried out with 20 databases from the uci repository show how adequate the proposed approach is; stacked generalization multi-classifier has been selected to perform the experimental comparisons.",
    "present_kp": [
      "classifier subset selection"
    ],
    "absent_kp": [
      "machine learning",
      "multiple-classifier systems",
      "evolutionary computation"
    ]
  },
  {
    "title": "energy cost evaluation of parallel algorithms for multiprocessor systems.",
    "abstract": "with the continuous development of hardware and software, graphics processor units (gpus) have been used in the general-purpose computation field. they have emerged as a computational accelerator that dramatically reduces the application execution time with cpus. to achieve high computing performance, a gpu typically includes hundreds of computing units. the high density of computing resource on a chip brings in high power consumption. therefore power consumption has become one of the most important problems for the development of gpus. this paper analyzes the energy consumption of parallel algorithms executed in gpus and provides a method to evaluate the energy scalability for parallel algorithms. then the parallel prefix sum is analyzed to illustrate the method for the energy conservation, and the energy scalability is experimentally evaluated using sparse matrix-vector multiply (spmv). the results show that the optimal number of blocks, memory choice and task scheduling are the important keys to balance the performance and the energy consumption of gpus.",
    "present_kp": [
      "gpus",
      "parallel algorithms",
      "energy scalability",
      "energy conservation",
      "performance"
    ],
    "absent_kp": []
  },
  {
    "title": "large-scale multi-task image labeling with adaptive relevance discovery and feature hashing.",
    "abstract": "this paper proposes a novel multi-label classification approach. it seamlessly incorporates the idea of multi-task feature hashing learning. it can capture the task relationships at task-level as well as feature-level.",
    "present_kp": [
      "feature hashing",
      "relevance discovery"
    ],
    "absent_kp": [
      "image classification",
      "multiple tasks"
    ]
  },
  {
    "title": "global change and the evolution of phenotypic plasticity in plants.",
    "abstract": "global change drivers create new environmental scenarios and selective pressures, affecting plant species in various interacting ways. plants respond with changes in phenology, physiology, and reproduction, with consequences for biotic interactions and community composition. we review information on phenotypic plasticity, a primary means by which plants cope with global change scenarios, recommending promising approaches for investigating the evolution of plasticity and describing constraints to its evolution. we discuss the important but largely ignored role of phenotypic plasticity in range shifts and review the extensive literature on invasive species as models of evolutionary change in novel environments. plasticity can play a role both in the short-term response of plant populations to global change as well as in their long-term fate through the maintenance of genetic variation. in new environmental conditions, plasticity of certain functional traits may be beneficial (i.e., the plastic response is accompanied by a fitness advantage) and thus selected for. plasticity can also be relevant in the establishment and persistence of plants in novel environments that are crucial for populations at the colonizing edge in range shifts induced by climate change. experimental studies show taxonomically widespread plastic responses to global change drivers in many functional traits, though there is a lack of empirical support for many theoretical models on the evolution of phenotypic plasticity. future studies should assess the adaptive value and evolutionary potential of plasticity under complex, realistic global change scenarios. promising tools include resurrection protocols and artificial selection experiments.",
    "present_kp": [
      "global change",
      "phenotypic plasticity",
      "adaptive",
      "constraints",
      "novel environment"
    ],
    "absent_kp": [
      "natural selection",
      "invasive plants"
    ]
  },
  {
    "title": "a computer aided cost estimation system for bga/dca technology.",
    "abstract": "the last decade has seen an increasing demand for smaller and more densely populated printed circuit boards (pcbs). this is partly due to the market driven need within the electronics industry of reducing the size of products while concurrently enhancing their capabilities. consequently, the electronics packaging industry is relying upon area array technologies such as ball grid array (bga) and direct chip attach (dca) as possible replacements for the traditional peripherally leaded surface mount packaging formats. however, since these technologies are still in their nascent stage, the cost benefits obtained from them need to be quantified in an effort to aid in justifying their use. this paper describes a computer aided cost estimation (cage) system which has been developed to justify the use of bga/dca devices.",
    "present_kp": [
      "direct chip attach",
      "computer aided cost estimation"
    ],
    "absent_kp": [
      "electronics manufacturing",
      "ball grid arrays",
      "software integration"
    ]
  },
  {
    "title": "a new model for health care e-insurance using credit points and service oriented architecture (soa).",
    "abstract": "nowadays the concept of e-insurance is about how to apply online for insurance services (e.g. policy plan, claim). in this study we tried to develop a new model which is not based on insurance company plans. this new model is based on credits, points and rewards for health care insurance. with using this new model, clients will be able to choose their own created health care insurance plan. on the other hand small business groups involved a few number of clinics or hospitals can create their own health and care insurance. the new model is more appropriate for short term insurance and small town or areas. as the insurance risk is higher than available insurance plans, in this model the insurance price and investing the premiums are more than normal insurer company's policies. moreover it can cover all who wants to be insured and also it is more flexible in terms of clients' needs. finally researcher used service oriented architecture (soa) to connect all clinics and hospitals which are involved in the plan behind an e-insurance website. therefore insured party just needs to connect e-insurance website in order to make an appointment for receiving medical care and etc.",
    "present_kp": [
      "service oriented architecture ",
      "e-insurance"
    ],
    "absent_kp": [
      "insurance risks"
    ]
  },
  {
    "title": "eulerian rotations of deformed nuclei for tddft calculations.",
    "abstract": "we discuss three practical methods for performing eulerian rotations of slater determinants in a three-dimensional cartesian geometry. in addition to the straightforward application of the active form of the quantum mechanical rotation operator, we introduce two methods using a passive position-space rotation followed by an active spin-space rotation, one after variation and the other before variation. these methods can be used to initialize reactions involving deformed nuclei where a particular alignment of the deformed nuclei with respect to the collision axis is desired. we show that doing the rotation before the variation is the most efficient way of generating such initial states.",
    "present_kp": [
      "slater determinant",
      "rotation",
      "tddft"
    ],
    "absent_kp": [
      "tdhf"
    ]
  },
  {
    "title": "a fast and memory-efficient discrete focal stack transform for plenoptic sensors.",
    "abstract": "plenoptic cameras are a new type of sensors that capture the four-dimensional lightfield of a scene. processing the recorded lightfield, they extend the capabilities of current commercial cameras. conventional cameras obtain photographs focusing at a determined depth. this photograph can be described through a projection of the four-dimensional lightfield onto two spatial dimensions. the collection of such images is the focal stack of the scene. the focal stack can be used to select an image refocused at a certain depth, to recover 3d information or to obtain all-in-focus images. there are several approaches to the computation of the focal stack. in this paper we propose a new technique to compute the focal stack by means of its frequency decomposition that can be seen as an extension of the discrete focal stack transform (dfst). this new approach decreases the computational complexity of the dfst maintaining an efficient memory use. experimental results are provided to show the validity of the technique and its extension to 3d processing and all-in-focus image computation is also studied.",
    "present_kp": [
      "plenoptic sensor",
      "lightfield",
      "focal stack"
    ],
    "absent_kp": [
      "computational photography",
      "multiview stereo. 3dtv"
    ]
  },
  {
    "title": "managing information systems for health services in a developing country: a case study using a contextualist framework.",
    "abstract": "investments in information technology (it) have been escalating in the health sector in both developed and developing countries. however, the failure rate of applications is of concern especially for countries with scarce resources. there is insufficient understanding of factors that lead to such failures in developing countries. a case study of implementing a computerised information system (is) for health services in the philippines is analysed using a contextualist framework. factors that led to the failure included ambiguity in the organisation and in responsibility for the project, lack of capacity to undertake large information systems development projects and inability to retain appropriate staff. however, when the historical and contextual issues were revealed and the interplay between the content, process and context of the change was analysed it was revealed that the content of the is was not responsive to the changes in the wider health system. the case study confirms the need to analyse and understand organisational, environmental and cultural issues in adopting models and procedures used elsewhere when managing information systems in developing countries.",
    "present_kp": [
      "information technology",
      "health services",
      "developing countries"
    ],
    "absent_kp": [
      "managing information services"
    ]
  },
  {
    "title": "an extensible, lightweight architecture for adaptive j2ee applications.",
    "abstract": "server applications with adaptive behaviors can adapt their functionality in response to environmental changes, and significantly reduce the on-going costs of system deployment and administration. however, developing adaptive server applications is challenging due to the complexity of server technologies and highly dynamic application environments. this paper presents an architecture framework, known as the adaptive server framework (asf). asf provides a clear separation between the implementation of adaptive behaviors and the server application business logic. this means a server application can be cost effectively extended with programmable adaptive features through the definition and implementation of control components defined in asf. furthermore, asf is a lightweight architecture in that it incurs low cpu overhead and memory usage. we demonstrate the effectiveness of asf through a case study, in which a server application dynamically determines the resolution and quality to scale an image based on the load of the server and network connection speed. the experimental evaluation demonstrates the performance gains possible by adaptive behaviors and the low overhead introduced by asf.",
    "present_kp": [
      "component",
      "j2ee"
    ],
    "absent_kp": [
      "adaptation",
      "software architecture"
    ]
  },
  {
    "title": "very high-order compact finite difference schemes on non-uniform grids for incompressible navier-stokes equations.",
    "abstract": "this article presents a family of very high-order non-uniform grid compact finite difference schemes with spatial orders of accuracy ranging from 4th to 20th for the incompressible navier stokes equations. the high-order compact schemes on non-uniform grids developed in shukla and zhong for linear model equations are extended to the full navier-stokes equations in the vorticity and streamfunction formulation. two methods for the solution of helmholtz and poisson equations using high-order compact schemes on non-uniform grids are developed. the schemes are constructed so that they maintain a high-order of accuracy not only in the interior but also at the boundary. second-order semi-implicit temporal discretization is achieved through an implicit backward differentiation scheme for the linear viscous terms and an explicit adam-bashforth scheme for the non-linear convective terms. the boundary values of vorticity are determined using an influence matrix technique. the resulting discretized system with boundary closures of the same high-order as the interior is shown to be stable, when applied to the two-dimensional incompressible navier-stokes equations, provided enough grid points are clustered at the boundary. the resolution characteristics of the high-order compact finite difference schemes are illustrated through their application to the one-dimensional linear wave equation and the two-dimensional driven cavity flow. comparisons with the benchmark solutions for the two-dimensional driven cavity flow, thermal convection in a square box and flow past an impulsively started cylinder show that the high-order compact schemes are stable and produce extremely accurate results on a stretched grid with more points clustered at the boundary.",
    "present_kp": [
      "compact schemes"
    ],
    "absent_kp": [
      "high-order methods",
      "viscous incompressible flow"
    ]
  },
  {
    "title": "emergence of un-correlated common-mode oscillations in the sensory cortex.",
    "abstract": "simultaneous eeg recordings from various cortical areas indicate the presence of common-mode, spatially coherent oscillations. these oscillations are characterized by a common wave form with a spatially distributed pattern of amplitude modulation (am). we observe highly reproducible am patterns across spatially separated channels within various areas, yet the temporal correlations between the channels are low. in the framework of the present research, a nonlinear, spatially distributed dynamical model of neuronal populations (kiii) is used for the interpretation of the observed spatial coherence. the theoretical findings are in good agreement with experiments performed with chronically implanted rabbits.",
    "present_kp": [
      "sensory cortex"
    ],
    "absent_kp": [
      "spatio-temporal eeg",
      "chaotic self-organization",
      "kiii model"
    ]
  },
  {
    "title": "optimization and coordination of svc-based supplementary controllers and psss to improve power system stability using a genetic algorithm.",
    "abstract": "in this paper, a lead-lag structure is proposed as a main damping controller for a static var. compensator (s vc) to diminish power system oscillations. to confirm the transient performance of the proposed controller, it was compared to a proportional integral derivative (pid) damping controller. power system stability improvement was thoroughly examined using these supplementary damping controllers as well as a power system. stabilizer (pss). the generic algorithm, (ca) is well liked in the academic environment due to its immediate perceptiveness, ease of performance, and ability to impressively solve highly nonlinear objectives. thus, the ca optimization technique was applied to solve an optimization problem, and to achieve optimal parameters of the svc-based supplementary damping controllers and pss. the coordinated design problem of these devices was formulated as an optimization problem, to reduce power system. oscillations. the transient performance of the damping controllers and pss were evaluated under a, severe disturbance for a single-machine infinite bus (smib) and multimachine power system. the nonlinear simulation results of the smib power system suggest that power system stability was increasingly improved using the coordinated design of the svc-based lead-lag controller and ps's, rather than the coordinated design of the svc-based p id controller and pss. furthermore, the interarea and local modes of the oscillations were superiorly damped using the proposed controller in the multimachine power system.",
    "present_kp": [
      "svc",
      "pss",
      "power system stability",
      "coordinated design"
    ],
    "absent_kp": [
      "lead-lag damping controller",
      "pid damping controller",
      "ga optimization technique"
    ]
  },
  {
    "title": "planning the location and rating of distributed energy storage in lv networks using a genetic algorithm with simulated annealing.",
    "abstract": "to fix voltage rise in lv networks, dnos traditionally reinforce lv networks. we show that the severity of the voltage problem depends on the penetration of pv. a new planning tool is produced for locating and sizing energy storage in networks. storage is shown to be cheaper than reinforcement depending on the pv penetration. we use the tool to show that small single phase storage reduces rating/capacity.",
    "present_kp": [
      "simulated annealing"
    ],
    "absent_kp": [
      "distributed storage and generation",
      "power distribution planning",
      "genetic algorithms"
    ]
  },
  {
    "title": "mapping trigger conditions onto trigger units during post-silicon validation and debugging.",
    "abstract": "on-chip trigger units are employed for detecting events of interest during post-silicon validation and debugging. their implementation constrains the trigger conditions that can be programmed at runtime. it is often the case that some trigger events of interest, which were not accounted for during design time, cannot be detected due to the constraints imposed by the hardware implementation of the trigger units. to address this issue, we present architectural features that can be included into the trigger units and discuss the algorithmic approach for automatically mapping trigger conditions onto the trigger units.",
    "present_kp": [
      "post-silicon validation and debugging",
      "trigger conditions",
      "trigger units"
    ],
    "absent_kp": []
  },
  {
    "title": "buzz-based recommender system.",
    "abstract": "in this paper, we describe a buzz-based recommender system based on a large source of queries in an ecommerce application. the system detects bursts in query trends. these bursts are linked to external entities like news and inventory information to find the queries currently in-demand which we refer to as buzz queries. the system follows the paradigm of limited quantity merchandising, in the sense that on a per-day basis the system shows recommendations around a single buzz query with the intent of increasing user curiosity, and improving activity and stickiness on the site. a semantic neighborhood of the chosen buzz query is selected and appropriate recommendations are made on products that relate to this neighborhood.",
    "present_kp": [
      "buzz"
    ],
    "absent_kp": [
      "novelty",
      "serendipity",
      "recommenders"
    ]
  },
  {
    "title": "test data compression and decompression based on internal scan chains and golomb coding.",
    "abstract": "we present a data compression method and decompression architecture for testing embedded cores in a system-on-a-chip (soc). the proposed approach makes effective use of golomb coding and the internal scan chain(s) of the core under test and provides significantly better results than a recent compression method that uses golomb coding and a separate cyclical scan register (csr). the major advantages of golomb coding of test data include very high compression, analytically predictable compression results, and a low-cost and scalable on-chip decoder. the use of the internal scan chain for decompression obviates the need for a csr, thereby reducing hardware overhead considerably. in addition, the novel interleaving decompression architecture allows multiple cores in an soc to be tested concurrently using a single ate i/o channel. we demonstrate the effectiveness of the proposed approach by applying it to the iscas 89 benchmark circuits.",
    "present_kp": [
      "decompression architecture"
    ],
    "absent_kp": [
      "automatic test equipment ",
      "embedded core testing",
      "precomputed test sets",
      "response vectors",
      "system-on-a-chip testing",
      "test set encoding",
      "testing time",
      "variable-to-variable-length codes"
    ]
  },
  {
    "title": "optimal sleep transistor synthesis under timing and area constraints.",
    "abstract": "leakage power reduction in nano-cmos designs has gained tremendous interest both in academia and industry. many techniques have been proposed in the literature for leakage power reduction and one of the prominent techniques for leakage power reduction is the use of sleep transistors as power-gating elements to cut-off sub-threshold leakage current in circuits when they are in stand-by mode. although sleep transistor insertion is very effective in cutting-off leakage, it also incurs timing, area and routing overhead. since most of the sleep transistor insertion methodologies do post layout insertion, care should be taken such that there is minimal perturbation of the original layout. over design of sleep transistors cells and sub-optimal sleep transistor placement must be avoided to achieve final design closure. since the sleep transistor area plays an important and prominent role in this aspect, it necessitates for optimal sleep transistor sizing and synthesis technique under area constraints. in this paper, we first provide a methodology for optimal sleep transistor synthesis under given area constraints. we then apply our technique to the general timing and area constraint driven row-based power-gating methodology proposed in [13] and show how optimal low leakage designs with constraints on timing and area can be designed.",
    "present_kp": [
      "sleep transistor",
      "row-based",
      "power-gating",
      "leakage power"
    ],
    "absent_kp": [
      "standard cell",
      "clustering"
    ]
  },
  {
    "title": "a framework for comparing task performance in real and virtual scenes.",
    "abstract": "in this paper, we describe a framework for comparing task performance in real and virtual environments. realistic graphics, rear projection, haptics and rapid prototyping are used to match the virtual scene to the real scene. we describe some preliminary placement tasks which were evaluated using eye-tracking and discuss our future plans for this framework.",
    "present_kp": [
      "task performance",
      "real and virtual environments"
    ],
    "absent_kp": [
      "attention",
      "visual perception"
    ]
  },
  {
    "title": "a scalable algorithm to order and annotate continuous observations reveals the metastable states visited by dynamical systems.",
    "abstract": "advances in it infrastructure have enabled the generation and storage of very large data sets describing complex systems continuously in time. these can derive from both simulations and measurements. analysis of such data requires the availability of scalable algorithms. in this contribution, we propose a scalable algorithm that partitions instantaneous observations (snapshots) of a complex system into kinetically distinct sets (termed basins). to do so, we use a combination of ordering snapshots employing the methods only essential parameter, i.e. , a definition of pairwise distance, and annotating the resultant sequence, the so-called progress index, in different ways. specifically, we propose a combination of cut-based and structural annotations with the former responsible for the kinetic grouping and the latter for diagnostics and interpretation. the method is applied to an illustrative test case, and the scaling of an approximate version is demonstrated to be o(nlogn) o ( n log n ) with n being the number of snapshots. two real-world data sets from river hydrology measurements and protein folding simulations are then used to highlight the utility of the method in finding basins for complex systems. both limitations and benefits of the approach are discussed along with routes for future research.",
    "present_kp": [
      "complex system",
      "scalable algorithm"
    ],
    "absent_kp": [
      "trajectory analysis",
      "minimum spanning tree",
      "free energy basins"
    ]
  },
  {
    "title": "worse: a workbench for model-based security engineering.",
    "abstract": "we propose a tool-supported approach to enhance the quality of security policies. for this goal, a workbench for model-based security engineering has been developed. it implements a uniform method to engineer policy-specific security models. it allows for detailed model analysis based on this uniform calculus. it implements a novel method for heuristic safety analysis of access control models.",
    "present_kp": [
      "security engineering"
    ],
    "absent_kp": [
      "security policy engineering",
      "model safety analysis",
      "information flow analysis",
      "security model checking"
    ]
  },
  {
    "title": "current feedback operational amplifier based oscillators.",
    "abstract": "this paper demonstrates the practicality of the current feedback operational amplifier (cfoa) in realizing grounded capacitor (or grounded resistor) oscillator circuits. the paper begins with a description of the minimum passive component oscillators, using one or two cfoas. next, two new single cfoa oscillators with independent control on the condition of oscillation are generated from the single cfoa minimum component oscillator. three new oscillator circuits using two cfoas are introduced. grounded capacitor and grounded resistor oscillators using three cfoas with independent control on the condition of oscillation and on the frequency of oscillation are also included. the proposed grounded component oscillators are suitable for vlsi implementation. pspice simulations and experimental results demonstrating the performance of some of the proposed oscillators are given.",
    "present_kp": [
      "oscillators",
      "current feedback"
    ],
    "absent_kp": [
      "op amps"
    ]
  },
  {
    "title": "simulation of service-oriented systems for mobile ad hoc networks.",
    "abstract": "the simulation of distributed systems that implement a service-oriented architecture (soa) imposes a set of requirements on their simulation models. these models must replicate the interactions that characterise the soa operational paradigm and the role of the entities that support an soa. when those interactions occur over mobile ad hoc networks (manets), simulation models must accurately reproduce the effects that mobility and other networking factors have on the system. this paper presents the architecture of a simulation environment for manet-interconnected service-oriented systems. the simulation environment combines the realistic soa support provided by osgi with the network simulation capabilities of ns-2. the application-layer elements of the simulation models built for this environment are also soa implementations, thus promoting structural model validity. this approach provides the opportunity of utilising elements or prototypes from the modelled systems as part of the simulation model, promoting its continuity. in addition, it takes advantage of the benefits that soa has brought to field of modelling and simulation.",
    "present_kp": [
      "simulation",
      "service-oriented architecture ",
      "mobile ad hoc networks ",
      "osgi",
      "ns-2"
    ],
    "absent_kp": []
  },
  {
    "title": "adaptive wavelet thresholding for image denoising and compression.",
    "abstract": "the first part of this paper proposes an adaptive, data-driven threshold for image denoising via wavelet soft-thresholding, the threshold is derived in a bayesian framework, and the prior used on the wavelet coefficients is the generalized gaussian distribution (ggd) widely used in image processing applications, the proposed threshold is simple and closed-form, and it is adaptive to each subband because it depends on data-driven estimates of the parameters. experimental results show that the proposed method, called bayesshrink, is typically within 5% of the mse of the best soft-thresholding benchmark with the image assumed known, it also outperforms donoho and johnstone's sureshrink most of the time. the second part of the paper attempts to further validate recent claims that lossy compression can be used for denoising, the bayesshrink threshold can aid in the parameter selection of a coder designed with the intention of denoising, and thus achieving simultaneous denoising and compression. specifically, the zero-zone in the quantization step of compression is analogous to the threshold value in the thresholding function. the remaining coder design parameters are chosen based on a criterion derived from rissanen's minimum description length (mdl) principle, experiments show that this compression method does indeed remove noise significantly, especially for large noise power, however, it introduces quantization noise and should he used only if bitrate were an additional concern to denoising.",
    "present_kp": [
      "image denoising",
      "wavelet thresholding"
    ],
    "absent_kp": [
      "adaptive method",
      "image compression",
      "image restoration"
    ]
  },
  {
    "title": "debugging distributed c programs by real time reply.",
    "abstract": "bugnet is a portable unix system designed to debug c programs distributed within a local area network. a graphics interface allows the user of a sun workstation to manage process groups and monitor process interactions very conveniently. bugnet gives information about interprocess communication, i/o events, and execution traces for each component process. it allows the user to detect an error visually, to roll global program state back to a time before the error, and to replay events almost exactly as they previously occurred. current work on bugnet is making the implementation easier to port, seeing if replay accuracy can be improved by minor adjustments in the unix process scheduler, linking it with unix dbx to control individual processes, and determining useful tools for filtering of long event strings and for detecting errors. the bugnet project is testing how well existing multiprocess c programs can be debugged without special hardware features that make porting difficult. an initial version is running on a network of suns. it currently reproduces real time execution sequences with an accuracy of 0.01 to 0.10 seconds.",
    "present_kp": [
      "network",
      "communication",
      "traces",
      "event",
      "project",
      "graphics",
      "group",
      "filtering",
      "replay",
      "errors",
      "strings",
      "control",
      "linking",
      "user",
      "component",
      "interaction",
      "program",
      "accuracy",
      "test",
      "implementation",
      "process",
      "version",
      "hardware",
      "distributed",
      "monitor",
      "feature",
      "sequence",
      "debugging",
      "tools",
      "global"
    ],
    "absent_kp": [
      "scheduling",
      "portability",
      "interfaces",
      "timing",
      "real-time",
      "informal",
      "management",
      "locality",
      "systems"
    ]
  },
  {
    "title": "an analytic approach to the asymptotic variance of trie statistics and related structures.",
    "abstract": "we develop analytic tools for the asymptotics of general trie statistics, which are particularly advantageous for clarifying the asymptotic variance. many concrete examples are discussed for which new fourier expansions are given. the tools are also useful for other splitting processes with an underlying binomial distribution. we specially highlight philippe flajolet's contribution in the analysis of these random structures.",
    "present_kp": [
      "variance"
    ],
    "absent_kp": [
      "digital trees",
      "binomial splitting process",
      "mellin transform",
      "periodic fluctuations",
      "contention resolution algorithms"
    ]
  },
  {
    "title": "object-oriented biomedical system modelingthe rationale.",
    "abstract": "a short tutorial and a rationale for object-oriented biomedical (continuous) system modelling (oobsm) are given. the paper investigates and defines what is needed in order to make the work with complex bio-medical and pathophysiological models easier, less error prone and conceptually clearer than is possible by using the existing modelling techniques. it also contains a specification of what is required in order to make such models and corresponding knowledge communicable among different research groups and in order to use such models as components in even more complex models. the work shows that hitherto available continuous system modelling languages and tools are less suitable for the construction of complex, interdisciplinary, multilevel, hierarchical models and model components and that those modelling languages do not allow for easy exchange and communication of the model knowledge between different research groups and sites. it concludes that object-oriented and distributed objects methodologies are both feasible and suitable for such modelling.",
    "present_kp": [
      "continuous system modelling",
      "model components",
      "hierarchical models"
    ],
    "absent_kp": [
      "object-oriented modelling",
      "biomedical system modelling",
      "knowledge representation",
      "knowledge communication",
      "knowledge encapsulation"
    ]
  },
  {
    "title": "on the quest for optimal rule learning heuristics.",
    "abstract": "the primary goal of the research reported in this paper is to identify what criteria are responsible for the good performance of a heuristic rule evaluation function in a greedy top-down covering algorithm. we first argue that search heuristics for inductive rule learning algorithms typically trade off consistency and coverage, and we investigate this trade-off by determining optimal parameter settings for five different parametrized heuristics. in order to avoid biasing our study by known functional families, we also investigate the potential of using metalearning for obtaining alternative rule learning heuristics. the key results of this experimental study are not only practical default values for commonly used heuristics and a broad comparative evaluation of known and novel rule learning heuristics, but we also gain theoretical insights into factors that are responsible for a good performance. for example, we observe that consistency should be weighted more heavily than coverage, presumably because a lack of coverage can later be corrected by learning additional rules.",
    "present_kp": [
      "inductive rule learning",
      "heuristics",
      "metalearning"
    ],
    "absent_kp": []
  },
  {
    "title": "an approach to extracting trunk from an image.",
    "abstract": "rendering realistic trees is quite important for simulating a 3d natural scene. separating the trunk from its background is the first step toward the 3d model construction of the tree. in this paper, a three-phase algorithm is developed to extract the trunk structure of the tree and hence segment the trunk from the image. some experiments were conducted and results confirmed the feasibility of proposed algorithm.",
    "present_kp": [],
    "absent_kp": [
      "trunk extraction",
      "trunk segmentation",
      "tree segmentation",
      "tree detection"
    ]
  },
  {
    "title": "disulfide connectivity prediction based on structural information without a prior knowledge of the bonding state of cysteines.",
    "abstract": "previous studies predicted the disulfide bonding patterns of cysteines using a prior knowledge of their bonding states. in this study, we propose a method that is based on the ensemble support vector machine (svm), with the structural features of cysteines extracted without any prior knowledge of their bonding states. this method is useful for improving the predictive performance of disulfide bonding patterns. for comparison, the proposed method was tested with the same dataset spx that was adopted in previous studies. the experimental results demonstrate that bridge classification and disulfide connectivity predictions achieve 96.5% and 89.2% accuracy, respectively, using the ensemble svm model, which outperforms the traditional method (51.5% and 51.0%, respectively) and the model that is based on a single-kernel svm classifier (94.6% and 84.4%, respectively). for protein chain and residue classifications, the sensitivity, specificity, and accuracy of ensemble and single-kernel svm approaches are better than those of the traditional methods. the predictive performances of the ensemble svm and single-kernel models are identical, indicating that the ensemble model can converge to the single-kernel model for some applications.",
    "present_kp": [
      "disulfide bonding pattern",
      "support vector machine"
    ],
    "absent_kp": [
      "multiple trajectory search",
      "ensemble classifier"
    ]
  },
  {
    "title": "a parallel network emulation method for evaluating the correctness and performance of applications.",
    "abstract": "network emulation system constructs a virtual network environment which has the characteristics of controllable and repeatable network conditions. this makes it possible to predict the correctness and performance of proposed new technology before deploying to internet. in this paper we present a methodology for evaluating the correctness and performance of applications based on the parnem, a parallel discrete event network emulator. parnem employs a bsp based real-time event scheduling engine, provides flexible interactive mechanism and facilitates legacy network models reuse. parnem allows detailed and accurate study of application behavior. comprehensive case studies covering bottleneck bandwidth measurement and distributed cooperative web caching system demonstrate that network emulation technology opens a wide range of new opportunities for examining the behavior of applications.",
    "present_kp": [
      "network emulation",
      "interactive mechanism"
    ],
    "absent_kp": [
      "performance evaluation",
      "time synchronization"
    ]
  },
  {
    "title": "diconic addition of failsafe fault-tolerance.",
    "abstract": "we present a divide-and-conquer method, called diconic, for automatic addition of failsafe fault-tolerance to distributed programs, where a failsafe program guarantees to meet its safety specification even when faults occur. specifically, instead of adding fault-tolerance to a program as a whole, we separately revise program actions so that the entire program becomes failsafe fault-tolerant. our diconic algorithm has the potential to utilize the processing power of a large number of machines working in parallel, thereby enabling automatic addition of failsafe fault-tolerance to distributed programs with a large number of processes. we formulate our diconic synthesis algorithm in terms of the satisfiability problem and demonstrate our approach for the byzantine generals problem and an industrial application.",
    "present_kp": [
      "satisfiability"
    ],
    "absent_kp": [
      "formal methods",
      "divide and conquer",
      "addition of fault-tolerance"
    ]
  },
  {
    "title": "application of neural networks and kano's method to content recommendation in web personalization.",
    "abstract": "as customers become more skilled in the use of internet, many companies have gradually established their websites with more and more enormous information to get future competition in electronic commerce (ec). however, the miscellaneous information often brings the users at a loss. web personalization provides a solution to improvement of information overloading on websites. the objective of web personalization is to give users a website they want or need, and thus knowing the needs of users is an important task for content recommendation in web personalization. in this article, we propose a hybrid approach for this task. the proposed approach trains the artificial neural networks to group users into different clusters, and applies the well-established kano's method to extracting the implicit needs from users in different clusters. finally, a real case of tour and travel websites applying the approach is presented to demonstrate the improvement of information overloading.",
    "present_kp": [
      "artificial neural networks",
      "kano's method",
      "information overloading",
      "web personalization"
    ],
    "absent_kp": []
  },
  {
    "title": "multi-objective optimization with a max-t-norm fuzzy relational equation constraint.",
    "abstract": "in this paper, we consider minimizing multiple linear objective functions under a max-t-norm fuzzy relational equation constraint. since the feasible domain of a maxarchimedean t-norm relational equation constraint is generally nonconvex, traditional mathematical programming techniques may have difficulty in yielding efficient solutions for such problems. in this paper, we apply the two-phase approach, utilizing the min operator and the average operator to aggregate those objectives, to yield an efficient solution. a numerical example is provided to illustrate the procedure.",
    "present_kp": [
      "multi-objective optimization",
      "fuzzy relational equation",
      "max-t-norm",
      "two-phase approach"
    ],
    "absent_kp": []
  },
  {
    "title": "the effect of a pro(28)thr point mutation on the local structure and stability of human galactokinase enzyme-a theoretical study.",
    "abstract": "galactokinase is responsible for the phosphorylation of alpha-d-galactose, which is an important step in the metabolism of the latter. malfunctioning of galactokinase due to a single point mutation causes cataracts and, in serious cases, blindness. this paper reports a study of the pro(28)thr point mutation using a variety of theories including molecular dynamics (md), mm-pbsa/gbsa calculations and aim analysis. altered h-bonding networks were detected based on geometric and electron density criteria that resulted in local unfolding of the beta-sheet secondary structure. another consequence was the decrease in stability (5-7 kcal mol(-1)) around this region, as confirmed by delta g(bind) calculations for the extracted part of the whole system. local unfolding was verified by several other md simulations performed with different duration, initial velocities and force field. based on the results, we propose a possible mechanism for the unfolding caused by the pro(28)thr point mutation.",
    "present_kp": [
      "human galactokinase",
      "molecular dynamics",
      "aim",
      "local unfolding"
    ],
    "absent_kp": [
      "mmpbsa/gbsa",
      "beta-sheet stability"
    ]
  },
  {
    "title": "unmanned aerial vehicle-aided communications system for disaster recovery.",
    "abstract": "after natural disasters such as earthquakes, floods, hurricanes, tornados and fires, providing emergency management schemes which mainly rely on communications systems is essential for rescue operations. to establish an emergency communications system during unforeseen events such as natural disasters, we propose the use of a team of unmanned aerial vehicles (uavs). the proposed system is a post-disaster solution and can be used whenever and wherever required. each uav in the team has an onboard computer which runs three main subsystems responsible for end-to-end communication, formation control and autonomous navigation. the onboard computer and the low-level controller of the uav cooperate to accomplish the objective of providing local communications infrastructure. in this study, the subsystems running on each uav are explained and evaluated by simulation studies and field tests using an autonomous helicopter. while the simulation studies address the efficiency of the end-to-end communication subsystem, the field tests evaluate the accuracy of the navigation subsystem. the results of the field tests and the simulation studies show that the proposed system can be successfully used in case of disasters to establish an emergency communications system.",
    "present_kp": [
      "unmanned aerial vehicles"
    ],
    "absent_kp": [
      "communication systems",
      "aircraft navigation",
      "distributed control"
    ]
  },
  {
    "title": "nonparametric fuzzy regression - k-nn and kernel smoothing techniques.",
    "abstract": "fuzzy regression without predefined functional form, or nonparametric fuzzy regression, is investigated. the two most basic nonparametric regression techniques in statistics, namely, k-nearest neighbor smoothing and kernel smoothing, are fuzzified and analyzed. algorithms are proposed to obtain the best smoothing parameters based on the minimization of cross-validation criteria.",
    "present_kp": [
      "nonparametric fuzzy regression",
      "kernel smoothing"
    ],
    "absent_kp": [
      "k-nn smoothing"
    ]
  },
  {
    "title": "a laminate parametrization technique for discrete ply-angle problems with manufacturing constraints.",
    "abstract": "in this paper we present a novel laminate parametrization technique for layered composite structures that can handle problems in which the ply angles are limited to a discrete set. in the proposed technique, the classical laminate stiffnesses are expressed as a linear combination of the discrete options and design-variable weights. an exact penalty function is employed to drive the solution toward discrete 0-1 designs. the proposed technique can be used as either an alternative or an enhancement to simp-type methods such as discrete material optimization (dmo). unlike mixed-integer approaches, our laminate parametrization technique is well suited for gradient-based design optimization. the proposed laminate parametrization is demonstrated on the compliance design of laminated plates and the buckling design of a laminated stiffened panel. the results demonstrate that the approach is an effective alternative to dmo methods.",
    "present_kp": [
      "laminate parametrization",
      "manufacturing constraints"
    ],
    "absent_kp": [
      "discrete angle problems"
    ]
  },
  {
    "title": "consert: constructing optimal name-based routing tables.",
    "abstract": "name-based routing belongs to a routing category different from address-based routing, it is usually adopted by content-oriented networks, the recently proposed named data networking (ndn). it populates routers with name-based routing tables, which are composed of name prefixes and their corresponding next hop(s). name-based routing tables are believed to have much larger size than ip routing tables, because of the large amount of name prefixes and the unbounded length of each prefix. this paper presents consertan algorithm that, given an arbitrary name-based routing table as input, computes a routing table with the minimal number of prefixes, while keeping equivalent forwarding behavior. the optimal routing table also supports incremental update. we formulate the consert algorithm and prove its optimality with an induction method. evaluation results show that, consert can reduce 18 to 45% prefixes in the synthetic routing tables depending on the distribution of the next hops, and meanwhile improve the lookup performance by more than 20%. prior efforts usually focus on compact data structures and lookup algorithms so as to reduce memory consumption and expedite lookup speed of the routing table, while consert compresses the routing table from another perspective: it removes the inherent redundancy in the routing table. therefore, consert is orthogonal to these prior efforts, thus the combination of consert and a prior compressing method would further optimize the memory consumption and lookup speed of the routing table. e.g., we can first adopt consert to achieve the optimal routing table, and afterwards apply namefilter , a two-stage-bloom-filter method, to that optimal table. this combination diminishes the memory consumption of the routing table data structure by roughly 88%, and increases the lookup throughput by around 17% simultaneously. the joint method outperforms each individual method in terms of memory savings and absolute lookup throughout increase.",
    "present_kp": [
      "name-based routing table",
      "optimal",
      "compressing"
    ],
    "absent_kp": []
  },
  {
    "title": "idd: a supervised interval distance-based method for discretization.",
    "abstract": "this paper introduces a new method for supervised discretization based on interval distances by using a novel concept of neighborhood in the target's space. the proposed method takes into consideration the order of the class attribute, when this exists, so that it can be used with ordinal discrete classes as well as continuous classes, in the case of regression problems. the method has proved to be very efficient in terms of accuracy and faster than the most commonly supervised discretization methods used in the literature. it is illustrated through several examples, and a comparison with other standard discretization methods is performed for three public data sets by using two different learning tasks: a decision tree algorithm and svm for regression.",
    "present_kp": [
      "supervised discretization",
      "interval distances"
    ],
    "absent_kp": [
      "classification",
      "ordinal regression"
    ]
  },
  {
    "title": "symbolic analysis of network security policies using rewrite systems.",
    "abstract": "first designed to enable private networks to be opened up to the outside world in a secure way, the growing complexity of organizations make firewalls indispensable to control information flow within a company. the central role they hold in the security of the organization information make their management a critical task and that is why for years many works have focused on checking and analyzing firewalls. the composition of firewalls, taking into account routing rules, has nevertheless often been neglected. in this paper, we propose to specify all components of a firewall, ie filtering and translation rules, as a rewrite system. we show that such specifications allow us to handle usual problems such as comparison, structural analysis and query analysis. we also propose a formal way to describe the composition of firewalls (including routing) in order to build a whole network security policy. the properties of the obtained rewrite system are strongly related to the properties of the specified networks and thus, classical theoretical and practical tools can be used to obtain relevant security properties of the security policies.",
    "present_kp": [
      "security policies",
      "rewrite systems",
      "firewalls"
    ],
    "absent_kp": [
      "tree automata"
    ]
  },
  {
    "title": "three-dimensional packings with rotations.",
    "abstract": "we present approximation algorithms for the three-dimensional strip packing problem, and the three-dimensional bin packing problem. we consider orthogonal packings where 90 degrees rotations are allowed. the algorithms we show for these problems have asymptotic performance bounds 2.64, and 4.89, respectively. these algorithms are for the more general case in which the bounded dimensions of the bin given in the input are not necessarily equal (that is, we consider bins for which the length. the width and the height are not necessarily equal). moreover, we show that these problems-in the general version-are as hard to approximate as the corresponding oriented version.",
    "present_kp": [
      "approximation algorithms"
    ],
    "absent_kp": [
      "packing problems",
      "orthogonal rotations"
    ]
  },
  {
    "title": "perception for collision avoidance and autonomous driving.",
    "abstract": "the navlab group at carnegie mellon university has a long history of development of automated vehicles and intelligent systems for driver assistance. the earlier work of the group concentrated on road following, cross-country driving, and obstacle detection. the new focus is on short-range sensing, to look all around the vehicle for safe driving. the current system uses video sensing, laser rangefinders, a novel light-stripe rangefinder, software to process each sensor individually, a map-based fusion system, and a probability based predictive model. the complete system has been demonstrated on the navlab i vehicle for monitoring the environment of a vehicle driving through a cluttered urban environment, detecting and tracking fixed objects, moving objects, pedestrians, curbs, and roads.",
    "present_kp": [
      "collision avoidance",
      "autonomous driving"
    ],
    "absent_kp": [
      "short-range surround sensing",
      "optical flow",
      "triangulation laser sensor",
      "curb detection",
      "lidar object detection",
      "sensor fusion",
      "collision prediction"
    ]
  },
  {
    "title": "a low-cost and calibration-free gaze estimator for soft biometrics: an explorative study.",
    "abstract": "an explorative study about gaze as soft-biometrics. a calibration free gaze estimator. gaze estimation from low-cost devices. a non-invasive approach for gaze estimation. quantitative and qualitative evaluation in real environments.",
    "present_kp": [
      "soft-biometrics"
    ],
    "absent_kp": [
      "free gaze estimation",
      "head pose estimation",
      "pupil detection"
    ]
  },
  {
    "title": "taxonomy development and assessment of global information technology outsourcing decisions.",
    "abstract": "purpose - this paper seeks to provide taxonomy and assessment methodologies for executives of global conglomerates with a selection of variables which can help them evaluate outsourcing decisions. design/methodology/approach - a range of established theories, which addressed outsourcing decisions, are identified. the major determinants of global outsourcing were then formulated into an integrated model. each of the variables identified was validated using multiple theories. finally, a weighted score index was used to demonstrate how the variables can be used to evaluate outsourcing decisions. findings - it provides an over-view of outsourcing theories about the variety of major reasons and their associated determinants as well as attributes that are relevant to decision-makers. it showed that the individual theories can be integrated into a global taxonomy. this taxonomy can be assessed using a weighted-index because the inputs and computation processes contain realistic qualitative and quantitative information. research limitations/implications - it is an assessment methodology that requires input and judgment from a variety of experts. persons having such expertise may be fairly costly and difficult to find. practical implications - a very simple, yet comprehensive, and useful taxonomy for executives making outsourcing decisions. the assessment index is a proven methodology that is used by business consultants for a variety of related applications involving decision choices. originality/value - this paper fulfills the need for a more comprehensive view and a systematic approach to the assessment of outsourcing decisions. the index of attributes identified and the evaluation technique proposed are a practical approach.",
    "present_kp": [
      "outsourcing"
    ],
    "absent_kp": [
      "communication technologies",
      "globalization"
    ]
  },
  {
    "title": "segmentation of tongue muscles from super-resolution magnetic resonance images.",
    "abstract": "the first attempt to segment tongue muscles from in vivo mr images is presented. the focus is on segmenting the genioglossus and inferior longitudinalis muscles. the game-theoretic approach is applied for landmark-based segmentation. the database will be publicly released for objective method comparison.",
    "present_kp": [
      "segmentation"
    ],
    "absent_kp": [
      "human tongue",
      "game theory",
      "magnetic resonance imaging",
      "atlasing"
    ]
  },
  {
    "title": "framework for user acceptance: clustering for fine-grained results.",
    "abstract": "in an attempt to uncover intra-group behavior similarities, we developed an open multi-level framework for understanding the process of technology acceptance by its users. we partitioned our population into groups by clustering at several levels and then for each level it was divided into subgroups with a measurement layer added to uncover subgroup influence. thus, by intersecting the resulting clusters of the set of models, the population was divided into subgroups that have similarities in the factors measured by the cluster layer models. subsequently we tested our framework in a university hospital setting; personality and prior technology background models were used in clustering via the five factor model and the technology readiness index. utaut was used in the measurement layer. our hypothesis that the subgroups have differing degrees of explained variance and different predictors was confirmed. our framework was open, because any model that results in a taxonomy of the population can be used to obtain meaningful clusters.",
    "present_kp": [
      "technology acceptance",
      "utaut",
      "clustering"
    ],
    "absent_kp": [
      "ffm",
      "tri",
      "big five"
    ]
  },
  {
    "title": "navigation of interactive sonifications and visualisations of time-series data using multi-touch computing.",
    "abstract": "this paper discusses interaction design for interactive sonification and visualisation of data in multi-touch contexts. interaction design for data analysis is becoming increasingly important as data becomes more openly available. we discuss how navigation issues such as zooming, selection, arrangement and playback of data relate to both the auditory and visual modality in different ways, and how they may be linked through the modality of touch and gestural interaction. for this purpose we introduce a user interface for exploring and interacting with representations of time-series data simultaneously in both the visual and auditory modalities.",
    "present_kp": [
      "interaction",
      "sonification",
      "data analysis"
    ],
    "absent_kp": [
      "tabletop"
    ]
  },
  {
    "title": "wikis in enterprise settings: a survey.",
    "abstract": "the wiki technology is increasingly being used in corporate environments to facilitate a broad range of tasks. this survey examines the use of wikis on a variety of organisational tasks that include the codification of explicit and tacit organisational knowledge and the formulation of corporate communities of practice, as well as more specific processes such as the collaborative information systems development, the interactions of the enterprise with third parties, management activities and organisational response in crisis situations. for each one of the aforementioned corporate functions, the study examines the findings of related research literature to highlight the advantages and concerns raised by the wiki usage and to identify specific solutions addressing them. finally, based on the above findings, the study discusses various aspects of the wiki usage in the enterprise and identifies trends and future research directions on the field.",
    "present_kp": [
      "organisational knowledge"
    ],
    "absent_kp": [
      "enterprise wikis",
      "corporate wikis",
      "wiki research"
    ]
  },
  {
    "title": "red tides prediction system using fuzzy reasoning and the ensemble method.",
    "abstract": "a red tide is a temporary natural phenomenon in which harmful algal blooms (habs) can lead to fin fish and shellfish dying en masse. for example, habs can damage sea farming on the coast of south korea, and generally have a bad influence on the coastal environment and sea ecosystem. prediction of red tide blooms, which consists of a categorical type and a numerical type, can minimize the mitigation cost of hab disasters and the suffering caused by the damage from red tide events. the first type of prediction has high precision but it represents a simple binary result, and the second can predict how much harm an algal increase causes, but its prediction has lower accuracy than the results of the categorical type. to enhance the automatic forecast of red tide, this paper proposes a red tide prediction method that uses fuzzy reasoning and the ensemble method to obtain prediction results for the categorical and numerical types. the proposed method improves the precision of categorical prediction because the ensemble classifier is enhanced by optimal data of the proposed preprocessing. the method forecasts a numerical prediction, such as the increasing density of red tide algae, using the fuzzy reasoning by which the accuracy of numerical results is improved by the proposed post-processing. the experimental results demonstrate that the proposed method achieves a better red tide prediction performance than other single classifiers.",
    "present_kp": [
      "red tides prediction",
      "fuzzy reasoning",
      "ensemble method",
      "harmful algal blooms"
    ],
    "absent_kp": [
      "enhancing prediction"
    ]
  },
  {
    "title": "large-time behaviour of solutions to the equations of one-dimensional nonlinear thermoviscoelasticity with memory.",
    "abstract": "this paper is concerned with the large-time behaviour of globally defined smooth solutions of the initial-boundary value problem for the one-dimensional nonlinear thermoviscoelasticity system with memory.",
    "present_kp": [
      "nonlinear thermoviscoelasticity"
    ],
    "absent_kp": [
      "global smooth solution",
      "materials with memory"
    ]
  },
  {
    "title": "a multilevel input system with force-sensitive elements.",
    "abstract": "force-sensitive multilevel input elements are introduced as the basic building blocks for compact-size input devices in mobile environments. compared with switch-type keys, multilevel elements can decrease the number of keys on a keyboard while maintaining the input capacity. a multilevel input mechanism using force-sensitive sensor pads is demonstrated in a three-level three-element tactile chording system with multimodal feedback. two schemes are introduced to segment the output range of the sensor into levels. for relatively unpracticed users, the scheme based on maximum finger forces gives an average error rate of 20.2% and an input time of 2.24 s for a chord of three inputs. reclassification of the experimental data using gaussian segmentation shows that significant improvement of the performance can be expected.",
    "present_kp": [
      "tactile",
      "input device",
      "multilevel"
    ],
    "absent_kp": [
      "user interface",
      "force sensitive"
    ]
  },
  {
    "title": "data visualization sliders.",
    "abstract": "computer sliders are a generic user input mechanism for specifying a numeric value from a range. for data visualization, the effectiveness of sliders may be increased by using the space inside the slider as an interactive color scale, a barplot for discrete data, and a density plot for continuous data. the idea is to show the selected values in relation to the data and its distribution. furthermore, the selection mechanism may be generalized using a painting metaphor to specify arbitrary, disconnected intervals while maintaining an intuitive user-interface.",
    "present_kp": [
      "data visualization",
      "value",
      "selection",
      "color",
      "scale",
      "input",
      "general",
      "user",
      "interval",
      "values",
      "paint",
      "space",
      "data",
      "relation",
      "effect",
      "generic"
    ],
    "absent_kp": [
      "information visualization",
      "metaphors",
      "thresholding",
      "interaction",
      "user interface",
      "distributed",
      "high interaction",
      "dynamic graphics",
      "continuation"
    ]
  },
  {
    "title": "semi-explicit solution and fast minimization scheme for an energy with a\"\" (1)-fitting and tikhonov-like regularization.",
    "abstract": "regularized energies with a\"\" (1)-fitting have attracted a considerable interest in the recent years and numerous aspects of the problem have been studied, mainly to solve various problems arising in image processing. in this paper we focus on a rather simple form where the regularization term is a quadratic functional applied on the first-order differences between neighboring pixels. we derive a semi-explicit expression for the minimizers of this energy which shows that the solution is an affine function in the neighborhood of each data set. we then describe the volumes of data for which the same system of affine equations leads to the minimum of the relevant energy. our analysis involves an intermediate result on random matrices constructed from truncated neighborhood sets. we also put in evidence some drawbacks due to the a\"\" (1)-fitting. a fast, simple and exact optimization method is proposed. by way of application, we separate impulse noise from gaussian noise in a degraded image.",
    "present_kp": [
      "random matrices"
    ],
    "absent_kp": [
      "non-smooth analysis",
      "l data-fitting",
      "image denoising",
      "signal denoising",
      "tikhonov regularization",
      "impulsive noise",
      "nonsmooth optimization",
      "numerical methods"
    ]
  },
  {
    "title": "a variant of inductive counting.",
    "abstract": "we present a new version of the inductive counting, accepting the complement of an nspace(s(n)) language nondeterministically in space o(s(n)), independent of whether s(n)greater than or equal to log n, but using an additional ''one-way pebble\" - a movable marker placed on the input tape. this reduces the space used by inductive counting to log n + o(s(n)) bits on the binary work tape and gives the weakest known nondeterministic device accepting a co-nspace(o(log n)) language.",
    "present_kp": [
      "inductive counting"
    ],
    "absent_kp": [
      "computational complexity",
      "space complexity",
      "sublogarithmic space"
    ]
  },
  {
    "title": "temperature distribution in steel during hot rolling: pseudo-bond graph view.",
    "abstract": "in hot rolling, a metal is given its final shape by plastically deforming the original stock. in this present work, the above mentioned deformation process is modelled, i.e., temperature, flow stress at each point in and around the deformation zone. a viable bond graph model has been developed to study the essential dynamics inside the material. low-carbon steel has been considered and the modelling is described for single-pass hot rolling.",
    "present_kp": [
      "pseudo-bond graph"
    ],
    "absent_kp": [
      "thermal analysis",
      "steel rolling mill",
      "modelling and simulation",
      "discretization"
    ]
  },
  {
    "title": "capacity bounds in random wireless networks.",
    "abstract": "we consider a receiving node, located at the origin, and a poisson point process (ppp) that models the locations of the desired transmitter as well as the interferers. interference is known to be non-gaussian in this scenario. the capacity bounds for additive non-gaussian channels depend not only on the power of interference (i.e., up to second order statistics) but also on its entropy power which is influenced by higher order statistics as well. therefore, a complete statistical characterization of interference is required to obtain the capacity bounds. while the statistics of sum of signal and interference is known in closed form, the statistics of interference highly depends on the location of the desired transmitter. in this paper, we show that there is a tradeoff between entropy power of interference on the one hand and signal and interference power on the other hand which have conflicting effects on the channel capacity. we obtain closed form results for the cumulants of the interference, when the desired transmitter node is an arbitrary neighbor of the receiver. we show that to find the cumulants, joint statistics of distances in the ppp will be required which we obtain in closed form. using the cumulants, we approximate the interference entropy power and obtain bounds on the capacity of the channel between an arbitrary transmitter and the receiver. our results provide insight and shed light on the capacity of links in a poisson network. in particular, we show that, in a poisson network, the closest hop is not necessarily the highest capacity link.",
    "present_kp": [
      "capacity",
      "interference",
      "poisson point process "
    ],
    "absent_kp": []
  },
  {
    "title": "convex: copy number variation estimation in exome sequencing data using hmm.",
    "abstract": "one of the main types of genetic variations in cancer is copy number variations (cnv). whole exome sequenicng (wes) is a popular alternative to whole genome sequencing (wgs) to study disease specific genomic variations. however, finding cnv in cancer samples using wes data has not been fully explored.",
    "present_kp": [],
    "absent_kp": [
      "cnv detection",
      "cancer genome",
      "targeted resequencing",
      "whole exome sequencing",
      "hidden markov models",
      "discrete wavelet transform"
    ]
  },
  {
    "title": "analogy considered harmful.",
    "abstract": "the computer is like a typewriter. the computer is like a filing cabinet. the computer is a personal servant ready to obey your every command. it is often claimed (e.g., carroll and thomas , rumelhart and norman ) that the best way to introduce a new user to a computer system is to draw an analogy between the computer and some situation familiar to the user. given the analogy, the new user can draw upon his knowledge about the familiar situation in order to reason about the workings of the mysterious new computer system. for example, if the new user wants to understand about how the computer file system works, he need only think about how an office filing cabinet works and then carry over this same way of thinking to the computer file system.",
    "present_kp": [
      "order",
      "knowledge",
      "user",
      "analog"
    ],
    "absent_kp": [
      "situated",
      "examples",
      "systems",
      "drawing",
      "file systems",
      "personality",
      "computation"
    ]
  },
  {
    "title": "risking trust in a public key infrastructure: old techniques of managing risk applied to new technology.",
    "abstract": "installing a public key infrastructure (pki) can change in the security model of an it operation in several ways. this article gives a layman's overview of what exactly a pki is, and how one can be built and operated safely and securely. first, the pki must be designed using the familiar principles of risk management, rather than trust management. next, although it is not widely appreciated, digital signatures are not equivalent to traditional signatures, and understanding this difference is crucial to understanding how a pki needs to be audited. lastly, i will show that for a pki to provide ongoing security, the principles of compromisecontainment and regular auditing must be adhered to.",
    "present_kp": [
      "digital signature",
      "public key",
      "pki",
      "risk management"
    ],
    "absent_kp": [
      "digital certificate",
      "certification authority",
      "trusted third party",
      "cryptography",
      "certification practice statement"
    ]
  },
  {
    "title": "a fixed-point theorem and applications to problems on sets with convex sections and to nash equilibria.",
    "abstract": "a new fixed-point theorem for a family of maps defined on product spaces is obtained. the new result requires the functions involved to satisfy the local intersection properties. previous results required the functions to have the open lower sections which are more restrictive conditions. new properties of multivalued maps are provided and applied to prove the new fixed-point theorem. applications to problems on sets with convex sections and to the existence of nash equilibria for a family of continuous functions are given.",
    "present_kp": [
      "fixed-point theorem",
      "sets with convex sections",
      "nash equilibria"
    ],
    "absent_kp": []
  },
  {
    "title": "on stability delay bounds of simple input-delayed linear and non-linear systems: computational results.",
    "abstract": "this paper deals with the problem of delay size stability analysis of single input-delayed linear and nonlinear systems. conventional reduction, reduction linked by sliding mode, and linear memoryless control approaches are used for simple input-delayed systems to obtain the stability conditions. several first order examples are investigated systematically to demonstrate the capabilities and limitations of the advanced stability analysis techniques including lyapunov-krasovskii functionals, newton-leibniz formula, and a newly addressed lagrange mean value theorem. numerical comparative results show the usefulness and effectiveness of the advanced delay size analysis techniques proposed in this paper.",
    "present_kp": [
      "input-delayed systems",
      "lagrange mean value theorem",
      "lyapunov-krasovskii functional"
    ],
    "absent_kp": [
      "reduction method",
      "sliding mode control"
    ]
  },
  {
    "title": "the ackermann approach for modal logic, correspondence theory and second-order reduction.",
    "abstract": "the problem of eliminating second-order quantification over predicate symbols is in general undecidable. since an application of second-order quantifier elimination is correspondence theory in modal logic, understanding when second-order quantifier elimination methods succeed is an important problem that sheds light on the kinds of axioms that are equivalent to first-order correspondence properties and can be used to obtain complete axiomatizations for modal logics. this paper introduces a substitution-rewrite approach based on ackermann's lemma to second-order quantifier elimination in modal logic. compared to related approaches, the approach includes a number of enhancements: the quantified symbols that need to be eliminated can be flexibly specified. the inference rules are restricted by orderings compatible with the elimination order, which provides more control and reduces non-determinism in derivations thereby increasing the efficiency and success rate. the approach is equipped with a powerful notion of redundancy, allowing for the flexible definition of practical simplification and optimization techniques. we present correctness, termination and canonicity results, and consider two applications: (i) computing first-order frame correspondence properties for modal axioms and rules, and (ii) rewriting second-order modal problems to equivalent simpler forms. the approach allows us to define and characterize two new classes of formulae, which are elementary and canonical, and subsume the class of sahlqvist formulae and the class of monadic inductive formulae.",
    "present_kp": [
      "modal logic",
      "correspondence theory",
      "second-order quantifier elimination"
    ],
    "absent_kp": [
      "ackermann lemma",
      "substitution method",
      "axiom and rule reductions"
    ]
  },
  {
    "title": "compressed histograms with arbitrary bucket layouts for selectivity estimation.",
    "abstract": "selectivity estimation is an important step of query optimization in a database management system, and multi-dimensional histogram techniques have proved promising for selectivity estimation. recent multi-dimensional histogram techniques such as genhist and stholes use an arbitrary bucket layout. this layout has the advantage of requiring a smaller number of buckets to model tuple densities than those required by the traditional grid or recursive layouts. however, the arbitrary bucket layout brings an inherent disadvantage of requiring more memory to store each bucket location information. this diminishes the advantage of requiring fewer buckets and, therefore, has an adverse effect on the resulting selectivity estimation accuracy. to our knowledge, however, no existing histogram-based technique with arbitrary layout addresses this issue. in this paper, we introduce the idea of bucket location compression and then demonstrate its effectiveness for improving selectivity estimation accuracy by proposing the stholes+ technique. stholes+ extends stholes by quantizing each coordinate of a bucket relative to the coordinate of the smallest enclosing bucket. this quantization increases the number of histogram buckets that can be stored in the histogram. our quantization scheme allows stholes+ to trade precision of histogram bucket locations for storing more buckets. experimental results show that stholes+ outperforms stholes on various data distributions, query distributions, and other factors such as available memory size, quantization resolution, and dimensionality of the data space.",
    "present_kp": [
      "selectivity",
      "histogram",
      "compression"
    ],
    "absent_kp": [
      "self-tuning"
    ]
  },
  {
    "title": "facial expression transfer method based on frequency analysis.",
    "abstract": "the subtle expression changes are salient in the frequency domain. a local expression deformation transfer method based on frequency analysis. dynamic expression synthesis for the source subject using expression manifold. a unified framework automatic dynamic expression transfer method.",
    "present_kp": [
      "expression transfer"
    ],
    "absent_kp": [
      "warping technique",
      "facial feature location",
      "frequency domain analysis"
    ]
  },
  {
    "title": "eaf: energy-aware adaptive free viewpoint video wireless transmission.",
    "abstract": "by transmitting views captured using two adjacent cameras in texture plus depth format, any middle view in between could be synthesized with depth image based rendering (dibr) technique. this provides users with continuous viewing angles for selection and is a key technology for a number of emerging applications such as free viewpoint tv. how to deliver the free viewpoint video contents to clients is one fundamental problem for these applications and the challenges exist in the large bandwidth requirement and frequent change of network environments. source encoding rates could be varied to fit different network conditions by changing frame rates, with extra battery life consumption due to skipped frames? interpolation before synthesizing the requested middle view. note that battery development is lagging behind mobile devices? function and computation capability development. how to trade off the source encoding rate reduction and extra energy consumption has not been formally solved as far as we understand. in this paper, we proposed eaf: an energy-aware adaptive free viewpoint video wireless transmission system, where we jointly considered the source encoding rate reduction and extra energy consumption with network constraints. simulation results showed that the proposed scheme advantages over competing schemes significantly in typical network scenarios.",
    "present_kp": [],
    "absent_kp": [
      "multi-view",
      "multimedia",
      "energy aware",
      "video streaming"
    ]
  },
  {
    "title": "implementation of a plasticity bond model for reinforced concrete.",
    "abstract": "this study examines numerical integration schemes for incremental, elastoplastic interface characterizations. the motivating problem is modeling bond of reinforcing bars to concrete in finite element analyses. five first order accurate numerical integration schemes are examined. four algorithms use backward euler integration, but the iterative methods for solving the resulting nonlinear systems of equations (i.e., the plastic correctors) differ. a proposed algorithm minimizes the number of nonlinear equations that are solved numerically; for the bond model usually only one nonlinear equation is solved numerically. the algorithm is comparatively robust with respect to local convergence for arbitrary interface displacements.",
    "present_kp": [
      "reinforced concrete",
      "bond model",
      "plasticity",
      "interface"
    ],
    "absent_kp": [
      "stress-point algorithm"
    ]
  },
  {
    "title": "joint reconstruction of image and motion in gated positron emission tomography.",
    "abstract": "we present a novel intrinsic method for joint reconstruction of both image and motion in positron emission tomography (pet). intrinsic motion compensation methods exclusively work on the measured data, without any external motion measurements. most of these methods separate image from motion estimation: they use deformable image registration/optical flow techniques in order to estimate the motion from individually reconstructed gates. then, the image is estimated based on this motion information. with these methods, a main problem lies in the motion estimation step, which is based on the noisy gated frames. the more noise is present, the more inaccurate the image registration becomes. as we show both visually and quantitatively, joint reconstruction using a simple deformation field motion model can compete with state-of-the-art image registration methods which use robust multilevel b-spline motion models.",
    "present_kp": [
      "motion compensation",
      "positron emission tomography ",
      "reconstruction"
    ],
    "absent_kp": [
      "gating"
    ]
  },
  {
    "title": "the dynamics of two entangled qubits exposed to classical noise: role of spatial and temporal noise correlations.",
    "abstract": "we investigate the decay of two-qubit entanglement caused by the influence of classical noise. we consider the whole spectrum of cases ranging from independent to fully correlated noise affecting each qubit. we take into account different spatial symmetries of noises, and the regimes of noise autocorrelation time. the latter can be either much shorter than the characteristic qubit decoherence time (markovian decoherence), or much longer (approaching the quasi-static bath limit). we express the entanglement of two-qubit states in terms of expectation values of spherical tensor operators which allows for transparent insight into the role of the symmetry of both the two-qubit state and the noise for entanglement dynamics.",
    "present_kp": [
      "two-qubit entanglement",
      "entanglement dynamics",
      "classical noise"
    ],
    "absent_kp": [
      "correlated noises"
    ]
  },
  {
    "title": "optimization of artificial neural networks for prediction of the unit cell parameters in orthorhombic perovskites. comparison with multiple linear regression.",
    "abstract": "the unit cell parameters (a, b, c) of orthorhombic perovskites (of a(2+)b(4+)o(3) and a(3+)b(3+)o(3) type) were predicted both using multiple linear regression analysis (mlr) and two types of artificial neural networks (ann). in these analyses, 70 compounds of above perovskite type were included: 47 in calibration set and 23 in test set, which were randomly chosen. in multiple linear regression, the unit cell parameters of 47 perovskites were expressed as bilinear function of the effective ionic radii of a and b cations, and then, using the obtained regression equation, the unit cell parameters of 23 perovskites were calculated and compared with the experimental data. predictions using the same sets and the same dependent and independent variables were also done by feed-forward and cascade-forward ann. the two different ann models were compared to mlr model by f-test using their root mean square error (rmsep). although the two models give excellent results, it could be concluded that ann have significantly better prediction abilities compared to mlr.",
    "present_kp": [
      "perovskites",
      "linear regression",
      "cell parameter"
    ],
    "absent_kp": []
  },
  {
    "title": "cherub: power consumption aware cluster resource management.",
    "abstract": "this paper presents an evaluation of acpi energy saving modes, and deduces the design and implementation of an energy saving daemon for clusters called cherub. the design of the cherub daemon is modular and extensible. since the only requirement is a central approach for resource management, cherub is suited for server load balancing (slb) clusters managed by dispatchers like linux virtual server (lvs), as well as for high performance computing (hpc) clusters. our experimental results show that cherub's scheduling algorithm works well, i.e. it will save energy, if possible, and avoids state-flapping.",
    "present_kp": [],
    "absent_kp": [
      "green computing",
      "cluster computing"
    ]
  },
  {
    "title": "exor: opportunistic multi-hop routing for wireless networks.",
    "abstract": "this paper describes exor, an integrated routing and mac protocol that increases the throughput of large unicast transfers in multi-hop wireless networks. exor chooses each hop of a packet's route after the transmission for that hop, so that the choice can reflect which intermediate nodes actually received the transmission. this deferred choice gives each transmission multiple opportunities to make progress. as a result exor can use long radio links with high loss rates, which would be avoided by traditional routing. exor increases a connection's throughput while using no more network capacity than traditional routing. exor's design faces the following challenges. the nodes that receive each packet must agree on their identities and choose one forwarder. the agreement protocol must have low overhead, but must also be robust enough that it rarely forwards a packet zero times or more than once. finally, exor must choose the forwarder with the lowest remaining cost to the ultimate destination. measurements of an implementation on a 38-node 802.11b test-bed show that exor increases throughput for most node pairs when compared with traditional routing. for pairs between which traditional routing uses one or two hops, exor's robust acknowledgments prevent unnecessary retransmissions, increasing throughput by nearly 35%. for more distant pairs, exor takes advantage of the choice of forwarders to provide throughput gains of a factor of two to four.",
    "present_kp": [
      "wireless",
      "802.11"
    ],
    "absent_kp": [
      "mesh"
    ]
  },
  {
    "title": "reducing message-length variations in resource-constrained embedded systems implemented using the controller area network (can) protocol.",
    "abstract": "the controller area network (can) protocol is widely used in low-cost embedded systems. can uses non return to zero (nrz) coding and includes a bit-stuffing mechanism. whilst providing an effective mechanism for clock synchronization, the bit-stuffing mechanism causes the can frame length to become (in part) a complex function of the data contents: variations in frame length can have a detrimental impact on the real-time behaviour of systems employing this protocol. in this paper, two software-based mechanisms for reducing the impact of can bit stuffing are considered and compared. the first approach considered is a modified version of a technique described elsewhere (e.g. nolte et al.). the second approach considered is a form of software bit stuffing (sbs). in both cases, not only the impact on message-length variations is addressed but also the implementation costs (including cpu and memory requirements) involved in creating practical implementation of each technique on a range of appropriate hardware platforms. it is concluded that the sbs technique is more effective in the reduction of message-length variations, but at the cost of an increase in cpu time and memory overheads and a reduction in the available data bandwidth. the choice of the most appropriate technique will, therefore, depend on the application requirements and the available resources.",
    "present_kp": [
      "bit stuffing"
    ],
    "absent_kp": [
      "distributed embedded system",
      "can protocol",
      "shared-clock scheduler",
      "timing jitter"
    ]
  },
  {
    "title": "transition from traditional to ict-enhanced learning environments in undergraduate chemistry courses.",
    "abstract": "this paper describes a three-year study conducted among chemistry instructors (professors and teaching assistants) at a post-secondary institution. the goal was to explore the integration process of information and communication technologies (ict) into traditional teaching. four undergraduate chemistry courses incorporated a course website, an electronic forum, computerized visualizations, and web-based projects, into their curriculum. the learning technologies were integrated to enhance inquiry-based learning, visualizations, and knowledge sharing. the current study investigated chemistry instructors' perceptions toward ict and their activities while practicing the newly introduced technologies. the findings showed that integrating new practices is a phase-dependent process that consists of promises as well as complexities. four transition steps were found to characterize the integration of ict-learning environments: non-active, support-dependant, partial-independant, and total-independant. findings indicated that the transition from traditional to ict-enhanced learning environments involves ambivalent feelings and dichotomy among instructors.",
    "present_kp": [
      "visualization"
    ],
    "absent_kp": [
      "improving classroom teaching",
      "post-secondary education",
      "teaching/learning strategies"
    ]
  },
  {
    "title": "experiences teaching data structures with java.",
    "abstract": "this paper describes our experiences incorporating java in a data structures course. we describe the features of java that made for a more interesting course, the difficulties that we encountered, and compare java to the prior languages used in this course, ada and c++. all in all, we found java to be a reasonable, but not overwhelming better, alternative. our students were particularly happy with the experiment.",
    "present_kp": [
      "teaching",
      "experience",
      "paper",
      "feature",
      "language",
      "student",
      "data structures"
    ],
    "absent_kp": []
  },
  {
    "title": "detecting emotional state of a child in a conversational computer game.",
    "abstract": "the automatic recognition of users communicative style within a spoken dialog system framework, including the affective aspects, has received increased attention in the past few years. for dialog systems, it is important to know not only what was said but also how something was communicated, so that the system can engage the user in a richer and more natural interaction. this paper addresses the problem of automatically detecting frustration, politeness, and neutral attitudes from a childs speech communication cues, elicited in spontaneous dialog interactions with computer characters. several information sources such as acoustic, lexical, and contextual features, as well as, their combinations are used for this purpose. the study is based on a wizard-of-oz dialog corpus of 103 children, 714 years of age, playing a voice activated computer game. three-way classification experiments, as well as, pairwise classification between polite vs. others and frustrated vs. others were performed. experimental results show that lexical information has more discriminative power than acoustic and contextual cues for detection of politeness, whereas context and acoustic features perform best for frustration detection. furthermore, the fusion of acoustic, lexical and contextual information provided significantly better classification results. results also showed that classification performance varies with age and gender. specifically, for the politeness detection task, higher classification accuracy was achieved for females and 1011 years-olds, compared to males and other age groups, respectively.",
    "present_kp": [],
    "absent_kp": [
      "emotion recognition",
      "spoken dialog systems",
      "children speech",
      "spontaneous speech",
      "natural emotions",
      "childcomputer interaction",
      "feature extraction"
    ]
  },
  {
    "title": "a simple method to design dielectric resonator-based filters and diplexers using implicit space mapping technique.",
    "abstract": "we present the design of a six pole chebyshev filter and a cascaded quadruple dielectric resonator (dr) filter using space mapping technique. implicit space mapping technique is used throughout and the design emerges within few iterations in both the cases. finite element method based hfss is used in constructing the fine model and agilent ads is used in constructing the coarse model. fine details such as tuning screws are included in the fine model. the same technique is also applied to a dr-based diplexer and is explained. in all the cases, the results obtained with the hardware match well with the analyzed results. the same procedure can be applied in designing much more complex structures such as multiplexers.",
    "present_kp": [
      "dielectric resonator",
      "filter",
      "diplexer",
      "space mapping"
    ],
    "absent_kp": [
      "coupling",
      "parameter extraction"
    ]
  },
  {
    "title": "on the expressiveness and complexity of atl.",
    "abstract": "atl is a temporal logic geared towards the specification and verification of properties in multi-agents systems. it allows to reason on the existence of strategies for coalitions of agents in order to enforce a given property. in this paper, we first precisely characterize the complexity of atl model-checking over alternating transition systems and concurrent game structures when the number of agents is not fixed. we prove that it is delta(p)(2)- and delta(p)(3)-complete, depending on the underlying multi-agent model (ats and cgs resp.). we also consider the same problems for some extensions of atl. we then consider expressiveness issues. we show how ats and cgs are related and provide translations between these models w.r.t. alternating bisimulation. we also prove that the standard definition of atl (built on modalities \"next\", \"always\" and \"until\") cannot express the duals of its modalities: it is necessary to explicitely add the modality \"release\".",
    "present_kp": [
      "temporal logic"
    ],
    "absent_kp": [
      "multi-agent systems",
      "model checking"
    ]
  },
  {
    "title": "deformable templates guided discriminative models for robust 3d brain mri segmentation.",
    "abstract": "automatically segmenting anatomical structures from 3d brain mri images is an important task in neuroimaging. one major challenge is to design and learn effective image models accounting for the large variability in anatomy and data acquisition protocols. a deformable template is a type of generative model that attempts to explicitly match an input image with a template (atlas), and thus, they are robust against global intensity changes. on the other hand, discriminative models combine local image features to capture complex image patterns. in this paper, we propose a robust brain image segmentation algorithm that fuses together deformable templates and informative features. it takes advantage of the adaptation capability of the generative model and the classification power of the discriminative models. the proposed algorithm achieves both robustness and efficiency, and can be used to segment brain mri images with large anatomical variations. we perform an extensive experimental study on four datasets of t1-weighted brain mri data from different sources (1,082 mri scans in total) and observe consistent improvement over the state-of-the-art systems.",
    "present_kp": [
      "brain image segmentation",
      "deformable templates",
      "discriminative models"
    ],
    "absent_kp": [
      "fusion",
      "generative models"
    ]
  },
  {
    "title": "accurate retinal blood vessel segmentation by using multi-resolution matched filtering and directional region growing.",
    "abstract": "a new method to extract retinal blood vessels from a colour fundus image is described. digital colour fundus images are contrast enhanced in order to obtain sharp edges. the green bands are selected and transformed to correlation coefficient images by using two sets of gaussian kernel patches of distinct scales of resolution. blood vessels are then extracted by means of a new algorithm, directional recursive region growing segmentation or d-rrgs. the segmentation results have been compared with clinically-generated ground truth and evaluated in terms of sensitivity and specificity. the results are encouraging and will be used for further application such as blood vessel diameter measurement.",
    "present_kp": [
      "retina",
      "fundus image",
      "blood vessel",
      "matched filter",
      "region growing"
    ],
    "absent_kp": []
  },
  {
    "title": "technology mapping for fpgas with embedded memory blocks.",
    "abstract": "modern field programmable gate arrays (fpgas) provide embedded memory blocks (embs) to be used as on-chip memories. in this paper, we explore the possibility of using embs to implement logic functions when they are not used as on-chip memory. we propose a general technology mapping problem for fpgas with embs for area and delay minimization and develop an efficient algorithm based on the concepts of maximum fanout free cone (mffc) and maximum fanout free subgraph (mffs) , named emb_pack, which minimizes the area after or before technology mapping by using embs while maintaining the circuit delay. we have tested emb_pack on mcnc benchmarks on altera's flex10k device family . the experimental results show that compared with the original mapped circuits generated from cutmap without using embs, emb_pack as postprocessing can further reduce up to 10% of the area on the mapped circuits while maintaining the layout delay by making efficient use of available emb resources. compared with cutmap-e without using embs, emb_pack as pre-mapping processing followed by cutmap-e can reduce 6% of the area while maintaining the circuit optimal delay.",
    "present_kp": [
      "fpgas",
      "use",
      "technology mapping",
      "benchmark",
      "device",
      "delay",
      "layout",
      "concept",
      "process",
      "map",
      "logic",
      "general",
      "algorithm",
      "paper",
      "resource",
      "circuits"
    ],
    "absent_kp": [
      "field-programmable-gate-array",
      "experimentation",
      "efficiency",
      "families",
      "optimality",
      "memorialized",
      "embedding",
      "exploration",
      "minimal"
    ]
  },
  {
    "title": "b-spline surfaces for ship hull design.",
    "abstract": "the use of true sculptured surface descriptions for design applications has been proposed by numerous authors. the actual implementation and use of interactive sculptured surface description techniques for design and production has been limited. the use of such techniques for ship hull design has been even more limited. the present paper describes a preliminary implementation of such a system for the design of ship hulls and for the production of towing tank models using numerical control techniques. the present implementation is based on a cartesian product b-spline surface description. implementation is on an evans and sutherland picture system supported by a pdp-11/45 minicomputer. the b-spline surface is manipulated by its associated polygonal net. both surface and net are three-dimensional. techniques both good and bad for 3-d picking of a polygon point when the net, its associated surface, and the 3-d picking cue independently exist and can be independently manipulated in three space are presented and discussed. the shape of a b-spline surface of fixed order is controlled by the location of the polygon net points, the number of multiple points at a particular net point, and the knot vector. frequently multiple points imply multiple knot vectors. practical techniques for controlling and shaping the surface with and without this assumption are discussed and the results illustrated. experience attained by interactively fitting a single fourth order b-spline surface patch to the forebody half of an actual ship hull described by three dimensional digitized points is discussed and the results illustrated.",
    "present_kp": [
      "fit",
      "point",
      "applications",
      "order",
      "use",
      "design",
      "polygon",
      "paper",
      "model",
      "shape",
      "practical",
      "control",
      "patch",
      "product",
      "author",
      "b-spline",
      "space",
      "implementation",
      "numerical control",
      "sculptured surface",
      "ship hull",
      "surface"
    ],
    "absent_kp": [
      "association",
      "computer graphics",
      "interaction",
      "vectorization",
      "systems",
      "locatability",
      "cadcam"
    ]
  },
  {
    "title": "fpga implementation of a near computation free image compression scheme based on adaptive decimation.",
    "abstract": "adaptive decimation is a technique that can be applied to compress images with good visual quality at low bit-rate. despite the low complexity of the method, the encoding process involves floating point computation that requires the use of medium speed processors in order to achieve real time operation. in this paper, a new approach has been taken to restructure the adaptive decimation algorithm to a form that includes only small amount of arithmetic operations. the revised algorithm can be implemented with simple logic circuits. the encoder is practically free from complicated numerical computation. a fpga implementation of the proposed algorithm was realized. the architecture of the encoder is simple and suitable for valuable applications in the development of low cost non-processor based multimedia products.",
    "present_kp": [
      "adaptive decimation",
      "computation free",
      "fpga"
    ],
    "absent_kp": [
      "interpolation"
    ]
  },
  {
    "title": "the study of wsn routing.",
    "abstract": "zigbee network layer, the main purpose of the agreement is to provide reliable and secure transmission. there are three kinds of networks topology in zigbee network layer: star topology, tree topology, mesh topology. this study was used in the experiment of the packet size and speed are not great, postponed plans did not change significantly. however, the same packet would be delayed with the lost, only a very small number, but if increasing the amount of packets with the transmission speed, delay will increase with the loss of. when over load, will be a substantial loss. from the experiments, the tree to delay longer than the star topology and mesh topology a long time, star topology is also informed that easy to synchronize the delay time is to do the least. although the delay of the mesh topology is better than the tree topology, but the mesh topology to play its characteristics, his performance is better than the star and tree topology.",
    "present_kp": [
      "mesh topology",
      "tree topology",
      "star topology"
    ],
    "absent_kp": []
  },
  {
    "title": "effect of varying concentration and temperature on steady and dynamic parameters of low concentration photovoltaic energy system.",
    "abstract": "low concentration photovoltaic (lcpv) system is designed and developed. commercially available crystalline silicon solar pv cells used under concentration. dynamic behavior of lcpv system is modeled and estimated. real-time analysis of lcpv system is presented.",
    "present_kp": [
      "concentration photovoltaic "
    ],
    "absent_kp": [
      "low-concentration photovoltaic  system",
      "dynamic resistance",
      "silicon solar pv module"
    ]
  },
  {
    "title": "on the numerical modelling of the multiphysics self piercing riveting process based on the finite element technique.",
    "abstract": "the development of reliable numerical models permits to investigate the manufacturing processes with very low incremental costs or prototyping efforts hence it provides a relevant help in process optimisation and gives great opportunity for making maximum use of sparse process data . among others the metal forming processes have heavily benefited from the finite element numerical computing technology . the self piercing riveting (spr) is a cold forming process which creates a strong mechanical interlock between two or more sheets by means of a semi-tubular rivet, which, pressed by a punch, pierces the upper sheet and flares into the bottom one. it is governed by complex multiphysics phenomena whose governing equations can be resolved using the finite element method. in this paper all the governing equations are fully reported along with the mathematics of the resolving method needed for setting up and simulate a finite element model of the self piercing riveting of an aluminium alloy. a case study of the spr of two sheets of the 6060t4 aluminium alloy using a steel rivet was investigated. the calculations were performed using the lsdyna finite element commercial code. the problems encountered and the solutions applied for the preparation of the model and the run of the calculation were presented and discussed. the obtained results were validated by comparison with data coming from a laboratory experiment.",
    "present_kp": [
      "self piercing riveting",
      "numerical modelling"
    ],
    "absent_kp": [
      "multiphysics governing equations"
    ]
  },
  {
    "title": "adaptive available bandwidth estimation for internet video streaming.",
    "abstract": "in this study, an adaptive available bandwidth estimation approach that is suitable for internet video streaming is developed. the algorithm exploits repetitive measurements and uses this redundancy to improve its video adaptation decision. the importance of available bandwidth estimation in internet applications has recently increased particularly because of the heterogeneity of the network links. many of the internet paths may contain wired and wireless links in which loss may happen due to congestion as well as link errors. hence, loss rate by itself is not a sufficient statistics for monitoring purposes. if the loss is due to congestion, video quality can then be decreased whereas if the loss is due to link error, no such action is necessary. moreover, in video streaming, such an estimate can be used to determine the new video rate if the quality is to be increased. in our approach, active probing packets are used to estimate bandwidth in very short time duration. the novelty of our estimator is its adaptivity in the sense that the overhead caused by the estimator is automatically reduced when congestion builds up. the trade off is reduced accuracy. such accuracy is not needed under congestion anyway and when things get back to normal, our estimator turns back to normal operation mode. we have integrated our algorithm into our video streamer and carried out experiments on both simulated and actual streaming applications on the internet. the results indicate that our estimator algorithm increases streaming performance substantially.",
    "present_kp": [
      "available bandwidth estimation",
      "congestion",
      "link errors"
    ],
    "absent_kp": [
      "bottleneck bandwidth",
      "rate adaptive video streaming"
    ]
  },
  {
    "title": "cost-oriented task allocation and hardware redundancy policies in heterogeneous distributed computing systems considering software reliability.",
    "abstract": "task allocation policy and hardware redundancy policy for distributed computing system (dcs) are of great importance as they affect many system characteristics such as system cost, system reliability and performance. in recent years, abundant research has been carried out on the optimal task allocation and/or hardware redundancy problem, most of which took a reliability-oriented approach, i.e., the optimization criterion was system reliability maximization. nevertheless, besides system reliability, other system characteristics such as system cost may be of great concern to management. in this paper, we take a cost-oriented approach to the optimal task allocation and hardware redundancy problem for dcs, which addresses both system cost and system reliability issues. a system cost model which could reflect the impact of system unreliability on system cost is developed, and by minimizing the total system cost, a satisfactory level of system reliability could be reached simultaneously. in the reliability modeling and analysis of dcs, we take both hardware reliability and software reliability into account. two numerical examples are given to illustrate the formulation and solution procedures, in which genetic algorithm is used. results show that based on the developed system cost model, appropriate decision-makings on task allocation and hardware redundancy policies for dcs could be made, and the result obtained seems to be a fairly good trade-off between system cost and system reliability.",
    "present_kp": [
      "distributed computing system",
      "task allocation",
      "cost model",
      "software reliability"
    ],
    "absent_kp": [
      "parallel redundancy"
    ]
  },
  {
    "title": "a qualitative case study of the adoption and use of an agricultural decision support system in the australian cotton industry: the socio-technical view.",
    "abstract": "in response to the call for research that considers the human as well as the technical aspects of information systems implementation, the authors report on an interpretive case study which explores the adoption and use of an agricultural decision support system (dss) cottonlogic in the australian cotton industry. the study was informed through the innovation-decision model by rogers and the technology-in-practice model by orlikowski using a socio-technical approach. it was found that participants who achieved a high level of implementation success were reflexive and resourceful in adapting the technology to their changing needs, often in ways unanticipated by dss builders.",
    "present_kp": [
      "cotton",
      "australia",
      "qualitative",
      "case study",
      "socio-technical",
      "implementation"
    ],
    "absent_kp": [
      "decision support systems",
      "farm management",
      "diffusion theory",
      "structurational model of technology"
    ]
  },
  {
    "title": "an innovationdiffusion view of implementation of enterprise resource planning (erp) systems and development of a research model.",
    "abstract": "firms around the world have been implementing enterprise resource planning (erp) systems since the 1990s to have an uniform information system in their respective organizations and to reengineer their business processes. through a case type analysis conducted in six manufacturing firms that have one of the widely used erp systems, various contextual factors that influenced these firms to implement this technology were understood using the six-stage model proposed by kwon and zmud. three types of erp systems, viz. sap, baan and oracle erp were studied in this research. implementation of erp systems was found to follow the stage model. the findings from the process model were used to develop the items for the causal model and in identifying appropriate constructs to group those items. in order to substantiate that the constructs developed to measure the causal model were congruent with the findings based on qualitative analysis, i.e. that the instrument appropriately reflects the understanding of the case interview; triangulation technique was used. the findings from the qualitative study and the results from the quantitative study were found to be equivalent, thus, ensuring a fair assessment of the validity and reliability of the instrument developed to test the causal model. the quantitative measures done only at these six firms are not statistically significant but the samples were used as a part of the triangulation method to collect data from multiple sources, to verify the respondents understanding of the scales and as an initial measure to see if my understanding from the qualitative studies were accurately reflected by the instrument. this instrument will be pilot tested first and administered to a large sample of firms.",
    "present_kp": [
      "process model",
      "causal model",
      "contextual factors",
      "triangulation"
    ],
    "absent_kp": [
      "enterprise resource planning  systems",
      "integration and performance"
    ]
  },
  {
    "title": "biasing bayesian optimization algorithm using case based reasoning.",
    "abstract": "studies show that application of the prior knowledge in biasing the estimation of distribution algorithms (edas), such as bayesian optimization algorithm (boa), increases the efficiency of these algorithms significantly. one of the main advantages of the edas over other optimization algorithms is that the former provides a trail of probabilistic models of candidate solutions with increasing quality. some recent studies have applied these probabilistic models, obtained from previously solved problems in biasing the boa algorithm, to solve the future problems. in this paper, in order to improve the previous works and reduce their disadvantages, a method based on case based reasoning (cbr) is proposed for biasing the boa algorithm. herein, after running boa for solving optimization problems, each problem, the corresponding solution, as well as the last bayesian network obtained from the boa algorithm, will be stored as an entry in the case-base. upon introducing a new problem, similar problems from the case-base are retrieved and the last bayesian networks of these solved problems are combined according to the degree of their similarity with the new problem; hence, a compound bayesian network is constructed. the compound bayesian network is sampled and the initial population for the boa algorithm is generated. this network will be applied efficiently for biasing future probabilistic models during the runs of boa for the new problem. the proposed method is tested on three well-known combinatorial benchmark problems. experimental results show significant improvements in algorithm execution time and quality of solutions, compared to previous methods.",
    "present_kp": [
      "bayesian optimization algorithm",
      "bias",
      "bayesian network",
      "case based reasoning",
      "initial population"
    ],
    "absent_kp": []
  },
  {
    "title": "a numerical study on mechanical performance of asphalt mixture using a meso-scale finite element model.",
    "abstract": "this paper concerns with meso-scale finite element (fe) modeling of asphalt mixtures. the proposed fe model is capable of simulating the complex geometry of several types of asphalt mixture in which different gradations of aggregates could be properly modeled. the meso-structure of the mixture is constructed by a novel technique identified in this paper. the main idea of such technique is that the aggregate particles could be randomly packed together into the simulation region, by defining a kind of artificial interaction forces among the particles. after that by the voronoi tessellation method, the set of the generated discrete grains will alter to space-filling, adjoining polyhedrons with respect to the real geometry so that it were possible to investigate the behavior of mixture by finite element method. such a model considers the main components of the asphalt mixture consisting of aggregate particles, mastic, interfacial zone and air voids. moreover, different moving wheel loads with different passing velocities are considered and their effects on the mechanical responses of the asphalt mixture are examined. the fe model gives some better insights into the behavior of mixture's components under moving loads. the responses include vertical displacement of the pavement surface as well as the development of stress in mixture components. the proposed models give results that are in agreement with theoretical predictions and previous studies.",
    "present_kp": [
      "asphalt mixture",
      "finite element method"
    ],
    "absent_kp": [
      "meso-scale model"
    ]
  },
  {
    "title": "expressive and efficient pattern languages for tree-structured data (extended abstract).",
    "abstract": "it would be desirable to have a query language for tree-structured data that is (1) as easily usable as sql, (2) as expressive as monadic second-order logic (mso), and (3) efficiently evaluable. the paper develops some ideas in this direction. towards (1) the specification of sets of vertices of a tree by combining conditions on their induced subtree with conditions on their path to the root is proposed. existing query languages allow regular expressions (hence mso logic) in path conditions but are limited in expressing subtree conditions. it is shown that such query languages fall short of capturing all mso queries. on the other hand, allowing a certain guarded fragment of mso-logic in the specification of subtree conditions results in a language fulfilling (2), (3) and, anguably, (1).",
    "present_kp": [
      "tree",
      "regular expression",
      "order",
      "pattern language",
      "language",
      "express",
      "sql",
      "query languages",
      "direct",
      "queries",
      "logic",
      "paper"
    ],
    "absent_kp": [
      "fragmentation",
      "efficiency",
      "abstraction",
      "usability",
      "combinational",
      "structural data"
    ]
  },
  {
    "title": "a fuzzy approach to resource aware automatic parallelization.",
    "abstract": "any realistic approach to automatic program parallelization must take into account practical issues related to the resource usage of parallel executions, such as the overheads associated with parallel tasks creation, migration of tasks to remote processors, and communication. the aim of granularity control techniques is avoiding such overheads undermining the benefits of parallel executions. for example, sufficient conditions have been proposed to ensure that the parallel execution of some given tasks will not take longer than their corresponding sequential execution. however, when the goal is to optimize the average execution time of several runs, such conditions can be very conservative, causing a loss in parallelization opportunities. to solve this problem, we have proposed novel conditions based on fuzzy logic and performed an experimental assessment with real programs. the results show that such conditions select the optimal type of execution in most cases and behave much better than the conservative conditions.",
    "present_kp": [
      "automatic parallelization",
      "granularity control"
    ],
    "absent_kp": [
      "fuzzy logic application",
      "parallel computing",
      "scheduling",
      "complexity analysis"
    ]
  },
  {
    "title": "a survey of experienced user perceptions about software design patterns.",
    "abstract": "although the concept of the software design pattern is well-established, there is relatively little empirical knowledge about the patterns that experienced users consider to be most valuable. to identify which patterns from the set catalogued by the gang of four are considered to be useful by experienced users, which ones are considered as not being useful, and why this is so. we undertook a web-based survey of experienced pattern users, seeking information about their experiences as software developers and maintainers. our sampling frame consisted of the authors of all of the pattern papers that we had identified in a preceding systematic review of studies of patterns. we received 206 usable responses, corresponding to a response rate of 19% from the original sampling frame. most respondents were involved with software development rather than maintenance. while patterns can provide a means of sharing knowledge schemas between designers, only three patterns were widely regarded as valuable. around one quarter of the patterns gained very low approval or worse. these observations need to be considered when using patterns; teaching students about the pattern concept; and planning empirical studies about patterns.",
    "present_kp": [
      "software design patterns",
      "survey",
      "software design"
    ],
    "absent_kp": []
  },
  {
    "title": "integrating radial basis function networks with case-based reasoning for product design.",
    "abstract": "this paper presents a case-based design expert system that automatically determines the design values of a product. we focus on the design problem of a shadow mask which is a core component of monitors in the electronics industry. in case-based reasoning (cbr), it is important to retrieve similar cases and adapt them to meet design specifications exactly. notably, difficulties in automating the adaptation process have prevented designers from being able to use design expert systems easily and efficiently. in this paper, we present a hybrid approach combining cbr and artificial neural networks in order to solve the problems occurring during the adaptation process. we first constructed a radial basis function network (rbfn) composed of representative cases created by k-means clustering. then, the representative case most similar to the current problem was adjusted using the network. the rationale behind the proposed approach is discussed, and experimental results acquired from real shadow mask design are presented. using the design expert system, designers can reduce design time and errors and enhance the total quality of design. furthermore, the expert system facilitates effective sharing of design knowledge among designers.",
    "present_kp": [
      "case-based reasoning ",
      "radial basis function network ",
      "design expert system",
      "product design"
    ],
    "absent_kp": []
  },
  {
    "title": "rna-mediated inhibition of hiv in a gene therapy setting.",
    "abstract": "at present, treatment for hiv-1 infection employs highly active anti-retroviral therapy (haart), which utilizes a combination of rt and protease inhibitors. unfortunately, hiv can escape many therapies because of its high mutation rate and the complexity of its pathogenesis. hiv-1 integrates into the cellular genome, which facilitates persistence and acts as a reservoir for reactivation and replication. as an alternative or adjuvant to chemotherapy we have been developing an rna-based gene therapy approach for the treatment of hiv-1 infection. this article summarizes the various rna based technologies that we have developed for potential application in a gene therapy setting.",
    "present_kp": [
      "gene therapy",
      "hiv-1"
    ],
    "absent_kp": [
      "ribozymes",
      "nucleolar rna decoys",
      "sirnas",
      "lentiviral vector"
    ]
  },
  {
    "title": "identifying and quantifying metabolites by scoring peaks of gc-ms data.",
    "abstract": "metabolomics is one of most recent omics technologies. it has been applied on fields such as food science, nutrition, drug discovery and systems biology. for this, gas chromatography-mass spectrometry (gc-ms) has been largely applied and many computational tools have been developed to support the analysis of metabolomics data. among them, amdis is perhaps the most used tool for identifying and quantifying metabolites. however, amdis generates a high number of false-positives and does not have an interface amenable for high-throughput data analysis. although additional computational tools have been developed for processing amdis results and to perform normalisations and statistical analysis of metabolomics data, there is not yet a single free software or package able to reliably identify and quantify metabolites analysed by gc-ms.",
    "present_kp": [
      "metabolomics",
      "gc-ms",
      "data analysis"
    ],
    "absent_kp": [
      "identification"
    ]
  },
  {
    "title": "optical properties of sic/azo plasmonic nano composite at infrared frequencies.",
    "abstract": "we report a study of the optical properties of a mixture of spherical inclusions within a host continuum. the structure of the designed sic/azo material is a binary composite of two different kinds of nano spheres, one made from a semiconductor material (sic) and the other from a plasmonic material (azo). its electromagnetic response is studied using the extended maxwellgarnett effective medium theory and a rigorous method based on layer-multiple-scattering theory. a very nice agreement between both treatments is established, rendering the effective medium approximation a useful guide for the experimentalist in the field.",
    "present_kp": [],
    "absent_kp": [
      "plasmonic composite",
      "extended maxwellgarnett theory",
      "layer-multiple-scattering method",
      "effective permittivity",
      "effective permeability"
    ]
  },
  {
    "title": "linear broadcast encryption schemes.",
    "abstract": "a new family of broadcast encryption schemes, which will be called linear broadcast encryption schemes (lbess), is presented in this paper by using linear algebraic techniques. this family generalizes most previous proposals and provides a general framework to the study of broadcast encryption schemes. we present a method to construct, for a general specification structure, lbess with a good trade-off between the amount of secret information stored by every user and the length of the broadcast message. in this way, we are able to find schemes that fit in situations that have not been considered before.",
    "present_kp": [
      "broadcast encryption"
    ],
    "absent_kp": [
      "distributed cryptography",
      "key distribution",
      "key predistribution schemes"
    ]
  },
  {
    "title": "programming living cells to function as massively parallel computers.",
    "abstract": "we have reprogrammed the genomes of living cells to construct massively parallel biological computers capable of processing two-dimensional images at a theoretical resolution of greater than 100 megapixels per square inch. first, we rewired a signal transduction pathway in escherichia coli to express a pigment producing enzyme under the control of red light. we then use the engineered bacteria as pixels in biological film. next, use the 'bacterial photography' technology as tool for the engineering of a massively parallel biological computer which uses cell-cell communication to compute the edges (light-dark boundaries) within images.",
    "present_kp": [],
    "absent_kp": [
      "edge detection",
      "cellular computing",
      "synthetic biology",
      "image processing"
    ]
  },
  {
    "title": "performance optimization of large non-negatively constrained least squares problems with an application in biophysics.",
    "abstract": "solving large non-negatively constrained least squares systems is frequently used in the physical sciences to estimate model parameters which best fit experimental data. analytical ultracentrifugation (auc) is an important hydrodynamic experimental technique used in biophysics to characterize macromolecules and to determine parameters such as molecular weight and shape. we previously developed a parallel divide and conquer method to facilitate solving the large systems obtained from auc experiments. new auc instruments equipped with multi-wavelength (mwl) detectors have recently increased the data sizes by three orders of magnitude. analyzing the mwl data requires significant compute resources. to better utilize these resources, we introduce a procedure allowing the researcher to optimize the divide and conquer scheme along a continuum from minimum wall time to minimum compute service units. we achieve our results by implementing a preprocessing stage performed on a local workstation before job submission.",
    "present_kp": [
      "analytical ultracentrifugation"
    ],
    "absent_kp": []
  },
  {
    "title": "influence of equivalent bolt length in finite element modeling of t-stub steel connections.",
    "abstract": "in this paper the development and implementation of a finite element model for simple t-stub steel connections is presented. material and geometric non-linearities as well as contact and friction have been implemented in the model. the model is validated by comparison with experimental data found in the literature, for configurations exhibiting different failure mechanisms and featuring different bolt preloading levels. the impact of bolt length considered in the model is investigated and is shown to be of primary importance. this issue is representative of the continuously increasing use of advanced numerical analysis, supported by progress in computational mechanics, as a tool for practical design of engineering structures.",
    "present_kp": [
      "steel connections",
      "t-stub",
      "finite element modeling",
      "contact"
    ],
    "absent_kp": [
      "material and geometric non-linearity"
    ]
  },
  {
    "title": "a novel fem model for biaxial non-crimp fabric composite materials under tension.",
    "abstract": "this paper presents a novel finite element based approach able to represent the complex architecture of the non-crimp fabric (ncf) composite materials. by means of the stiffness averaging method, implemented in the research oriented fem (finite element method) code b2000, the developed model is able to simulate the ncfs mechanical performances. applications to simple coupons loaded in tension are presented in order to demonstrate the capability and the effectiveness of the presented approach. nevertheless, the proposed methodology can be applied and extended to all ncf geometries. first, for validation purposes the numerical results detained for a specific configuration have been compared with experimental results available from literature. then, a parametric study has been carried out to investigate the influence of the bundle waviness on the tension stiffness. finally, due to the degradation of the in-plane mechanical properties, the presence of the stitching has been investigated.",
    "present_kp": [
      "ncf",
      "stiffness averaging method",
      "stitching",
      "waviness",
      "fem"
    ],
    "absent_kp": [
      "rve",
      "processing variables"
    ]
  },
  {
    "title": "implementing polyinstantiation as a strategy for electronic commerce customer relationship management.",
    "abstract": "polyinstantiation is the situation where multiple records sharing the same identifier value occur in one table. multi-level secure (mls) data models manage and utilize polyinstantiation to provide a secure way of handling classified information. customer relationship management (crm) systems for e-business can leverage the strategy of managed polyinstantiation by implementing mls technology to coordinate b2c interactions in order to build long-term loyalty. this approach can be used to address some of the challenges faced by providers and adopters of e-business crm technology solutions. a pilot study evaluated polyinstantiated information-presentation strategy as a means of enhancing relationships between e-businesses and their customers. the results support the idea that customers perceive the benefits of their special customer status as a function of how the relevant data are presented.",
    "present_kp": [
      "customer relationship management ",
      "polyinstantiation"
    ],
    "absent_kp": [
      "customer behavior",
      "e-commerce crm",
      "information presentation",
      "market segmentation",
      "multi-level secure  data model"
    ]
  },
  {
    "title": "bayesian age-period-cohort modeling and prediction - bamp.",
    "abstract": "the software package b a m p provides a method of analyzing incidence or mortality data on the lexis diagram, using a bayesian version of an age-period-cohort model. a hierarchical model is assumed with a binomial model in the first-stage. as smoothing priors for the age, period and cohort parameters random walks of first and second order, with and without an additional unstructured component are available. unstructured heterogeneity can also be included in the model. in order to evaluate the model fit, posterior deviance, dic and predictive deviances are computed. by projecting the random walk prior into the future, future death rates can be predicted.",
    "present_kp": [
      "prediction"
    ],
    "absent_kp": [
      "bayesian hierarchical models",
      "age-period-cohort models"
    ]
  },
  {
    "title": "knowledge based manufacturing system (kbms).",
    "abstract": "production management, in batch type manufacturing environment, is regarded by the current research community as a very complex task. this paper claims that the complexity is a result of the system approach where management performance relies on decisions made at a too early stage in the manufacturing process. decisions are made and stored in company databases by engineers who are neither economists nor production planner's experts. this paper presents a new method where engineer's task is not to make decisions but rather to prepare a knowledge-based \"road map\". the road map method does introduce flexibility and dynamics in the manufacturing process and thus simplifies the decision making process in production planning. each user will generate a routine that meets his/her needs at the time of needs by using kbms capp. thereby this method increases dramatically manufacturing efficiency.",
    "present_kp": [
      "manufacturing"
    ],
    "absent_kp": [
      "scheduling",
      "capacity planning",
      "shop floor control"
    ]
  },
  {
    "title": "document retrieval by projection based frequency distribution.",
    "abstract": "in document retrieval task, random projection (rp) is a useful technique of dimension reduction. it can be obtained very quickly yet the recalculation is not necessary to any changes. however, in lower dimension, random projection has instability by randomness in itself. in this investigation, we propose a new technique, called skewed projection (sp), for dimension reduction based on term frequency distribution. by our experiments, we show that we can take advantages of local independence thus we can obtain efficient retrieval for documents which belong to specific application area. also we examine document size by which we can determine term distribution.",
    "present_kp": [
      "skewed projection",
      "random projection",
      "dimension reduction"
    ],
    "absent_kp": [
      "information retrieval",
      "vector space model"
    ]
  },
  {
    "title": "robust blind watermarking mechanism for motion data streams.",
    "abstract": "the commercial reuse of 3d motion capture (mocap) data in animation and life sciences raises issues with respect to its copyright. in order to improvise content protection of mocap data, we devise a substitutive blind watermarking technique. this technique visualizes 3d mocap data as a series of non-intersecting cluster of triangles. bits are encoded inside the triangles by using an extended substitutive bit encoder in spatial domain. the encoding supports watermark imperceptibility and develops robustness against affine transforms (rotation, translation, scaling), noise addition, reordering and sample loss attacks. security of the scheme can be enhanced by adding secret embedding distances between clusters, which are based on a secret key used for watermarking purpose.",
    "present_kp": [
      "spatial",
      "watermarking",
      "imperceptibility",
      "encoding",
      "blind"
    ],
    "absent_kp": [
      "decoding"
    ]
  },
  {
    "title": "computational expressiveness of genetic systems.",
    "abstract": "we introduce genetic systems, a formalism inspired by genetic regulatory networks and suitable for modelin g the interactions between the genes and the proteins, acting as regulatory products. the generation of: new objects, representing proteins, is driven by genetic gates: a new object is produced when all the activator objects are available in the system, and no inhibitor object is available. activators are not consumed by the application of such an evolution rule. objects disappear because of degradation: each object is equipped with a lifetime, and the object decays when such a lifetime expires. we investigate the computational expressiveness of genetic systems: we show that they are turing equivalent by providing encodings of random access machines in genetic systems.",
    "present_kp": [
      "genetic regulatory networks"
    ],
    "absent_kp": [
      "concurrency and distributed computation",
      "molecular computation"
    ]
  },
  {
    "title": "identifying the nature of stomach diseases by ultrasonography based on genetic neural network.",
    "abstract": "clinical features and ultrasound signs of 76 subjects including 23 health subjects are collected. all samples are divided into the training set (38 samples) and the test set (38 samples) based on mean vector similarity. each set contains 19 benign and 19 malignant. multiple linear regression model (mlr-model), back propagation neural network model (bpn-model) and genetic algorithm-based back propagation neural network model (gabpn-model) are established for distinguishing malignant from benign stomach diseases and are trained using the training set. then three models are tested using the test set. the accuracy, the sensitivity and the specificity for the test set, gabpn-model are 92.1%, 89.5% and 94.7%, bpn-model are 89.5%, 89.5% and 89.5%, mlr-model are 89.5%, 84.2% and 94.7%. areas under curve of gabpn-model, bpn-model and mlr-model are respectively 0.978, 0.945 and 0.958 in roc analysis. these results confirm that color doppler ultrasound can be used as a tool for distinguishing benign from malignant stomach diseases (p < 0.01). genetic algorithm-based neural network outperforms the multivariate linear regression and the back propagation neural network to establish a model identifying the nature of stomach diseases.",
    "present_kp": [
      "genetic algorithm",
      "stomach diseases",
      "ultrasonography"
    ],
    "absent_kp": [
      "computer-assisted diagnosis",
      "artificial neural network"
    ]
  },
  {
    "title": "the selection and assemblage of approximation functions and disposal of its singularity in axisymmetric drbem for heat transfer problems.",
    "abstract": "in this paper, a group of approximation functions f with summed form are developed in the dual reciprocity boundary element method (drbem) based on the three-dimensional poisson-type equation whose right-hand side is radial basis function. the singularity on symmetrical axis of the function f is eliminated by integral averaging and selecting different assemblage of the functions f on the basis of the different characteristic of computational domains. by solving the transient heat conduction problems in solid cylinder and sphere, the laminar convective heat transfer problem in tube and the solidliquid phase change process of sphere, the feasibility of present selecting and assembling function f and its singularity processing are well proved.",
    "present_kp": [
      "dual reciprocity boundary element method",
      "singularity processing",
      "heat transfer"
    ],
    "absent_kp": [
      "approximation function f",
      "axisymmetric problems"
    ]
  },
  {
    "title": "a new coupled fluidstructure modeling methodology for running ductile fracture.",
    "abstract": "a coupled fluidstructure modeling methodology for running ductile fracture in pressurized pipelines has been developed. the pipe material and fracture propagation have been modeled using the finite-element method with a ductile fracture criterion. the finite-volume method has been employed to simulate the fluid flow inside the pipe, and the resulting pressure profile was applied as a load in the finite-element model. choked-flow theory was used for calculating the flow through the pipe crack. a comparison to full-scale tests of running ductile fracture in steel pipelines pressurized with hydrogen and with methane has been done, and very promising results have been obtained.",
    "present_kp": [
      "fluidstructure",
      "fracture",
      "pipeline"
    ],
    "absent_kp": [
      "fem",
      "cfd",
      "leak"
    ]
  },
  {
    "title": "a dempster-shafer theoretic framework for boosting based ensemble design.",
    "abstract": "training set resampling based ensemble design techniques are successfully used to reduce the classification errors of the base classifiers. boosting is one of the techniques used for this purpose where each training set is obtained by drawing samples with replacement from the available training set according to a weighted distribution which is modified for each new classifier to be included in the ensemble. the weighted resampling results in a classifier set, each being accurate in different parts of the input space mainly specified the sample weights. in this study, a dynamic integration of boosting based ensembles is proposed so as to take into account the heterogeneity of the input sets. an evidence-theoretic framework is developed for this purpose so as to take into account the weights and distances of the neighboring training samples in both training and testing boosting based ensembles. the effectiveness of the proposed technique is compared to the adaboost algorithm using three different base classifiers.",
    "present_kp": [
      "boosting"
    ],
    "absent_kp": [
      "evidential pattern classification",
      "classifier ensembles",
      "dynamic classifier combination",
      "sample neighborhood information"
    ]
  },
  {
    "title": "bpeldebugger: an effective bpel-specific fault localization framework.",
    "abstract": "a formal fault localization framework for bpel programs. synthesization of existing fault localization techniques in the proposed framework. an empirical study that validated the feasibility of the proposed framework. significant improvements in fault localization effectiveness are observed.",
    "present_kp": [
      "bpel",
      "fault localization"
    ],
    "absent_kp": [
      "service compositions",
      "fault localization guidelines",
      "integration and interaction faults"
    ]
  },
  {
    "title": "development of a moving artificial compressibility solver on unified coordinates.",
    "abstract": "based on the unified eulerian and lagrangian coordinate transformations, the unsteady incompressible navier-stokes equations with artificial compressibility effects are developed. as we know, the eulerian coordinates cause excessive numerical diffusion across flow discontinuities, slip lines in particular. the lagrangian coordinates, on the other hand, can resolve slip lines sharply but cause severe grid deformation, resulting in large errors and even breakdown of the computation. recently, hui et al. (j. comput. phys. 1999; 153:596-637) have introduced a unified coordinate system that moves with velocity hq, q being the velocity of the fluid particle. it includes the eulerian system as a special case when h = 0, and the lagrangian when h = 1, and was shown for the two-dimensional unsteady euler equations of compressible flow to be superior than both eulerian and lagrangian systems. in the framework of unified coordinates, our work will derive the unsteady incompressible flow equations and moving geometry equations, when hq equals and velocity in conservation form and is updated simultaneous] during each time step. thus, the accurate estimation of geometry conservation and controlling the grid velocity or the h value based on the unified approach can keep numerical stability and avoid computation breakdown caused by moving body or boundary layers (considered as slip lines in lagrangian coordinates). also, the existing high-resolution riemann solver can be extended to discretize the current unified incompressible flow equations. our benchmark tests including the lid-driven cavity flow and backward step flow, oscillating flat plat and pulsating stenotic tube are used to validate the computations. the results verify the accuracy and robustness of the unified artificial compressibility solver on the moving body simulation.",
    "present_kp": [
      "unified coordinates",
      "incompressible flow",
      "moving body simulation"
    ],
    "absent_kp": [
      "geometry conservation law",
      "artificial compressibility fluid-structure interaction"
    ]
  },
  {
    "title": "single-case study in rehabilitation with sam method (sense and mind): a proposal and analysis.",
    "abstract": "the aim of the present case report is to describe a new rehabilitation approach for traumatic brain injury (tbi).",
    "present_kp": [
      "rehabilitation",
      "sam method",
      "tbi"
    ],
    "absent_kp": [
      "embodied cognition",
      "neuropsychology",
      "spatial skills",
      "process-oriented approach"
    ]
  },
  {
    "title": "enhanced and hierarchical structure algorithm for data imbalance problem in semantic extraction under massive video dataset.",
    "abstract": "data imbalance problem often exists in our real life dataset, especial for massive video dataset, however, the balanced data distribution and the same misclassification cost are assumed in traditional machine learning algorithms, thus, it will be difficult for them to accurately describe the true data distribution, and resulting in misclassification. in this paper, the data imbalance problem in semantic extraction under massive video dataset is exploited, and enhanced and hierarchical structure (called ehs) algorithm is proposed. in proposed algorithm, data sampling, filtering and model training are considered and integrated together compactly via hierarchical structure algorithm, thus, the performance of model can be improved step by step, and is robust and stability with the change of features and datasets. experiments on trecvid2010 semantic indexing demonstrate that our proposed algorithm has much more powerful performance than that of traditional machine learning algorithms, and keeps stable and robust when different kinds of features are employed. extended experiments on trecvid2010 surveillance event detection also prove that our ehs algorithm is efficient and effective, and reaches top performance in four of seven events.",
    "present_kp": [
      "data imbalance",
      "enhanced and hierarchical structure ",
      "semantic indexing",
      "surveillance event detection",
      "massive video dataset",
      "trecvid"
    ],
    "absent_kp": []
  },
  {
    "title": "using equivalence-checking to verify robustness to denial of service.",
    "abstract": "in this paper, we introduce a new security property which intends to capture the ability of a cryptographic protocol being resistant to denial of service. this property, called impassivity, is formalised in the framework of a generic value-passing process algebra, called security protocol process algebra, extended with local function calls, cryptographic primitives and special semantics features in order to cope with cryptographic protocols. impassivity is defined as an information flow property founded on bisimulation-based non-deterministic admissible interference. a sound and complete proof method, based on equivalence-checking, for impassivity is also derived. the method extends results presented in a previous paper on admissible interference and its application to the analysis of cryptographic protocols. our equivalence-checking method is illustrated throughout the paper on the tcp/ip connection protocol and on the 1kp secure electronic payment protocol.",
    "present_kp": [
      "equivalence-checking",
      "denial of service",
      "protocols",
      "process algebra",
      "admissible interference",
      "bisimulation"
    ],
    "absent_kp": []
  },
  {
    "title": "an interactive system for computer-aided design of printed circuit boards.",
    "abstract": "this paper describes an interactive system for the design of printed circuit boards. the system uses an interactive graphics terminal for solving the placement and routing problems. it can be used for designing two-layer boards with integrated circuit modules and discrete components. a substantial portion of the wire-routing is performed automatically and the remaining wires are routed manually in a single interactive session. the system provides significant savings in both design time and processing costs over batch processing and manual methods.",
    "present_kp": [
      "computer-aided design"
    ],
    "absent_kp": [
      "printed circuit board design",
      "design automation"
    ]
  },
  {
    "title": "a class of composable and preemptible high-level petri nets with an application to multi-tasking systems.",
    "abstract": "this paper presents an extension of an algebra of high-level petri nets with operations for suspension and abortion. these operations are sound with respect to the semantics of preemption, and can be applied to the modelling of the semantics of high-level parallel programming languages with preemption-related features. as an illustration, the paper gives an application to the modelling of a multi-tasking system in a parallel programming language, which is provided with a concurrent semantics based on petri nets and for which implemented tools can be used.",
    "present_kp": [
      "petri nets",
      "preemption"
    ],
    "absent_kp": [
      "compositionality",
      "tasks"
    ]
  },
  {
    "title": "facial emotion and gesture reproduction method for substitute robot of remote person.",
    "abstract": "ceos of big companies may travel frequently to give their philosophies and policies to the employees who arc working at world wide branches. video technology makes it possible to give their lectures anywhere and anytime in the world very easily. however, 2-dimentional video systems lack the reality. if we can give natural realistic lectures through humanoid robots. ceos do not need to meet the employees in person. they can save their time and money for traveling. we propose a substitute robot of remote person. the substitute robot is a humanoid robot that can reproduce the lecturers' facial expressions and body movements, and that can send the lecturers to everywhere in the world instantaneously with the feeling of being at a live performance. there arc two major tasks for the development; they arc the facial expression recognition/reproduction and the body language reproduction. for the former task, we proposed a facial expression recognition method based oil a neural network model. we recognized five emotions, or surprise, anger, sadness, happiness and no emotion, in real time. we also developed a facial robot to reproduce the recognized emotion on the robot face. through experiments, we showed that the robot could reproduce the speakers' emotions with its face. for the latter task, we proposed it degradation control method to reproduce the natural movement of the lecturer even when a robot rotary joint fails. for the fundamental stage of our research for this sub-system, we proposed a control method for the front view movement model, or 2-dimentional model.",
    "present_kp": [
      "gesture reproduction",
      "substitute robot"
    ],
    "absent_kp": [
      "facial emotion expression"
    ]
  },
  {
    "title": "differential response of nnos knockout mice to mdma (ecstasy)- and methamphetamine-induced psychomotor sensitization and neurotoxicity.",
    "abstract": "it has been shown that mice deficient in neuronal nitric oxide synthase (nnos) gene are resistant to cocaine-induced psychomotor sensitization and methamphetamine (meth)-induced dopaminergic neurotoxicity. the present study was undertaken to investigate the hypothesis that nnos has a major role in dopamine (da)- but not serotonin (5-hydroxytryptamine; 5-ht)-mediated effects of psychostimulants. the response of nnos knockout (ko) and wild-type (wt) mice to the psychomotor-stimulating and neurotoxic effects of 3,4-methylenedioxymethamphetamine (mdma; ecstasy) and meth were investigated. repeated administration of mdma for 5 days resulted in psychomotor sensitization in both wt and nnos ko mice, while repeated administration of meth caused psychomotor sensitization in wt but not in ko mice. sensitization to both mdma and meth was persistent for 40 days in wt mice, but not in nnos ko mice. these findings suggest that the induction of psychomotor sensitization to mdma and meth is no independent and no dependent, respectively, while the persistence of sensitization to both drugs is no dependent. for the neurochemical studies, a high dose of mdma caused marked depletion of 5-ht in several brain regions of both wt and ko mice, suggesting that the absence of the nnos gene did not afford protection against mdma-induced depletion of 5-ht. striatal dopaminergic neurotoxicity caused by high doses of mdma and meth in wt mice was partially prevented in ko mice administered with mdma, but it was fully precluded in ko mice administered with meth. the differential response of nnos ko mice to the behavioral and neurotoxic effects of mdma and meth suggests that the nnos gene is required for the expression and persistence of da-mediated effects of meth and mdma, while 5-ht-mediated effects of mdma (induction of sensitization and 5-ht depletion) are not dependent on nnos.",
    "present_kp": [
      "nitric oxide ",
      "knockout mice",
      "neurotoxicity",
      "dopamine",
      "serotonin"
    ],
    "absent_kp": [
      "locomotor activity"
    ]
  },
  {
    "title": "automatic programming methodologies for electronic hardware fault monitoring.",
    "abstract": "this paper presents three variants of genetic programming (gp) approaches for intelligent online performance monitoring of electronic circuits and systems. reliability modeling of electronic circuits can be best performed by the stressor-susceptibility interaction model. a circuit or a system is considered to be failed once the stressor has exceeded the susceptibility limits. for on-line prediction, validated stressor vectors may be obtained by direct measurements or sensors, which after pre-processing and standardization are fed into the gp models. empirical results are compared with artificial neural networks trained using backpropagation algorithm and classification and regression trees. the performance of the proposed method is evaluated by comparing the experiment results with the actual failure model values. the developed model reveals that gp could play an important role for future fault monitoring systems.",
    "present_kp": [
      "genetic programming",
      "neural networks",
      "fault monitoring",
      "electronic hardware"
    ],
    "absent_kp": [
      "decision trees",
      "computational intelligence"
    ]
  },
  {
    "title": "reproducing kernel hilbert spaces with odd kernels in price prediction.",
    "abstract": "for time series of futures contract prices, the expected price change is modeled conditional on past price changes. the proposed model takes the form of regression in a reproducing kernel hilbert space with the constraint that the regression function must be odd. it is shown how the resulting constrained optimization problem can be reduced to an unconstrained one through appropriate modification of the kernel. in particular, it is shown how odd, even, and other similar kernels emerge naturally as the reproducing kernels of hilbert subspaces induced by respective symmetry constraints. to test the validity and practical usefulness of the oddness assumption, experiments are run with large real-world datasets on four futures contracts, and it is demonstrated that using odd kernels results in a higher predictive accuracy and a reduced tendency to overfit.",
    "present_kp": [
      "odd kernel",
      "price prediction",
      "reproducing kernel hilbert space "
    ],
    "absent_kp": [
      "financial time series",
      "kernel ridge regression "
    ]
  },
  {
    "title": "computational simulation modelling of bioreactor configurations for regenerating human bladder.",
    "abstract": "the objective of this study was to investigate a bioreactor suitable for human bladder regeneration. simulations were performed using the computational fluid dynamic tools. the thickness of the bladder scaffold was 3mm, similar to the human bladder, and overall hold-up volume within the spherical shape scaffold was 755ml. all simulations were performed using (i) brinkman equation on porous regions using the properties of 1% chitosan-1% gelatin structures, (ii) michaelis-menten type rate law nutrient consumption for smooth muscle cells (smcs) and (iii) mackie-meares relationship for determining effective diffusivities. steady state simulations were performed using flow rates from 0.5 to 5ml/min. two different inlet shapes: (i) straight entry at the centre (design 1) and (ii) entry with an expansion (design 2) were simulated to evaluate shear stress distribution. also, mimicking bladder shape of two inlets (design 3) was tested. design 2 provided the uniform shear stress at the inlet and nutrient distribution, which was further investigated for the effect of scaffold locations within the reactor: (i) attached with a 3-mm open channel (design 2-a), (ii) flow through with no open channel (design 2-b) and (iii) porous structure suspended in the middle with 1.5-mm open channel on either side (design 2-c). in design 2-a and 2-c, fluid flow occurred by diffusion dominant mechanisms. furthermore, the designed bioreactor is suitable for increased cell density of smcs. these results showed that increasing the flow rate is necessary due to the decreased permeability at cell densities similar to the human bladder.",
    "present_kp": [
      "smc"
    ],
    "absent_kp": [
      "tissue engineering",
      "cfd"
    ]
  },
  {
    "title": "codes based on bck-algebras.",
    "abstract": "the notion of a bck-valued function on a set is introduced, and related properties are investigated. codes generated by bck-valued functions are established.",
    "present_kp": [],
    "absent_kp": [
      "bck-function",
      "q-cut",
      "cut function"
    ]
  },
  {
    "title": "rule-based management for simulation in agricultural decision support systems.",
    "abstract": "rule-based management systems can offer the farmer or consultant the opportunity to better approximate current or proposed management options, especially as they relate to dynamic conditions in farm fields and across the whole farm. most existing attempts at rule-based management within simulation-oriented agricultural decision support systems (dss) involve limited extension of fixed management dates in response to environmental conditions, or involve rules for implementing limited management events such as fertilizer applications or irrigations. a comprehensive rule-based management system for agricultural dss was developed that allows simulated management events to occur in response to flexible producer-defined rules and to weather and management induced changes in the soilcrop system over time and space. the system provides a simple, english-based rules language, a rules development editor, and software to parse and interpret these rules and provide linkages to application dss software packages such as great plains framework for agricultural resource management (gpfarm). rules can be quickly and easily developed that cover management activities for individual management units (mus) or groups of mus. feedback to the simulation package at each time step provides for generation of site- and time-specific management events across the application. tests of the rule-based system on: wheat (triticum aestivum l.), corn (zea mays l.), fallow and wheatfallow crop rotations used in eastern colorado showed that management events were simulated within the correct time windows and in the proper sequence. dates for simulated events varied as expected across each rotational cycle as a function of temporal conditions. additional work is anticipated to allow dynamic calculation of event attributes, capture and implementation of producer time priorities, and a simplified menu system for the rule editor.",
    "present_kp": [],
    "absent_kp": [
      "whole-farm management",
      "production rules",
      "soilcrop models"
    ]
  },
  {
    "title": "estimation of predictive uncertainties in flood wave propagation in a river channel using adjoint sensitivity analysis.",
    "abstract": "this paper applies adjoint sensitivity analysis to flash flood wave propagation in a river channel. a numerical model, based on the st-venant equations and the corresponding adjoint equations, determines the sensitivities of predicted water levels to uncertainities in key controls such as inflow hydrograph, channel topography, frictional resistance and infiltration rate. sensitivities are calculated in terms of a measuring function that quantifies water levels greater than certain safe threshold levels along the channel. the adjoint model has been verified by means of an identical twin experiment. the method is applied to a simulated flash flood in a river channel. the sensitivities to key controls are evaluated and ranked and the effects of individual and combined uncertainties on the predicted flood impact are also quantified.",
    "present_kp": [
      "adjoint sensitivity analysis",
      "st-venant equations"
    ],
    "absent_kp": [
      "data assimilation",
      "open channel flow",
      "numerical models"
    ]
  },
  {
    "title": "identifying material properties of a dielectric motor.",
    "abstract": "purpose - the purpose of the paper is to define a methodology for identifying the electric conductivity and permittivity of lossy dielectric materials employed in a class of small dielectric motors, knowing the motor torque. reference is made to data measured on an existing prototype. design/methodology/approach - the motor operates because of the interaction between a rotating electric field, generated by a three-phase system of electrodes, and the charge density induced at the surface of the rotor, which lags with respect to the field. the stator and the rotor are hollow cylinders made of dielectric materials. a finite-element model of the motor has been developed. for a given set of material properties, the time-averaged value of starting torque acting on the rotor is evaluated by means of the maxwell stress tensor. findings - a family of curves of starting torque vs conductivity for different values of rotor permittivity are obtained. each curve is well approximated by a lorentz distribution. originality/value - a field model of the motor was exploited to estimate the conductivity that gives rise to a prescribed value of the starting torque. the solution of the underlying inverse problem, which is ill-posed, can give an helpful insight for maximizing the torque.",
    "present_kp": [],
    "absent_kp": [
      "electrically operated devices",
      "numerical analysis",
      "electrical conductivity"
    ]
  },
  {
    "title": "estimation of the principle curvatures of approximated surfaces.",
    "abstract": "this paper presents a method for estimating curvature values of a surface, which is given only approximatively, e.g., by measured data. the presented method requires estimates of the curvature of curves. these lead, along with the theorems from euler and meusnier, to the values of surface curvature. methods are presented, which converge with the different error order of o(h(2)) and o(h(2n)). for the first of them explicit formulae are given. it is proved that the error order remains the same through all further calculations until the final estimation of surface curvature is found.",
    "present_kp": [
      "curvature",
      "surface",
      "estimation"
    ],
    "absent_kp": []
  },
  {
    "title": "performance analysis of an improved soft preemptive scheme for unevenly distributed traffic in optical wavelength-division multiplexing networks.",
    "abstract": "in optical wavelength-division multiplexing (wdm) networks, traffic can be unevenly distributed across the network causing inefficient utilization of resources. to solve this problem, an improved soft preemptive (sp) scheme is proposed by considering dynamic resource distribution to deal with the uneven network utilization. a novel unevenly distributed traffic model in cross-time-zone networks is also presented to evaluate the efficiency of the new scheme. compared with other schemes such as normal shortest path first (spf) routing and wavelength conversion (wc), the new proposed scheme results demonstrate significantly better performance with respect to the network utilization and overall network blocking probability.",
    "present_kp": [
      "unevenly distributed traffic",
      "wavelength conversion"
    ],
    "absent_kp": [
      "optical wdm networks",
      "net work utilization",
      "soft preemption",
      "traffic engineering"
    ]
  },
  {
    "title": "an effective taint-based software vulnerability miner.",
    "abstract": "purpose - the purpose of this paper is to propose an approach to detect indirect memory-corruption exploit (imce) at runtime on binary code, which is often caused by integer conversion error. real-world attacks were evaluated for experimentation. design/methodology/approach - current dynamic analysis detects attacks by enforcing low level policy which can only detect control-flow hijacking attack. the proposed approach detects imce with high level policy enforcement using dynamic taint analysis. unlike low-level policy enforced on instruction level, the authors' policy is imposed on memory operation routine. the authors implemented a fine-grained taint analysis system with accurate taint propagation for detection. findings - conversion errors are common and most of them are legitimate. taint analysis with high-level policy can accurately block imce but have false positives. proper design of data structures to maintain taint tag can greatly improve overhead. originality/value - this paper proposes an approach to block imce with high-level policy enforcement using taint analysis. it has very low false negatives, though still causes certain false positives. the authors made several implementation contributions to strengthen accuracy and performance.",
    "present_kp": [
      "data structures",
      "exploit",
      "taint analysis",
      "policy"
    ],
    "absent_kp": [
      "data security",
      "data management",
      "computer software"
    ]
  },
  {
    "title": "cronoa code for the simulation of chemical weathering.",
    "abstract": "we present a code crono for simulation of chemical weathering. dissolution kinetics is combined with the calculation of thermodynamic equilibrium. this algorithm simulates the regolith forming in time. this algorithm emulates low-temperature systems that do not reach the equilibrium. the application of the code was illustrated by an example of weathering of basalts.",
    "present_kp": [],
    "absent_kp": [
      "kinetics of minerals dissolution",
      "thermodynamic modeling",
      "waterrock systems"
    ]
  },
  {
    "title": "approximating minimum size weakly-connected dominating sets for clustering mobile ad hoc networks.",
    "abstract": "we present a series of approximation algorithms for finding a small weakly-connected dominating set (wcds) in a given graph to be used in clustering mobile ad hoc networks. the structure of a graph can be simplified using wcds's and made more succinct for routing in ad hoc networks. the theoretical performance ratio of these algorithms is o (ln ?) compared to the minimum size wcds, where ? is the maximum degree of the input graph. the first two algorithms are based on the centralized approximation algorithms of guha and khuller cite guha-khuller-1998 for finding small connected dominating sets (cds's). the main contribution of this work is a completely distributed algorithm for finding small wcds's and the performance of this algorithm is shown to be very close to that of the centralized approach. comparisons between our work and some previous work (cds-based) are also given in terms of the size of resultant dominating sets and graph connectivity degradation.",
    "present_kp": [
      "network",
      "weakly-connected dominating set",
      "dominating set",
      "approximation algorithms",
      "structure",
      "input",
      "approximation",
      "size",
      "performance",
      "clustering",
      "graph",
      "routing",
      "algorithm",
      "mobile ad hoc networks",
      "distributed algorithm",
      "ad hoc network"
    ],
    "absent_kp": [
      "centrality",
      "connection"
    ]
  },
  {
    "title": "incorporating partial matches within multiobjective pharmacophore identification.",
    "abstract": "this paper describes the extension of our earlier multiobjective method for generating plausible pharmacophore hypotheses to incorporate partial matches. diverse sets of molecules rarely adopt exactly the same binding mode, and so allowing the identification of partial matches allows our program to be applied to larger and more diverse datasets. the method explores the conformational space of a series of ligands simultaneously with their alignment using a multiobjective genetic algorithm (moga). the principles of pareto ranking are used to evolve a diverse set of pharmacophore hypotheses that are optimised on conformational energy of the ligands, the goodness of the overlay and the volume of the overlay. a partial match is defined as a pharmacophoric feature that is present in at least two, but not all, of the ligands in the set. the number of ligands that map to a given pharmacophore point is taken into account when evaluating an overlay. the method is applied to a number of test cases extracted from the protein data bank (pdb) where the true overlay is known.",
    "present_kp": [
      "pharmacophore",
      "moga",
      "multiobjective genetic algorithm",
      "partial matches"
    ],
    "absent_kp": [
      "molecular alignment",
      "multiobjective optimisation"
    ]
  },
  {
    "title": "dna sequence reconstruction based on genetic algorithm.",
    "abstract": "it is becoming increasingly important to develop a novel process for determining the letters of our genetic code, known as dna sequencing. this task is performed by large datasets using the combination of heuristic methods with little mathematical calculation. in this paper, we present a new method for dna sequence reconstruction using genetic algorithm, which is able to predict the actual dna sequence. the performance of the genetic algorithm is evaluated with respect to previous methods in the literature. the results indicate that the proposed new method is superior to previous methods. finally we compare the results of the experiment and discuss the performance of the proposed method in the dna sequence reconstruction.",
    "present_kp": [
      "dna sequence"
    ],
    "absent_kp": [
      "subsequence",
      "oligonucleotide",
      "population construction",
      "fitness function",
      "crossover",
      "mutation"
    ]
  },
  {
    "title": "self-compensating design for focus variation.",
    "abstract": "process variations have become a bottleneck for predictable and high-yielding ic design and fabrication. linewidth variation (?l) due to defocus in a chip is largely systematic after the layout is completed, i.e., dense lines \"smile\" through focus while isolated (iso) lines \"frown\". in this paper, we propose a design flow that allows explicit compensation of focus variation, either within a cell (self-compensated cells) or across cells in a critical path (self-compensated design). assuming that iso and dense variants are available for each library cell, we achieve designs that are more robust to focus variation. design with a self-compensated cell library incurs ~11-12% area penalty while compensating for focus variation. across-cell optimization with a mix of dense and iso cell variants incurs ~6-8% area overhead compared to the original cell library, while meeting timing constraints across a large range of focus variation (from 0 to 0.4um). a combination of original and iso cells provides an even better self-compensating design option, with only 1% area overhead. circuit delay distributions are tighter with self-compensated cells and self-compensated design than with a conventional design methodology.",
    "present_kp": [
      "focus",
      "compensation",
      "layout",
      "variation"
    ],
    "absent_kp": [
      "aclv",
      "manufacturability"
    ]
  },
  {
    "title": "a software-only solution to use scratch pads for stack data.",
    "abstract": "a dynamic scratch pad memory (spm) management scheme for program stack data with the objective of processor power reduction is presented. basic technique does not need the spm size at compile time, does not mandate any hardware changes, does not need profile information, and seamlessly integrates support for recursive functions. stack frames are managed using a software spm manager, integrated into the application binary, and shows average energy savings of 32% along with a performance improvement of 13%, on benchmarks from mibench. spm management can be further optimized and made pointer safe, by knowing the spm size.",
    "present_kp": [
      "scratch pad memory "
    ],
    "absent_kp": [
      "cache",
      "compilers",
      "embedded systems",
      "static analysis"
    ]
  },
  {
    "title": "a new algorithm for fixed point quantum search.",
    "abstract": "the standard quantum search lacks a feature, enjoyed by many classical algorithms, of having a fixed point, i.e. monotonic convergence towards the solution. recently a fixed point quantum search algorithm has been discovered, referred to as the phase-pi/3 search algorithm, which gets around this limitation. while searching a database for a target state, this algorithm reduces the error probability from epsilon to epsilon(2q+1) using q oracle queries, which has since been proved to be asymptotically optimal. a different algorithm is presented here, which has the same worst-case behavior as the phase-pi/3 search algorithm but much better average-case behavior. furthermore the new algorithm gives epsilon(2q+1) convergence for all integral q, whereas the phase-pi/3 search algorithm requires q to be (3(n) - 1)/2 with n a positive integer. in the new algorithm, the operations are controlled by two ancilla qubits, and fixed point behavior is achieved by irreversible measurement operations applied to these ancillas. it is an example of how measurement can allow us to bypass some restrictions imposed by unitarity on quantum computing.",
    "present_kp": [
      "ancilla",
      "fixed point",
      "measurement",
      "quantum search algorithm"
    ],
    "absent_kp": [
      "limit cycle"
    ]
  },
  {
    "title": "test enrichment for path delay faults using multiple sets of target faults.",
    "abstract": "test sets for path delay faults in circuits with large numbers of paths are typically generated for path delay faults associated with the longest circuit paths. it is shown that such test sets may not detect faults associated with the next-to-longest paths. this may lead to undetected failures since shorter paths may fail without any of the longest paths failing. in addition, paths that appear to be shorter may actually be longer than the longest paths if the procedure used for estimating path length is inaccurate. a test enrichment procedure is proposed that increases significantly the number of faults associated with the next-to-longest paths that are detected by a test set without increasing its size. this is achieved by targeting both types of faults, but ensuring that the test generation procedure would detect the faults associated with the longest paths, while allowing the procedure the flexibility of detecting or not detecting the faults associated with the next-to-longest paths. the proposed procedure thus improves the quality of the test set without increasing its size. the test enrichment procedure is built on top of a new and effective dynamic test compaction, procedure in order to demonstrate that test enrichment is effective even for compact test sets.",
    "present_kp": [
      "dynamic test compaction",
      "path delay faults",
      "test generation"
    ],
    "absent_kp": []
  },
  {
    "title": "some problems of computer-aided testing and \"interview-like tests\".",
    "abstract": "computer-based testing - is an effective teacher's tool, intended to optimize course goals and assessment techniques in a comparatively short time. however, this is accomplished only if we deal with high-quality tests. it is strange, but despite the 100-year history of testing theory (see, anastasi, a., urbina, s. (1997). psychological testing. upper saddle river, nj: prentice-hall) there still exist some misconceptions. modern wide-spread systems for computer based course management and testing reveal a set of problems corresponding to certain features of testing methods. this article is devoted to some omissions typical to several course management systems (e.g., moodle and blackboard). these omissions and the ways of avoiding them are shown in a simple test intended to verify student knowledge. we suggest a special test description language dedicated to drawing your attention to the mathematical aspects of test quality. the language can also be realized in computer software. we provide an example of such software in this article.",
    "present_kp": [
      "computer-based testing",
      "course management systems",
      "moodle",
      "blackboard",
      "test description language"
    ],
    "absent_kp": [
      "teaching techniques",
      "chopin",
      "test validity",
      "economy",
      "reliability",
      "difficulty of a test",
      "guessing probability",
      "measures of similarity",
      "inseparable hypotheses"
    ]
  },
  {
    "title": "numerical simulation of viscous flows with free surface around realistic hull forms with transom.",
    "abstract": "this paper describes a method for simulation of viscous flows with a free surface around realistic hull forms with a transom, which has been developed based on a finflo rans solver with a moving mesh. a dry-transom model is proposed and implemented for the treatment of flows off the transom. the bulk rans flow with the artificial compressibility is solved by a cell-centred finite volume multigrid scheme and the free surface deformed by wave motions is tracked by satisfying the kinematic and dynamic free-surface boundary conditions on the actual location of the surface. the effects of turbulence on flows are evaluated with the baldwin-lomax turbulence model without a wall function. a test case is modern container ship model with a transom, the hamburg test case. the calculated results are validated and they agree well with the measured results in terms of the free-surface waves and the total resistance coefficient. furthermore. the numerical solutions successfully captured many important features of the complicated interaction of the free surface with viscous flows around transom stern ships. in addition, the convergence performance and the grid refinement studies are also investigated.",
    "present_kp": [
      "dry-transom model",
      "finflo rans solver",
      "moving mesh"
    ],
    "absent_kp": [
      "modern hull forms with a transom",
      "viscous free surface flows"
    ]
  },
  {
    "title": "an analysis of language-level support for self-adaptive software.",
    "abstract": "self-adaptive software has become increasingly important to address the new challenges of complex computing systems. to achieve adaptation, software must be designed and implemented by following suitable criteria, methods, and strategies. past research has been mostly addressing adaptation by developing solutions at the software architecture level. this work, instead, focuses on finer-grain programming language-level solutions. we analyze three main linguistic approaches: metaprogramming, aspect-oriented programming, and context-oriented programming. the first two are general-purpose linguistic mechanisms, whereas the third is a specific and focused approach developed to support context-aware applications. this paradigm provides specialized language-level abstractions to implement dynamic adaptation and modularize behavioral variations in adaptive systems. the article shows how the three approaches can support the implementation of adaptive systems and compares the pros and cons offered by each solution.",
    "present_kp": [
      "design",
      "context",
      "self-adaptive software",
      "context-oriented programming"
    ],
    "absent_kp": [
      "languages",
      "autonomic computing"
    ]
  },
  {
    "title": "a lattice-based approach for updating access control policies in real-time.",
    "abstract": "real-time update of access control policies, that is, updating policies while they are in effect and enforcing the changes immediately and automatically, is necessary for many dynamic environments. examples of such environments include disaster relief and war zone. in such situations, system resources may need re-configuration or operational modes may change, necessitating a change of policies. for the system to continue functioning, the policies must be changed immediately and the modified policies automatically enforced. in this paper, we propose a solution to this problemwe consider real-time update of access control policies in the context of a database system. in our model, a database consists of a set of objects that are read and updated through transactions. access to the data objects are controlled by access control policies which are stored in the form of policy objects. we consider an environment in which different kinds of transactions execute concurrently; some of these may be transactions updating policy objects. updating policy objects while they are deployed can lead to potential security problems. we propose algorithms that not only prevent such security problems, but also ensure serializable execution of transactions. the algorithms differ on the degree of concurrency provided and the kinds of policies each can update.",
    "present_kp": [],
    "absent_kp": [
      "security policies",
      "concurrency control",
      "transaction management"
    ]
  },
  {
    "title": "one case, four theories.",
    "abstract": "in this chapter, clinical material illustrates key theoretical concepts and underscores the value of heinz kohut's radical approach to psychoanalysis. the psychodynamic treatment of matthew spans over a decade and traces the therapist's immersion in four clinical modalities. the transition from kleinian and british object relations orientations to a therapeutic style informed by psychoanalytic self psychology and intersubjective systems theory broke through impasses generated in the earlier chapters of matthew's therapy. the empathic listening stance, the impact of the analyst's subjectivity on the treatment's progress, and the vital role of selfobject experiences in the development, restoration, and maintenance of an individual's sense of self constitute a few of the crucial and enduring curative elements brought to the field of psychoanalysis and psychodynamic therapy by kohut's pioneering efforts.",
    "present_kp": [
      "self psychology",
      "intersubjective systems theory",
      "impasse",
      "sense of self",
      "kohut"
    ],
    "absent_kp": [
      "empathy",
      "comparative psychoanalysis",
      "organizing principles",
      "self-cohesion"
    ]
  },
  {
    "title": "4-sets-cut-decoding algorithms for random codes based on quasigroups.",
    "abstract": "the decoding speed is the biggest problem of random codes based on quasigroups proposed elsewhere. these codes are a combination of cryptographic algorithms and error-correcting codes. in our previous paper we proposed cut-decoding algorithm which is 4.5 times faster than the original one for code (72,288). in this paper, four new modifications (so-called 4-sets-cut-decoding algorithms) of this algorithm are proposed in order to obtain an improvement of the decoding speed. we present and analyze several experimental results obtained with different algorithms for random codes based on quasigroups. it is shown that using new algorithms, improvement of the efficiency and decoding speed is obtained. also, we derive the upper bound for packet-error probability obtained with cut-decoding and 4-sets-cut-decoding algorithm. at the end, some methods for reducing the number of unsuccessful decodings in the new proposed algorithms are considered.",
    "present_kp": [
      "random code",
      "error-correcting code",
      "packet-error probability",
      "quasigroup"
    ],
    "absent_kp": [
      "crypt-coding",
      "bit-error probability"
    ]
  },
  {
    "title": "a robust method for handling low density regions in hybrid simulations for collisionless plasmas.",
    "abstract": "a robust method to handle vacuum and near vacuum regions in hybrid simulations for space and astrophysical plasmas is presented. the conventional hybrid simulation model dealing with kinetic ions and a massless charge-neutralizing electron fluid is known to be susceptible to numerical instability due to divergence of the whistler-mode wave dispersion, as well as division-by-density operation in regions of low density. consequently, a pure vacuum region is not allowed to exist in the simulation domain unless some ad hoc technique is used. to resolve this difficulty, an alternative way to introduce finite electron inertia effect is proposed. contrary to the conventional method, the proposed one introduces a correction to the electric field rather than the magnetic field. it is shown that the generalized ohm's law correctly reduces to laplace's equation in a vacuum which therefore does not involve any numerical problems. in addition, a variable ion-to-electron mass ratio is introduced to reduce the phase velocity of high frequency whistler waves at low density regions so that the stability condition is always satisfied. it is demonstrated that the proposed model is able to handle near vacuum regions generated as a result of nonlinear self-consistent development of the system, as well as pure vacuum regions set up at the initial condition, without losing the advantages of the standard hybrid code.",
    "present_kp": [
      "collisionless plasma",
      "hybrid simulation"
    ],
    "absent_kp": [
      "kinetic simulation"
    ]
  },
  {
    "title": "emergent cooperative goal-satisfaction in large-scale automated-agent systems.",
    "abstract": "cooperation among autonomous agents has been discussed in the dai community for several years. papers about cooperation (conte et al., 1991; rosenschein, 1986), negotiation (kraus and wilkenfeld, 1991), distributed planning (conry et al., 1988), and coalition formation (ketchpel, 1994; sandholm and lesser, 1997), have provided a variety of approaches and several algorithms and solutions to situations wherein cooperation is possible. however, the case of cooperation in large-scale multi-agent systems (mas) has not been thoroughly examined. therefore, in this paper we present a framework for cooperative goal-satisfaction in large-scale environments focusing on a low-complexity physics-oriented approach. the multi-agent systems with which we deal are modeled by a physics-oriented model. according to the model, mas inherit physical properties, and therefore the evolution of the computational systems is similar to the evolution of physical systems. to enable implementation of the model, we provide a detailed algorithm to be used by a single agent within the system. the model and the algorithm are appropriate for large-scale, dynamic, distributed problem solver systems, in which agents try to increase the benefits of the whole system. the complexity is very low, and in some specific cases it is proved to be optimal. the analysis and assessment of the algorithm are performed via the well-known behavior and properties of the modeling physical system.",
    "present_kp": [
      "multi-agent systems"
    ],
    "absent_kp": [
      "task allocation"
    ]
  },
  {
    "title": "visualizing image collections using high-entropy layout distributions.",
    "abstract": "mechanisms for visualizing image collections are essential for browsing and exploring their content. this is especially true when metadata are ineffective in retrieving items due to the sparsity or esoteric nature of text. an obvious approach is to automatically lay out sets of images in ways that reflect relationships between the items. however, dimensionality reduction methods that map from high-dimensional content-based feature distributions to low-dimensional layout spaces for visualization often result in displays in which many items are occluded whilst large regions are empty or only sparsely populated. furthermore, such methods do not consider the shape of the region of layout space to be populated. this paper proposes a method, high-entropy layout distributions. that addresses these limitations. layout distributions with low differential entropy are penalized. an optimization strategy is presented that finds layouts that have high differential entropy and that reflect inter-image similarities. efficient optimization is obtained using a step-size constraint and an approximation to quadratic (renyi) entropy. two image archives of cultural and commercial importance are used to illustrate and evaluate the method. a comparison with related methods demonstrates its effectiveness.",
    "present_kp": [],
    "absent_kp": [
      "content-based browsing",
      "high-entropy layout distribution ",
      "image layouts",
      "manifold learning",
      "renyi entropy"
    ]
  },
  {
    "title": "hard problems in similarity searching.",
    "abstract": "the closest substring problem is one of the most important problems in the field of computational biology. it is stated as follows: given a set of t sequences s1,s2,,st over an alphabet ?, and two integers k,d with d?k, can one find a string s of length k and, for all i=1,2,,t, substrings oi of si, all of length k, such that d(s,oi)?d (for all i=1,2,,t)? (here, d(.,.) represents the hamming distance). closest substring was shown to be np-hard and w-hard with respect to the number t of input sequences; recently, an important number of results concerning the parameterized computational complexity of closest substring has been added in evans et al.. in this paper we introduce and analyze two variants of the closest substring problem, obtained by imposing restrictions on the pairwise distances between the substrings oi: the bounded hamming distance constraint asks that d(oi,oj)?p, for all i,j?{1,2,,t} (where p<2d is a given constant) and yields the problem called bccs; the sum-of-pairs constraint asks that ?1?i&lt;j?td(oi,oj)?p ? 1 ? i &lt; j ? t d ( o i , o j ) ? p (where p<dt(t?1) is a given constant) and yields the problem called sccs.",
    "present_kp": [],
    "absent_kp": [
      "np-complexity",
      "parameterized complexity"
    ]
  },
  {
    "title": "a scalable farm skeleton for hybrid parallel and distributed programming.",
    "abstract": "multi-core processors and clusters of multi-core processors are ubiquitous. they provide scalable performance yet introducing complex and low-level programming models for shared and distributed memory programming. thus, fully exploiting the potential of shared and distributed memory parallelization can be a tedious and error-prone task: programmers must take care of low-level threading and communication (e.g. message passing) details. in order to assist programmers in developing performant and reliable parallel applications algorithmic skeletons have been proposed. they encapsulate well-defined, frequently recurring parallel and distributed programming patterns, thus shielding programmers from low-level aspects of parallel and distributed programming. in this paper we take on the design and implementation of the well-known farm skeleton. in order to address the hybrid architecture of multi-core clusters we present a two-tier implementation built on top of mpi and openmp. on the basis of three benchmark applications, including a simple ray tracer, an interacting particles system, and an application for calculating the mandelbrot set, we illustrate the advantages of both skeletal programming in general and this two-tier approach in particular.",
    "present_kp": [
      "algorithmic skeletons",
      "farm skeleton"
    ],
    "absent_kp": [
      "high-level parallel programming",
      "shared/distributed memory"
    ]
  },
  {
    "title": "patterns from the sky - satellite image analysis using pulse coupled neural networks for pre-processing, segmentation and edge detection.",
    "abstract": "in this work we attempt to distinguish land from water in satellite images, specifically images taken by the forte satellite. first, we successfully approximate areas hidden by stationary artefacts in the image. we then segment regions of land from water. finally, we determine the boundaries of the surrounding landmasses.",
    "present_kp": [
      "satellite images",
      "segmentation",
      "artefacts",
      "forte"
    ],
    "absent_kp": [
      "textures",
      "pcnn"
    ]
  },
  {
    "title": "dynamic influent pollutant disturbance scenario generation using a phenomenological modelling approach.",
    "abstract": "activated sludge models are widely used for simulation-based evaluation of wastewater treatment plant (wwtp) performance. however, due to the high workload and cost of a measuring campaign on a full-scale wwtp, many simulation studies suffer from lack of sufficiently long influent flow rate and concentration time series representing realistic wastewater influent dynamics. in this paper, a simple phenomenological modelling approach is proposed as an alternative to generate dynamic influent pollutant disturbance scenarios. the presented set of models is constructed following the principles of parsimony (limiting the number of parameters as much as possible), transparency (using parameters with physical meaning where possible) and flexibility (easily extendable to other applications where long dynamic influent time series are needed). the proposed approach is sub-divided in four main model blocks: 1) model block for flow rate generation, 2) model block for pollutants generation (carbon, nitrogen and phosphorus), 3) model block for temperature generation and 4) model block for transport of water and pollutants. the paper is illustrated with the results obtained during the development of the dynamic influent of the benchmark simulation model no. 2 (bsm2). the series of simulations show that it is possible to generate a dry weather influent describing diurnal flow rate dynamics (low rate at night, high rate during day time), weekend effects (with different flow rate during weekends, compared to weekdays), holiday effects (where the wastewater production is assumed to be different for a number of weeks) and seasonal effects (with variations in the infiltration and thus also the flow rate to the wwtp). in addition, the dry weather model can be extended with a rain and storm weather generator, where the proposed phenomenological model can also mimic the first flush effect from the sewer network and the influent dilution phenomena that are typically observed at full-scale wwtps following a rain event. finally, the extension of the sewersystem can be incorporated in the influent dynamics as well: the larger the simulated sewer network, the smoother the simulated diurnal flow rate and concentration variations. in the discussion, it is pointed out how the proposed phenomenological models can be expanded to other applications, for example to represent heavy metal or organic micro-pollutant loads entering the treatment plant.",
    "present_kp": [
      "wastewater treatment plant",
      "bsm2"
    ],
    "absent_kp": [
      "dynamic disturbances",
      "influent modelling",
      "urban drainage"
    ]
  },
  {
    "title": "follow the money - assessing the allocation of e-rate funds.",
    "abstract": "expanding the principles of the universal provision of telecommunications services to encompass elementary and secondary schools and libraries, lawmakers paved the way for the creation of the e-rate program by passing the telecommunications act in 1996. the e-rate program aims to bridge the digital divide in advanced telecommunications by offering services to eligible schools and libraries for educational purposes at discounted rates. currently in its sixth cycle, the e-rate program has provided nearly us$13 billion in discounts to eligible organizations. the program has inspired much controversy and criticism, yet studies provide evidence that the program is effectively helping to enhance the provision of services to traditionally disadvantaged communities. this study challenges these claims and presents empirical evidence that suggests e-rate resources are not systematically allocated to underprivileged, rural states where funds would most effectively help to bridge the digital gap in access to telecommunications technologies.",
    "present_kp": [
      "e-rate",
      "telecommunications technologies",
      "digital divide"
    ],
    "absent_kp": [
      "universal access",
      "resource allocation",
      "telecommunications act of 1996"
    ]
  },
  {
    "title": "measurements and simulations of the heat transfer on end windings of an induction machine.",
    "abstract": "purpose - for an accurate simulation of the temperature distribution inside an electrical machine a method for deriving the convective heat transfer coefficient numerically would be desirable. the purpose of this paper is to present a reliable simulation setup, which is able to reproduce the measured, convective heat transfer coefficient at certain spots on the end windings of an electric machine. design/methodology/approach - the heat flux density on certain spots on the end windings of an induction motor have been measured with heat flux sensors, in order to find out the convective heat transfer coefficient. to identify the air mass flow inside a cooling duct of an encapsulated cooling circuit during the operation of the motor, the pressure loss inside the duct has been measured. the measured data for temperature and air mass flow have been used as boundary conditions for the identification of the convective heat transfer coefficient with a commercial software for computational fluid dynamics (cfd). findings - the measured data for the local convective heat transfer coefficients have been compared to the results of the numerical simulation for various rotational velocities. the quality of the simulated convective heat transfer coefficient depending on the rotational velocity meets the measured values. owing to the used simplified model, the quantity of the measured values differ strongly around the simulated coefficient for the convective heat transfer. originality/value - the derivation of the convective heat transfer is a challenging subject in cfd but has become more reliable with the invention of the sst and the sas-sst turbulence model. in the present work, measurements on the end windings have been compared to simulation results derived with the sas-sst turbulence model.",
    "present_kp": [
      "cooling",
      "fluid dynamics",
      "simulation",
      "heat transfer"
    ],
    "absent_kp": [
      "electric machines",
      "thermal analysis",
      "thermal variables measurement",
      "traction motors"
    ]
  },
  {
    "title": "local redundant polymorphism query elimination.",
    "abstract": "dynamic polymorphism is a powerful yet costly feature of object-oriented programming languages. to improve performance, several techniques have been developed to simplify or remove polymorphism operations when they are not needed. however, these techniques have little effect on sites that are actively polymorphic. in this paper we present an alternate approach to the optimization of operations such as virtual dispatches and type tests. rather than attempt to eliminate the polymorphism mechanism, we identify situations where resolved type and method information can be shared across multiple polymorphism operations. in short, we perform a partial redundancy elimination transform over the loads and tests that constitute polymorphism queries. we describe the realization of our technique in the jikes rvm and present its effects on the dacapo and specjvm98 benchmarks. rigorous measurements show a mean improvement, including several statistically significant results, over a configuration with no dispatch optimization and over one that employs guarded inlining. we underscore the potential of our approach by demonstrating speedups of up to 14% on a highly polymorphic benchmark. our results show the approach is both successful and complementary to techniques that focus on degenerate polymorphism.",
    "present_kp": [
      "virtual dispatch",
      "redundancy elimination"
    ],
    "absent_kp": [
      "java"
    ]
  },
  {
    "title": "acm international workshop on interactive multimedia on mobile and portable devices (immpd'11).",
    "abstract": "with the mobile and portable devices become ubiquitous for people's daily life, how to design user interfaces of these products that enable natural, intuitive and fun interaction is one of the main challenges the multimedia community is facing. following several successful events, the acm international workshop on interactive multimedia on mobile and portable devices (immpd'11) aims to bring together researchers from both academia and industry in domains including computer vision, audio and speech processing, machine learning, pattern recognition, communications, human-computer interaction, and media technology to share and discuss recent advances in interactive multimedia.",
    "present_kp": [
      "human-computer interaction",
      "multimedia",
      "computer vision"
    ],
    "absent_kp": [
      "consumer electronics",
      "pattern recognition."
    ]
  },
  {
    "title": "decision-making under time pressure with different information sources and performance-based financial incentives - part 2.",
    "abstract": "in part 2, we examine the viability of the new symbolic language that we described in part 1, in a specific setting. using an abstract classification task that involves decision-making under time pressure, we study multiple measures of subject performance at this task using the new language vis-a-vis written and spoken english. initial experimental results suggest that, despite its relative novelty, the proposed language is at least as effective as the more traditional communication modes in the specific setting examined, while succinctly conveying what must be conveyed.",
    "present_kp": [
      "decision-making",
      "time pressure",
      "symbolic language"
    ],
    "absent_kp": [
      "multimedia systems",
      "mobile computing",
      "ex-ante dss evaluation",
      "induced value theory"
    ]
  },
  {
    "title": "air gap formation by uv-assisted decomposition of cvd material.",
    "abstract": "a sacrificial material deposited by cvd is used to demonstrate air gap formation in single damascene structures by uv-assisted decomposition. the material is removed through a porous low-k cap, after completion of the damascene scheme. the porosity of the low-k cap is shown to be critical for efficient air gap formation. capacitance reduction of ?50% is demonstrated using this technique compared to conventional sioc(h) interconnects and an effective dielectric constant of 1.7 is extrapolated.",
    "present_kp": [
      "low-k",
      "sacrificial material",
      "capacitance reduction",
      "cvd",
      "decomposition"
    ],
    "absent_kp": [
      "air gaps"
    ]
  },
  {
    "title": "recovering distributed objects.",
    "abstract": "distributed multithreaded applications operating in shared-nothing environments present challenges to classical fault tolerance mechanisms. the piecewise determinism assumption is lost (due to multithreading), and data must be replicated (because of the shared-nothing environment). in this paper, we explore a systematic approach to providing fault tolerance, by considering data-race-free programs that have the benefits of piecewise determinism and yet allow multithreading. we base our logging and recovery algorithm on a logical ring structure that allows the underlying distributed system to migrate threads, migrate and replicate objects, and perform multi-object transactions.",
    "present_kp": [
      "distributed objects",
      "fault tolerance",
      "logging"
    ],
    "absent_kp": [
      "distributed systems"
    ]
  },
  {
    "title": "bio and health informatics meets cloud : biovlab as an example.",
    "abstract": "the exponential increase of genomic data brought by the advent of the next or the third generation sequencing (ngs) technologies and the dramatic drop in sequencing cost have driven biological and medical sciences to data-driven sciences. this revolutionary paradigm shift comes with challenges in terms of data transfer, storage, computation, and analysis of big bio/medical data. cloud computing is a service model sharing a pool of configurable resources, which is a suitable workbench to address these challenges. from the medical or biological perspective, providing computing power and storage is the most attractive feature of cloud computing in handling the ever increasing biological data. as data increases in size, many research organizations start to experience the lack of computing power, which becomes a major hurdle in achieving research goals. in this paper, we review the features of publically available bio and health cloud systems in terms of graphical user interface, external data integration, security and extensibility of features. we then discuss about issues and limitations of current cloud systems and conclude with suggestion of a biological cloud environment concept, which can be defined as a total workbench environment assembling computational tools and databases for analyzing bio/medical big data in particular application domains.",
    "present_kp": [
      "cloud computing",
      "big data",
      "user interface",
      "data integration",
      "analysis",
      "security"
    ],
    "absent_kp": [
      "bioinformatics",
      "workflow"
    ]
  },
  {
    "title": "correlation and the time interval in multiple regression models.",
    "abstract": "in this paper we investigate the time interval effect of multiple regression models in which some of the variables are additive and some are multiplicative. the effect on the partial regression and correlation coefficients is influenced by the selected time interval. we find that the partial regression and correlation coefficients between two additive variables approach one-period values as n increases. when one of the variables is multiplicative, they will approach zero in the limit. we also show that the decreasing speed of the n-period correlation coefficients between both multiplicative variables is faster than others, except that a one-period correlation has a higher positive value. the results of this paper can be widely applied in various fields where regression or correlation analyses are employed.",
    "present_kp": [
      "correlation coefficient",
      "time interval"
    ],
    "absent_kp": [
      "partial regression coefficient"
    ]
  },
  {
    "title": "linear time-dependent constraints programming with msvl.",
    "abstract": "this paper investigates specifying and solving linear time-dependent constraints with an interval temporal logic programming language msvl. to this end, linear constraint statements involving linear equality and non-strict inequality are first defined. further, the time-dependent relations in the constraints are specified by temporal operators, such as . thus, linear time-dependent constraints can be smoothly incorporated into msvl. moreover, to solve the linear constraints within msvl by means of reduction, the operational semantics for linear constraints is given. in particular, semantic equivalence rules and transition rules within a state are presented, which enable us to reduce linear equations, inequalities and optimization problems in a convenient way. besides, the operational semantics is proved to be sound. finally, a production scheduling application is provided to illustrate how our approach works in practice.",
    "present_kp": [
      "linear time-dependent constraints",
      "temporal logic",
      "msvl",
      "operational semantics"
    ],
    "absent_kp": []
  },
  {
    "title": "from business intelligence to semantic data stream management.",
    "abstract": "evolution of business intelligence with emergence of big data technologies. new technologies and approaches the 3vs (volume, velocity and variety) of big data. stream reasoning over big data. summarizing data streams (semantic and classic data). semantic data matching in stream context.",
    "present_kp": [
      "data stream",
      "business intelligence",
      "stream reasoning"
    ],
    "absent_kp": [
      "linked data"
    ]
  },
  {
    "title": "a roadmap towards sustainable self-aware service systems.",
    "abstract": "self-awareness and self-adaptation have become primary concerns in large-scale systems as they have become too complex to be managed by human administrators alone, but rather require a new blend of coordination mechanisms between people and software services. this paper presents a roadmap to effective and efficient system adaptation through coupling self-awareness of global-level goals with sustainability constraints. sustainability of large-scale systems challenges self-adaptation approaches by its intrinsic characters of global and long-lasting effects. we introduce five levels of awareness: (i) event-awareness, (ii) situation-awareness, (iii) adaptability awareness, (iv) goal-awareness, and (v) future-awareness. within each level we introduce applicable principles and subsequently outline necessary models, algorithms, and protocols. the approach puts special focus on the interdependencies of human and service elements.",
    "present_kp": [
      "sustainability",
      "roadmap",
      "self-awareness",
      "service system"
    ],
    "absent_kp": [
      "adaptation coordination",
      "context patterns"
    ]
  },
  {
    "title": "a method for recognizing overlapping elliptical bubbles in bubble image.",
    "abstract": "direct imaging technology is an effective and convenient method for the estimation of bubble size distribution (bsd). however, overlapping bubble has an influence on bsd when gas holdup is more than 1%. in this paper, we present a new method of overlapping elliptical bubble recognition to determine bubble size. the method mainly includes two steps: contour segmentation and segment grouping. contour segmentation is on the assumption that the concave points in the dominant point sequence are always the connecting points, and segment grouping is mainly based on the average distance deviation criterion. both simulated images and real bubble images are used to evaluate this new method. the results show that it is effective in the recognition of overlapping elliptical bubbles and have a potential in other elliptical object recognition. in the last, two methods are used for bsd estimation. it is found that the bubble size (such d10 or d32) estimated by the ignore method is slightly smaller than that estimated by the recognition method.",
    "present_kp": [
      "direct imaging",
      "bubble recognition",
      "overlapping",
      "bubble size distribution"
    ],
    "absent_kp": []
  },
  {
    "title": "calculation by artificial compressibility method and virtual flux method on gpu.",
    "abstract": "in this study, artificial compressibility method and virtual flux method were implemented on gpus. because gpus are recognized as massively parallel computers, dp-lur was employed as time integration method. in spite of slow convergence characteristics of dp-lur, calculation by the coupling of dp-lur and gpu is about 15 times faster in time than that of lu-sgs and single cpu. virtual flux method, which enables to calculate flow around curved surface on the cartesian grid is also implemented to artificial compressibility method on gpu program, which was implemented in this study. gpu code of virtual flux method is about 8 times faster than cpu code.",
    "present_kp": [
      "virtual flux method",
      "the cartesian grid"
    ],
    "absent_kp": [
      "cuda"
    ]
  },
  {
    "title": "a synthetic view of belief revision with uncertain inputs in the framework of possibility theory.",
    "abstract": "this paper discusses belief revision under uncertain inputs in the framework of possibility theory. this framework is flexible enough to allow for numerical and ordinal revision procedures. it is emphasized that revision under uncertain inputs can be understood in two different ways, depending on whether the input is viewed as a constraint to be enforced, or as an unreliable piece of information. two revision rules are proposed to implement these forms of revision. ir is shown that m. a. williams's transmutations, originally defined in the setting of spohn's functions, can be captured in possibility theory, as well as boutilier's natural revision. the use of conditioning greatly simplifies the description of these belief change operations. lastly, preliminary results on implementing revision rules at the syntactic level are given.",
    "present_kp": [
      "belief revision",
      "possibility theory"
    ],
    "absent_kp": [
      "possibilistic logic",
      "adjustment"
    ]
  },
  {
    "title": "a regional approach to foot and ankle mri.",
    "abstract": "this chapter presents a regional anatomic approach to mri applications in the foot and ankle. from a clinical perspective, patients often describe their symptoms in terms of the part of the foot that hurts and when and how it hurts. clinical questioning and physical diagnosis pursue this line as well, trying to narrow down the diagnostic possibilities. there are conditions that may blur the anatomic distinctions for forefoot, midfoot, rearfoot, and ankle; involve more than one region of the foot simultaneously; or occur in any area of the foot. the chapter also includes a separate section on the presentations of inflammatory arthritides in foot and ankle joints.",
    "present_kp": [
      "foot",
      "ankle"
    ],
    "absent_kp": [
      "metatarsalgia",
      "heel pain",
      "tendon injuries",
      "plantar plate",
      "fracture"
    ]
  },
  {
    "title": "profiling the non-users: examination of life-position indicators, sensation seeking, shyness, and loneliness among users and non-users of social network sites.",
    "abstract": "the aim of the current study is to explore if there are differences between users and non-users of social network sites in terms of their sensation seeking, life-position indicators, shyness, and loneliness. using data from a survey of adults 1976years old, results revealed that compared to an average facebook user, a non-user is significantly older and scores higher on shyness and loneliness, is less socially active, and less prone to sensation seeking activities. facebook is not a substitute channel of communication for those who are shy and lonely and lack face-to-face interactions. this study extends our knowledge of digital divide, uses and gratifications theory, and social enhancement hypothesis.",
    "present_kp": [
      "facebook",
      "digital divide",
      "sensation seeking"
    ],
    "absent_kp": [
      "life satisfaction"
    ]
  },
  {
    "title": "computing with coupled chaotic neuronal maps.",
    "abstract": "chaos computing is a new paradigm of an unconventional computing that exploits the extreme non linearity of chaotic systems. we propose a stragey for chaos based computation in one-way coupled chaotic neuronal maps. in the drive-response unit, either the output state of the response system or the synchronization error between drive response systems are used to obtain basic logic operation.",
    "present_kp": [
      "chaos computing"
    ],
    "absent_kp": [
      "chaotic dynamics",
      "control and synchronization"
    ]
  },
  {
    "title": "robust f0 estimation based on complex lpc analysis for irs filtered noisy speech.",
    "abstract": "this paper proposes a novel robust fundamental frequency (f0) estimation algorithm based on complex-valued speech analysis for an analytic speech signal. since analytic signal provides spectra only over positive frequencies, spectra can be accurately estimated in low frequencies. consequently, it is considered that f0 estimation using the residual signal extracted by complex valued speech analysis can perform better for f0 estimation than that for the residual signal extracted by conventional real-valued lpc analysis. in this paper, the autocorrelation function weighted by amdf is adopted for the f0 estimation criterion and four signals; speech signal, analytic speech signal, lpc residual and complex lpc residual, are evaluated for the f0 estimation. speech signals used in the experiments were an irs filtered speech corrupted by adding white gaussian noise or pink noise whose noise levels are 10, 5, 0, -5 [db]. the experimental results demonstrate that the proposed algorithm based on complex lpc residual can perform better than other methods in noisy environment.",
    "present_kp": [
      "speech analysis",
      "f0 estimation",
      "analytic signal",
      "complex lpc residual",
      "irs filtered speech"
    ],
    "absent_kp": []
  },
  {
    "title": "modeling wikipedia admin elections using multidimensional behavioral social networks.",
    "abstract": "wikipedia admins are editors entrusted with special privileges and duties, responsible for the community management of wikipedia. they are elected using a special procedure defined by the wikipedia community, called request for adminship (rfa). because of the growing amount of management work (quality control, coordination, maintenance) on the wikipedia, the importance of admins is growing. at the same time, there exists evidence that the admin community is growing more slowly than expected. we present an analysis of the rfa procedure in the polish-language wikipedia, since the procedures introduction in 2005. with the goal of discovering good candidates for new admins that could be accepted by the community, we model the admin elections using multidimensional behavioral social networks derived from the wikipedia edit history. we find that we can classify the votes in the rfa procedures using this model with an accuracy level that should be sufficient to recommend candidates. we also propose and verify interpretations of the dimensions of the social network. we find that one of the dimensions, based on discussion on wikipedia talk pages, can be validly interpreted as acquaintance among editors, and discuss the relevance of this dimension to the admin elections.",
    "present_kp": [
      "wikipedia",
      "trust"
    ],
    "absent_kp": [
      "collaboration"
    ]
  },
  {
    "title": "a computational model for ratbot locomotion based on cyborg intelligence.",
    "abstract": "ratbots with electric stimulation in their brains possess not only their own biological sensation, perception, memory, and locomotion control abilities, but also machine visual sensation, memory and computing functionalities. with electrodes implanted in the medial forebrain bundle (mfb), we propose here a hybrid bio-machine locomotion system in the ratbots, processing the machine visual inputs, forming hybrid multiple memory system and outputting locomotion commands for navigation behaviors. to illustrate the enhanced performance of the ratbots theoretically, a computational model is presented to show how the multiple memory system affects the central pattern generator (cpg) generating the gait pattern and running velocity. compared with the extensive data from behavioral experiments, the results output from the proposed computational model fit the data of the decision accuracy and the relative velocity well, thus shown that the model provides a possible locomotion control mechanism innately in the cyborg systems.",
    "present_kp": [
      "cyborg systems",
      "ratbot locomotion",
      "central pattern generator",
      "computational model"
    ],
    "absent_kp": []
  },
  {
    "title": "strained ingaasp multi-quantum-well structures for inp-based wide linewidth and polarization-insensitive semiconductor optical amplifiers.",
    "abstract": "we have calculated the band structure of 1.55?m ingaasp/ingaasp multi-quantum-well structures using lttingerkohn hamiltonian taking into account the strain in the quantum wells (qws) and barriers, and the confinement in the quantum wells. using the calculated dispersion curves and oscillator strength between the different interband transitions, we have determined the optical gain in te and tm mode and the spontaneous amplified emission as a function of injected carrier density in devices composed of quantum wells with different thicknesses. we find that an optical gain linewidth larger than 130nm with a te/tm polarization dependence lower than 1db can be obtained using a three-quantum-well in0.53ga0.47as0.96p0.04/ingaasp active layer with quantum well thicknesses of 10, 14 and 19nm.",
    "present_kp": [
      "ingaasp",
      "quantum wells"
    ],
    "absent_kp": [
      "soa"
    ]
  },
  {
    "title": "buying and selling an asset over the finite time horizon: a non-parametric approach.",
    "abstract": "we consider the problem of buying an asset and selling it later in the open market within a limited time-period. in such a situation, it is usually assumed that the market prices are random observations from a known distribution. however, we propose in the paper the rank-based trading strategy that does not require any distributional assumption. we only assume that the agent's utility depends on the actual ranks of the purchase and selling prices of the asset. the non-parametric trading policy, which maximizes the agent's expected utility, can be stated with a sequence of critical ranks; the agent must buy an asset at time j if the relative rank of its purchase price is larger than the pre-specified critical rank at that time. likewise, the agent must sell the asset at time k if the relative rank of its selling price is less than the pre-specified critical rank at time k. finally, we conduct a simulation experiment to analyze the effect of the auto-correlation in market prices on the performance of the optimal trading policy.",
    "present_kp": [],
    "absent_kp": [
      "dynamic programming",
      "sequential decision analysis",
      "optimal stopping rule"
    ]
  },
  {
    "title": "a loop accelerator for low power embedded vliw processors.",
    "abstract": "the high transistor density afforded by modern vlsi processes have enabled the design of embedded processors that use clustered execution units to deliver high levels of performance. however, delivering data to the execution resources in a timely manner remains a major problem that limits ilp. it is particularly significant for embedded systems where memory and power budgets are limited. a distributed address generation and loop acceleration architecture for vliw processors is presented. this decentralized on-chip memory architecture uses multiple srams to provide high intra-processor bandwidth. each sram has an associated stream address generator capable of implementing a variety of addressing modes in conjunction with a shared loop accelerator. the architecture is extremely useful for generating application specific embedded processors, particularly for processing input data which is organized as a stream. the idea is evaluated in the context of a fine grain vliwarchitecture executing complex perception algorithms such as speech and visual feature recognition. transistor level spice simulations are used to demonstrate a 159x improvement in the energy delay product when compared to conventional architectures executing the same applications.",
    "present_kp": [
      "vliw",
      "embedded systems"
    ],
    "absent_kp": [
      "low power design"
    ]
  },
  {
    "title": "knowledge propagation in large image databases using neighborhood information.",
    "abstract": "the aim of this paper is to reduce to a minimum the level of human intervention in the semantic annotation process of images. ideally, only one copy of each object of interest would be labeled manually, and the labels would then be propagated automatically to all other occurrences of the objects in the database. to that end, we propose a neighbor-based influence propagation approach kprop which builds a voting model and propagates the knowledge associated to some objects to similar objects. we show that kprop can perform efficiently through matrix computations and achieve better performance with fewer labeled examples per object.",
    "present_kp": [
      "neighborhood"
    ],
    "absent_kp": [
      "classification",
      "image annotation"
    ]
  },
  {
    "title": "a generalized model for scratch detection.",
    "abstract": "this paper presents a generalization of kokaram's model for scratch lines detection on digital film materials. it is based on the assumption that scratch is not purely additive on a given image but shows also a destroying effect. this result allows us to design a more efficacious scratch detector which performs on a hierarchical representation of a degraded image, i.e., on its cross section local extrema. thanks to weber's law, the proposed detector even works well on slight scratches resulting completely automatic, except for the scratch color (black or white). the experimental results show that the proposed detector works better in terms of good detection and false alarms rejection with a lower computing time.",
    "present_kp": [
      "scratch detection",
      "weber's law"
    ],
    "absent_kp": [
      "digital film restoration"
    ]
  },
  {
    "title": "robust wavelet sliding-mode control via time-variant sliding function.",
    "abstract": "in this paper, a new robust wavelet time-variant sliding-mode control (rwtvsmc) for an uncertain nonlinear system is presented. the proposed method is composed of two controllers, based on a time variant sliding equation. for this purpose a neural wavelet controller is designed to approximate an ideal controller based on the wavelet network approximation. also a robust controller is designed to achieve h(infinity) tracking performance. new terminologies, rejection parameter and rejection regulator, for filtering all un-modeled frequencies are defined. a time-variant sliding equation based on the time-variant rejection parameter to achieve the best tracking performance is then presented. in addition, two theorems and one lemma which facilitate design of robust wavelet sliding-mode control are proved. also, two simulation examples are presented to illustrate the performance and the advantages of the proposed method.",
    "present_kp": [
      "robust control",
      "time-variant sliding-mode",
      "rejection regulator"
    ],
    "absent_kp": [
      "wavelet networks"
    ]
  },
  {
    "title": "localising missing plants in squared-grid patterns of discontinuous crops from remotely sensed imagery.",
    "abstract": "the purpose of this work is to localise and characterise missing plants on very high resolution (vhr) aerial images of agricultural parcels, in the case of discontinuous crops like wine and olive tree, which are planted according to a squared-grid pattern. it aims to establish an assisted, image processing system for remote sensed images, allowing the inventory the missing or withering plants, and the monitoring of their evolution during time. the global approach considers the planted parcel as a topological graph of vertices, whose reciprocal location conforms to a set of geometrical rules about orientation and length. the proposed system initiates the graph from the original image; then it adds missing vertices and refines its knowledge of the spatial pattern on an iterative basis. quality indicators are assigned at each added vertex, and several stopping criteria are estimated for each iteration, permitting an automated use of the algorithm. test cases have been conducted on two data sets of three parcels each: olive groves and goblet vineyards. the results are compared to validation data. they show an efficient reconstruction of the geometry and satisfactory omission-commission errors; they allow drawing up a typology of the major errors, and propose calibration parameters based on a sensitivity analysis. the main improvements include essentially the preprocessing, filtering step of the initial image. the process is being used for languedocian vineyards (france), and may be potentially usable for other problematic with the same kind of spatial patterns.",
    "present_kp": [
      "graph"
    ],
    "absent_kp": [
      "remote sensing",
      "digital image analysis",
      "point pattern",
      "olea europae l.",
      "vitis vinifera l."
    ]
  },
  {
    "title": "applied analysis for improving rail-network operations.",
    "abstract": "crew-scheduling component is analyzed by a column generation approach. results show that efficiency of crew schedules can be improved by more than 9%. additional driver training will increase the improvement to 15%. a detailed reliability analysis is demonstrated. a main causes for delays and unreliability problems is heavy passenger load.",
    "present_kp": [
      "column generation",
      "efficiency",
      "delay"
    ],
    "absent_kp": [
      "crew scheduling",
      "set covering",
      "rail reliability",
      "punctuality"
    ]
  },
  {
    "title": "saccade-related remapping of target representations between topographic maps: a neural network study.",
    "abstract": "the goal of this study was to explore how a neural network could solve the updating task associated with the double-saccade paradigm, where two targets are flashed in succession and the subject must make saccades to the remembered locations of both targets. because of the eye rotation of the saccade to the first target, the remembered retinal position of the second target must be updated if an accurate saccade to that target is to be made. we trained a three-layer, feed-forward neural network to solve this updating task using back-propagation. the networks inputs were the initial retinal position of the second target represented by a hill of activation in a 2d topographic array of units, as well as the initial eye orientation and the motor error of the saccade to the first target, each represented as 3d vectors in brainstem coordinates. the output of the network was the updated retinal position of the second target, also represented in a 2d topographic array of units. the network was trained to perform this updating using the full 3d geometry of eye rotations, and was able to produce the updated second-target position to within a 1 rms accuracy for a set of test points that included saccades of up to 70. emergent properties in the network's hidden layer included sigmoidal receptive fields whose orientations formed distinct clusters, and predictive remapping similar to that seen in brain areas associated with saccade generation. networks with the larger numbers of hidden-layer units developed two distinct types of units with different transformation properties: units that preferentially performed the linear remapping of vector subtraction, and units that performed the nonlinear elements of remapping that arise from initial eye orientation.",
    "present_kp": [
      "predictive",
      "remapping",
      "neural network"
    ],
    "absent_kp": [
      "updating saccade",
      "model",
      "superior colliculus",
      "topographic representation"
    ]
  },
  {
    "title": "ship speed optimization: concepts, models and combined speed-routing scenarios.",
    "abstract": "clarification of some important issues as regards ship speed optimization at the operational level. development of models that optimize ship speed for a spectrum of routing scenarios. clarification of concepts and misconceptions. incorporation of fundamental parameters that weigh heavily in speed and routing decisions. identification of properties of optimal solution.",
    "present_kp": [
      "ship speed optimization"
    ],
    "absent_kp": [
      "green maritime logistics",
      "ship routing"
    ]
  },
  {
    "title": "fast and robust extraction of surrogate respiratory signal from intra-operative liver ultrasound images.",
    "abstract": "in model-based respiratory motion estimation for the liver or other abdominal organs, the surrogate respiratory signal is usually obtained by using special tracking devices from skin or diaphragm, and subsequently applied to parameterize a 4d motion model for prediction or compensation. however, due to the intrinsic limits and economical costs of these tracking devices, the identification of the respiratory signal directly from intra-operative ultrasound images is a more attractive alternative.",
    "present_kp": [
      "respiratory motion"
    ],
    "absent_kp": [
      "ultrasound liver images",
      "similarity metric",
      "adaptive searching"
    ]
  },
  {
    "title": "heuristic constraints enforcement for training of and rule extraction from a fuzzy/neural architecture - part ii: implementation and application.",
    "abstract": "this paper is the second of two companion papers. the foundations of the proposed method of heuristic constraint enforcement on membership functions for knowledge extraction from a fuzzy/neural architecture was given in part i. part ii develops methods for forming constraint sets using the constraints and techniques for finding acceptable solutions that conform to all available a priori information. moreover, methods of integration of enforcement methods into the training of the fuzzy-neural architecture are discussed. the proposed technique is illustrated on a fuzzy-and classification problem and a motor fault detection problem. the results indicate that heuristic constraint enforcement on membership functions leads to extraction of heuristically acceptable membership functions in the input and output spaces. although the method is described on a specific fuzzy/neural architecture, it is applicable to any realization of a fuzzy inference system, including adaptive and/or static fuzzy inference systems.",
    "present_kp": [
      "constraint enforcement",
      "knowledge extraction"
    ],
    "absent_kp": [
      "neural-fuzzy architectures",
      "set theory"
    ]
  },
  {
    "title": "j2ee server scalability through ejb replication.",
    "abstract": "with the development of internet-based business, web applications are becoming increasingly complex. the j2ee specification aims at enabling the design of such web application servers. these servers have to ensure scalability and availability of the supported applications. scalibility can be achieved using replication techniques or partitionning techniques. the aim of this paper is to compare these approaches. in a j2ee web application server, one important component is the ejb tier. in this context, the jonas web application server provides an example of ejb replication system called cmi (cluster method invocation). in a first step, this paper presents a performance evaluation of cmi. it then introduces incrementally an alternative scheme based on partitionning and shows the performance benefits compared to cmi.",
    "present_kp": [
      "ejb",
      "replication",
      "scalability",
      "partition",
      "j2ee"
    ],
    "absent_kp": []
  },
  {
    "title": "development graphs - proof management for structured specifications.",
    "abstract": "development graphs are a toot for dealing with structured specifications in a formal program development in order to ease the management of change and reusing proofs. in this work, we extend development graphs with hiding (e.g. hidden operations). hiding is a particularly difficult to realize operation, since it does not admit such a good decomposition of the involved specifications as other structuring operations do. we develop both a semantics and proof rules for development graphs with hiding. the rules are proven to be sound, and also complete relative to an oracle for conservative extensions. we also show that an absolutely complete set of rules cannot exist. the whole framework is developed in a way independent of the underlying logical system (and thus also does not prescribe the nature of the parts of a specification that may be hidden). we also show how various other logic independent specification formalisms can be mapped into development graphs; thus, development graphs can serve as a kernel formalism for management of proofs and of change.",
    "present_kp": [
      "structuring"
    ],
    "absent_kp": [
      "algebraic specification",
      "proof calculus",
      "institutions"
    ]
  },
  {
    "title": "lattice boltzmann simulation of water and gas flow in porous gas diffusion layers in fuel cells reconstructed from micro-tomography.",
    "abstract": "the porous gas diffusion layers (gdls) are key components in hydrogen fuel cells. during their operation the cells produce water at the cathode, and to avoid flooding, the water has to be removed out of the cells. how to manage the water is therefore an important issue in fuel cell design. in this paper we investigated water flow in the gdls using a combination of the lattice boltzmann method and x-ray computed tomography at the micron scale. water flow in the gdl depends on waterair surface tension and hydrophobicity. to correctly represent the watergas surface tension, the formations of water droplets in air were simulated, and the watergas surface tension was obtained by fitting the simulated results to the younglaplace formula. the hydrophobicity is represented by the watergas-fabric contact angle. for a given watergas surface tension the value of the contact angle was determined by simulating the formations of water droplets on a solid surface with different hydrophobicity. we then applied the model to simulate water intrusion into initially dry gdls driven by a pressure gradient in attempts to understand the impact of hydrophobicity on water distribution in the gdls. the structures of the gdl were acquired by x-ray micro-tomography at a resolution of 1.7 microns. the simulated results revealed that with an increase in hydrophobicity, water transport in gdls changes from piston-flow to channelled flow.",
    "present_kp": [
      "gas diffusion layer",
      "lattice boltzmann method",
      "x-ray computed tomography",
      "hydrophobicity"
    ],
    "absent_kp": [
      "two-phase flow"
    ]
  },
  {
    "title": "create interactive web illustrations with google maps.",
    "abstract": "the use of google maps to display geographic content is commonplace. google maps is also useful in displaying high-resolution images on the web, allowing visitors to move, zoom and pan within the image view window. a digital image is needed, along with a google account to obtain a google maps application programming interface (api) key, web space to load content that can use javascript, a computer that can run java and an html editing application. gmap image cutter software may be downloaded from the university college london centre for advanced spatial analysis (casa) site. the image cutter software divides a hi-resolution image into many small tiles. these tiles enable zooming to levels specified by the user during initial image cutting. an html file and a folder of image tiles are generated, usable on local systems with no further processing, but requiring a google maps api key when posted to a web server. where a fully featured digital imaging library solution is impractical or unavailable, this technique enables interactive display of a high-resolution image with the familiar google maps controls. though the underlying technology is complex, creating a web illustration this way is easy, platform neutral and produces visually appealing results.",
    "present_kp": [
      "web",
      "google maps"
    ],
    "absent_kp": [
      "digital images"
    ]
  },
  {
    "title": "path delay analysis for hierarchical building block layout system.",
    "abstract": "this paper describes a path delay analysis system which employs an accurate signal delay calculation method for mos lsis, taking poly resistance into account. the system takes mask patterns generated by a hierarchical building block layout system as inputs, and verifies timing margins of a large scale random logic lsi in a module-wise bottom up fashion. path delay analysis using a critical path trace algorithm and an enumerative path trace algorithm in combination is effective in locating critical timing regions in a chip and in analyzing critical paths in the regions in detail.",
    "present_kp": [
      "hierarchic",
      "method",
      "region",
      "analysis",
      "delay",
      "module",
      "layout",
      "building block",
      "logic",
      "account",
      "timing",
      "algorithm",
      "paper",
      "critic",
      "effect",
      "pattern"
    ],
    "absent_kp": [
      "traces",
      "systems",
      "locatability",
      "signaling",
      "large-scale",
      "randomization",
      "combinational"
    ]
  },
  {
    "title": "wind speed-up process on the windward slope of dunes in dune fields.",
    "abstract": "wind speed-up is a key physical factor affecting the transportation of windblown sand on the windward slope of a sand dune. it contributes to the understanding of dune morphology, evolution of dune field and dunes' migration. in this study, a computational model is proposed to calculate the wind speed-up factor on the windward slope of sand dunes in a dune field. this model not only reflects the influence of neighboring dunes, but also incorporates the effect of incoming wind speed, sand diameter and thickness of sand supply which have been little considered by previous studies. the computational complexity of our model is much lower than that of the jackson-hunt theory and the computational fluid dynamics. based on the wind speed-up model, we investigate the probability density function of wind speed-up factor in a dune field, and give an empirical expression of wind speed-up factor, which provides a basis for accurate prediction of wind speed on the windward slope of a sand dune in a real dune field.",
    "present_kp": [
      "wind speed-up factor",
      "windward slope of sand dunes",
      "sand supply",
      "dune field",
      "sand diameter"
    ],
    "absent_kp": []
  },
  {
    "title": "external storage middleware for wireless devices with limited resources.",
    "abstract": "this paper introduces an external storage middleware, that offers a set of functions (api - application program interface) to mobile applications and facilitates the transfer of files between a mobile device and external storage servers regardless of the available wireless network. the middleware selects the best wireless service available. in addition, the files exchanged between the mobile device and the external storage server are encrypted to provide security when traveling through the network. as a use case, it was developed an automatic file swapper service for mobile devices. the service is running on the mobile device optimizing its available storage space. our middleware was tested with the following wireless services: wi-fi, gprs, mms.",
    "present_kp": [
      "mms",
      "middleware",
      "wi-fi",
      "gprs"
    ],
    "absent_kp": [
      "ftp"
    ]
  },
  {
    "title": "associativity-based on-demand multi-path routing in mobile ad hoc networks.",
    "abstract": "this paper is primarily concerned with multi-path routing in mobile ad hoc networks (manets). we propose a novel associativity-based on-demand source routing protocol for manets that attempts to establish relatively stable path(s) between the source and the destination. we introduce a new notion for gauging the temporal and spatial stability of nodes, and hence the paths interconnecting them. the proposed protocol is compared with other unipath (dsdv and aodv) and multi-path (aomdv) routing protocols. we investigate the performance in terms of throughput, normalized routing overhead, packet delivery ratio etc. all on-demand protocols show good performance in mobile environments with less traffic overhead compared to proactive approaches, but they are prone to longer end-to-end delays due to route discovery and maintenance.",
    "present_kp": [
      "associativity",
      "multi-path routing",
      "mobile ad hoc networks"
    ],
    "absent_kp": [
      "node-disjoint"
    ]
  },
  {
    "title": "an approximate truthful mechanism for combinatorial auctions with single parameter agents.",
    "abstract": "mechanism design seeks algorithms whose inputs are provided by selfish agents who would lie if advantageous. incentive compatible mechanisms compel the agents to tell the truth by making it in their self-interest to do so. often, as in combinatorial auctions, such mechanisms involve the solution of np-hard problems. unfortunately, approximation algorithms typically destroy incentive compatibility. randomized rounding is a commonly used technique for designing approximation algorithms. we devise a version of randomized rounding that is incentive compatible, giving a truthful mechanism for combinatorial auctions with single parameter agents (e.g., \"single minded bidders\") that approximately maximizes the social value of the auction. we discuss two orthogonal notions of truthfulness for a randomized mechanism, truthfulness with high probability and in expectation, and give a mechanism that achieves both simultaneously.we consider combinatorial auctions where multiple copies of many different items are on sale, and each bidder i desires a subset s i . given a set of bids, the problem of finding the allocation of items that maximizes total valuation is the well-known setpacking problem. this problem is np-hard, but for the case of items with many identical copies the optimum can be approximated very well. to turn this approximation algorithm into a truthful auction mechanism we overcome two problems: we show how to make the allocation algorithm monotone , and give a method to compute the appropriate payments efficiently.",
    "present_kp": [
      "value",
      "approximation algorithms",
      "method",
      "combinatorial auctions",
      "agent",
      "approximation",
      "auction",
      "probability",
      "case",
      "version",
      "social",
      "algorithm",
      "allocation",
      "incentive compatibility"
    ],
    "absent_kp": [
      "computation",
      "mechanical design",
      "randomization"
    ]
  },
  {
    "title": "erp modeling: a comprehensive approach.",
    "abstract": "we present a generic reverse engineering process, aimed at developing a model that captures the available alternatives at different application levels of an enterprise resource planning (erp) system. such a model is needed when erp systems are aligned with the needs of the enterprise in which they are implemented. in order to support the erp implementation process, the model should describe the entire scope of the erp system's functionality and the alternative business processes it supports, as well as the interdependencies among them. we analyze the desired properties a modeling language should satisfy to be applied in constructing an erp system model. this analysis, which follows the cooperative requirements engineering with scenarios classification framework in its adapted erp modeling form, results in a set of criteria for evaluating modeling languages for this purpose. using these criteria, we evaluate the objectprocess modeling methodology and apply it for generating a detailed erp system model. the generic process and detailed criteria we develop can serve for comprehensive erp modeling, as well as for obtaining a model of other process-supportive off-the-shelf systems that are of generic and configurable nature.",
    "present_kp": [
      "enterprise resource planning",
      "reverse engineering",
      "modeling languages"
    ],
    "absent_kp": [
      "objectprocess methodology"
    ]
  },
  {
    "title": "visual exploration across biomedical databases.",
    "abstract": "though biomedical research often draws on knowledge from a wide variety of fields, few visualization methods for biomedical data incorporate meaningful cross-database exploration. a new approach is offered for visualizing and exploring a query-based subset of multiple heterogeneous biomedical databases. databases are modeled as an entity-relation graph containing nodes (database records) and links (relationships between records). users specify a keyword search string to retrieve an initial set of nodes, and then explore intra- and interdatabase links. results are visualized with user-defined semantic substrates to take advantage of the rich set of attributes usually present in biomedical data. comments from domain experts indicate that this visualization method is potentially advantageous for biomedical knowledge exploration.",
    "present_kp": [],
    "absent_kp": [
      "data exploration and discovery",
      "bioinformatics",
      "information visualization"
    ]
  },
  {
    "title": "design analysis of the propulsion and control system of an underactuated remotely operated vehicle using axiomatic design theory - part 1.",
    "abstract": "in this paper, the system design issues of the propulsion and control system of the rov 11 are analyzed and addressed. the design concept, some of the upgraded features of the rov 11 in comparison to rov i and the unified pilot training and control system developed, will also be briefly discussed in this paper.",
    "present_kp": [
      "axiomatic design theory",
      "propulsion and control system"
    ],
    "absent_kp": []
  },
  {
    "title": "exploring open narrative structures with tangibles.",
    "abstract": "the constructed narratives construction kit is a tangible game interface built on the 802.15.4 wireless protocol designed for supporting players social awareness of others who are also engaged in the collaborative building activity.",
    "present_kp": [],
    "absent_kp": [
      "802.15.4 mesh networks",
      "computer supported collaborative play",
      "public space interfaces",
      "tangible social interface",
      "collaborative design"
    ]
  },
  {
    "title": "bypass extended stack processing for anti-thrashing replacement in shared last level cache of chip multiprocessors.",
    "abstract": "chip multiprocessors (cmps) allow different applications to share llc (last level cache). since each application has different cache capacity demand, llc capacity should be partitioned in accordance with the demands. existing partitioning algorithms estimate the capacity demand of each core by stack processing considering the lru (least recently used) replacement policy only. however, anti-thrashing replacement algorithms like bip (binary insertion policy) and bip-bypass emerged to overcome the thrashing problem of lru replacement policy in a working set greater than the available cache size. since existing stack processing cannot estimate the capacity demand with anti-thrashing replacement policy, partitioning algorithms also cannot partition cache space with anti-thrashing replacement policy. in this letter, we prove that bip replacement policy is not feasible to stack processing but bip-bypass is. we modify stack processing to accommodate bip-bypass. in addition, we propose the pipelined hardware of modified stack processing. with this hardware, we can get the success function of the various capacities with anti-thrashing replacement policy and assess the cache capacity of shared cache adequate to each core in real time.",
    "present_kp": [
      "last level cache",
      "stack processing",
      "replacement policy",
      "anti-thrashing"
    ],
    "absent_kp": [
      "cache partitioning",
      "chip multi-processors"
    ]
  },
  {
    "title": "singularity analysis of fine-tuning stewart platform for large radio telescope using genetic algorithm.",
    "abstract": "a new singularity analysis method for general six degree-of-freedom (dof) stewart platform using genetic algorithm (ga) is proposed in this paper. the jacobian matrix of stewart platform is first deduced, then the square of determinant of the jacobian matrix is selected as the objective function, and the minimal of this objective function is searched in the workspace of stewart platform by the ga. the singularity of stewart platform depends on this minimal objective function: if this value is zero, the singularity of stewart platform will take place, otherwise, the stewart platform is singularity-free. the effectiveness of this new genetic singularity analysis method is validated by the singularity analysis of a six-dof fine-tuning stewart platform for the next generation large radio telescope. the results have shown that the fine-tuning stewart platform is singularity-free, which has laid a solid base for the requirement of high precision trajectory tracking for the next generation large radio telescope.",
    "present_kp": [
      "stewart platform",
      "singularity analysis",
      "genetic algorithm",
      "large radio telescope"
    ],
    "absent_kp": [
      "mechatronics"
    ]
  },
  {
    "title": "increasing the number of effective registers in a low-power processor using a windowed register file.",
    "abstract": "low-power embedded processors utilize compact instruction encodings to achieve small code size. instruction sizes of 8 to 16 bits are common. such encodings place tight restrictions on the number of bits available to encode operand specifiers, and thus on the number of architected registers. the central problem with this approach is that performance and power are often sacrificed as the burden of operand supply is shifted from the register file to the memory due to the limited number of registers. in this paper, we investigate the use of a windowed register file to address this problem by providing more registers than allowed in the encoding. the registers are organized as a set of identical register windows where at each point in the execution there is a single active window. special window management instructions are used to change the active window and to transfer values between windows. the goal of this design is to give the appearance of a large register file without compromising the instruction encoding. to support the windowed register file, we designed and implemented a novel graph partitioning based compiler algorithm that partitions virtual registers within a given procedure across multiple windows. on a 16-bit embedded processor with a parameterized register window, an average of 10% improvement in application performance and 7% reduction in system power was achieved as an eight-register design was scaled from one to four windows.",
    "present_kp": [
      "graph partitioning",
      "register window",
      "instruction encoding",
      "embedded processor",
      "low-power"
    ],
    "absent_kp": [
      "window assignment"
    ]
  },
  {
    "title": "simple proofs of classical theorems in discrete geometry via the guth-katz polynomial partitioning technique.",
    "abstract": "recently guth and katz (arxiv:<phone>, 2010) invented, as a step in their nearly complete solution of erdas's distinct distances problem, a new method for partitioning finite point sets in r-d , based on the stone-tukey polynomial ham-sandwich theorem. we apply this method to obtain new and simple proofs of two well known results: the szemer,di-trotter theorem on incidences of points and lines, and the existence of spanning trees with low crossing numbers. since we consider these proofs particularly suitable for teaching, we aim at self-contained, expository treatment. we also mention some generalizations and extensions, such as the pach-sharir bound on the number of incidences with algebraic curves of bounded degree.",
    "present_kp": [
      "incidences",
      "crossing number",
      "polynomial ham-sandwich"
    ],
    "absent_kp": [
      "algebraic techniques",
      "spanning tree with low crossing number",
      "partitioning polynomial"
    ]
  },
  {
    "title": "frequency domain metamodelling of a feedback queue.",
    "abstract": "schruben and cogliano introduced frequency domain experiments as a tool for metamodel identification. to design a frequency domain experiment, the experimenter must choose appropriate values for certain experimental variables such as oscillation frequency, window size, and range of oscillation. in this paper, we demonstrate that these experimental variables affect the outcome of frequency domain experiments and the magnitude of these effects are model dependent.",
    "present_kp": [
      "affect",
      "values",
      "size",
      "design",
      "identification",
      "tool",
      "demonstrate",
      "feedback",
      "model",
      "paper",
      "effect"
    ],
    "absent_kp": [
      "variability",
      "dependencies",
      "experimentation",
      "metamodeling",
      "experience",
      "windows"
    ]
  },
  {
    "title": "matrix formulation for the calculation of structural systems reliability.",
    "abstract": "based on matrix algebra and matrix formulations, a new technique for the calculation of structural systems reliability of girder bridges is presented in this paper. this technique guarantees a high degree of accuracy and at the same time allows the calculation to be performed in an automatic and efficient manner. this technique is illustrated by a numerical example and its main advantages are discussed in the conclusions.",
    "present_kp": [
      "reliability",
      "matrix formulation",
      "girder bridge"
    ],
    "absent_kp": [
      "probability of failure",
      "safety margin",
      "failure path formulation",
      "sequence path formulation"
    ]
  },
  {
    "title": "a multi-agent system for acquiring and sharing lessons learned.",
    "abstract": "this paper presents a multi-agent system for knowledge management (km) in research and development (r&d) projects. r&d teams have no time to organize project information, nor to articulate the rationale behind the actions that generated the information. our aim is to provide a system for helping team members to explicit knowledge, and to allow them to share their experiences, i.e. lessons learned (ll), without asking them too much extra-work. the article focuses on how we intend to help the team members to feed the system with ll, using the operations they perform on desktop computers, and how we intend to exploit the ll by using a case-based reasoning engine. we have been developing a prototype of such a km system for a cooperative project.",
    "present_kp": [
      "knowledge management",
      "lessons learned"
    ],
    "absent_kp": [
      "multi-agents",
      "architecture"
    ]
  },
  {
    "title": "knowledge based perceptual anchoring.",
    "abstract": "perceptual anchoring is the process of creating and maintaining a connection between the sensor data corresponding to a physical object and its symbolic description. it is a subset of the symbol grounding problem, introduced by harnad (phys. d, nonlinear phenom. 42(13):335346, 1990) and investigated over the past years in several disciplines including robotics. this phd dissertation focuses on a method for grounding sensor data of physical objects to the corresponding semantic descriptions, in the context of cognitive robots where the challenge is to establish the connection between percepts and concepts referring to objects, their relations and properties. we examine how knowledge representation can be used together with an anchoring framework, so as to complement the meaning of percepts while supporting better linguistic interaction with the use of the corresponding concepts. the proposed method addresses the need to represent and process both perceptual and semantic knowledge, often expressed in different abstraction levels, while originating from different modalities. we then focus on the integration of anchoring with a large scale knowledge base system and with perceptual routines. this integration is applied in a number of studies, where in the context of a smart home, several evaluations spanning from spatial and commonsense reasoning to linguistic interaction and concept acquisition.",
    "present_kp": [
      "anchoring",
      "knowledge representation",
      "symbol grounding"
    ],
    "absent_kp": [
      "cognitive perception",
      "common-sense information"
    ]
  },
  {
    "title": "spatio-temporal approximate reasoning over complex objects.",
    "abstract": "we discuss the problems of spatio-temporal reasoning in the context of hierarchical information maps and approximate reasoning networks (ar networks). hierarchical information maps are used for representations of domain knowledge about objects, their parts, and their dynamical changes. ar networks are patterns constructed over sensory measurements and they are discovered from hierarchical information maps and experimental data. they make it possible to approximate domain knowledge, i.e., complex spatio-temporal concepts and reasonings represented in hierarchical information maps. experiments with classifiers based on ar schemes using a road traffic simulator are also briefly presented.",
    "present_kp": [
      "complex objects",
      "spatio-temporal reasoning",
      "information maps",
      "pattern",
      "ar schemes",
      "ar networks"
    ],
    "absent_kp": [
      "concept approximation",
      "sensor measurement"
    ]
  },
  {
    "title": "nqp(c) = co-c=p.",
    "abstract": "adleman, demarrais, and huang introduced the nondeterministic quantum polynomial-time complexity class nqp as an analogue of np. fortnow and rogers implicitly showed that, when the amplitudes are rational numbers, nqp is contained in the complement of c=p. fenner, green, homer, and pruim improved this result by showing that, when the amplitudes are arbitrary algebraic numbers, nqp coincides with co-c=p. in this paper we prove that, even when the amplitudes are arbitrary complex numbers, nqp still remains identical to co-c=p. as an immediate corollary, bqp differs from nqp when the amplitudes are unrestricted.",
    "present_kp": [],
    "absent_kp": [
      "computational complexity",
      "theory of computation"
    ]
  },
  {
    "title": "exploring the usability of the iso reference terminology model for nursing actions in representing oriental nursing actions.",
    "abstract": "this study examined the extent to which the iso reference terminology model for nursing actions represents oriental nursing actions in a computerized nursing documentation system to share data and foster communication between oriental nursing care and conventional nursing care. the narrative nursing notes of 545 patients retrieved from a nursing documentation system in an oriental medicine teaching hospital were analyzed. among 49,118 entries, 933 were recorded as nursing actions. each entry was decomposed in a set of single statements. a total of 1209 nursing action statements were derived and mapped to the components of the model. these processes were reviewed and validated by two domain experts and a nursing terminology expert. all of the oriental nursing actions documented contained a word or phrase that described the action and target in the model. the recipient of care was expressed explicitly in 1.2% of statements. the most frequently used action terms were administering (19.7%), teaching (16.5%), and explaining (13.6%). the target terms that indicated unique oriental nursing concepts included sasang constitution differentiation, removal of acupuncture needles, herb moxibustion, oriental massage, and oriental medication. the findings demonstrate that oriental nursing actions can be represented using the iso reference terminology model for nursing actions. further specification of the components of the model will be useful to achieve consistent mapping across different settings. the addition of component qualifiers should also be taken into consideration to describe nursing actions at a more granular level.",
    "present_kp": [
      "nursing care",
      "terminology",
      "oriental medicine"
    ],
    "absent_kp": [
      "nursing records",
      "standards"
    ]
  },
  {
    "title": "robust weighted lad regression.",
    "abstract": "the least squares linear regression estimator is well-known to be highly sensitive to unusual observations in the data, and as a result many more robust estimators have been proposed as alternatives. one of the earliest proposals was least-sum of absolute deviations (lad) regression, where the regression coefficients are estimated through minimization of the sum of the absolute values of the residuals. lad regression has been largely ignored as a robust alternative to least squares, since it can be strongly affected by a single observation (that is, it has a breakdown point of 1/n 1 / n , where n is the sample size). in this paper we show that judicious choice of weights can result in a weighted lad estimator with much higher breakdown point. we discuss the properties of the weighted lad estimator, and show via simulation that its performance is competitive with that of high breakdown regression estimators, particularly in the presence of outliers located at leverage points. we also apply the estimator to several data sets.",
    "present_kp": [
      "breakdown point",
      "leverage points",
      "outliers"
    ],
    "absent_kp": [
      "robust regression"
    ]
  },
  {
    "title": "webbase: a repository of web pages.",
    "abstract": "in this paper, we study the problem of constructing and maintaining a large shared repository of web pages. we discuss the unique characteristics of such a repository, propose an architecture, and identify its functional modules. we focus on the storage manager module, and illustrate how traditional techniques for storage and indexing can be tailored to meet the requirements of a web repository. to evaluate design alternatives, we also present experimental results from a prototype repository called webbase, that is currently being developed at stanford university.",
    "present_kp": [
      "repository",
      "webbase",
      "architecture"
    ],
    "absent_kp": [
      "storage management"
    ]
  },
  {
    "title": "spider monkey optimization algorithm for numerical optimization.",
    "abstract": "swarm intelligence is one of the most promising area for the researchers in the field of numerical optimization. researchers have developed many algorithms by simulating the swarming behavior of various creatures like ants, honey bees, fish, birds and the findings are very motivating. in this paper, a new approach for numerical optimization is proposed by modeling the foraging behavior of spider monkeys. spider monkeys have been categorized as fissionfusion social structure based animals. the animals which follow fissionfusion social systems, split themselves from large to smaller groups and vice-versa based on the scarcity or availability of food. the proposed swarm intelligence approach is named as spider monkey optimization (smo) algorithm and can broadly be classified as an algorithm inspired by intelligent foraging behavior of fissionfusion social structure based animals.",
    "present_kp": [
      "optimization",
      "fissionfusion social system",
      "spider monkey optimization"
    ],
    "absent_kp": [
      "swarm intelligence based algorithm"
    ]
  },
  {
    "title": "markov models of internet traffic and a new hierarchical mmpp model.",
    "abstract": "the first part of this paper gives a short tutorial survey of internet traffic modeling, focusing on recent advances in markov models showing pseudo-lrd (long range dependence) characteristics that match those measured on the internet. the interest in markov models of internet traffic, in spite of the impossibility to achieve true lrd or self-similarity, lies in the possibility of exploiting powerful analytical techniques to predict the network performance, which is the ultimate goal when adopting models to either study existing networks or design new ones. then, the paper describes a new mmpp (markov modulated poisson process) traffic model that accurately approximates the lrd characteristics of internet traffic traces over the relevant time scales. the heart of the model is based on the notion of sessions and flows, trying to mimic the real hierarchical generation of packets in the internet. the proposed model is simple and intuitive: its parameters have a physical meaning, and the model can be tuned with only a few input parameters. results prove that the queuing behavior of the traffic generated by the mmpp model is coherent with the one produced by real traces collected at our institution edge router under several different traffic loads. due to its characteristics, the proposed mmpp traffic model can be used as a simple and manageable tool for ip network dimensioning, design and planning: the paper provides examples of its application in both simulative and theoretical analysis.",
    "present_kp": [
      "internet traffic",
      "mmpp"
    ],
    "absent_kp": [
      "markovian models"
    ]
  },
  {
    "title": "a superlinearly convergent predictor-corrector method for degenerate lcp in a wide neighborhood of the central path with o(root nl)-iteration complexity.",
    "abstract": "an interior point method for monotone linear complementarity problems acting in a wide neighborhood of the central path is presented. the method has -iteration complexity and is superlinearly convergent even when the problem does not possess a strictly complementary solution.",
    "present_kp": [
      "linear complementarity problem"
    ],
    "absent_kp": [
      "interior-point algorithm",
      "large neighbourhood",
      "superlinear convergence"
    ]
  },
  {
    "title": "from vienna to california: a journey across disciplines - an interview with heinz von foerster.",
    "abstract": "purpose - seeks to promote an understanding of a multidisciplinary approach to artificial intelligence and the humanities. design/methodology/approach - this paper is based on an interview with heinz von foerster. findings - describes von foerster's personal and intellectual journey which made him a transdisciplinary scientist and the founder and director of the biological computer laboratory. originality/value - provides a better understanding in how the complexity of scientific practice should be reflected in an open and flexible attitude towards the objects of enquiry.",
    "present_kp": [
      "artificial intelligence"
    ],
    "absent_kp": [
      "cybernetics"
    ]
  },
  {
    "title": "modeling and minimization of interconnect energy dissipation in nanometer technologies.",
    "abstract": "as the technology sizes of semiconductor devices continue to decrease, the effect of nanometer technologies on interconnects, such as crosstalk glitches and timing variations, become more significant. in this paper, we study the effect of nanometer technologies on energy dissipation in interconnects. we propose a new power estimation technique which considers dsm effects, resulting in significantly more accurate energy dissipation estimates than transition-count based methods for on-chip interconnects. we also introduce an energy minimization technique which attempts to minimize large voltage swings across the cross-coupling capacitances between interconnects. even though the number of transitions may increase, our method yields a decrease in power consumption of up to 50%.",
    "present_kp": [
      "method",
      "power consumption",
      "technologies",
      "transit",
      "device",
      "variation",
      "energy",
      "crosstalk",
      "interconnect",
      "energy minimization",
      "timing",
      "model",
      "paper",
      "effect"
    ],
    "absent_kp": [
      "couples",
      "- power estimation",
      "minimal"
    ]
  },
  {
    "title": "interactive learning and management of visual information via human-like software robot.",
    "abstract": "to achieve smooth real-world interaction between people and computers, we developed a system that displays a three-dimensional computer-graphic human-like image from the waist up (anthropomorphic software robot: hereinafter \"robot\") on the display, that interactively sees and hears, and that has fine and detailed control functions such as facial expressions, line of sight, and pointing at targets with its finger. the robot visually searches and identifies persons and objects in real space that it has learned in advance (registered space, which was our office in this case), manages the history information of the places and times it found objects and/or persons, and tells the user, indicating their three-dimensional positions with line of sight and its finger. it interactively learns new objects and persons together with their names and owners. by using this function, the robot can engage in simple dialogue (do a task) with the user.",
    "present_kp": [
      "anthropomorphic software robot"
    ],
    "absent_kp": [
      "multimodal interaction",
      "computer vision",
      "machine learning",
      "real-world intelligence"
    ]
  },
  {
    "title": "the mixmax random number generator.",
    "abstract": "in this paper, we study the randomness properties of unimodular matrix random number generators. under well-known conditions, these discrete-time dynamical systems have the highly desirable k-mixing properties which guarantee high quality random numbers. it is found that some widely used random number generators have poor kolmogorov entropy and consequently fail in empirical tests of randomness. these tests show that the lowest acceptable value of the kolmogorov entropy is around 50. next, we provide a solution to the problem of determining the maximal period of unimodular matrix generators of pseudo-random numbers. we formulate the necessary and sufficient condition to attain the maximum period and present a family of specific generators in the mixmax family with superior performance and excellent statistical properties. finally, we construct three efficient algorithms for operations with the mixmax matrix which is a multi-dimensional generalization of the famous cat-map. first, allowing to compute the multiplication by the mixmax matrix with o(n n ) operations. second, to recursively compute its characteristic polynomial with o(n2 n 2 ) operations, and third, to apply skips of large number of steps s to the sequence in o(n2 n 2 log(s s )) operations.",
    "present_kp": [],
    "absent_kp": [
      "pseudo-random number generator",
      "kolmogorov k-system",
      "deterministic chaos"
    ]
  },
  {
    "title": "approximate inference for medical diagnosis.",
    "abstract": "computer-based diagnostic decision support systems (dsss) will play an increasingly important role in health care. due to the inherent probabilistic nature of medical diagnosis, a dss should preferably be based on a probabilistic model. in particular, bayesian networks provide a powerful and conceptually transparent formalism for probabilistic modeling. a drawback is that bayesian networks become intractable for exact computation if a large medical domain is to be modeled in detail. this has obstructed the development of a useful system for internal medicine. advances in approximation techniques, e.g. using variational methods with tractable structures, have opened new possibilities to deal with the computational problem. however, the only way to assess the usefulness of these methods for a dss in practice is by actually building such a system and evaluating it by users. in the coming years, we aim to build a dss for anaemia based on a detailed probabilistic model, and equipped with approximate methods to study the practical feasibility and the usefulness of this approach in medical practice. in this paper, we will sketch how variational techniques with tractable structures can be used in a typical model for medical diagnosis. we provide numerical results on artificial problems. in addition, we describe our approach to develop the bayesian network for the dss and show some preliminary results.",
    "present_kp": [],
    "absent_kp": [
      "medical decision support",
      "bayesian belief networks",
      "variational approximations"
    ]
  },
  {
    "title": "two-level service-oriented architecture based on product-line.",
    "abstract": "software product-line engineering is the successful reuse of technology when applied to component-based software development. the main concept and structure of this technology is developing reusable core assets by applying commonality and variability, and then developing new software reusing these core assets. recently, the emergence of service-oriented environments, called soa, has provided flexible reuse environments by reusing pre-developed component structure as service units; this is platform-independent and can integrate into heterogeneous environments. the core asset of an soa is the service. therefore, we can increase the reusability of an soa by combining it with the concept of a product-line. these days, there exists research that combines soa and product-lines, taking into account reusability. however, current research does not consider the interaction between the provider and consumer in soa environments. furthermore, this research tends to focus on more fragmentary aspects of product-line engineering, such as modeling and proposing variability in services. in this paper, we propose a mechanism named 2-level soa, including a supporting environment. this proposed mechanism deploys and manages the reusable service. in addition, by reusing and customizing this reusable service, we can develop and generate new services. our proposed approach provides a structure to maximize the flexibility of soa, develops services that consider systematic reuse, and constructs service-oriented applications by reusing this pre-developed reusable service. therefore, our approach can increase both efficiency and productivity when developing service-oriented applications.",
    "present_kp": [
      "soa"
    ],
    "absent_kp": [
      "service reuse",
      "service variability",
      "uddi",
      "service oriented development"
    ]
  },
  {
    "title": "diffusive neural network.",
    "abstract": "a non-connectionist model of a neuronal network based on passive diffusion of neurotransmitters is presented as an alternative to hard-wired artificial neural networks. classic thermodynamical approach shows that the diffusive network is capable of exhibiting asymptotic stability and a dynamics resembling that of a chaotic system. basic computational capabilities of the net are discussed based on the equivalence with a turing machine. the model offers a way to represent mass-sustained brain functions in terms of recurrent behaviors in the phase space.",
    "present_kp": [
      "turing machine"
    ],
    "absent_kp": [
      "ligandreceptor interaction",
      "cell-assemblies",
      "mass-sustained functions"
    ]
  },
  {
    "title": "structured collaborative workflow design.",
    "abstract": "workflow design is often an effort of distributed and heterogeneous teams, thus making tool support for collaboration a necessity. we present a novel concept of collaborative workflow design which combines cooperation and workflow model analysis. workflow analysis is simplified using workflow metrics, which help identifying problematic aspects of the workflow model. our findings are implemented in a collaborative workflow design system, which is easily accessible on the web, but provides a desktop-like user experience.",
    "present_kp": [
      "collaborative workflow design"
    ],
    "absent_kp": [
      "cscw",
      "bpel",
      "web 2.0",
      "rich internet applications"
    ]
  },
  {
    "title": "futility-based offspring sizing.",
    "abstract": "parameter control in evolutionary algorithms (eas) has been shown to be beneficial; however, the control of offspring size has so far received very little attention. this paper introduces futility-based offspring sizing (fubos), a method for controlling offspring size on a per generation basis without even requiring the user to set an initial offspring size value. we show that on several complex problems, an ea employing fubos performs on par with a highly tuned, fixed offspring size ea while being far more efficient in terms of fitness evaluations and much easier to tune as well.",
    "present_kp": [
      "offspring sizing",
      "evolutionary algorithm",
      "parameter control"
    ],
    "absent_kp": [
      "parameterless evolutionary algorithm",
      "optimization"
    ]
  },
  {
    "title": "approximate fuzzy analysis of linear structural systems applying intervening variables.",
    "abstract": "linear static structures with fuzzy parameters and fuzzy loading are considered. structural response represented approximately using intervening variables. a single structural analysis and sensitivity analysis are required. approach captures nonlinearity of response with respect to uncertain parameters.",
    "present_kp": [
      "intervening variables"
    ],
    "absent_kp": [
      "fuzzy structural analysis",
      "approximation concepts",
      "uncertain structural parameters",
      "uncertain loading"
    ]
  },
  {
    "title": "visualization in string theory.",
    "abstract": "the tools for visualization of relativistic string dynamics in various topological classes are developed. new theoretical results, obtained with the aid of these tools, are described.",
    "present_kp": [],
    "absent_kp": [
      "scientific visualization",
      "extremal surfaces",
      "complex dynamical systems"
    ]
  },
  {
    "title": "improved approximations for the hotlink assignment problem.",
    "abstract": "let g = (v e) be a graph representing a web site, where nodes correspond to pages and arcs to hyperlinks. in this context, hotlinks are defined as shortcuts (new arcs) added to web pages of g in order to reduce the time spent by users to reach their desired information. in this article, we consider the problem where g is a rooted directed tree and the goal is minimizing the expected time spent by users by assigning at most k hotlinks to each node. for the most studied version of this problem where at most one hotlink can be added to each node, we prove the existence of two fptas's which optimize different objectives considered in the literature: one minimizes the expected user path length and the other maximizes the expected reduction in user path lengths. these results improve over a constant factor approximation for the expected length and over a ptas for the expected reduction, both obtained recently in jacobs . indeed, these fptas's are essentially the best possible results one can achieve under the assumption that p not equal np. another contribution we give here is a 16-approximation algorithm for the most general version of the problem where up to k hotlinks can be assigned from each node. this algorithm runs in o(vertical bar v vertical bar log vertical bar v vertical bar) time and it turns to be the first algorithm with constant approximation for this problem.",
    "present_kp": [
      "hotlink assignment"
    ],
    "absent_kp": [
      "approximation algorithms"
    ]
  },
  {
    "title": "skie: a heterogeneous environment for hpc applications.",
    "abstract": "technological directions for innovative hpc software environments are discussed in this paper. we focus on industrial user requirements of heterogeneous multidisciplinary applications, performance portability, rapid prototyping and software reuse, integration and interoperability of standard tools. the various issues are demonstrated with reference to the pqe2000project and its programming environment skeleton-based integrated environment (skie). skieincludes a coordination language, skiecl, allowing the designers to express, in a primitive and structured way, efficient combinations of data parallelism and task parallelism. the goal is achieving fast development and good efficiency for applications in different areas. modules developed with standard languages and tools are encapsulated into skieclstructures to form the global application. performance models associated to the coordination language allow powerful optimizations to be introduced both at run time and at compile time without the direct intervention of the programmer. the paper also discusses the features of the skieenvironment related to debugging, performance analysis tools, visualization and graphical user interface. a discussion of the results achieved in some applications developed using the environment concludes the paper.",
    "present_kp": [],
    "absent_kp": [
      "parallel programming environments",
      "parallel programming models",
      "structured parallel programming"
    ]
  },
  {
    "title": "hybridization of volumetric and surface models for the computation of the t/r ec probe response due to a thin opening flaw.",
    "abstract": "purpose - the purpose of this paper is to describe the development of simulation tools dedicated to eddy current non destructive testing (ecndt) on planar structures implying planar defects. two integral approaches using the green dyadic formalism are considered. design/methodology/approach - the surface integral model (sew) is dedicated to ideal cracks, whereas the volume integral method is adapted to general volumetric defects. findings - the authors observed that sea provides satisfactory results, except in some critical transmitting/receiving (t/r) configurations. this led us to propose a hybrid method based on the combination of the two previous ones. originality/value - this method enables to simulate ecndt on planar structures implying defects with a small opening using t/r probes.",
    "present_kp": [],
    "absent_kp": [
      "eddy currents",
      "tests and testing",
      "surface defects",
      "integral equations"
    ]
  },
  {
    "title": "a new shared segment protection method for survivable networks with guaranteed recovery time.",
    "abstract": "shared segment protection (ssp), compared with shared path protection (spp), and shared link protection (slp), provides an optimal protection configuration due to the ability of maximizing spare capacity sharing, and reducing the restoration time in cases of a single link failure. this paper provides a thorough study on ssp under the gmpls-based recovery framework, where an effective survivable routing algorithm for ssp is proposed. the tradeoff between the price (i.e., cost representing the amount of resources, and the blocking probability), and the restoration time is extensively studied by simulations on three networks with highly dynamic traffic. we demonstrate that the proposed survivable routing algorithm can be a powerful solution for meeting stringent delay upper bounds for achieving high restorability of transport services. this can significantly improve the network reliability, and enable more advanced, mission critical services in the networks. the comparison among the three protection types further verifies that the proposed scheme can yield significant advantages over shared path protection, and shared link protection.",
    "present_kp": [],
    "absent_kp": [
      "complete routing information scenario",
      "integer linear program ",
      "segment shared protection ",
      "shared risk group ",
      "single failure scenario",
      "working and protection paths"
    ]
  },
  {
    "title": "providing end-to-end service level agreements across multiple isp networks.",
    "abstract": "due to the autonomous nature of isps, the service level agreement (sla) offering is currently confined to within a single provider network. in this work, we examine some methods of extending the sla offering across isp boundaries. we introduce three policies to coordinate the end-to-end performance guarantee in multiple isp networks: the least-effort, the most-effort, and the equal-distribution policies. these policies refer to different manners in which the service-level constraints are distributed among all transit networks. we study the impacts of these policies on the overall isp community when sla is required. we evaluate the effectiveness of these policies in terms of both the network performance and the isps monetary profit. the results show that the policy choice depends on the network load, as well as the isp cost structure.",
    "present_kp": [
      "service level agreement"
    ],
    "absent_kp": [
      "provisioning",
      "protection",
      "ip-over-wdm network"
    ]
  },
  {
    "title": "a spatially explicit methodology for a priori estimation of field survey effort in environmental observation networks.",
    "abstract": "when establishing environmental monitoring programmes, it crucial to make reliable cost estimates, especially where a field survey is involved. this paper presents a methodology for creating a spatial measure of a field survey effort (se). a set of relevant variables affecting a se (e.g. areas with rough terrain, or distant from the main road network) was classified using fuzzy sets and then combined to produce spatially explicit effort indicators, which were integrated to a single measure using an analytic hierarchy process (ahp). to evaluate this approach and identify the limits for its application, three spatially nested case studies were used to test the spatial expression of se and the scalable capacity of the method itself. the presented methodology could cope with variations in the scale and data resolution, retrieving a coherent estimate of se across the different case studies. the presented methodology is therefore useful for (i) testing the network designs for sampling bias related to se, (ii) comparing alternative sampling designs, (iii) assessing the sampling costs and (iv) supporting the human and logistical resource management.",
    "present_kp": [
      "environmental monitoring"
    ],
    "absent_kp": [
      "ecological field survey",
      "monitoring effort",
      "logistic optimization"
    ]
  },
  {
    "title": "chester: towards a personal medication advisor.",
    "abstract": "dialogue systems for health communication hold out the promise of providing intelligent assistance to patients through natural interfaces that require no training to use. but in order to make the development of such systems cost effective, we must be able to use generic techniques and components which are then specialized as needed to the specific health problem and patient population. in this paper, we describe chester, a prototype intelligent assistant that interacts with its user via conversational natural spoken language to provide them with information and advice regarding their prescribed medications. chester builds on our prior experience constructing conversational assistants in other domains. the emphasis of this paper is on the portability of our generic spoken dialogue technology, and presents a case study of the application of these techniques to the development of a dialogue system for health communication.",
    "present_kp": [],
    "absent_kp": [
      "spoken dialogue system",
      "personal medical advisor",
      "prescription compliance",
      "artificial intelligence"
    ]
  },
  {
    "title": "computational simulation for the morphological evolution of nonaqueous phase liquid dissolution fronts in two-dimensional fluid-saturated porous media.",
    "abstract": "this paper deals with the computational aspects of nonaqueous phase liquid (napl) dissolution front instability in two-dimensional fluid-saturated porous media of finite domains. after the governing equations of an napl dissolution system are briefly described, a combination of the finite element and finite difference methods is proposed to solve these equations. in the proposed numerical procedure, the finite difference method is used to discretize time, while the finite element method is used to discretize space. two benchmark problems, for which either analytical results or previous solutions are available, are used to verify the proposed numerical procedure. the related simulation results from these two benchmark problems have demonstrated that the proposed numerical procedure is useful and applicable for simulating the morphological evolution of napl dissolution fronts in two-dimensional fluid-saturated porous media of finite domains. as an application, the proposed numerical procedure has been used to simulate morphological evolution processes for three kinds of napl dissolution fronts in supercritical napl dissolution systems. it has been recognized that: (1) if the zhao number of an napl dissolution system is in the lower range of the supercritical zhao numbers, the fundamental mode is predominant; (2) if the zhao number is in the middle range of the supercritical zhao numbers, the (normal) fingering mode is the predominant pattern of the napl dissolution front; and (3) if the zhao number is in the higher range of the supercritical zhao numbers, the fractal mode is predominant for the napl dissolution front.",
    "present_kp": [
      "nonaqueous phase liquid",
      "computational simulation",
      "fingering mode",
      "fractal mode",
      "porous media"
    ],
    "absent_kp": [
      "residual saturation"
    ]
  },
  {
    "title": "mobile agents for mobile tourists: a user evaluation of gulliver's genie.",
    "abstract": "how mobile computing applications and services may be best designed, implemented and deployed remains the subject of much research. one alternative approach to developing software for mobile users that is receiving increasing attention from the research community is that of one based on intelligent agents. recent advances in mobile computing technology have made such an approach feasible. we present an overview of the design and implementation of an archetypical mobile computing application, namely that of an electronic tourist guide. this guide is unique in that it comprises a suite of intelligent agents that conform to the strong intentional stance. however, the focus of this paper is primarily concerned with the results of detailed user evaluations conducted on this system. within the literature, comprehensive evaluations of mobile context-sensitive systems are sparse and therefore, this paper seeks, in part, to address this deficiency.",
    "present_kp": [
      "user evaluation",
      "mobile computing",
      "intelligent agents"
    ],
    "absent_kp": [
      "context-sensitive service delivery"
    ]
  },
  {
    "title": "a study on the design of vme system controller.",
    "abstract": "for fa (factory automation) and ate (automatic test equipment) in the industrial area, the standard bus is required to increase the system performance of multiprocessor environment. vme (versa module european package format) bus is appropriated to the standard bus but has the features that is the small of package and the low density of board. beside, the density of board and semiconductor have grown to become a significant issues that affect the development time, project cost and field diagnostics. to fit this trend, in this paper, the author composed revision c.1 (ieee std. p1014-1987) of the integrated environment for the main function such as arbitration, interrupt and interface between vmebus and several control modules. also the designed vme system controller is implemented on fpga that can be located even into slot 1. the control and function modules are coded with vhdl mid-fixed description method and then those operations are verified by simulation. as a result of experiment, the author confirmed that the most important about the operation of bus timer that bus error signal should occur within 56 mu s, and both control and function modules have the reciprocal operation correctly. thus, the constructed vhdl library will be able to apply the system based vmebus and asic design.",
    "present_kp": [
      "factory automation",
      "vme system controller",
      "vhdl",
      "fpga"
    ],
    "absent_kp": []
  },
  {
    "title": "untangling a planar graph.",
    "abstract": "a straight-line drawing delta of a planar graph g need not be plane but can be made so by untangling it, that is, by moving some of the vertices of g. let shift(g, delta) denote the minimum number of vertices that need to be moved to untangle d. we show that shift(g, delta) is np-hard to compute and to approximate. our hardness results extend to a version of 1bendpointsetembeddability, a well-known graph-drawing problem. further we define fix(g, delta) = n - shift(g, delta) to be the maximum number of vertices of a planar n-vertex graph g that can be fixed when untangling d. we give an algorithm that fixes at least root((log n) - 1)/log log n vertices when untangling a drawing of an n-vertex graph g. if g is outerplanar, the same algorithm fixes at least root n/2 vertices. on the other hand, we construct, for arbitrarily large n, an n-vertex planar graph g and a drawing delta(g) of g with fix(g, delta(g)) <= root n - 2 + 1 and an n-vertex outerplanar graph h and a drawing delta(h) of h with fix(h, delta(h)) <= 2 root n - 1 + 1. thus our algorithm is asymptotically worst-case optimal for outerplanar graphs.",
    "present_kp": [
      "straight-line drawing",
      "untangling"
    ],
    "absent_kp": [
      "graph drawing",
      "planarity",
      "np-hardness",
      "hardness of approximation",
      "moving vertices",
      "point-set embeddability"
    ]
  },
  {
    "title": "evaluating the performance of a discrete manufacturing process using rfid: a case study.",
    "abstract": "we use an enhanced rfid system within a manufacturers shop floor. we evaluate the workflow performance of a work layout redesign. rfid data analysis through kpis indicates bottleneck operations. rfid middleware development improves data aggregation. improving workflow visibility advances manufacturing flexibility.",
    "present_kp": [
      "rfid",
      "middleware development"
    ],
    "absent_kp": [
      "rapid manufacturing",
      "material flow",
      "real-time location system",
      "workflow layout"
    ]
  },
  {
    "title": "an exploratory model of knowledge flow barriers within healthcare organizations.",
    "abstract": "healthcare today is mainly knowledge-based and the diffusion of medical knowledge is imperative for proper treatment of patients. our study of the industry explored barriers to knowledge flow using a cultural historical activity theory framework. our work was exploratory and qualitative in nature, and consisted of three phases: in-depth interviews to explore medical knowledge flow barriers resulting in a model; a case study using a survey approach to test and modify the model; and a delphi study to validate the generalizability of the model. we concluded that knowledge flow experienced five barriers: knowledge source, knowledge receiver, knowledge transfer, knowledge flow context, and the organizational context. furthermore, these were correlated.",
    "present_kp": [
      "knowledge flow barrier",
      "medical knowledge",
      "cultural historical activity theory ",
      "case study"
    ],
    "absent_kp": [
      "qualitative approach"
    ]
  },
  {
    "title": "analysis of actinomycin d-dna model complexes using a quantum-chemical criterion: mulliken overlap populations.",
    "abstract": "the binding of the antitumoral drug actinomycin d to single- and double-stranded dna was investigated using molecular modeling in the frame of mm+ molecular mechanics and ami semi-empirical method. two other programs, especially conceived to analyze hydrogen-bonding patterns in biological macromolecules, hbexplore, based on geometrical criteria and shb_interactions, based on quantum-chemical criteria (mulliken overlap populations), were also used. the results account for the non-cooperative intercalative binding process previously investigated, and outline the contribution of specific hydrogen bonding as well as c-(ho)-o-...(n) and other atom-atom intermolecular interactions to the stabilization of the actinomycin d-dna complexes. they also support the hemi-intercalation model proposed in literature for the actinomycin d-ssdna complex.",
    "present_kp": [
      "actinomycin d",
      "shb_interactions",
      "overlap populations"
    ],
    "absent_kp": [
      "nucleic acids",
      "hydrogen bonds"
    ]
  },
  {
    "title": "customization of user interfaces to reduce errors and enhance user acceptance.",
    "abstract": "reconfiguration allows customizing a user interface according to own preferences. reconfiguration reduced error rates and enhanced user acceptance. promising approach that warrants further exploration.",
    "present_kp": [
      "reconfiguration",
      "customization",
      "user acceptance"
    ],
    "absent_kp": [
      "process control"
    ]
  },
  {
    "title": "cxc chemokines: angiogenesis, immunoangiostasis, and metastases in lung cancer.",
    "abstract": "cxc chemokines have been found to be important in the regulation of angiogenesis and tumor-related immunity, and in promoting organ-specific metastases. this review highlights the importance of cxc chemokine ligands and receptors in mediating non-small cell lung cancer tumor-associated angiogenesis, immunoangiostasis, and organ-specific metastases. these findings may ultimately lead to clinical strategies to target cxc chemokines in non-small cell lung cancer.",
    "present_kp": [
      "cxc chemokines",
      "angiogenesis"
    ],
    "absent_kp": [
      "tumor growth",
      "tumor metastases"
    ]
  },
  {
    "title": "an efficient approach for the rank aggregation problem.",
    "abstract": "this paper presents some computational properties of the rank-distance, a measure of similarity between partial rankings. we show how this distance generalizes the spearman footrule distance, preserving its good computational complexity: the rank-distance between two partial rankings can be computed in linear time, and the rank aggregation problem can be solved in polynomial time. further, we present a generalization of the rank-distance to strings, which permits to solve the median string problem in polynomial time. this appears rather surprising to us given the fact that for other non-trivial string distances, such as edit-distance, this problem is np-hard.",
    "present_kp": [
      "ranking",
      "rank aggregation problem",
      "string distance"
    ],
    "absent_kp": [
      "assignment problem",
      "bipartite matching of given cardinality and weight"
    ]
  },
  {
    "title": "a phased reuse adoption model.",
    "abstract": "reuse is easy to understand but challenging to institute. instituting reuse can be made easier by a reuse adoption model. the software productivity consortium's reuse adoption model has been used widely and successfully in the five years of its existence. however, time and experience have suggested ways to improve the model. this paper describes the consortium's new model. whereas the old model was process-based and independent of a particular reuse methodology, the new model is phased and integrates the consortium's synthesis methodology. this paper discusses the old model and how it motivated the phased model. it then presents the phased model and describes experiences with it.",
    "present_kp": [
      "reuse adoption"
    ],
    "absent_kp": [
      "software reuse",
      "software management",
      "domain engineering"
    ]
  },
  {
    "title": "adaptive frequency decomposition of eeg with subsequent expert system analysis.",
    "abstract": "we present a hybrid system for automatic analysis of clinical routine eeg, comprising a spectral analysis and an expert system. eeg raw data are transformed into the timefrequency domain by the so-called adaptive frequency decomposition. the resulting frequency components are converted into pseudo-linguistic facts via fuzzification. finally, an expert system applies symbolic rules formulated by the neurologist to evaluate the extracted eeg features. the system detects artefacts, describes alpha rhythm by frequency, amplitude, and stability and after artefact rejection detects pathologic slow activity. all results are displayed as linguistic terms, numerical values and maps of temporal extent, giving an overview about the clinical routine eeg.",
    "present_kp": [
      "eeg",
      "spectral analysis",
      "expert system"
    ],
    "absent_kp": [
      "fuzzy logic"
    ]
  },
  {
    "title": "scheduling light-trails on wdm rings.",
    "abstract": "we consider the problem of scheduling communication on optical wdm (wavelength division multiplexing) networks using the light-trails technology. we seek to design scheduling algorithms such that the given transmission requests can be scheduled using a minimum number of wavelengths (optical channels). we provide algorithms and close lower bounds for two versions of the problem on an n processor linear array/ring network. in the stationary version, the pattern of transmissions (given) is assumed to not change over time. for this, a simple lower bound is c, the congestion or the maximum total traffic required to pass through any link. we give an algorithm that schedules the transmissions using o(c + log n) wavelengths. we also show a pattern for which omega (c + log n/ log log n) wavelengths are needed. in the on-line version, the transmissions arrive and depart dynamically, and must be scheduled without upsetting the previously scheduled transmissions. for this case we give an on-line algorithm which has competitive ratio theta (log n). we show that this is optimal in the sense that every on-line algorithm must have competitive ratio omega(log n). we also give an algorithm that appears to do well in simulations (for the classes of traffic we consider), but which has competitive ratio between omega(log(2) n/ log log n) and o(log(2) n). we present detailed simulations of both our algorithms.",
    "present_kp": [
      "wavelength division multiplexing",
      "wdm",
      "light-trail",
      "scheduling",
      "on-line",
      "algorithm",
      "simulation"
    ],
    "absent_kp": [
      "reconfigurable bus architecture",
      "routing",
      "wavelength assignment",
      "approximation"
    ]
  },
  {
    "title": "dynamic circuit provisioning in all-optical wdm networks using lightpath switching.",
    "abstract": "in this paper we investigate the problem of provisioning holding-time-aware (hta) dynamic circuits in all-optical wavelength division multiplexed (wdm) networks. we employ a technique called lightpath switching (lps) wherein the data transmission may begin on one lightpath and switch to a different lightpath at a later time. lightpath switches are transparent to the user and are managed by the network. allowing lps creates a number of segments that can use independent lightpaths. we first compare the performance of traditional routing and wavelength (rwa) assignment to routing and wavelength assignment with lps. we show that lps can significantly reduce blocking compared to traditional rwa. we then address the problem of routing dynamic anycast hta dynamic circuits. we propose two heuristics to solve the anycast rwa problem: anycast with continuous segment (acs) and anycast with lightpath switching (alps). in alps we exercise lps, and provision a connection request by searching for the best candidate destination node is such a way that the network resources are utilized efficiently. in acs we do not allow a connection request to switch lightpaths. the lightpaths to each candidate destination node of a request are computed using traditional rwa algorithms. we first compare the performance of acs to alps and observe that alps achieves better blocking than acs. furthermore, we also compare the performance of these two anycast rwa algorithms to the traditional unicast rwa algorithm. we show that the anycast rwa algorithms presented here significantly outperform the traditional unicast rwa algorithms.",
    "present_kp": [
      "lightpath switching",
      "anycast",
      "rwa",
      "wdm"
    ],
    "absent_kp": [
      "holding-time-aware  lightpaths"
    ]
  },
  {
    "title": "difficulties in the neuroscience of creativity: jazz improvisation and the scientific method.",
    "abstract": "creativity is a fundamental and remarkable human capacity, yet the scientific study of creativity has been limited by the difficulty of reconciling the scientific method and creative processes. we outline several hurdles and considerations that should be addressed when studying the cognitive neuroscience of creativity and suggest that jazz improvisation may be one of the most useful experimental models for the study of spontaneous creativity. more broadly, we argue that studying creativity in a way that is both scientifically and ecologically valid requires collaboration between neuroscientists and artists.",
    "present_kp": [
      "creativity",
      "neuroscience",
      "improvisation",
      "jazz"
    ],
    "absent_kp": []
  },
  {
    "title": "unsupervised language model adaptation for handwritten chinese text recognition.",
    "abstract": "we propose an unsupervised language model (lm) adaptation framework for handwritten chinese text recognition. we use a two-pass recognition strategy with a pre-defined multi-domain lm set. the adaptive lm is dynamically generated by three methods of model selection, model combination and model reconstruction. we compress the lm set by split vector quantization and principal component analysis.",
    "present_kp": [
      "unsupervised language model adaptation"
    ],
    "absent_kp": [
      "character string recognition",
      "chinese handwriting recognition",
      "language model compression"
    ]
  },
  {
    "title": "strong convergence for a countable family of strict pseudocontractions in q-uniformly smooth banach spaces.",
    "abstract": "we introduce a new iterative algorithm for finding a common fixed point of a countable family of strict pseudocontractions in q-uniformly smooth and uniformly convex banach spaces. we then prove that the sequence generated by the proposed algorithm converges strongly to a common fixed point of an infinite family of strict pseudocontractions. our results mainly improve and extend the results announced by yao et al. .",
    "present_kp": [
      "iterative algorithm",
      "q-uniformly smooth banach spaces",
      "strong convergence",
      "strict pseudocontractions"
    ],
    "absent_kp": [
      "common fixed points"
    ]
  },
  {
    "title": "interval-valued intuitionistic fuzzy qualiflex method with a likelihood-based comparison approach for multiple criteria decision analysis.",
    "abstract": "qualiflex (i.e., qualitative flexible multiple criteria method) is a useful outranking method used for multiple criteria decision analysis. this paper uses the main structure of qualiflex to develop an interval-valued intuitionistic fuzzy qualiflex outranking method with a likelihood-based comparison approach for handling multiple criteria decision-making problems within a decision environment of interval-valued intuitionistic fuzzy sets. we propose the concept of using the likelihood of fuzzy preference relations to compare interval-valued intuitionistic fuzzy numbers. to address diversiform preference types, we represent the decision-makers various forms of preference structures and assess the criterion weights using incomplete information. using a criterion-wise preference of alternatives via comparison of the likelihoods, we develop a new qualiflex-based model to measure the level of concordance of the complete preference order for managing multiple criteria decisions. the feasibility and applicability of the proposed methods are illustrated using a practical example, namely, the selection of a suitable bridge construction method. a comparative analysis with other relevant methods is conducted to validate the effectiveness of the proposed methodology.",
    "present_kp": [
      "qualiflex",
      "outranking method",
      "multiple criteria decision analysis",
      "likelihood",
      "interval-valued intuitionistic fuzzy set",
      "incomplete information"
    ],
    "absent_kp": []
  },
  {
    "title": "extracting grammar from programs: evolutionary approach.",
    "abstract": "the paper discusses context-free grammar (cfg) inference using genetic-programming with application to inducing grammars from programs written in simple domain-specific languages. grammar-specific heuristic operators and non-random construction of the initial population are proposed to achieve this task. suitability of the approach is shown by small examples where the underlying cfg's are successfully inferred.",
    "present_kp": [],
    "absent_kp": [
      "grammar induction",
      "grammar inference",
      "learning from positive and negative examples",
      "genetic programming"
    ]
  },
  {
    "title": "numerical simulation of spherical couette flow.",
    "abstract": "a spectral method is used to study numerically flow between two concentric rotating spheres. analytical expressions of eigenfunctions of a stokes operator in the spherical gap region are presented, numerical experiments are carried out by applying these expressions to the spectral approximation of spherical couette flow, and numerical results are given.",
    "present_kp": [
      "stokes operator"
    ],
    "absent_kp": [
      "navier-stokes equations",
      "spherical-couette flow"
    ]
  },
  {
    "title": "players clustering based on graph theory tor tactics analysis purpose in soccer videos.",
    "abstract": "in this paper, a new method for clustering of players in order to analyze games in soccer videos is proposed. the proposed method classifies players who are closely related in terms of soccer tactics into one group. considering soccer tactics, the players in one group are located near each other. for this reason, the euclidean distance between the players is an effective measurement for the clustering of players. however, the distance is not sufficient to extract tactics-based groups. therefore, we utilize a modified version of the community extraction method, which finds community structure by dividing a non-directed graph. the use of this method in addition to the distance enables accurate clustering of players.",
    "present_kp": [
      "soccer videos",
      "soccer tactics",
      "clustering",
      "community"
    ],
    "absent_kp": []
  },
  {
    "title": "selection of load-transfer functions for passive lateral loading of pile groups.",
    "abstract": "a finite element study has been carried out to determine the pile-soil-pile interaction behaviour for closely spaced pile rows and groups under passive lateral loading from soil movement. a horizontal section close to the piles was studied to determine the effects of pile spacing and soil constitutive law on the load-transfer relationships of the piles. the study has revealed links between the soil stress-strain law, the soil deformation mechanism and the pile load-transfer curves. interaction behaviour was seen to depend on the prevailing deformation mechanism which in turn was governed by the soil constitutive law. elastic-plastic and power law soil models were applied. interaction factors suitable for design use to account for increasing lateral pressure on piles during passive lateral loading have been produced for a range of pile spacings and power law soil exponents. interaction between piles increased both with reduction of pile spacing and with increase of soil exponent-the less soil stiffness degradation with shear strain, the greater the interaction between piles at a given spacing. this suggests that the passive interaction factors calculated using elastic methods are likely to overestimate the effects of pile-soil-pile interaction. :",
    "present_kp": [
      "pile-soil-pile interaction",
      "passive interaction factors"
    ],
    "absent_kp": [
      "passive lateral pile group loading",
      "soil deformation mechanisms",
      "finite element analysis"
    ]
  },
  {
    "title": "the generation of arbitrary order, non-classical, gauss-type quadrature for transport applications.",
    "abstract": "a method is presented, based upon the stieltjes method (1884), for the determination of non-classical gauss-type quadrature rules, and the associated sets of abscissae and weights. the method is then used to generate a number of quadrature sets, to arbitrary order, which are primarily aimed at deterministic transport calculations. the quadrature rules and sets detailed include arbitrary order reproductions of those presented by abu-shumays in [4]and[8] (known as the qr sets, but labelled qra here), in addition to a number of new rules and associated sets; these are generated in a similar way, and we label them the qrs quadrature sets. the method presented here shifts the inherent difficulty (encountered by abu-shumays) associated with solving the non-linear moment equations, particular to the required quadrature rule, to one of the determination of non-classical weight functions and the subsequent calculation of various associated inner products. once a quadrature rule has been written in a standard form, with an associated weight function having been identified, the calculation of the required inner products is achieved using specific variable transformations, in addition to the use of rapid, highly accurate quadrature suited to this purpose. the associated non-classical gauss quadrature sets can then be determined, and this can be done to any order very rapidly. in this paper, instead of listing weights and abscissae for the different quadrature sets detailed (of which there are a number), the matlab code written to generate them is included as appendix?d. the accuracy and efficacy (in a transport setting) of the quadrature sets presented is not tested in this paper (although the accuracy of the qra quadrature sets has been studied in [12]and[13]), but comparisons to tabulated results listed in [8] are made. when comparisons are made with one of the azimuthal qra sets detailed in [8], the inherent difficulty in the method of generation, used there, becomes apparent, with the highest order tabulated sets showing unexpected anomalies. although not in an actual transport setting, the accuracy of the sets presented here is assessed to some extent, by using them to approximate integrals (over an octant of the unit sphere) of various high order spherical harmonics. when this is done, errors in the tabulated qra sets present themselves at the highest tabulated orders, whilst combinations of the new qrs quadrature sets offer some improvements in accuracy over the original qra sets. finally, in order to offer a quick, visual understanding of the various quadrature sets presented, when combined to give product sets for the purposes of integrating functions confined to the surface of a sphere, three-dimensional representations of points located on an octant of the unit sphere (as in [8]and[12]) are shown.",
    "present_kp": [
      "qr set"
    ],
    "absent_kp": [
      "particle transport",
      "discrete ordinates",
      "numerical integration",
      "non-classical quadrature",
      "qrs set",
      "octant-range",
      "orthogonal polynomial"
    ]
  },
  {
    "title": "agent-based workflow management for rfid-enabled real-time reconfigurable manufacturing.",
    "abstract": "recent developments in wireless technologies have created opportunities for developing reconfigurable wireless manufacturing systems with real-time traceability, visibility and interoperability in shop-floor planning, execution and control. this paper proposes to use agent-based workflow management as a mechanism to facilitate interactions among rfid-enabled reconfigurable manufacturing resources. a production process is modelled as a workflow network. its nodes correspond to the work (process), and its edges to flows of control and data. nodes are represented as agents and edges as messages. as a sandwich layer, agents wrap manufacturing services around a work-cell and their operational logics/intelligence for cost-effectively collecting and processing real-time manufacturing data, forming so-called work-cell gateways. a reference framework for a shop-floor gateway is proposed based on the three key components: workflow management, manufacturing services universal description, discovery and integration (namely ms-uddi) and work-cell agents. work-cell agents are packaged, registered and published at ms-uddi as web services which are easily reused and reconfigured in the workflow for a specific production process. finally, a prototype system is presented to demonstrate how the proposed method is used to define and execute a real-time reconfigurable manufacturing project.",
    "present_kp": [
      "workflow management",
      "reconfigurable manufacturing",
      "rfid"
    ],
    "absent_kp": [
      "multiple agent systems",
      "real-time wireless manufacturing",
      "auto id"
    ]
  },
  {
    "title": "an easygrid portal for scheduling system-aware applications on computational grids.",
    "abstract": "one of the objectives of computational grids is to offer applications the collective computational power of distributed but typically shared heterogeneous resources. unfortunately, efficiently harnessing the performance potential of such systems (i.e. how and where applications should execute on the grid) is a challenging endeavor due principally to the very distributed, shared and heterogeneous nature of the resources involved. a crucial step towards solving this problem is the need to identify both an appropriate scheduling model and scheduling algorithm(s). this paper presents a tool to aid the design and evaluation of scheduling policies suitable for efficient execution of system-aware parallel applications on computational grids.",
    "present_kp": [],
    "absent_kp": [
      "grid computing",
      "task scheduling",
      "simulation",
      "modeling",
      "globus toolkit"
    ]
  },
  {
    "title": "reliable framework for rfid devices.",
    "abstract": "rfid technology requires deployment that ensures reliability. we have developed the middleware rf 2 id, a reliable framework for rfid, to improve reliability at software level. two key abstractions 1) virtual reader: the distributed computational element 2) virtual path: the communication channel among virtual readers are used to improve the system reliability. the key idea is to use the notion of path at system level that matches the flow of data stream to improve system efficiency. we have designed the middleware solution rf 2 id and evaluated it using experimental testbed and emulation based study for item tracking applications. the evaluation of the system provides higher accuracy level using a scalable solution. we plan to further extend the system to provide support for item location applications as well.",
    "present_kp": [
      "item tracking"
    ],
    "absent_kp": [
      "rfid middleware",
      "path based system"
    ]
  },
  {
    "title": "local, distributed weighted matching on general and wireless topologies.",
    "abstract": "in this paper, we present and discuss a distributed algorithm for the local message passing communication model that constructs a (1-?)-approximate maximum weight matching in a graph (? > 0). the approach has a deterministic runtime of o(1/? 2 log n. t mis (n o(1/?) )), where t mis (m) denotes the distributed time of computing a maximal independent set on a graph with m nodes. an immediate result of the presented approach is a local algorithm with expected o(log 2 n) time complexity. if the given graph stems from a wireless ad-hoc network, characterized by the bounded growth property, we can further improve the algorithm by a preprocessing step that first locally constructs a colored clustering structure. this structure is then used in the matching algorithm to speed up the procedure significantly; we obtain a deterministic local algorithm that requires o(log n log* n) communication rounds. the main part of the algorithm works by repeatedly modifying and improving an existing matching in the network, and can thus be easily adapted to cope with changing and mobile topologies.",
    "present_kp": [
      "local algorithm",
      "maximum weight matching"
    ],
    "absent_kp": [
      "approximation"
    ]
  },
  {
    "title": "beam-forming module for backhaul link in a relay-aided 4g network.",
    "abstract": "a novel beam-forming module based on wilkinson power divider technology, including attenuators and phase shifter chips is designed, fabricated and evaluated to be incorporated in a relay station connecting it with the base station under a 4g network. the proposed module is a 1:8 port circuit, utilizing two substrates, providing approximately 700mhz bandwidth over 3.5ghz frequency band and less than ?20db transmission line coupling. moreover an external control unit that feeds the beam-forming module with code-words that define the proper amplitude/phase of the excitation currents is established and described. the presented module is connected to a planar array and tested for two beam-forming scenarios, providing satisfactory radiation patterns.",
    "present_kp": [
      "beam-forming",
      "relay station",
      "wilkinson power divider",
      "attenuators"
    ],
    "absent_kp": [
      "phase shifters"
    ]
  },
  {
    "title": "industrial safety perception among post-graduate engineering students.",
    "abstract": "this paper describes industrial safe perception among university of aberdeen post-graduate engineering student. the immediate objective of this research is to identify safety perceptions of different high risk occupational industries. results obtained were analyzed and compared with health and safety executives (hse) reports as well as the oil and gas producer safety performance report. the result showed that the participants perceive the oil and gas industry is safe, second only to the aviation industry and that the oil and gas industry is safest in the european region. the research also concluded that participants perceptions were more influenced towards the concept of accident severity/dread rather than the concept of accident probability.",
    "present_kp": [
      "health and safety"
    ],
    "absent_kp": [
      "risk perception",
      "occupational risk",
      "questionnaire",
      "industrial risk"
    ]
  },
  {
    "title": "computer systems analysis of spaceflight induced changes in left ventricular mass.",
    "abstract": "circulatory adaptations resulting in postflight orthostasis have frequently been observed in response to space travel. it has been postulated that a decrement in left ventricular mass (lvm) found after microgravity exposure may be the central component in this cardiovascular deconditioning. however, a physiologic mechanism responsible for these changes in the myocardium has not been determined. in this study, we examined the sequential alterations in echocardiographic measured lvm from preflight to landing day and 3 days into the postflight recovery period. in a previous study in returning astronauts we found a comparative 9.1% reduction in postflight lvm that returned to preflight values by the third day of recovery. this data was further evaluated in a systems analysis approach using a well-established advanced computer model of circulatory functioning. the computer model incorporates the physiologic responses to changes in pressures, flows and hydraulics within the circulatory system as affected by gravitational forces. myocardial muscle progression to atrophy or hypertrophy in reaction to the circulatory load conditions is also included in the model. the integrative computer analysis suggests that these variations in lvm could be explained by simple fluid shifts known to occur during spaceflight and can reverse within a few days after reentry into earth's gravity. according to model predictions, the reductions in lvm found upon exposure to microgravity are a result of a contraction of the myocardial interstitial fluid space secondary to a loss in the plasma volume. this hypothesis was additionally supported by the published ground-based study in which we followed the alterations in lvm and plasma volume in normal subjects in which hypovolemia was induced by simple dehydration. in the hypovolemic state, plasma volume was reduced in these subjects and was significantly correlated with echocardiographic measurements of lvm. based on these experimental findings and the performance of the computer systems analysis it appears that reductions in lvm observed after spaceflight may be secondary to fluid exchanges produced by common physiologic mechanisms. reductions in lvm observed after microgravity exposure have been previously postulated to be a central component of spaceflight-induced cardiovascular deconditioning. however, a recent study has demonstrated a return of astronauts lvm to preflight values by the third day after landing through uncertain mechanisms. a systems analysis approach using computer simulation techniques allows for a dissection of the complex physiologic control processes and a more detailed examination of the phenomena. from the simulation studies and computer analysis it appears that microgravity induced reductions in lvm may be explained by considering physiologic fluid exchanges rather than cardiac muscle atrophy.",
    "present_kp": [
      "computer model",
      "spaceflight",
      "microgravity",
      "astronaut"
    ],
    "absent_kp": [
      "system analysis",
      "venticular mass"
    ]
  },
  {
    "title": "transmitting to colocated users in wireless ad hoc and sensor networks.",
    "abstract": "we consider wireless ad hoc networks and sensor networks where a remotely located source is transmitting information to a destined user embedded within a group of k densely packed physically colocated users enjoying favorable signal-to-noise ratio (snr) conditions among themselves, but suffering from quasi-static flat rayleigh fading with respect to the source. stringent delay constraints require that information, once available, be transmitted immediately and delivered reliably to its destination during a period of one fading block, precluding waiting until the destined user enjoys favorable fading conditions with respect to the source. a cooperative transmission strategy is proposed for this scenario and its expected throughput is investigated. the strategy exhibits a substantial gain in throughput, especially when the colocation gain factor is high. in addition, a broadcast approach is incorporated into the transmission strategy suggesting further throughput benefits.",
    "present_kp": [
      "ad hoc networks",
      "expected throughput",
      "sensor networks"
    ],
    "absent_kp": [
      "cooperative diversity",
      "fading channels",
      "outage capacity",
      "relay channel",
      "wireless networks"
    ]
  },
  {
    "title": "a token-bucket based notification traffic control mechanism for ims presence service.",
    "abstract": "presence is a service that allows a user to be informed about the specified state of another user. presence service has become a key enabler for next-generation applications such as instant messaging, push-to-talk and web2.0. however, recent studies show that the notification traffic of presence service causes heavy signaling load on ip multimedia subsystem (ims) network. this paper introduced a token-bucket based notification traffic control (tntc) mechanism, which is an application layer solution deployed at the presence server. the tntc aims at upgrading valid access probability while controlling the notification traffic. a mathematical model of a queuing system is proposed to describe tntc. we analyzed its main probability features and investigated the effects of different parameters on the performance of tntc. extensive simulations verified that tntc can effectively control notification traffic and perform better than the existing schemes in terms of valid access probability and update arrival rate.",
    "present_kp": [
      "notification traffic control",
      "presence service",
      "ims"
    ],
    "absent_kp": [
      "delayed update"
    ]
  },
  {
    "title": "maximum likelihood estimation for filtering thresholds.",
    "abstract": "information filtering systems based on statistical retrieval models usually compute a numeric score indicating how well each document matches each profile. documents with scores above profile-specific dissemination thresholds are delivered. an optimal dissemination threshold is one that maximizes a given utility function based on the distributions of the scores of relevant and non-relevant documents. the parameters of the distribution can be estimated using relevance information, but relevance information obtained while filtering is biased . this paper presents a new method of adjusting dissemination thresholds that explicitly models and compensates for this bias. the new algorithm, which is based on the maximum likelihood principle, jointly estimates the parameters of the density distributions for relevant and non-relevant documents and the ratio of the relevant document in the corpus. experiments with trec-8 and trec-9 filtering track data demonstrate the effectiveness of the algorithm.",
    "present_kp": [
      "utility function",
      "estimates",
      "relevance",
      "bias",
      "paper",
      "model",
      "filtering",
      "threshold",
      "method",
      "systems",
      "data",
      "demonstrate",
      "algorithm",
      "effect",
      "retrieval model",
      "maximum likelihood"
    ],
    "absent_kp": [
      "statistics",
      "computation",
      "experience",
      "informal",
      "optimality",
      "tracking",
      "distributed",
      "profiles",
      "documentation"
    ]
  },
  {
    "title": "improved convected particle domain interpolation method for coupled dynamic analysis of fully saturated porous media involving large deformation.",
    "abstract": "based on the u-p form governing equations and the convected particle domain interpolation technique, the improved convected particle domain interpolation based material point method (ccpdi) is developed in this paper for the coupled dynamic and contact analysis of fully saturated porous media involving large deformation. the numerical artifact noises due to material points crossing computational grid boundaries that usually occur in large deformation are eliminated by using the smoother interpolation functions presented in the convected particle domain interpolation technique. the discrete equations are derived in the framework of the generalized interpolation material point method. the boundary load tracking algorithm and a modified contact algorithm are proposed based on the definition of particle domains to apply accurately the surface loads on the motive boundaries and to capture correctly the contact time/behaviors in large deformation problems, respectively. simulations of several representative one- and two-dimensional problems are presented to demonstrate the accuracy and effectiveness of the proposed methods. compared to those obtained by using the coupling material point method and/or finite element method, the simulation results illustrate that the proposed ccpdi method can be successfully used in simulating the large deformation coupled dynamic responses of the solid skeleton and fluid phase in fully saturated porous media and the large deformation impact between solid and saturated porous bodies.",
    "present_kp": [
      "saturated porous media",
      "convected particle domain interpolation",
      "dynamic analysis",
      "large deformation"
    ],
    "absent_kp": [
      "dynamic contact"
    ]
  },
  {
    "title": "a soa for ubiquitous communication management.",
    "abstract": "this paper presents an application framework for ubiquitous communication management. the framework -- called hermes after the messenger of gods in greek mythology -- is to provide a solid foundation for applications that are aware of users' communication needs and present appropriate tool support. in this paper, ubiquitous communication management, the framework itself, its approach to web-based data replication and the framework's communication-related utilization of web services will be discussed.",
    "present_kp": [
      "communication management",
      "data replication"
    ],
    "absent_kp": [
      "peer-to-peer architecture",
      "ubiquitous connectivity",
      "data reconciliation",
      "sensors",
      "service-oriented architecture",
      "contact management",
      "event-driven architecture",
      "instant messaging"
    ]
  },
  {
    "title": "measuring participants' perception on facilitation in group support systems meetings.",
    "abstract": "facilitation is often considered to be one of the key factors in the successful application of group support systems. research on gss facilitation has revealed insight into the types of tasks performed by facilitators and the potential positive effects of facilitation on group consensus and satisfaction. however, earlier research has hardly approached gss facilitation from the participants' point of view. this paper presents a study in which a questionnaire was developed and distributed to 182 participants of facilitated gss meeting in order to measure their perception on various facilitation tasks. the results suggested three categories of facilitation tasks that are perceived important by participants. each of these categories strongly correlated with participants' meeting satisfaction. further research is needed to refine these categories so that the instrument may be used to evaluate a facilitator's performance.",
    "present_kp": [
      "facilitation",
      "satisfaction",
      "group support systems"
    ],
    "absent_kp": [
      "moderation",
      "groupware"
    ]
  },
  {
    "title": "an efficient approach to removing geometric degeneracies.",
    "abstract": "our aim is to perturb the input so that an algorithm designed under the hypothesis of input non-degeneracy can execute on arbitrary instances. the deterministic scheme of [emca] was the first efficient method and was applied to two important predicates. here it is extended in a consistent manner to another two common predicates, thus making it valid for most algorithms in computational geometry. it is shown that this scheme incurs no extra algebraic complexity over the original algorithm while it increases the bit complexity by a factor roughly proportional to the dimension of the geometric space. the second contribution of this paper is a variant scheme for a restricted class of algorithms that is asymptotically optimal with respect to the algebraic as well as the bit complexity. both methods are simple to implement and require no symbolic computation. they also conform to certain criteria ensuring that the solution to the original input can be restored from the output on the perturbed input. this is immediate when the input to solution mapping obeys a continuity property and requires some case-specific work otherwise. finally we discuss extensions and limitations to our approach.",
    "present_kp": [
      "symbolic computation",
      "method",
      "input",
      "factor",
      "space",
      "map",
      "case",
      "complexity",
      "algorithm",
      "paper",
      "scheme",
      "class",
      "computational geometry"
    ],
    "absent_kp": [
      "efficiency",
      "optimality",
      "extensibility",
      "consistency",
      "continuation"
    ]
  },
  {
    "title": "crisp analogs of fuzzy sets.",
    "abstract": "with standard operators the fuzzy sets defined over a universe of discourse respects the structure of a de morgan algebra. via an injective canonical mapping we establish an isomorphism to a de morgan algebra of crisp subsets of an extended universe. the canonical mapping gives a natural extension of any algebraic operator acting on fuzzy sets to a crisp analog. this proves that the algebraic aspects of fuzzy set theory can be completely described within the framework of crisp set theory.",
    "present_kp": [
      "fuzzy sets",
      "de morgan algebra"
    ],
    "absent_kp": [
      "crisp sets",
      "quasi-boolean algebra",
      "cartesian product"
    ]
  },
  {
    "title": "a novel two-stage weak classifier selection approach for adaptive boosting for cascade face detector.",
    "abstract": "it is well-known for adaboost to select out the optimal weak classifier with the least sample-weighted error rate, which might be suboptimal for minimizing the nave error rate. in this paper, a novel variant of adaboost named otboost is proposed to learn optimal thresholded node classifiers for cascade face detector. in otboost, a two-stage weak classifier selection approach based on adaptive boosting framework is applied to minimize both the sample-weighted error rate and the optimal-thresholded multi-set class-weighted error rate. besides, a new sample set called selection set is also applied to prevent overfitting on the training set. several upright frontal cascade face detectors are learned, which shows that the otboost strong classifiers have much better convergence ability than the adaboost ones with the cost of slightly worse generalization ability. some otboost based cascade face detectors have acceptable performance on the cmu+mit upright frontal face test set.",
    "present_kp": [
      "adaboost",
      "weak classifier selection"
    ],
    "absent_kp": [
      "face detection"
    ]
  },
  {
    "title": "database selection for processing k nearest neighbors queries in distributed environments.",
    "abstract": "we consider the processing of digital library queries, consisting of a text component and a structured component in distributed environments. the text component can be processed using techniques given in previous papers such as [7, 8, 11]. in this paper, we concentrate on the processing of the structured component of a distributed query. histograms are constructed and algorithms are given to provide estimates of the desirabilities of the databases with respect to the given query. databases are selected in descending order of desirability. an algorithm is also given to select tuples from the selected databases. experimental results are given to show that the techniques provided here are effective and efficient.",
    "present_kp": [
      "select",
      "histogram",
      "order",
      "nearest neighbor",
      "structure",
      "environments",
      "k nearest neighbors",
      "text",
      "process",
      "queries",
      "algorithm",
      "paper",
      "database selection",
      "distributed",
      "effect",
      "database",
      "query",
      "component"
    ],
    "absent_kp": [
      "efficiency",
      "experimentation",
      "query processing",
      "digital libraries",
      "distributed databases"
    ]
  },
  {
    "title": "selecting predictors for traffic control by methods of the mathematical theory of democracy.",
    "abstract": "the mathematical theory of democracy operates on the indices of popularity and universality which are used to find socially optimal representatives and representative bodies. regarded mathematically, neither the society, nor its representatives are necessarily human, so that some objects can represent the behavior of other objects. this idea is applied to selecting predictors of traffic jams which occur the city ring of hagen, germany; the traffic situations at street intersections all over the town are regarded as representatives of future traffic situations at the ring intersections. as a result, a reliable prediction can be attained by already five surveillance cameras installed at appropriate locations. the selection of such locations and processing the data captured by the cameras constitute the subjects of this paper.",
    "present_kp": [
      "traffic control",
      "representatives",
      "representative bodies",
      "mathematical theory of democracy",
      "indices of popularity and universality"
    ],
    "absent_kp": [
      "congestions",
      "statistical tests"
    ]
  },
  {
    "title": "the effects of graphical overviews on knowledge acquisition in hypertext.",
    "abstract": "abstract a central aspect of designing hypertext for learning concerns the structure of the information in the hypertext and the view the learner is offered of this structure. in this study, a hypertext environment was enhanced with a graphical overview that represented the basic, inherent, structure of the domain and the layout was designed in such a way that learners were unobtrusively encouraged to follow a sequence of exploration that followed the domain structure. this so-called visual lay-out was compared with two lay-outs that presented randomly positioned nodes. one of these two lay-outs contained hints (using highlighting) to stimulate learners to follow a domain related exploration similar to the one incorporated in the visual lay-out. the other (control) lay-out did not provide such hints. results showed that participants from both the visual and the hints conditions demonstrated a more domain-related exploration pattern than participants from the control condition. participants in the visual lay-out did not show a better recall of the content of the nodes as such, but showed a significantly better acquisition of knowledge of structure than participants from the other two conditions. these data indicate that a visual display conveys knowledge in its own right and that knowledge gained does not depend on the exploration route followed in the hypertext material.",
    "present_kp": [],
    "absent_kp": [
      "control group",
      "hypermedia",
      "motivation",
      "navigation",
      "self-directed",
      "undergraduate",
      "visual representation"
    ]
  },
  {
    "title": "a multivariate statistical analysis of the developing human brain in preterm infants.",
    "abstract": "preterm delivery accounts for 5% of all deliveries and its consequences contribute to significant individual, medical, and social problems. the neuroanatomical substrates of these disorders are not known, but are essential for understanding mechanisms of causation, and developing strategies for intervention. in the recent years, multivariate pattern recognition methods that analyse all voxels simultaneously have been proposed to characterise the neuroanatomical differences between a reference group of magnetic resonance (mr) images and the population under investigation. most of these techniques have overcome the difficulty of dealing with the inherent high dimensionality of 3d mr brain image data by using pre-processed segmented images or a small number of specific features. however, an intuitive way of mapping the classification results back into the original image domain for further interpretation remains challenging. in this paper, we propose the idea of using principal components analysis (pca) plus the maximum uncertainty linear discriminant analysis (mlda) approach to classify and analyse mr brain images that have been aligned with either affine or non-rigid registration techniques. this approach avoids the computation costs intrinsic to commonly used covariance-based optimisation processes for solving small sample size problems, resulting in a simple and efficient implementation for the maximisation and interpretation of the fishers classification results. in order to demonstrate the effectiveness of the approach, we have used a neonatal mr brain data set that contains images of 93 preterm infants at term equivalent age and 20 term controls. our results indicate that the two-stage linear framework makes clear the statistical differences between the control and preterm samples, showing a classification accuracy of 95.0% and 97.8% for the controls and preterms samples, respectively, using the leave-one-out method. moreover, it provides a simple and intuitive method of visually analysing the differences between preterm infants at term equivalent age and the control group, such as differences in cerebrospinal fluid spaces, structure of the corpus callosum, and subtle differences in myelination.",
    "present_kp": [
      "small sample size",
      "brain images",
      "preterm infants"
    ],
    "absent_kp": [
      "multivariate statistics"
    ]
  },
  {
    "title": "soar: smalltalk without bytecodes.",
    "abstract": "we have implemented smalltalk-80 on an instruction-level simulator for a risc microcomputer called soar. measurements suggest that even a conventional computer can provide high performance for smalltalk-80 by abandoning the 'smalltalk virtual machine' in favor of compiling smalltalk directly to soar machine code, linearizing the activation records on the machine stack, eliminating the object table, and replacing reference counting with a new technique called generation scavenging. in order to implement these techniques, we had to find new ways of hashing objects, accessing often-used objects, invoking blocks, referencing activation records, managing activation record stacks, and converting the virtual machine images.",
    "present_kp": [
      "measurement",
      "activation",
      "order",
      "generation",
      "hashing",
      "stack",
      "code",
      "smalltalk",
      "bytecode",
      "object",
      "instruction",
      "virtual machine",
      "reference counting",
      "microcomputer",
      "records"
    ],
    "absent_kp": [
      "simulation",
      "computation",
      "high-performance",
      "compilation",
      "imaging",
      "management"
    ]
  },
  {
    "title": "a hybrid one-then-two stage algorithm for computationally expensive electromagnetic design optimization.",
    "abstract": "purpose - the purpose of this paper is to propose a surrogate model-assisted optimization algorithm which effectively searches for the optimum at the earliest opportunity, avoiding the need for a large initial experimental design, which may be wasteful. design/methodology/approach - the methodologies of two-stage and one-stage selection of points are combined for the first time. after creating a small experimental design, a one-stage kriging algorithm is used to search for the optimum for a fixed number of iterations. if it fails to locate the optimum, the points it samples are then used in lieu of a traditional experimental design to initialize a two-stage algorithm. findings - the proposed approach was tested on a mathematical test function. it was found that the optimum could be located, without necessarily constructing an accurate surrogate model first. the algorithm performed well on an electromagnetic design problem, out performing both a random search and a genetic algorithm, in significantly fewer iterations. the results suggest a new interpretation of surrogate models - merely as tools for constructing a utility function to locate the optimum of an unknown function, as opposed to actual approximations of the unknown function. research limitations/implications - the research was carried out on unconstrained problems only. the findings have implications for modem experimental designs, as the proposed algorithm can often locate the optimum without necessarily constructing an accurate surrogate model. originality/value - the two paradigms of one-stage and two-stage selection of points in surrogate-model assisted optimization are combined for the first time. also, it is believed that this is the first time that the methodology of one-stage optimization has been used in optimal electromagnetic design.",
    "present_kp": [],
    "absent_kp": [
      "optimization techniques"
    ]
  },
  {
    "title": "a new scan architecture for both low power testing and test volume compression under soc test environment.",
    "abstract": "a new scan architecture for both low power testing and test volume compression is proposed. for low power test requirements, only a subset of scan cells is loaded with test stimulus and captured with test responses by freezing the remaining scan cells according to the distribution of unspecified bits in the test cubes. in order to optimize the proposed process, a novel graph-based heuristic is proposed to partition the scan chains into several segments. for test volume reduction, a new lfsr reseeding based test compression scheme is proposed by reducing the maximum number of specified bits in the test cube set, s max, virtually. the performance of a conventional lfsr reseeding scheme highly depends on s max. in this paper, by using different clock phases between an lfsr and scan chains, and grouping the scan cells by a graph-based grouping heuristic, s max could be virtually reduced. in addition, the reduced scan rippling in the proposed test compression scheme can contribute to reduce the test power consumption, while the reuse of some test results as the subsequent test stimulus in the low power testing scheme can reduce the test volume size. experimental results on the largest iscas89 benchmark circuits show that the proposed technique can significantly reduce both the average switching activity and the peak switching activity, and can aggressively reduce the volume of the test data, with little area overhead, compared to the previous methods.",
    "present_kp": [
      "low power testing",
      "test compression"
    ],
    "absent_kp": [
      "system on a chip",
      "scan testing"
    ]
  },
  {
    "title": "interval-valued (alpha, beta)-fuzzy k-algebras.",
    "abstract": "the notion of interval-valued fuzzy sets was first introduced by zadeh in 1975 as a generalization of fuzzy sets. using the concept of interval-valued fuzzy sets, we introduce a new kind of generalized fuzzy subalgebra of a k-algebra called, an interval-valued (alpha, beta)-fuzzy subalgebra. we present some interesting properties of an interval-valued (alpha, beta)-fuzzy subalgebra of a k-algebra. some characterization theorems of the interval-valued (epsilon, epsilon boolean or q)-fuzzy subalgebra are established. next we investigate the properties of interval-valued fuzzy subalgebras with thresholds. finally we present characterization theorems of implication based interval-valued fuzzy subalgebras.",
    "present_kp": [
      "k-algebras",
      "fuzzy subalgebras with thresholds"
    ],
    "absent_kp": [
      "interval-valued fuzzy points",
      "level sets",
      "-fuzzy subalgebras",
      "implication operator"
    ]
  },
  {
    "title": "asymptotic behaviour of bi-infinite words.",
    "abstract": "we present a description of asymptotic behaviour of languages of bi-infinite words obtained by iterating morphisms defined on free monoids.",
    "present_kp": [
      "bi-infinite words",
      "morphisms"
    ],
    "absent_kp": [
      "iteration",
      "boundary set"
    ]
  },
  {
    "title": "supporting effective health and biomedical information retrieval and navigation: a novel facet view interface evaluation.",
    "abstract": "there is a need to provide a more effective user interface to facilitate non-domain experts health information seeking in authoritative online databases such as medline. we developed a new topic cluster based information navigation system called simmed. instead of offering a list of documents, simmed presents users with a list of ranked clusters. topically similar documents are grouped together to provide users with a better overview of the search results and to support exploration of similar literature within a cluster. we conducted an empirical user study to compare simmed to a traditional document list based search interface. a total of 42 study participants were recruited to use both interfaces for health information exploration search tasks. the results showed that simmed is more effective in terms of users perceived topic knowledge changes and their engagement in user-system interactions. we also developed a new metric to assess users efforts to find relevant citations. on average, users need significantly fewer clicks to find relevant information in simmed than in the baseline system. comments from study participants indicated that simmed is more helpful in finding similar citations, providing related medical terms, and presenting better organized search results, particularly when the initial search is unsatisfactory. findings from the study shed light on future health and biomedical information retrieval system and interface designs.",
    "present_kp": [],
    "absent_kp": [
      "exploratory searching",
      "biomedical information retrieval systems",
      "cluster and facet view",
      "search interface evaluation"
    ]
  },
  {
    "title": "a fast and accurate calibration algorithm for real-time locating systems based on the received signal strength indication.",
    "abstract": "with the recent growth of ieee 802.11 based wireless local area networks (wlans), there has been new research attempts to use the wlan technology for location estimation applications in deep indoor environments where traditional locating algorithms are not effective. the received signal strength indication (rssi) fingerprinting that uses wlan infrastructure is a compelling approach due to its fast online tracking capability and to its accurate location estimation. however, the rssi requires an exhaustive calibration phase to build a fingerprint database. to mitigate the time consuming property of the calibration process, several studies have proposed a number of propagation prediction models that derive the distance of a target object from rssi values at the expense of lower location accuracy. in this study, we propose a new locating algorithm that combines the calibration procedure with the fingerprint prediction model. our simulation results show that our new approach not only reduces the calibration time, but it also provides comparable location accuracy to that of the conventional calibration-based locating algorithm.",
    "present_kp": [
      "rssi"
    ],
    "absent_kp": [
      "location-based services",
      "real-time location systems",
      "path-loss prediction"
    ]
  },
  {
    "title": "an integrated driver warning system for driver and pedestrian safety.",
    "abstract": "a driver assistance system that correlates obstacles with driver view is proposed. fuzzy-rules-based subsystems are used for analysis under different road conditions. one set of fuzzy rules for when a vehicle or pedestrian ahead is detected. one set of fuzzy rules for when no immediate obstacle is detected. lab experiments and comparisons using real-world data show good performance.",
    "present_kp": [
      "fuzzy"
    ],
    "absent_kp": [
      "driving safety",
      "stereo vision",
      "histogram of oriented gradient",
      "adaboost"
    ]
  },
  {
    "title": "maximum volume cuboids for arbitrarily shaped in-situ rock blocks as determined by discontinuity analysisa genetic algorithm approach.",
    "abstract": "the block stone industry is one of the main commercial use of rock. the economic potential of any block quarry depends on the recovery rate, which is defined as the total volume of useful rough blocks extractable from a fixed rock volume in relation to the total volume of moved material. the natural fracture system, the rock type(s) and the extraction method used directly influence the recovery rate. the major aims of this study are to establish a theoretical framework for optimising the extraction process in marble quarries for a given fracture system, and for predicting the recovery rate of the excavated blocks. we have developed a new approach by taking into consideration only the fracture structure for maximum block recovery in block quarries. the complete model uses a linear approach based on basic geometric features of discontinuities for 3d models, a tree structure (ts) for individual investigation and finally a genetic algorithm (ga) for the obtained cuboid volume(s). we tested our new model in a selected marble quarry in the town of ?scehisar (afyonkarah?sarturkey).",
    "present_kp": [
      "tree structure",
      "genetic algorithm"
    ],
    "absent_kp": [
      "dimensional stone",
      "3d fracture model",
      "optimisation"
    ]
  },
  {
    "title": "on the efficiency of cluster-based approaches for motion detection using body sensor networks.",
    "abstract": "body sensor networks (bsn) are an emerging application that places sensors on the human body. given that a bsn is typically powered by a battery, one of the most critical challenges is how to prolong the lifetime of all sensor nodes. recently, using clusters to reduce the energy consumption of bsn has shown promising results. one of the important parameters in these cluster-based algorithms is the selection of cluster heads (chs). most prior works selected chs either probabilistically or based on nodes' residual energy. in this work, we first discuss the efficiency of cluster-based approaches for saving energy. we then propose a novel cluster head selection algorithm to maximize the lifetime of a bsn for motion detection. our results show that we can achieve above 90% accuracy for the motion detection, while keeping energy consumption as low as possible.",
    "present_kp": [
      "body sensor network",
      "motion detection"
    ],
    "absent_kp": [
      "energy conservation",
      "knn"
    ]
  },
  {
    "title": "modeling of human artery tissue with probabilistic approach.",
    "abstract": "probabilistic approach is used to model the inhomogeneity of human artery tissue. tissue properties are represented by a statistical function with normal distribution. mean value of the material parameters are identified using genetic algorithm. empirical 3-sigma rule is used for reliability study of the statistical model. the statistical model represents the human artery properties accurately.",
    "present_kp": [
      "probabilistic approach"
    ],
    "absent_kp": [
      "human arterial tissue",
      "uncertainty analysis",
      "tissue modeling",
      "medical simulation"
    ]
  },
  {
    "title": "user-guided symbiotic space-sharing of real workloads.",
    "abstract": "symbiotic space-sharing is a technique that can improve system throughput by executing parallel applications in combinations and configurations that alleviate pressure on shared resources. we have shown prototype schedulers that leverage such techniques to improve throughput by 20% over conventional space-sharing schedulers when resource bottlenecks are known. such evaluations have utilized benchmark workloads and proposed that schedulers be informed of resource bottlenecks by users at job submission time; in this work, we investigate the accuracy with which users can actually identify resource bottlenecks in real applications and the implications of these predictions for symbiotic space-sharing of production workloads. using a large hpc platform, a representative application workload, and a sampling of expert users, we show that user inputs are of value and that for our chosen workload, user-guided symbiotic scheduling can improve throughput over conventional space-sharing by 15-22%.",
    "present_kp": [
      "symbiotic space-sharing"
    ],
    "absent_kp": [
      "resource allocation",
      "high performance computing",
      "job scheduling"
    ]
  },
  {
    "title": "automatic enrichment of semantic relation network and its application to word sense disambiguation.",
    "abstract": "the most fundamental step in semantic information processing (sip) is to construct knowledge base (kb) at the human level; that is to the general understanding and conception of human knowledge. wordnet has been built to be the most systematic and as close to the human level and is being applied actively in various works. in one of our previous research, we found that a semantic gap exists between concept pairs of wordnet and those of real world. this paper contains a study on the enrichment method to build a kb. we describe the methods and the results for the automatic enrichment of the semantic relation network. a rule based method using wordnet's glossaries and an inference method using axioms for wordnet relations are applied for the enrichment and an enriched wordnet (e-wordnet) is built as the result. our experimental results substantiate the usefulness of e-wordnet. an evaluation by comparison with the human level is attempted. moreover, wsd-semnet, a new word sense disambiguation (wsd) method in which e-wordnet is applied, is proposed and evaluated by comparing it with the state-of-the-art algorithm.",
    "present_kp": [],
    "absent_kp": [
      "knowledge management applications",
      "knowledge base management",
      "text analysis",
      "knowledge acquisition",
      "dictionaries",
      "semantic networks"
    ]
  },
  {
    "title": "deterministic quantum key distribution using stabilizer quantum code.",
    "abstract": "employing a stabilizer quantum code (sqc), a quantum deterministic communication scheme is proposed to transmit secret messages with unconditional security. the proposed scheme has great capacity to transmit the messages since the utilized sqc can encode more than one qubit of the secret message in the message transmission phase for one scheme run. by utilizing the syndromes of the destroyed qubits, the damaged states can be exactly recovered from the received travel photons, which means that the scheme can be implemented in the imperfect channel with high fidelity.",
    "present_kp": [
      "quantum deterministic communication scheme",
      "quantum key distribution"
    ],
    "absent_kp": [
      "quantum error-correction code"
    ]
  },
  {
    "title": "fast transform from an adaptive multi-wavelet representation to a partial fourier representation.",
    "abstract": "we present a fast algorithm to compute the partial transformation of a function represented in an adaptive pseudo-spectral multi-wavelet representation to a partial fourier representation. such fast transformations are useful in many contexts in physics and engineering, where changes of representation from a piece wise polynomial basis to a fourier basis. the algorithm is demonstrated for a gaussian in one and in three dimensions. for 2d, we apply this approach to a gaussian in a periodic domain. the accuracy and the performance of this method is compared with direct summation.",
    "present_kp": [
      "multi-wavelet"
    ],
    "absent_kp": [
      "multi-resolution analysis",
      "fourier transform",
      "fft"
    ]
  },
  {
    "title": "a spacetime discontinuous galerkin method for hyperbolic heat conduction.",
    "abstract": "non-fourier conduction models remedy the paradox of infinite signal speed in the traditional parabolic heat equation. for applications involving very short length or time scales, hyperbolic conduction models better represent the physical thermal transport processes. this paper reviews the maxwellcattaneovernotte modification of the fourier conduction law and describes its implementation within a spacetime discontinuous galerkin (sdg) finite element method that admits jumps in the primary variables across element boundaries with arbitrary orientation in space and time. a causal, advancing-front meshing procedure enables a patch-wise solution procedure with linear complexity in the number of spacetime elements. an h-adaptive scheme and a special sdg shock-capturing operator accurately resolve sharp solution features in both space and time. numerical results for one spatial dimension demonstrate the convergence properties of the sdg method as well as the effectiveness of the shock-capturing method. simulations in two spatial dimensions demonstrate the proposed methods ability to accurately resolve continuous and discontinuous thermal waves in problems where rapid and localized heating of the conducting medium takes place.",
    "present_kp": [
      "hyperbolic heat conduction",
      "discontinuous galerkin",
      "finite element"
    ],
    "absent_kp": [
      "shock capturing",
      "adaptive analysis"
    ]
  },
  {
    "title": "geometry theorem proving by decomposing polynomial system into strong regular sets.",
    "abstract": "this paper presents a complete method to prove geometric theorem by decomposing the corresponding polynomial system. into strong regular sets, by which one can compute some components for which the geometry theorem is true and exclude other components for which the geometry theorem is false. two examples are given to show that the geometry theorems are conditionally true for some components which are excluded by other methods.",
    "present_kp": [
      "strong regular set"
    ],
    "absent_kp": [
      "zero decomposition",
      "automated geometry theorem proving",
      "subsidiary condition"
    ]
  },
  {
    "title": "low-complexity multiuser receiver scheme for stc-cdma communication system and its performance analysis.",
    "abstract": "by utilizing the complex orthogonality of spacetime coding, a multiuser receiver scheme is proposed for full-diversity spacetime coded cdma systems for effective decoding. the scheme can reduce the high decoding complexity of the existing scheme greatly, and achieve low calculation complexity. based on the system symbol error rate (ser) analysis and moment generation function, the average ser and bit error rate (ber) are derived in detail, and the high-accuracy approximate expressions are obtained. with these expressions, the system performance can be effectively evaluated in theory. simulation results show that the proposed receiver scheme performance is very close to the existing scheme performance, the derived ser and ber expressions can match the simulated values well, and thus the validity of the theoretical analysis is verified.",
    "present_kp": [
      "spacetime coding",
      "ser",
      "ber"
    ],
    "absent_kp": [
      "low-complexity receiver",
      "code division multiple access "
    ]
  },
  {
    "title": "selective disassembly for virtual prototyping as applied to de-manufacturing.",
    "abstract": "selective disassembly involves separating a selected set of components from an assembly. applications for selective disassembly include de-manufacturing (maintenance and recycling), and assembling. this paper presents a new methodology for performing design for selective disassembly analysis on the cad model of an assembly. the methodology involves the following three steps: (i) identifying the components to be selectively disassembled for de-manufacturing by a software program or designer, (ii) determining an optimal (e.g. minimal cost) disassembly sequence for the selected components that involves a computationally efficient two-level reduction procedure: (a) the determination of a set of sequences with an objective of minimal component removals via a wave propagation approach that topologically order components in an assembly for selective disassembly, and (b) the evaluation of resulting sequences based on an objective function (e.g. minimal cost) to identify an optimal sequence, and (iii) performing disassembly design decisions based on the evaluated optimal sequence. preliminary implementation results of the selective disassembly methodology in sequencing and disassembly cost evaluation, and application of the selective disassembly technique for de-manufacturing assessment are presented.",
    "present_kp": [
      "selective disassembly",
      "sequencing",
      "cost evaluation",
      "wave propagation"
    ],
    "absent_kp": [
      "product development",
      "vr-cad system",
      "a3d"
    ]
  },
  {
    "title": "energy-efficient task scheduling algorithms on heterogeneous computers with continuous and discrete speeds.",
    "abstract": "a large number of computing servers and personal electronic devices waste a tremendous amount of energy and emit a considerable amount of carbon dioxide, which is the major contribution to the greenhouse effect. thus, it is necessary to significantly reduce pollution and substantially lower energy usage. green computing techniques are utilized in a myriad of applications in energy conservation and environment improvement. new green task scheduling algorithms for heterogeneous computers with changeable continuous speeds and changeable discrete speeds are developed to reduce energy consumption as much as possible and finish all tasks before a deadline. a newly proven theorem can determine the optimal speed for tasks assigned to a computer with continuous speeds. this project seeks to develop innovative green task scheduling algorithms that have two main steps: heuristically assigning tasks to computers, and setting optimal or near-optimal speeds for all tasks assigned to each computer. sufficient simulation results indicate that the algorithm with the best task schedule varied. thus, two hybrid algorithms for continuous and discrete speeds are created separately to obtain the best task schedule among candidate task schedules. potential research applications include incorporating energy-efficient software into mobile devices, sensor networks, data centers, and cloud computing systems.",
    "present_kp": [
      "green computing",
      "task scheduling"
    ],
    "absent_kp": [
      "energy reduction",
      "power-aware methods",
      "pollution reduction"
    ]
  },
  {
    "title": "a competitive and cooperative co-evolutionary approach to multi-objective particle swarm optimization algorithm design.",
    "abstract": "multi-objective particle swarm optimization (mopso) is an optimization technique inspired by bird flocking, which has been steadily gaining attention from the research community because of its high convergence speed. on the other hand, in the face of increasing complexity and dimensionality of todays application coupled with its tendency of premature convergence due to the high convergence speeds, there is a need to improve the efficiency and effectiveness of mopso. in this paper a competitive and cooperative co-evolutionary approach is adapted for multi-objective particle swarm optimization algorithm design, which appears to have considerable potential for solving complex optimization problems by explicitly modeling the co-evolution of competing and cooperating species. the competitive and cooperative co-evolution model helps to produce the reasonable problem decompositions by exploiting any correlation, interdependency between components of the problem. the proposed competitive and cooperative co-evolutionary multi-objective particle swarm optimization algorithm (ccpso) is validated through comparisons with existing state-of-the-art multi-objective algorithms using established benchmarks and metrics. simulation results demonstrated that ccpso shows competitive, if not better, performance as compared to the other algorithms.",
    "present_kp": [
      "particle swarm optimization"
    ],
    "absent_kp": [
      "multi-objective optimization",
      "competitivecooperative co-evolution"
    ]
  },
  {
    "title": "a comparative analysis of interactive arithmetic learning in the classroom and computer lab.",
    "abstract": "we compared the usage of interactive arithmetic in classroom and computer lab. we used 3 groups: (1) classroom, (2) computer lab and (3) mixed usage of both. groups interacting with the system in the classroom achieved higher learning. peer interaction looks to be the key factor for learning improvement.",
    "present_kp": [],
    "absent_kp": [
      "interpersonal computer",
      "individual feedback",
      "teaching arithmetic",
      "educational computer lab",
      "1:n education",
      "1:1 education"
    ]
  },
  {
    "title": "fuzzy generalized ordered weighted averaging distance operator and its application to decision making.",
    "abstract": "we develop the fuzzy generalized ordered weighted averaging distance (fgowad) operator. it is a new aggregation operator that uses the main characteristics of the generalized owa (gowa), the ordered weighted averaging distance (owad) and uncertain information represented as fuzzy numbers. this operator includes a wide range of distance measures and aggregation operators such as the fuzzy maximum distance, the fuzzy minimum distance, the fuzzy normalized hamming distance (fnhd), the fuzzy weighted hamming distance (fwhd), the fuzzy normalized euclidean distance (fned), the fuzzy weighted euclidean distance (fwed), the fuzzy ordered weighted averaging distance (fowad) operator, the fuzzy euclidean ordered weighted averaging distance (feowad) operator and the fuzzy generalized ordered weighted averaging (fgowa) operator. we study some of its main properties. finally, we apply the developed operator to a multi-person decision-making problem regarding the selection of strategies.",
    "present_kp": [
      "distance measure",
      "fuzzy number",
      "decision making"
    ],
    "absent_kp": [
      "gowa operator"
    ]
  },
  {
    "title": "fabrication and adhesion strength of cu/nicr/polyimide films for flexible printed circuits.",
    "abstract": "the adhesion strength of a cu/nicr/polyimide flexible copper clad laminate (fccl), was evaluated according to the thickness of the nicr (ni:cr=95:5 ratio) seed layer using the 90 peel test. the changes in the morphology, chemical bonding and adhesion properties were characterized by sem, afm and xps. the peel strength of the fccl increased with increasing thickness of the nicr seed layer, due to the increase in the ion bombardment caused by the higher power used in the nicr sputtering process. this increase in the fccl peel strength was attributed to the lower proportion of cn bonds and higher proportion of co bonds in the polyimide surface. the adhesion strength between the metal and polyimide was mostly attributed to the chemical interaction between the metal layer and the functional groups of the polyimide.",
    "present_kp": [
      "flexible copper clad laminate ",
      "flexible printed circuit",
      "adhesion"
    ],
    "absent_kp": [
      "roll-to-roll process",
      "electroplating"
    ]
  },
  {
    "title": "visualized cognitive knowledge map integration for p2p networks.",
    "abstract": "this study proposes a visualized cognitive knowledge map integration system, called viscog, to facilitate knowledge management on p2p networks. by using the som (self-organized map)-like model, egocentric som (esom), viscog can merge the other peers' knowledge artifacts (e.g., documents) under a focal peer's knowledge structure and visually present the cognitive knowledge map of the p2p network. the experimental results from evaluating viscog performance show that viscog can retain an individual peer's knowledge structure while articulating with those of other peers to build its cognitive knowledge map.",
    "present_kp": [
      "knowledge map",
      "egocentric som "
    ],
    "absent_kp": [
      "self-organizing map ",
      "peer-to-peer "
    ]
  },
  {
    "title": "locking protocols for materialized aggregate join views.",
    "abstract": "the maintenance of materialized aggregate join views is a well-studied problem. however, to date the published literature has largely ignored the issue of concurrency control. clearly, immediate materialized view maintenance with transactional consistency, if enforced by generic concurrency control mechanisms, can result in low levels of concurrency and high rates of deadlock. while this problem is superficially amenable to well-known techniques, such as fine-granularity locking and special lock modes for updates that are associative and commutative, we show that these previous high concurrency locking techniques do not fully solve the problem, but a combination of a \"value-based\" latch pool and these previous high concurrency locking techniques can solve the problem.",
    "present_kp": [
      "concurrency"
    ],
    "absent_kp": [
      "relational databases",
      "transaction processing"
    ]
  },
  {
    "title": "introductory paper: reflections on conceptual modelling.",
    "abstract": "the objective of this introductory paper is twofold. on one hand, it shows the guest editors's view about the complex field of conceptual modelling. to do that, we discuss some concepts related to this topic, as well as the relation that nowadays exists between this process and the other parts of software development. on the other hand, this introductory paper describes the papers that are included in this special issue, connecting each of these papers with the view that the guest editors have about the field.",
    "present_kp": [
      "conceptual modelling"
    ],
    "absent_kp": [
      "conceptual models",
      "computational models"
    ]
  },
  {
    "title": "a qos enhanced framework and trust model for effective web services selection.",
    "abstract": "service oriented architecture (soa) has become a promising paradigm for software development. one of the most important research topics in soa is web service selection which means to identify best services among a bunch of services with same or similar functions but having different qos (quality of service). many previous approaches, such as qos models with quality criteria and selection algorithm, have been proposed to optimize web service selection. however, in current research, quality values normally come from service providers, who have high possibility to exaggerate these values for advertisement. it is also argued that reputation based on an average user rating is not enough to indicate the trust degree of web services and service provider. in addition, handling dynamic nature of web services is still a challenging problem for dynamical web service selection. in this paper, these problems are focused. first a qos enhanced framework for effective web service selection is proposed. then a trust model is built, which is composed of tqos model, decision model and trust correction. it is claimed that a web service can be regarded as trustful if qos values received by consumers and tested by registry are no less than qos values promised by providers. a prototype of the proposed framework is implemented, including sc agent, sr agent and qos enhanced sr. in addition, a scenario about a tour agency's web service selection according to its business process is implemented. to validate effectiveness of proposed approach, we compared it with other approaches, such as euclid approach and fuzzy approach. numerical simulation shows that proposed approach performances better other approaches in terms of obtained quality values.",
    "present_kp": [
      "service oriented architecture",
      "web services",
      "web service selection",
      "trust model",
      "tqos",
      "trust correction"
    ],
    "absent_kp": [
      "quality of services"
    ]
  },
  {
    "title": "execution of natural language requirements using state machines synthesised from behavior trees.",
    "abstract": "this paper defines a transformation from behavior tree models to uml state machines. behavior trees are a graphical modelling notation for capturing and formalising dynamic system behaviour described in natural language requirements. but state machines are more widely used in software development, and are required for use with many tools, such as test case generators. combining the two approaches provides a formal path from natural language requirements to an executable model of the system. this in turn facilitates requirements validation and transition to model-driven software development methods. the approach is demonstrated by defining a mapping from behavior trees to uml state machines using the atlas transformation language (atl) in the eclipse modeling framework. a security-alarm system case study is used to illustrate the use of behavior trees and execution to debug requirements.",
    "present_kp": [
      "requirements",
      "requirements validation",
      "behavior trees",
      "uml state machine"
    ],
    "absent_kp": [
      "behavior engineering",
      "mde",
      "model transformation"
    ]
  },
  {
    "title": "distributed multivariate regression based on influential observations.",
    "abstract": "large-scale data sets are sometimes logically and physically distributed in separate databases. the issues of mining these data sets are not just their sizes, but also the distributed nature. the complication is that communicating all the data to a central database would be too slow. to reduce communication costs, one could compress the data during transmission. another method is random sampling. we propose an approach for distributed multivariate regression based on sampling and discuss its relationship with the compression method. the central idea is motivated by the observation that, although communication is limited, each individual site can still scan and process all the data it holds. thus it is possible for the site to communicate only influential samples without seeing data in other sites. we exploit this observation and derive a method that provides tradeoff between communication cost and accuracy. experimental results show that it is better than the compression method and random sampling.",
    "present_kp": [
      "sampling"
    ],
    "absent_kp": [
      "learning curve",
      "distributed data mining",
      "multivariate linear regression"
    ]
  },
  {
    "title": "bonnroute: algorithms and data structures for fast and good vlsi routing.",
    "abstract": "we present the core elements of bonnroute: advanced data structures and algorithms for fast and high-quality routing in modern technologies. global routing is based on a combinatorial approximation scheme for min-max resource sharing. detailed routing uses exact shortest path algorithms, based on a shape-based data structure for pin access and a two-level track-based data structure for long-distance connections. all algorithms are very fast. compared to an industrial router (on 32nm and 22nm chips), bonnroute is over two times faster, has 5% less netlength, 20% less vias, and reduces detours by more than 90%.",
    "present_kp": [
      "algorithms",
      "global routing",
      "detailed routing"
    ],
    "absent_kp": [
      "design",
      "routing optimization"
    ]
  },
  {
    "title": "synthesis of water networks considering the sustainability of the surrounding watershed.",
    "abstract": "this paper presents a new mathematical programming model for the optimal synthesis of recycle and reuse networks considering simultaneously the integration of the water network system and the surrounding watershed to satisfy process and environmental constraints. the model considers the optimal location of the new industrial facility to integrate its wastewater discharge to the environment with the surrounding watershed through a disjunctive formulation. the pollutants discharged for the new plant are tracked simultaneously with the other discharges to the watershed (i.e., residential, sanitary, industrial and extractions), and the natural phenomena that affect the composition of the watershed (i.e., evaporation, filtration, etc.), in addition to the chemical reactions that are carried out in the rivers. the objective function consists in minimizing the total annual cost that is constituted by the installation of the new plant cost (including the transportation for raw materials, products and services, as well as the land cost), the wastewater treatment costs (including the piping cost) and the fresh sources cost. two example problems were used to show the applicability of the proposed methodology.",
    "present_kp": [
      "recycle and reuse networks"
    ],
    "absent_kp": [
      "material flow analysis",
      "optimization",
      "optimal location of a new plant",
      "watersheds",
      "sustainable systems",
      "mass integration",
      "water integration"
    ]
  },
  {
    "title": "extensions of the ho and lee interest-rate model to the multinomial case.",
    "abstract": "the paper presents a state dependent multinomial model of intertemporal changes in the term structure of interest rates. the model is a one-factor interest-rate model within the markov family models for short-term interest rate and it extends the ho and lee binomial model. we derive the theoretical basis of the multinomial model, suggest a computational framework to evaluate the model's parameters and investigate the suitability of the model for the italian market.",
    "present_kp": [
      "term structure",
      "multinomial model"
    ],
    "absent_kp": [
      "pricing"
    ]
  },
  {
    "title": "development for a web-based edm laboratory in manufacturing engineering.",
    "abstract": "a web-based learning framework of electrical discharge machining (edm) is presented in this research. the architecture supports 'hands-on' exercise in precision manufacturing field for distance education and allows learners to practice and access an edm-based virtual environment by means of the internet. meanwhile, a learning website has been planned and constructed in order to facilitate interactivity and support edm. in this research, open-source software tools and virtual interactive technology available are integrated to develop an emulated, effective, learning environment for distance education. the learning architecture is built independently on specific hardware and software of edm configuration. the paper concludes with an example and the implementation phase for learning the prototype environment.",
    "present_kp": [
      "electrical discharge machining",
      "education"
    ],
    "absent_kp": [
      "web-based learning system"
    ]
  },
  {
    "title": "field study on methods for elicitation of preferences using a mobile digital assistant for a dynamic tour guide.",
    "abstract": "knowing tourists' individual preferences provides the possibility to offer personalized tours. the challenge is to capture these preferences using a mobile device. during a field study in grlitz three methods for elicitation were evaluated by computing the correlation between the tourists' and the algorithms' rankings. the results served to clarify fundamental questions en route to develop a personal tour guide. 1) is it possible to seed a general interest profile in the mobile context with all its distractions that allows the accurate prediction of actual rankings of sights? 2) are the interest profiles sufficiently diverse to base personalized tours on individual interest profiles instead of interest prototypes? 3) how do personalized tours affect the spatial behavior of tourists, do they really visit a broader set of attractions than before? analyzing the interest profiles gives an insight into their actual diversity, discusses their necessity and helps simulating an improved distribution of tourists at a destination.",
    "present_kp": [
      "personalized tour",
      "dynamic tour guide"
    ],
    "absent_kp": [
      "preference elicitation",
      "mobile recommender"
    ]
  },
  {
    "title": "prediction of fault-prone software modules using a generic text discriminator.",
    "abstract": "this paper describes a novel approach for detecting fault-prone modules using a spam filtering technique. fault-prone module detection in source code is important for the assurance of software quality. most previous fault-prone detection approaches have been based on using software metrics. such approaches, however, have difficulties in collecting the metrics and constructing mathematical models based on the metrics. because of the increase in the need for spam e-mail detection, the spam filtering technique has progressed as a convenient and effective technique for text mining. in our approach, fault-prone modules are detected in such a way that the source code modules are considered text files and are applied to the spam filter directly. to show the applicability of our approach, we conducted experimental applications using source code repositories of java based open source developments. the result of experiments shows that our approach can correctly predict 78% of actual fault-prone modules as fault-prone.",
    "present_kp": [
      "fault-prone module",
      "prediction",
      "spam filter"
    ],
    "absent_kp": []
  },
  {
    "title": "realizing disjoint degree sequences of span at most two: a tractable discrete tomography problem.",
    "abstract": "we consider the problem of coloring a grid using p colors with the requirement that each row and each column has a specific total number of entries of each color. ryser (1957), and independently gale (1957) , obtained a necessary and sufficient condition for the existence of such a coloring when two colors are considered. this characterization yields a linear-time algorithm for constructing the coloring when it exists. later, gardner etal. (2000), and chrobak and drr (2001) , showed that the problem is np-hard when p?7 p ? 7 and p?4 p ? 4 , respectively. the case p=3 p = 3 was an open problem for several years and has been recently settled by drr etal. (2009): it is np-hard too. this grid coloring problem is equivalent to finding disjoint realizations of two degree sequences d1,d2 d 1 , d 2 in a complete bipartite graph kx,y k x , y . these kinds of questions are well studied when one of the degree sequences has span zero or one, where the span of a function is the difference between its maximum and its minimum values. in [4], chen and shastri (1989) showed a necessary and sufficient condition for the existence of a coloring when d1+d2 d 1 + d 2 restricted to x or y has span at most one. in terms of discrete tomography this latter condition means that for two colors, the sum of the number of occurrences of these colors in each row is k or k+1 k + 1 , for some integer k . in the present paper we prove an analog to chen and shastris characterization when d1+d2 d 1 + d 2 restricted to x and to y has span at most two. that is, there exist integers k1 k 1 and k2 k 2 such that the sum of the number of occurrences of two of the colors in each row is k1?1,k1 k 1 ? 1 , k 1 or k1+1 k 1 + 1 , and in each column is k2?1,k2 k 2 ? 1 , k 2 or k2+1 k 2 + 1 . our characterization relies on a new natural condition called the total saturation condition which, when not satisfied, gives a non-existence certificate of such a coloring that can be checked in polynomial time.",
    "present_kp": [
      "discrete tomography",
      "degree sequences",
      "complete bipartite graph"
    ],
    "absent_kp": []
  },
  {
    "title": "energetic analysis of fragment docking and application to structure-based pharmacophore hypothesis generation.",
    "abstract": "we have developed a method that uses energetic analysis of structure-based fragment docking to elucidate key features for molecular recognition. this hybrid ligand- and structure-based methodology uses an atomic breakdown of the energy terms from the glide xp scoring function to locate key pharmacophoric features from the docked fragments. first, we show that glide accurately docks fragments, producing a root mean squared deviation (rmsd) of < 1.0 for the top scoring pose to the native crystal structure. we then describe fragment-specific docking settings developed to generate poses that explore every pocket of a binding site while maintaining the docking accuracy of the top scoring pose. next, we describe how the energy terms from the glide xp scoring function are mapped onto pharmacophore sites from the docked fragments in order to rank their importance for binding. using this energetic analysis we show that the most energetically favorable pharmacophore sites are consistent with features from known tight binding compounds. finally, we describe a method to use the energetically selected sites from fragment docking to develop a pharmacophore hypothesis that can be used in virtual database screening to retrieve diverse compounds. we find that this method produces viable hypotheses that are consistent with known active compounds. in addition to retrieving diverse compounds that are not biased by the co-crystallized ligand, the method is able to recover known active compounds from a database screen, with an average enrichment of 8.1 in the top 1% of the database.",
    "present_kp": [
      "fragments",
      "docking accuracy",
      "enrichment"
    ],
    "absent_kp": [
      "virtual screening",
      "fragment-based drug design"
    ]
  },
  {
    "title": "application of dispersion-relation-preserving theory to develop a two-dimensional convectiondiffusion scheme.",
    "abstract": "in this paper a finite difference scheme is developed within the nine-point semi-discretization framework for the convectiondiffusion equation. the employed pade approximation renders a fourth-order temporal accuracy and the spatial approximation of convection terms accommodates the dispersion relation. the artificial viscosity introduced in the two-dimensional convectiondiffusion-reaction (cdr) equation for stability reasons is analytically derived. constraints on the mesh size and time interval for rendering a monotonic matrix are also rigorously derived. to validate the proposed method, we investigate several problems that are amenable to the exact solutions. the results with good rates of convergence are obtained for the investigated scalar and navierstokes problems.",
    "present_kp": [
      "nine-point",
      "pade approximation",
      "fourth-order",
      "dispersion relation",
      "monotonic"
    ],
    "absent_kp": [
      "convectiondiffusion-reaction equation"
    ]
  },
  {
    "title": "grey nomads in australia.",
    "abstract": "lifestyle factors have been identified as being very important in determining health in later life. nutrition, exercise, and social environment all interact to promote, or to limit, opportunities for an active and healthy post-working life. not only are rates of chronic illness and disability reduced through the promotion of healthy lifestyles, but also quality of life is maintained through the compression of morbidity. governments in australia, as in the european union and north america, have highlighted the importance of behavioral change in health promotion strategies with the aim of having an impact on the health-related lifestyles of their populations. this paper examines the example of a group of older australians, the grey nomads, who may present opportunities for examining health-related lifestyle changes. the term grey nomad refers to a portion of the older population in australia who choose to use their later years and retirement as opportunities for travel and leisure, mainly within the confines of the australian continent. as such, they are similar to groups in north america, such as the snow birds, who travel to the southern united states to escape the colder winters of more northerly latitudes. similar seasonal migrations occur from northern to southern europe. what all share in common is an active culture/lifestyle of attempting to age successfully. grey nomads also participate in the creation of what can be termed postmodern communities, where they and other regular travelers may develop a sense of community feeling with others who are also regularly returning to the same spot year after year. social support is highly predictive of health outcomes and such mobile communities may prove a positive factor in promoting good health. in this paper we examine whether the grey nomads represent a good model for improving health-related lifestyles in later life",
    "present_kp": [
      "grey nomads"
    ],
    "absent_kp": [
      "successful aging",
      "third age"
    ]
  },
  {
    "title": "virtual investigation of pulmonary airways in volumetric computed tomography.",
    "abstract": "this paper addresses the issue of non-invasive investigation and functional assessment of pulmonary airways reconstructed from multi detector computed tomography clinical acquisitions. such an analysis combines accurate 3d meshing of the inner bronchial wall surface and navigation and interactivity tools based on a robust central axis representation. a reliable endoluminal investigation of airways via virtual bronchoscopy is possible regardless of their anatomical/pathological specificity (small caliber bronchi, severe stenoses,...). computational fluid dynamics simulations on real airway geometries allow to assess functional modifications induced by physiopathological changes.",
    "present_kp": [
      "central axis",
      "pulmonary airways",
      "virtual bronchoscopy"
    ],
    "absent_kp": [
      "adaptive meshing",
      "endoluminal navigation",
      "fluid flow simulation",
      "3d reconstruction"
    ]
  },
  {
    "title": "exploiting fault model correlations to accelerate seu sensitivity assessment.",
    "abstract": "nowadays, integrated circuit technologies are increasingly being more susceptible to ionizing radiation effects. in order to assess the reliability of a digital system performing a specific application and to identify the most critical failure effects, radiation experiments and fault injection campaigns are usually performed, which may be costly and time-expensive. this paper proposes a fully automated, practical methodology for accelerating single-event-upset (seu) fault injection campaigns in digital circuits. the main underlying principle is based on the correlation between the effects of the seu fault model with the stuck-at (sa) one. circuital and functional analysis and experimental case studies confirm the effectiveness of the proposed solutions.",
    "present_kp": [
      "digital circuits",
      "fault injection",
      "reliability"
    ],
    "absent_kp": [
      "fault simulation",
      "soft error rate "
    ]
  },
  {
    "title": "adolescence and the trajectory of alcohol use: introduction to part vi.",
    "abstract": "research in the area of adolescent alcohol use is progressing rapidly, as exemplified by the chapters in this section. basic animal research in rodents has revealed adolescents to be more sensitive than adults to ethanol-induced disruptions in brain plasticity, although adolescents conversely are relatively insensitive to ethanol cues that serve to moderate intake. risks for excessive alcohol consumption due to genetic background have been shown in primate research to be exacerbated by adverse early life experiences. studies in clinical populations have revealed neurocognitive deficits evident years following adolescent alcohol abuse, along with evidence that some neural changes may predate adolescent alcohol abuse, whereas others appear to be a consequence of this abuse. further research is needed to detail determinants and consequences of adolescent alcohol abuse and to identify potential protective factors to diminish the propensity for excessive use of alcohol during this critical developmental period.",
    "present_kp": [
      "alcohol",
      "adolescence",
      "brain",
      "ethanol"
    ],
    "absent_kp": []
  },
  {
    "title": "a robust real-time video stabilization algorithm.",
    "abstract": "the acquisition of digital video usually suffers from undesirable camera jitters due to unstable camera motions. in this paper, we propose a robust real-time video stabilization algorithm that alleviates the undesirable jitter motions from the unstable video to produce a stabilized video. in the proposed algorithm, we first compute the sparse optical flow vectors between successive frames, followed by estimating the camera motion by fitting the computed optical flow vectors to a simplified affine motion model with a robust trimmed least squares method. then the computed camera motion parameters are smoothed temporally to reduce the motion fluctuations by using a regularization method. finally, we transform all frames in the video sequence based on the original and smoothed camera motions to obtain a stabilized video. experimental results are given to demonstrate the stabilization performance and the efficiency of the proposed algorithm.",
    "present_kp": [
      "video stabilization"
    ],
    "absent_kp": [
      "optical flow computation",
      "camera motion estimation",
      "robust estimation",
      "motion smoothing"
    ]
  },
  {
    "title": "parts-based segmentation with overlapping part models using markov chain monte carlo.",
    "abstract": "a probabilistic method is proposed for segmenting multiple objects that overlap or are in close proximity to one another. a likelihood function is formulated that explicitly models overlapping object appearance. priors on global appearance and geometry (including shape) are learned from example images. markov chain monte carlo methods are used to obtain samples from a posterior distribution over model parameters from which expectations can be estimated. the method is described in detail for the problem of segmenting femur and tibia in x-ray images. the result is a probabilistic segmentation that quantifies uncertainty, conditioned upon the model, so that measurements such as joint space can be made with associated uncertainty.",
    "present_kp": [
      "probabilistic segmentation",
      "markov chain monte carlo"
    ],
    "absent_kp": [
      "model-based segmentation"
    ]
  },
  {
    "title": "application of a recurrent neural network to space diversity in sdma and cdma mobile communication systems.",
    "abstract": "linear and non-linear adaptive algorithms are investigated for space division multiple access (sdma). sdma is one of the emerging techniques for multiple access of users in mobile radio, which uses spatial distribution of users for their differentiation. the performance of the linens square root kalman (srk) algorithm for sdma is compared to that of the non-linear recurrent neural network (rnn) technique. the proposed sdma-rnn technique is evaluated over rician fading channels. and it shows improved bit error rate (ber) performance e in comparison with the linear srk-based technique. the performance of sdma-rnn is also compared with that of code division multiple acc ess (cdma) systems, showing that it could he used as a viable alternative scheme for multiple access of users. finally, a hybrid cdma-sdma system is proposed combining: cdma and sdma-rnn systems. hybrid cdma-sdma exhibits a very good potential for increase in the capacity and the performance of mobile communications systems.",
    "present_kp": [
      "recurrent neural network",
      "space division multiple access "
    ],
    "absent_kp": [
      "adaptive space diversity combining",
      "code division multiple access ",
      "real-time recurrent learning algorithm"
    ]
  },
  {
    "title": "an automatic diffferentiation platform: odyssee.",
    "abstract": "numerous automatic differentiation strategies can be imagined to produce all kind of derivative programs under a wide range of complexity constraints, but there is no way to prototype and evaluate them on real size applications with reasonable effort. since the development of an automatic differentiation platform is prohibitively expensive, using an existing platform to share investments between the different research teams is a good solution. odyssee is an open automatic differentiation tool that enables the development of program analysis as well as program transformation for automatic differentiation. it has been used to differentiate large size industrial programs (300 000 lines of fortran 77) and to prototype diverse new automatic differentiation algorithms. its source is now freely available, a cooperative research project can therefore be based on it without financial or contractual constraint.",
    "present_kp": [
      "automatic differentiation",
      "odyssee"
    ],
    "absent_kp": [
      "computational differentiation",
      "open platform",
      "source transformation tool"
    ]
  },
  {
    "title": "cognitive heuristics in design: instructional strategies to increase creativity in idea generation.",
    "abstract": "this paper explores the use of heuristics as cognitive strategies invoked during the process of design. heuristics are reasoning processes that do not guarantee the best solution, but often lead to potential solutions by providing a simple cognitive \"shortcut.\" we propose that designers use specific design heuristics to explore the problem space of potential designs, leading to the generation of creative solutions. we test whether design heuristics can be taught to novices, and suggest their use will facilitate the design process at multiple levels of instruction. in the present empirical study, we evaluate a set of six instructional heuristics and validate their effectiveness with product concepts generated by novice designers. six hundred seventy-three drawings were created by 120 first-year college students under four instructional conditions. drawings were coded according to their content, use of heuristics, creativity, and practicality. the most creative concepts emerged from the experimental conditions where heuristics were introduced. heuristics appeared to help the participants \"jump\" to a new problem space, resulting in more varied designs, and a greater frequency of designs judged as more creative. our findings suggest that simple demonstration of design heuristics may, at times, be sufficient to stimulate divergent thinking, perhaps because these heuristics are readily grasped and contextual application is not required. based on these findings, a conceptual model for design education emphasizing the importance of using a variety of heuristics is proposed. this model suggests that learning can be enhanced through exposure to a variety of design heuristics, and can supplement formal education and foster personal development in design learning.",
    "present_kp": [
      "creativity",
      "design heuristics"
    ],
    "absent_kp": [
      "cognitive processes",
      "design pedagogy",
      "empirical studies"
    ]
  },
  {
    "title": "uniform local well-posedness to the density-dependent navier-stokes-maxwell system.",
    "abstract": "in this paper we establish the uniform local-in-time existence and uniqueness of classical solutions to the density-dependent navier-stokes-maxwell system. we then apply this uniform result to investigate the zero dielectric constant limit and the vanishing viscosity limit to navier-stokes-maxwell system. we obtain the well-known density-dependent magnetohydrodynamic equations when the dielectric constant goes to zero.",
    "present_kp": [
      "navier-stokes-maxwell system",
      "density-dependent",
      "uniform local well-posedness",
      "zero dielectric constant limit",
      "vanishing viscosity limit"
    ],
    "absent_kp": []
  },
  {
    "title": "formal correctness of a quadratic unification algorithm.",
    "abstract": "we present a case study using acl2 to verify a nontrivial algorithm that uses efficient data structures. the algorithm receives as input two first-order terms, and it returns a most general unifier of these terms if they are unifiable, failure otherwise. the verified implementation stores terms as directed acyclic graphs by means of a pointer structure. its time complexity is o(n(2)) and its space complexity o(n), and it can be executed in acl2 at a speed comparable to a similar c implementation. we report the main issues encountered to achieve this formally verified implementation.",
    "present_kp": [
      "acl2",
      "unification algorithm"
    ],
    "absent_kp": [
      "verification"
    ]
  },
  {
    "title": "mapping tasks onto nodes: a parallel local neighborhood approach.",
    "abstract": "we consider the problem of mapping processes onto computing nodes so as to reduce the execution time by minimizing communication delays. our approach relies on a genetic algorithm implementation of the local neighborhood search (lns) approach and is called genetic-lns or glns. we also present our parallel version of the glns algorithm, called parallel genetic local neighborhood search (p-glns). lns, gnls, and p-glns were implemented and compared. simulations demonstrate that the glns algorithm has better performance than lns, and that, when the workload is sufficiently high, the p-glns algorithm achieved near linear scalability.",
    "present_kp": [
      "genetic algorithm"
    ],
    "absent_kp": [
      "mapping onto nodes",
      "parallel local neighborhood search"
    ]
  },
  {
    "title": "unsupervised recursive sequence processing.",
    "abstract": "the self-organizing map (som) is a valuable tool for data visualization and data mining for potentially high-dimensional data of an a priori fixed dimensionality. we investigate soms for sequences and propose the som-s architecture for sequential data. sequences of potentially infinite length are recursively processed by integrating the currently presented item and the recent map activation, as proposed in the somsd presented in ieee. we combine that approach with the hyperbolic neighborhood of ritter, in order to account for the representation of possibly exponentially increasing sequence diversification over time. discrete and real-valued sequences can be processed efficiently with this method, as we will show in experiments. temporal dependencies can be reliably extracted from a trained som. u-matrix methods, adapted to sequence processing soms, allow the detection of clusters also for real-valued sequence elements.",
    "present_kp": [
      "self-organizing map",
      "sequence processing",
      "u-matrix"
    ],
    "absent_kp": [
      "recurrent models",
      "hyperbolic som",
      "markov models"
    ]
  },
  {
    "title": "the response of plasma catecholamines in rats simultaneously exposed to immobilization and painful stimuli.",
    "abstract": "immobilization represents a strong stressor inducing a profound increase in plasma epinephrine and norepinephrine levels. we have previously demonstrated that a subcutaneous injection of formalin (0.2 ml of 4% solution/100 g bw) attenuated the immobilization-induced elevation of plasma epinephrine levels in rats. in the present study, we investigated whether other painful and stressful stimuli, such as capsaicin, hydrochloric acid, mechanical pressure, heat, and cold, might also attenuate the increase of plasma epinephrine in rats exposed to acute immobilization stress. with the exception of formalin, all of the painful stimuli applied failed to attenuate the increase of plasma epinephrine levels in immobilized animals. our data suggest that the attenuation of an immobilization-induced increase in plasma epinephrine levels is specific for subcutaneous formalin administration.",
    "present_kp": [
      "capsaicin",
      "epinephrine",
      "formalin",
      "immobilization",
      "norepinephrine",
      "painful stimuli"
    ],
    "absent_kp": []
  },
  {
    "title": "scheduling the cleaning actions for a fouled heat exchanger subject to ageing: minlp formulation.",
    "abstract": "this paper addresses the problem of scheduling the cleaning actions of a heat exchanger undergoing fouling and ageing. the existence of two discrete layers leads to the use of two cleaning methods which differ in their ability to remove the aged layer. a mixed integer nonlinear programming (minlp) model is presented and solved for three case studies with different ageing rates. the optimal schedule consists of: (i) the timing of the cleanings and (ii) the cleaning mode in each instant.",
    "present_kp": [
      "ageing",
      "cleaning",
      "fouling",
      "minlp",
      "scheduling"
    ],
    "absent_kp": [
      "heat exchangers"
    ]
  },
  {
    "title": "surface reflectance and normal estimation from photometric stereo.",
    "abstract": "in this paper, we propose a new photometric stereo method for estimating diffuse reflection and surface normal from color images. using dichromatic reflection model, we introduce surface chromaticity as a matching invariant for photometric stereo, which serves as the foundation of the theory of this paper. an extremely simple and robust reflection components separation method is proposed based on the invariant. our separation method differs from most previous methods which either assume dependencies among pixels or require segmentation. we also show that a linear relationship between the image color and the surface normal can be obtained based on this invariant. the linear relationship turns the surface normal estimation problem into a linear system that can be solved exactly or via least-squares optimization. we present experiments on both synthetic and real images, which demonstrate the effectiveness of our method.",
    "present_kp": [
      "reflection components separation",
      "dichromatic reflection model",
      "chromaticity",
      "photometric stereo"
    ],
    "absent_kp": [
      "specular reflection"
    ]
  },
  {
    "title": "sparse poisson noisy image deblurring.",
    "abstract": "deblurring noisy poisson images has recently been a subject of an increasing amount of works in many areas such as astronomy and biological imaging. in this paper, we focus on confocal microscopy, which is a very popular technique for 3-d imaging of biological living specimens that gives images with a very good resolution (several hundreds of nanometers), although degraded by both blur and poisson noise. deconvolution methods have been proposed to reduce these degradations, and in this paper, we focus on techniques that promote the introduction of an explicit prior on the solution. one difficulty of these techniques is to set the value of the parameter, which weights the tradeoff between the data term and the regularizing term. only few works have been devoted to the research of an automatic selection of this regularizing parameter when considering poisson noise; therefore, it is often set manually such that it gives the best visual results. we present here two recent methods to estimate this regularizing parameter, and we first propose an improvement of these estimators, which takes advantage of confocal images. following these estimators, we secondly propose to express the problem of the deconvolution of poisson noisy images as the minimization of a new constrained problem. the proposed constrained formulation is well suited to this application domain since it is directly expressed using the antilog likelihood of the poisson distribution and therefore does not require any approximation. we show how to solve the unconstrained and constrained problems using the recent alternating-direction technique, and we present results on synthetic and real data using well-known priors, such as total variation and wavelet transforms. among these wavelet transforms, we specially focus on the dual-tree complex wavelet transform and on the dictionary composed of curvelets and an undecimated wavelet transform.",
    "present_kp": [
      "poisson noise"
    ],
    "absent_kp": [
      "alternating-direction method ",
      "constrained minimization",
      "discrepancy principle",
      "regularizing parameter selection",
      "3-d confocal microscopy deconvolution"
    ]
  },
  {
    "title": "error characteristics and calibration-free techniques for wireless lan-based location estimation.",
    "abstract": "using wireless lan technology for location estimation provides alternate means to enable location-based applications without investment in sensor network infrastructure and special hardware. however, the main drawback of wireless lan-based location systems is calibration of signal strength as a function of location in spatially high-density, which consumes manual labor and needs to be carried out repeatedly. in this paper, we analyze empirical error characteristics of calibration-based location algorithms such as triangulation in various spatial densities of calibration, using commercially available wireless lan products. then, we propose triangular interpolation and extrapolation (tix), a calibration-free location algorithm, and present empirical performance evaluation. tix can achieve mean distance error within 5.4 m, which is comparable to within 4.7 m errors of the calibration-based algorithms. we also present theoretical analysis on error characteristics of the location algorithms deriving accuracy limits and quantifying the effect of rf measurement and calibration.",
    "present_kp": [
      "wireless lan",
      "extrapolation",
      "calibration",
      "interpolation",
      "triangulation"
    ],
    "absent_kp": [
      "static scene analysis",
      "rf-based location estimation"
    ]
  },
  {
    "title": "a complete plane strain fictitious stress boundary element method for poroelastic media.",
    "abstract": "a fully coupled generalized plane strain boundary element model for determining the distribution of stress and pore pressure around underground openings in poroelastic media is developed. it is based on the indirect boundary element method of fictitious stress extended to complete plane strain analysis. new fundamental solutions for longitudinal forces were derived for the development of the model which uses elements with a constant variation of fictitious forces and sources in both space and time. as an example, the problem of a borehole drilled in an arbitrary direction in a triaxial stress field is considered. the results indicate that the fictitious stress method is an accurate and suitable means for complete plane strain poroelastic analyses of underground openings.",
    "present_kp": [
      "complete plane strain",
      "fictitious stress",
      "generalized plane strain",
      "pore pressure"
    ],
    "absent_kp": [
      "anti-plane shear",
      "line force",
      "line source",
      "point force",
      "point source",
      "poroelasticity"
    ]
  },
  {
    "title": "virtually homosexual: technoromanticism, demarginalisation and identity formation among homosexual males.",
    "abstract": "coming out is a key stage in the identity formation process for the homosexual male when the individual discloses his homosexual status to himself and others. although previous research has indicated that homosexual men often use the internet and computer-mediated communication (cmc) during the identity formation process to discover and develop their sexual and self-identities, studies to date have focused on their use of text-based cmc with scant attention paid to experiences within virtual worlds. this study explored whether homosexual males use virtual worlds in the sexual identity formation process and, specifically, the applicability of technoromanticism within this context. qualitative retrospective biographical interviews were undertaken with 12 self-selected individuals who had engaged with virtual worlds before or during their sexual identity development. the case model (community, anonymity, sexual experimentation, and escape) was developed to characterise the key themes emerging from the data and illustrate the enactment of technoromanticism by homosexual males within virtual worlds. it is concluded that technoromanticism in virtual worlds can only have a profound impact on individuals if the individuals personal development online is transferred offline as there is a potential to become toxically immersed and thus stall or halt the identity development process altogether.",
    "present_kp": [
      "homosexual males",
      "virtual worlds",
      "technoromanticism",
      "coming out",
      "identity formation",
      "cmc"
    ],
    "absent_kp": []
  },
  {
    "title": "a uniform approach to logic programming semantics.",
    "abstract": "part of the theory of logic programming and nonmonotonic reasoning concerns the study of fixed-point semantics for these paradigms. several different semantics have been proposed during the last two decades, and some have been more successful and acknowledged than others. the rationales behind those various semantics have been manifold, depending on one's point of view, which may be that of a programmer or inspired by commonsense reasoning, and consequently the constructions which lead to these semantics are technically very diverse, and the exact relationships between them have not yet been fully understood. in this paper, we present a conceptually new method, based on level mappings, which allows to provide uniform characterizations of different semantics for logic programs. we will display our approach by giving new and uniform characterizations of some of the major semantics, more particular of the least model semantics for definite programs, of the fitting semantics, and of the well-founded semantics. a novel characterization of the weakly perfect model semantics will also be provided.",
    "present_kp": [
      "level mapping",
      "fitting semantics",
      "well-founded semantics",
      "least model semantics"
    ],
    "absent_kp": [
      "stable semantics",
      "weak stratification"
    ]
  },
  {
    "title": "empowering students through digital game authorship: enhancing concentration, critical thinking, and academic achievement.",
    "abstract": "digital game-based learning is a popular strategy for engaging students by making learning fun. actively involving students as designers and producers of digital games may have even greater potential for student empowerment through enhancing concentration and engagement, fostering higher order thinking, and improving learning outcomes. thus, this study empirically investigated the impact of digital game authoring on students' concentration, critical thinking skills, and academic achievement. a total of 67 students in two seventh-grade classes participated in this 19-week-long experiment, and were divided into an experimental group (32 students designing digital games) and a comparison group (35 students designing flash animations). the interdisciplinary approach involved integrating biology and computer programming classes. students in the experimental group designed digital games based upon biology course content, while the comparison group collaboratively produced flash animations based upon the same course content. the experimental results, using mancova for pretest, posttest, and delayed posttest scores, demonstrate significant improvements in critical thinking skills, and academic achievement, with increased retention of both course content and critical thinking skills observed for the delayed posttest. for concentration, a relative advantage for the experimental group as compared with the comparison group was noted, but did not reach statistical significance. based on the results of this study, implications for practitioners and researchers are provided, including the integration of programming or computer science with other courses for digital game authoring and the evaluation of other learning outcomes such as creative thinking, problem-solving, and flow.",
    "present_kp": [],
    "absent_kp": [
      "applications in subject areas",
      "authoring tools and methods",
      "cooperative/collaborative learning",
      "interdisciplinary projects",
      "teaching/learning strategies"
    ]
  },
  {
    "title": "an efficient tree-based algorithm for mining sequential patterns with multiple minimum supports.",
    "abstract": "sequential pattern mining (spm) is an important technique for determining time-related behavior in sequence databases. in real-life applications, the frequencies for various items in a sequence database are not exactly equal. if all items are set with the same minimum support, the rare item problem may result, meaning that we are unable to effectively retrieve interesting patterns regardless of whether minsup is set too high or too low. liu (2006) first included the concept of multiple minimum supports (mmss) to spm. it allows users to specify the minimum item support (mis) for each item according to its natural frequency. a generalized sequential pattern-based algorithm, named multiple supports generalized sequential pattern (ms-gsp), was also developed to mine complete set of sequential patterns. however, the ms-gsp adopts candidate generate-and-test approach, which has been recognized as a costly and time-consuming method in pattern discovery. for the efficient mining of sequential patterns with mmss, this study first proposes a compact data structure, called a preorder linked multiple supports tree (plms-tree), to store and compress the entire sequence database. based on a plms-tree, we develop an efficient algorithm, multiple supports conditional pattern growth (mscp-growth), to discover the complete set of patterns. the experimental result shows that the proposed approach achieves more preferable findings than the ms-gsp and the conventional spm.",
    "present_kp": [
      "sequential patterns",
      "multiple minimum supports"
    ],
    "absent_kp": [
      "data mining",
      "plwap-tree"
    ]
  },
  {
    "title": "the development of casc.",
    "abstract": "researchers who make theoretical advances also need some way to demonstrate that an advance really does have general, overall positive consequences for system performance. for this it is necessary to evaluate the system on a set of problems that is sufficiently large and diverse to be somehow representative of the intended application area as a whole. it is only a small step from system evaluation to a communal system competition. the cade atp system competition (casc) has been run annually since 1996. any competition is difficult to design and organize in the first instance, and to then run over the years. in order to obtain the full benefits of a competition, a thoroughly organized event, with an unambiguous and motivated design, is necessary. for some issues relevant to the casc design, inevitable constraints have emerged. for other issues there have been several choices, and decisions have had to be made. this paper describes the evolution of casc, paying particular attention to its design, design changes, and organization.",
    "present_kp": [
      "competition"
    ],
    "absent_kp": [
      "automated theorem proving",
      "history"
    ]
  },
  {
    "title": "a review of procedures to evolve quantum algorithms.",
    "abstract": "there exist quantum algorithms that are more efficient than their classical counterparts; such algorithms were invented by shor in 1994 and then grover in 1996. a lack of invention since grovers algorithm has been commonly attributed to the non-intuitive nature of quantum algorithms to the classically trained person. thus, the idea of using computers to automatically generate quantum algorithms based on an evolutionary model emerged. a limitation of this approach is that quantum computers do not yet exist and quantum simulation on a classical machine has an exponential order overhead. nevertheless, early research into evolving quantum algorithms has shown promise. this paper provides an introduction into quantum and evolutionary algorithms for the computer scientist not familiar with these fields. the exciting field of using evolutionary algorithms to evolve quantum algorithms is then reviewed.",
    "present_kp": [
      "evolving quantum algorithms",
      "evolutionary algorithms",
      "quantum algorithms"
    ],
    "absent_kp": [
      "quantum computing",
      "genetic algorithms",
      "genetic programming"
    ]
  },
  {
    "title": "online intrusion alert aggregation with generative data stream modeling.",
    "abstract": "alert aggregation is an important subtask of intrusion detection. the goal is to identify and to cluster different alerts-produced by low-level intrusion detection systems, firewalls, etc.-belonging to a specific attack instance which has been initiated by an attacker at a certain point in time. thus, meta-alerts can be generated for the clusters that contain all the relevant information whereas the amount of data (i.e., alerts) can be reduced substantially. meta-alerts may then be the basis for reporting to security experts or for communication within a distributed intrusion detection system. we propose a novel technique for online alert aggregation which is based on a dynamic, probabilistic model of the current attack situation. basically, it can be regarded as a data stream version of a maximum likelihood approach for the estimation of the model parameters. with three benchmark data sets, we demonstrate that it is possible to achieve reduction rates of up to 99.96 percent while the number of missing meta-alerts is extremely low. in addition, meta-alerts are generated with a delay of typically only a few seconds after observing the first alert belonging to a new attack instance.",
    "present_kp": [
      "intrusion detection",
      "alert aggregation"
    ],
    "absent_kp": [
      "generative modeling",
      "data stream algorithm"
    ]
  },
  {
    "title": "chaotic dynamics of the fractional order nonlinear system with time delay.",
    "abstract": "this paper presents the fractional order model of a nonlinear autonomous continuous-time difference-differential equation with only one variable. numerical simulation results of the fractional order model demonstrate the existence of chaos when system order (qge 0.2). values of the delay time (tau ) in which chaotic behavior is observed at system order (q) are quantitatively defined using the largest lyapunov exponents obtained from the output time series.",
    "present_kp": [
      "chaos",
      "largest lyapunov exponents"
    ],
    "absent_kp": [
      "fractional operators",
      "fractional order time delayed nonlinear system"
    ]
  },
  {
    "title": "network integration testing: concepts, test specifications and tools for automatic telecommunication services verification.",
    "abstract": "the purpose of this tutorial is to provide concepts and historical background of the network integration testing (nit) methodology. nit is a grey box testing technique that is aimed at verifying the correct behaviour of interconnected networks (operated by different operators) in provisioning services to end users, or the behaviour of a complex network operated by a unique operator. the main technical concepts behind this technique are presented along with the history of some international projects that have contributed to its early definition and application. european institute for research and strategic studies in telecommunication (eurescom) has actually been very active, with many projects, in defining the nit basic methodology and providing actual nit specifications (for narrow-band and broad-band services, covering both voice and data). eurescom has also been acting as a focal point in the area, e.g., encouraging the industry in developing commercial tools supporting nit. in particular, the eurescom p412 project (<phone>) first explicitly defined the nit methodology (the methodological aspects include test notation, test implementation, test processes, distributed testing and related co-ordination aspects). p412 applied the methodology to isdn whilst another project, p410, applied nit to data services. the p613 project (<phone>) extended the basic nit methodology to the broad band and gsm. more into details, the areas covered currently by nit test specifications developed by eurescom projects include n-isdn, n-isup, pots, b-isdn, b-isup, ip over atm, atm/fr, gsm, focusing also on their inter-working cases (e.g., isdn/isdn, isdn/gsm, etc.). etsi, the european telecommunication standards institute, also contributed to nit development (e.g., the definition of the tsp1+ protocol, used for the functional co-ordination and timing synchronisation of all tools involved in a distributed testing session). the paper also discusses nit in relation to the recent major changes (processes) within the telecommunication (tlc) community. beyond the new needs coming from the pure technical aspects (integration of voice and data, fixed mobile convergence, etc.) the full deregulation of the tlc sector has already generated new processes and new testing needs (e.g., interconnection testing) that had a significant influence on the methodology. nit is likely to continue to develop further in the future according to the needs of telecom operators, authorities, users associations and suppliers.",
    "present_kp": [
      "network integration testing",
      "gsm",
      "isdn",
      "fr",
      "atm",
      "testing"
    ],
    "absent_kp": [
      "end-to-end",
      "node-to-node"
    ]
  },
  {
    "title": "a multiqueue service room mac protocol for wireless networks with multipacket reception.",
    "abstract": "an adaptive medium-access control (mac) protocol for heterogeneous networks with finite population is proposed. referred to as the multiqueue service room (mqsr) protocol, this scheme is capable of handling users with different quality-of-service (qos) constraints. by exploiting the multipacket reception (mpr) capability, the mqsr protocol adaptively grants access to the mpr channel to a number of users such that the expected number of successfully received packets is maximized in each slot. the optimal access protocol avoids unnecessary empty slots for light traffic and excessive collisions for heavy traffic. it has superior throughput and delay performance as compared to, for example, the slotted aloha with the optimal retransmission probability. this protocol can be applied to random-access networks with multimedia traffic.",
    "present_kp": [
      "medium-access control ",
      "multipacket reception ",
      "random-access network"
    ],
    "absent_kp": []
  },
  {
    "title": "local and global effects of mg2+ on ago and mirna-target interactions.",
    "abstract": "three magnesium ions (mg2+), named mg1 (in mid domain), mg2 and mg3 (both in piwi domain), located at the small rna binding domain of argonaute (ago) protein, are important for sequence-specific mirna-target interactions. such conjunction between the ago protein and mirna raises the question: how do mg2+ ions participate in the recognition process of mirna by ago or its target. furthermore, it is still unclear whether the mg2+ ions contribute to the local or global stability of the mirna complex. in this work, we have performed a series of 16 independent molecular dynamic simulations (md) to characterize the functions of mg2+, hydration patterns and the conformational events involved in the mirna-target interactions. the cross correlation analysis shows that mg1 and mg2 significantly enhance a locally cooperated movement of the paz, piwi and mid domains with the average correlation coefficient of similar to 0.65, producing an \"open-closed\" motion (rotation angle, 46.5a degrees) between the paz and piwi domains. binding of mg3 can globally stabilize the whole ago protein with the average rmsd of similar to 0.34 , compared with the systems in absence of mg3 (average rmsd = similar to 0.43 ). three structural water molecules surrounding the mg2+-binding regions also stabilize these ions, thus facilitating the recognition of mirna to its target. in addition, the thermodynamic analysis also verifies the positive contribution of all three mg2+ to the binding of mirna to ago, as well as the importance mg2 plays in the cleavage of the mirna targets.",
    "present_kp": [
      "argonaute",
      "mg2+",
      "molecular dynamic simulation"
    ],
    "absent_kp": [
      "mirna-mrna",
      "mm/gbsa"
    ]
  },
  {
    "title": "on the characteristics of information dissemination paths in vehicular ad hoc networks on the move.",
    "abstract": "recently, intelligent transportation systems (its) is becoming an important research topic. one goal for its is to disseminate important information among vehicles in a timely and efficient manner. in the its research community, inter-vehicle communications (ivc) is considered a way that may be able to achieve this goal. an information network built on top of vehicles using ivc can be viewed as a type of mobile ad hoc networks. although several information dissemination protocols for mobile ad hoc networks have been proposed, how well they can be applied to a vehicle-formed mobile ad hoc network is still poorly studied. without doubt, no matter what methodology is taking for designing an ivc suitable information dissemination protocol, the protocol performance is fundamentally limited by the potential degree of physical-link connectivity. thus, in this paper, some potential link attributes of ivc networks, such as the lifetime of a transmission path between two vehicles, are studied based on reasonable vehicle traces generated by a microscopic traffic simulator. although no specific protocol is focused on in this paper, the studied results can potentially contribute to performance improvement and/or protocol efficiently for any existed or developing protocol.",
    "present_kp": [
      "intelligent transportation systems",
      "physical-link connectivity",
      "vehicular ad hoc network"
    ],
    "absent_kp": [
      "inter vehicle communications"
    ]
  },
  {
    "title": "an object-oriented programming framework for boundary integral equation methods.",
    "abstract": "this work presents an object-oriented numerical framework for the development of computer programs based on the boundary element method (bem). the proposed design provides a large set of classes developed specially to handle those entities most commonly found in bem solution procedures. the framework is divided into three basic sets of classes that enable the analyst to build the code accordingly to the type of problem to be solved. the design also allows the use of an arbitrary number of subregions, which are connected automatically by imposing compatibility conditions on the interfaces. code fragments illustrate how the use of the proposed framework can simplify the development cycle of research software based on the bem.",
    "present_kp": [
      "object-oriented programming",
      "boundary element method",
      "subregions"
    ],
    "absent_kp": [
      "finite element method",
      "engineering software"
    ]
  },
  {
    "title": "a structure-preserving method for the quaternion lu decomposition in quaternionic quantum theory.",
    "abstract": "in this paper, for the first time, the structure-preserving gauss transformation is defined. then by means of its real representation matrix, we present a novel structure-preserving algorithm for the lu decomposition of a quaternion matrix. numerical experiments show that the structure-preserving algorithm is better than that in the newest quaternion toolbox for matlab (qtfm).",
    "present_kp": [
      "quaternion matrix",
      "lu decomposition",
      "structure-preserving algorithm"
    ],
    "absent_kp": []
  },
  {
    "title": "inductor design of 20-v boost converter for low power 3d solid state drive with nand flash memories.",
    "abstract": "a 3d-integrated solid state drive (ssd) with the boost converter can achieve both the low power and the fast write-operation at the small die area of the nand flash memory. the performance of the boost converter, however, is critically affected by the inductor, because the output voltage of the boost converter, the rising time, and the energy consumption during the boost are determined by the inductor. therefore, this paper proposes a design methodology of the inductor of the boost converter for the 3d ssd. by using the boost converter with the optimized inductor, the energy during write-operation of the proposed 1.8-v 3d-ssd is decreased by 68% compared with the conventional 3.3-v 3d-ssd with the charge pump.",
    "present_kp": [
      "charge pump",
      "boost converter",
      "ssd",
      "inductor design"
    ],
    "absent_kp": []
  },
  {
    "title": "simulation methods for quantum walks on graphs applied to formal language recognition.",
    "abstract": "we describe an algorithm which automates the generation of appropriate shift and coin operators for a discrete time quantum walk, given the adjacency matrix of the graph over which the walk is run. this gives researchers the freedom to numerically investigate any discrete time quantum walk over graphs of a computationally tractable size by greatly reducing the time required to initialise a given walk. we then describe one application in which the swift initialisation of walks has enabled systematic investigations of walks over a large number of structures. new results concerning this application, which is to formal language recognition, are described. the reliability of these results, as well as the general suitability of numerical analysis as a tool for investigating discrete time quantum walks, are briefly discussed. we also mention specific python packages which facilitate our simulations and analysis, motivating the use of high level programming languages in this context.",
    "present_kp": [
      "quantum walks",
      "formal language recognition",
      "algorithm",
      "simulation"
    ],
    "absent_kp": []
  },
  {
    "title": "image tag refinement towards low-rank, content-tag prior and error sparsity.",
    "abstract": "the vast user-provided image tags on the popular photo sharing websites may greatly facilitate image retrieval and management. however, these tags are often imprecise and/or incomplete, resulting in unsatisfactory performances in tag related applications. in this work, the tag refinement problem is formulated as a decomposition of the user-provided tag matrix d into a low-rank refined matrix a and a sparse error matrix e, namely d = a + e, targeting the optimality measured by four aspects: 1) low-rank: a is of low-rank owing to the semantic correlations among the tags; 2) content consistency: if two images are visually similar, their tag vectors (i.e., column vectors of a) should also be similar; 3) tag correlation: if two tags co-occur with high frequency in general images, their co-occurrence frequency (described by two row vectors of a) should also be high; and 4) error sparsity: the matrix e is sparse since the tag matrix d is sparse and also humans can provide reasonably accurate tags. all these components finally constitute a constrained yet convex optimization problem, and an efficient convergence provable iterative procedure is proposed for the optimization based on accelerated proximal gradient method. extensive experiments on two benchmark flickr datasets, with 25k and 270k images respectively, well demonstrate the effectiveness of the proposed tag refinement approach.",
    "present_kp": [
      "low-rank",
      "error sparsity",
      "content consistency",
      "tag refinement",
      "tag correlation"
    ],
    "absent_kp": [
      "social images"
    ]
  },
  {
    "title": "simultaneous inpainting for image structure and texture using anisotropic heat transfer model.",
    "abstract": "we propose a pde-based image inpainting method using anisotropic heat transfer model, which can simultaneously propagate the structure and texture information. in structure inpainting, the propagating direction and intensity are related to image contents, and the strength of propagation along gradient direction is made inversely proportional to the magnitude of gradient. in texture inpainting, the added texture term reflects periodicity along the texture and its perpendicular direction. for numerical implementation, the step size of finite difference is adaptively chosen according to the curvature, leading to fewer iteration steps and satisfactory inpainting quality. compared with other high order pde methods and layered methods, the proposed approach is more concise and doesnt need image decomposition. experiments are carried out to show effectiveness of the method.",
    "present_kp": [
      "image inpainting",
      "structure",
      "texture",
      "pde",
      "heat transfer",
      "anisotropic",
      "finite difference"
    ],
    "absent_kp": []
  },
  {
    "title": "the distribution of adiponectin receptors on human peripheral blood mononuclear cells.",
    "abstract": "adiponectin, an adipocytokine with anti-inflammatory and insulin-sensitizing properties, may provide a mechanism by which insulin resistance accelerates autoimmunity in type 1 diabetes (t1d). its actions are mediated by two receptors, adiponectin receptors 1 (adipor1) and 2 (adipor2). in this study, we measured their distribution on human peripheral mononuclear cells by flow cytometry. adipor1 is present approximately on 1% of t cells, 93% of monocytes, 47% of b cells, and 21% of nk cells (p < 0.01 for difference between subsets). the distribution of adipor2 was found to be similar (r= 0.992, p < 0.01), and staining could be blocked in an antigen-specific manner. we were also able to confirm our finding at an rna level by pcr using sequence-specific primers. our data are consistent with an immunoregulatory role for adiponectin in t1d.",
    "present_kp": [
      "adiponectin",
      "insulin resistance",
      "type 1 diabetes"
    ],
    "absent_kp": []
  },
  {
    "title": "your time and my time: a temporal approach to groupware calendar systems.",
    "abstract": "groupware calendar systems (gcs)on-line, networked calendar softwareare increasingly being used in large organizations. this paper presents a study of the use of gcs in organizations. it reviews major issues in the area, showing that current efforts lack attention to the temporal aspect, which seems to be at the center of a gcs. sociotemporal order is a foundation of social organization, and as such it is embedded in the use of gcs. the paper reports on a survey of gcs use in a business organization to show the relevancy of the temporal approach. it concludes by showing the increasing importance of gcs in particular and calendar technologies in general in new patterns of working (virtual, mobile, and flexible) in emerging forms of organizations.",
    "present_kp": [
      "groupware calendar systems",
      "calendar technologies",
      "groupware"
    ],
    "absent_kp": [
      "social time",
      "clock time",
      "time management"
    ]
  },
  {
    "title": "a riccati-based primal interior point solver for multistage stochastic programming - extensions.",
    "abstract": "we show that a riccati-based multistage stochastic programming solver for problems with separable convex linear/nonlinear objective developed in previous papers can be extended to solve more general stochastic programming problems. with a lagrangean relaxation approach, also local and global equality constraints can be handled by the riccati-based primal interior point solver. the efficiency of the approach is demonstrated on a 10 staged stochastic programming problem containing both local and global equality constraints. the problem has 1.9 million scenarios, 67 million variables and 119 million constraints, and was solved in 97 min on a 32 node pc cluster.",
    "present_kp": [
      "stochastic programming"
    ],
    "absent_kp": [
      "interior point methods",
      "parallel computations"
    ]
  },
  {
    "title": "a note on yaos theorem about pseudo-random generators.",
    "abstract": "yaos theorem gives an equivalence between the indistinguishability of a pseudo-random generator and the unpredictability of the next bit from an asymptotic point of view. in this paper we present with detailed proofs, modified versions of yaos theorem which can be of interest for the study of practical cryptographic primitives. in particular we consider non-asymptotic versions. we study the case of one pseudo-random generator, then the case of a family of pseudo-random generators with the same fixed length and finally we consider the asymptotic case. we compute in each case the cost of the reduction (in the sense of complexity theory) between the two algorithms.",
    "present_kp": [
      "complexity",
      "yaos theorem",
      ""
    ],
    "absent_kp": [
      "cryptography",
      "distinguishable",
      "prediction",
      "pseudo-randomness",
      "pseudo-random number generator"
    ]
  },
  {
    "title": "preferential normal fuzzy subgroups.",
    "abstract": "in this paper we consider the notion of normality on preferential fuzzy subgroups of a finite group g and define the associated concept of a normal pinned-flag. we discuss the preferential equality of quotients and products of normal preferential fuzzy subgroups. further normalizers of fuzzy subgroups under preferential equality are briefly dealt with. examples are given to illustrate the structure of preferential normal fuzzy subgroups and normal pinned-flags.",
    "present_kp": [
      "preferential equality",
      "normal fuzzy subgroup",
      "normal pinned-flags",
      "normalizer"
    ],
    "absent_kp": [
      "keychain"
    ]
  },
  {
    "title": "an orientation inference framework for surface reconstruction from unorganized point clouds.",
    "abstract": "in this paper, we present an orientation inference framework for reconstructing implicit surfaces from unoriented point clouds. the proposed method starts from building a surface approximation hierarchy comprising of a set of unoriented local surfaces, which are represented as a weighted combination of radial basis functions. we formulate the determination of the globally consistent orientation as a graph optimization problem by treating the local implicit patches as nodes. an energy function is defined to penalize inconsistent orientation changes by checking the sign consistency between neighboring local surfaces. an optimal labeling of the graph nodes indicating the orientation of each local surface can, thus, be obtained by minimizing the total energy defined on the graph. the local inference results are propagated over the model in a front-propagation fashion to obtain the global solution. the reconstructed surfaces are consolidated by a simple and effective inspection procedure to locate the erroneously fitted local surfaces. a progressive reconstruction algorithm that iteratively includes more oriented points to improve the fitting accuracy and efficiently updates the rbf coefficients is proposed. we demonstrate the performance of the proposed method by showing the surface reconstruction results on some real-world 3-d data sets with comparison to those by using the previous methods.",
    "present_kp": [
      "graph optimization",
      "implicit surface",
      "orientation inference",
      "surface reconstruction"
    ],
    "absent_kp": [
      "belief propagation"
    ]
  },
  {
    "title": "a location model for a web service intermediary.",
    "abstract": "recently, web services have entered the competition for a new type of distributed e-commerce platform. this paper studies the placement of servers of a web service intermediary. the intermediary serves as a common interface to its clients while obtaining web services from independent providers. the intermediary locates its servers by minimizing costs where a major component is network latency. we propose and study an integer programming formulation to determine the locations and usage rates of the servers of an intermediary. a greedy heuristic method is used to obtain good solutions to the problem.",
    "present_kp": [
      "web services",
      "network latency"
    ],
    "absent_kp": [
      "web services intermediary",
      "facility location",
      "heuristic algorithm"
    ]
  },
  {
    "title": "continuity focused choice of maxima: yet another defuzzification method.",
    "abstract": "in this paper, we present a new defuzzification method for a mamdani fuzzy controller. the method selects an element of the core of the fuzzy output set as defuzzification value, in a way that the continuity of the controller can be preserved. however, in order for this continuity to be guaranteed, some constraints on the components of the controller have to be taken into account. except for some constraints of continuity, convexity and points of intersection for the input and output linguistic terms, we will define the property of linguistic continuity that has to be fulfilled by the controller. further we show that the new defuzzification method is computationally very efficient and we point out how the fuzzy controller can be tuned by modifying the output linguistic terms. finally, we also give two variants of the proposed formula.",
    "present_kp": [
      "defuzzification"
    ],
    "absent_kp": [
      "fuzzy sets",
      "control theory"
    ]
  },
  {
    "title": "multimodal estimation of user interruptibility for smart mobile telephones.",
    "abstract": "context-aware computer systems are characterized by the ability to consider user state information in their decision logic. one example application of context-aware computing is the smart mobile telephone. ideally, a smart mobile telephone should be able to consider both social factors (i.e., known relationships between contactor and contactee) and environmental factors (i.e., the contactee's current locale and activity) when deciding how to handle an incoming request for communication.toward providing this kind of user state information and improving the ability of the mobile phone to handle calls intelligently, we present work on inferring environmental factors from sensory data and using this information to predict user interruptibility. specifically, we learn the structure and parameters of a user state model from continuous ambient audio and visual information from periodic still images, and attempt to associate the learned states with user-reported interruptibility levels. we report experimental results using this technique on real data, and show how such an approach can allow for adaptation to specific user preferences.",
    "present_kp": [
      "smart mobile telephones",
      "user interruptibility"
    ],
    "absent_kp": [
      "hmms",
      "hierarchical hmms",
      "scene learning",
      "context awareness"
    ]
  },
  {
    "title": "modelling of ph and inorganic carbon speciation in estuaries using the composition of the river and seawater end members.",
    "abstract": "an equilibrium model based on the co2 system was developed to model ph throughout the estuarine salinity range using the composition of the river and seawater end members. in order to validate the model, a spectrophotometric method was used to measure ph in an estuary and laboratory mixing experiments. the model successfully represented the measured ph values (0.1 ph units) for three different river water end member compositions. the ph decreased at low salinities (s?02), increased at intermediate salinities (s?215), and then stabilised for higher salinity values. changes in the inorganic carbon speciation in the low salinity region of estuaries result in assimilation of river borne co2(aq).",
    "present_kp": [
      "estuarine",
      "co2 system",
      "inorganic carbon"
    ],
    "absent_kp": [
      "spectrophotometric ph measurement"
    ]
  },
  {
    "title": "stochastic stability of markovian jump bam neural networks with leakage delays and impulse control.",
    "abstract": "this paper deals with the globally exponential stability of impulsive bidirectional associative memory (bam) neural networks with both markovian jump parameters and mixed time delays. the jumping parameters are determined by a continuous-time, discrete-state markov chain. different from the previous literature, the mixed time delays considered here comprise discrete, distributed and leakage time-varying delays. by using the lyapunovkrasovskii functional having triple integral terms and model transformation technique, some novel sufficient delay-dependent conditions are derived to ensure the globally exponential stability in the mean square of the suggested system. moreover, the derivatives of time delays are not necessarily zero or smaller than one since several free matrices are introduced in our results. finally, a numerical example and its simulations are provided to demonstrate the effectiveness of the theoretical results.",
    "present_kp": [
      "markovian jump bam neural networks",
      "leakage delays",
      "lyapunovkrasovskii functional",
      "impulse control"
    ],
    "absent_kp": [
      "global exponential stability"
    ]
  },
  {
    "title": "manifold discriminant regression learning for image classification.",
    "abstract": "least square regression (lsr) and its variants have been widely used for classification tasks. however, lsr-based methods ignore the local geometry structure of the data and the transformation matrix is not sparse or robust. in this paper, a novel linear regression (lr) framework is proposed for image classification. two concrete algorithms are proposed under the framework, which are named manifold discriminant regression learning (mdrl) and robust manifold discriminant regression learning (rmdrl). mdrl introduces different norms for different purposes in the learning steps. mdrl introduces a within-class graph and between-class graph to compute an optimal subspace that can separate data points belonging to different class as far as possible and keep the data points from the same class closely. mdrl joints different norms constraints to generate sparse projections for feature extraction and classification. to enhance the robustness of discriminative lsr (dlsr), rmdrl uses the nuclear norm as a regularization term to learn a robust projection matrix. extensive experiments are conducted on many databases to evaluate the performance of the proposed methods and the states-of-the-art algorithms. the experimental results indicate that our proposed methods outperform the related algorithms.",
    "present_kp": [
      "discriminant",
      "linear regression",
      "image classification"
    ],
    "absent_kp": [
      "manifold regularization"
    ]
  },
  {
    "title": "neural network based focal liver lesion diagnosis using ultrasound images.",
    "abstract": "present study proposes a computer-aided diagnostic system to assist radiologists in identifying focal liver lesions in b-mode ultrasound images. the proposed system can be used to discriminate focal liver diseases such as cyst, hemangioma, hepatocellular carcinoma and metastases, along with normal liver. the study is performed with 111 real ultrasound images comprising of 65 typical and 46 atypical images, which were taken from 88 subjects. these images are first enhanced and then regions of interest are segmented into 800 non-overlapping segmented regions-of-interest. subsequently 208-texture based features are extracted from each segmented region-of-interest. a two step neural network classifier is designed for classification of five liver image categories. in the first step, a neural network classifier gives classification among five liver image categories. if neural network decision is for more than one class as obtained from the first step, binary neural network classifiers are used in the second step for crisp classification between two classes. test results of two-step neural network classifier showed correct decisions of 432 out of 500 segmented regions-of-interest in test set with classification accuracy of 86.4%. the classifier has given correct diagnosis of 90.3% (308/340) in the tested segmented regions-of-interest from typical cases and 77.5% (124/160) in tested segmented regions-of-interest from atypical cases.",
    "present_kp": [
      "ultrasound",
      "focal liver lesions",
      "classification",
      "neural network classifier"
    ],
    "absent_kp": [
      "feature extraction"
    ]
  },
  {
    "title": "hex-splines: a novel spline family for hexagonal lattices.",
    "abstract": "this paper proposes a new family of bivariate, nonseparable splines, called hex-splines, especially designed for hexagonal lattices. the starting point of the construction is the indicator function of the voronoi cell, which is used to define in a natural way the first-order hex-spline. higher order hex-splines are obtained by successive convolutions. a mathematical analysis of this new bivariate spline family is presented. in particular, we derive a closed form for a hex-spline of arbitrary order. we also discuss important properties, such as their fourier transform and the fact they form a riesz basis. we also highlight the approximation order. for conventional rectangular lattices, hex-splines revert to classical separable tensor-product b-splines. finally, some prototypical applications and experimental results demonstrate the usefulness of hex-splines for handling hexagonally sampled data.",
    "present_kp": [
      "hexagonal lattices"
    ],
    "absent_kp": [
      "approximation theory",
      "bivariate splines",
      "sampling theory"
    ]
  },
  {
    "title": "treewidth and minimum fill-in on permutation graphs in linear time.",
    "abstract": "permutation graphs form a well-studied subclass of cocomparability graphs. permutation graphs are the cocomparability graphs whose complements are also cocomparability graphs. a triangulation of a graph g is a graph h that is obtained by adding edges to g to make it chordal. lino triangulation of g is a proper subgraph of h then h is called a minimal triangulation. the main theoretical result of the paper is a characterisation of the minimal triangulations of a permutation graph, that also leads to a succinct and linear-time computable representation of the set of minimal triangulations. we apply this representation to devise linear-time algorithms for various minimal triangulation problems on permutation graphs, in particular, we give linear-time algorithms for computing treewidth and minimum fill-in on permutation graphs.",
    "present_kp": [
      "treewidth",
      "minimal triangulation",
      "linear time",
      "permutation graphs"
    ],
    "absent_kp": [
      "graph algorithms",
      "interval graphs"
    ]
  },
  {
    "title": "evolutionary online services.",
    "abstract": "this paper present a technique based on genetic algorithms for generating online adaptive services. online adaptive systems provide flexible services to a mass of clients/users for maximizing some system goals; they dynamically adapt the form and the content of the issued services while the population of clients evolve over time. the idea of online genetic algorithms (online gas) is to use the online clients response behavior as a fitness function in order to produce the next generation of services. the principle implemented in online gas, \"the application environment is the fitness\", allow to model highly evolutionary domains where both services providers and clients change and evolve over time. the flexibility and the adaptive behavior of this approach seems to be very relevant and promising for applications characterized by highly dynamical features such as in the web domain (online newspapers, e-markets, websites and advertising engines). nevertheless the proposed technique has a more general aim for application environments characterized by a massive number of anonymous clients/users which require personalized services, such as in the case of many new it applications.",
    "present_kp": [
      "genetic algorithms"
    ],
    "absent_kp": [
      "evolutionary computation",
      "adaptive models",
      "online consumer behavior"
    ]
  },
  {
    "title": "resource space view tour mechanism.",
    "abstract": "the resource space model is a new semantic data model for managing various resources. on the basis of the model, this paper proposes a view mechanism for finding and reusing legacy resource spaces according to users' idiosyncratic interests. it establishes a flexible reorganizing mechanism of legacy resource spaces to reduce redundant work. as a viewpoint organization strategy, the resource subject tree is proposed as an effective way of discovering desirable resource space views. a case study is presented to demonstrate the proposed approach.",
    "present_kp": [
      "data model",
      "resource space model",
      "resource space view",
      "viewpoint"
    ],
    "absent_kp": []
  },
  {
    "title": "a vision based human robot interface for robotic walkthroughs in a biotech laboratory.",
    "abstract": "both robots and personal computers established new markets about 30 years ago and were enabling factors in automation and information technology. however, while you can see personal computers in almost every home nowadays, the domain of robots in general still is mostly restricted to industrial automation. due to the physical impact of robots, a safe design is essential, which most robots still lack of and therefore prevent their application for personal use, although a slow change can be noticed by the introduction of dedicated robots for specific tasks, which can be classified as service robots. moreover, as more and more robots are designed as service robots, their developers face the challenge of reducing the machines' complexity and providing smart user interface methods. ideally the robot would be able to cooperate with a human, just like another human would.",
    "present_kp": [],
    "absent_kp": [
      "life sciences",
      "model based tracking",
      "hri",
      "lab automation"
    ]
  },
  {
    "title": "an entropy-based stability algorithm for regulating the movement of manet nodes.",
    "abstract": "this paper proposes an algorithm that enables mobile nodes to implement self-regulated movements in mobile ad-hoc networks (manets). it is important for mobile nodes to maintain a certain level of network-based stability by harmonizing these nodes' movements autonomously due to their limited transmission range and dynamic topology. entropy methods based on relative position are suggested, as a means for mobile nodes to regulate their local movements. simulations show that an early warning mechanism is suitable to maintain movement-based stability. isolation can be reduced by 99%, with an increased network cost of 12% higher power consumption, using the proposed algorithm.",
    "present_kp": [
      "relative position",
      "entropy",
      "stability",
      "isolation"
    ],
    "absent_kp": [
      "mobile ad hoc nnetworks",
      "topology control",
      "movement profile"
    ]
  },
  {
    "title": "multiperiod optimization for the design and planning of multiproduct batch plants.",
    "abstract": "this paper presents a general multiperiod optimization model, which simultaneously solves the design and planning decisions in multiproduct batch plants. therefore, the trade-offs between both problems are taken into account as well as variations due to seasonal effects, demand patterns, etc. from the design point of view, the model is formulated considering batch and semicontinuous units, the allocation of intermediate storage, and structural decisions. following the usual procurement policy, equipment is provided using discrete sizes. from the planning point of view, the formulation takes into account both products and raw materials inventories, product demands and raw materials supplies that vary seasonally in a multiperiod approach. the objective is the maximization of an economic function, which considers incomes, and both investment and operation costs. a plant that produces five oleoresins in seven stages is used to illustrate this approach.",
    "present_kp": [
      "multiperiod optimization",
      "multiproduct batch plants"
    ],
    "absent_kp": [
      "batch process design"
    ]
  },
  {
    "title": "unsupervised color-texture segmentation based on soft criterion with adaptive mean-shift clustering.",
    "abstract": "an improved approach for j value segmentation (jseg) is presented for unsupervised color-texture segmentation. instead of the color quantization algorithm used in jseg, an automatic classification method using adaptive rnean-shift (ams) clustering is applied for nonparametric clustering of image data set. the clustering results are used to construct gaussian mixture modelling for the calculation of soft j value. the region growing algorithm used in jseg is then applied in segmenting the image based on the multiscale soft j-images. experiments show that the improved method overcomes the limitations of jseg successfully and is more robust.",
    "present_kp": [
      "color-texture segmentation",
      "jseg",
      "adaptive mean-shift clustering",
      "soft j value"
    ],
    "absent_kp": [
      "gaussian mixture modeling"
    ]
  },
  {
    "title": "monolithic integration of a high-performance clustered insulated gate bipolar transistor with low-voltage components to form a 3kv intelligent power chip.",
    "abstract": "for the first time, we evaluate the feasibility of monolithic integration of low-voltage components, such as n and p channel mosfets, into a 3kv novel planar power semiconductor device, called the clustered insulated gate bipolar transistor, to realise an intelligent power chip. the power device employs mos control with a thyristor to lower the on-state conduction losses and a unique self-clamping feature that provides current saturation at high gate voltages and enables the incorporation of low-voltage devices without any additional processing. this combination paves the way for realising an intelligent power chip with enhanced performance with respect to on-chip temperature, over-current and over-voltage protection circuitry.",
    "present_kp": [
      "clustered insulated gate bipolar transistor",
      "mos"
    ],
    "absent_kp": [
      "intelligence power chip",
      "power intergrated chip",
      "nmos",
      "pmos",
      "igbt"
    ]
  },
  {
    "title": "fault-tolerant routings with minimum optical index.",
    "abstract": "we construct sets of routings in the complete directed graph (k) over right arrow (n) that tolerate up to f failures of nodes or links. these routings are optimal with respect to several desirable criteria. in addition, our routings have minimum (or close to minimum) possible optical indices, which means that wavelengths can be assigned to the directed paths in the routings in an efficient manner. this property is useful in the context of optical networks.",
    "present_kp": [
      "optical network",
      "routing"
    ],
    "absent_kp": [
      "fault tolerance"
    ]
  },
  {
    "title": "differential shape statistical analysis.",
    "abstract": "a novel statistical approach that involves differential shape is proposed to analyze contour segments. first, a moment-based algorithm to represent the differential contour segment in an efficient way is introduced. then, a curvature mean-shift method is adopted to search for the salient features. an optimized function is also developed to segment a contour into parts based on its structural properties. compared with some other methods used in css (curvature scale space) and shock graphs, our method is more powerful for shape contour analysis, especially for the incomplete or occluded contours. experiments show that our method can track salient parts in real-time and give a judgment of the basic shape properties such as symmetry.",
    "present_kp": [
      "differential shape",
      "statistical analysis"
    ],
    "absent_kp": [
      "shape analysis"
    ]
  },
  {
    "title": "solution methods for scheduling of heterogeneous parallel machines applied to the workover rig problem.",
    "abstract": "a novel mathematical model for the workover rig scheduling. identifying several classes of valid inequalities. developed a very efficient hyper-heuristic method. developed a branch, price and cut algorithm for the problem. work is based on a case study of petrobras, brazilian petroleum company.",
    "present_kp": [
      "workover rig scheduling",
      "branch",
      "price and cut"
    ],
    "absent_kp": [
      "arc-time-index formulation",
      "hyper-heuristics"
    ]
  },
  {
    "title": "measuring similarity to detect qualified links.",
    "abstract": "the early success of link-based ranking algorithms was predicated on the assumption that links imply merit of the target pages. however, today many links exist for purposes other than to confer authority. such links bring noise into link analysis and harm the quality of retrieval. in order to provide high quality search results, it is important to detect them and reduce their influence. in this paper, a method is proposed to detect such links by considering multiple similarity measures over the source pages and target pages. with the help of a classifier, these noisy links are detected and dropped. after that, link analysis algorithms are performed on the reduced link graph. the usefulness of a number of features are also tested. experiments across 53 query-specific datasets show our approach almost doubles the performance of kleinberg's hits and boosts bharat and henzinger's imp algorithm by close to 9% in terms of precision. it also outperforms a previous approach focusing on link farm detection.",
    "present_kp": [
      "link analysis"
    ],
    "absent_kp": [
      "link classification",
      "web search engine",
      "web spam"
    ]
  },
  {
    "title": "behavioral experiments in networked trade.",
    "abstract": "we report on an extensive series of highly controlled human subject experiments in networked trade. our point of departure is a simple and well-studied bipartite network exchange model, for which previous work has established a detailed equilibrium theory relating wealth to network topology. a notable feature of this theory is its prediction that there may be significant local variation in equilibrium wealths and prices purely as a result of structural asymmetries in the network. our experiments mix recent lines of thought from algorithmic game theory, behavioral economics and social network theory, and are among the first and largest behavioral economics experiments on network effects conducted to date. they continue a line of human subject experiments on networked games and optimization allowing only local interactions.",
    "present_kp": [
      "behavioral economics"
    ],
    "absent_kp": [
      "network economics"
    ]
  },
  {
    "title": "content-adaptive motion estimation algorithm for coarse-grain svc.",
    "abstract": "a joint model of scalable video coding (svc) uses exhaustive mode and motion searches to select the best prediction mode and motion vector for each macroblock (mb) with high coding efficiency at the cost of computational complexity. if major characteristics of a coding mb such as the complexity of the prediction mode and the motion property can be identified and used in adjusting motion estimation (me), one can design an algorithm that can adapt coding parameters to the video content. this way, unnecessary mode and motion searches can be avoided. in this paper, we propose a content-adaptive me for svc, including analyses of mode complexity and motion property to assist mode and motion searches. an experimental analysis is performed to study interlayer and spatial correlations in the coding information. based on the correlations, the motion and mode characteristics of the current mb are identified and utilized to adjust each step of me at the enhancement layer including mode decision, search-range selection, and prediction direction selection. experimental results show that the proposed algorithm can significantly reduce the computational complexity of svc while maintaining nearly the same rate distortion performance as the original encoder.",
    "present_kp": [
      "motion estimation ",
      "scalable video coding ",
      "spatial correlation"
    ],
    "absent_kp": [
      "interlayer correlation"
    ]
  },
  {
    "title": "in silico pharmacophore modeling on known pyridinium oxime reactivators of cyclosarin (gf) inhibited ache to aid discovery of potential, more efficacious novel non-oxime reactivators.",
    "abstract": "cyclohexyl methylphosphonofluoridate (cyclosarin, cyclosin, gf) is a highly toxic organophosphorus (op) nerve agent considered as potential warfare threats and known to be resistant to conventional oxime antidotal therapy. to aid discovery of novel antidotes for gf toxicity, a three-dimensional in silico pharmacophore model for reactivation efficacy against gf intoxication is presented. the model was generated from published experimental percentage reactivation data on oximes as changes of ache/buche activities in the whole blood after cyclosarin intoxication and administration. the generated pharmacophore model was found to contain a hydrogen bond donor site and two ring aromatic sites as necessary optimal features for reactivation of gf intoxication. stereo-electronic features of oximes reported by us earlier provided guidance to develop the model and were found to be consistent with the reported structure activity data. furthermore, from virtual screening of two commercial databases, maybridge and chemnavigator using map-fitting of the model led us to identify two new non-oxime compounds showing reactivation efficacy within 10-fold range of 2-pam for dfp-inhibited ache. since gf is a g simulator like dfp (diisopropylfluorophosphate), the model should have the potential for discovery of novel reactivators against gf intoxication.",
    "present_kp": [
      "cyclosarin ",
      "oximes",
      "reactivation"
    ],
    "absent_kp": [
      "3d pharmacophore model",
      "acetylcholinesterase ",
      "butyrylcholinesterase buche)",
      "in vitro"
    ]
  },
  {
    "title": "collision detection for deformable objects.",
    "abstract": "interactive environments for dynamically deforming objects play an important role in surgery simulation and entertainment technology. these environments require fast deformable models and very efficient collision handling techniques. while collision detection for rigid bodies is well investigated, collision detection for deformable objects introduces additional challenging problems. this paper focuses on these aspects and summarizes recent research in the area of deformable collision detection. various approaches based on bounding volume hierarchies, distance fields and spatial partitioning are discussed. in addition, image-space techniques and stochastic methods are considered. applications in cloth modeling and surgical simulation are presented.",
    "present_kp": [
      "deformable collision detection",
      "distance field"
    ],
    "absent_kp": [
      "self-collision detection",
      "continuous collision detection",
      "stochastic collision detection",
      "image-space collision detection",
      "bounding-volume hierarchy",
      "spatial subdivision"
    ]
  },
  {
    "title": "a modular network scheme for unsupervised 3d object recognition.",
    "abstract": "this paper presents an unsupervised learning scheme for recognizing 3d objects from their 2d projected images. the scheme consists of a mixture of nonlinear autoencoders which can compress various views of 3d objects into representations that indicate the view direction. we evaluate the performance of the proposed modular network scheme through simulations using 3d wire-frame objects and discuss its related issues on object representations in the primate visual cortex.",
    "present_kp": [
      "unsupervised learning",
      "3d object recognition",
      "autoencoders"
    ],
    "absent_kp": [
      "modular networks"
    ]
  },
  {
    "title": "designing for small display screens.",
    "abstract": "wireless access to the internet via pdas (personal digital assistants) provides web type services in the mobile world. what we are lacking are design guidelines for such pda services. for web publishing, however, there are many resources to look for guidelines. the guidelines can be classified according to which aspect of the web media they are related: software/hardware, content and its organization, or aesthetics and layout. in order to be applicable to pda services, these guidelines have to be modified. in this paper we analyze the main characteristics of pdas and their influence to the guidelines.",
    "present_kp": [
      "personal digital assistant",
      "guidelines"
    ],
    "absent_kp": [
      "world wide web"
    ]
  },
  {
    "title": "exploiting task-level concurrency in a programmable network interface.",
    "abstract": "programmable network interfaces provide the potential to extend the functionality of network services but lead to instruction processing overheads when compared to application-specific network interfaces. this paper aims to offset those performance disadvantages by exploiting task-level concurrency in the workload to parallelize the network interface firmware for a programmable controller with two processors. by carefully partitioning the handler procedures that process various events related to the progress of a packet, the system can minimize sharing, achieve load balance, and efficiently utilize on-chip storage. compared to the uniprocessor firmware released by the manufacturer, the parallelized network interface firmware increases throughput by 65% for bidirectional udp traffic of maximum-sized packets, 157% for bidirectional udp traffic of minimum-sized packets, and 32--107% for real network services. this parallelization results in performance within 10--20% of a modern asic-based network interface for real network services.",
    "present_kp": [
      "network",
      "task",
      "event",
      "performance",
      "programmable network interface",
      "throughput",
      "service",
      "interfaces",
      "paper",
      "traffic",
      "control",
      "processor",
      "sharing",
      "partition",
      "firmware",
      "procedure",
      "workload",
      "parallel",
      "storage",
      "functional",
      "concurrency",
      "instruction",
      "process"
    ],
    "absent_kp": [
      "parallel programming",
      "applications",
      "systems",
      "ethernet",
      "load balancing",
      "programmer"
    ]
  },
  {
    "title": "agile and object oriented practices in embedded systems.",
    "abstract": "embedded systems are the most prevalent of all computer systems in the world. more than 99% of all computer/microcontroller products that are sold each year are single purpose embedded systems rather than workstations, desktops, laptops, or server systems. this workshop will gather embedded systems programmers and engineers to discuss how agile and object oriented practices affect the design and implementation of embedded systems. we will explore how/whether a product design is affected when an embedded system implements functionality that is inherently object oriented in nature. we will explore how constraints that are unique to embedded systems affect the adoption of agile and objected oriented processes and practices. the primary goal for this workshop is to provide feedback to the embedded systems community on which practices are judged as useful and which are not.",
    "present_kp": [
      "embedded systems",
      "agile"
    ],
    "absent_kp": []
  },
  {
    "title": "toward agile: an integrated analysis of quantitative and qualitative field data on software development agility.",
    "abstract": "as business and technology environments change at an unprecedented rate, software development agility to respond to changing user requirements has become increasingly critical for software development performance. agile software development approaches, which emphasize sense-and-respond, self-organization, cross-functional teams, and continuous adaptation, have been adopted by an increasing number of organizations to improve their software development agility. however, the agile development literature is largely anecdotal and prescriptive, lacking empirical evidence and theoretical foundation to support the principles and practices of agile development. little research has empirically examined the software development agility construct in terms of its dimensions, determinants, and effects on software development performance. as a result, there is a lack of understanding about how organizations can effectively implement an agile development approach. using an integrated research approach that combines quantitative and qualitative data analyses, this research opens the black box of agile development by empirically examining the relationships among two dimensions of software development agility (software team response extensiveness and software team response efficiency), two antecedents that can be con trolled (team autonomy and team diversity), and three aspects of software development performance (on-time completion, on-budget completion, and software functionality). our pls results of survey, responses of 399 software project managers suggest that the relationships among these variables are more complex than what has been perceived by the literature. the results suggest a tradeoff relationship between response extensiveness and response efficiency. these two agility dimensions impact software development performance differently: response efficiency positively affects all of on-time completion, on-budget completion, and software functionality, whereas response extensiveness positively affects only software functionality. the results also suggest that team autonomy has a positive effect on response efficiency and a negative effect on response extensiveness, and that team diversity has a positive effect on response extensiveness, we conducted 10 post hoc case studies to qualitatively cross-validate our pls results and provide rich, additional insights regarding the complex, dynamic interplays between autonomy, diversity, agility, and performance. the qualitative analysis also provides explanations for both supported and unsupported hypotheses. we discuss these qualitative analysis results and conclude with the theoretical and practical implications of our research findings for agile development approaches.",
    "present_kp": [
      "software development agility",
      "agile software development",
      "team autonomy",
      "team diversity",
      "software development performance"
    ],
    "absent_kp": [
      "requirement change",
      "partial least square",
      "case study"
    ]
  },
  {
    "title": "currentvoltage characteristics of p-si/carbon junctions fabricated by pulsed laser deposition.",
    "abstract": "amorphous carbon/p-si junctions were fabricated at different temperatures using krf excimer laser (?=248nm, pulsed duration 20ns). the currentvoltage measurements of the devices showed diode characteristics. the value of various junction parameters such as ideality factor, barrier height, and series resistance were determined from forward bias iv characteristics, cheung method, and nordes function. there was a good agreement between the diodes parameters obtained from these methods. the ideality factor of ?1.12 and barrier height of ?0.37ev were estimated using currentvoltage characteristics for films grown at room temperature.",
    "present_kp": [
      "carbon",
      "pulsed laser",
      "junction parameters"
    ],
    "absent_kp": [
      "thin films"
    ]
  },
  {
    "title": "how productivity improves in hands-free continuous dictation tasks: lessons learned from a longitudinal study.",
    "abstract": "speech recognition technology continues to improve, but users still experience significant difficulty using the software to create and edit documents. the reported composition speed using speech software is only between 8 and 15 words per minute , much lower than people's normal speaking speed of 125-150 words per minute. what causes the huge gap between natural speaking and composing using speech recognition? is it possible to narrow the gap and make speech recognition more promising to users? in this paper we discuss users' learning processes and the difficulties they experience as related to continuous dictation tasks using state of the art automatic speech recognition (asr) software. detailed data was collected for the first time on various aspects of the three activities involved in document composition tasks: dictation, navigation, and correction. the results indicate that navigation and error correction accounted for big chunk of the dictation task during the early stages of interaction. as users gained more experience, they became more efficient at dictation, navigation and error correction. however, the major improvements in productivity were due to dictation quality and the usage of navigation commands. these results provide insights regarding the factors that cause the gap between user expectation with speech recognition software and the reality of use, and how those factors changed with experience. specific advice is given to researchers as to the most critical issues that must be addressed.",
    "present_kp": [
      "error correction",
      "speech recognition software"
    ],
    "absent_kp": [
      "automatic speech recognition technologies"
    ]
  },
  {
    "title": "lets keep in touch online: a facebook aware virtual human interface.",
    "abstract": "a virtual human is an effective interface for interacting with users and plays an important role in carrying out certain tasks. as social networking sites are getting more and more popular, we propose a facebook aware virtual human. the social networking sites are used to empower virtual humans for interpersonal conversational interaction in this paper. we combine internet world, physical world and 3d virtual world together to create a new interface for users to interact with an autonomous virtual human which can behave like a real modern human. in order to take advantages of social networking sites, virtual human gathers information of a user from its profile, its likes, dislikes and gauge mood from most recent status update. in two user studies, we investigated whether and how this new interface can enhance humanvirtual human interaction. some positive results concluded from these studies will be guidelines on research and development of future virtual human interfaces.",
    "present_kp": [
      "virtual humans",
      "social networking sites"
    ],
    "absent_kp": [
      "human-virtual human interaction",
      "dialogue"
    ]
  },
  {
    "title": "spectral methods for thesaurus construction.",
    "abstract": "traditionally, popular synonym acquisition methods are based on the distributional hypothesis, and a metric such as jaccard coefficients is used to evaluate the similarity between the contexts of words to obtain synonyms for a query. on the other hand, when one tries to compile and clean a thesaurus, one often already has a modest number of synonym relations at hand. could something be done with a half-built thesaurus alone? we propose the use of spectral methods and discuss their relation to other network-based algorithms in natural language processing (nlp), such as page rank and bootstrapping. since compiling a thesaurus is very laborious, we believe that adding the proposed method to the toolkit of thesaurus constructors would significantly ease the pain in accomplishing this task.",
    "present_kp": [
      "synonym acquisition",
      "thesaurus"
    ],
    "absent_kp": [
      "synonym extraction",
      "spectral clustering",
      "graph laplacian"
    ]
  },
  {
    "title": "cumulative distribution networks and the derivative-sum-product algorithm: models and inference for cumulative distribution functions on graphs.",
    "abstract": "we present a class of graphical models for directly representing the joint cumulative distribution function (cdf) of many random variables, called cumulative distribution networks (cdns). unlike graphs for probability density and mass functions, for cdfs the marginal probabilities for any subset of variables are obtained by computing limits of functions in the model, and conditional probabilities correspond to computing mixed derivatives. we will show that the conditional independence properties in a cdn are distinct from the conditional independence properties of directed, undirected and factor graphs, but include the conditional independence properties of bi-directed graphs. in order to perform inference in such models, we describe the 'derivative-sum-product' (dsp) message-passing algorithm in which messages correspond to derivatives of the joint cdf. we will then apply cdns to the problem of learning to rank players in multiplayer team-based games and suggest several future directions for research.",
    "present_kp": [
      "graphical models",
      "cumulative distribution function",
      "message-passing algorithm",
      "inference"
    ],
    "absent_kp": []
  },
  {
    "title": "a variational approach via bipotentials for a class of frictional contact problems.",
    "abstract": "we study a class of frictional contact problems with prescribed normal stress, for nonlinearly elastic materials. using a bipotential which depends on the constitutive map and its fenchel conjugate, and a potential which depends on the prescribed normal stress and the coefficient of friction, we deliver a weak formulation which consists of a system of two variational inequalities. the unknown is the pair of displacement vector and cauchy stress tensor. we prove the existence and the uniqueness of the weak solution by using minimization arguments. we also discuss some connections of the new variational approach to previous variational approaches.",
    "present_kp": [
      "frictional contact",
      "prescribed normal stress",
      "bipotential",
      "weak solution",
      ""
    ],
    "absent_kp": [
      "nonlinear elastic constitutive law",
      "subdifferential inclusion"
    ]
  },
  {
    "title": "cryptanalysis of publicly verifiable authenticated encryption.",
    "abstract": "recently, ma and chen proposed a new authenticated encryption scheme with public verifiability. the signer can generate a signature with message recovery for a specified recipient. with a dispute, the recipient has ability to convert the signature into an ordinary one that can be verified by anyone without divulging her/his private key and the message. however, we point out that any adversary can forge a converted signature in this article.",
    "present_kp": [
      "authenticated encryption scheme"
    ],
    "absent_kp": [
      "discrete logarithms",
      "signcryption"
    ]
  },
  {
    "title": "gold infrastructure for virtual organizations.",
    "abstract": "the paper discusses the gold project (grid-based information models to support the rapid innovation of new high value-added chemicals) whose principal aim is to carry out research and development into enabling technologies to support the formation, operation and termination of virtual organizations. the paper discusses the outcome of this research, which is the gold middleware infrastructure. the infrastructure has been implemented in the form of a set of middleware components, which address issues such as trust, security, contract monitoring and enforcement, information management and coordination. we discuss all these issues in turn and more importantly we demonstrate how current ws standards can be used to implement these issues. in addition, the paper follows a top down approach starting with a brief outline on the architectural elements derived during the requirements engineering phase and demonstrates how these elements were mapped onto actual services that were implemented according to service-oriented architecture principles and related technologies.",
    "present_kp": [
      "virtual organizations",
      "middleware"
    ],
    "absent_kp": [
      "web services security",
      "ws -* standards"
    ]
  },
  {
    "title": "counter tree diagrams: a unified framework for analyzing fast addition algorithms.",
    "abstract": "this paper presents a unified representation of fast addition algorithms based on counter tree diagrams (ctds). by using ctds, we can describe and analyze various adder architectures in a systematic way without using specific knowledge about underlying arithmetic algorithms. examples of adder architectures that can be handled by ctds include redundant-binary (rb) adders, signed-digit (sd) adders, positive-digit (pd) adders, carry-save adders, parallel counters (e.g., 3-2 counters and 4-2 counters) and networks of such basic adders/counters. this paper also discusses the ctd-based analysis of carry-propagation-free adders using various number representations.",
    "present_kp": [
      "parallel counters"
    ],
    "absent_kp": [
      "computer arithmetic algorithms",
      "multipliers",
      "datapath",
      "vlsi",
      "circuit synthesis"
    ]
  },
  {
    "title": "mobile imagery exchange (mix) toolkit: data sharing for the unconnected.",
    "abstract": "this paper proposes a healthcare solution that enables the sharing of medical videos in areas without satellite, broadband or mobile internet connectivity. rural areas often encounter issues such as shortage of electricity, poor telecommunication services and lack of health facilities which have led to a deficiency of medical care in remote areas. the deployment of mobile devices for healthcare services is vital to alleviating these concerns. a majority of mobile device owners are located in developing nations, and the number of users is continually rising. these technologies effectively support remote health care due to their low-power requirements, multiple functionalities and custom applications. the mobile imagery exchange (mix) toolkit is a software application capable of converting echocardiogram videos into text for transmission through short message service (sms), over a global system for mobile communications network. a smartphone or computer installed with the mix toolkit can be utilized to deliver effective medical care within rural regions. to significantly shrink data size, video, image and file processing techniques are applied for analysis, segmentation, compression and conversion. experiments show that data size reduction rates above 60 until 95% are achievable when transforming imagery into text. furthermore, in optimal conditions, the average sms per image ranges from 9 to 23 text messages.",
    "present_kp": [
      "health care",
      "sms"
    ],
    "absent_kp": [
      "echocardiography",
      "gsm",
      "image processing"
    ]
  },
  {
    "title": "chronological order of reversal events on rickettsia genus.",
    "abstract": "traditional algorithms for sorting permutations by signed reversals output one solution while the solution space can be huge. the enumeration of traces of solutions for this problem can be a powerful tool to help the study of rear-rangement scenarios which only include reversals. through the analysis of the permutations of six members of the rickettsia genus in relation with their common ancestral, we were able to produce all possible scenarios and infer some chronological order over the reversal events that occurred during the evolution of these species. our results matched with the scenario proposed in the literature.",
    "present_kp": [
      "evolution"
    ],
    "absent_kp": [
      "genome rearrangement"
    ]
  },
  {
    "title": "measurement of two-dimensional distribution profile of lead in a flame by planar laser-induced fluorescence spectroscopy.",
    "abstract": "producing refuse-derived fuel (rdf) is one of the most effective measures of refuse treatment. however, rdf often consists of high level of lead. to reduce lead emission during combustion, understanding of lead behaviors in flames is required. in this study, we have applied planar laser-induced fluorescence spectroscopy to detect lead not only in a non-luminous methaneair flame, but also in a luminous rdf flame. in a methaneair flame, the number density of pb atoms does not depend on the flame temperature, but also on the combustion environments. in rdf flames, because of acceleration of oxidizing process of pb, pb fluorescence profile obtained at 25% o2 became weaker than that at 20% o2.",
    "present_kp": [
      "planar laser-induced fluorescence",
      "lead"
    ],
    "absent_kp": [
      "refuse derived fuel",
      "oh radical"
    ]
  },
  {
    "title": "some characteristics of logistic and bessel random variables.",
    "abstract": "products, ratios and sums of random variables arise explicitly in many areas. of the sciences, engineering and medicine. this has increased the need to have available the widest possible range of statistical results on products, ratios and sums of random variables. in this note, the exact distributions of xy, x/y and x + y are derived when x and y are logistic and bessel random variables distributed independently of each other. tabulations of the associated percentage points obtained by inverting the derived distributions are also provided.",
    "present_kp": [
      "sums of random variables"
    ],
    "absent_kp": [
      "bessel function distribution",
      "logistic distribution",
      "products of random variables",
      "ratios of random variables"
    ]
  },
  {
    "title": "pancyclicity of recursive circulant graphs.",
    "abstract": "in this paper, we study the existence of cycles of all lengths in the recursive circulant graphs, and we show a necessary and sufficient condition for the graph being pancyclic and bipancyclic.",
    "present_kp": [
      "recursive circulant graphs"
    ],
    "absent_kp": [
      "interconnection networks",
      "pancyclic property"
    ]
  },
  {
    "title": "an integrated digital cmos time-to-digital converter with sub-gate-delay resolution.",
    "abstract": "an integrated digital cmos time-to-digital converter with sub-gate delay lsb width and 50 ps single-shot precision sigma-value has been designed and implemented for a laser range-finding application. the measurement is based on a counter and a novel two-step parallel interpolation that uses only 32 delay elements to provide 128 lsbs in the interpolator within the reference clock cycle. the circuit was fabricated in the ams 0.8 mu m cmos process and the current consumption of the circuit is < 20 ma from a single +5 v supply.",
    "present_kp": [
      "time-to-digital converter",
      "laser range-finding"
    ],
    "absent_kp": [
      "time interval measurement",
      "delay locked loop",
      "delay line"
    ]
  },
  {
    "title": "an empirical examination of factors affecting college students proactive stickiness with a web-based english learning environment.",
    "abstract": "based on the sct and u&g theory, an integrated theoretical model is developed. the results integrate findings of cognitive science & mass communication research. the cognitive, technology and social factors are identified as critical determinants. the proposed determinants significantly affect students learning gratifications. the identified critical factors impact students proactive stickiness with wbel.",
    "present_kp": [
      "proactive stickiness",
      "learning gratifications",
      "web-based english learning"
    ],
    "absent_kp": [
      "uses and gratifications theory",
      "social cognitive theory"
    ]
  },
  {
    "title": "numerical study of hydrogen permeation flux in ytterbium doped strontium cerate and thulium doped strontium cerate (ii).",
    "abstract": "this study analyses the hydrogen permeation flux model with (1) modification sin the defect concentration calculations where the concentration of substitutional cation on cerium site is utilised as the independent variable for the calculation, instead of the previous step-wise calculation with the concentration of oxygen vacancy as the independent variable and (2) the additional terms to include the oxygen partial pressure gradients for calculation of hydrogen permeation flux. the modification in the defect concentration method allows a short model simulation run time, which consequently allows incorporation of the concentration constraints in the parametric sensitivity analysis, but still produces the same set of defect concentrations as calculated in the previous methods. it is also found in this study that the discrepancy between the model and experimental results (in terms of the effect of changes in hydrogen partial pressure gradients on the hydrogen permeation flux) is not due to the influence of oxygen partial pressure gradients. parametric sensitivity analysis shows that there is no significant difference in the sensitivity of the model by comparing case a and b. the result of parameter tuning to predict the hydrogen permeation flux for 5% thulium doped strontium cerate in case b shows a similar trend to the previous study (case a). these results suggest negligible oxygen ion conductivities in these types of membrane, as reported in the literature.",
    "present_kp": [
      "hydrogen permeation flux model",
      "parametric sensitivity analysis"
    ],
    "absent_kp": [
      "mixed protonic and ionic conduction",
      "hydrogen separation"
    ]
  },
  {
    "title": "some new solitonary solutions of the modified benjaminbonamahony equation.",
    "abstract": "in this paper, we use the exp-function method to construct some new soliton solutions of the benjaminbonamahony and modified benjaminbonamahony equations. these equations have important and fundamental applications in mathematical physics and engineering sciences. the exp-function method is used to find the soliton solution of a wide class of nonlinear evolution equations with symbolic computation. this method provides the concise and straightforward solution in a very easier way. the results obtained in this paper can be viewed as a refinement and improvement of the previously known results.",
    "present_kp": [
      "exp-function method",
      "soliton solutions",
      "nonlinear evolution equations"
    ],
    "absent_kp": []
  },
  {
    "title": "gpu-based computation of discrete periodic centroidal voronoi tessellation in hyperbolic space.",
    "abstract": "periodic centroidal voronoi tessellation (cvt) in hyperbolic space provides a nice theoretical framework for computing the constrained cvt on high-genus (genus>1) surfaces. this paper addresses two computational issues related to such a hyperbolic cvt framework: (1) efficient reduction of unnecessary site copies in neighbor domains on the universal covering space, based on two special rules; (2) gpu-based parallel algorithms to compute a discrete version of the hyperbolic cvt. our experiments show that with the dramatically reduced number of unnecessary site copies in neighbor domains and the gpu-based parallel algorithms, we significantly speed up the computation of cvt for high-genus surfaces. the proposed discrete hyperbolic cvt guarantees to converge and produces high-quality results.",
    "present_kp": [
      "centroidal voronoi tessellation",
      "universal covering space",
      "hyperbolic space"
    ],
    "absent_kp": [
      "gpu algorithm"
    ]
  },
  {
    "title": "longitudinal clinical course following pharmacological treatment of methamphetamine psychosis which persists after long-term abstinence.",
    "abstract": "the present article investigated clinical symptoms and their longitudinal clinical course following pharmacological treatment in 32 female incarcerated patients suffering from methamphetamine (meth) psychosis who were referred to psychiatric consultation. the length of meth-abuse periods of the patients ranged from 2 to 31 years. a total of 31 patients suffered from psychosis at abstinence after 531 months from the self-injection of meth. nine of these 31 patients experienced episodes of psychotic relapse. the following symptoms were observe",
    "present_kp": [
      "methamphetamine psychosis",
      "pharmacological treatment"
    ],
    "absent_kp": [
      "persistent type",
      "symptom dimensions",
      "differential prognoses",
      "liability to extrapyramidal symptoms"
    ]
  },
  {
    "title": "a competitive inventory model with reallocation on a plane market.",
    "abstract": "this paper considers a duopolistic inventory model with reallocation. customers move according to the rectilinear distance over a plane market and choose a player whom they first visit with a probability dependent on their positions. this model can be interpreted as one of the unitsquare games with pure strategies of continuous cardinary. we consider the optimal strategies for two players from a viewpoint of pure strategies. the purpose of this paper is concretely to find an equilibrium point with respect to the optimal ordering quantities which minimize the total costs by using a payoff bimatrix in the deterministic model and to find the optimal solution in the stochastic model. as can be seen in this paper, there is a unique equilibrium point.",
    "present_kp": [
      "duopolistic inventory model"
    ],
    "absent_kp": [
      "reallocation of excess demand",
      "game theory",
      "equilibrium point analysis",
      "stochastic demand"
    ]
  },
  {
    "title": "robust stability analysis of generalized neural networks with multiple discrete delays and multiple distributed delays.",
    "abstract": "using the lyapunovkrasovskii functional method and the linear matrix inequality (lmi) technique, this paper is concerned with the robust stability of generalized neural networks with multiple discrete delays and multiple distributed delays. the global stability of the equilibrium point is proved under mild conditions, where the activation function is neither differentiable nor strictly monotone. for the considered system, a novel robust stability criterion of the system is derived, which can be easily solved by efficient convex optimization algorithms. and two numerical examples are given to justify the obtained results.",
    "present_kp": [
      "generalized neural networks",
      "lyapunovkrasovskii functional method",
      "multiple discrete delays",
      "multiple distributed delays"
    ],
    "absent_kp": []
  },
  {
    "title": "constructing a fuzzy-knowledge-based- system: an application for assessing the financial condition of public schools.",
    "abstract": "financial evaluation in the public sector can utilize some of the tools developed for evaluation in the private sector. however, the emphasis on the bottom line characterized by productivity measures does not adequately address all of the issues faced by public institutions. recent collaborations by researchers in management science and public administration have led to the successful development of an analytical approach that combines fuzzy set theory and knowledge based systems to produce a tool for evaluating the general performance of public institutions. successful implementations have included evaluations of the management of state governments, the financial administration of large cities, and the credit worthiness of public institutions. this paper describes a recent collaborative project, funded by the state of new york, to develop a system to evaluate the financial condition of the state's nearly 700 school districts.",
    "present_kp": [
      "financial evaluation"
    ],
    "absent_kp": [
      "fuzzy logic",
      "knowledge-based-systems",
      "performance evaluation"
    ]
  },
  {
    "title": "motivation and learning preferences of information technology learners in south african secondary schools.",
    "abstract": "the information technology (it) subject presented in south african secondary schools is considered to be a difficult subject. the programming component of it is believed to be the main cause of this difficulty. learners who struggle with programming are unable to obtain above average marks in it, as the programming component has the largest weighting in the it subject framework. the aim of the research upon which this paper is based is to identify factors related to learner achievement in programming and the it subject. the two areas that this paper investigates are learner motivation towards programming and the learning preferences of it learners. the motivated strategies for learning questionnaire (mslq) is used to determine learner motivation and the visual, aural, read/write and kinesthetic (vark) questionnaire is used to determine the learning preferences of it learners. both questionnaires provide interesting results and observations. the self-efficacy for learning and performance and control of learning beliefs motivational subscales seem to influence the performance of learners at the different schools. the vark questionnaire results for this study indicate that the learning preferences of it learners may influence understanding of programming concepts and also that it is best to present content to it learners using a balance between all four modal groups (visual, aural, read/write and kinesthetic) to ensure that the learning preferences of all learners are met. the findings of this research indicate the impact that motivation and learning preferences possibly have on the understanding of programming concepts and overall achievement in programming and the it subject. the contribution of this paper is the identification of the mslq and vark questionnaires as methods that can be used to improve teaching strategies in south african secondary schools.",
    "present_kp": [
      "vark questionnaire",
      "information technology"
    ],
    "absent_kp": [
      "introductory programming"
    ]
  },
  {
    "title": "simulation study on rotating radial electromagnetic force for use in dynamic balancing.",
    "abstract": "a tentative idea of introducing rotating electromagnetic force for dynamic balancing is presented in the paper. a method of generating the force is put forward, the force generated has adjustable magnitude, speed and phase angle. working characteristics of the force are analyzed phase windings fed with half sine currents are designed to restrain pulsation of the force vector in space. simulation results of a model indicate that a rotating force vector can be obtained by selecting a proper number of phases simultaneously fed with currents, which is able to meet the needs of ordinary engineering problems.",
    "present_kp": [
      "simulation"
    ],
    "absent_kp": [
      "electromagnetic fields",
      "magnetic forces"
    ]
  },
  {
    "title": "preparing highly entangled six-qubit genuine state in ion-trap system.",
    "abstract": "we propose a scheme for preparing a highly entangled six qubit genuine state in an ion-trap system. we evaluate the influence of lasers' small intensity fluctuation and find it is negligible in our scheme. our scheme is feasible with the present techniques.",
    "present_kp": [
      "highly entangled six-qubit genuine state",
      "ion-trap system"
    ],
    "absent_kp": [
      "preparation proposal"
    ]
  },
  {
    "title": "a splitting method for incompressible flows with variable density based on a pressure poisson equation.",
    "abstract": "a new fractional time-stepping technique for solving incompressible flows with variable density is proposed. the main feature of this method is that, as opposed to other known algorithms, the pressure is determined by just solving one poisson equation per time step, which greatly reduces the computational cost. the stability of the method is proved and the performance of the method is numerically illustrated.",
    "present_kp": [
      "fractional time-stepping"
    ],
    "absent_kp": [
      "variable density flows",
      "navier\u2013stokes",
      "projection method"
    ]
  },
  {
    "title": "discrimination discovery in scientific project evaluation: a case study.",
    "abstract": "discovering contexts of unfair decisions in a dataset of historical decision records is a non-trivial problem. it requires the design of ad hoc methods and techniques of analysis, which have to comply with existing laws and with legal argumentations. while some data mining techniques have been adapted to the purpose, the state-of-the-art of research still needs both methodological refinements, the consolidation of a knowledge discovery in databases (kdd) process, and, most of all, experimentation with real data. this paper contributes by presenting a case study on gender discrimination in a dataset of scientific research proposals, and by distilling from the case study a general discrimination discovery process. gender bias in scientific research is a challenging problem, that has been tackled in the social sciences literature by means of statistical regression. however, this approach is limited to test an hypothesis of discrimination over the whole dataset under analysis. our methodology couples data mining, for unveiling previously unknown contexts of possible discrimination, with statistical regression, for testing the significance of such contexts, thus obtaining the best of the two worlds.",
    "present_kp": [
      "discrimination discovery",
      "gender bias",
      "case study",
      "data mining"
    ],
    "absent_kp": [
      "situation testing",
      "kdd process"
    ]
  },
  {
    "title": "automatic tuning of two-level caches to embedded applications.",
    "abstract": "the power consumed by the memory hierarchy of a microprocessor can contribute to as much as 50% of the total microprocessor system power, and is thus a good candidate for optimizations. we present an automated method for tuning two-level caches to embedded applications for reduced energy consumption. the method is applicable to both a simulation-based exploration environment and a hardware-based system prototyping environment. we introduce the two-level cache tuner, or tcat - a heuristic for searching the huge solution space of possible configurations. the heuristic interlaces the exploration of the two cache levels and searches the various cache parameters in a specific order based on their impact on energy. we show the integrity of our heuristic across multiple memory configurations and even in the presence of hardware/software partitioning -- a common optimization capable of achieving significant speedups and/or reduced energy consumption. we apply our exploration heuristic to a large set of embedded applications. our experiments demonstrate the efficacy of our heuristic: on average the heuristic examines only 7% of the possible cache configurations, but results in cache sub-system energy savings of 53%, only 1% more than the optimal cache configuration. in addition, the configured cache achieves an average speedup of 30% over the base cache configuration due to tuning of cache line size to the application's needs.",
    "present_kp": [],
    "absent_kp": [
      "architecture tuning",
      "embedded systems",
      "low energy",
      "cache hierarchy",
      "configurable cache",
      "cache exploration",
      "low power",
      "cache optimization"
    ]
  },
  {
    "title": "controllability and observability of impulsive fractional linear time-invariant system.",
    "abstract": "in this paper, we deal with the controllability and observability of impulsive fractional linear time-invariant (if-lti for short) system. our main purpose is to built some necessary and sufficient conditions of controllability and observability for the if-lti system. at the same time, we establish some conclusions of controllability and observability for a continuous fractional lti system, which is a special case of the if-lti system. examples are given to illustrate our results.",
    "present_kp": [
      "if-lti system",
      "controllability",
      "observability"
    ],
    "absent_kp": [
      "continuous lti system"
    ]
  },
  {
    "title": "prediction of acute toxicity of organophosphorus pesticides using topological indices.",
    "abstract": "topological indices were used in the prediction of the acute toxicity (intraperitoneal and oral ld50) of organophosphorus pesticides on rats. models with six variables for the prediction of ld50-i.p. (r = 0.849, q(2) = 0.613) and eight variables for ld50-oral (r = 0.906, q(2) = 0.701) were selected. external group and cross-validation by use of leave-n-out tests were also performed in order to assess the stability and the prediction performance of the selected topological models.",
    "present_kp": [
      "acute toxicity",
      "organophosphorus pesticides"
    ],
    "absent_kp": [
      "topological descriptors",
      "multilinear regression"
    ]
  },
  {
    "title": "image description with nonseparable two-dimensional charlier and meixner moments.",
    "abstract": "this paper presents two new sets of nonseparable discrete orthogonal charlier and meixner moments describing the images with noise and that are noise-free. the basis functions used by the proposed nonseparable moments are bivariate charlier or meixner polynomials introduced by tratnik et al. this study discusses the computational aspects of discrete orthogonal charlier and meixner polynomials, including the recurrence relations with respect to variable x and order n. the purpose is to avoid large variation in the dynamic range of polynomial values for higher order moments. the implementation of nonseparable charlier and meixner moments does not involve any numerical approximation, since the basis function of the proposed moments is orthogonal in the image coordinate space. the performances of charlier and meixner moments in describing images were investigated in terms of the image reconstruction error, and the results of the experiments on the noise sensitivity are given.",
    "present_kp": [
      "charlier",
      "meixner"
    ],
    "absent_kp": [
      "bivariate discrete orthogonal polynomials",
      "nonseparable discrete orthogonal moments",
      "three-term recurrence relations",
      "second order linear partial difference equations"
    ]
  },
  {
    "title": "computer-aided diagnosis of diabetic retinopathy: a review.",
    "abstract": "diabetes mellitus may cause alterations in the retinal microvasculature leading to diabetic retinopathy. unchecked, advanced diabetic retinopathy may lead to blindness. it can be tedious and time consuming to decipher subtle morphological changes in optic disk, microaneurysms, hemorrhage, blood vessels, macula, and exudates through manual inspection of fundus images. a computer aided diagnosis system can significantly reduce the burden on the ophthalmologists and may alleviate the inter and intra observer variability. this review discusses the available methods of various retinal feature extractions and automated analysis.",
    "present_kp": [
      "retina",
      "retinopathy",
      "computer-aided diagnosis"
    ],
    "absent_kp": [
      "fundus imaging",
      "pattern classification",
      "image processing"
    ]
  },
  {
    "title": "anonymous geo-forwarding in manets through location cloaking.",
    "abstract": "in this paper, we address the problem of destination anonymity for applications in mobile ad hoc networks where geographic information is ready for use in both ad hoc routing and internet services. geographic forwarding becomes a lightweight routing protocol in favor of the scenarios. traditionally, the anonymity of an entity of interest can be achieved by hiding it among a group of other entities with similar characteristics, i.e., an anonymity set. in mobile ad hoc networks, generating and maintaining an anonymity set for any ad hoc node is challenging because of the node mobility and, consequently, the dynamic network topology. we propose protocols that use the destination position to generate a geographic area called an anonymity zone (az). a packet for a destination is delivered to all the nodes in the az, which make up the anonymity set. the size of the anonymity set may decrease, because nodes are mobile, yet the corresponding anonymity set management is simple. we design techniques to further improve node anonymity and reduce communication overhead. we use analysis and extensive simulation to study the node anonymity and routing performance and to determine the parameters that most impact the anonymity level that can be achieved by our protocol.",
    "present_kp": [
      "anonymity"
    ],
    "absent_kp": [
      "ad hoc routing protocol",
      "anonymous routing protocol",
      "georouting protocol",
      "communication privacy"
    ]
  },
  {
    "title": "reliable analysis of transverse vibrations of timoshenkomindlin beams with respect to uncertain shear correction factor.",
    "abstract": "the dependence of minimal eigenfrequencies on the shear correction factor is investigated for homogeneous prismatic beams with both ends either simply supported or clamped. it is proved that the eigenfrequency is simple and its derivative with respect to the shear correction factor is positive.",
    "present_kp": [
      "shear correction factor",
      "eigenfrequency"
    ],
    "absent_kp": [
      "uncertain data",
      "eigenvalue"
    ]
  },
  {
    "title": "extended synchronous dataflow for efficient dsp system prototyping.",
    "abstract": "though synchronous dataflow (sdf) graph has been a successful input specification language for digital signal processing (dsp) applications, lack of support for global states makes it unsuitable for multimedia signal processing applications that need global states for efficient implementation. in this paper, we propose synchronous piggybacked dataflow (spdf), an extension of sdf model to accommodate global states without side effects. global states are accessed by a special block that piggybacks the global state update request on data samples. such an extension enlarges the domain of application where dataflow representation can be used for rapid system prototyping. the only penalty it incurs is scheduling complexity since the scheduler now considers control dependency as well as data dependency. we present the static analysis of the spdf model and an implementation technique for memory efficient code synthesis. finally, we show experimental results with a real life example, mpeg-audio decoder, to present the novelty and usefulness of our approach.",
    "present_kp": [
      "synchronous dataflow",
      "global states",
      "synchronous piggybacked dataflow"
    ],
    "absent_kp": [
      "block reusability"
    ]
  },
  {
    "title": "a parallel framework for in-memory construction of term-partitioned inverted indexes.",
    "abstract": "with the advances in cloud computing and huge rams provided by 64-bit architectures, it is possible to tackle large problems using memory-based solutions. construction of term-based, partitioned, parallel inverted indexes is a communication intensive task and suitable for memory-based modeling. in this paper, we provide an efficient parallel framework for in-memory construction of term-based partitioned, inverted indexes. we show that, by utilizing an efficient bucketing scheme, we can eliminate the need for the generation of a global vocabulary. we propose and investigate assignment schemes that can reduce the communication overheads while minimizing the storage and final query processing imbalance. we also present a study on how communication among processors should be carried out with limited communication memory in order to reduce the total inversion time. we present several different communication-memory organizations and discuss their advantages and shortcomings. the conducted experiments indicate promising results.",
    "present_kp": [],
    "absent_kp": [
      "index inversion",
      "term-based partitioning",
      "parallel inversion",
      "memory-based inversion"
    ]
  },
  {
    "title": "bayesian characterization of uncertainty in intra-subject non-rigid registration.",
    "abstract": "in settings where high-level inferences are made based on registered image data, the registration uncertainty can contain important information. in this article, we propose a bayesian non-rigid registration framework where conventional dissimilarity and regularization energies can be included in the likelihood and the prior distribution on deformations respectively through the use of boltzmanns distribution. the posterior distribution is characterized using markov chain monte carlo (mcmc) methods with the effect of the boltzmann temperature hyper-parameters marginalized under broad uninformative hyper-prior distributions. the mcmc chain permits estimation of the most likely deformation as well as the associated uncertainty. on synthetic examples, we demonstrate the ability of the method to identify the maximum a posteriori estimate and the associated posterior uncertainty, and demonstrate that the posterior distribution can be non-gaussian. additionally, results from registering clinical data acquired during neurosurgery for resection of brain tumor are provided; we compare the method to single transformation results from a deterministic optimizer and introduce methods that summarize the high-dimensional uncertainty. at the site of resection, the registration uncertainty increases and the marginal distribution on deformations is shown to be multi-modal.",
    "present_kp": [
      "bayesian",
      "uncertainty",
      "non-rigid",
      "registration",
      "mcmc"
    ],
    "absent_kp": [
      "elastic"
    ]
  },
  {
    "title": "granule-oriented programming.",
    "abstract": "a program will become obsolete or less effective in solving domain problems due to many reasons. one of the main reasons can be the fact that the program does not fit its context. the context of a program is defined as a collection of functionalities that support the program to solve domain problems, e.g., runtime environmental supports, meta-strategies, architectural supports, etc. unfitness phenomena exist in many software systems, which lead the systems prematurely end their life cycles, or decrease their performance and accuracy in solving problems. in existing programming systems, from the perspective of language expressivity, little attention has been paid to this unfitness problem. granule-oriented programming is an evolutionary metaphor in which programs are ground into code granules in order to localize their unfitness parts as explicitly as possible, and then the code granules are compounded into the target program, in which a code granulation space, one to express program in a well-formed and multi-layered framework, is formed. in this paper, we propose and briefly describe the notion of granule-oriented programming.",
    "present_kp": [
      "granule-oriented programming",
      "code granule",
      "code granulation space"
    ],
    "absent_kp": [
      "languages",
      "design",
      "reflection",
      "program grinding",
      "context distribution",
      "object-oriented programming"
    ]
  },
  {
    "title": "performance evaluation of multi-guard channel schemes in broadband mobile networks.",
    "abstract": "in this paper, we proposed two multi-guard channel schemes (mgcs) for multi-threshold and multirate services. the purpose of mgcs is proposed to guarantee the quality of service for multimedia wireless cellular networks. another contribution of this paper is to develop two generalized analytical models, two-dimensional markov chain model, to investigate the performance of two proposed scheme and compared them with each other. the analytical results show that mgcs 2 achieves better results on the handoff dropping probability and new call blocking probability.",
    "present_kp": [
      "multi-guard channel",
      "broadband mobile networks",
      "multirate services"
    ],
    "absent_kp": [
      "priority based cac schemes"
    ]
  },
  {
    "title": "measuring process flexibility and agility.",
    "abstract": "in their attempt to improve their systems and architectures, organizations need to be aware of the types of flexibility and agility and the current level of each type of flexibility and agility. flexibility is the general ability to react to changes, whilst agility is the speed in responding to variety and changes both flexibility and agility are diverse concepts that are hard to grasp. in this paper the types of flexibility and agility of business processes is discussed on a foundation level and an approach to measure the level of flexibility and agility is proposed. a case study of the flexibility and agility measurement is used to demonstrate the approach. the illustration is used to discuss the difficulties and limitations of the measurement approach. there is no uniform definition of or view on flexibility and agility. this makes it hard to develop a measurement approach. furthermore, as business processes can be different, this might result in different metrics for measuring the level of flexibility and agility. there is no single measure and for each type of business process and flexibility and agility should always measure by a combination of metrics. in addition, both qualitative and quantitative metrics should be used to measure the level of flexibility and agility.",
    "present_kp": [
      "flexibility",
      "agility"
    ],
    "absent_kp": [
      "business process management"
    ]
  },
  {
    "title": "an alternative domain/boundary element technique for analyzing plates on two-parameter elastic foundations.",
    "abstract": "this paper presents an alternative domain/boundary element technique for analyzing plates on two-parameter elastic foundations, in which the fundamental solution for the linear plate theory is used in the formulation. the main advantages of using this technique are that the kernels of the boundary integral equations are conveniently established and can be used for solving plate problems with various boundary conditions as well as mixed boundary conditions. the surface integration of the kernels for the foundation pressure is evaluated using a numerical procedure presented herein instead of the conventional gausslegendre method, resulting in the reduction of computing time. the application of higher-order elements, i.e. cubic elements, for improving the solution is adopted. numerical results of several problems with various boundary conditions are given to demonstrate the accuracy and validity of the method.",
    "present_kp": [
      "plates",
      "two-parameter elastic foundations",
      "cubic elements"
    ],
    "absent_kp": [
      "boundary element method",
      "domain integrals"
    ]
  },
  {
    "title": "network utility maximization over partially observable markovian channels.",
    "abstract": "we study throughput utility maximization in a multi-user network with partially observable markovian channels. here, instantaneous channel states are unavailable and all controls are based on partial channel information provided by ack/nack feedback from past transmissions. equivalently, we formulate a restless multi-armed bandit problem in which we seek to maximize concave functions of the time average reward vector. such problems are generally intractable and in our problem the set of all achievable throughput vectors is unknown. we use an achievable region approach by optimizing the utility functions over a non-trivial reduced throughput region, constructed by randomizing well-designed round robin policies. using a new ratio maxweight rule, we design admission control and channel scheduling policies that stabilize the network with throughput utility that is near-optimal within the reduced throughput region. the ratio maxweight rule generalizes existing maxweight-type policies for the optimization of frame-based control systems with policy-dependent frame sizes. our results are applicable to limited channel probing in wireless networks, dynamic spectrum access in cognitive radio networks, and target tracking of unmanned aerial vehicles.",
    "present_kp": [
      "markovian channels",
      "restless multi-armed bandit",
      "cognitive radio",
      "achievable region approach"
    ],
    "absent_kp": [
      "lyapunov drift analysis",
      "ratio max-weight policy",
      "stochastic network optimization",
      "opportunistic spectrum access"
    ]
  },
  {
    "title": "tailoring transport and dielectric properties by surface passivation of silicon nanowires with polyacrylic acid/tio2 nanoparticles composite.",
    "abstract": "silicon nanowires (sinws) based composite devices were fabricated. effect of surface capping on sinws was investigated. transport and dielectric properties were observed. space charge limited conduction is dominant conduction mechanism.",
    "present_kp": [
      "silicon nanowires",
      "composite devices",
      "dielectric properties"
    ],
    "absent_kp": [
      "space charge limited current"
    ]
  },
  {
    "title": "towards event detection in an audio-based sensor network.",
    "abstract": "in this paper, we describe an experiment where we gathered audio information from a series of conventional wired microphones installed in a typical university setting. we also obtained visual information from cameras located in the same area. we set out to see if audio analysis could be used to assist our existing visual event detection system, and to note any improvements. we were not concerned with identifying or classifying what was detected in the audio. our aim was to keep audio processing to a minimum, as this would enable wireless sensor networks to be used in the future. we present the results of analysis of audio information based on the mean of the volume, the zero-crossing rate, and the frequency. we found that detecting events based on their volume returned satisfactory results.",
    "present_kp": [
      "sensor networks"
    ],
    "absent_kp": [
      "audio surveillance",
      "security monitoring"
    ]
  },
  {
    "title": "the forest vegetation simulator: a review of its structure, content, and applications.",
    "abstract": "the forest vegetation simulator (fvs) is a distance-independent, individual-tree forest growth model widely used in the united states to support management decisionmaking. stands are the basic projection unit, but the spatial scope can be many thousands of stands. the temporal scope is several hundred years at a resolution of 510 years. projections start with a summary of current conditions evident in the input inventory data. fvs contains a self-calibration feature that uses measured growth rates to modify predictions for local conditions. component models predict the growth and mortality of individual trees, and extensions to the base model represent disturbance agents including insects, pathogens, and fire. the component models differ depending on the geographic region represented by regionally specific model variants. the differences are due to data availability and the applicability of existing models. the model supports specification of management rules in the input, such as thinning if density is too high. the rules can be extended to represent other factors. for example, the effect of climate change on stand development by entering rules that specify how growth and mortality will change in response to changing climate. applications range from development of silvicultural prescription for single stands to landscape and large regional assessments. key issues addressed with fvs include forest development, wildlife habitat, pest outbreaks, and fuels management. the predictions are used to gain insights into how forested environments will respond to alternative management actions. broad-scale forest management policies have been studied with fvs. for the 30 years since the model was initially introduced, the development team has anticipated and provided needed enhancements and maintained a commitment to working with and training users. the existence of an adequate user interface and the continued use of the original programming language are often overlooked factors for the success of this model. future work will focus on improving fvs by adopting recent biometric techniques and including new information linking geomorphology to mortality and growth. extending the model to more closely represent biophysical processes and adapting the model so that it is more relevant to management questions related to predicted climate change are also foci. providing ways to dynamically link fvs to other models is our current strategy for providing major new capabilities.",
    "present_kp": [],
    "absent_kp": [
      "forest growth and yield",
      "forest planning",
      "forest succession",
      "forest disturbances",
      "decision support",
      "prognosis model"
    ]
  },
  {
    "title": "experimental study of intelligent controllers under uncertainty using type-1 and type-2 fuzzy logic.",
    "abstract": "uncertainty is an inherent part in control systems used in real world applications. the use of new methods for handling incomplete information is of fundamental importance. type-1 fuzzy sets used in conventional fuzzy systems cannot fully handle the uncertainties present in control systems. type-2 fuzzy sets that are used in type-2 fuzzy systems can handle such uncertainties in a better way because they provide us with more parameters and more design degrees of freedom. this paper deals with the design of control systems using type-2 fuzzy logic for minimizing the effects of uncertainty produced by the instrumentation elements, environmental noise, etc. the experimental results are divided in two classes, in the first class, simulations of a feedback control system for a non-linear plant using type-1 and type-2 fuzzy logic controllers are presented; a comparative analysis of the systems response in both cases was performed, with and without the presence of uncertainty. for the second class, a non-linear identification problem for time-series prediction is presented. based on the experimental results the conclusion is that the best results are obtained using type-2 fuzzy systems.",
    "present_kp": [],
    "absent_kp": [
      "interval type-2 fuzzy sets",
      "fuzzy control",
      "type-2 fuzzy logic systems",
      "system identification"
    ]
  },
  {
    "title": "discovering models of behavior for concurrent workflows.",
    "abstract": "understanding the dynamic behavior of a workflow is crucial for being able to modify, maintain, and improve it. a particularly difficult aspect of some behavior is concurrency. automated techniques which seek to mine workflow data logs to discover information about the workflows must be able to handle the concurrency that manifests itself in the workflow executions. this paper presents techniques to discover patterns of concurrent behavior from traces of workflow events. the techniques are based on a probabilistic analysis of the event traces. using metrics for the number, frequency, and regularity of event occurrences, a determination is made of the likely concurrent behavior being manifested by the system. discovering this behavior can help a workflow designer better understand and improve the work processes they are managing.",
    "present_kp": [],
    "absent_kp": [
      "dynamic analysis",
      "workflow data analysis",
      "behavior inference",
      "concurrency analysis",
      "workflow model discovery"
    ]
  },
  {
    "title": "toward cognitive support for owl justifications.",
    "abstract": "justifications are the dominant form of explanation for entailments of owl ontologies, with popular owl ontology editors, such as protg 4, providing justification-based explanation facilities. a justification is a minimal subset of an ontology which is sufficient for an entailment to hold; they correspond to the premises of a proof. unlike proofs, however, justifications do not articulate how their axioms support the entailment. we frequently observe that ontology developers find certain justifications difficult to work with; and while in some cases the sources of difficulty are obvious (such as a large number of axioms), we do not have a good general understanding of what makes justifications easy or difficult for ontology users. in this paper, we present an approach to determining the cognitive complexity of justifications for entailments of owl ontologies. we describe an exploratory study which forms the basis for a cognitive complexity model that predicts the complexity of owl justifications, and present the results of validating that model via experiments involving owl users. this is concluded by an investigation into strategies owl users apply to support them in understanding justifications. our contributions include an evaluation of the cognitive complexity model, new insights into the complexity of justifications for entailments of owl ontologies, a significant corpus with novel analyses of justifications suitable for experimentation, and an experimental protocol suitable for model validation and refinement.",
    "present_kp": [
      "explanation"
    ],
    "absent_kp": [
      "semantic web",
      "description logics",
      "web ontology language",
      "ontology debugging"
    ]
  },
  {
    "title": "a systematic analysis of duplicate records in scopus.",
    "abstract": "we perform a systematic analysis of the problem of duplicate records in scopus. in the seven journals included in the analysis, 12.4% of the records are duplicates. most duplicate records are due to orthographic differences in journal titles. journal name changes and title variations also play a role.",
    "present_kp": [
      "duplicate records"
    ],
    "absent_kp": [
      "bibliographic control guidelines",
      "bibliometric indicators overdimensionalized",
      "indexing errors",
      "scopus database"
    ]
  },
  {
    "title": "numerical modeling of two-phase hysteresis combined with an interface condition for heterogeneous porous media.",
    "abstract": "this paper presents a numerical implementation of two-phase capillary hysteresis and its combination with a capillary interface condition for the treatment of heterogeneities. the hysteresis concepts chosen in this work are first implemented in a node-centered fv discretization scheme and subsequently combined with the interface condition that predicts sharp saturation discontinuities at material interfaces, based on a pressure equilibrium concept. this approach allows for the approximation of history-dependent, and at the same time discontinuous, saturations at material interfaces. the resulting model provides a well-defined evolution of the hysteretic capillary pressure-saturation relationships at material interfaces that is independent of the grid spacing. as demonstrated with a simple 1-d example, this concept therefore offers the advantage that the solution of a two-phase flow problem involving hysteresis does not relate to the grid resolution at the material interfaces.",
    "present_kp": [
      "hysteresis",
      "numerical modeling",
      "heterogeneous",
      "interface condition",
      "two-phase flow"
    ],
    "absent_kp": []
  },
  {
    "title": "ds2: a dht-based substrate for distributed services.",
    "abstract": "dht (distributed hash table) algorithms are very efficient for distributed data management. as one kind of p2p overlay, dht overlay also has the advantages of high reliability, high scalability and low cost. because of these advantages, dht has been proposed to form server farms such as dht-based nosql databases, sip server farms, ims server farms, openflow controller farms, etc. this paper presents ds2, a dht-based substrate designed for the application server farms providing distributed services. ds2 facilitates the deployment of dht-based distributed services in three aspects. first, ds2 offers a powerful data model to manage complex data. second, ds2 provides application message routing function and workload migration function, which help application server farm to achieve load balance, failover and service continuity. third, ds2 allows deploying application server farms across data centers. ds2 deploying across data centers is optimized to provide better performance and service continuity. we have implemented a ds2 prototype and used it in ztes p2p cscfprototype and service routing prototype to enable ims services and service routing services successfully.",
    "present_kp": [
      "distributed hash table",
      "nosql"
    ],
    "absent_kp": [
      "big data"
    ]
  },
  {
    "title": "corticosterone in chicken eggs.",
    "abstract": "abstract: birds are discussed as models for prenatal stress. in this study, several experiments were conducted to gain basic knowledge of if, how, and when maternal adrenocortical activity is reflected by corticosterone concentrations in the egg. radiolabeled corticosterone was administered to 10 laying hens to investigate the uptake into as well as the distribution within the eggs. the yolk was dissected in concentric layers and analyzed. less than 1% of the administered radioactivity entered the egg but was, however, not evenly distributed. on the day after injection, highest radioactivity (bq/g) was detected in the albumen and the outmost layer, whereas concentration peaked 7 days later in the inner layers. in two other experiments, increased plasma levels of corticosterone were induced by injection of adrenocorticotropic hormone (acth) or feeding of corticosterone. again, yolk disks were cut in layers and analyzed with a corticosterone enzyme immunoassay. no effect of the acth administration was detected, whereas feeding of corticosterone resulted in increased immunoreactive corticosterone concentrations in the yolk. straight-phase high-performance liquid chromatographic (hplc) separations were also performed to characterize immunoreactive steroids in the yolk. two close-eluting peaks at the approximate elution position of corticosterone could be observed after the feeding experiment, whereas in untreated control eggs they were absent. it was concluded that transfer from plasma to egg is low for corticosterone and that further investigations concerning the transport mechanisms and the exact nature of yolk steroids are necessary.",
    "present_kp": [
      "corticosterone",
      "eggs",
      "chicken"
    ],
    "absent_kp": [
      "poultry"
    ]
  },
  {
    "title": "integrating hierarchical balanced scorecard with fuzzy linguistic for evaluating operating room performance in hospitals.",
    "abstract": "health care organizations are operating in a complex environment. the competitive and dynamic health care sector has spurred hospitals into delivering greater flexibility and quality of services. an efficient performance evaluation system is essential for controlling, monitoring and improving service quality in health care organizations. the performance evaluation of operating room (or) is a useful work for managers to control the operational process of or team so as to promote the performance. this paper explores the use of a management tool: balanced scorecard (bsc), which facilitates managers to meet multiple strategic goals, and fuzzy linguistic method for evaluating or performance. bsc is a strategic planning and management system that is used extensively in business and industry, government and nonprofit organizations. first, a model is developed for measuring the acceptable performance of or based on the interaction financial, customers, internal business process and learning and growth perspective. after that, bsc structure integrated with fuzzy linguistic is proposed for measuring and improving the service. the aim of this study was to build a performance evaluation system for or and use a fuzzy linguistic to convert the subjective cognition of managers into an information entity and confirmation of improvement. this research results are able to help the organisation to evaluate and revise its strategy and generally to adopt modern management approaches in every day practise.",
    "present_kp": [
      "operating room",
      "performance evaluation",
      "balanced scorecard"
    ],
    "absent_kp": [
      "fuzzy linguistic approach"
    ]
  },
  {
    "title": "an empirical exploration of the distributions of the chidamber and kemerer object-oriented metrics suite.",
    "abstract": "the object-oriented metrics suite proposed by chidamber and kemerer (ck) is a measurement approach towards improved object-oriented design and development practices. however, existing studies evidence traces of collinearity between some of the metrics and low ranges of other metrics, two facts which may endanger the validity of models based on the ck suite. as high correlation may be an indicator of collinearity, in this paper, we empirically determine to what extent high correlations and low ranges might be expected among ck metrics. to draw as much general conclusions as possible, we extract the ck metrics from a large data set (200 public domain projects) and we apply statistical meta-analysis techniques to strengthen the validity of our results. homogeneously through the projects, we found a moderate (similar to0.50) to high correlation (>0.80) between some of the metrics and low ranges of other metrics. results of this empirical analysis supply researchers and practitioners with three main advises: a) to avoid the use in prediction systems of ck metrics that have correlation more than 0.80 b) to test for collinearity those metrics that present moderate correlations (between 0.50 and 0.60) c) to avoid the use as response in continuous parametric regression analysis of the metrics presenting low variance. this might therefore suggest that a prediction system may not be based on the whole ck metrics suite, but only on a subset consisting of those metrics that do not present either high correlation or low ranges.",
    "present_kp": [
      "ck metrics",
      "collinearity",
      "meta-analysis"
    ],
    "absent_kp": [
      "object-orientation",
      "software metrics"
    ]
  },
  {
    "title": "anycast-aware transport for content delivery networks.",
    "abstract": "anycast-based content delivery networks (cdns) have many properties that make them ideal for the large scale distribution of content on the internet. however, because routing changes can result in a change of the endpoint that terminates the tcp session, tcp session disruption remains a concern for anycast cdns, especially for large file downloads. in this paper we demonstrate that this problem does not require any complex solutions. in particular, we present the design of a simple, yet efficient, mechanism to handle session disruptions due to endpoint changes. with our mechanism, a client can continue the download of the content from the point at which it was before the endpoint change. furthermore, cdn servers purge the tcp connection state quickly to handle frequent switching with low system overhead. we demonstrate experimentally the effectiveness of our proposed mechanism and show that more complex mechanisms are not required. specifically, we find that our mechanism maintains high download throughput even with a reasonably high rate of endpoint switching, which is attractive for load balancing scenarios. moreover, our results show that edge servers can purge tcp connection state after a single timeout-triggered retransmission without any tangible impact on ongoing connections. besides improving server performance, this behavior improves the resiliency of the cdn to certain denial of service attacks.",
    "present_kp": [
      "anycast",
      "content delivery networks"
    ],
    "absent_kp": [
      "connection disruption"
    ]
  },
  {
    "title": "geometry of decomposition dependent evolutions of mixed states.",
    "abstract": "we examine evolutions where each component of a given decomposition of a mixed quantal state evolves independently in a unitary fashion. the geometric phase and parallel transport conditions for this type of decomposition dependent evolution are delineated. we compare this geometric phase with those previously defined for unitarily evolving mixed states, and mixed state evolutions governed by completely positive maps.",
    "present_kp": [
      "geometric phase",
      "mixed state"
    ],
    "absent_kp": [
      "conditional dynamics"
    ]
  },
  {
    "title": "genetic watermarking based on transform-domain techniques.",
    "abstract": "an innovative watermarking scheme based on genetic algorithms (ga) in the transform domain is proposed. it is robust against watermarking attacks, which are commonly employed in the literature. in addition, the watermarked image quality is also considered. in this paper, we employ ga for optimizing both the fundamentally conflicting requirements. watermarking with ga is easy for implementation. we also examine the effectiveness of our scheme by checking the fitness function in ga, which includes both factors related to robustness and invisibility. simulation results also show both the robustness under attacks, and the improvement in watermarked image quality with ga.",
    "present_kp": [
      "fitness function"
    ],
    "absent_kp": [
      "digital watermarking",
      "genetic algorithm ",
      "discrete cosine transform ",
      "peak signal-to-noise ratio ",
      "normalized cross correlation "
    ]
  },
  {
    "title": "log-based rollback recovery without checkpoints of shared memory in software dsm.",
    "abstract": "a common approach to fault-tolerant software dsm is to take checkpoints with message logging. our remote logging has low overhead because each node saves the coherence-related data into the memory of a remote node through a high-speed system area network. for more lightweight fault-tolerant dsm, in this paper, we mainly focused on eliminating shared memory checkpointing during failure-free execution. each node independently takes the checkpoints of execution states and non-shared data only. when a node fails, it regenerates its pages from the remote copies in live nodes. in order to efficiently reconstruct pages, we also introduced a xor-diffing technique. the diff logs, which have been created by xor operations during failure-free execution, can be applicable to any version of remote copies either backward or forward for recovery. our scheme reduces the checkpointing overhead and also alleviates the imbalance in execution times among nodes due to independent checkpointing.",
    "present_kp": [
      "checkpointing",
      "rollback recovery",
      "logging"
    ],
    "absent_kp": [
      "fault tolerance",
      "software distributed shared memory",
      "cluster computing"
    ]
  },
  {
    "title": "trigonometrically fitted fifth-order runge-kutta methods for the numerical solution of the schrodinger equation.",
    "abstract": "two trigonometrically fitted methods based on a classical runge-kutta method of kutta-nystrom are being constructed. the new methods maintain the fifth algebraic order of the classical one but also have some other significant properties. the most important one is that in the local truncation error of the new methods the powers of the energy are lower and that keeps the error at lower values, especially at high values of energy. the error analysis justifies the actual results when integrating the radial schrodinger equation, where the high efficiency of the new methods is shown.",
    "present_kp": [
      "radial schrodinger equation",
      "energy"
    ],
    "absent_kp": [
      "explicit runge-kutta methods",
      "exponential fitting",
      "trigonometrical fitting",
      "resonance problem"
    ]
  },
  {
    "title": "integration testing of object-oriented components using finite state machines.",
    "abstract": "in object-oriented terms, one of the goals of integration testing is to ensure that messages from objects in one class or component are sent and received in the proper order and have the intended effect on the state of the objects that receive the messages. this research extends an existing single-class testing technique to integration testing of multiple classes. the single-class technique models the behaviour of a single class as a finite state machine, transforms the representation into a data flow graph that explicitly identifies the definitions and uses of each state variable of the class, and then applies conventional data flow testing to produce test case specifications that can be used to test the class. this paper extends those ideas to inter-class testing by developing flow graphs, finding paths between pairs of definitions and uses, detecting some infeasible paths and automatically generating tests for an arbitrary number of classes and components. it introduces flexible representations for message sending and receiving among objects and allows concurrency among any or all classes and components. data flow graphs are stored in a relational database and database queries are used to gather def-use information. this approach is conceptually simple, mathematically precise, quite powerful and general enough to be used for traditional data flow analysis. this testing approach relies on finite state machines, database modelling and processing techniques and algorithms for analysis and traversal of directed graphs. the paper presents empirical results of the approach applied to an automotive system. this work was prepared by u.s. government employees as part of their official duties and is, therefore, a work of the u.s. government and not subject to",
    "present_kp": [
      "data flow testing",
      "finite state machines",
      "object-oriented"
    ],
    "absent_kp": [
      "software integration testing",
      "data modelling"
    ]
  },
  {
    "title": "topology-aware tile mapping for clusters of smps.",
    "abstract": "we propose a technique to optimize the performance of applications using distributed dense arrays and characterized by a nearest-neighbor communication profile by exploiting the topology of smp clusters. the topological information is used to map array tiles to processors to reduce network communication and improve utilization of shared memory for inter-process communication. the potential benefits of using the smp-aware mapping are demonstrated through a simulation, as well as a real application solving a wind-driven ocean circulation model on an ibm sp. on 256 processors, the execution time was reduced by almost 30 percent without any changes to the original application source code. the proposed mapping approach is applicable to multiple programming models and distributed array management systems.",
    "present_kp": [
      "clusters of smps"
    ],
    "absent_kp": [
      "topology awareness",
      "communication optimization",
      "data layout optimization"
    ]
  },
  {
    "title": "optical modes of chiral photonic composites.",
    "abstract": "we report on the eigenmodes of photonic crystals consisting of submicron homogeneous chiral spheres in a nonchiral isotropic medium, by means of full electrodynamic calculations using the layer-multiple-scattering method. it is shown that resonant modes of the individual spheres give rise to narrow bands that hybridize with the extended bands of the appropriate symmetry associated with light propagation in an underlying effective chiral medium. the resulting photonic dispersion diagram exhibits remarkable features, such as strong band bending away from the bragg points with consequent negative-slope dispersion inside the first brillouin zone and sizable frequency gaps specific to each polarization mode. we present a rigorous group-theory analysis to explain features of the calculated photonic band structure, peculiar to a system which possesses time-reversal but not space-inversion symmetry, and discuss some interesting aspects of the underlying physics.",
    "present_kp": [
      "photonic crystal",
      "chiral medium",
      "band structure"
    ],
    "absent_kp": [
      "band gap"
    ]
  },
  {
    "title": "hardness results for the center and median string problems under the weighted and unweighted edit distances.",
    "abstract": "given a finite set of strings, the median string problem consists in finding a string that minimizes the sum of the edit distances to the strings in the set. approximations of the median string are used in a very broad range of applications where one needs a representative string that summarizes common information to the strings of the set. it is the case in classification, in speech and pattern recognition, and in computational biology. in the latter, median string is related to the key problem of multiple alignment. in the recent literature, one finds a theorem stating the np-completeness of the median string for unbounded alphabets. however, in the above mentioned areas, the alphabet is often finite. thus, it remains a crucial question whether the median string problem is np-complete for bounded and even binary alphabets. in this work, we provide an answer to this question and also give the complexity of the related center string problem. moreover, we study the parameterized complexity of both problems with respect to the number of input strings. in addition, we provide an algorithm to compute an optimal center under a weighted edit distance in polynomial time when the number of input strings is fixed.",
    "present_kp": [
      "multiple alignment",
      "np-complete",
      "parameterized complexity"
    ],
    "absent_kp": [
      "concensus string",
      "tree alignment",
      "lcs"
    ]
  },
  {
    "title": "the influence of prototype fidelity and aesthetics of design in usability tests: effects on user behaviour, subjective evaluation and emotion.",
    "abstract": "an empirical study examined the impact of prototype fidelity on user behaviour, subjective user evaluation and emotion. the independent factors of prototype fidelity (paper prototype, computer prototype, fully operational appliance) and aesthetics of design (high vs. moderate) were varied in a between-subjects design. the 60 participants of the experiment were asked to complete two typical tasks of mobile phone usage: sending a text message and suppressing a phone number. both performance data and a number of subjective measures were recorded. the results suggested that task completion time may be overestimated when a computer prototype is being used. furthermore, users appeared to compensate for deficiencies in aesthetic design by overrating the aesthetic qualities of reduced fidelity prototypes. finally, user emotions were more positively affected by the operation of the more attractive mobile phone than by the less appealing one.",
    "present_kp": [
      "usability test",
      "prototype fidelity",
      "aesthetics",
      "mobile phone"
    ],
    "absent_kp": []
  },
  {
    "title": "computer use in older adults: determinants and the relationship with cognitive change over a 6year episode.",
    "abstract": "cognitively challenging activities may support the mental abilities of older adults. the use of computers and the internet provides divergent cognitive challenges to older persons, and in previous studies, positive effects of computer and internet use on the quality of life have been demonstrated. the present study addresses two research aims regarding predictors of computer use and the relationship between computer use and changes in cognitive abilities over a 6-year period in both younger (2449years) and older adults (older than 50years). data were obtained from an ongoing study into cognitive aging: the maastricht aging study, involving 1823 normal aging adults who were followed for 9years. the results showed age-related differences in predictors of computer use: the only predictor in younger participants was level of education, while in older participants computer use was also predicted by age, sex and feelings of loneliness. protective effects of computer use were found for measures of selective attention and memory, in both older and younger participants. effect sizes were small, which suggests that promotion of computer activities in older adults to prevent cognitive decline may not be an efficient strategy.",
    "present_kp": [
      "computer use",
      "internet",
      "cognitive aging"
    ],
    "absent_kp": [
      "elderly"
    ]
  },
  {
    "title": "target tracking using monte carlo simulation.",
    "abstract": "this paper proposes and studies an implementation of the bootstrap stochastic simulation approach for estimating a hybrid system - the bootstrap multiple model (bmm) algorithm. the bmm filter is applied to tracking maneuvering target. the tracking capabilities of the filter are demonstrated by computer simulation.",
    "present_kp": [
      "target tracking"
    ],
    "absent_kp": [
      "multiple model estimation",
      "bootstrap algorithm"
    ]
  },
  {
    "title": "on ranking the effectiveness of searches.",
    "abstract": "there is a growing interest in estimating the effectiveness of search. two approaches are typically considered: examining the search queries and examining the retrieved document sets. in this paper, we take the latter approach. we use four measures to characterize the retrieved document sets and estimate the quality of search. these measures are (i) the clustering tendency as measured by the cox-lewis statistic, (ii) the sensitivity to document perturbation, (iii) the sensitivity to query perturbation and (iv) the local intrinsic dimensionality. we present experimental results for the task of ranking 200 queries according to the search effectiveness over the trec (discs 4 and 5) dataset. our ranking of queries is compared with the ranking based on the average precision using the kendall t statistic. the best individual estimator is the sensitivity to document perturbation and yields kendall t of 0.521. when combined with the clustering tendency based on the cox-lewis statistic and the query perturbation measure, it results in kendall t of 0.562 which to our knowledge is the highest correlation with the average precision reported to date.",
    "present_kp": [
      "task",
      "quality",
      "use",
      "correlation",
      "queries",
      "knowledge",
      "paper",
      "effect",
      "search",
      "query",
      "ranking",
      "cluster"
    ],
    "absent_kp": [
      "precise",
      "measurement",
      "sensitive",
      "statistics",
      "estimates",
      "experimentation",
      "documentation",
      "locality",
      "query performance prediction"
    ]
  },
  {
    "title": "a secure boolean-based multi-secret image sharing scheme.",
    "abstract": "present a random image generating function to generate a random image from secret images or shared images. raise the sharing capacity to n/n. the shared images are more random than previous work. the sharing time is nearly the recovering time and both of them are under 1s.",
    "present_kp": [
      "boolean"
    ],
    "absent_kp": [
      "multiple secret images",
      "xor"
    ]
  },
  {
    "title": "electronic structure study using density functional theory in organic dendrimers.",
    "abstract": "the electronic and structural properties of pyrrolic ring derivatives were studied using density functional theory (dft) in terms of their application as organic semiconductor materials in photovoltaic devices. the b3lyp hybrid functional in combination with pople type 6-31g(d) basis set with a polarization function was used in order to determine the optimized geometries and the electronic properties of the ground state, while transition energies and excited state properties were obtained from time-dependent (td)-dft with b3lyp/6-31g(d) calculation. the investigation of pyrrolic derivatives formed by the arrangement of several monomeric units revealed that three-dimensional (3d) conjugated architectures in which the combination of a triphenylamine (tpa) core with pi-conjugated rings attached to the core, present the best geometric and electronic characteristics for use as an organic semiconductor material. the highest occupied molecular orbital (homo) - lowest unoccupied molecular orbital (lumo) energy gap was decreased in 3d-structures that extend the absorption spectrum toward longer wavelengths, revealing a feasible intramolecular charge transfer process in these systems. all calculations in this work were performed using the gaussian 03 w software package.",
    "present_kp": [
      "density functional theory"
    ],
    "absent_kp": [
      "organic photovoltaics",
      "dendrimers and semiconductors"
    ]
  },
  {
    "title": "game-theoretic surveillance over arbitrary floor plan using a video camera network.",
    "abstract": "coordinated multi-resolution tracking over arbitrary floor plan is addressed using a game-theoretic approach. an enhanced radial sweep algorithm is devised to find the polygon of visibility at any point on or inside a polygon that contains vision-obstructing polygonal entities. by sampling the edges of a polygon and edges of any polygonal hole inside that polygon, a two-pass 01 programming process is formulated to find a near-optimal set of camera samples that can dynamically cover, at a high probability, area under surveillance in the presence of camera handoffs. radius multiplier is introduced to handle partial visibility and is set to 1 by default to avoid insolvability of 01 programming problems. as a remedy to excessive redundancy triggered by camera clustering, we set camera redundancy to a fixed value of 3 for any block with concave valid area. branch-and-cut algorithm is employed to solve 01 programming problems. assigning a fixed value to camera redundancy of blocks with concave valid area, setting radius multiplier to a nonzero value, and utilizing secondary utility yielded better simulation results for various types of floor plans. raising camera redundancy of blocks with non-concave valid area contributed to performance boost and in the meantime, increased the number of cameras needed.",
    "present_kp": [
      "camera network",
      "polygon of visibility",
      "multi-resolution tracking"
    ],
    "absent_kp": [
      "game theory",
      "nash equilibrium",
      "camera placement"
    ]
  },
  {
    "title": "particle swarm clustering ensemble.",
    "abstract": "extracting natural groups of the unlabeled data is known as clustering. to improve the stability and robustness of the clustering outputs, clustering ensembles have emerged recently. in this paper, an ensemble of particle swarm clustering algorithms is proposed. that is, the members of the ensemble are based on the cooperative swarms clustering approaches. the performance of the proposed particle swarm clustering ensemble is evaluated using dierent data sets and is compared to that of other clustering techniques.",
    "present_kp": [
      "clustering ensemble"
    ],
    "absent_kp": [
      "particle swarm optimization",
      "multiple cooperative swarms",
      "data clustering"
    ]
  },
  {
    "title": "active overload prevention based adaptive map selection in hmipv6 networks.",
    "abstract": "multi-level mobile anchor points (map) architecture is deployed in large-scale wireless/mobile networks using hmipv6 to achieve better mobility service, while selecting the most suitable serving map for the mobile nodes (mns) to enhance the whole network performance has been a critical issue. an adaptive map selection based on active overload prevention (map-aop) hence is proposed. the map periodically evaluates the load status by using dynamic weighted load evaluation algorithm, and then sends the load information to the covered access routers (ar) by using the expanded routing advertisement message in a dynamic manner. taking achieving the load balancing among the available maps, the current serving ar executes the active overload prevention to select map candidates for the mn pending a handover, and then adaptively selects an optimal one from the candidates by comprehensively considering the system cost and the average handover latency caused by each candidate. the simulation conducted on the ns-2 platform indicates that map-aop outperforms the comparative map selection schemes with the optimized system cost and average handover latency, and better load balancing.",
    "present_kp": [
      "hmipv6",
      "map",
      "dynamic weighted load evaluation",
      "active overload prevention"
    ],
    "absent_kp": []
  },
  {
    "title": "method for stakeholder identification in interorganizational environments.",
    "abstract": "stakeholders are the first emerging challenge in any software project. their identification is a critical task for success. nevertheless, many authors consider them as a default product of a non-explained identification process. several aspects must be considered when the project is carried out in environments where multiple organizations interact. these complex contexts demand extremely hard efforts. stakeholders must be identified taking into account their attributes (types, roles), which must be extended and defined for these environments. in general, there are no methodologies that allow performing this task in a systematic way for the development of interorganizational information systems. the paper proposes a method for carrying out stakeholder identification considering the diverse dimensions involved in interorganizational environments: organizational, interorganizational and external. it allows the systematic specification of all the people, groups and organizations whose interests and needs can affect or are affected by the interorganizational system. also diverse stakeholders attributes such as types, roles, influence and interest are defined, analyzed, and included in the method. they are all important in later stages of any software project.",
    "present_kp": [
      "stakeholders",
      "interorganizational information systems"
    ],
    "absent_kp": [
      "interorganizational networks"
    ]
  },
  {
    "title": "using industry based data sets in software engineering research.",
    "abstract": "this paper describes the use of software project development data obtained from industry based projects. it argues the importance of carrying out a preliminary data analysis procedure for software development cost estimation. the paper also presents the limitations of using these industrial data (isbsg r9 and bank63 data) based on the above research. current state of the research and further work is discussed.",
    "present_kp": [],
    "absent_kp": [
      "isbsg data set"
    ]
  },
  {
    "title": "an erlang-like law for gprs/edge engineering and its first validation on live traffic.",
    "abstract": "this paper is a contribution to the generic problem of having a simple and accurate analytical model to dimension gsm/(e)gprs radio cells where voice and data traffic are mixed. radio resources are assumed to be shared according to the so-called partial partitioning scheme where some channels are dedicated to voice, others to data, and the remaining are shared by voice and data. voice has a total preemption priority over data on the shared part. this work shows how to calibrate an erlang-like law based on markovian modeling for this complex sharing resource situation and to validate it against real data. we present a complete validation framework on a live network for a simple and efficient gsm/(e)gprs analytical tool.",
    "present_kp": [
      "modeling",
      "edge",
      "erlang",
      "gprs"
    ],
    "absent_kp": [
      "measurement",
      "markov chain",
      "performance evaluation"
    ]
  },
  {
    "title": "web 2.0 geospatial visual analytics for improved urban flooding situational awareness and assessment.",
    "abstract": "situational awareness of urban flooding during storm events is important for disaster and emergency management. however, no general purpose tools yet exist for rendering rainfall accumulations in real-time at the resolution of hydrologic units used for modeling. this demonstration will exhibit a novel web 2.0 visual analytical approach for understanding and adaptively managing urban flooding issues. the approach generates a geospatial-temporal map of rainfall within urban hydrologic units (sewer-sheds) in real-time. the polygon-averaged rainfall data is generated using virtual sensors which provide customized real-time data products derived from national weather service weather radar data using ncsa's workflow tools. time-series kml (keyhole markup language) layers are generated, where each kml layer represents a particular slice of the geospatial color-coded sewershed map. such time-aware kml can be replayed as a movie in the web-based google earth environment. this geospatial visual analytic approach can provide decision markers and communities a powerful resource for assessment of neighborhood flooding issues. we will demonstrate our technology using historical and real-time rainfall data in the metropolitan chicago area to show the effectiveness of such approach. future work by combining additional ground-truth flooding data will allow us move towards real-time improved decision support for flooding and stormwater management.",
    "present_kp": [
      "virtual sensor",
      "keyhole markup language",
      "rainfall",
      "workflow",
      "web 2.0",
      "geospatial visual analytics"
    ],
    "absent_kp": [
      "animation",
      "nexrad"
    ]
  },
  {
    "title": "effective location based services with dynamic data management in mobile environments.",
    "abstract": "with the proliferation of mobile computing technologies, location based services have been identified as one of the most promising target application. we classify mobile information service domains based on feature characteristics of the information sources and different patterns of mobile information access. by carefully examining the service requirements, we identify the dynamic data management problem that must be addressed for effective location based services in mobile environments. we then devise a general architecture and cost model for servicing both location independent and location dependent data. based on the architecture and cost model, we propose a set of dynamic data management strategies that employs judicious caching, proactive server pushing and neighborhood replication to reduce service cost and improve response time under changing user mobility and access patterns. detail behavior analysis helps us in precisely capturing when and how to apply these strategies. simulation results suggest that different strategies are effective for different types of data in response to different patterns of movement and information access.",
    "present_kp": [
      "location based services",
      "dynamic data management"
    ],
    "absent_kp": [
      "mobile information service model",
      "cost analysis"
    ]
  },
  {
    "title": "program page reference patterns.",
    "abstract": "this paper describes a set of measurements of the memory reference patterns of some programs. the technique used to obtain these measurements is unusually efficient. the data is presented in graphical form to allow the reader to \"see\" how the program uses memory. constant use of a page and sequential access of memory are easily observed. an attempt is made to classify the programs based on their referencing behavior. from this analysis it is hoped that the reader will gain some insights as to the effectiveness of various memory management policies.",
    "present_kp": [
      "measurement",
      "program",
      "use",
      "analysis",
      "behavior",
      "data",
      "paper",
      "access",
      "effect",
      "management",
      "pattern"
    ],
    "absent_kp": [
      "paging",
      "policy",
      "efficiency",
      "memorialized",
      "graphics"
    ]
  },
  {
    "title": "fase: a framework for scalable performance prediction of hpc systems and applications.",
    "abstract": "as systems of computers become more complex in terms of their architecture, interconnect and heterogeneity, the optimum configuration and utilization of these machines becomes a major challenge. to reduce the penalties caused by poorly configured systems, simulation is often used to predict the performance of key applications to be executed on the new systems. simulation provides the capability to observe component and system characteristics (e.g. performance and power) in order to make vital design decisions. however, simulating high-fidelity models can be very time consuming and even prohibitive when evaluating large-scale systems. the fast and accurate simulation environment (fase) framework seeks to support large-scale system simulation by using high-fidelity models to capture the behavior of only the performance-critical components while employing abstraction techniques to capture the effects of those components with little impact on the system. in order to achieve this balance of accuracy and simulation speed, fase provides a methodology and associated toolset to evaluate numerous architectural options. this approach allows users to make system design decisions based on quantifiable demands of their key applications rather than using manual analysis which can be error prone and impractical for large systems. the framework accomplishes this evaluation through a novel approach of combining discrete-event simulation with an application characterization scheme in order to remove unnecessary details while focusing on components critical to the performance of the application. in this paper, we present the methodology and techniques behind fase and include several case studies validating systems constructed using various applications and interconnects.",
    "present_kp": [
      "performance prediction",
      "application characterization",
      "discrete-event simulation"
    ],
    "absent_kp": [
      "high-performance computing"
    ]
  },
  {
    "title": "optimization of magnetic sensor arrays for current measurement based on swarm intelligence and d-optimality.",
    "abstract": "purpose - the purpose of this paper is to improve the accuracy of innovative current transducers based on magnetic sensor arrays. design/methodology/approach - the positions and the orientations of the magnetic sensors are optimized. the objective function is defined according to d-optimality theory and the optimization problem solved using particle swarm optimization (pso). findings - pso is shown to be efficient for the optimization problem and that, following the d-optimality approach, a significant improvement in the measurement accuracy is obtained in terms of the errors typically used to classify current transducers. research limitations/implications - the paper does not take into account the effects of external magnetic fields. practical implications - a criterion and a methodology are suggested in order to design more accurate current transducers based on magnetic sensors. originality/value - a new methodology to optimize magnetic sensor arrays for current measurement.",
    "present_kp": [
      "sensors",
      "transducers"
    ],
    "absent_kp": [
      "optimization techniques"
    ]
  },
  {
    "title": "evaluation of heuristic techniques for test vector ordering.",
    "abstract": "vector reordering is an essential task in testing vlsi systems because it affects this process from two perspectives: power consumption and correlation among data. the former feature is crucial and if not properly controlled during testing, may result in permanent failure of the device-under-test (dut). the atter feature is a so important because correlation is captured by coding schemes to efficiently compress test data and ease memory requirements of automatic-test-equipment (ate),while reducing the volume of data and lowering the test application time. reordering however is np-complete. this paper presents an evaluation of different heuristic techniques for vector reordering using iscas85 and iscas89 benchmark circuits in terms of time and quality. for this application, it is shown that the best heuristic technique is not the famous christofides or lin-kernighan, but the multi-fragment technique.",
    "present_kp": [
      "ate",
      "test vector ordering",
      "test data",
      "power consumption"
    ],
    "absent_kp": [
      "soc",
      "compression"
    ]
  },
  {
    "title": "a revaluation of frame difference in fast and robust motion detection.",
    "abstract": "in this paper we propose a robust approach to detect moving objects for video surveillance applications. we demonstrate that a jointly use of frame by frame difference with a background subtraction algorithm allows us to have a strong and fast pixel foreground classification without the need of previous background learning. the joint difference algorithm uses frame difference information to correct pixels classification made by a background subtraction algorithm while selectively updating the background model according to such classification. in this way we should perform motion segmentation also in presence of environmental changes such as illumination variations or \"waking persons\". the algorithm is capable of 15 fps tracking of moving people on 640-480 unsampled color images; results on both vssn06 and wallflower [8] benchmark videos are presented.",
    "present_kp": [
      "tracking",
      "motion segmentation"
    ],
    "absent_kp": [
      "background modeling"
    ]
  },
  {
    "title": "a multivariable multiobjective predictive controller.",
    "abstract": "predictive control of mimo processes is a challenging problem which requires the specification of a large number of tuning parameters (the prediction horizon, the control horizon and the cost weighting factor). in this context, the present paper compares two strategies to design a supervisor of the multivariable generalized predictive controller (mgpc), based on multiobjective optimization. thus, the purpose of this work is the automatic adjustment of the mgpc synthesis by simultaneously minimizing a set of closed loop performances (the overshoot and the settling time for each output of the mimo system). first, we adopt the weighted sum method (wsm), which is an aggregative method combined with a genetic algorithm (ga) used to minimize a single criterion generated by the wsm. second, we use the non-dominated sorting genetic algorithm ii (nsga-ii) as a pareto method and we compare the results of both the methods. the performance of the two strategies in the adjustment of multivariable predictive control is illustrated by a simulation example. the simulation results confirm that a multiobjective, pareto-based ga search yields a better performance than a single objective ga.",
    "present_kp": [
      "closed loop performance",
      "generalized predictive control",
      "multiobjective optimization",
      "weighted sum method",
      "nsga-ii"
    ],
    "absent_kp": [
      "coupled multivariable system"
    ]
  },
  {
    "title": "filtering xml documents using xpath expressions and aspect-oriented programming.",
    "abstract": "in this paper, we present the design and implementation of a filtering approach for xml documents which is based on xpath expressions and aspect-oriented programming (aop). the class of xpath expressions used allows for branching, wildcards and descendant relationships between nodes. for the embedding of simple paths into xpath expressions, a dynamic programming approach is proposed. the aop paradigm, which provides a means for encapsulating crosscutting concerns in software, is introduced to integrate the filtering approach in the broader context of event-based parsing of xml documents using sax.",
    "present_kp": [
      "sax",
      "xpath",
      "aspect-oriented programming",
      "xml"
    ],
    "absent_kp": []
  },
  {
    "title": "exponential type orbitals with generalized hyperbolic cosine functions for atomic systems.",
    "abstract": "radial basis functions, constructed from slater type rn 1e r r n 1 e r and generalized exponential type rn 1e r? r n 1 e r ? functions with the generalized hyperbolic cosine type functions coshpq(?r) cosh p q ( ? r ) and coshpq(?r?) cosh p q ( ? r ? ) , where p and q are arbitrary parameters, are proposed and applied to hartreefockroothaan calculations of atomic systems. the performance of new basis functions within the minimal basis sets framework has been compared to numerical hartreefock results and previous results presented by similar basis functions in the literature. the results obtained by the new basis sets surpass the accuracy of existing basis sets of similar hyperbolic cosine type functions.",
    "present_kp": [
      "generalized hyperbolic cosine function",
      "exponential type orbital",
      "hartreefockroothaan calculation"
    ],
    "absent_kp": [
      "noninteger principal quantum number"
    ]
  },
  {
    "title": "the critical role of velocity storage in production of motion sickness.",
    "abstract": "we propose that motion sickness is mediated through the orientation properties of velocity storage in the vestibular system that tend to align eye velocity produced by the angular vestibulo-ocular reflex (avor) with gravito-inertial acceleration (gia). (gia is the sum of the linear accelerations acting on the head. in the absence of translational accelerations, gravity is the gia.) we further postulate that motion sickness produced by cross-coupled vestibular stimulation can be characterized by a metric composed of the disparity between the axis of eye rotation and the gia, the strength of the response to angular motion, and the response duration, as determined by the central vestibular time constant, that is, by the time constant of velocity storage. the nodulus and uvula of the vestibulocerebellum are likely to be the central sites where the disparity is sensed, where the vestibular time constants are habituated, and where links are made to the autonomic system to produce the symptoms and signs.",
    "present_kp": [
      "vestibular",
      "nodulus",
      "uvula",
      "vestibulocerebellum"
    ],
    "absent_kp": [
      "head movements",
      "nystagmus",
      "vertigo",
      "roll while rotating"
    ]
  },
  {
    "title": "a comment to the papers by opthof and leydesdorff, scientometrics, 88, <phone>, 2011 and waltman et al., scientometrics, 88, <phone>, 2011.",
    "abstract": "in this comment, we re-evaluate an example using a \"thermodynamic\" paradigm to show how bibliometrics can incorporate normalization into the evaluative process. the motivation for this is the recent exchange in the pages of this journal from two groups that have taken different positions on how normalization should be done.",
    "present_kp": [
      "bibliometrics",
      "normalization"
    ],
    "absent_kp": [
      "performance",
      "indicators",
      "quality",
      "quantity",
      "quasity",
      "energy",
      "exergy",
      "p-index",
      "h-index"
    ]
  },
  {
    "title": "a cmos elliptic low-pass switched capacitor ladder filter for video communication using bilinear implementation.",
    "abstract": "this paper presents an approach to design of a high frequency switched capacitor filters for video communication networks. in the case of low osr, using common methods reduces the sndr and accuracy of the filter. bilinear implementation techniques are applied to an untried fifth-order elliptic sc filter, and are simulated in a 0.35mm cmos technology. cutoff frequency of the filter is 3.6mhz and with bilinear implementation, we achieve 72db sfdr, 62db thd and stop band attenuation greater than -30db. all the capacitors are scaled down in order to reduce the settling time of the amplifiers. the circuit operates with +3v supply and typically dissipates 15 mw when sampled at 18mhz and has full swing about 1.8v.",
    "present_kp": [
      "switched capacitor filter"
    ],
    "absent_kp": [
      "bilinear integrator",
      "video communication filter"
    ]
  },
  {
    "title": "completely induced l-fuzzy topological spaces.",
    "abstract": "the first aim of this paper is to introduce and to study the concepts of complete scott continuity' and 'completely induced l-fuzzy topological space'. the second is to discuss the connections between some separation, countability and covering properties of an ordinary topological space and its corresponding completely induced l-fuzzy topological space.",
    "present_kp": [
      "completely induced l-fuzzy topological space"
    ],
    "absent_kp": [
      "fuzzy lattice",
      "prime element",
      "regular open set",
      "semi-regular topological space",
      "completely scott continuous function"
    ]
  },
  {
    "title": "observer-controller design for three dimensional overhead cranes using time-scaling.",
    "abstract": "in this paper we address the design of an observer-controller for a three degrees of freedom overhead crane. we consider a linear model of the crane where the length of the suspending rope is a time-varying parameter. the set of models given by frozen values of the rope length can be reduced to a single time-invariant reference model using suitable time-scalings. we construct a controller and an observer for the reference model assigning the desired closed loop eigenvalues for both system and estimation error. the time-scaling relations can be used to derive a control law for the time-varying system that implements an implicit gain-scheduling [6]. a second gain-scheduling is used to choose suitable closed-loop eigenvalues for different values of the load and lifting/lowering operations. using a lyapunov-like theorem, it is also possible to rnd relative upper bounds for the rate of change of the varying parameter that ensure the stability of the time-varying system.",
    "present_kp": [
      "overhead cranes"
    ],
    "absent_kp": [
      "gain scheduling",
      "linear time-varying systems",
      "pole placement"
    ]
  },
  {
    "title": "virtual prototype and experimental research on gear multi-fault diagnosis using wavelet-autoregressive model and principal component analysis method.",
    "abstract": "gear systems are an essential element widely used in a variety of industrial applications. since approximately 80% of the breakdowns in transmission machinery are caused by gear failure, the efficiency of early fault detection and accurate fault diagnosis are therefore critical to normal machinery operations. reviewed literature indicates that only limited research has considered the gear multi-fault diagnosis, especially for single, coupled distributed and localized faults. through virtual prototype simulation analysis and experimental study, a novel method for gear multi-fault diagnosis has been presented in this paper. this new method was developed based on the integration of wavelet transform (wt) technique, autoregressive (ar) model and principal component analysis (pca) for fault detection. the wt method was used in the study as the de-noising technique for processing raw vibration signals. compared with the noise removing method based on the time synchronous average (tsa), the wt technique can be performed directly on the raw vibration signals without the need to calculate any ensemble average of the tested gear vibration signals. more importantly, the wt can deal with coupled faults of a gear pair in one operation while the tsa must be carried out several times for multiple fault detection. the analysis results of the virtual prototype simulation prove that the proposed method is a more time efficient and effective way to detect coupled fault than tsa, and the fault classification rate is superior to the tsa based approaches. in the experimental tests, the proposed method was compared with the mahalanobis distance approach. however, the latter turns out to be inefficient for the gear multi-fault diagnosis. its defect detection rate is below 60%, which is much less than that of the proposed method. furthermore, the ability of the ar model to cope with localized as well as distributed gear faults is verified by both the virtual prototype simulation and experimental studies.",
    "present_kp": [
      "virtual prototype",
      "pca"
    ],
    "absent_kp": [
      "gear fault diagnosis",
      "wavelet ar model"
    ]
  },
  {
    "title": "bounded partial-order reduction.",
    "abstract": "eliminating concurrency errors is increasingly important as systems rely more on parallelism for performance. exhaustively exploring the state-space of a program's thread interleavings finds concurrency errors and provides coverage guarantees, but suffers from exponential state-space explosion. two prior approaches alleviate state-space explosion. (1) dynamic partial-order reduction (dpor) provides full coverage and explores only one interleaving of independent transitions. (2) bounded search provides bounded coverage by enumerating interleavings that do not exceed a bound. in particular, we focus on preemption-bounding. combining partial-order reduction with preemption-bounding had remained an open problem. we show that preemption-bounded search explores the same partial orders repeatedly and consequently explores more executions than unbounded dpor, even for small bounds. we further show that if dpor simply uses the preemption bound to prune the state space as it explores new partial orders, it misses parts of the state space reachable in the bound and is therefore unsound. the bound essentially induces dependences between otherwise independent transitions in the dpor state space. we introduce bounded partial order reduction (bpor), a modification of dpor that compensates for bound dependences. we identify properties that determine how well bounds combine with partial-order reduction. we prove sound coverage and empirically evaluate bpor with preemption and fairness bounds. we show that by eliminating redundancies, bpor significantly reduces testing time compared to bounded search. bpor's faster incremental guarantees will help testers verify larger concurrent programs.",
    "present_kp": [
      "bounded partial-order reduction",
      "dynamic partial-order reduction",
      "concurrency",
      "fairness"
    ],
    "absent_kp": [
      "algoirthms",
      "verification",
      "liveness",
      "model checking",
      "shared-memory programs",
      "software testing"
    ]
  },
  {
    "title": "effect of gelation and ball milling on the optical absorption and emission of macrilon yellow dye molecules trapped in silica prepared by the solgel method.",
    "abstract": "organic dye molecules (macrilon yellow) were embedded into silicon dioxide matrix by the solgel method. optical measurements at room temperature of absorption and emission were performed at different steps of the gelation process, starting when the suspension was liquid and finishing when it got the solid state. optical absorption spectra show the dependence with gelation process and with the milling process. the intensity of the absorption band at 420 nm increases with the viscosity of the suspension. this effect is more important for the milled suspension, working with the same pigment concentration. the gelation and the milling processes also affect the emission spectra. the emission spectra has two bands centered at about 520 and 560 nm. during gelation the relative intensity of the band at about 560 nm increases respect the band at about 520 nm. the emission band of the sample made with milled suspension is three times more intense than that of the not milled suspension.",
    "present_kp": [
      "optical absorption",
      "emission",
      "solgel"
    ],
    "absent_kp": [
      "dyes",
      "agglomerates",
      "dispersion"
    ]
  },
  {
    "title": "learning in games using the imprecise dirichlet model.",
    "abstract": "we propose a new learning model for finite strategic-form two-player games based on fictitious play and walley's imprecise dirichlet model . this model allows the initial beliefs of the players about their opponent's strategy choice to be near-vacuous or imprecise instead of being precisely defined. a similar generalization can be made as the one proposed by fudenberg and kreps for fictitious play, where assumptions about immediate behavior are replaced with assumptions about asymptotic behavior. we also obtain similar convergence results for this generalization: if there is convergence, it will be to an equilibrium.",
    "present_kp": [
      "learning",
      "fictitious play",
      "imprecise dirichlet model",
      "two-player games"
    ],
    "absent_kp": [
      "imprecise probability models",
      "decision making"
    ]
  },
  {
    "title": "determining the automorphism group of the linear ordering polytope.",
    "abstract": "in this paper, we explore the combinatorial automorphism group of the linear ordering polytope for each n>1. we establish that this group is isomorphic to if n>2 (and to if n=2). in doing so, we provide a simple and unified interpretation of all the automorphisms.",
    "present_kp": [
      "linear ordering polytope",
      "automorphism group"
    ],
    "absent_kp": [
      "facets"
    ]
  },
  {
    "title": "on the core, the weber set and convexity in games with a priori unions.",
    "abstract": "this paper deals with the concepts of core and weber set with a priori unions la owen. as far as we know, the owen approach to games with a priori unions has never been studied from the coalitional stability point of view. thus we introduce the coalitional core and coalitional weber set and characterize the class of convex games with a priori unions by means of the relationships between both solution concepts.",
    "present_kp": [
      "a priori unions",
      "coalitional core",
      "coalitional weber set",
      "convexity"
    ],
    "absent_kp": [
      "cooperative games"
    ]
  },
  {
    "title": "skolem machines.",
    "abstract": "the skolem machine is a turing-complete machine model where the instructions are first-order formulas of a specific form. we introduce skolem machines and prove their logical correctness and completeness. skolem machines compute queries for the geolog language, a rich fragment of first-order logic. the concepts of geolog trees and complete geolog trees are defined, and these tree concepts are used to show logical correctness and completeness of skolem machine computations. the universality of skolem machine computations is demonstrated. lastly, the paper outlines implementation design issues using an abstract machine model approach.",
    "present_kp": [
      "skolem machines",
      "geolog language",
      "geolog trees",
      "correctness",
      "completeness",
      "universality"
    ],
    "absent_kp": [
      "finitary geometric logic"
    ]
  },
  {
    "title": "cell signaling.",
    "abstract": "this review explores advances in our understanding of dynamicism in cellular signaling. areas highlighted include the role of stochasticity in producing diversity in analogous signaling circumstances; population desynchronization's effect in masking newly appreciated repetitive bursts in protein phosphorylation and messenger rna production; double-positive feedback interactions and their ability to synchronize multiple signal transduction pathways; scaffolding proteins control over signaling feedback; and frequency-responsive transcriptional regulation as an example of dynamicism in signaling.",
    "present_kp": [
      "signal transduction",
      "feedback"
    ],
    "absent_kp": [
      "gene transcription",
      "oscillations",
      "tnf",
      "nf-kappab",
      "jnk",
      "p38",
      "erk",
      "temporal"
    ]
  },
  {
    "title": "refactoring of timing graphs and its use in capturing topological correlation in ssta.",
    "abstract": "reconvergent paths in circuits have been a nuisance in various computer-aided design (cad) algorithms, but no elegant solution to deal with them has been found yet. in statistical static timing analysis (ssta), they cause difficulty in capturing topological correlation. this paper presents a technique that in arbitrary block-based ssta reduces the error caused by ignoring topological correlation. we interpret a timing graph as an algebraic expression made up of addition and maximum operators. we define the division operation on the expression and propose algorithms that modify factors in the expression without expansion. as a result, the algorithms produce an expression to derive the latest arrival time with better accuracy in ssta. existing techniques handling reconvergent fanouts usually use dependency lists, requiring quadratic space complexity. instead, the proposed technique has linear space complexity by using a new directed acyclic graph search algorithm. our results show that it outperforms an existing technique in speed and memory usage with comparable accuracy. more important, the proposed technique is not limited to ssta and is potentially applicable to various issues due to reconvergent paths in timing-related cad algorithms.",
    "present_kp": [
      "reconvergent paths",
      "static timing analysis"
    ],
    "absent_kp": [
      "common path pessimism",
      "process variation"
    ]
  },
  {
    "title": "second order symmetric duality in nondifferentiable multiobjective fractional programming with cone convex functions.",
    "abstract": "in the present paper, we consider mond-weir type nondifferentiable second order fractional symmetric dual programs over arbitrary cones and derive duality results under second order k?f-convexity/k?f-pseudoconvexity assumptions. our results generalize several known results in the literature.",
    "present_kp": [
      "second order symmetric duality",
      "multiobjective fractional programming",
      "k?f-convexity",
      ""
    ],
    "absent_kp": [
      "efficient solution"
    ]
  },
  {
    "title": "on some properties of covering based approximations of classifications of sets.",
    "abstract": "approximations of classifications, introduced and studied by grzymala busse is a notion different from the notion of approximations of sets (pawlak ). in fact the equivalence classes of approximate classifications cannot be arbitrary sets. busse had established properties of approximations of classifications which were recently extended to necessary and sufficient type theorems by tripathy et. al. four types of covering based rough sets have been obtained as generalization of basic rough sets. also another covering rough set from a topological point of view has been obtained (14, 15, 16, 17, 18, 19]. attempts have been made in to extend the above results to covering based rough sets. in this paper we carry out this study further by deriving results for all the five types of covering based rough sets. these results can be used to derive rules for information systems with domains of attributes values being covers instead of partitions.",
    "present_kp": [
      "classifications",
      "rough sets",
      "approximations of classifications"
    ],
    "absent_kp": []
  },
  {
    "title": "is complexity really the enemy of software security.",
    "abstract": "software complexity is often hypothesized to be the enemy of software security. we performed statistical analysis on nine code complexity metrics from the javascript engine in the mozilla application framework to investigate if this hypothesis is true. our initial results show that the nine complexity measures have weak correlation (?=0.30 at best) with security problems for mozilla javascript engine. the study should be replicated on more products with design and code-level metrics. it may be necessary to create new complexity metrics to embody the type of complexity that leads to security problems.",
    "present_kp": [
      "software complexity"
    ],
    "absent_kp": [
      "fault prediction",
      "software metrics",
      "security metrics",
      "vulnerability prediction",
      "reliability"
    ]
  },
  {
    "title": "study of the ultrasonic compaction process of composite laminates - part i: process modeling.",
    "abstract": "recent advances in out-of-autoclave manufacturing processes for composite materials may require the compaction of the pre-preg plies just after placing them. ultrasonic compactors are a good solution for this task. the quality of the compaction will depend on the frequency and amplitude of the ultrasonic vibration, which produces a viscous heating in the resin. the increase in temperature liquefies the resin, thus helping the air bubbles to escape. the study of how this heat is generated and distributed is fundamental to better understand and optimize the compaction process. in this work, the formulation of a thermomechanical model for the viscous heating of an uncured laminate of pre-preg plies due to ultrasonic vibrations is presented. in addition, a thermal model for the heat transfers to obtain the temperature distribution during the compaction process of composite layers will be shown.",
    "present_kp": [
      "ultrasonic compaction"
    ],
    "absent_kp": [
      "composites",
      "heat generation",
      "modelling"
    ]
  },
  {
    "title": "slip-adaptive walk of quadruped robot.",
    "abstract": "in this paper, we investigated the effects of the friction condition on walking pattern and energy efficiency, and based on the results, we proposed two new slip-adaptive strategies for generating a slip-adaptive walk. the first strategy for a slip-adaptive walk uses a slip reflex via a central pattern generator (cpg) to change the walking pattern. the second strategy for a walk uses a force control to immediately compensate a slip. using these strategies, a walk, which is adaptive to varying friction conditions and slips, becomes possible. the validity of the proposed method is confirmed through simulation and experimentation.",
    "present_kp": [
      "quadruped robot",
      "slip-adaptive",
      "force control"
    ],
    "absent_kp": [
      "dynamic walk"
    ]
  },
  {
    "title": "on range and response: dimensions of process flexibility.",
    "abstract": "there are two dimensions to process flexibility: range versus response. range is the extent to which a system can adapt, while response is the rate at which the system can adapt. although both dimensions are important, the existing literature does not analytically examine the response dimension vis-a-vis the range dimension. in this paper, we model the response dimension in terms of uniformity of production cost. we distinguish between primary and secondary production where the latter is more expensive. we examine how the range and response dimension interact to affect the performance of the process flexible structure. we provide analytical lower bounds to show that under all scenarios on response flexibility, moderate form of range flexibility (via chaining structure) still manages to accrue non-negligible benefits vis-a-vis the fully flexible structure (the bound is 29.29% when demand is normally distributed). we show further that given limited resources, upgrading system response dimension outperforms upgrading system range dimension in most cases. this confirms what most managers believe in intuitively. we observe also that improving system response can provide even more benefits when coupled with initiatives to reduce demand variability. this is in direct contrast with range flexibility, which is more valuable when the system has higher variability.",
    "present_kp": [],
    "absent_kp": [
      "probability: renewal processes",
      "production: process flexibility",
      "facility planning: design"
    ]
  },
  {
    "title": "one step forward: linking wireless self-organizing network validation techniques with formal testing approaches.",
    "abstract": "wireless self-organizing networks (wsons) have attracted considerable attention from the network research community; however, the key for their success is the rigorous validation of the properties of the network protocols. applications of risk or those demanding precision (like alert-based systems) require a rigorous and reliable validation of deployed network protocols. while the main goal is to ensure the reliability of the protocols, validation techniques also allow the establishment of their correctness regarding the related protocols' requirements. nevertheless, even if different communities have carried out intensive research activities on the validation domain, wsons still raise new issues for and challenging constraints to these communities. we thus, advocate the use of complementary techniques coming from different research communities to efficiently address the validation of wson protocols. the goal of this tutorial is to present a comprehensive review of the literature on protocol engineering techniques and to discuss difficulties imposed by the characteristics of wsons on the protocol engineering community. following the formal and nonformal classification of techniques, we provide a discussion about components and similarities of existing protocol validation approaches. we also investigate how to take advantage of such similarities to obtain complementary techniques and outline new challenges.",
    "present_kp": [
      "protocol validation",
      "wireless self-organizing networks"
    ],
    "absent_kp": [
      "algorithms",
      "verification",
      "performance"
    ]
  },
  {
    "title": "waveguide mode solver based on neumann-to-dirichlet operators and boundary integral equations.",
    "abstract": "for optical waveguides with high index-contrast and sharp corners, existing full-vectorial mode solvers including those based on boundary integral equations typically have only second or third order of accuracy. in this paper, a new full-vectorial waveguide mode solver is developed based on a new formulation of boundary integral equations and the so-called neumann-to-dirichlet operators for sub-domains of constant refractive index. the method uses the normal derivatives of the two transverse magnetic field components as the basic unknown functions, and it offers higher order of accuracy where the order depends on a parameter used in a graded mesh for handling the corners. the method relies on a standard nystrom method for discretizing integral operators and it does not require analytic properties of the electromagnetic field (which are singular) at the corners.",
    "present_kp": [
      "optical waveguide",
      "waveguide mode solver",
      "boundary integral equation",
      "neumann-to-dirichlet operator"
    ],
    "absent_kp": []
  },
  {
    "title": "adaptively entropy-based weighting classifiers in combination using dempstershafer theory for word sense disambiguation.",
    "abstract": "in this paper we introduce an evidential reasoning based framework for weighted combination of classifiers for word sense disambiguation (wsd). within this framework, we propose a new way of defining adaptively weights of individual classifiers based on ambiguity measures associated with their decisions with respect to each particular pattern under classification, where the ambiguity measure is defined by shannons entropy. we then apply the discounting-and-combination scheme in dempstershafer theory of evidence to derive a consensus decision for the classification task at hand. experimentally, we conduct two scenarios of combining classifiers with the discussed method of weighting. in the first scenario, each individual classifier corresponds to a well-known learning algorithm and all of them use the same representation of context regarding the target word to be disambiguated, while in the second scenario the same learning algorithm applied to individual classifiers but each of them uses a distinct representation of the target word. these experimental scenarios are tested on english lexical samples of senseval-2 and senseval-3 resulting in an improvement in overall accuracy.",
    "present_kp": [
      "word sense disambiguation",
      "entropy"
    ],
    "absent_kp": [
      "computational linguistics",
      "classifier combination",
      "dempsters rule of combination"
    ]
  },
  {
    "title": "performance evaluation of ad hoc routing protocols for military communications.",
    "abstract": "mobile ad hoc networks (manets) are of much interest to both the research community and the military because of the potential to establish a communication network in any situation that involves emergencies. examples are search-and-rescue operations, military deployment in hostile environments, and several types of police operations. one critical open issue is how to route messages considering the characteristics of these networks. the nodes act as routers in an environment without a fixed infrastructure, the nodes are mobile, the wireless medium has its own limitations compared to wired networks, and existing routing protocols cannot be employed, at least without modifications. over the last few years, a number of routing protocols have been proposed and enhanced to address the issue of routing in manets. it is not clear how those different protocols perform under different environments. one protocol may be the best in one network configuration but the worst in another. this article provides an analysis and performance evaluation of those protocols that may be suitable for military communications. the evaluation is conducted in two phases. in the first phase, we compare the protocols based on qualitative metrics to locate those that may fit our evaluation criteria. in the second phase, we evaluate the selected protocols from the first phase based on quantitative metrics in a mobility scenario that reflects tactical military movements. the results disclose that there is no routing protocol in the current stage without modifications that can provide efficient routing to any size of network, regardless of the number of nodes and the network load and mobility.",
    "present_kp": [
      "manet",
      "routing protocols"
    ],
    "absent_kp": [
      "ad hoc wireless networks",
      "network simulator "
    ]
  },
  {
    "title": "functional networks models for rapid characterization of thin films: an application to ultrathin polycrystalline silicon germanium films.",
    "abstract": "functional network predictive model for film characterization is constructed. 154 experimental data sets were used for training and testing. it accurately predicts poly-sige film thickness, resistivity and deposition rate. the models can be used to proactively control film properties.",
    "present_kp": [
      "functional network",
      "poly-sige"
    ],
    "absent_kp": [
      "prediction",
      "cvd",
      "modeling"
    ]
  },
  {
    "title": "light stemming approaches for the french, portuguese, german and hungarian languages.",
    "abstract": "this paper describes and evaluates various general stemming approaches for the french, portuguese (brazilian), german and hungarian languages. based on the clef test-collections, we demonstrate that light stemmers for the french, portuguese and hungarian languages perform well, and reasonably well for the german language. variations in mean average precision among the different stemming approaches are also evaluated and sometimes they are found statistically significant.",
    "present_kp": [
      "stemmer",
      "portuguese",
      "hungarian",
      "german"
    ],
    "absent_kp": [
      "natural language processing",
      "stemming for french"
    ]
  },
  {
    "title": "piv and lda measurements of secondary flow in a meandering channel for overbank flow.",
    "abstract": "secondary flow in a compound meandering channel with straight floodplain banks for overbank was investigated by a visualization method and velocity measurement using three-component laser doppler anemometor (lda). the secondary flow in a cross section was visualized by the neutral buoyant tracer method with a submergible video camera. secondary flow vectors in a cross section were obtained by using piv software with captured frames from video source through pc and also by lda measurements. from the comparison of the piv and lda results, it is found that piv data show good agreement in quality with lda measurements when the secondary flow is strong and stable as shown in this paper.",
    "present_kp": [
      "piv",
      "lda measurement",
      "compound meandering channel"
    ],
    "absent_kp": [
      "visualization of secondary flow"
    ]
  },
  {
    "title": "exploiting concurrency in a dbms implementation for production systems.",
    "abstract": "an important aspect of integration of ai and dbms technology is identifying functional similarities in database processing and reasoning with rules. in this paper, we focus on applying concurrency techniques to rule-based production systems. we tailor dbms concurrent execution to a production system environment and investigate the resulting concurrent execution strategies for productions. this research is in conjunction with a novel dbms mechanism for testing if the left hand side conditions of productions are satisfied. this set-oriented mechanism uses a special data structure implemented using relational tables. to demonstrate the equivalence of a serial and a concurrent (interleaved) execution strategy, for a set of productions, we assume a locking scheme to enforce serializability and specify what locks must be obtained. we define a logical commit point for a production. after this point the execution of a production is independent of all other (executing) productions. we compare the number of possible serial and parallel execution schedules.",
    "present_kp": [
      "point",
      "serializability",
      "tailor",
      "tables",
      "paper",
      "dbms",
      "product",
      "reasoning",
      "aspect",
      "research",
      "strategies",
      "systems",
      "test",
      "parallel",
      "functional",
      "implementation",
      "concurrency",
      "process",
      "demonstrate",
      "commit",
      "relation",
      "database",
      "rules",
      "locking",
      "scheme"
    ],
    "absent_kp": [
      "scheduling",
      "data structures",
      "technologies",
      "environments",
      "integrability"
    ]
  },
  {
    "title": "from simple structure to sparse components: a review.",
    "abstract": "the article begins with a review of the main approaches for interpretation the results from principal component analysis (pca) during the last 5060 years. the simple structure approach is compared to the modern approach of sparse pca where interpretable solutions are directly obtained. it is shown that their goals are identical but they differ by the way they are realized. next, the most popular and influential methods for sparse pca are briefly reviewed. in the remaining part of the paper, a new approach to define sparse pca is introduced. several alternative definitions are considered and illustrated on a well-known data set. finally, it is demonstrated, how one of these possible versions of sparse pca can be used as a sparse alternative to the classical rotation methods.",
    "present_kp": [],
    "absent_kp": [
      "simple structure loadings",
      "orthogonal and oblique rotations",
      "factor analysis",
      "sparse component loadings",
      "sparseness inducing constraints",
      "lasso",
      "constrained optimization on matrix manifolds",
      "projected gradients"
    ]
  },
  {
    "title": "a lifespan-aware reliability scheme for raid-based flash storage.",
    "abstract": "due to the ever-growing capacity of flash memory along with its good properties such as low-power consumption and high performance, flash-based ssds (solid state disks) are anticipated to be used in the storage of high-end server systems. however, the reliability problem of flash devices is becoming increasingly serious. the number of p/e (program/erase) cycles allowed to each flash block is too small, especially less than 10,000 for mlc (multi-level cell) flash memory. furthermore, the bit error rate of flash memory becomes rapidly high as the number of p/e cycles increases. to relieve these problems, we present a lifespan-aware reliability scheme, which adopts raid technologies together with eccs (error correction codes). first, our scheme dynamically manages the size of striping group to cope with the increasing error rates of flash memory as the number of p/e cycles increases. second, we use a device-aware log block mapping scheme, which uses different reliability policies for data blocks and log blocks by taking advantage of the characteristics of each block type. third, we use small amount of storage class memory (scm) to save parity blocks temporarily. by absorbing frequent updates of parity into scm, we can extend the lifespan of flash memory. simulation experiments show that our scheme obtains high reliability with minimum space overhead as well as improved i/o performances compared to traditional raid-5.",
    "present_kp": [
      "reliability",
      "raid"
    ],
    "absent_kp": [
      "ssd ",
      "ftl ",
      "nand flash memory"
    ]
  },
  {
    "title": "fast image recovery using variable splitting and constrained optimization.",
    "abstract": "we propose a new fast algorithm for solving one of the standard formulations of image restoration and reconstruction which consists of an unconstrained optimization problem where the objective includes an data-fidelity term and a nonsmooth regularizer. this formulation allows both wavelet-based (with orthogonal or frame-based representations) regularization or total-variation regularization. our approach is based on a variable splitting to obtain an equivalent constrained optimization formulation, which is then addressed with an augmented lagrangian method. the proposed algorithm is an instance of the so-called alternating direction method of multipliers, for which convergence has been proved. experiments on a set of image restoration and reconstruction benchmark problems show that the proposed algorithm is faster than the current state of the art methods.",
    "present_kp": [
      "augmented lagrangian",
      "image restoration",
      "variable splitting"
    ],
    "absent_kp": [
      "compressive sensing",
      "convex optimization",
      "image reconstruction",
      "inverse problems",
      "total variation",
      "wavelets"
    ]
  },
  {
    "title": "multi-class supervised classification of electrical borehole wall images using texture features.",
    "abstract": "electrical borehole wall images represent micro-resistivity measurements at the borehole wall. the lithology reconstruction is often based on visual interpretation done by geologists. this analysis is very time-consuming and subjective. different geologists may interpret the data differently. in this work, linear discriminant analysis (lda) in combination with texture features is used for an automated lithology reconstruction of odp (ocean drilling program) borehole 1203a drilled during leg 197. six rock groups are identified by their textural properties in resistivity data obtained by a formation mircoscanner (fms). although discriminant analysis can be used for multi-class classification, non-optimal decision criteria for certain groups could emerge. for this reason, we use a combination of 2-class (binary) classifiers to increase the overall classification accuracy. the generalization ability of the combined classifiers is evaluated and optimized on a testing dataset where a classification rate of more than 80% for each of the six rock groups is achieved. the combined, trained classifiers are then applied on the whole dataset obtaining a statistical reconstruction of the logged formation. compared to a single multi-class classifier the combined binary classifiers show better classification results for certain rock groups and more stable results in larger intervals of equal rock type.",
    "present_kp": [
      "lithology reconstruction",
      "discriminant analysis",
      "texture features",
      "multi-class classification",
      "binary classifier"
    ],
    "absent_kp": [
      "micro-resistivity data"
    ]
  },
  {
    "title": "performance evaluation of turbo code over impulsive noise channel.",
    "abstract": "performance of turbo codes over an impulsive noise channel is analyzed by extending an evaluation method over awgn channels. burst noise generation is considered with respect to the noise model by applying a hidden markov model (hmm). a bound calculation method is derived by using a combined trellis which consists of code trellis and hmm trellis. in the simulation, an iterative decoder using the combined trellis into each component decoder is proposed. by using this method, the simulation results show the expectation of the coincidence with the calculated bounds at larger e-b/n-0 for various conditions. search results of optimum component code using proposed bound are shown.",
    "present_kp": [
      "turbo codes",
      "impulsive noise"
    ],
    "absent_kp": [
      "performance bounds"
    ]
  },
  {
    "title": "a force threshold-based position controller for legged locomotion.",
    "abstract": "taking inspiration from local leg feedback control loops present in animal legs, a force threshold-based position (ftp) controller is presented to aid with legged locomotion over irregular terrain. the algorithm uses pre-planned position trajectories and force feedback to either elevate or depress the foot. the ftp controller isolates the control of each leg to use only localized feedback, which can result in greater responsiveness to the terrain when compared to a centralized controller arbitrating all of the joint positions in a high degree of freedom system. the controller is robust to terrain elevations without using visual sensors, a priori terrain information, inertial sensing or inter-leg communication. results of the ftp controller applied to a hexapod system in simulation and on an experimental system are shown in this paper. the algorithm also has the potential for expansion to bipeds, quadrupeds and other biologically-inspired forms.",
    "present_kp": [
      "biologically-inspired"
    ],
    "absent_kp": [
      "hexapod-walking",
      "force-feedback",
      "local-leg-control",
      "uneven-terrain"
    ]
  },
  {
    "title": "building access control models with attribute exploration.",
    "abstract": "the use of lattice-based access control models has been somewhat restricted by their complexity. we argue that attribute exploration from formal concept analysis can help create lattice models of manageable size, while making it possible for the system designer to better understand dependencies between different security categories in the domain and, thus, providing certain guarantees for the relevance of the constructed model to a particular application. in this paper, we introduce the method through an example.",
    "present_kp": [
      "lattice-based access control models",
      "attribute exploration"
    ],
    "absent_kp": [
      "mandatory access control models",
      "concept lattices",
      "implications"
    ]
  },
  {
    "title": "maximal recovery network coding under topology constraint.",
    "abstract": "network coding (nc) within wireless sensor networks (wsns) can be viewed as the mapping of efficient channel codes to the data generated within the network. in particular, this perspective of code-on-network-graphs (cng) can be exploited to map source data generated within wsn (of size k) to a variable nodes subset in low-density parity check (ldpc) codes. the resulting fixed size symbol stream when transmitted through the network suffers erasures. at sink, an average of z source symbols can be recovered by employing belief propagation decoding. in this paper, we determine cng code ensembles that achieve maximal recovery (z/k) for different erasure rates and network topological constraints corresponding to node transmission range. an analytic framework to predict code performance under transmission range constraints is developed. additionally, necessary condition for code stability was derived using fixed-point stability analysis. optimal solutions for a wsn with 1000 nodes are determined using differential evolution algorithm. we outline a distributed algorithm for generating a sequence of encoded symbols adhering to the designed code ensemble. the performance of the designed cng code is demonstrated to be superior to random nc and growth code based ensembles, as well as resilient to network size and inter-connectivity variations.",
    "present_kp": [
      "network coding ",
      "wireless sensor networks "
    ],
    "absent_kp": [
      "low-density parity check  codes",
      "network channel coding",
      "partial recovery"
    ]
  },
  {
    "title": "an interactive bi-objective shortest path approach: searching for unsupported nondominated solutions.",
    "abstract": "in many network routing problems several conflicting objectives must be considered. even for the bi-objective shortest path problem, generating and presenting the whole set of nondominated solutions (paths) to a decision maker, in general, is not effective because the number of these paths can be very large. interactive procedures are adequate to overcome these drawbacks. current et al. proposed an interactive approach based on a nise-like procedure to search for nondominated supported solutions and using auxiliar constrained shortest path problems to carry out the search inside the duality gaps. in this paper we propose a new interactive approach to search for unsupported nondominated solutions (lying inside duality gaps) based on a k-shortest path procedure. both approaches are compared.",
    "present_kp": [
      "shortest path",
      "network routing"
    ],
    "absent_kp": [
      "multiple criteria"
    ]
  },
  {
    "title": "performance analysis of carousel systems with double shuttle.",
    "abstract": "this paper deals with automated carousel systems which have been widely used in various industrial applications. we attempt to measure analytically the effects of double shuttle of the storage/retrieval machine on the throughput of standard and double carousel systems. for the double carousel system, a retrieval sequencing rule is proposed which utilizes the characteristics of two independently rotating sub-carousels. and then the expected four command cycle time models are developed under the proposed rule. through sensitivity studies, we compare the performance of the two carousel systems working on the four command cycles with those on the dual command cycles.",
    "present_kp": [
      "double carousel",
      "double shuttle",
      "four command cycle"
    ],
    "absent_kp": [
      "standard carousel"
    ]
  },
  {
    "title": "an answer to a user's plea.",
    "abstract": "an era of consumer awareness and power is upon us. although the great silent majority of computer system users (or consumers) may not be aware of it yet, the dawn of their era of awareness and power is near! with the development of multiaccess computer systems and networks of computers, the user is beginning to get the opportunity to employ his choice of system to execute his programs. even now, some fortunate users are no longer constrained to use the possibly inefficient services of the neighborhood computer monopoly, their organization's local computer center. they are able to run their programs at installations that give them the most for their money, merely by punching the buttons of their telephone. as this era dawns, directors of inefficient computer centers must be wary lest their empires wither from lack of users. but alas, the poor user! a few problems must be solved even as his era dawns! how does he know that computing at his local computer centre costs too much? how can he determine which system gives him the biggest bang for his buck?",
    "present_kp": [
      "network",
      "program",
      "organization",
      "service",
      "awareness",
      "power",
      "use",
      "systems",
      "users",
      "install",
      "user"
    ],
    "absent_kp": [
      "developer",
      "locality",
      "computation"
    ]
  },
  {
    "title": "influence of thoracic sympathectomy on cardiac induced oscillations in tissue blood volume.",
    "abstract": "the photoplethysmographic (ppg) signal, which measures cardiac-induced changes in tissue blood volume by light transmission measurements, shows spontaneous fluctuations. in this study, ppg was simultaneously measured in the right and left index fingers of 16 patients undergoing thoracic sympathectomy, and, from each ppg pulse, the amplitude of the pulse (am) and its maximum (bl) were determined. the parameter am/bl is proportional to the cardiac-induced blood volume increase, which depends on the arterial wall compliance. am/bl increased after the thoracic sympathectomy treatment (for male patients, from 2.60 +/- 1.49% to 4.81 +/- 1.21%), as sympathetic denervation decreases arterial tonus in skin. the very low-frequency (vlf) fluctuations of bl or am showed high correlation (0.90 +/- 0.11 and 0.92 +/- 0.07, respectively) between the right and left hands before the thoracic sympathectomy, and a significant decrease in the right-left correlation coefficient (to 0.54 +/- 0.22 and 0.76 +/- 0.20, respectively) after the operation. the standard deviation of the bl or am vlf fluctuations also reduced after the treatment, indicating sympathetic mediation of the vlf ppg fluctuations. the study also shows that the analysis of the ppg signal and the vlf fluctuations of the ppg parameters enable the assessment of the change in sympathetic nervous system activity after thoracic sympathectomy.",
    "present_kp": [
      "sympathetic nervous system",
      "thoracic sympathectomy"
    ],
    "absent_kp": [
      "photoplethysmography",
      "haemodynamics"
    ]
  },
  {
    "title": "a two-stage hybrid flowshop scheduling problem with a function constraint and unrelated alternative machines.",
    "abstract": "this article addresses a two-stage hybrid flowshop scheduling problem with unrelated alternative machines. the problem to be studied has m unrelated alternative machines at the first machine center followed by a second machine center with a common processing machine in the system. the objective is to minimize the makespan of the system. for the processing of any job, it is assumed that the operation can be partially substituted by other machines in the first center, depending on its machining constraints. such scheduling problems occur in certain practical applications such as semiconductors, electronics manufacturing, airplane engine production, and petrochemical production. we demonstrate that the addressed problem is np-hard and then provide some heuristic algorithms to solve the problem efficiently. the experimental results show that the combination of the modified johnson's rule and the first-fit rule provides the best solutions within all proposed heuristics. scope and purpose a survey of the scheduling literature reveals that although a lot of research focusing on fsm1,m2 has been done, no study has taken the functional constraint and unrelated alternative machines cases into account simultaneously. however, such scheduling problems occur in certain practical manufacturing environments such as semiconductors, electronics manufacturing, airplane engine production, and petrochemical production. hence, the purpose of this report is to propose some efficient heuristics for solving the fsm1,m2 problem. for the addressed problem, first, we demonstrate that the problem belongs to an np case; then, 16 heuristics are provided for dealing with the problem. the computational results demonstrate the effectiveness of the heuristics.",
    "present_kp": [
      "hybrid flowshop scheduling",
      "unrelated alternative machines",
      "heuristic algorithm"
    ],
    "absent_kp": []
  },
  {
    "title": "identification via location-profiling in gsm networks.",
    "abstract": "as devices move within a cellular network, they register their new location with cell base stations to allow for the correct forwarding of data. we show it is possible to identify a mobile user from these records and a pre-existing location profile, based on previous movement. two different identification processes are studied, and their performances are evaluated on real cell location traces. the best of those allows for the identification of around 80% of users. we also study the misidentified users and characterise them using hierarchical clustering techniques. our findings highlight the difficulty of anonymizing location data, and firmly establish they are personally identifiable.",
    "present_kp": [
      "identification",
      "location profile",
      "cellular network"
    ],
    "absent_kp": [
      "location privacy"
    ]
  },
  {
    "title": "wireless sensing and identification based on radar cross section variability measurement of passive electromagnetic sensors.",
    "abstract": "in this paper, we present the wireless measurement of various physical quantities from the analysis of the radar cross section variability of passive electromagnetic sensors. the technique uses a millimeter frequency-modulated continuous-wave radar for both remote sensing and wireless identification of sensors. long reading ranges (up to some decameters) are reached at the expense of poor measurement resolution (typically 10%). a review of recent experimental results is reported for illustration purposes.",
    "present_kp": [
      "remote sensing"
    ],
    "absent_kp": [
      "wireless sensor network",
      "passive sensor",
      "backscattering",
      "frequency modulation",
      "fmcw radar",
      "millimeter wave",
      "chipless rfid"
    ]
  },
  {
    "title": "analyzing the capabilities of crowdsourcing services for text summarization.",
    "abstract": "this paper presents a detailed analysis of the use of crowdsourcing services for the text summarization task in the context of the tourist domain. in particular, our aim is to retrieve relevant information about a place or an object pictured in an image in order to provide a short summary which will be of great help for a tourist. for tackling this task, we proposed a broad set of experiments using crowdsourcing services that could be useful as a reference for others who want to rely also on crowdsourcing. from the analysis carried out through our experimental setup and the results obtained, we can conclude that although crowdsourcing services were not good to simply gather gold-standard summaries (i.e., from the results obtained for experiments 1, 2 and 4), the encouraging results obtained in the third and sixth experiments motivate us to strongly believe that they can be successfully employed for finding some patterns of behaviour humans have when generating summaries, and for validating and checking other tasks. furthermore, this analysis serves as a guideline for the types of experiments that might or might not work when using crowdsourcing in the context of text summarization.",
    "present_kp": [
      "text summarization",
      "crowdsourcing services"
    ],
    "absent_kp": [
      "information retrieval",
      "crowdflower",
      "mechanical turk"
    ]
  },
  {
    "title": "robust exponential stability criteria of uncertain stochastic systems with time-varying delays.",
    "abstract": "this article investigates the problem of delay-dependent exponential stability in mean square for continuous-time linear stochastic systems with structured uncertainties and time-varying delays. by applying descriptor model transformation of the systems, a new type of lyapunov-krasovskii functional is constructed, and by introducing some free weighting matrices, some new delay-dependent and delay-independent stability criteria are derived respectively in terms of an lmi algorithm. the new stability criteria are less conservative than existing results. numerical examples demonstrate that the new criteria are effective and are an improvement over existing results.",
    "present_kp": [
      "stochastic systems",
      "exponential stability in mean square",
      "time-varying delays"
    ],
    "absent_kp": [
      "linear matrix inequality"
    ]
  },
  {
    "title": "cellular automata computations and secret key cryptography.",
    "abstract": "in this paper, cellular automata (cas) are used to design a symmetric key cryptography system based on vernam cipher. cas are applied to generate a pseudo-random numbers sequence (pns) which is used during the encryption process. the quality of pnss highly depends on the set of applied ca rules. rules of radius r=1 and 2 for nonuniform one-dimensional cas have been considered. a new set of rules has been discovered using an evolutionary technique called cellular programming. this set provides very high quality encryption, and the system is very resistant to attempts of breaking the cryptography key.",
    "present_kp": [
      "cellular automata",
      "cellular programming",
      "symmetric key cryptography",
      "vernam cipher"
    ],
    "absent_kp": [
      "random number generators"
    ]
  },
  {
    "title": "the complete proof on the optimal ordering policy under cash discount and trade credit.",
    "abstract": "huang ((2005), 'buyer's optimal ordering policy and payment policy under supplier credit', international journal of systems science, 36, 801-807) investigates the buyer's optimal ordering policy and payment policy under supplier credit. his inventory model is correct and interesting. basically, he uses an algebraic method to locate the optimal solution of the annual total relevant cost trc(t) and ignores the role of the functional behaviour of trc(t) in locating the optimal solution of it. however, as argued in this article, huang needs to explore the functional behaviour of trc(t) to justify his solution. so, from the viewpoint of logic, the proof about theorem 1 in huang has some shortcomings such that the validity of theorem 1 in huang is questionable. the main purpose of this article is to remove and correct those shortcomings in huang and present the complete proofs for huang.",
    "present_kp": [
      "inventory",
      "trade credit",
      "cash discount"
    ],
    "absent_kp": [
      "eoq models"
    ]
  },
  {
    "title": "a survey of digital forensic investigator decision processes and measurement of decisions based on enhanced preview.",
    "abstract": "this work focuses on two studies: a survey of digital forensic investigators' investigation and decision processes, and a study of the accuracy of decisions to exclude an exhibit from receiving further in-depth analysis based on an enhanced preview. this study describes the surveyed investigators' generalized investigation process model in terms of the investigators' decisions, including social considerations. it is shown that decision-making based on a less in-depth enhanced preview of exhibits helped to accurately exclude unrelated exhibits before receiving an in-depth analysis while always including exhibits that contained relevant suspect data in child exploitation material investigations.",
    "present_kp": [
      "enhanced preview",
      "child exploitation material investigation"
    ],
    "absent_kp": [
      "digital forensic investigation",
      "investigator decisions",
      "digital investigation processes",
      "law enforcement"
    ]
  },
  {
    "title": "fuzzy arithmetic based on dimension-adaptive sparse grids: a case study of a large-scale finite element model under uncertain parameters.",
    "abstract": "fuzzy arithmetic provides a powerful tool to introduce uncertainty into mathematical models. with zadeh's extension principle, one can obtain a fuzzy-valued extension of any real-valued objective function. an efficient and accurate approach to computing expensive multivariate functions of fuzzy numbers is given by fuzzy arithmetic based on sparse grids. in many cases, not all uncertain input parameters carry equal weight, or the objective model exhibits separable structure. these characteristics can be exploited by dimension-adaptive algorithms. as a result, the treatment of even higher-dimensional problems becomes possible. this is demonstrated in this paper by a case study involving two large-scale finite element models in vibration engineering that are subjected to fuzzy-valued input data.",
    "present_kp": [
      "fuzzy numbers",
      "extension principle",
      "sparse grids"
    ],
    "absent_kp": [
      "computing fuzzy functions",
      "uncertainty modeling"
    ]
  },
  {
    "title": "empowering recommender systems using trust and argumentation.",
    "abstract": "we investigate integration of fuzzy and argumentation based trust in reasoning. as the acceptance of the arguments increases, so does users trust in the system. the approach can be implemented for any recommendation application. it can resolve conflicts in opinions about a product and improve recommendations. the proposed approach generates more accurate, relevant and novel recommendations.",
    "present_kp": [
      "recommendation",
      "trust",
      "argumentation",
      "reasoning"
    ],
    "absent_kp": [
      "intelligent agents",
      "fuzzy logic"
    ]
  },
  {
    "title": "a new approach to fuzzy regression models with application to business cycle analysis.",
    "abstract": "recently, fuzzy regression analysis has been largely applied in the modeling of economic or financial data. however, those data often exhibit certain kinds of linguistic terms, for instance: very good, a little reclining or stable, in the business cycle or the growth rate of gdp, etc. the goal of this paper is to construct a fuzzy regression model by fuzzy parameters estimation using the fuzzy samples. it deals with imprecise measurement of observed variables, fuzzy least square estimation and nonparametric methods. this is different from the assumptions as well as the estimation techniques of the classical analysis. empirical results demonstrate that our new approach is efficient and more realistic than the traditional regression analysis.",
    "present_kp": [
      "fuzzy regression",
      "fuzzy parameter"
    ],
    "absent_kp": [
      "triangular membership function",
      "h-cut",
      "methods of least square"
    ]
  },
  {
    "title": "design and optimization of distributed sensing coverage in wireless sensor networks.",
    "abstract": "for many sensor network applications, such as military surveillance, it is necessary to provide full sensing coverage to a security-sensitive area while, at the same time, minimizing energy consumption and extending system lifetime by leveraging the redundant deployment of sensor nodes. in this paper, we propose a surveillance service for sensor networks based on a distributed energy-efficient sensing coverage protocol. in the protocol, each node is able to dynamically decide a schedule for itself to guarantee a certain degree-of-coverage (doc) with average energy consumption inversely proportional to the node density. several optimizations and extensions are proposed to enhance the basic design with a better load-balance feature and a longer network lifetime. we consider and address the impact of the target size and the unbalanced initial energy capacity of individual nodes to the network lifetime. several practical issues such as the localization error, irregular sensing range, and unreliable communication links are addressed as well. simulation shows that our protocol extends system lift-time significantly with low energy consumption. it outperforms other state-of-the-art schemes by as much as 50% reduction in energy consumption and as much as 130% increase in the half- life of the network.",
    "present_kp": [
      "design",
      "sensor networks",
      "sensing coverage"
    ],
    "absent_kp": [
      "algorithms",
      "performance",
      "energy conservation"
    ]
  },
  {
    "title": "natural language processing technologies for developing a language learning environment.",
    "abstract": "so far, computer-assisted language learning (call) comes in many different flavors. our research work focuses on developing an integrated e-learning environment that allows improving language skills in specific contexts. integrated e-learning environment means that it is a web-based solution that performs language learning tasks using common working environments like, for instance, web browsers or email clients. it should be accessible on different platforms, even on mobile devices. natural language processing (nlp) forms the technological basis for developing such a learning framework. the paper gives an overview of the state-of-the-art in this area. therefore, on the one hand, it explains creation processes for nlp resources and gives an overview of corpora. on the other hand, it describes existing nlp standards. based on our requirements, the paper gives special attention to the evaluation and comparison of toolkits that can suitably support the planned implementation. an outlook at the end points out necessary developments in e-learning to keep in mind.",
    "present_kp": [
      "computer-assisted language learning",
      "natural language processing",
      "integrated e-learning"
    ],
    "absent_kp": [
      "semantic web technologies",
      "learning of foreign languages"
    ]
  },
  {
    "title": "kinematic synthesis of a four-link mechanism with rolling contacts for motion and function generation.",
    "abstract": "in this paper, the kinematic synthesis of a four-link mechanism with rolling contacts is investigated. this mechanism comprises a two-fingered gripper and a grasped object. the synthesis equations used for motion generation and function generation are established. the number of free choices in design variables for the kinematic synthesis is also discussed. furthermore, the optimization-based numerical technique is applied to solve the design equations. the optimized solutions are illustrated to discuss the kinematic states of the mechanism. it is also shown that the optimization-based method is effective in finding the admissible synthesis solution of the mechanism.",
    "present_kp": [
      "rolling contacts",
      "motion generation",
      "function generation",
      "optimization"
    ],
    "absent_kp": []
  },
  {
    "title": "designing online auctions with past performance information.",
    "abstract": "we investigate the value of past performance information in the context of keyword advertising auctions, where advertisers differ both in valuation-per-click and in the numbers of clicks they can generate (their performance). we focus on weighted unit-price-contract (upc) auctions, in which bidders bid unit prices and pay accordingly if they win, and their bids are weighted by factors based on their own past performance. we characterize the efficient and the revenue-maximizing weighting factors and apply our framework to study yahoo!'s and google's auction designs, each of which can be viewed as a special case of weighted upc auctions.",
    "present_kp": [
      "upc auctions",
      "google",
      "keyword advertising"
    ],
    "absent_kp": [
      "search engine",
      "multi-dimensional values"
    ]
  },
  {
    "title": "a tuplespace event model for mashups.",
    "abstract": "inter-widget communication is essential for enterprise mashup applications. to implement it, current mashup platforms use the publish/subscribe pattern. however, the way publish/subscribe is used in these platforms requires a lot of manual wiring between widgets. in this paper, we propose a new unified widget event model (uwem), which is conceptually an extension of linda tuplespaces. uwem separates event publishers and subscribers in space, time, and reference. using the keyboard-level model (klm) we show that uwem requires fewer operations to build typical mashups than conventional mashup platforms. we have implemented uwem in a popular enterprise mashup framework, and performed an empirical study that compares uwem with the established approach for creating mashups. the study confirms the klm predictions, and shows that uwem is significantly more efficient than the established approach.",
    "present_kp": [
      "widget",
      "tuplespace",
      "mashup",
      "event model"
    ],
    "absent_kp": []
  },
  {
    "title": "a class of bottleneck expansion problems.",
    "abstract": "in this paper we consider how to increase the capacities of the elements in a set e efficiently so that the capacity of a given family of subsets of e can be increased to the maximum extent while the total cost for the increment of capacity is within a given budget bound. we transform this problem into finding the minimum weight element of when the weight of each element of e is a linear function of a single parameter and propose an algorithm for solving this problem. we further discuss the problem for some special cases. especially, when e is the edge set of a network and the family consists of all spanning trees, we give a strongly polynomial algorithm. there are many capacity expansion problems in real life. for example, a transportation network may need to increase the flow capacity, and a production system may need to enhance its productive ability. it is quite often that the capacity of a scheme is decided by its bottleneck capacity, i.e. the capacity of the weakest part, and people need to choose from all possible schemes the one with the largest bottleneck capacity. mathematically, this is a maxmin problem. capacity expansion is unavoidably related to the available budget. so, we need to consider how to use the limited budget to expand the bottleneck capacity to the largest extent. the purpose of the paper is to give a general method for a class of bottleneck expansion problems which is easy to use and obtains solutions in polynomial time.",
    "present_kp": [
      "polynomial algorithm"
    ],
    "absent_kp": [
      "bottleneck problem",
      "inverse optimization problem",
      "minimum-weight element problem",
      "parametric linear programming"
    ]
  },
  {
    "title": "sample-based estimation of correlation ratio with polynomial approximation.",
    "abstract": "sensitivity analysis has become a natural step in the uncertainty analysis framework. as there is no general sensitivity measure that would capture all information on impact of input factors on model output, analysts tend to combine various measures to obtain a broader image of interactions between different modes. this article concentrates on the correlation ratio, demonstrates methods for calculating this quantity efficiently and accurately, and compares the results. a new method inspired by artificial intelligence techniques emerges as outperforming the familiar methods.",
    "present_kp": [
      "correlation ratio",
      "sensitivity analysis"
    ],
    "absent_kp": [
      "sobol indices"
    ]
  },
  {
    "title": "a new fair non-repudiation protocol for secure negotiation and contract signing.",
    "abstract": "the participation of an e-notary, acting as an on-line trusted third party is required in some scenarios, such as business to business, intellectual property rights contracting, or even as a legal requirement, in contract signing is frequently necessary. this e-notary gives validity to the contract or performs some tasks related to the contract, e. g. contract registration. in the abovementioned contracting scenarios, two important additional features are needed: the negotiation of the e-contract and confidentiality. however, until now, e-contract signing protocols have not considered these issues as an essential part of the protocol. in this paper, we present a new protocol which is designed to make negotiation and contract signing processes secure and confidential. moreover, compared to other previous proposals based on an on-line trusted third party, this protocol reduces the e-notary's workload. finally, we describe how the protocol is being used to achieve agreements on the rights of copyrighted works.",
    "present_kp": [
      "contract signing protocol",
      "secure negotiation",
      "confidentiality"
    ],
    "absent_kp": [
      "fair exchange",
      "intellectual property rights contracts"
    ]
  },
  {
    "title": "simulation study of the cardiovascular functional status in hypertensive situation.",
    "abstract": "an extended cardiovascular model was established based on our previous work to study the consequences of physiological or pathological changes to the homeostatic functions of the cardiovascular system. to study hemodynamic changes in hypertensive situations, the impacts of cardiovascular parameter variations (peripheral vascular resistance, arterial vessel wall stiffness and baroreflex gain) upon hemodynamics and the short-term regulation of the cardiovascular system were investigated. for the purpose of analyzing baroregulation function, the short-term regulation of arterial pressure in response to moderate dynamic exercise for normotensive and hypertensive cases was studied through computer simulation and clinical experiments. the simulation results agree well with clinical data. the results of this work suggest that the model presented in this paper provides a useful tool to investigate the functional status of cardiovascular system in normal or pathological conditions.",
    "present_kp": [
      "cardiovascular system",
      "hemodynamics",
      "baroregulation"
    ],
    "absent_kp": [
      "pulse waveform",
      "hypertension"
    ]
  },
  {
    "title": "enhanced hierarchical classification via isotonic smoothing.",
    "abstract": "hierarchical topic taxonomies have proliferated on the world wide web , and exploiting the output space decompositions they induce in automated classification systems is an active area of research. in many domains, classifiers learned on a hierarchy of classes have been shown to outperform those learned on a flat set of classes. in this paper we argue that the hierarchical arrangement of classes leads to intuitive relationships between the corresponding classifiers' output scores, and that enforcing these relationships as a post-processing step after classification can improve its accuracy. we formulate the task of smoothing classifier outputs as a regularized isotonic tree regression problem, and present a dynamic programming based method that solves it optimally. this new problem generalizes the classic isotonic tree regression problem, and both, the new formulation and algorithm, might be of independent interest. in our empirical analysis of two real-world text classification scenarios, we show that our approach to smoothing classifier outputs results in improved classification accuracy.",
    "present_kp": [
      "hierarchical classification",
      "dynamic programming"
    ],
    "absent_kp": [
      "regularized isotonic regression",
      "taxonomy"
    ]
  },
  {
    "title": "power-saving mechanisms for energy efficient ieee 802.16e/m.",
    "abstract": "efforts to reduce co2 emission regarded as a main reason of the green house effect are widely performed in information and communications technology (ict) industry. in particular, as fast growing mobile communications services consume more energy, there are wide efforts to increase energy efficiency in the area of mobile station (ms), radio base stations (bs), network controllers, and core networks. user's concern, however, is more focused on optimization of energy efficiency in ms with limited battery capacity, because ms consumes much energy for wide broadband data services in data-centric communications services with 4g technology rather than legacy voice-centric communications services. the key idea of power saving mechanism (psm) in ms is to operate sleep-mode that the ms turns down main elements when there is no data to receive/transmit in order to save battery power. with legacy voice-centric communication services, traffic pattern is rather simple and well-known so that the sleep-mode of psm is well operated. however, in 4g technology for various data-centric services power saving mechanism should be adaptive to changing traffic condition in order to achieve optimal energy efficiency. thus, holistic perspective for power saving techniques is needed in consideration of characteristics of services and qos constraints, multiple applications support, remained battery power, handover process, multicast broadcast service (mbs) support, and so on. this paper surveys various power saving mechanisms proposed for ieee 802.16e/m. we first present the basic operating mechanism of psm and research issues for performance enhancement. based on their limitations and potentials, we then review several proposals of psm for ieee 802.16m, an evolutional technology of ieee 802.16e. it will be interesting for readers to observe that once psm was standardized in ieee 802.16e, many new ideas have been proposed to improve the performance of the original psm, and some of them are survived and adopted to new standard technique in the advanced ieee 802.16m.",
    "present_kp": [
      "energy efficiency",
      "ieee 802.16e/m",
      "power-saving mechanism",
      "sleep-mode"
    ],
    "absent_kp": []
  },
  {
    "title": "crime hotspot mapping using the crime related factors-a spatial data mining approach.",
    "abstract": "the technique of hotspot mapping is widely used in analysing the spatial characteristics of crimes. the spatial distribution of crime is considered to be related with a variety of socio-economic and crime opportunity factors. but existing methods usually focus on the target crime density as input without utilizing these related factors. in this study, we introduce a new crime hotspot mapping tool-hotspot optimization tool (hot). hot is an application of spatial data miming to the field of hotspot mapping. the key component of hot is the geospatial discriminative patterns (gdpatterns) concept, which can capture the differences between two classes in a spatial dataset. experiments are done using a real world dataset from a northeastern city in the united states and the pros and cons of utilizing related factors in hotspot mapping are discussed. comparison studies with the hot spot analysis tool implemented by esri arcmap 10.1 validate that hot is capable of accurately mapping crime hotspots.",
    "present_kp": [
      "crime hotspot",
      "hotspot optimization tool",
      "spatial data mining",
      "geospatial discriminative pattern"
    ],
    "absent_kp": []
  },
  {
    "title": "charge transport in high-? stacks for charge-trapping memory applications: a modeling perspective (invited).",
    "abstract": "charge trapping (ct) memories could be a promising technology option for further nand flash scaling. the assessment of the scalability limits and ultimate performances of this technology demands for the comprehensive understanding of the physical mechanisms governing device operation and reliability, which requires accurate physics-based models reproducing the electrical device characteristics. the basic features of the models presented in the literature for ct memory devices are reviewed, underlining their similarities and differences, and highlighting their importance in order to achieve a comprehensive understanding of the physical mechanisms responsible for ct device operation and reliability. a physical model describing the charge transport in nitride and high-? stacks is also presented, which allows gaining further insights into reliability issues related to charge localization and high-? tunnel and blocking dielectrics, like the effects of the blocking alumina layer and the band-gap engineered tunnel dielectrics on the tanos device retention.",
    "present_kp": [
      "reliability",
      "tanos"
    ],
    "absent_kp": [
      "charge-trapping devices",
      "high-? dielectrics",
      "device physics",
      "device modeling"
    ]
  },
  {
    "title": "equilibria of perceptrons for simple contingency problems.",
    "abstract": "the contingency between cues and outcomes is fundamentally important to theories of causal reasoning and to theories of associative learning. researchers have computed the equilibria of rescorla-wagner models for a variety of contingency problems, and have used these equilibria to identify situations in which the rescorla-wagner model is consistent, or inconsistent, with normative models of contingency. mathematical analyses that directly compare artificial neural networks to contingency theory have not been performed, because of the assumed equivalence between the rescorla-wagner learning rule and the delta rule training of artificial neural networks. however, recent results indicate that this equivalence is not as straightforward as typically assumed, suggesting a strong need for mathematical accounts of how networks deal with contingency problems. one such analysis is presented here, where it is proven that the structure of the equilibrium for a simple network trained on a basic contingency problem is quite different from the structure of the equilibrium for a rescorla-wagner model faced with the same problem. however, these structural differences lead to functionally equivalent behavior. the implications of this result for the relationships between associative learning, contingency theory, and connectionism are discussed.",
    "present_kp": [
      "artificial neural networks",
      "associative learning",
      "contingency"
    ],
    "absent_kp": []
  },
  {
    "title": "heuristics for dependency conjectures in proteomic signaling pathways.",
    "abstract": "a key issue in the study of protein signaling networks is understanding the relationships among proteins in the network. understanding these relationships in the context of a network is one of the major challenges for modern biology . in the laboratory a time series of protein modification measurements is taken in order that relationships among the activations can be conjectured. laubenbacher and stigler have developed an algorithm to make conjectures concerning gene expression. their algorithm analyses the relations as variables in polynomials, using techniques based in computational algebra. this paper focuses on heuristics for applying their method to conjecture dependencies between proteins in signal transduction networks.",
    "present_kp": [
      "heuristics",
      "signal transduction"
    ],
    "absent_kp": [
      "glycolysis regulation",
      "proteomics",
      "computational biology"
    ]
  },
  {
    "title": "using model checking with symbolic execution to verify parallel numerical programs.",
    "abstract": "we present a method to verify the correctness of parallel programs that perform complex numerical computations, including computations involving floating-point arithmetic. the method requires that a sequential version of the program be provided, to serve as the specification for the parallel one. the key idea is to use model checking, together with symbolic execution, to establish the equivalence of the two programs.",
    "present_kp": [
      "program",
      "use",
      "method",
      "parallel",
      "correctness",
      "numerical program",
      "version",
      "symbolic execution",
      "floating-point",
      "model checking"
    ],
    "absent_kp": [
      "parallel programming",
      "spin",
      "high performance computing",
      "message passing interface",
      "mpi",
      "finite state verification",
      "complexity",
      "concurrency"
    ]
  },
  {
    "title": "comprehensive study of mixed eccentricity fault diagnosis in induction motors using signature analysis.",
    "abstract": "modeling and simulation studies of an induction motor always help in identifying the parameter to characterize the asymmetrical fault in the machine. hence in this paper, an air gap eccentric induction motor is modeled using multiple coupled circuit approach and 2d-modified winding function theory. the machine model is simulated under different eccentricity conditions to obtain the motor current spectra, power spectra and power factor spectra to detect the eccentricity related frequency components and the results are compared. all these analysis are based on the variation in the amplitude of mixed eccentricity related frequency component in these parameters with the variation in the level of eccentricity in the machine. a new fault severity detection method based on co-variance analysis is presented to predict the degree of deterioration in the health of the machine due to air gap eccentricity from the installation stage.",
    "present_kp": [
      "modeling",
      "modified winding function theory",
      "simulation"
    ],
    "absent_kp": [
      "eigen value",
      "motor current signature analysis ",
      "psd analysis"
    ]
  },
  {
    "title": "predicting the volume of comments on online news stories.",
    "abstract": "on-line news agents provide commenting facilities for readers to express their views with regard to news stories. the number of user supplied comments on a news article may be indicative of its importance or impact. we report on exploratory work that predicts the comment volume of news articles prior to publication using five feature sets. we address the prediction task as a two stage classification task: a binary classification identifies articles with the potential to receive comments, and a second binary classification receives the output from the first step to label articles \"low\" or \"high\" comment volume. the results show solid performance for the former task, while performance degrades for the latter.",
    "present_kp": [
      "prediction",
      "comment volume"
    ],
    "absent_kp": [
      "feature engineering"
    ]
  },
  {
    "title": "providing time- and space- efficient procedure calls for asynchronous software thread integration.",
    "abstract": "asynchronous software thread integration (asti) provides fine-grain concurrency in real-time threads by statically scheduling (integrating) code from primary threads into secondary threads, reducing the context switching needed and allowing recovery of fine-grain idle time. unlike sti, asti allows asynchronous thread progress.current asti techniques do not support procedure calls in the secondary thread because they lead to timing conflicts during static scheduling. asti requires knowing the secondary thread's instruction execution schedule to guide placement of real-time instructions from the primary thread. a secondary thread procedure called from multiple sites will have ambiguous timing at compile time.in this paper we remove this constraint using both procedure inlining and cloning. we present a formal approach to choosing a subset of calls to inline and to remove the timing conflicts in the remaining call sites in an efficient fashion. excessive inlining and cloning both lead to code size explosion, while poor choices in timing conflict elimination slow program execution. the cloned threads show a significant speedup when compared to non-cloned versions yet have low code expansion. the techniques presented here have been implemented in our post-pass compiler thrint and demonstrated on a benchmark suite of secondary threads representative of low-end embedded systems.",
    "present_kp": [
      "fine-grain concurrency"
    ],
    "absent_kp": [
      "hardware to software migration"
    ]
  },
  {
    "title": "a hybrid speech emotion recognition system based on spectral and prosodic features.",
    "abstract": "in this paper, we present a hybrid speech emotion recognition system exploiting both spectral and prosodic features in speech. for capturing the emotional information in the spectral domain, we propose a new spectral feature extraction method by applying a novel non-uniform subband processing, instead of the mel-frequency subbands used in mel-frequency cepstral coefficients (mfcc). for prosodic features, a set of features that are closely correlated with speech emotional states are selected. in the proposed hybrid emotion recognition system, due to the inherently different characteristics of these two kinds of features (e.g., data size), the newly extracted spectral features are modeled by gaussian mixture model (gmm) and the selected prosodic features are modeled by support vector machine (svm). the final result of the proposed emotion recognition system is obtained by combining the results from these two subsystems. experimental results show that (1) the proposed non-uniform spectral features are more effective than the traditional mfcc features for emotion recognition; (2) the proposed hybrid emotion recognition system using both spectral and prosodic features yields the relative recognition error reduction rate of 17.0% over the traditional recognition systems using only the spectral features, and 62.3% over those using only the prosodic features.",
    "present_kp": [
      "speech emotion recognition",
      "non-uniform subband processing",
      "spectral feature",
      "prosodic feature"
    ],
    "absent_kp": []
  },
  {
    "title": "finding multiple global optima for unconstrained discrete location problems.",
    "abstract": "we consider a class of problems where a given number p of facility locations must be selected from a set of s potential locations so as to optimize a pre-determined fitness function. there exist outstanding location problems in this class, such as the p-median, max-covering, maxcap, and maxprofit problems. these location problems may contain more than a single global optimal solution. obtaining multiple global optimal solutions allows us to consider other characteristics in the process of selecting the preferred solution. in , a new 'multimodal' algorithm (gasub) was presented for solving the previously mentioned location problems. in the paper, the algorithm was fine-tuned to obtain a single global optimum (with 100% success) as fast as possible, and it was compared with two widely used techniques, that is, the standard optimizer xpress-mp and the multi start heuristic. in this paper, we propose a new set of parameter values so that gasub can explore the space more deeply and consequently find several alternative global optimal solutions. moreover, two coarse-grain parallelizations of gasub are presented, cggasub and cggasub_mo. cggasub is able to reduce the computational time of gasub where solutions of the same quality are generated, while cggasub_mo finds more alternative global optima than the sequential version and spending similar computational time in the process. gasub, with the new parameter setting and also the parallel algorithms, will be evaluated using a comprehensive computational set of experiments. additionally, some criteria can also be used to select one of the known global optimal solutions when several alternatives optimize the corresponding objective function.",
    "present_kp": [
      "discrete location"
    ],
    "absent_kp": [
      "global optimization",
      "evolutionary computing",
      "multimodal genetic algorithms"
    ]
  },
  {
    "title": "a theoretical study on the hydrolysis mechanism of carbon disulfide.",
    "abstract": "the hydrolysis mechanism of cs2 was studied using density functional theory. by analyzing the structures of the reactant, transition states, intermediates, and products, it can be concluded that the hydrolysis of cs2 occurs via two mechanisms, one of which is a two-step mechanism (cs2 first reacts with an h2o, leading to the formation of the intermediate cos, then cos reacts with another h2o, resulting in the formation of h2s and co2). the other is a one-step mechanism, where cs2 reacts with two h2o molecules continuously, leading to the formation of h2s and co2. by analyzing the thermodynamics and the change in the kinetic function, it can be concluded that the rate-determining step involves h and oh in h2o attacking s and c in cs2, respectively, causing the c=s double bond to change into a single bond. the two mechanisms are competitive. when performing the hydrolysis of cs2 with a catalyst, the optimal temperature is below 252 degrees c.",
    "present_kp": [
      "carbon disulfide",
      "hydrolysis mechanism"
    ],
    "absent_kp": [
      "dft",
      "quantum chemistry calculation"
    ]
  },
  {
    "title": "the development and initial testing of the internet consequences scales (icons).",
    "abstract": "the internet has become a tool for everyday use in the lives of many people; however, little is known about the consequences of using the internet on the well-being of individuals. this article describes the development of the internet consequences scale (icons), a tool to measure the physical, behavioral, economic, and psychosocial consequences of internet use. content validity was established using a panel of experts in internet communications, and construct validity was established using a confirmatory factor analysis. reliability of the icons was established statistically using cronbach's alpha. the result was a 44-item tool containing four subscales to measure the consequences of internet use.",
    "present_kp": [
      "consequences",
      "internet consequences scale",
      "internet use"
    ],
    "absent_kp": [
      "instrument development"
    ]
  },
  {
    "title": "image contrast enhancement by global and local adjustment of gray levels.",
    "abstract": "various contrast enhancement methods such as histogram equalization (he) and local contrast enhancement (lce) have been developed to increase the visibility and details of a degraded image. we propose an image contrast enhancement method based on the global and local adjustment of gray levels by combining he with lce methods. for the optimal combination of both, we introduce a discrete entropy. evaluation of our experimental results shows that the proposed method outperforms both the he and lce methods.",
    "present_kp": [
      "contrast enhancement",
      "histogram equalization",
      "local contrast enhancement"
    ],
    "absent_kp": []
  },
  {
    "title": "fuzzy control of an underactuated robot with a fuzzy microcontroller.",
    "abstract": "in this work the position control of a planar underactuated manipulator with two revolute joints is considered. a dynamic model of the system is presented and a fuzzy control strategy is proposed. fuzzy logic allows empirical rules to be translated into a control algorithm. a fuzzy microcontroller is adopted for the practical implementation of the system. the results of several experiments are presented and discussed.",
    "present_kp": [
      "fuzzy control",
      "fuzzy microcontroller"
    ],
    "absent_kp": [
      "underactuated robots",
      "non-holonomic systems",
      "nonlinear systems control"
    ]
  },
  {
    "title": "a case study in embedded systems design: an engine control unit.",
    "abstract": "a number of techniques and software tools for embedded system design have been recently proposed. however, the current practice in the designer community is heavily based on manual techniques and on past experience rather than on a rigorous approach to design. to advance the state of the art it is important to address a number of relevant design problems and solve them to demonstrate the power of the new approaches. we chose an industrial example in automotive electronics to validate our design methodology: an existing commercially available engine control unit. we discuss in detail the specification, the implementation philosophy, and the architectural trade-off analysis. we analyze the results obtained with our approach and compare them with the existing design underlining the advantages offered by a systematic approach to embedded system design in terms of performance and design time.",
    "present_kp": [
      "engine control"
    ],
    "absent_kp": [
      "hardware/software co-design",
      "hardware/software co-simulation",
      "architecture selection and mapping",
      "automotive applications"
    ]
  },
  {
    "title": "a study of the effect of term proximity on query expansion.",
    "abstract": "query expansion terms are often used to enhance original query formulations in document retrieval. such terms are usually selected from the entire documents or from windows or passages surrounding query term occurrences. arguably, the semantic relatedness between terms weakens with the increase in the distance separating them. in this paper we report a study that was conducted to systematically evaluate different distance functions for selecting query expansion terms. we propose a distance factor that can be effectively combined with the statistical term association measure of mutual information for selecting query expansion terms. evaluation of the trec collection shows that distance-weighted mutual information is more effective than mutual information alone in selecting terms for query expansion.",
    "present_kp": [
      "query expansion",
      "term proximity",
      "mutual information"
    ],
    "absent_kp": [
      "information retrieval",
      "word collocation"
    ]
  },
  {
    "title": "a seva soft sensor method based on self-calibration model and uncertainty description algorithm.",
    "abstract": "soft sensors are widely used to estimate process variables that are difficult to measure online. however, due to poor quality of input data and deterioration of prediction model as time passes, soft sensors make poor performance. we have been constructing a principal component analysis (pca) model before performing a prediction. furthermore, the just-in-time (jit) learning model has been improved and served as prediction model for self validating (seva) soft sensors. the proposed soft sensor not only carries out internal quality assessment but also generates multiple types of output data, including the prediction values (pv), input sensor status (iss), validated measurement (vm), output sensor status (oss) and the uncertainty values (uv) which represent the credibility of soft sensors' output. the effectiveness of the proposed seva soft sensors is demonstrated through a case study of a wastewater treatment process.",
    "present_kp": [
      "seva",
      "soft sensor",
      "uncertainty",
      "wastewater"
    ],
    "absent_kp": [
      "just-in-time learning",
      "ensemble learning"
    ]
  },
  {
    "title": "compact and adaptive spatial pyramids for scene recognition.",
    "abstract": "most successful approaches on scene recognition tend to efficiently combine global image features with spatial local appearance and shape cues. on the other hand, less attention has been devoted for studying spatial texture features within scenes. our method is based on the insight that scenes can be seen as a composition of micro-texture patterns. this paper analyzes the role of texture along with its spatial layout for scene recognition. however, one main drawback of the resulting spatial representation is its huge dimensionality. hence, we propose a technique that addresses this problem by presenting a compact spatial pyramid (sp) representation. the basis of our compact representation, namely, compact adaptive spatial pyramid (casp) consists of a two-stages compression strategy. this strategy is based on the agglomerative information bottleneck (aib) theory for (i) compressing the least informative sp features, and, (ii) automatically learning the most appropriate shape for each category. our method exceeds the state-of-the-art results on several challenging scene recognition data sets.",
    "present_kp": [
      "scene recognition",
      "spatial pyramids",
      "texture"
    ],
    "absent_kp": [
      "dimensionality reduction",
      "agglomerative information theory"
    ]
  },
  {
    "title": "design of on-line algorithms using hitting times.",
    "abstract": "random walks are well known for playing a crucial role in the design of randomized off-line as well as on-line algorithms. in this work we prove some basic identities for ergodic markov chains (e.g., an interesting characterization of reversibility in markov chains is obtained in terms of first passage times). besides providing new insight into random walks on weighted graphs, we show how these identities give us a way of designing competitive randomized on-line algorithms for certain well-known problems.",
    "present_kp": [
      "random walk",
      "graph",
      "markov chain",
      "reversibility",
      "first passage time"
    ],
    "absent_kp": [
      "competitive ratio",
      "m-matrices"
    ]
  },
  {
    "title": "dynamic background subtraction based on local dependency histogram.",
    "abstract": "traditional background subtraction methods perform poorly when scenes contain dynamic backgrounds such as waving tree branches, spouting fountain, illumination changes, camera jitters, etc. in this paper, from the view of spatial context, we present a novel and effective dynamic background method with three contributions. first, we present a novel local dependency descriptor, called local dependency histogram (ldh), to effectively model the spatial dependencies between a pixel and its neighboring pixels. the spatial dependencies contain substantial evidence for differentiating dynamic background regions from moving objects of interest. second, based on the proposed ldh, an effective approach to dynamic background subtraction is proposed, in which each pixel is modeled as a group of weighted ldhs. labeling a pixel as foreground or background is done by comparing the ldh computed in current frame against its model ldhs. the model ldhs are adaptively updated by the current ldh. finally, unlike traditional approaches using a fixed threshold to judge whether a pixel matches to its model, an adaptive thresholding technique is also proposed. experimental results on a diverse set of dynamic scenes validate that the proposed method significantly outperforms traditional methods for dynamic background subtraction.",
    "present_kp": [
      "dynamic background",
      "background subtraction",
      "spatial dependencies",
      "adaptive thresholding"
    ],
    "absent_kp": [
      "moving object detection"
    ]
  },
  {
    "title": "segmentation of kidney from ultrasound images based on texture and shape priors.",
    "abstract": "this paper presents a novel texture and shape priors based method for kidney segmentation in ultrasound (us) images. texture features are extracted by applying a bank of gabor filters on test images through a two-sided convolution strategy. the texture model is constructed via estimating the parameters of a set of mixtures of half-planed gaussians using the expectation-maximization method. through this texture model, the texture similarities of areas around the segmenting curve are measured in the inside and outside regions, respectively. we also present an iterative segmentation framework to combine the texture measures into the parametric shape model proposed by leventon and faugeras. segmentation is implemented by calculating the parameters of the shape model to minimize a novel energy function. the goal of this energy function is to partition the test image into two regions, the inside one with high texture similarity and low texture variance, and the outside one with high texture variance. the effectiveness of this method is demonstrated through experimental results on both natural images and us data compared with other image segmentation methods and manual segmentation.",
    "present_kp": [
      "image segmentation",
      "kidney segmentation",
      "texture and shape prior"
    ],
    "absent_kp": [
      "ultrasound image processing"
    ]
  },
  {
    "title": "data consistency for p2p collaborative editing.",
    "abstract": "peer-to-peer (p2p) networks are very efficient for distributing content. we want to use this potential to allow not only distribution but collaborative editing of this content. existing collaborative editing systems are centralised or depend on the number of sites. such systems cannot scale when deployed on p2p networks. in this paper, we propose a new model for building a collaborative editing system. this model is fully decentralised and does not depend on the number of sites.",
    "present_kp": [
      "collaborative editing"
    ],
    "absent_kp": [
      "concurrency control",
      "cscw",
      "optimistic replication"
    ]
  },
  {
    "title": "extension of the vikor method for group decision making with interval-valued intuitionistic fuzzy information.",
    "abstract": "the aim of this paper is to extend the vikor method for multiple attribute group decision making in interval-valued intuitionistic fuzzy environment, in which all the preference information provided by the decision-makers is presented as interval-valued intuitionistic fuzzy decision matrices where each of the elements is characterized by interval-valued intuitionistic fuzzy number, and the information about attribute weights is partially known, which is an important research field in decision science and operation research. first, we use the interval-valued intuitionistic fuzzy hybrid geometric operator to aggregate all individual interval-valued intuitionistic fuzzy decision matrices provided by the decision-makers into the collective interval-valued intuitionistic fuzzy decision matrix, and then we use the score function to calculate the score of each attribute value and construct the score matrix of the collective interval-valued intuitionistic fuzzy decision matrix. from the score matrix and the given attribute weight information, we establish an optimization model to determine the weights of attributes, and then determine the interval-valued intuitionistic positive-ideal solution and interval-valued intuitionistic negative-ideal solution. we use the different distances to calculate the particular measure of closeness of each alternative to the interval-valued intuitionistic positive-ideal solution. according to values of the particular measure, we rank the alternatives and then select the most desirable one(s). finally, a numerical example is used to illustrate the applicability of the proposed approach.",
    "present_kp": [
      "multiple attribute group decision making ",
      "vikor method"
    ],
    "absent_kp": [
      "interval-valued intuitionistic fuzzy number "
    ]
  },
  {
    "title": "multiobjective regression modifications for collinearity.",
    "abstract": "in this work we develop a new multivariate technique to produce regressions with interpretable coefficients that are close to and of the same signs as the pairwise regression coefficients. using a multiobjective approach to incorporate multiple and pairwise regressions into one objective we reduce this technique to an eigenproblem that represents a hybrid between regression and principal component analyses. we show that our approach corresponds to a specific scheme of ridge regression with a total matrix added to the matrix of correlations. one of the main goals of multiple regression modeling is to assess the importance of predictor variables in determining the prediction. however, in practical applications inference about the coefficients of regression can be difficult because real data is correlated and multicollinearity causes instability in the coefficients. in this paper we present a new technique to create a regression model that maintains the interpretability of the coefficients. we show with real data that it is possible to generate a model with coefficients that are similar to easily interpretable pairwise relations of predictors with the dependent variable, and this model is similar to the regular multiple regression model in predictive ability.",
    "present_kp": [
      "multicollinearity",
      "ridge regression"
    ],
    "absent_kp": [
      "multiobjective optimization",
      "principal component analysis",
      "net effects"
    ]
  },
  {
    "title": "efficient block-wise temporally consistent contour extraction in image sequences.",
    "abstract": "in this paper we investigate the problem of temporally consistent consecutive contour extraction in image sequences, including both single and multiple boundaries. by formulating this problem in the form of an optimal surface detection in 3d volume, we are able to resort to a graph-theoretic approach for exact solution. in order to cope with the high computational complexity caused by the potential unboundedness in time (i.e., an image sequence can be arbitrarily long) and heavy noise, we propose three approximate block-wise variants to accelerate the solution process. the effectiveness and efficiency of our approach is exemplarily demonstrated on simulated data and real ultrasound data for arterial wall detection. it is shown that the approximate variants dramatically reduce the computation time without loss of solution quality.",
    "present_kp": [
      "contour extraction"
    ],
    "absent_kp": [
      "temporal consistency",
      "graph search",
      "block-wise approximation"
    ]
  },
  {
    "title": "terminating constraint set satisfiability and simplification algorithms for context-dependent overloading.",
    "abstract": "algorithms for constraint set satisfiability and simplification of haskell type class constraints are used during type inference in order to allow the inference of more accurate types and to detect ambiguity. unfortunately, both constraint set satisfiability and simplification are in general undecidable, and the use of these algorithms may cause non-termination of type inference. this paper presents algorithms for these problems that terminate on any given input, based on the use of a criterion that is tested on each recursive step. the use of this criterion eliminates the need of imposing syntactic conditions on haskell type class and instance declarations in order to guarantee termination of type inference in the presence of multi-parameter type classes, and allows program compilation without the need of compiler flags for lifting such restrictions. undecidability of the problems implies the existence of instances for which the algorithm incorrectly reports unsatisfiability, but we are not aware of any practical example where this occurs.",
    "present_kp": [
      "haskell",
      "constraint set satisfiability",
      "termination"
    ],
    "absent_kp": [
      "constraint set simplification"
    ]
  },
  {
    "title": "a new quadrilateral shell element using 16 degrees of freedom.",
    "abstract": "purpose - the purpose of this paper is to present a quadrilateral shell element using 16 degrees offreedom (dof) (12 translations and four rotations) which makes a pair with morley's triangle at 12 dof this latter has been updated by batoz who later proposed an extension to a quadrilateral (\"dkq16\") but only with special interpolation functions for an elastic behaviour of the material. precisely, it is in order to release from this strong limitation that a completely different formulation is proposed here. design/methodology/approach - the development of this new quadrilateral called \"dks16\" involves three stages. the first one starts from morley's triangle updated by batoz (\"dkt12\") to derive a rotation-free (rf) triangular element (\"s3\"). the second stage consists in generalising this triangle to a rf quadrilateral (\"s4\"). during the final leg, the s4 and dkt12 main features are combined to give the quadrilateral \"dks16\". findings - other parameters being equal, the type of finite element chosen for the forming stage simulation has a great influence on further springback result even in software with automatic remeshing. particularly, it is pointed out that the rf shell elements s3 and s4 as well as the triangle dkt12 are less sensitive to the mesh size than classical shell elements with six dof per node. but, even if some improvements of in-plane shear have been proposed, stamping codes users are reluctant to use triangles. that is why this paper presents an attempt to extrapolate a quadrilateral (dks16) from the triangle dkt12 via s3 and s4 elements formulation. numerous examples showing convergence and accuracy are presented: irregular meshes, large displacement analyses and deep-drawing simulations. practical implications - the triangular \"s3\" element is already implemented in radioss (r) software and its implementation - as well as the one of \"dkt12\" - is in progress in pam-stamp, both as \"user elements\". the next step will be the implementation of the quadrilateral \"s4\" (rf) and, maybe, the element \"dks16\" since both are cheaper in terms of computation time and are found interesting for sheet forming. originality/value - it seems obvious that curvatures are more exactly captured in rf elements (when nodes slide on die radius) since they are imposed in terms of translations instead of traditional nodal rotations not managed by contact conditions. as the neighbours are involved, a drawback of these rf elements is their complex formulation in case of branching surfaces and/or abrupt variations in material behaviour and/or thickness. this is not the case for elements such as dkt12 or dks16, good candidates to add to the (long) list of cheap shell elements for large scale computations typical of sheet metal forming.",
    "present_kp": [
      "sheet metal"
    ],
    "absent_kp": [
      "formed materials"
    ]
  },
  {
    "title": "optimal placement of multicast and wavelength converting nodes in multicast optical virtual private network.",
    "abstract": "this paper addresses multicast-nodes (mc-nodes) and wavelength converting-nodes (wc-nodes) placement problem in multicast optical virtual private network (m-ovpn). this problem is motivated by the high cost incurred in the usage of optical splitters and converters in the vpn due to complexity and power considerations. an analytical model for computing the approximate blocking probability in m-ovpn with wavelength conversion has been developed. the placement of both mc-nodes and wc-nodes is determined using our proposed innovative removal algorithm together with the blocking performance computed. we perform simulation studies of our algorithms using nsfnet as the network topology. the results show that the removal heuristic is able to locate the optimal placement location within a much shorter time and with higher efficiency compared to existing heuristic. in addition, we also show the flexibility of the heuristics by introducing the hybrid (combined removal) and dual (concurrent removal) removal mode.",
    "present_kp": [
      "virtual private network",
      "wavelength conversion"
    ],
    "absent_kp": [
      "multicast ovpn",
      "power splitters",
      "wdm"
    ]
  },
  {
    "title": "designing tangible artefacts for playful interactions and dialogues.",
    "abstract": "this paper reports on the design process and iterative development of two tangible artefacts that aim to encourage and explore playful interactions and dialogues between grandchildren and grandparents living at separate locations. these designed prototypes respond to the magic box which is a cultural probe specifically created to explore playful activity at-a-distance in a non-electronic way. this paper reports on the process of project definition, technical design requirements, scenario creation and iterative prototype development. we interpret the ethnographic data from the magic box research; we develop activity scenarios to describe potential activities; and we design and develop working interaction prototypes to be tested in the field in future studies.",
    "present_kp": [
      "design",
      "playful interaction"
    ],
    "absent_kp": [
      "intergenerational communication",
      "interaction design",
      "phatic technologies",
      "ludic activity"
    ]
  },
  {
    "title": "analytical validation of the bemapplication of the bem to the electrocardiographic forward and inverse problem.",
    "abstract": "the objective of this study is to analytically validate a boundary element (be) formulation for the relationship between the transmembrane potential on the heart's surface and the potential on the body surface applying a concentric spherical test geometry. the relative difference (reldif) between the potential on the outer sphere of the test geometry computed analytically and numerically is determined by 3.59% for the coarse discretization (48 bes) and by 0.46% in the case of the finer subdivision (192 bes). in the inverse problem, the transmembrane potential on the inner sphere is estimated numerically from the electric potential on the outer sphere by using a minimum-norm least-square approach. the relative differences found are 20.2% when no measurement noise is added and 26.4% in the presence of 2% additional gaussian noise. the be formulation is also applied to real world data for solving the electrocardiographic inverse problem. a normal volunteer's inhomogeneous thorax (outer thorax surface, surfaces of the lungs, epicardial heart surface) is modelled by 424 bes. the same inverse method is then applied in order to reconstruct the transmembrane potential on the epicardium from the measured body surface potential (bsp) data during normal ventricular depolarisation.",
    "present_kp": [
      "transmembrane potential",
      "inverse problem"
    ],
    "absent_kp": [
      "electrocardiographic forward problem"
    ]
  },
  {
    "title": "reliable anchor-based sensor localization in irregular areas.",
    "abstract": "localization is a fundamental problem in wireless sensor networks and its accuracy impacts the efficiency of location-aware protocols and applications, such as routing and storage. most previous localization algorithms assume that sensors are distributed in regular areas without holes or obstacles, which often does not reflect real-world conditions, especially for outdoor deployment of wireless sensor networks. in this paper, we propose a novel scheme called reliable anchor-based localization (ral), which can greatly reduce the localization error due to the irregular deployment areas. we first provide theoretical analysis of the minimum hop length for uniformly distributed networks and then show its close approximation to empirical results, which can assist in the construction of a reliable minimal hop-length table offline. using this table, we are able to tell whether a path is severely detoured and compute a more accurate average hop length as the basis for distance estimation. at runtime, the ral scheme 1) utilizes the reliable minimal hop length from the table as the threshold to differentiate between reliable anchors and unreliable ones, and 2) allows each sensor to determine its position utilizing only distance constraints obtained from reliable anchors. the simulation results show that ral can effectively filter out unreliable anchors and therefore improve the localization accuracy.",
    "present_kp": [
      "wireless sensor networks",
      "reliable anchor"
    ],
    "absent_kp": [
      "range-free localization"
    ]
  },
  {
    "title": "semantic mapping from natural language questions to owl queries.",
    "abstract": "natural language question-and-answering is one of the most convenient means for communicating with the semantic web, which is typically in the form of online knowledge bases encoded in web ontology language (owl). to understand a natural language question, it is essential that it can be translated into a query that is understandable by the knowledge bases. this article is concerned with the task of semantic mapping from natural language questions to owl queries, and proposes an automatic and domain-independent mapping framework, called three-phases semantic mapping (tpsm). the tpsm framework approaches the task of semantic mapping in three interrelated phases: (i) formalizing knowledge, (ii) building semantic mapping, and (iii) combining owl queries. first, formalizing knowledge formalizes the units of mapping in the natural language and owl knowledge. second, semantic mapping builds the transverse mapping between questions and owl knowledge, as formalized in the first phase, by means of working with the so-called fuzzy constraint satisfaction problems (fcsp). third, combining owl queries obtains valid resource description framework (rdf) models by applying predefined templates and their corresponding combining methods. we have implemented a prototype semantic mapping system based on the framework, and have conducted a series of experimental validations involving owl knowledge bases in different domains as queried by various types of questions.",
    "present_kp": [
      "natural language question-and-answering",
      "semantic web",
      "semantic mapping",
      "fuzzy constraint satisfaction problem"
    ],
    "absent_kp": []
  },
  {
    "title": "mesoscopic study of concrete ii: nonlinear finite element analysis.",
    "abstract": "at mesoscopic scale, concrete may be regarded as a three-phase composite consisting of coarse aggregate, mortar matrix and interfacial zones. its composite behavior can be studied by generating a random aggregate structure which resembles the mesoscopic structure of concrete and analyzing the interaction between the three phases using the finite element method. a method of generating random aggregate structures taking into account the size, shape and spatial distributions of aggregate particles has been developed and presented in part i of the paper. herein, a nonlinear finite element method suitable for mesoscopic study of concrete is developed. goodman type interface elements are used to model the interfacial zones. cracking and nonlinear constitutive properties of the materials are taken into account. a failure criterion combining tensile strength and fracture toughness is adopted. stress relief as cracks propagate is also allowed for. an adaptive incremental displacement controlled iterative scheme which can deal with postpeak behavior is employed. the method is applied to study the strain localization of concrete under uniaxial tension in a numerical example.",
    "present_kp": [
      "concrete",
      "mesoscopic study",
      "random aggregate structure",
      "nonlinear finite element analysis",
      "strain localization"
    ],
    "absent_kp": [
      "composite materials",
      "size effect"
    ]
  },
  {
    "title": "desktop haptic virtual assembly using physically based modelling.",
    "abstract": "this research investigates the feasibility of using a desktop haptic virtual environment as a design tool for evaluating assembly operations. bringing virtual reality characteristics to the desktop, such as stereo vision, further promotes the use of this technology into the every day engineering design process. in creating such a system, the affordability and availability of hardware/software tools is taken into consideration. the resulting application combines several software packages including vr juggler, open dynamics engine (ode)/open physics abstraction layer (opal), openhaptics, and opengl/glm/glut libraries to explore the benefits and limitations of combining haptics with physically based modelling. the equipment used to display stereo graphics includes a stereographics emitter, crystal eyes shutter glasses, and a high refresh rate crt monitor. one or two-handed force feedback is obtained from various phantom haptic devices from sensable technologies inc. the applications ability to handle complex part interactions is tested using two different computer systems, which approximate the higher and lower end of a typical engineers workstation. different test scenarios are analyzed and results presented.",
    "present_kp": [
      "physically based modelling",
      "virtual reality"
    ],
    "absent_kp": [
      "haptic i/o",
      "computer-aided design"
    ]
  },
  {
    "title": "inhibited spontaneous emission and electromagnetic instability of an atom in the near zone from the surface of an active medium.",
    "abstract": "the back reaction field and the self-consistent dynamics of a radiating dipole atom situated near a plane boundary between two isotropic media with different values of complex dielectric constants is studied. it is shown that spontaneous emission of an excited atom is suppressed, and electromagnetic instability of the ground state is possible, if an atom is located in the vicinity of an active medium which amplifies the propagating radiation. the effect is due to the modified and phase-shifted near field of an oscillating atomic dipole, which prevails over the usual radiation reaction field related to the wave zone. the role of the nonradiative energy transfer from the medium to an atom is emphasized. the necessary conditions for the reversal of the total electromagnetic back reaction are found, and possible experimental realizations of the predicted phenomena are discussed. in particular, it is argued that an atom initially in its ground state can get spontaneously excited and emit photon when going to the upper energy level.",
    "present_kp": [
      "spontaneous emission",
      "electromagnetic instability"
    ],
    "absent_kp": [
      "back reaction force",
      "nonradiative transfer"
    ]
  },
  {
    "title": "image change detection using gaussian mixture model and genetic algorithm.",
    "abstract": "in this paper, we propose a novel method for unsupervised change detection in multi-temporal satellite images of the same scene using gaussian mixture model (gmm) and genetic algorithm (ga). the difference image data computed from multi-temporal satellite images of the same scene is modelled by using n components gmm. ga is used to estimate the parameters of the gmm. then, the gmm of the difference image data is partitioned into two sets of distributions representing data distributions of changed and unchanged pixels by minimizing a cost function using ga. bayesian inference is exploited together with the estimated data distributions of changed and unchanged pixels to achieve the final change detection result. the proposed method does not need any parameter tuning process, and is completely automatic. as a case study for the unsupervised change detection, multi-temporal advanced synthetic aperture radar (asar) images acquired by esa envisat on the recent flooding area in bangladesh and parts of india brought on by two weeks of persistent rain and multi-temporal optical images acquired by landsat 5 tm on a part of alaska are considered. change detection results are shown on real data and comparisons with the state-of-the-art techniques are provided.",
    "present_kp": [
      "gaussian mixture model",
      "genetic algorithm",
      "bayesian inference",
      "change detection",
      "difference image",
      "optical image",
      "advanced synthetic aperture radar"
    ],
    "absent_kp": [
      "parameter estimation",
      "log-ratio image",
      "remote sensing"
    ]
  },
  {
    "title": "existential witness extraction in classical realizability and via a negative translation.",
    "abstract": "we show how to extract existential witnesses from classical proofs using krivine's classical realizability-where classical proofs are interpreted as lambda-terms with the call/cc control operator. we first recall the basic framework of classical realizability (in classical second-order arithmetic) and show how to extend it with primitive numerals for faster computations. then we show how to perform witness extraction in this framework, by discussing several techniques depending on the shape of the existential formula. in particular, we show that in the sigma(0)(1)-case, krivine's witness extraction method reduces to friedman's through a well-suited negative translation to intuitionistic second-order arithmetic. finally we discuss the advantages of using call/cc rather than a negative translation, especially from the point of view of an implementation.",
    "present_kp": [
      "classical realizability"
    ],
    "absent_kp": [
      "proof theory",
      "classical lambda-calculus",
      "program extraction"
    ]
  },
  {
    "title": "a novel communication range recognition (crr) scheme for spatial localization of passive rfid tags.",
    "abstract": "rfid (radio frequency identification) technology is expected to be used as a localization tool. by the localization of rfid tags, a mobile robot equipped with an rfid reader can recognize the surrounding environment. in this paper, we propose a novel effective scheme called the communication range recognition (crr) scheme for localizing rfid tags. in this scheme, an rfid reader determines the boundaries of the communication range when it is appropriately positioned by the robot. we evaluate the estimated position accuracy through numerous experiments. we show that the moving distance of the rfid reader in the proposed scheme is lower than that in conventional schemes.",
    "present_kp": [
      "rfid reader",
      "rfid tag",
      "localization",
      "communication range recognition"
    ],
    "absent_kp": [
      "estimated position error"
    ]
  },
  {
    "title": "synchronization and stable phase-locking in a network of neurons with memory.",
    "abstract": "we consider a network of three identical neurons whose dynamics is governed by the hopfield's model with delay to account for the finite switching speed of amplifiers (neurons). we show that in a certain region of the space of (alpha, beta), where alpha and beta are the normalized parameters measuring, respectively, the synaptic strength of self-connection and neighbourhood-interaction, each solution of the network is convergent to the set of synchronous states in the phase space, and this synchronization is independent of the size of the delay. we also obtain a surface; as the graph of a continuous function of tau = tau(alpha, beta) (the normalized delay) in some region of (alpha, beta), where hopf bifurcation of periodic solutions takes place. we describe a continuous curve on such a surface where the system undergoes mode-interaction and we describe the change of patterns from stable synchronous periodic solutions to the coexistence of two stable phase-locked oscillations and several unstable mirror-reflecting waves and standing waves.",
    "present_kp": [
      "neuron",
      "network",
      "synchronization",
      "delay",
      "phase-locking",
      "wave"
    ],
    "absent_kp": [
      "multistability"
    ]
  },
  {
    "title": "supervised automatic evaluation for summarization with voted regression model.",
    "abstract": "the high quality evaluation of generated summaries is needed if we are to improve automatic summarization systems. although human evaluation provides better results than automatic evaluation methods, its cost is huge and it is difficult to reproduce the results. therefore, we need an automatic method that simulates human evaluation if we are to improve our summarization system efficiently. although automatic evaluation methods have been proposed, they are unreliable when used for individual summaries. to solve this problem, we propose a supervised automatic evaluation method based on a new regression model called the voted regression model (vrm). vrm has two characteristics: (1) model selection based on corrected aic to avoid multicollinearity, (2) voting by the selected models to alleviate the problem of overfitting. evaluation results obtained for tsc3 and duc2004 show that our method achieved error reductions of about 1751% compared with conventional automatic evaluation methods. moreover, our method obtained the highest correlation coefficients in several different experiments.",
    "present_kp": [
      "automatic summarization",
      "automatic evaluation",
      "regression model"
    ],
    "absent_kp": [
      "text summarization challenge",
      "document understanding conference"
    ]
  },
  {
    "title": "classification of laryngeal disorders based on shape and vascular defects of vocal folds.",
    "abstract": "vocal folds on videolaryngostroboscopy images are detected by hog for examination. vocal fold images are classified into five laryngeal disorder types. we exploit shape and vascular features of vocal folds for classification. an average classification success rate of 81% is achived. we demonstrate that visible vessels of vocal folds can act as a prognostic marker.",
    "present_kp": [],
    "absent_kp": [
      "laryngeal image analysis",
      "classification of vocal fold disorders",
      "histogram of oriented gradients",
      "vascular vectors",
      "vessel centerline extraction",
      "measurement of vocal fold shape defects"
    ]
  },
  {
    "title": "analysis of feature selection stability on high dimension and small sample data.",
    "abstract": "feature selection is an important step when building a classifier on high dimensional data. as the number of observations is small, the feature selection tends to be unstable. it is common that two feature subsets, obtained from different datasets but dealing with the same classification problem, do not overlap significantly. although it is a crucial problem, few works have been done on the selection stability. the behavior of feature selection is analyzed in various conditions, not exclusively but with a focus on t-score based feature selection approaches and small sample data. the analysis is in three steps: the first one is theoretical using a simple mathematical model; the second one is empirical and based on artificial data; and the last one is based on real data. these three analyses lead to the same results and give a better understanding of the feature selection problem in high dimension data.",
    "present_kp": [
      "feature selection",
      "small sample",
      "stability"
    ],
    "absent_kp": [
      "low n/d"
    ]
  },
  {
    "title": "a comparison of messy ga and permutation based ga for job shop scheduling.",
    "abstract": "this paper presents the results of a fair comparison between a messy ga and a permutation based simple ga as applied to a job shop scheduling system. an examination is made at a macro level in terms of performance and quality of schedules achieved and conclusions are drawn as to the superiority of messy ga or otherwise.",
    "present_kp": [
      "job shop scheduling"
    ],
    "absent_kp": [
      "repeating permutation representation",
      "messy genetic algorithms"
    ]
  },
  {
    "title": "exploring motor cortical plasticity using transcranial magnetic stimulation in humans.",
    "abstract": "abstract: it is generally accepted that functional properties of the motor cortex in adult humans can be altered through behavioral or pharmacological manipulations, as well as in some pathological conditions. the ability and capacity of adult human cortex to undergo any adaptive or reorganizational changes is referred to as plasticity. much of the evidence concerning motor cortical plasticity have been derived from studies using the non-invasive technique of transcranial magnetic stimulation (tms). tms has proven to be a suitable tool to explore representational plasticity and to interact with neuronal activity in settings of induction protocols either alone or coupled with altered sensory inputs. furthermore, plastic changes induced by motor learning protocols have attracted particular interest because of their relevance in functional recovery. recent studies support the view that learning in human motor cortex occurs through long-term potentiation (ltp)-like mechanisms. purposeful modulation of motor cortical plastic changes by manipulative tms protocols may offer useful rehabilitative strategies in patients with chronic motor deficits.",
    "present_kp": [
      "transcranial magnetic stimulation",
      "motor learning",
      "human"
    ],
    "absent_kp": [
      "motor cortex plasticity"
    ]
  },
  {
    "title": "supporting conceptual awareness with pedagogical agents.",
    "abstract": "this paper describes a series of efforts in building and conceptualizing software agents for distributed collaborative learning. the agents are referred to as pedagogical agents. we have integrated pedagogical agents within two collaborative environments, teamwave workplace and future learning environment. the role of agents in these environments differs from past work on software agents in their function as extended awareness mechanisms, focusing on task and concept awareness (conceptual awareness). our approach is stimulated by meads theory of the 'generalized other'. the agents collect statistical information on user activity and analyze the information based on principles of collaboration and knowledge building (participation, group interaction, and scientific discourse). furthermore, the agents define a trajectory in a pedagogical agent design space, which we define in terms of four dimensions: presentation, intervention, task, and pedagogy. we end the paper by comparing our approach with related work.",
    "present_kp": [
      "group interaction",
      "generalized other",
      "awareness",
      "pedagogical agents"
    ],
    "absent_kp": [
      "virtual learning environments"
    ]
  },
  {
    "title": "a characteristic particle method for the saint venant equations.",
    "abstract": "a novel characteristic particle method is developed for the saint venant equations in the present study. contrary to the conventional fixed-grid or moving particle methods, the present formulation is built by reallocating the computational particles along the characteristic curves. both the right- and left-running characteristics are faithfully traced and imitated with their associated computational particles. the annoying numerical nuisance in realizing advection term for the fixed-grid arrangement can be effectively gotten rid of. besides, special particles with dual states to the enforcement of the rankine-hugonlot relation are deliberately imposed to emulate the shock structure. efficacy of this formulation is verified by solving some benchmark problems with significant transient effects. computational results are meticulously compared with available analytical solutions. it is concluded that the proposed characteristic particle method will be a useful tool to replicate transient phenomena revealed by the saint venant equations.",
    "present_kp": [
      "characteristic particle method",
      "saint venant equations"
    ],
    "absent_kp": [
      "dual-state particle",
      "rankine-hugoniot relation",
      "open-channel flow",
      "dam-break flow",
      "tidal bore flow"
    ]
  },
  {
    "title": "performance analysis of a device-to-device communication-based random access scheme for machine-type communications.",
    "abstract": "group paging was proposed by the 3rd generation partnership project to reduce the awesome paging overhead in machine-type communication (mtc) that usually involves a huge number of devices performing small data transmissions. under group paging, mtc devices with identical or similar traffic characteristics are assembled into a paging group; and all the devices in a paging group is paged by a single group paging message. after receiving a group paging message, all the devices in a paging group simultaneously contend for the random access channel to transmit their random access requests, thereby resulting in congestion in the random access network in the case of large group size. to tackle this problem of group paging, we previously proposed an improved random access (i-ra) scheme. in this paper we develop a mathematical model based on poisson arrival approximation method to analyze the performance of the i-ra scheme. we derive closed-form analytical formulas for performance metrics, such as successful access probability, collision probability, average access delay, the cumulative distribution function of preamble transmission, etc. the accuracy of the analytical formulas is validated through simulation. based on the formulas, we conduct a numerical study to investigate the impact of diverse parameters on the performance of the i-ra scheme. moreover, we compare the performance between the i-ra scheme and the existing schemes by numerical studies. the mathematical model developed and the formulas derived in this paper can guide the implementation of the i-ra scheme in mtc.",
    "present_kp": [
      "group paging",
      "random access"
    ],
    "absent_kp": [
      "device-to-device communications",
      "cluster communication",
      "machine type communications"
    ]
  },
  {
    "title": "fastinf: an efficient approximate inference library.",
    "abstract": "the fastinf c++ library is designed to perform memory and time efficient approximate inference in large-scale discrete undirected graphical models. the focus of the library is propagation based approximate inference methods, ranging from the basic loopy belief propagation algorithm to propagation based on convex free energies. various message scheduling schemes that improve on the standard synchronous or asynchronous approaches are included. also implemented are a clique tree based exact inference, gibbs sampling, and the mean field algorithm. in addition to inference, fastinf provides parameter estimation capabilities as well as representation and learning of shared parameters. it offers a rich interface that facilitates extension of the basic classes to other inference and learning methods.",
    "present_kp": [
      "graphical models",
      "loopy belief propagation",
      "approximate inference"
    ],
    "absent_kp": [
      "markov random field"
    ]
  },
  {
    "title": "pumping test interpretation by combination of latin hypercube parameter sampling and analytical models.",
    "abstract": "pumping tests in groundwater reservoirs are a much used and recommended method to derive hydraulic properties of aquifers and aquitards at the field scale. interpretation of pumping tests can be done by fitting analytical or numerical models to the obtained field data using optimisation procedures. in this paper, an interpretation methodology is presented and implemented in the form of a computer code that combines analytical solutions for confined, semi-confined and unconfined single and multiple layer schematisations, with random parameter generator routines to find best fitting parameter sets to measured pumping test data. as this requires a large number of monte carlo (mc) runs, the number of required simulations is restricted by using a stratified instead of a pure random sampling technique. latin hypercube sampling (lhs) is used as the stratified random sampler. the analytical solutions are defined in the laplace domain and inverted numerically using the well-known stehfest algorithm. the program uses a number of iteration cycles during which parameters sets are generated within predefined limits using the lhs technique. parameters sets which satisfy a chosen maximum value for a selected objective function (root mean square (rmse) or mean relative deviation (mrd)) are retained and used to update parameter limits for the next cycle. the objective function criterion is decreased during subsequent cycles while the limits of the sampling intervals are adapted. the number of determinable parameters is dependent on the aquifer schematisation and conceptual model that is chosen. after each cycle, statistics of the parameter values of the realisations which satisfy the objective function criterion are calculated. the method is demonstrated with examples including synthetic datasets and field data. the synthetic data examples show that the program is able to retrieve the parameter values used for generating drawdown sets very well. typical runtimes on a pc are no more than a few minutes. the program can easily be extended with additional analytical solutions for other schematisations.",
    "present_kp": [
      "groundwater",
      "pumping test",
      "interpretation",
      "analytical solution",
      "latin hypercube sampling"
    ],
    "absent_kp": [
      "hydraulic parameters"
    ]
  },
  {
    "title": "determining the sampling time frame for in-vehicle data recorder measurement in assessing drivers.",
    "abstract": "method for choosing the required sampling time frame for in-vehicle data recorders. the method is demonstrated on cab driver population ivdr data. analysis indicates that ?300h per driver result in a stable driving pattern. future use of method in evaluation of driving patterns and changes over time.",
    "present_kp": [
      "in-vehicle data recorder"
    ],
    "absent_kp": [
      "driving behavior patterns",
      "driving assessments"
    ]
  },
  {
    "title": "intermediate storage in batch processing systems under stochastic equipment failures.",
    "abstract": "the role of intermediate storage in buffering a process train from the effects of stochastically varied equipment failures is studied. two models of upstream failures are investigated analytical formulae are derived for the distributions of minimal and maximal values of the intermediate storage hold up, by means of which the favorable storage volume can be determined. also, an algorithm and computer program is presented for designing the favorable volume of an intermediate storage. the program can also be used for studying the general behavior of such systems with the aid of simulation. an example is presented for illustrating the applicability of the analytical formulae and the computer program in sizing the intermediate storage in batch processing systems.",
    "present_kp": [
      "intermediate storage",
      "stochastic equipment failure"
    ],
    "absent_kp": [
      "batch operation",
      "hold up variation"
    ]
  },
  {
    "title": "dynamic customer lifetime value prediction using longitudinal data: an improved multiple kernel svr approach.",
    "abstract": "customer lifetime value (clv), as an important metric in customer relationship management (crm), has attracted widespread attention over the last decade. most clv prediction models do not take into consideration the dynamics of the customer purchase behavior and changes of the marketing environment such as the adoption of different promotion policies. in this study, a framework for the dynamic clv prediction using longitudinal data is presented. in the framework, both the dynamic customer purchase behavior and customized promotions are considered. an improved multiple kernel support vector regression (mk-svr) approach is developed to predict the future clv and select the best promotion using both the customer behavioral variables and controlled variable about multiple promotions. computational experiments using two databases show that the mk-svr exhibits good prediction performance and the usage of longitudinal data in the mk-svr facilitate the dynamic prediction and promotion optimization.",
    "present_kp": [
      "customer relationship management",
      "customer lifetime value"
    ],
    "absent_kp": [
      "data mining",
      "dynamic multi-step-ahead prediction",
      "support vector machine",
      "multiple kernel learning"
    ]
  },
  {
    "title": "real-time constrained linear discriminant analysis to target detection and classification in hyperspectral imagery.",
    "abstract": "in this paper, we present a constrained linear discriminant analysis (clda) approach to hyperspectral image detection and classification as well as its real-time implementation. the basic idea of clda is to design an optimal transformation matrix which can maximize the ratio of inter-class distance to intra-class distance while imposing the constraint that different class centers after transformation are along different directions such that different classes can be better separated. the solution turns out to be a constrained version of orthogonal subspace projection (osp) implemented with a data whitening process. the clda approach can be applied to solve both detection and classification problems. in particular, by introducing color for display the classification is achieved with a single classified image where a pre-assigned color is used to display a specified class. the real-time implementation is also developed to meet the requirement of on-line image analysis when the immediate data assessment is critical. the experiments using hydice data demonstrate the strength of clda approach in discriminating the targets with subtle spectral difference.",
    "present_kp": [
      "detection",
      "classification",
      "constrained linear discriminant analysis ",
      "hyperspectral imagery"
    ],
    "absent_kp": [
      "real-time processing"
    ]
  },
  {
    "title": "a novel acknowledgment-based approach against collude attacks in manet.",
    "abstract": "routing misbehavior and collusion attack in mobile ad hoc networks (manets) are the main topics in this paper. various techniques prevent routing misbehaviors have been published, but most of them do not consider the collusion attacks. therefore, we propose the scheme called nack to simultaneously detect routing misbehavior and collusion attacks, and to mitigate the effects of routing attacks. furthermore, nack scheme adopts an acknowledgment-based approach and timestamps comparison to resist the attacks mentioned above. the security analysis and simulation results are presented to evaluate the performance of nack.",
    "present_kp": [
      "collusion attack",
      "timestamp",
      "mobile ad hoc networks"
    ],
    "absent_kp": []
  },
  {
    "title": "flexible case-based retrieval for comparative genomics.",
    "abstract": "comparative genomics represents a key instrument to discover or validate phylogenetic relationships, to give insights on genome evolution, and to infer metabolic functions of a given organism. a tool for properly supporting comparative genomics is of paramount importance in several application domains.",
    "present_kp": [
      "comparative genomics"
    ],
    "absent_kp": [
      "case-based reasoning",
      "flexible retrieval",
      "index-based retrieval",
      "interactivity"
    ]
  },
  {
    "title": "the role of early fundamental frequency rises and elbows in french word segmentation.",
    "abstract": "the study examined intonational cues to word beginnings in french. french has an optional early rise in fundamental frequency (f0) starting at the beginning of a content word. the role of this rise in segmentation by human listeners had previously been suggested, but not empirically tested. in experiment 1, participants listened to noise-masked items like le ballon de mmentos/le ballon de mes manteaux , differing in segmentation and presence of an early rise. they interpreted early rises as markers of content word beginnings. in experiment 2, the alignment of the early rise was manipulated in nonword sequences like . listeners were more likely to perceive two words (mes lamondines) when the early rise started at the second syllable and a single content (non)word (mlamondine) when it started at the first. experiment 3 showed that a simple f0 elbow at a function wordcontent word boundary also cued a content word beginning. this pattern and its potential use in word segmentation had not been previously reported in the literature. these intonational cues, like other cues to word segmentation, influenced rather than determined segmentation decisions. the influence of other cues (e.g., duration, word frequency) is also discussed. these results are the first evidence that french listeners use intonational information as cues to content word beginning. these cues are particularly important because the beginning of a word is a privileged position in word recognition and because, unlike many other cues (e.g., stress in english), they identify actual rather than potential word boundaries. the results provide support for an autosegmental-metrical account of the intonational phonology of french in which the early rise is a bitonal (lh) phrase accent that serves as a cue to content word beginnings. the cue is strongest when both tones (lh) are realized, which leads to an early rise, but can still be used if only the l is realized, which leads to a simple elbow. these results illustrate the importance of expanding studies of the range of cues to word segmentation to include intonational cues.",
    "present_kp": [
      "word segmentation",
      "intonation",
      "french"
    ],
    "absent_kp": [
      "speech segmentation",
      "prosody"
    ]
  },
  {
    "title": "language and emotional knowledge: a case study on ability and disability in williams syndrome.",
    "abstract": "williams syndrome provides a striking test case for discourses on disability, because the characteristics associated with williams syndrome involve a combination of abilities and disabilities. for example, williams syndrome is associated with disabilities in mathematics and spatial cognition. however, williams syndrome individuals also tend to have a unique strength in their expressive language skills, and are socially outgoing and unselfconscious when meeting new people. children with williams are said to be typically unafraid of strangers and show a greater interest in contact with adults than with their peers. this apparently keen social knowledge is a counterexample to the discussion of disability among academic philosophers, especially philosophers in the early modern period. locke infamously used the example of disability to claim that descartes arguments in favor of innate ideas were incorrect. on the contrary, williams syndrome may stand as an example of innate social knowledge; something that could benefit current discourse in philosophy, disability theory, and medical ethics.",
    "present_kp": [
      "williams syndrome"
    ],
    "absent_kp": [
      "self-reflexive attitudes",
      "sociability",
      "hyperlexia",
      "amygdala",
      "planum temporale"
    ]
  },
  {
    "title": "a refactoring method for cache-efficient swarm intelligence algorithms.",
    "abstract": "with advances in hardware technology, conventional approaches to software development are not effective for developing efficient algorithms for run-time environments. the problem comes from the overly simplified hardware abstraction model in the software development procedure. the mismatch between the hypothetical hardware model and real hardware design should be compensated for in designing an efficient algorithm. in this paper, we focus on two schemes: one is the memory hierarchy, and the other is the algorithm design. both the cache properties and the cache-aware development are investigated. we then propose a few simple guidelines for revising a developed algorithm in order to increase the utilization of the cache. to verify the effectiveness of the guidelines proposed, optimization techniques, including particle swarm optimization (pso) and the genetic algorithm (ga), are employed. simulation results demonstrate that the guidelines are potentially helpful for revising various algorithms.",
    "present_kp": [
      "cache",
      "memory hierarchy",
      "swarm intelligence",
      "particle swarm optimization",
      "genetic algorithm"
    ],
    "absent_kp": [
      "miss rate"
    ]
  },
  {
    "title": "optimizing goods assignment and the vehicle routing problem with time-dependent travel speeds.",
    "abstract": "many mathematical models have been presented for the vehicle routing problem (vrp) in research papers. most of them assume that the travel speeds are constant, and ignore the fact that travel speeds can change throughout the day. in this research, based on time-dependent travel speeds, an optimization method is proposed for solving goods assignment and vehicle routing problems. moreover, in the optimization method, the travel time calculation satisfies the \"non-passing\" property, which is ignored in most research papers. a real case of a 3c warehousing company and a larger simulated problem are introduced to illustrate the proposed method. the results show that the proposed method is efficient and effective in solving problems.",
    "present_kp": [
      "time-dependent travel speed",
      "vehicle routing problem"
    ],
    "absent_kp": [
      "tabu search"
    ]
  },
  {
    "title": "low-complexity constant multiplication based on trigonometric identities with applications to ffts.",
    "abstract": "in this work we consider optimized twiddle factor multipliers based on shift-and-add-multiplication. we propose a low-complexity structure for twiddle factors with a resolution of 32 points. furthermore, we propose a slightly modified version of a previously reported multiplier for a resolution of 16 points with lower round-off noise. for completeness we also include results on optimal coefficients for eight-points resolution. we perform finite word length analysis for both coefficients and round-off errors and derive optimized coefficients with minimum complexity for varying requirements.",
    "present_kp": [
      "fft",
      "constant multiplication"
    ],
    "absent_kp": [
      "complex multiplier",
      "shift-and-add multiplication"
    ]
  },
  {
    "title": "improved model- and view-dependent pruning of large botanical scenes.",
    "abstract": "we present an optimized pruning algorithm that allows for considerable geometry reduction in large botanical scenes while maintaining high and coherent rendering quality. we improve upon previous techniques by applying model-specific geometry reduction functions and optimized scaling functions. for this we introduce the use of precision and recall (pr) as a measure of quality to rendering and show how pr-scores can be used to predict better scaling values. we conducted a user-study letting subjects adjust the scaling value, which shows that the predicted scaling matches the preferred ones. finally, we extend the originally purely stochastic geometry prioritization for pruning to account for view-optimized geometry selection, which allows to take global scene information, such as occlusion, into consideration. we demonstrate our method for the rendering of scenes with thousands of complex tree models in real-time.",
    "present_kp": [],
    "absent_kp": [
      "precision/recall",
      "level of detail",
      "tree rendering"
    ]
  },
  {
    "title": "losses scaling in soft magnetic materials.",
    "abstract": "purpose - the paper presents an application of the scaling theory in a description of energy losses in soft magnetic materials in order to improve an agreement between measurements and theoretical models. design/methodology/approach - the scaling theory allows the description of energy losses by a generalized homogenous function, which depends on scaling exponents alpha, beta and amplitudes gamma((n)). the values of the scaling exponents and amplitudes were estimated on the basis of measurement data of total energy losses. findings - the main findings of the paper are: the linear relationships between the scaling exponents a and 13, the data collapse of energy losses and the scaling laws for asymptotic exponents of energy losses derivatives. research limitations/implications - the origin of the data collapse and the relationship between the scaling exponents will be the subject of further research with the aid of renormalization group method. practical implications - the paper could be useful both for device designers and researchers involve in computational electromagnetism. particularly, the data collapse allows a comparison of energy loss values measured in laboratories on different samples and by different methods. originality/value - the application of the scaling theory in a description of energy losses in soft magnetic materials improves an agreement between measurement and theoretical models.",
    "present_kp": [
      "magnetism",
      "energy"
    ],
    "absent_kp": [
      "physical properties of materials"
    ]
  },
  {
    "title": "open-source tools for dynamical analysis of liley's mean-field cortex model.",
    "abstract": "a parallel open-source implementation of liley's mean-field cortex model, in petsc. we implement fully implicit time integration of nonlinear and variational equations. we perform equilibrium continuation, with computation of inhomogeneous eigenmodes. we compute periodic solutions with newtonkrylov iteration.",
    "present_kp": [
      ""
    ],
    "absent_kp": [
      "mean-field modelling",
      "hyperbolic partial differential equations",
      "numerical partial differential equations"
    ]
  },
  {
    "title": "static worst-case lifetime estimation of wireless sensor networks: a case study on vigilnet.",
    "abstract": "this paper proposes a feasible static analysis approach to estimate the worst-case lifetime of wsns, with specific focus on vigilnet. we statically estimate the lifetime of each node in vigilnet with a fixed initial energy budget through a hybrid approach, which integrates an integer linear programming (ilp) based method to obtain the worst-case cpu energy consumption and domain-specific scenarios analysis to compute the worst-case radio energy consumption. our experimental results indicate that this static approach can safely and accurately estimate the worst-case lifetime for wsns.",
    "present_kp": [
      "wsns",
      "worst-case lifetime",
      "vigilnet"
    ],
    "absent_kp": [
      "static timing analysis",
      "worst-case energy consumption"
    ]
  },
  {
    "title": "corner occupying theorem for the two-dimensional integral rectangle packing problem.",
    "abstract": "this paper proves a corner occupying theorem for the two-dimensional integral rectangle packing problem, stating that if it is possible to orthogonally place n arbitrarily given integral rectangles into an integral rectangular container without overlapping, then we can achieve a feasible packing by successively placing a rectangle onto a bottom-left corner in the container. based on this theorem, we might develop efficient heuristic algorithms for solving the integral rectangle packing problem. in fact, as a vague conjecture, this theorem has been implicitly mentioned with different appearances by many people for a long time.",
    "present_kp": [
      "rectangle packing",
      "bottom-left",
      "corner occupying theorem"
    ],
    "absent_kp": [
      "np hard"
    ]
  },
  {
    "title": "18 mbit/s carrier frequency offset-spread spectrum (cfo-ss) system using 2.4 ghz ism band.",
    "abstract": "a wireless communications system with a transmission speed of 18 mbit/s is presented using the 2.4 ghz ism band. this system employs the \"carrier frequency offset-spread spectrum (cfo-ss)\" scheme and the \"dual-polarization staggered transmission (dpst)\" scheme. the 18 mbit/s cfo-ss system (named \"cfo-ss18\") was developed and its performance evaluated in fields. in this paper, the detailed operating principle of cfo-ss and dpst schemes, together with the specifications and structures of cfo-ss 18, are presented. results of indoor and field tests obtained by using cfo-ss18 are also presented.",
    "present_kp": [
      "cfo-ss",
      "2.4 ghz ism band"
    ],
    "absent_kp": [
      "wireless access",
      "high speed",
      "dual-polarization technique"
    ]
  },
  {
    "title": "three dimensional threaded fastener meshing algorithm.",
    "abstract": "understanding the stresses in the root of a thread of a bolt has never been easy. ideally one would like to put a strain gage right at the root and measure them directly, but the placement of the gage and its wires are impossible without changing the insitu environment. engineers then turned to analytical models, but soon discovered that two dimensional models where not giving the correct results because the geometry is not axis symmetric and three dimensional models where too difficult to mesh with today's cad tools. a team consisting of ford engineers and computer programers developed a program that will automatically generate a full three dimensional model of a thread fastener and mating threaded block by using a combination of cylindrical meshing techniques and a method of revolving profiles in a helical coordinate system. the bolt meshing program is able to input parameters about the thread fastener and block: diameter, pitch, length, etc., and output a complete three dimensional model in the form of an abaqus or nastran finite element input deck. this model can then run through the abaqus finite element program and the resulting stress field computed. this paper will focus on the algorithm used to generate the mesh.",
    "present_kp": [
      "threaded fastener",
      "bolt"
    ],
    "absent_kp": [
      "solid meshing"
    ]
  },
  {
    "title": "measurements of flow distribution in a thin resin layer during ultraviolet nanoimprint lithography by means of digital holographic particle-tracking velocimetry.",
    "abstract": "micro-dhptv was used to measure an uv-curable resin flow at in situ nil process. we succeeded in measuring the 4-dimensions flow distribution in the thin resin layer. it was clearly shown that the presence of a release agent affects the uv-curable resin flow. we believe this study will provide a better understanding of the flow behavior of uv-curable resins.",
    "present_kp": [
      "nanoimprint lithography",
      "particle-tracking velocimetry",
      "uv-curable resin",
      "release agent"
    ],
    "absent_kp": [
      "flow measurement"
    ]
  },
  {
    "title": "an approach to multi-modal human-machine interaction for intelligent service robots.",
    "abstract": "the paper describes a multi-modal scheme for human-robot interaction suited for a wide range of intelligent service robot applications. operating in un-engineered, cluttered, and crowded environments, such robots have to be able to actively contact potential users in their surroundings and to offer their services in an appropriate manner. starting from a real application scenario, the usage of a robot as mobile information kiosk in a home store, some reliable methods for vision-based interaction, sound analysis and speech output have been developed. these methods are integrated into a prototypical interaction cycle that can be assumed as a general approach to human-machine interaction. experimental results demonstrate the strengths and weaknesses of the proposed methods.",
    "present_kp": [
      "human-robot interaction",
      "service robot application"
    ],
    "absent_kp": [
      "people detection",
      "people tracking",
      "multi-modal interaction"
    ]
  },
  {
    "title": "effects of a fractional friction with power-law memory kernel on string vibrations.",
    "abstract": "in this paper we give an analytical treatment of a wave equation for a vibrating string in the presence of a fractional friction with power-law memory kernel. the exact solution is obtained in terms of the mittag-leffler type functions and a generalized integral operator containing a four parameter mittag-leffler function in the kernel. the method of separation of the variables, laplace transform method and sturmliouville problem are used to solve the equation analytically. the asymptotic behaviors of the solution of a special case fractional differential equation are obtained directly from the analytical solution of the equation and by using the tauberian theorems. the proposed model may be used for describing processes where the memory effects of complex media could not be neglected.",
    "present_kp": [
      "wave equation",
      "mittag-leffler function"
    ],
    "absent_kp": [
      "frictional power-law memory kernel",
      "caputo time fractional derivative",
      "fractional integral operator",
      "fractional differential operator"
    ]
  },
  {
    "title": "a version of the swendsen-wang algorithm for restoration of images degraded by poisson noise.",
    "abstract": "an algorithm for restoration of images degraded by poisson noise is proposed. the algorithm belongs to the family of markov chain monte carlo methods with auxiliary variables. we explicitly use the fact that medical images consist of finitely many, often relatively few, grey-levels. the continuous scale of grey-levels is discretized in an adaptive way, so that a straightforward application of the swendsen-wang (phys. rev. lett. 58 (1987) 86) algorithm becomes possible. partial decoupling method due to higdon (j. am. statist. assoc. 93 (1998) 442, 585) is also incorporated into the algorithm. simulation results suggest that the algorithm is reliable and efficient.",
    "present_kp": [
      "swendsen-wang algorithm",
      "markov chain monte carlo"
    ],
    "absent_kp": [
      "bayesian image restoration",
      "gibbs distributions",
      "gibbs sampler",
      "intensity estimation"
    ]
  },
  {
    "title": "sampling of discrete materials - ii. quantitative approach-sampling of zero-dimensional objects.",
    "abstract": "parts ii and iii of this series are initiated by a joint discussion of features related to the lot. part ii then delineates the central elements of the theory of sampling for zero-dimensional objects. it is necessary to be brief within the limited format of the present tutorial series, but all essential model rigour has been maintained. an attempt has been made to focus on the central mathematical theoretical core of tos while also showing how this relates directly to sampling practise (materials, equipment and procedures). a highlight of the latter issue concerns experimental estimation of the fundamental sampling error (fse). part ii is also fundamental for further developments in part iii, as it presents a complete overview discussion of the basic sampling operation of the one-dimensional object as well.",
    "present_kp": [
      "discrete",
      "quantitative approach",
      "zero-dimensional objects",
      "theory of sampling"
    ],
    "absent_kp": [
      "sampling of particulate matter"
    ]
  },
  {
    "title": "comparison of aligned friedman rank and parametric methods for testing interactions in split-plot designs.",
    "abstract": "parametric methods are commonly used despite evidence that model assumptions are often violated. various statistical procedures have been suggested for analyzing data from multiple-group repeated measures (i.e., split-plot) designs when parametric model assumptions are violated (e.g., akritas and arnold (j. amer. statist. assoc. 89 (1994) 336); brunner and langer (biometrical j. 42 (2000) 663)), including the use of friedman ranks. the effects of friedman ranking on data and the resultant test statistics for single sample repeated measures designs have been examined (e.g., harwell and serlin (comput. statist. data anal. 17 (1994) 35; comm. statist. simulation comput. 26 (1997) 605); zimmerman and zumbo (j. experiment. educ. 62 (1993) 75)). however, there have been fewer investigations concerning friedman ranks applied to multiple groups of repeated measures data (e.g., beasley (j. educ. behav. statist. 25 (2000) 20); rasmussen (british j. math. statist. psych. 42 (1989) 91)). we investigate the use of friedman ranks for testing the interaction in a split-plot design as a robust alternative to parametric procedures. we demonstrated that the presence of a repeated measures main effect may reduce the power of interaction tests performed on friedman ranks. aligning the data before applying friedman ranks was shown to produce more statistical power than simply analyzing friedman ranks. results from a simulation study showed that aligning the data (i.e., removing main effects) before applying friedman ranks and then performing either a univariate or multivariate test can provide more statistical power than parametric tests if the error distributions are skewed.",
    "present_kp": [
      "friedman ranks",
      "split-plot design",
      "repeated measures",
      "interaction tests"
    ],
    "absent_kp": [
      "aligned ranks"
    ]
  },
  {
    "title": "exposing hpc and sequential applications as services through the development and deployment of a saas cloud.",
    "abstract": "a framework for the deployment of saas clouds aimed at supporting scientific research. a novel resource selection approach which can automate complex deployment methodologies such as cloud bursting. simplified deployment of software as a services through the publication of attributes. the automated development of hybrid hpc clouds. framework and cloud feasibility and performance proofs demonstrated through a bio-informatics workflow.",
    "present_kp": [
      "saas clouds"
    ],
    "absent_kp": [
      "hpc application service",
      "application deployment and exposure",
      "high performance computing"
    ]
  },
  {
    "title": "a combinatorial problem on trapezoidal words.",
    "abstract": "in this paper, we investigate some combinatorial properties concerning the family of the so-called trapezoidal words. trapezoidal words, considered in de luca (theoret. comput. sci. 218 (1999) 13-39) are finite words over the two-letter alphabet a = {a, b} whose subword complexity has the same behaviour as that of finite sturmian words. in de luca (theoret. comput. sci. 218 (1999) 13-39) it has been proved that the family of finite sturmian words is properly contained in that one of trapezoidal words. we carry on with the studying of the family of trapezoidal words and, in particular, of its relation with that one of finite sturmian words.",
    "present_kp": [
      "subword complexity",
      "sturmian words"
    ],
    "absent_kp": [
      "special factors"
    ]
  },
  {
    "title": "pattern-oriented associative rule-based patent classification.",
    "abstract": "this paper proposes an innovative pattern-oriented associative rule-based approach to construct automatic triz-based patent classification system. derived from associative rule-based text categorization, the new approach does not only discover the semantic relationship among features in a document by their co-occurrence, but also captures the syntactic information by manually generalized patterns. we choose 7 classes which address 20 of the 40 triz principles and perform experiments upon the binary set for each class. compared with three currently popular classification algorithms (svm, c4.5 and nb), the new approach shows some improvement. more importantly, this new approach has its own advantages, which were discussed in this paper as well.",
    "present_kp": [
      "triz",
      "triz principles",
      "patent classification",
      "text categorization"
    ],
    "absent_kp": [
      "associated rule-based approach"
    ]
  },
  {
    "title": "an evolutionary algorithm for pure fuzzy flowshop scheduling problems.",
    "abstract": "the pure flowshop scheduling problem is here investigated from a perspective considering me uncertainty associated with the execution of shop floor activities. being the flowshop problem is np complete, a large number of heuristic algorithms have been proposed in literature to determine an optimal solution. unfortunately, these algorithms usually assume a simplifying hypothesis: the problem data are assumed as deterministic, i.e. job processing times and the due dates are expressed through a unique value, which does not reflect the real process variability. for this reason, some authors have recently proposed the use of a fuzzy set theory to model the uncertainty in scheduling problems. in this paper, a proper genetic algorithm has been developed for solving the fuzzy flowshop scheduling problem. the optimisation involves two different objectives: the completion time minimisation and the due date fulfilment; both the single and multi-objective configurations have been considered. a new ranking criterion has been proposed and its performance has been tested through a set of test problems. a numerical analysis confirms the efficiency of the proposed optimisation procedure.",
    "present_kp": [
      "flowshop"
    ],
    "absent_kp": [
      "fuzzy scheduling",
      "optimisation algorithms"
    ]
  },
  {
    "title": "from cluster ensemble to structure ensemble.",
    "abstract": "this paper investigates the problem of integrating multiple structures which are extracted from different sets of data points into a single unified structure. we first propose a new generalized concept called structure ensemble for the fusion of multiple structures. unlike traditional cluster ensemble approaches the main objective of which is to align individual labels obtained from different clustering solutions, the structure ensemble approach focuses on how to unify the structures obtained from different data sources. based on this framework, a new structure ensemble approach called the probabilistic bagging based structure ensemble approach (bsea) is designed, which integrates the bagging technique, the force based self-organizing map (fbsom) and the normalized cut algorithm into the proposed framework. bsea views structures obtained from different datasets generated by the bagging technique as nodes in a graph, and adopts graph theory to find the most representative structure. in addition, the force based self-organizing map (fbsom), which is a generalized form of som, is proposed to serve as the basic clustering algorithm in the structure ensemble framework. finally, a new external index called correlation index (ci), which considers the correlation relationship of both the similarity and dissimilarity between the predicted solution and the true solution, is proposed to evaluate the performance of bsea. the experiments show that (i) the performance of bsea outperforms most of the state-of-the-art clustering approaches, and (ii) bsea performs well on datasets from the uci repository and real cancer gene expression profiles.",
    "present_kp": [
      "cluster ensemble",
      "structure ensemble"
    ],
    "absent_kp": []
  },
  {
    "title": "on the finite increment calculus method for stabilizing advection-diffusion equations, analysis and computation of the stabilization parameter.",
    "abstract": "in this work we are concerned with the finite increment calculus (fic) method. the method has been developed for efficient approximation of advection-diffusion equations with high peclet numbers. since the natural application of fic is within the framework of the fem, we consider the bvp in a weak sense on finite dimensional spaces. here we provide a result on existence and uniqueness of the solution as well as an error analysis. also we propose a choice of the stabilization parameter. we test the method on some troublesome 2d problems.",
    "present_kp": [
      "finite increment calculus",
      "advection-diffusion",
      "stabilization parameter",
      "peclet number"
    ],
    "absent_kp": [
      "sharp gradient",
      "stabilized-fem"
    ]
  },
  {
    "title": "term structure of interest rates and the expectation hypothesis: the euro area.",
    "abstract": "this paper investigates the informational content of the yield curve in the european market using data on the italian term structures. according to the expectation hypothesis theory (eht) the current forward rate equals the future short rate plus a constant risk premium that is time invariant but maturity dependent. this theory has been widely tested in the empirical literature providing various findings according to the country where it has been applied and to the segment of the yield curve examined or the period under study. the standard approach to test the eht uses the regression techniques assuming data on spot rates and their first differences to be stationary. recently an increasing number of studies evidenced the non stationarity of interest rates time series and some tests of the eht are formulated using term spread and forward-spot spread which are stationary. a new strand of literature suggests to investigate the eht using a restricted var framework. in this paper, following , we test if the expectation hypothesis holds using cointegration and error correction analysis. for the period under study results suggest that the long and short term interest rates are cointegrated and therefore subject to a long equilibrium path, providing evidence that the eht holds for the italian and the european market.",
    "present_kp": [
      "term structure of interest rates",
      "cointegration"
    ],
    "absent_kp": [
      "expectations hypothesis",
      "error correction model"
    ]
  },
  {
    "title": "towards content-based patent image retrieval: a framework perspective.",
    "abstract": "in this article, we discuss the potential benefits, the requirements and the challenges involved in patent image retrieval and subsequently, we propose a framework that encompasses advanced image analysis and indexing techniques to address the need for content-based patent image search and retrieval. the proposed framework involves the application of document image pre-processing, image feature and textual metadata extraction in order to support effectively content-based image retrieval in the patent domain. to evaluate the capabilities of our proposal, we implemented a patent image search engine. results based on a series of interaction modes, comparison with existing systems and a quantitative evaluation of our engine provide evidence that image processing and indexing technologies are currently sufficiently mature to be integrated in real-world patent retrieval applications.",
    "present_kp": [
      "patent",
      "retrieval",
      "search engine"
    ],
    "absent_kp": [
      "content-based search",
      "images",
      "drawings",
      "figures",
      "hybrid"
    ]
  },
  {
    "title": "development and application of a framework for evaluating multi-mode voting risks.",
    "abstract": "purpose - the purpose of this paper is to investigate the risks associated with online voting and to compare them with more traditional voting modes. design/methodology/approach - a modified version of the operationally critical threat, asset, and vulnerability evaluation (octave) approach from the cert coordination center (r) at carnegie-mellon university is used for developing a framework for comparing threats for different stakeholders. in addition, these risks and threats are quantified, offering an opportunity to conduct a multi-mode risk analysis in a manner independent of the underlying voting modes. the framework is exemplified using data from officials who had been involved in an actual municipal election, in which registered voters were given the option of voting through the internet. findings - what is instructive in the context of this study is that the \"low-tech\" threats such as large-scale mail theft of election notifications and family member coercion may in fact be significant for internet voting, and the sensationalized threats mentioned by the media may pale in comparison in terms of vulnerabilities. research limitations/implications - conclusions drawn from applying the methods may be very sensitive to parameters chosen for quantification, especially since estimates of probabilities of threats may vary in order of magnitude. originality/value - this paper demonstrates a quantitative and comparative analysis for internet voting, something which does not seem to be adequately addressed in the literature.",
    "present_kp": [
      "internet"
    ],
    "absent_kp": [
      "elections",
      "canada"
    ]
  },
  {
    "title": "e-learning multimedia applications: towards an engineering of content creation.",
    "abstract": "in the same manner that e-learning applications are becoming increasingly important at the university, there are still some critical questions that should be solved with the objective of making use of the potential offered by current web technologies. the creation of contents that are able of capturing the attention of interest of the students and their disposal in an appropriate way constitute the main purpose of this work. the teaching content engineering expounded shows the different stages that should form part of the process. a development team, composed of different professional profiles, will work together with the lecturers of the subject to which the contents are been created, i.e. multimedia videos and interactive applications. this process should be developed according to a methodology that assure the use of appropriate resources, all that tasks -suitable of being- should be modularized and factorized. this paper presents the acquired experience in the development and use of multimedia contents for e-learning applications, created for some of the subjects of the degree in computer science engineering. the deliveries of these contents make use of internet and video streaming techniques. the result of the work shows the students satisfaction, including their comments.",
    "present_kp": [
      "teaching content engineering"
    ],
    "absent_kp": [
      "didactic objectives",
      "active learning",
      "passive learning"
    ]
  },
  {
    "title": "interpretation of partial least-squares regression models with varimax rotation.",
    "abstract": "the varimax rotation for factor analysis is used to orthogonally transform the factor subspace, resulting from partial least-square regression (plsr). if the factors are nearly orthogonal, the transformation may help to interpret the physical meaning of each factor without altering the results of a plsr model. a case study shows that after the varimax rotation, the loading matrix satisfies the simple structure criterion and improves its explanatory ability.",
    "present_kp": [
      "partial least-squares regression",
      "factor subspace",
      "varimax rotation",
      "factor analysis"
    ],
    "absent_kp": []
  },
  {
    "title": "monitoring schedules for randomly deployed sensor networks.",
    "abstract": "given n sensors and m targets, a monitoring schedule is a partition of the sensor set such that each part of the partition can monitor all targets. monitoring schedules are used to maximize the time all targets are monitored when there is no possibility of replacing the batteries of the sensors. each part of the partition is used for one unit of time, and thus the goal is to maximize the number of parts in the partition. we present distributed algorithms for monitoring schedule under the following assumptions: 1) identical sensors can each monitor all targets within a certain radius, 2) the n sensors are randomly distributed uniformly in a large square containing the targets, 3) the number of sensors is high enough given the area the square, and 4) the communication range is twice the sensing range (thus any two sensors which can monitor the same target can communicate in one hop). our results hold with high probability. with the further assumptions that the sensors are capable (for example, by gps) of knowing their exact geographic position, and targets fill out the square, our schedule has at least (1-?) opt parts, where opt is the optimum solution. without geographic position we show that a previously proposed distributed algorithm can be modified to achieve a constant approximation ratio. our algorithms run in a polylogarithmic number of communication rounds, with the exact running time depending on assumptions on the information a sensor receives when packets collide.",
    "present_kp": [
      "monitoring schedule",
      "sensor network"
    ],
    "absent_kp": [
      "localized algorithm",
      "approximation algorithm"
    ]
  },
  {
    "title": "axial and off-axial dynamic transitions in uniaxially anisotropic heisenberg ferromagnet: a comparison.",
    "abstract": "uniaxially anisotropic heisenberg ferromagnet, in the presence of a magnetic field varying sinusoidally in time, is studied by monte carlo simulation. the axial (field applied only along the direction of anisotropy) and off-axial (field applied only along the direction which is perpendicular to the direction of anisotropy) dynamic transitions are studied. by studying the distribution of the dynamic order parameter component, it is observed that the axial transition is discontinuous for low anisotropy and becomes continuous in high anisotropy. the off axial transition is found to be continuous for all values of anisotropy. in the infinite anisotropy limit, both types of transitions are compared with that observed in an ising ferromagnet for the same value of the field and frequency. the infinitely anisotropic axial transition and dynamic transition in the ising ferromagnet occur at different temperatures, whereas the infinitely anisotropic off-axial transition and the equilibrium ferro-para transition in the ising model occur at the same temperature.",
    "present_kp": [
      "monte carlo simulation",
      "heisenberg ferromagnet"
    ],
    "absent_kp": [
      "dynamical phase transition",
      "uniaxial anisotropy"
    ]
  },
  {
    "title": "performance of a riskfree time warp operating system.",
    "abstract": "optimistic methods of synchronizing parallel discrete event simulations can be risky by sending (positive) messages (events) before they have been committed. risky methods often use anti-messages (negative messages) to cancel incorrectly sent positive messages. riskfree methods are more conservative, they do not send messages until they are known to be correct. the time warp operating system (twos) uses anti-messages. riskfree twos is implemented and tested on the standard twos benchmarks. performance of the riskfree twos is dependent on the amount of lookahead in the simulation. good lookahead was required for even reasonable performance. tracker, a simulation of the riskfree simulation, is used to give idealized best case riskfree performance. berisky is an example simulation which has a speedup of n for time warp, but only a speedup of 2 for riskfree methods.",
    "present_kp": [
      "warp",
      "speedup",
      "method",
      "use",
      "simulation",
      "benchmark",
      "event",
      "parallel",
      "message",
      "performance",
      "operating system",
      "case",
      "posit"
    ],
    "absent_kp": [
      "dependencies",
      "examples",
      "standardization",
      "timing"
    ]
  },
  {
    "title": "a real-time performance evaluation model for distributed software with reliability constrains.",
    "abstract": "in this paper, we propose an approach for the real-time performance analysis of distributed software with reliability constraints, called athena. the approach is based on the real-time and reliability performance analysis of distributed program. in athena, two important factors, imperfect nodes and the links reliability, are introduced. the algorithms proposed in athena generates sub-graphs, counts the reliability of each sub-graph, calculates the transmission time for all the transmission paths of each data file, and computes response time of each data file with reliability constraint. in this way, the real-time performance of distributed software with reliability constrains can be evaluated.",
    "present_kp": [
      "distributed software",
      "reliability",
      "real-time",
      "performance evaluation"
    ],
    "absent_kp": []
  },
  {
    "title": "improving the scalability of parallel n-body applications with an event-driven constraint-based execution model.",
    "abstract": "the scalability and efficiency of graph applications are significantly constrained by conventional systems and their supporting programming models. technology trends such as multicore, manycore, and heterogeneous system architectures are introducing further challenges and possibilities for emerging application domains such as graph applications. this paper explores the parallel execution of graphs that are generated using the barnes-hut algorithm to exemplify dynamic workloads. the workloads are expressed using the semantics of an exascale computing execution model called parallex. for comparison, results using conventional execution model semantics are also presented. we find improved load balancing during runtime and automatic parallelism discovery by using the advanced semantics for exascale computing.",
    "present_kp": [
      "n-body",
      "parallex",
      "barnes-hut"
    ],
    "absent_kp": [
      "parallelization"
    ]
  },
  {
    "title": "a lossy 3d wavelet transform for high-quality compression of medical video.",
    "abstract": "in this paper, we present a lossy compression scheme based on the application of the 3d fast wavelet transform to code medical video. this type of video has special features, such as its representation in gray scale, its very few interframe variations, and the quality requirements of the reconstructed images. these characteristics as well as the social impact of the desired applications demand a design and implementation of coding schemes especially oriented to exploit them. we analyze different parameters of the codification process, such as the utilization of different wavelets functions, the number of steps the wavelet function is applied to, the way the thresholds are chosen, and the selected methods in the quantization and entropy encoder. in order to enhance our original encoder, we propose several improvements in the entropy encoder: 3d-conscious run-length, hexadecimal coding and the application of arithmetic coding instead of huffman. our coder achieves a good trade-off between compression ratio and quality of the reconstructed video. we have also compared our scheme with mpeg-2 and ezw, obtaining better compression ratios up to 119% and 46%, respectively for the same psnr.",
    "present_kp": [
      "medical video",
      "3d fast wavelet transform"
    ],
    "absent_kp": [
      "telemedicine",
      "high-quality lossy compression"
    ]
  },
  {
    "title": "partitioning of unstructured grid meshes using boltzmann machine neural networks.",
    "abstract": "properly adapted boltzmann machine neural networks are used to devise effective unstructured grid partitioners that are capable of providing equally loaded grid subsets with minimum interface, for concurrent data-handling on parallel computers. the partitioning scheme is based on recursive bisections so that the outcome always consists of 2n partitions. two different techniques are introduced to speed up theotherwise costlypartitioning process and several variants are considered. in particular, a transformation of bipolar hopfield-type neural networks is developed providing an effective multi-scale approach. results on a number of test cases are presented in order to assess the performance of the proposed techniques.",
    "present_kp": [
      "neural networks",
      "boltzmann machine"
    ],
    "absent_kp": [
      "graph partitioning"
    ]
  },
  {
    "title": "a note on the hadwiger number of circular arc graphs.",
    "abstract": "the intention of this note is to motivate the researchers to study hadwiger's conjecture for circular arc graphs. let eta(g) denote the largest clique minor of a graph g, and let (x)(g) denote its chromatic number. hadwiger's conjecture states that eta(g) >= (x)(g) and is one of the most important and difficult open problems in graph theory. from the point of view of researchers who are sceptical of the validity of the conjecture, it is interesting to study the conjecture for graph classes where eta(g) is guaranteed not to grow too fast with respect to (x)(g), since such classes of graphs are indeed a reasonable place to look for possible counterexamples. we show that in any circular arc graph g, eta(g) <= 2(x)(g) - 1, and there is a family with equality. so, it makes sense to study hadwiger's conjecture for this family.",
    "present_kp": [
      "circular arc",
      "hadwiger's conjecture",
      "minor"
    ],
    "absent_kp": [
      "graph coloring",
      "combinatorial problems"
    ]
  },
  {
    "title": "a metadata driven approach to performing complex heterogeneous database schema migrations.",
    "abstract": "enterprise software is evolving at a faster rate than ever before with customers expecting upgrades to occur regularly. these upgrades not only have complex consequences for legacy software but the database upgrade also. this paper discusses the challenges associated with relational database schema migrations which commonly occur with major upgrade releases of enterprise software. the most prevalent method of performing a schema migration is to execute sql script files before or after the software upgrade. this approach performs poorly with large or complex database migrations and also requires separate script files for each supported database vendor. a tool was developed for a complex database upgrade of an enterprise product which uses xml in a metadata-driven approach. the key advantages include the ability to abstract complexity, provide multi-database vendor support and make the database migration more manageable between software releases. this marks an evolutionary step towards autonomic self-migrations.",
    "present_kp": [
      "autonomic",
      "database",
      "upgrades",
      "migrations",
      "metadata"
    ],
    "absent_kp": []
  },
  {
    "title": "on-line algorithms for computing mean and variance of interval data, and their use in intelligent systems.",
    "abstract": "when we have only interval ranges [ x ? i , x i ] of sample values x1,,xn, what is the interval [ v ? , v ] of possible values for the variance v of these values? there are quadratic time algorithms for computing the exact lower bound v on the variance of interval data, and for computing v under reasonable easily verifiable conditions. the problem is that in real life, we often make additional measurements. in traditional statistics, if we have a new measurement result, we can modify the value of variance in constant time. in contrast, previously known algorithms for processing interval data required that, once a new data point is added, we start from the very beginning. in this paper, we describe new algorithms for statistical processing of interval data, algorithms in which adding a data point requires only o(n) computational steps.",
    "present_kp": [
      "interval data",
      "mean",
      "variance"
    ],
    "absent_kp": [
      "on-line data processing"
    ]
  },
  {
    "title": "principles for designing data-/compute-intensive distributed applications and middleware systems for heterogeneous environments.",
    "abstract": "the nature of distributed systems is constantly and steadily changing as the hardware and software landscape evolves. porting applications and adapting existing middleware systems to ever changing computational platforms has become increasingly complex and expensive. therefore, the design of applications, as well as the design of next generation middleware systems, must follow a set of guiding principles in order to insure long-term survivability without costly re-engineering. from our practical experience, the key determinants to success in this endeavor are adherence to the following principles: (1) design for change; (2) provide for storage subsystem i/o coordination; (3) employ workload partitioning and load balancing techniques; (4) employ caching; (5) schedule the workload; and (6) understand the workload. in order to support these principles, we have collected extensive experimental results comparing three middleware systems targeted at data- and compute-intensive applications implemented by our research group during the course of the last decade, on a single data- and compute-intensive application. the main contribution of this work is the analysis of a level playing field, where we discuss and quantify how adherence to these guiding principles impacts overall system throughput and response time.",
    "present_kp": [
      "middleware systems",
      "heterogeneous environments"
    ],
    "absent_kp": [
      "data-/compute-intensive applications",
      "computational science applications"
    ]
  },
  {
    "title": "the use of mixtures for dealing with non-normal regression errors.",
    "abstract": "in many situations, the distribution of the error terms of a linear regression model departs significantly from normality. it is shown, through a simulation study, that an effective strategy to deal with these situations is fitting a regression model based on the assumption that the error terms follow a mixture of normal distributions. the main advantage, with respect to the usual approach based on the least-squares method is a greater precision of the parameter estimates and confidence intervals. for the parameter estimation we make use of the em algorithm, while confidence intervals are constructed through a bootstrap method.",
    "present_kp": [
      "em algorithm"
    ],
    "absent_kp": [
      "kurtosis",
      "location-scale mixtures",
      "normal probability plot",
      "residual analysis",
      "skewness",
      "switching regression"
    ]
  },
  {
    "title": "genetic evolution of nonlinear material constitutive models.",
    "abstract": "material constitutive model is highly nonlinear and multimodal in the large parameter space. a genetic evolution algorithm is thus proposed for its recognition. the nonlinear stressstrain relationship presented by several added multinomials, whose structure is not simplified, is automatically recognized from global response information, e.g., load vs. reflection data, obtained from a structure test through genetic evolution in global space. nonlinear finite element analysis is used as a bridge to build a relationship between stressstrain data and loaddeflection information. the potential of the proposed method is demonstrated by applying it to the macro-mechanical modeling of nonlinear behavior in advanced composite materials. a nonlinear material model for the ply is recognized using experimental results on a lamina plate s to be a modification of hahntsai model . the obtained nonlinear constitutive model is subsequently used to predict nonlinear behaviors of the [(30)6]s and [(0/45)4]s plates. the results are satisfying. this modeling procedure can be used as a method to guide to improve analysis of nonlinear behavior and damage of composite materials.",
    "present_kp": [
      "composite"
    ],
    "absent_kp": [
      "genetic algorithm",
      "constitutive equations"
    ]
  },
  {
    "title": "a review on gabor wavelets for face recognition.",
    "abstract": "due to the robustness of gabor features against local distortions caused by variance of illumination, expression and pose, they have been successfully applied for face recognition. the facial recognition technology (feret) evaluation and the recent face verification competition (fvc2004) have seen the top performance of gabor feature based methods. this paper aims to give a detailed survey of state of the art 2d face recognition algorithms using gabor wavelets for feature extraction. existing problems are covered and possible solutions are suggested.",
    "present_kp": [
      "gabor wavelets",
      "face recognition"
    ],
    "absent_kp": [
      "joint timefrequency analysis"
    ]
  },
  {
    "title": "on the ideals of secant varieties of segre varieties.",
    "abstract": "we establish basic techniques for determining the ideals of secant varieties of segre varieties. we solve a conjecture of garcia, stillman, and sturmfels on the generators of the ideal of the first secant variety in the case of three factors and solve the conjecture set-theoretically for an arbitrary number of factors. we determine the low degree components of the ideals of secant varieties of small dimension in a few cases.",
    "present_kp": [
      "secant variety"
    ],
    "absent_kp": [
      "algebraic complexity",
      "segre variety",
      "border rank"
    ]
  },
  {
    "title": "bem for dynamic analysis using compact supported radial basis functions.",
    "abstract": "this paper presents a novel dynamic modelling of structures. the main idea of the present formulation is to re-formulate the dual reciprocity method (drm) using compact supported radial basis functions (cs rbf). the fictitious displacement and traction particular solution kernels are derived, for the first time, for four classes of cs rbf. additional complementary solutions are derived to replace the particular solutions outside the zone of the compact radius (?). the continuity of the two solutions and their derivatives up to the third derivatives are ensured along the circumference of the compact supported edge circle. the present formulation is general, as proved validity for modelling structures bounded by interior and exterior domains (structures in infinite domains) which have never been studied previously using the drm.",
    "present_kp": [
      "compact supported radial basis functions",
      "structures",
      "dual reciprocity method",
      "infinite domains"
    ],
    "absent_kp": [
      "boundary element method",
      "dynamics"
    ]
  },
  {
    "title": "the construction of possibility measures from samples on t-semi-partitions.",
    "abstract": "we address the (generalized) extension problem for possibility measures: given a map defined on a family of(fuzzy) sets, is it possible to extend it to a (generalized) possibility measure? the extension problem for possibility measures is known to be equivalent to a system of sup-t equations, with t a t-norm. a key role is played by the greatest solution (of type inf-j, with j a border implicator). when the family of sets considered is a semi-partition, another important solution (of type sup-t, with t a t-norm) can be identified. in the treatment of the generalized possibilistic extension problem, we show that a fuzzification of the greatest solution also plays a central role. on the other hand, an immediate fuzzification of the sup-t type solution is investigated. general necessary and sufficient conditions for this fuzzification to be a solution are established. this fuzzification is then further discussed in the case of a t-semi-partition or a t-partition. finally, we investigate possible criteria for extendability, inspired by wang's classical criterion of p-consistency.",
    "present_kp": [
      "p-consistency",
      "possibilistic extension problem",
      "possibility measure",
      "sup-t equation",
      "t-semi-partition"
    ],
    "absent_kp": []
  },
  {
    "title": "new exponential convergence on sicnns with time-varying leakage delays and neutral type distributed delays.",
    "abstract": "this paper is concerned with a shunting inhibitory cellular neural networks (sicnns) with time-varying leakage delays and neutral type continuously distributed delays. some criteria are established for the existence and global exponential stability of pseudo almost periodic solutions for the addressed model with pseudo almost periodic coefficients and delays. moreover, without assuming pseudo almost periodicity on coefficients and delays, a novel criterion is also given to ensure that all solutions of the considered neural networks converge exponentially to zero vector, which is new and complement previously known results.",
    "present_kp": [
      "shunting inhibitory cellular neural networks",
      "neutral type",
      "leakage delay",
      "pseudo almost periodic solution",
      "exponential convergence",
      ""
    ],
    "absent_kp": []
  },
  {
    "title": "an instructional design of open source networking laboratory and curriculum.",
    "abstract": "computer networking is one of the most challenging subjects to learn and to teach in an efficient way. on one hand, students may find the topic too technical and dry when presented. on the other hand, most it instructors still primarily use lectures as the exclusive means to teach. as stated in sigite computing curricula, it is strongly recommended to incorporate hands-on lab components into teaching as they help students apply the theory to solve real-world problems. in this paper, we designed and implemented open source lab and curriculum that cover a set of learning outcomes recommended by sigite it computing curricula. the design is guided by one of most commonly used instructional design models - dick and carey model. this research is aimed at producing students that can configure, maintain and troubleshoot the network from a \"hands-on\" perspective. in addition, the labs developed in this research, which map the sigite it model curricula, will be helpful in the efforts to pursue abet accreditation",
    "present_kp": [
      "curriculum",
      "network",
      "laboratory"
    ],
    "absent_kp": []
  },
  {
    "title": "complex patterns.",
    "abstract": "we outline some results of our current research on developing a methodology for solving problems of spatio-temporal reasoning. we consider classifiers for complex concepts in spatio-temporal reasoning that are constructed hierarchically. we emphasise the fact that the construction of such hierarchical classifiers should be supported by domain knowledge. approximate reasoning networks (ar networks) are proposed for approximation of reasoning schemes expressed in natural language. such reasoning schemes are extracted from knowledge bases representing domain knowledge. this approach makes it possible to induce classifiers for complex concepts by constructing them along schemes of reasoning extracted from domain knowledge.",
    "present_kp": [
      "pattern",
      "spatio-temporal reasoning",
      "classifier"
    ],
    "absent_kp": [
      "complex object",
      "concept approximation",
      "rough sets",
      "network of classifiers"
    ]
  },
  {
    "title": "a matrixmatrix multiplication methodology for single/multi-core architectures using simd.",
    "abstract": "in this paper, a new methodology for speeding up matrixmatrix multiplication using single instruction multiple data unit, at one and more cores having a shared cache, is presented. this methodology achieves higher execution speed than atlas state of the art library (speedup from 1.08 up to 3.5), by decreasing the number of instructions (load/store and arithmetic) and the data cache accesses and misses in the memory hierarchy. this is achieved by fully exploiting the software characteristics (e.g. data reuse) and hardware parameters (e.g. data caches sizes and associativities) as one problem and not separately, giving high quality solutions and a smaller search space.",
    "present_kp": [
      "matrixmatrix multiplication",
      "data cache",
      "multi-core",
      "simd"
    ],
    "absent_kp": [
      "cache associativity",
      "memory management"
    ]
  },
  {
    "title": "interaction in asynchronous discussion forums: peer facilitation techniques.",
    "abstract": "peer facilitation is proposed as a solution to counter limited interaction in asynchronous online discussions. however, there is a lack of empirical research on online peer facilitation. this study identifies, through cross-case comparison of two graduate-level blended courses attended by asian pacific students, the actual peer facilitation techniques that could encourage online interaction. analyses of interviews and online discussion transcripts suggest that techniques such as showing appreciation and considering others' viewpoints encourage online interaction. however, instructors intending to incorporate peer-facilitated online discussions should also consider the influence of factors such as the design of the online discussion activity and learners' cultural background as some participants could consider challenging others' ideas culturally inappropriate and need to be encouraged through techniques such as general invitation to contribute. facilitators might also re-consider the use of certain traditionally recommended strategies such as directing an online message at specific participants to encourage responses. this study suggests that doing so could sometimes backfire and discourage online contributions.",
    "present_kp": [
      "interaction",
      "peer facilitation techniques"
    ],
    "absent_kp": [
      "asynchronous online discussion forums",
      "knowledge construction",
      "participation",
      "student facilitators"
    ]
  },
  {
    "title": "soil-pipe interaction due to tunnelling: assessment of winkler modulus for underground pipelines.",
    "abstract": "one of the key problems in the implementation of winkler models to analyze tunnelling effects on existing pipelines lies in the assessment of subgrade modulus under external soil displacement. in this paper, an expression of the winkler subgrade modulus for a pipeline buried at arbitrary depth and subjected to free soil displacement with arbitrary curve shape is given. using superposition principle and the fourier integral, the subgrade modulus of an infinite beam resting on the surface of an elastic half space and buried infinitely are obtained respectively. then the influence of embedment depth is estimated based on mindlin and kelvin solution. the validity of the proposed subgrade modulus is verified by comparison with the results from an elastic continuum solution and two centrifuge model tests for the responses of buried pipeline due to nearby tunnelling. thereafter, parametric studies are shown to assess the accuracy of the proposed subgrade modulus by comparing with an elastic continuum solution in homogeneous and non-homogeneous soil stratum and the amount of error is estimated.",
    "present_kp": [
      "winkler subgrade modulus",
      "underground pipeline"
    ],
    "absent_kp": [
      "tunnel excavation",
      "soil movement"
    ]
  },
  {
    "title": "generalized jeffreys rule of conditioning and evidence combining rule for a priori probabilistic knowledge in conditional evidence theory.",
    "abstract": "in bayesian probabilistic approach for uncertain reasoning, one basic assumption is that a priori knowledge about the uncertain variable is modeled by a probability distribution. when new evidence representable by a constant set is available, the bayesian conditioning is used to update a priori knowledge. in the conventional d-s evidence theory, all bodies of evidence about the uncertain variable are imprecise and uncertain. all bodies of evidence are combined by so-called dempsters rule of combination to achieve a combined body of evidence without considering a priori knowledge. from our point of view, when identifying the true value of an uncertain variable, bayesian approach and evidence theory can cooperate to deal with uncertain reasoning. firstly all imprecise and uncertain bodies of evidence about the uncertain variable are fused to achieve a combined evidence based on a priori knowledge, then the a posteriori probability distribution is achieved from a priori probability distribution by conditioning on the combined evidence. in this paper we firstly deal with the knowledge updating problem where a priori knowledge is represented by a probability distribution and new evidence is represented by a random set. then we review the conditional evidence theory which resolves the knowledge combining problem based on a priori probabilistic knowledge. finally we discuss the close relationship between knowledge updating procedure and knowledge combining procedure presented in this paper. we show that a posteriori probability conditioned on fused body of evidence satisfies the bayesian parallel combination rule.",
    "present_kp": [
      "random set",
      "knowledge updating",
      "knowledge combining",
      "generalized jeffreys rule",
      "bayesian parallel combination rule",
      "conditional evidence theory"
    ],
    "absent_kp": [
      "conditioned combination rule",
      "target identification"
    ]
  },
  {
    "title": "many-to-one throughput capacity of ieee 802.11 multihop wireless networks.",
    "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multihop networks, in which many sources send data to a sink. an example of a practical scenario is that of a multihop mesh network connecting source and relay nodes to an internet gateway. in the trivial case where all source nodes are just one hop from the sink, the system throughput can approach l(s), where l(s) is the throughput capacity of an isolated link consisting of just one transmitter and one receiver. in the nontrivial case where some source nodes are more than one hop away, one can still achieve a system throughput of l(s) by sacrificing and starving the non-one-hop source nodes-however, this degenerates to an unacceptable trivial solution. we could approach the problem by the following partitioning: preallocate some link capacity al(s) (0 <= a <= 1) at the sink to the one-hop source nodes and then determine the throughput for the source nodes that are two or more hops away based on the remaining capacity l = (1 - a)l(s). the throughput of the one-hop nodes will be around al(s). this paper investigates the extent to which the remaining capacity l can be used efficiently by the source traffic that is two or more hops away. we find that for such source traffic, a throughput of l is not achievable under 802.11. we introduce the notion of \"canonical networks,\" a general class of regularly structured networks that allow us to investigate the system throughput by varying the distances between nodes and other operating parameters. when all links have equal length, we show that 2l/3 is the upper bound for general networks, including random topologies and canonical networks. when the links are allowed to have different lengths, we show that the throughput capacity of canonical networks has an analytical upper bound of 3l/4. the tightness of the bound is confirmed by simulations of 802.11 canonical networks, in which we obtain simulated throughputs of 0.74l when the source nodes are two hops away and 0.69l when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing typically achieve throughputs far below 3l/4. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved by more than 150 percent: indeed, in a dense network, deactivating some of the relay nodes near the sink can lead to a higher throughput.",
    "present_kp": [
      "many-to-one",
      "one-to-many",
      "802.11",
      "multihop networks"
    ],
    "absent_kp": [
      "wireless mesh networks",
      "data-gathering networks",
      "wi-fi"
    ]
  },
  {
    "title": "an examination of how a cross-section of academics use computer technology when writing academic papers.",
    "abstract": "a cross-section of 361 faculty, graduate and undergraduate students completed a survey that assessed computer availability, experience, attitudes toward computers, and use of computers while engaged in academic writing. overall, computers served as a tool for all participants, however, undergraduates in the math and computer science areas were more comfortable with computers than others. experience with computers increased with academic level, suggesting that academics currently use, and have been using, computers throughout their careers. generally, there were few differences as a function of discipline or gender. participants indicated different reasons for using computers during the process of writing academic papers relative to written/hard copies. their responses indicate that these two formats may facilitate the writing process in unique ways. rather than viewing continued use of hard copy as a transitional period to more extensive computer use, it may be that hard copy offers cognitive supports that may not be available in computer writing software.",
    "present_kp": [],
    "absent_kp": [
      "post-secondary education",
      "adult learning",
      "teaching/learning strategies",
      "media in education"
    ]
  },
  {
    "title": "computer attitude scales: how relevant today.",
    "abstract": "since the increase in the use of computers as learning tools, there have been many scales developed that can measure various aspects of computer attitude. the aim of this study is to examine the relevance of a number of these scales and determine whether they are still appropriate for use today. four computer attitude scales, which are used widely, are assessed. it was found that the scales are reliable. however, the degree of predictive validity for each scale varied and the various underlying constructs appear to reflect different aspects of attitude. in addition, these constructs had changed since the original creation of the scales. this suggests that the traditional style of computer attitude scale is no longer as relevant as when first developed, and judicious selection and care need to be exercised over their use and the interpretation of findings.",
    "present_kp": [
      "attitude scales",
      "validity"
    ],
    "absent_kp": [
      "computer attitudes",
      "reliability"
    ]
  },
  {
    "title": "a framework for the performance analysis of concurrent b-tree algorithms.",
    "abstract": "many concurrent b-tree algorithms have been proposed, but they have not yet been satisfactorily analyzed. when transaction processing systems require high levels of concurrency, a restrictive serialization technique on the b-tree index can cause a bottleneck. in this paper, we present a framework for constructing analytical performance models of concurrent b-tree algorithms. the models can predict the response time and maximum throughput. we analyze three algorithms: naive lock-coupling, optimistic descent, and the lehman-yao algorithm. the analyses are validated by simulations of the algorithms on actual b-trees. simple and instructive rules of thumb for predicting performance are also derived. we apply the analyses to determine the effect of database recovery on b-tree concurrency.",
    "present_kp": [
      " framework ",
      "tree",
      "recovery",
      "simulation",
      "systems",
      "performance model",
      "performance",
      "response time",
      "concurrency",
      "throughput",
      "process",
      "trees",
      "algorithm",
      "paper",
      "model",
      "transaction",
      "database",
      "effect",
      "performance analysis",
      "rules",
      "predict"
    ],
    "absent_kp": [
      "indexing",
      "couples",
      "locking"
    ]
  },
  {
    "title": "evaluating the performance of geographical locations within scientific networks using an aggregation-randomization-re-sampling approach (arr).",
    "abstract": "knowledge creation and dissemination in science and technology systems are perceived as prerequisites for socioeconomic development. the efficiency of creating new knowledge is considered to have a geographical component, that is, some regions are more capable in terms of scientific knowledge production than others. this article presents a method of using a network representation of scientific interaction to assess the relative efficiency of regions with diverse boundaries in channeling knowledge through a science system. in a first step, a weighted aggregate of the betweenness centrality is produced from empirical data (aggregation). the subsequent randomization of this empirical network produces the necessary null model for significance testing and normalization (randomization). this step is repeated to provide greater confidence about the results (re-sampling). the results are robust estimates for the relative regional efficiency of brokering knowledge, which is discussed along with cross-sectional and longitudinal empirical examples. the network representation acts as a straightforward metaphor of conceptual ideas from economic geography and neighboring disciplines. however, the procedure is not limited to centrality measures, nor is it limited to geographical aggregates. therefore, it offers a wide range of applications for scientometrics and beyond.",
    "present_kp": [
      "scientometrics",
      "geography"
    ],
    "absent_kp": [
      "collaboration"
    ]
  },
  {
    "title": "an incident information management framework based on data integration, data mining, and multi-criteria decision making.",
    "abstract": "an effective incident information management system needs to deal with several challenges. it must support heterogeneous distributed incident data, allow decision makers (dms) to detect anomalies and extract useful knowledge, assist dms in evaluating the risks and selecting an appropriate alternative during an incident, and provide differentiated services to satisfy the requirements of different incident management phases. to address these challenges, this paper proposes an incident information management framework that consists of three major components. the first component is a high-level data integration module in which heterogeneous data sources are integrated and presented in a uniform format. the second component is a data mining module that uses data mining methods to identify useful patterns and presents a process to provide differentiated services for pre-incident and post-incident information management. the third component is a multi-criteria decision-making (mcdm) module that utilizes mcdm methods to assess the current situation, find the satisfactory solutions, and take appropriate responses in a timely manner. to validate the proposed framework, this paper conducts a case study on agrometeorological disasters that occurred in china between 1997 and 2001. the case study demonstrates that the combination of data mining and mcdm methods can provide objective and comprehensive assessments of incident risks.",
    "present_kp": [
      "incident information management",
      "data integration",
      "data mining"
    ],
    "absent_kp": [
      "multiple criteria decision making",
      "decision support system"
    ]
  },
  {
    "title": "a butterfly effect: highly insecticidal resistance caused by only a conservative residue mutated of drosophila melanogaster acetylcholinesterase.",
    "abstract": "acetylcholinesterase (ache) and its mutation recently emerged as a significant research area, due to its resistance against organophosphate and carbamate insecticides. residue g265, which is always a conservative residue, mutated to a265 is the most frequent mutant of ache in drosophila populations. however, only this mutation caused a 'butterfly effect' that gives high insecticidal resistance. herein, the models of sensitive strain (dm-s) and the resistance strain (dm-r) were constructed, to give a total of 2000 ps molecular dynamics simulation and to reveal the insecticidal resistance mechanism, with implied, the active gorge of dm-r was much less flexible than that of dm-s. the \"back door\" channel was widened to accelerate the detoxication against insecticides by the conformation changing of w83 and i161. all the distances (s238-h480, s238-g150, s238-g151, y71-m153) in dm-r became smaller than those in dm-s, which may deeply influence the binding between the insecticides and dmache.",
    "present_kp": [
      "dmache",
      "insecticidal resistance",
      "molecular dynamics"
    ],
    "absent_kp": [
      "covalent docking"
    ]
  },
  {
    "title": "a high fidelity atm traffic and network simulator.",
    "abstract": "the design of an atm traffic and network (atm-tn) simulator which characterizes cell level network behavior is presented. the simulator incorporates three classes of atm traffic source models: an aggregate ethernet model, an mpeg model and a world wide webb transactions model. six classes of atm switch architectures are modeled including output buffered, shared memory buffered and cross bar switch models, and then multistage switches which can be built from these three basic models. the atm-tn simulator can be used to characterize arbitrary atm networks with dynamic multimedia traffic loads. call set up and tear down via atm signaling is implemented in addition to the various types of cell traffic streams generated by voice, video and data. the simulator is built on a simple, efficient simulation language called simkit which is capable of supporting both fast sequential and parallel execution. parallel execution is supported using warpkit, an optimistically synchronized kernel that is aimed at shared memory multiprocessor platforms such as the silicon graphics powerchallenge and sun spare 1000 series machines. the paper outlines general requirements for atm traffic and network simulation, presents an atm-tn simulator architecture, describes its major components and discusses the major issues associated with cell level atm modeling and simulation.",
    "present_kp": [
      "requirements",
      "network",
      "voice",
      "video",
      "platform",
      "simulation",
      "signaling",
      "design",
      "network simulation",
      "general",
      "graphics",
      "paper",
      "model",
      "traffic",
      "ethernet",
      "modeling and simulation",
      "component",
      "shared memory",
      "parallel",
      "aggregate",
      "behavior",
      "architecture",
      "dynamic",
      "language",
      "types",
      "multimedia",
      "data",
      "switch",
      "transaction",
      "class"
    ],
    "absent_kp": [
      "efficiency",
      "synchronization",
      "association",
      "shared memory multiprocessors"
    ]
  },
  {
    "title": "assessment of potential-based fluid finite elements for seismic analysis of dam-reservoir systems.",
    "abstract": "this paper assesses the use of a potential-based fluid finite element formulation to investigate earthquake excited dam-reservoir systems. the mathematical background of the analytical and numerical techniques is presented in a unified format. frequency and time-domain analyses are conducted to validate the potential-based finite element formulation. a case study of a typical dam-reservoir system subjected to earthquake loading is presented. the dynamic response of the system is discussed to illustrate the effects of fluid-structure interaction and reservoir bottom absorption. the validated potential-based fluid elements and boundary conditions are shown to perform adequately for practical seismic analysis of dam-reservoir systems.",
    "present_kp": [
      "finite elements",
      "fluid-structure interaction"
    ],
    "absent_kp": [
      "dam safety",
      "earthquake engineering",
      "frequency-domain response",
      "time-domain response"
    ]
  },
  {
    "title": "an improved particle swarm optimization for exponential stabilization of a singular linear time-varying system.",
    "abstract": "this paper derives an optimization problem for exponential stabilization condition of a singular linear time-varying system governed by the second-order vector differential equations and proposes an improved particle swarm optimization (pso) method, called the adaptive fuzzy pso with a constriction factor (afpso-cf) algorithm, for solving the optimization problem of exponential stabilization. the proposed afpso-cf algorithm adaptively adjusts the accelerating coefficients of pso by using the fuzzy set theory to improve global searching ability of controller parameters. compared with the standard particle swarm optimization (spso), the pso with a constriction factor (pso-cf), the quadratic interpolation pso (qipso), the unified pso (upso), the fully informed particle swarm (fips) and the comprehensive learning pso (clpso) algorithms, the experiment results show that the proposed method significantly performs better than those algorithms.",
    "present_kp": [
      "fuzzy",
      "pso",
      "time-varying system"
    ],
    "absent_kp": [
      "second-order singular system"
    ]
  },
  {
    "title": "fast insertion-based optimization of bounding volume hierarchies.",
    "abstract": "we present an algorithm for fast optimization of bounding volume hierarchies (bvh) for efficient ray tracing. we perform selective updates of the hierarchy driven by the cost model derived from the surface area heuristic. in each step, the algorithm updates a fraction of the hierarchy nodes to minimize the overall hierarchy cost. the updates are realized by simple operations on the tree nodes: removal, search and insertion. our method can quickly reduce the cost of the hierarchy constructed by the traditional techniques, such as the surface area heuristic. we evaluate the properties of the proposed method on fourteen test scenes of different complexity including individual objects and architectural scenes. the results show that our method can improve a bvh initially constructed with the surface area heuristic by up to 27% and a bvh constructed with the spatial median split by up to 88%.",
    "present_kp": [
      "bvh",
      "ray tracing"
    ],
    "absent_kp": [
      "surface area heuristics"
    ]
  },
  {
    "title": "mining frequent itemsets in time-varying data streams.",
    "abstract": "mining frequent itemsets in data streams is beneficial to many real-world applications but is also a challenging task since data streams are unbounded and have high arrival rates. moreover, the distribution of data streams can change over time, which makes the task of maintaining frequent itemsets even harder. in this paper, we propose a false-negative oriented algorithm, called twim, that can find most of the frequent itemsets, detect distribution changes, and update the mining results accordingly. experimental results show that our algorithm performs as good as other false-negative algorithms on data streams without distribution change, and has the ability to detect changes over time-varying data streams in -time with a high accuracy rate.",
    "present_kp": [
      "data stream",
      "frequent itemset"
    ],
    "absent_kp": []
  },
  {
    "title": "cumulative distribution function of a geometric poisson distribution.",
    "abstract": "the geometric poisson distribution (also called plya-aeppli) is a particular case of the compound poisson distribution. we propose to express the general term of this distribution through a recurrence formula leading to a linear algorithm for the computation of its cumulative distribution function. practical implementation with a special care for numerical computations is proposed and validated. the article ends with an example of application of these results for the computation of pattern statistics in biological sequences modelized by a markov model.",
    "present_kp": [
      "compound poisson"
    ],
    "absent_kp": [
      "polya-aeppli",
      "confluent hypergeometric function",
      "kummer"
    ]
  },
  {
    "title": "cross-validation approximation in functional linear regression.",
    "abstract": "cross-validation has been widely used in the context of statistical linear models and multivariate data analysis. recently, technological advancements give possibility of collecting new types of data that are in the form of curves. statistical procedures for analysing these data, which are of infinite dimension, have been provided by functional data analysis. in functional linear regression, using statistical smoothing, estimation of slope and intercept parameters is generally based on functional principal components analysis (fpca), that allows for finite-dimensional analysis of the problem. the estimators of the slope and intercept parameters in this context, proposed by hall and hosseini-nasab , are based on fpca, and depend on a smoothing parameter that can be chosen by cross-validation. the cross-validation criterion, given there, is time-consuming and hard to compute. in this work, we approximate this cross-validation criterion by such another criterion so that we can turn to a multivariate data analysis tool in some sense. then, we evaluate its performance numerically. we also treat a real dataset, consisting of two variables; temperature and the amount of precipitation, and estimate the regression coefficients for the former variable in a model predicting the latter one.",
    "present_kp": [
      "cross-validation",
      "functional data analysis",
      "regression",
      ""
    ],
    "absent_kp": [
      "eigenfunction",
      "eigenvalue",
      "stochastic expansion"
    ]
  },
  {
    "title": "subjective quality evaluation via paired comparison: application to scalable video coding.",
    "abstract": "scalable video coding is a powerful solution for content delivery in many interactive multimedia services due to its adaptability to varying terminal and network constraints. in order to successfully exploit such adaptability, it is necessary to understand users' preference among various scalability options and consequently develop an optimal bit rate adaptation strategy. in this paper, we present a study of subjective quality assessment of scalable video coding, which investigates the influence of the combination of scalability options on perceived quality with the goal of providing guidelines for an adaptive strategy that selects the optimal combination for a given bandwidth constraint. in particular, the study is based on paired comparison of stimuli that is suitable for our goal due to its simplicity and easiness. we propose a new method, called paired evaluation via analysis of reliability (pear), which analyzes paired comparison results and produces not only quality scores but also intuitive measures of confidence of the scores for significance analysis. results and analysis of extensive subjective tests for two different scalable video codecs and high definition contents are described, from which general consistent conclusions are drawn. the video and subjective data used in the paper are publicly available to the research community.(1)",
    "present_kp": [
      "paired comparison",
      "scalable video coding",
      "subjective test"
    ],
    "absent_kp": [
      "bradley-terry model",
      "content distribution",
      "multimedia quality assessment"
    ]
  },
  {
    "title": "comparison of semi-analytical and fem solutions in a certain magnetic fluid dynamics problem.",
    "abstract": "purpose - to provide a new semi-analytical procedure which is much faster than fem and for this reason can be applied in a reconstruction of an interface between two conducting fluids (magnetic fluid dynamics problem) by means of magnetic field tomography. design/methodology/approach - three approaches are compared: a simple analytical solution (as1), a modified semi-analytical solution (as2), and the finite element method solution. the modified semi-analytical approach takes into account an information about azimuthal spatial harmonics received from the fourier analysis of magnetic flux density distributions calculated by fem. as1 and as2 have been compared for different modes of the interface using fem solution as a reference. findings - it is shown that for small perturbations the as2 in every case provides smaller errors than as1 although for some modes (14,24) the quality of the solution is still not satisfactory. originality/value - this paper describes a new technique for the analysis of electromagnetic field which can be also applied in other problems.",
    "present_kp": [
      "fluid dynamics",
      "magnetic"
    ],
    "absent_kp": [
      "finite element analysis"
    ]
  },
  {
    "title": "privacy protection in pervasive systems: state of the art and technical challenges.",
    "abstract": "pervasive and mobile computing applications are dramatically increasing the amount of personal data released to service providers as well as to third parties. data includes geographical and indoor positions of individuals, their movement patterns as well as sensor-acquired data that may reveal individuals physical conditions, habits, and, in general, information that may lead to undesired consequences like unsolicited advertisement or more serious ones like discrimination and stalking. in this survey paper, at first we consider representative classes of pervasive applications, and identify the requirements they impose in terms of privacy and trade-off with service quality. then, we review the most prominent privacy preservation approaches, we discuss and summarize them in terms of the requirements. finally, we take a more holistic view of the privacy problem by discussing other aspects that turn out to be crucial for the widespread adoption of privacy enhancing technologies. we discuss technical challenges like the need for tools augmenting the awareness of individuals and to capture their privacy preferences, as well as legal and economic challenges. indeed, on one side privacy solutions must comply to ethical and legal requirements, and not prevent profitable business models, while on the other side it is unlikely that privacy preserving solutions will become practical and effective without new regulations.",
    "present_kp": [
      "pervasive applications"
    ],
    "absent_kp": [
      "data privacy",
      "anonymity",
      "obfuscation"
    ]
  },
  {
    "title": "combination of partially non-distinct beliefs: the cautious-adaptive rule.",
    "abstract": "the combination rule is critical in an evidence based fusion process. the conjunctive rule is most common eventhough when the cognitive independence - distinctness - assumption is often questionable. a new combination rule is tested here in both discrete and continuous cases, accounting for a partial non-distinctness between evidences. it is based on 'generalized discounting', that we define for separable basic belief assignments (bbas) or basic belief densities (bbds), to be applied to the source correlation derived from the cautious rule. this correlation can be specified in both considered cases of consonant bbas/bbds (as proposed by dubois et al.) and separable bbas/bbds (as proposed by denceux). then, the so-called 'cautious-adaptive' rule varies between the conjunctive rule and the cautious one, depending on the discounting level. in the gaussian case with standard deviation a, the evidence non-distinctness will be parameterized by a factor q is an element of [0, 1] dividing sigma. it leads to the generalized discounting needed in the cautious-adaptive formulation.",
    "present_kp": [
      "combination rule",
      "discounting"
    ],
    "absent_kp": [
      "evidence theory",
      "separable belief",
      "consonant belief"
    ]
  },
  {
    "title": "qos management in educational digital library environments.",
    "abstract": "advances in multimedia computing technologies offer new approaches to the support of computer-assisted education and training within many application domains. novel interactive presentation tools can be built to enhance traditional teaching methods with more active learning. since a variety of user expectations are possible in such an environment, research must address the incorporation of these factors into presentation tools. during an interactive learning/training process, presentation tools must be able to handle various types of delays. a flexibly adjustable quality of service (qos) should thus be supported. in this paper, we investigate a framework and systematic strategies for supporting the continuous and synchronized retrieval and presentation of multimedia data streams in a client/server distributed multimedia environment for educational digital libraries. specifically, we establish a practical framework for specifying multimedia objects, tasks, schedules, and synchronization constraints between media streams. we identify the qos parameters critical to the support of multimedia presentations for learning and training activities. based on the proposed framework and qos specifications, we develop presentation scheduling and buffer management strategies which can enforce the specified qos requirements in an educational digital library environment.",
    "present_kp": [
      "quality of service ",
      "multimedia",
      "synchronization",
      "digital libraries"
    ],
    "absent_kp": []
  },
  {
    "title": "towards a standard arabic information processing vocabulary (reprinted from computer standards and interfaces, vol 7, pg 343-348, 1988).",
    "abstract": "this paper is concerned with the development of a standard arabic information processing vocabulary. it has been considered that the development of such a vocabulary does not only involve the translation of information processing terms into arabic, but it also includes the provision of standard definitions for them. the paper establishes the necessary ground for the required plan by reviewing the special features of the information processing terms, and how they have been dealt with by various institutions and individuals, both internationally, and within the arab world. the proposed plan consists of several tasks, and considers the necessity of obtaining pan-arab recognition of the target vocabulary, and the importance of cooperation with international efforts.",
    "present_kp": [
      "information processing",
      "standards",
      "vocabulary",
      "terms",
      "definitions"
    ],
    "absent_kp": [
      "arabization",
      "computers"
    ]
  },
  {
    "title": "list version of l (d,s)-labelings.",
    "abstract": "let g = (v, e) be a simple graph, and for all v is an element of v, let l(v) be a list of colors assigned to v. we shall assume throughout that the colors are natural numbers. for nonnegative integers d, s, define chi(d, s)(l) (g) to be the smallest integer k such that for every list assignment with vertical bar l(v)vertical bar = k for all v is an element of v one can choose a color c(v) is an element of l(v) for every vertex in such a way that vertical bar c(v) - c(w)vertical bar >= d for all vw is an element of e and vertical bar c(v) - c(w)vertical bar >= s for all pairs v, w of vertices having distance 2 in g. for a given list assignment such a coloring c is called an l(d, s)-list labeling. we prove a general bound for chi(d, s)(l)(g) depending on the maximum degree of g. furthermore, we study this parameter for trees, and also for the particular classes of paths and stars. polynomial algorithms are designed for deciding whether a given list assignment admits an l (d, s) -list labeling on paths (for a given s unrestricted) and on trees (for s = 1).",
    "present_kp": [],
    "absent_kp": [
      "channel assignment problem",
      "l-labeling",
      "list coloring"
    ]
  },
  {
    "title": "computing highly accurate or exact p-values using importance sampling.",
    "abstract": "especially for discrete data, standard first order p-values can suffer from poor accuracy, even for quite large sample sizes. moreover, different test statistics can give practically different results. there are several approaches to computing p-values which do not suffer these defects, such as parametric bootstrap p-values or the partially maximised p-values ofberger and boos (1994). both these methods require computing the exact tail probability of the approximate p-value as a function of the nuisance parameter/s, known as the significance profile. for most practical problems, this is not computationally feasible. i develop an importance sampling approach to this problem. a major advantage is that significance can be simultaneously estimated at a grid of nuisance parameter values, without the need for smoothing away the simulation noise. the theory is fully developed for generalised linear models. the importance distribution is selected from the same generalised linear model family but with parameters biased towards an optimal point on the boundary of the tail-set. for logistic regression at least, standard guidelines for selecting the importance distribution can fail quite badly and a conceptually simple alternative algorithm for selecting these parameters is developed. this may have application to importance sampling more generally.",
    "present_kp": [
      "bootstrap",
      "logistic regression"
    ],
    "absent_kp": [
      "exact tests"
    ]
  },
  {
    "title": "understanding software project risk: a cluster analysis.",
    "abstract": "understanding software project risk can help in reducing the incidence of failure. building on prior work, software project risk was conceptualized along six dimensions. a questionnaire was built and 507 software project managers were surveyed. a cluster analysis was then performed to identify aspects of low, medium, and high risk projects. an examination of risk dimensions across the levels revealed that even low risk projects have a high level of complexity risk. for high risk projects, the risks associated with requirements, planning and control, and the organization become more obvious. the influence of project scope, sourcing practices, and strategic orientation on project risk dimensions was also examined. results suggested that project scope affects all dimensions of risk, whereas sourcing practices and strategic orientation had a more limited impact. a conceptual model of project risk and performance was presented.",
    "present_kp": [
      "software project risk",
      "cluster analysis",
      "scope"
    ],
    "absent_kp": [
      "project management",
      "outsourcing"
    ]
  },
  {
    "title": "cepstrum derived from differentiated power spectrum for robust speech recognition.",
    "abstract": "in this paper, cepstral features derived from the differential power spectrum (dps) are proposed for improving the robustness of a speech recognizer in presence of background noise. these robust features are computed from the speech signal of a given frame through the following four steps. first, the short-time power spectrum of speech signal is computed from the speech signal through the fast fourier transform algorithm. second, dps is obtained by differentiating the power spectrum with respect to frequency. third, the magnitude of dps is projected from linear frequency to the mel scale and smoothed by a filter bank. finally, the outputs of the filter bank are transformed to cepstral coefficients by the discrete cosine transform after a nonlinear transformation. it is shown that this new feature set can be decomposed as the superposition of the standard cepstrum and its nonlinearly liftered counterpart. while a linear lifter has no effect on the continuous density hidden markov model based speech recognition, we show that the proposed feature set embedded with a nonlinear liftering transformation is quite effective for robust speech recognition. for this, we conduct a number of speech recognition experiments (including isolated word recognition, connected digits recognition, and large vocabulary continuous speech recognition) in various operating environments and compare the dps features with the standard mel-frequency cepstral coefficient features used with cepstral mean normalization and spectral subtraction techniques.",
    "present_kp": [
      "robust speech recognition",
      "hidden markov model",
      "differential power spectrum",
      "linear liftering",
      "cepstral mean normalization",
      "spectral subtraction"
    ],
    "absent_kp": []
  },
  {
    "title": "lagrangian support vector regression via unconstrained convex minimization.",
    "abstract": "in this paper, a simple reformulation of the lagrangian dual of the 2-norm support vector regression (svr) is proposed as an unconstrained minimization problem. this formulation has the advantage that its objective function is strongly convex and further having only m variables, where m is the number of input data points. the proposed unconstrained lagrangian svr (ulsvr) is solvable by computing the zeros of its gradient. however, since its objective function contains the non-smooth plus function, two approaches are followed to solve the proposed optimization problem: (i) by introducing a smooth approximation, generate a slightly modified unconstrained minimization problem and solve it; (ii) solve the problem directly by applying generalized derivative. computational results obtained on a number of synthetic and real-world benchmark datasets showing similar generalization performance with much faster learning speed in accordance with the conventional svr and training time very close to least squares svr clearly indicate the superiority of ulsvr solved by smooth and generalized derivative approaches.",
    "present_kp": [
      "generalized derivative approach",
      "smooth approximation",
      "support vector regression",
      "unconstrained convex minimization"
    ],
    "absent_kp": []
  },
  {
    "title": "on fair and optimal multi-source ip-multicast.",
    "abstract": "we investigate the problem of maximizing multicast throughput under a fairness constraint. multiple server nodes wish to communicate to their intended set of client nodes over a shared network infrastructure. our goal is to devise distributed algorithms to construct multicast sessions, one for each server node, such that (a) the network infrastructure is optimally utilized and (b) the network resources are fairly distributed between multicast sessions, i.e., no individual session claims more than a prescribed share of the network bandwidth resources. we are particularly interested in multi-tree multicast strategies in which every multicast session may contain many multicast trees. we show how the use of multiple trees increases network throughput and the load distribution in the network. we propose a class of round-robin algorithms that are based on successive selection of multicast trees for each multicast session, in a loosely cooperative, yet distributed fashion. our best algorithm, the cooperative shortest path tree packing (csptp) algorithm, performs well in a variety of scenarios, ranging from very sparse to dense applications. through extensive simulations on random networks, we compare the performance of our algorithms with those commonly used in ip-multicast as well as theoretical upper bounds derived from network coding formulations. we show that the csptp can improve the throughput, and often achieves about 90% of the theoretical upper bound.",
    "present_kp": [],
    "absent_kp": [
      "ip multicast",
      "steiner tree packing",
      "minimum weight trees",
      "optimal throughput",
      "resource distribution"
    ]
  },
  {
    "title": "novel floating inductance and fdnr simulators employing ccii+s.",
    "abstract": "in this paper, a floating inductance and frequency-dependent negative resistance (fdnr) depending on the passive element selection is presented. the proposed circuit employs only plus-type second-generation current conveyors (ccii+s) as active elements, together with two resistors and two capacitors for realizing floating inductance and fdnr. both of the capacitors in the floating inductance realization are grounded. also, electronically tunable floating fdnr is obtained with the proposed circuit. the nonideality effects of the current conveyors on the proposed circuit are given. the proposed circuit is used in a low-pass ladder filter, and simulated with spice to exhibit its performance.",
    "present_kp": [
      "floating inductance and frequency-dependent negative resistance",
      "current conveyor"
    ],
    "absent_kp": [
      "inductor"
    ]
  },
  {
    "title": "the effect of the mean particle size on the dynamic behaviour of catalyzed olefin polymerization fluidized bed reactors.",
    "abstract": "recent developments in modelling gas-phase catalyzed olefin fluidized bed polymerization reactors are critically examined in order to quantify the effect of the mean particle size on the dynamic behaviour of the fbr a comprehensive population balance model is developed to investigate the effect of particle size distribution in gasphase fluidized bed olefin polymerization reactors. to assess the impact of mean particle size on the dynamic behaviour of an fbr, a new bubble-growth model is proposed. simulation results show that model predictions are strongly influenced by the choice of the mean particle diameter, since its values dramatically affects the main fluidization parameters (e.g., solids terminal velocity, maximum stable bubble size, minimum fluidization velocity and the heat and mass transfer coefficients) in the bed.",
    "present_kp": [
      "olefin polymerization",
      "fluidized bed reactor"
    ],
    "absent_kp": [
      "population balances"
    ]
  },
  {
    "title": "dea cross-efficiency evaluation based on pareto improvement.",
    "abstract": "we propose a cross-efficiency evaluation approach based on pareto improvement. our approach generates pareto-optimal cross efficiencies for the dmus. our approach has good power of improving the cross-efficiencies of the dmus. the cross-efficiency of each dmu will be equal to its self-evaluated efficiency. the cross-efficiencies of all dmus can be generated by only a common set of weights.",
    "present_kp": [
      "cross-efficiency evaluation",
      "pareto improvement",
      "pareto-optimal cross efficiencies"
    ],
    "absent_kp": [
      "data envelopment analysis "
    ]
  },
  {
    "title": "the cuidado music browser: an end-to-end electronic music distribution system.",
    "abstract": "the ist project cuidado, which ran from january 2001 to december 2003, produced the first entirely automatic chain for extracting and exploiting musical metadata for browsing music. the sony csl laboratory is primarily interested in the context of popular music browsing in large-scale catalogues. first, we are interested in human-centred issues related to browsing popular music. popular here means that the music accessed to is widely distributed, and known to many listeners. second, we consider popular browsing of music, i.e., making music accessible to non-specialists (music lovers), and allowing sharing of musical tastes and information within communities, departing from the usual, single user view of digital libraries. this research project covers all areas of the music-to-listener chain, from music descriptiondescriptor extraction from the music signal, or data mining techniquessimilarity based access and novel music retrieval methods such as automatic sequence generation, and user interface issues. this paper describes the scientific and technical issues at stake, and the results obtained.",
    "present_kp": [
      "metadata",
      "music browser",
      "similarity",
      "popular music"
    ],
    "absent_kp": [
      "cultural metadata",
      "acoustic metadata",
      "editorial metadata"
    ]
  },
  {
    "title": "work item tagging: communicating concerns in collaborative software development.",
    "abstract": "in collaborative software development projects, work items are used as a mechanism to coordinate tasks and track shared development work. in this paper, we explore how \"tagging,\" a lightweight social computing mechanism, is used to communicate matters of concern in the management of development tasks. we present the results from two empirical studies over 36 and 12 months, respectively, on how tagging has been adopted and what role it plays in the development processes of several professional development projects with more than 1,000 developers in total. our research shows that the tagging mechanism was eagerly adopted by the teams, and that it has become a significant part of many informal processes. different kinds of tags are used by various stakeholders to categorize and organize work items. the tags are used to support finding of tasks, articulation work, and information exchange. implicit and explicit mechanisms have evolved to manage the tag vocabulary. our findings indicate that lightweight informal tool support, prevalent in the social computing domain, may play an important role in improving team-based software development practices.",
    "present_kp": [
      "tagging",
      "software development",
      "articulation work",
      "work items"
    ],
    "absent_kp": [
      "collaboration",
      "task management"
    ]
  },
  {
    "title": "the optimal group consensus deviation measure for multiplicative preference relations.",
    "abstract": "consistency and consensus measures are two important procedures employed in group decision making with multiplicative preference relations (mprs). in this paper, we first establish the concept of individual consistency deviation degree between the original mpr and its optimal estimation. then we develop a group consensus deviation degree optimization model (gco model) by minimizing the weighted arithmetic average of individual consistency deviation degrees. our established theorems enlist the conditions for the existence of the optimal solution, the satisfactory solution, and non-inferior solution to the gco model. these results also provide an existence condition of redundant mpr in group decision making. additionally, we show that the optimal value function of the gco model converges to 0, which implies that the consensus degree of dms converges as the number of the decision makers increases indefinitely.",
    "present_kp": [
      "group decision making",
      "group consensus",
      "multiplicative preference relation",
      "satisfactory solution"
    ],
    "absent_kp": []
  },
  {
    "title": "robust object tracking with online multiple instance learning.",
    "abstract": "in this paper, we address the problem of tracking an object in a video given its location in the first frame and no other information. recently, a class of tracking techniques called \"tracking by detection\" has been shown to give promising results at real-time speeds. these methods train a discriminative classifier in an online manner to separate the object from the background. this classifier bootstraps itself by using the current tracker state to extract positive and negative examples from the current frame. slight inaccuracies in the tracker can therefore lead to incorrectly labeled training examples, which degrade the classifier and can cause drift. in this paper, we show that using multiple instance learning (mil) instead of traditional supervised learning avoids these problems and can therefore lead to a more robust tracker with fewer parameter tweaks. we propose a novel online mil algorithm for object tracking that achieves superior results with real-time performance. we present thorough experimental results (both qualitative and quantitative) on a number of challenging video clips.",
    "present_kp": [
      "multiple instance learning"
    ],
    "absent_kp": [
      "visual tracking",
      "online boosting"
    ]
  },
  {
    "title": "a numerical algorithm for the construction of efficient quadrature rules in two and higher dimensions.",
    "abstract": "we present a numerical algorithm for the construction of efficient, high-order quadratures in two and higher dimensions. quadrature rules constructed via this algorithm possess positive weights and interior nodes, resembling the gaussian quadratures in one dimension. in addition, rules can be generated with varying degrees of symmetry, adaptable to individual domains. we illustrate the performance of our method with numerical examples, and report quadrature rules for polynomials on triangles, squares, and cubes, up to degree 50. these formulae are near optimal in the number of nodes used, and many of them appear to be new.",
    "present_kp": [
      "gaussian quadrature",
      "triangle",
      "square",
      "cube"
    ],
    "absent_kp": [
      "multivariate integration",
      "point elimination method",
      "least squares newtons method"
    ]
  },
  {
    "title": "robust modeling for multivariate calibration transfer by the successive projections algorithm.",
    "abstract": "the use of the successive projections algorithm (vspa) to select variables for building robust transferable multiple linear regression (mlr) models is investigated. the robustness requirement is explicitly taken into account by introducing some spectra acquired by the slave instrument in the validation set that guides vspa selection. the transfer samples are selected by the classic kennard-stone (ks) algorithm or a modified version of vspa (sspa) that operates across the rows of the instrumental response matrix, instead of the columns. the proposed approach was tested in two data sets, each set consisting of spectra obtained by infrared spectrometry in two different instruments. the first data set consists of gasoline spectra, which are employed to predict the distillation temperature at which 90% of the sample has evaporated (t 90%). the second data set consists of corn spectra, which are employed for moisture determination. the robust mlr models were compared to a pls model employing piecewise direct standardization (pds) to correct the slave spectra. in both data sets, the mean prediction errors at the slave instrument for the robust vspa-mlr and the pls-pds models were comparable, and slightly better for vspa-mlr. the proposed approach appears to be a valid alternative to the commonly used pls-pds technique.",
    "present_kp": [
      "multivariate calibration transfer",
      "robust modeling",
      "successive projections algorithm",
      "piecewise direct standardization"
    ],
    "absent_kp": [
      "ft-ir spectrometry",
      "kennard-stone algorithm"
    ]
  },
  {
    "title": "a new subspace method for blind estimation of selective mimo-stbc channels.",
    "abstract": "in this paper, a new technique for the blind estimation of frequency and/or time-selective multiple-input multiple-output (mimo) channels under space-time block coding (stbc) transmissions is presented. the proposed method relies on a basis expansion model (bem) of the mimo channel, which reduces the number of parameters to be estimated, and includes many practical stbc-based transmission scenarios, such as stbc-orthogonal frequency division multiplexing (ofdm), space-frequency block coding (sfbc), time-reversal stbc, and time-varying stbc encoded systems. inspired by the unconstrained blind maximum likelihood (uml) decoder, the proposed criterion is a subspace method that efficiently exploits all the information provided by the stbc structure, as well as by the reduced-rank representation of the mimo channel. the method, which is independent of the specific signal constellation, is able to blindly recover the mimo channel within a small number of available blocks at the receiver side. in fact, for some particular cases of interest such as orthogonal stbc-ofdm schemes, the proposed technique blindly identifies the channel using just one data block. the complexity of the proposed approach reduces to the solution of a generalized eigenvalue (gev) problem and its computational cost is linear in the number of sub-channels. an identifiability analysis and some numerical examples illustrating the performance of the proposed algorithm are also provided.",
    "present_kp": [
      "space-time block coding ",
      "orthogonal frequency division multiplexing ",
      "space-frequency block coding ",
      "time-reversal stbc"
    ],
    "absent_kp": [
      "time-varying channels",
      "blind channel estimation",
      "second-order statistics "
    ]
  },
  {
    "title": "three-block bi-focal pls (3bif-pls) and its application in qsar.",
    "abstract": "when x and y are multivariate, the two-block partial least squares (pls) method is often used. in this paper, we outline an extension addressing a special case of the three-block (x/y/z) problem, where z sits \"under\" y. we have called this approach three-block bi-focal pls (3bif-pls). it views the x/y relationship as the dominant problem, and seeks to use the additional information in z in order to improve the interpretation of the y-part of the x/y association. two data sets are used to illustrate 3bif-pls. example i relates to single point mutants of haloalkane dehalogenase from sphingomonas paucimobilis ut26 and their ability to transform halogenated hydrocarbons, some of which are found as organic pollutants in soil. example ii deals with soil remediation capability of bacteria. whole bacterial communities are monitored over time using \"dna-fingerprinting\" technology to see how pollution affects population composition. since the data sets are large, hierarchical multivariate modelling is invoked to compress data prior to 3bif-pls analysis. it is concluded that the 3bif-pls approach works well. the paper contains a discussion of pros and cons of the method, and hints at further developmental opportunities.",
    "present_kp": [
      "qsar",
      "pls"
    ],
    "absent_kp": [
      "two-block pls",
      "three-block pls",
      "hierarchical modelling"
    ]
  },
  {
    "title": "spikes that count: rethinking spikiness in neurally embedded systems.",
    "abstract": "spiky neural networks are widely used in neural modeling, due to their biological relevance and high computational power. in this paper we investigate the usage of spiking dynamics in embedded artificial neural networks, that serve as a control mechanism for evolved autonomous agents performing a counting task. the synaptic weights and spiking dynamics are evolved using a genetic algorithm. we compare evolved spiky networks with evolved mccullochpitts networks, while confronting new questions about the nature of spikiness and its contribution to the neurocontroller's processing. we show that in a memory-dependent task, network solutions that incorporate spiking dynamics can be less complex and easier to evolve than networks involving mccullochpitts neurons. we identify and rigorously characterize two distinct properties of spiking dynamics in embedded agents: spikiness dynamic influence and spikiness functional contribution.",
    "present_kp": [
      "spiking",
      "counting"
    ],
    "absent_kp": [
      "evolutionary computation",
      "neurocontroller analysis"
    ]
  },
  {
    "title": "exploiting statistical correlations for proactive prediction of program behaviors.",
    "abstract": "this paper presents a finding and a technique on program behavior prediction. the finding is that surprisingly strong statistical correlations exist among the behaviors of different program components (e.g., loops) and among different types of program level behaviors (e.g., loop trip-counts versus data values). furthermore, the correlations can be beneficially exploited: they help resolve the proactivity-adaptivity dilemma faced by existing program behavior predictions, making it possible to gain the strengths of both approaches--the large scope and earliness of offline-profiling--based predictions, and the cross-input adaptivity of runtime sampling-based predictions. the main technique contributed by this paper centers on a new concept, seminal behaviors. enlightened by the existence of strong correlations among program behaviors, we propose a regression based framework to automatically identify a small set of behaviors that can lead to accurate prediction of other behaviors in a program. we call these seminal behaviors. by applying statistical learning techniques, the framework constructs predictive models that map from seminal behaviors to other behaviors, enabling proactive and cross-input adaptive prediction of program behaviors. the prediction helps a commercial compiler, the ibm xl c compiler, generate code that runs up to 45% faster (5%-13% on average), demonstrating the large potential of correlation-based techniques for program optimizations.",
    "present_kp": [
      "correlation",
      "program behavior"
    ],
    "absent_kp": []
  },
  {
    "title": "vsplit: a cross-layer architecture for v2i tcp services over 802.11.",
    "abstract": "this article proposes vsplit, a new architecture based on tcp cross-layering and splitting techniques for optimizing the transport layer performance in vehicular networks for internet-based vehicle-to-infrastructure (v2i) communications. our architecture mainly pretends to enhance the performance of tcp handovers in 802.11 networks. vsplit includes a cross-layer tcp protocol, called vsplit-tcp, that adapts the congestion control during the handover, learning the new characteristics of the network after the handover using the mechanisms provided by the ieee 802.21 media independent handover (mih) services. vsplit has been implemented and tested in the ns-3 simulator. we include the some of the most interesting performance evaluation results, which show a good performance of our proposal for the intended scenario.",
    "present_kp": [
      "vehicular network",
      "handover",
      "tcp",
      "802.21"
    ],
    "absent_kp": []
  },
  {
    "title": "toward structural sparsity: an explicit (ell _{2}/ell _0) approach.",
    "abstract": "as powerful tools, machine learning and data mining techniques have been widely applied in various areas. however, in many real-world applications, besides establishing accurate black box predictors, we are also interested in white box mechanisms, such as discovering predictive patterns in data that enhance our understanding of underlying physical, biological and other natural processes. for these purposes, sparse representation and its variations have been one of the focuses. more recently, structural sparsity has attracted increasing attentions. in previous research, structural sparsity was often achieved by imposing convex but non-smooth norms such as ({ell _{2}/ell _{1}}) and group ({ell _{2}/ell _{1}}) norms. in this paper, we present the explicit ({ell _2/ell _0}) and group ({ell _2/ell _0}) norm to directly approach the structural sparsity. to tackle the problem of intractable ({ell _2/ell _0}) optimizations, we develop a general lipschitz auxiliary function that leads to simple iterative algorithms. in each iteration, optimal solution is achieved for the induced subproblem and a guarantee of convergence is provided. furthermore, the local convergent rate is also theoretically bounded. we test our optimization techniques in the multitask feature learning problem. experimental results suggest that our approaches outperform other approaches in both synthetic and real-world data sets.",
    "present_kp": [
      "structural sparsity"
    ],
    "absent_kp": [
      "sparse learning",
      "multitask learning"
    ]
  },
  {
    "title": "analysis of the average execution time for a self-stabilizing leader election algorithm.",
    "abstract": "this paper focuses on the self-stabilizing leader election algorithm of xu and srimani [10] that finds a leader in a tree graph. the worst case execution time for this algorithm is o(n(4)), where n is the number of nodes in the tree. we show that the average execution time for this algorithm, assuming two different scenarios, is much lower than o(n(4)). in the first scenario, the algorithm assumes an equiprobable daemon and it only privileges a single node at a time. the average execution time for this case is o(n(2)). for the second case, the algorithm can privilege multiple nodes at a time. we eliminate the daemon from this algorithm by making random choices to avoid interference between neighboring nodes. the execution time for this case is o(n). we also show that for specific tree graphs, these results reduce even more.",
    "present_kp": [
      "leader election"
    ],
    "absent_kp": [
      "average execution time analysis",
      "self-stabilization"
    ]
  },
  {
    "title": "an optimal qos-based web service selection scheme.",
    "abstract": "quality-of-service (qos) in web services encompasses various non-functional issues such as performance, dependability and security, etc. as more and more web services become available, qos capability is becoming a decisive factor to distinguishing services. this study proposes an efficient service selection scheme to help service requesters select services by considering two different contexts: single qos-based service discovery and qos-based optimization of service composition. based on qos measurement metrics, this study proposes multiple criteria decision making and integer programming approaches to select the optimal service. experimental results show that the scheme is not only efficient, but also works well for complicated scenarios.",
    "present_kp": [
      "qos",
      "web services",
      "integer programming"
    ],
    "absent_kp": [
      "multi-objective optimization"
    ]
  },
  {
    "title": "a majorize-minimize subspace approach for l(2)-l(0) image regularization.",
    "abstract": "in this work, we consider a class of differentiable criteria for sparse image computing problems, where a nonconvex regularization is applied to an arbitrary linear transform of the target image. as special cases, it includes edge-preserving measures or frame-analysis potentials commonly used in image processing. as shown by our asymptotic results, the l(2) - l(0) penalties we consider may be employed to provide approximate solutions to l(0) penalized optimization problems. one of the advantages of the proposed approach is that it allows us to derive an efficient majorize-minimize subspace algorithm. the convergence of the algorithm is investigated by using recent results in nonconvex optimization. the fast convergence properties of the proposed optimization method are illustrated through image processing examples. in particular, its effectiveness is demonstrated on several data recovery problems.",
    "present_kp": [
      "nonconvex optimization"
    ],
    "absent_kp": [
      "truncated quadratic function",
      "edge preservation",
      "sparse representations",
      "majorize-minimize algorithms",
      "inverse problems",
      "denoising",
      "deblurring",
      "reconstruction",
      "kurdyka-lojasiewicz inequality"
    ]
  },
  {
    "title": "efficient longest executable path search for programs with complex flows and pipeline effects.",
    "abstract": "current development tools for embedded real-time systems do not efficiently support the timing aspect. the most important timing parameter for scheduling and system analysis is the worst-case execution time (wcet) of a program.this paper presents a fast and effective wcet calculation method that takes account of low-level machine aspects like pipelining and caches, and high-level program flow like loops and infeasible paths. the method is more efficient than previous path-based approaches, and can easily handle complex programs. by separating the low-level from the high-level analysis, the method is easy to retarget.experiments confirm that speed does not sacrifice precision, and that programs with extreme numbers of potential execution paths can be analyzed quickly.",
    "present_kp": [
      "wcet",
      "path search",
      "program flow"
    ],
    "absent_kp": [
      "embedded systems",
      "pipeline timing",
      "hard real-time"
    ]
  },
  {
    "title": "on the efficiency of semi-implicit and semi-lagrangian spectral methods for the calculation of incompressible flows.",
    "abstract": "classical semi-implicit backward euler/adams-baskforth time discretizations of the navier-stokes equations induce, for high-reynolds number flows, severe restrictions on the time step. such restrictions can be relaxed by using semi-lagrangian schemes essentially based on splitting the full problem into an explicit transport step and an implicit diffusion step. in comparison with the standard characteristics method, the semi-lagrangian method has the advantage of being much less cpu time consuming where spectral methods are concerned. this paper is devoted to the comparison of the 'semi-implicit' and 'semi-lagrangian' approaches, in terms of stability, accuracy and computational efficiency. numerical results on the advection equation, burger's equation and finally two- and three-dimensional navier-stokes equations, using spectral elements or a collocation method, are provided.",
    "present_kp": [
      "characteristics method",
      "high-reynolds number flows",
      "navier-stokes equations",
      "spectral methods"
    ],
    "absent_kp": [
      "semi-lagrangian methods"
    ]
  },
  {
    "title": "absolute and specific measures of research group excellence.",
    "abstract": "a desirable goal of scientific management is to introduce, if it exists, a simple and reliable way to measure the scientific excellence of publicly funded research institutions and universities to serve as a basis for their ranking and financing. while citation-based indicators and metrics are easily accessible, they are far from being universally accepted as way to automate or inform evaluation processes or to replace evaluations based on peer review. here we consider absolute measurements of research excellence at an amalgamated, institutional level and specific measures of research excellence as performance per head. using biology research institutions in the uk as a test case, we examine the correlations between peer review-based and citation-based measures of research excellence on these two scales. we find that citation-based indicators are very highly correlated with peer-evaluated measures of group strength, but are poorly correlated with group quality. thus, and almost paradoxically, our analysis indicates that citation counts could possibly form a basis for deciding on, how to fund research institutions, but they should not be used as a basis for ranking them in terms of quality.",
    "present_kp": [],
    "absent_kp": [
      "scientometrics",
      "scientific evaluation",
      "higher education"
    ]
  },
  {
    "title": "dips: an efficient pointer swizzling strategy for incremental uncaching environments.",
    "abstract": "pointer swizzling improves the performance of oodbmss by reducing the number of table lookups. however, the object replacement incurs the unswizzling overhead. in this paper, we propose a new pointer swizzling strategy, the dynamic indirect pointer swizzling (dips). dips dynamically applies pointer swizzling techniques in order to reduce the overhead of unswizzling. dips uses the temporal locality information which is gathered by the object buffer manager. the information is used to select the object to whose pointers the pointer swizzling techniques are applied and to dynamically bind the pointer swizzling techniques using the virtual function mechanism. we show the efficiency of the proposed strategy through experiments over various object buffer sizes and workloads.",
    "present_kp": [
      "pointer swizzling"
    ],
    "absent_kp": [
      "databases",
      "persistent objects",
      "smart pointers"
    ]
  },
  {
    "title": "power allocation associated with optimal cooperation level determination for half-duplex coded cooperative systems.",
    "abstract": "in this paper, we have developed two power allocation strategies in a half-duplex coded cooperative scheme under the transmit rate and the total power constraints, respectively. we further analyzed the effect of cooperation level and determined the optimal cooperation level based on these power allocation strategies.",
    "present_kp": [
      "power allocation",
      "half-duplex"
    ],
    "absent_kp": [
      "coded cooperation",
      "optimization of cooperation level"
    ]
  },
  {
    "title": "an robdd-based combinatorial method for the evaluation of yield of defect-tolerant systems-on-chip.",
    "abstract": "in this paper, we develop a combinatorial method for the evaluation of the functional yield of defect-tolerant systems-on-chip (soc). the method assumes that random manufacturing defects are produced according to a model in which defects cause the failure of given components of the system following a distribution common to all defects. the distribution of the number of defects is arbitrary. the yield is obtained by conditioning on the number of defects that result in the failure of some component and performing recursive computations over a reduced ordered binary decision diagram (robdd) representation of the fault-tree function of the system. the method has excellent error control. numerical experiments seem to indicate that the method is efficient and, with some exceptions, allows the analysis with affordable computational resources of systems with very large numbers of components.",
    "present_kp": [
      "combinatorial method",
      "defect-tolerant systems-on-chip ",
      "manufacturing defects",
      "reduced ordered binary decision diagram ",
      "yield"
    ],
    "absent_kp": []
  },
  {
    "title": "development and validation of a supg finite element scheme for the compressible navier-stokes equations using a modified inviscid flux discretization.",
    "abstract": "this paper considers the streamline-upwind petrov-galerkin (supg) method applied to the unsteady compressible navier-stokes equations in conservation-variable form. the spatial discretization, including a modified approach for interpolating the inviscid flux terms in the supg finite element formulation, and the second-order accurate time discretization are presented. the numerical method is discussed in detail. the performance of the algorithm is then investigated by considering inviscid flow past a circular cylinder. validation of the finite element formulation via comparisons with experimental data for high-mach number perfect gas laminar flows is presented, with a specific focus on comparisons with experimentally measured skin friction and convective heat transfer on a 15 degrees compression ramp.",
    "present_kp": [
      "supg",
      "finite element",
      "inviscid flux discretization",
      "validation"
    ],
    "absent_kp": [
      "compressible flows",
      "shock capturing"
    ]
  },
  {
    "title": "dynamic simulation assessment of collaboration strategies to manage demand gap in high-tech product diffusion.",
    "abstract": "there is no question that many high-tech supply chains operate in a context of high process and market uncertainties due to shorter product life cycles. when introducing a new product, these supply chains must manage the cost of supply, including the cost of capacity and inventories, with revenues from the products demand over its life cycle. however, in early phase of introduction after earlier buyers purchase, there might be a demand gap for a period followed by a sudden surge. to stay responsive and serve the market downstream after such gaps, two important decisions must be made: (a) the sizing of the capacity, and (b) the level of collaboration. it is the intention of this paper to show that the chosen level of collaboration effects significantly on managing the gap in the demand trajectory in new high-tech product diffusion. we study the impact of different collaboration strategies like vendor managed inventory (vmi), jointly managed inventory (jmi), and a collaborative planning, forecasting & replenishment (cpfr) model using system dynamics based simulation and compare the results with a non-collaborative chain. our results yield insights into effectiveness of collaboration in managing the dynamics of demand gap.",
    "present_kp": [
      "high-tech product diffusion",
      "collaboration strategies",
      "dynamic simulation"
    ],
    "absent_kp": []
  },
  {
    "title": "grobner bases with respect to several orderings and multivariable dimension polynomials.",
    "abstract": "let d = k [x] be a ring of ore polynomials over afield k and let a partition of the set of indeterminates into p disjoint subsets be fixed. considering d as a filtered ring with the natural p-dimensional filtration, we introduce a special type of reduction in a free d-module and develop the corresponding grobner basis technique (in particular, we obtain a generalization of the buchberger algorithm). using such a modification of the grobner basis method, we prove the existence of a hilbert-type dimension polynomial in p variables associated with a finitely generated filtered d-module, give a method of computation and describe invariants of such a polynomial. the results obtained are applied in differential algebra where the classical theorems on differential dimension polynomials are generalized to the case of differential structures with several basic sets of derivation operators.",
    "present_kp": [
      "ore polynomials",
      "grobner basis",
      "dimension polynomial"
    ],
    "absent_kp": [
      "differential ring",
      "differential module",
      "differential field extension"
    ]
  },
  {
    "title": "modification semantics in now-relative databases.",
    "abstract": "most real-world databases record time-varying information. in such databases, the notion of the current time, or now, occurs naturally and prominently. for example, when capturing the past states of a relation using begin and end time columns, tuples that are part of the current state have some past time as their begin time and now as their end time. while the semantics of such variable databases has been described in detail and is well understood, the modification of variable databases remains unexplored. this paper defines the semantics of modifications involving the variable now. more specifically, the problems with modifications in the presence of now are explored, illustrating that the main problems are with modifications of tuples that reach into the future. the paper defines the semantics of modificationsincluding insertions, deletions, and updatesof databases without now, with now, and with values of the type now+?, where ? is a non-variable time duration. to accommodate these semantics, three new timestamp values are introduced. finally, implementation is explored. we show how to represent the variable now with columns of standard sql data types and give a mapping from sql on now-relative data to standard sql on these columns. the paper thereby completes the semantics, the querying, and the modification of now-relative databases.",
    "present_kp": [
      "sql",
      "now",
      "updates"
    ],
    "absent_kp": [
      "temporal data",
      "temporal query language",
      "now-relative information"
    ]
  },
  {
    "title": "determining the best suited semantic events for cognitive surveillance.",
    "abstract": "state-of-the-art systems on cognitive surveillance identify and describe complex events in selected domains, thus providing end-users with tools to easily access the contents of massive video footage. nevertheless, as the complexity of events increases in semantics and the types of indoor/outdoor scenarios diversify, it becomes difficult to assess which events describe better the scene, and how to model them at a pixel level to fulfill natural language requests. we present an ontology-based methodology that guides the identification, step-by-step modeling, and generalization of the most relevant events to a specific domain. our approach considers three steps: (1) end-users provide textual evidence from surveilled video sequences; (2) transcriptions are analyzed top-down to build the knowledge bases for event description; and (3) the obtained models are used to generalize event detection to different image sequences from the surveillance domain. this framework produces user-oriented knowledge that improves on existing advanced interfaces for video indexing and retrieval, by determining the best suited events for video understanding according to end-users. we have conducted experiments with outdoor and indoor scenes showing thefts, chases, and vandalism, demonstrating the feasibility and generalization of this proposal.",
    "present_kp": [
      "cognitive surveillance"
    ],
    "absent_kp": [
      "event modeling",
      "content-based video retrieval",
      "ontologies",
      "advanced user interfaces"
    ]
  },
  {
    "title": "estorys: a visual storyboard system supporting back-channel communication for emergencies.",
    "abstract": "in this paper we present a new web mashup system for helping people and professionals to retrieve information about emergencies and disasters. today, the use of the web during emergencies, is confirmed by the employment of systems like flickr, twitter or facebook as demonstrated in the cases of hurricane katrina, the july 7, 2005 london bombings, and the april 16, 2007 shootings at virginia polytechnic university. many pieces of information are currently available on the web that can be useful for emergency purposes and range from messages on forums and blogs to georeferenced photos. we present here a system that, by mixing information available on the web, is able to help both people and emergency professionals in rapidly obtaining data on emergency situations by using multiple web channels. in this paper we introduce a visual system, providing a combination of tools that demonstrated to be effective in such emergency situations, such as spatio/temporal search features, recommendation and filtering tools, and storyboards. we demonstrated the efficacy of our system by means of an analytic evaluation (comparing it with others available on the web), an usability evaluation made by expert users (students adequately trained) and an experimental evaluation with 34 participants.",
    "present_kp": [],
    "absent_kp": [
      "mashups",
      "collaboration systems",
      "emergency management"
    ]
  },
  {
    "title": "residual layer-free reverse nanoimprint lithography on silicon and metal-coated substrates.",
    "abstract": "in this work we demonstrate that reverse nanoimprint lithography is a feasible and flexible lithography technique applicable to the transfer of micro and nano polymer structures with no residual layer over areas of cm2 areas on silicon, metal and non-planar substrates. we used a flexible polydimethylsiloxane stamp with hydrophobic features. we present residual layer-free patterns imprinted using a commercial poly(methylmethacrylate) thermoplastic polymer over silicon, nickel and pre-patterned substrates. our versatile patterning technology is adaptable to free form nano structuring and has coupling to adhesion technologies.",
    "present_kp": [
      "nanoimprint lithography"
    ],
    "absent_kp": [
      "injection molding",
      "functional surfaces"
    ]
  },
  {
    "title": "theory of exchange.",
    "abstract": "in the context of production activity, several parameters play an important role in the total cost estimation and its optimization. these parameters include facility setup cost, inventory carrying cost, and manufacturing cost for the basic model. shortages can be incorporated in certain environment and costs associated with shortages need to be included in the analysis. it is expected that the industries will run their manufacturing facility at an optimum level. in the multi-product manufacture, optimum common cycle time approach is usually adopted and all the items are produced in each cycle. a situation may occur in the real world, in which a particular parameter concerning an item is exchanged with that of another item. it is of interest to examine the aftereffects. otherwise also, for the purpose of internal benchmarking, a deliberate exchange of parameters can take place. this can be implemented in case of cost improvement. a generalized approach is presented and discussion is made with reference to various parameters.",
    "present_kp": [
      "exchange of parameters"
    ],
    "absent_kp": [
      "multi-item production",
      "input parameters"
    ]
  },
  {
    "title": "a sparse nonparametric hierarchical bayesian approach towards inductive transfer for preference modeling.",
    "abstract": "in this paper, we present a novel methodology for preference learning based on the concept of inductive transfer. specifically, we introduce a nonparametric hierarchical bayesian multitask learning approach, based on the notion that human subjects may cluster together forming groups of individuals with similar preference rationale (but not identical preferences). our approach is facilitated by the utilization of a dirichlet process prior, which allows for the automatic inference of the most appropriate number of subject groups (clusters), as well as the employment of the automatic relevance determination (ard) mechanism, giving rise to a sparse nature for our model, which significantly enhances its computational efficiency. we explore the efficacy of our novel approach by applying it to both a synthetic experiment and a real-world music recommendation application. as we show, our approach offers a significant enhancement in the effectiveness of knowledge transfer in statistical preference learning applications, being capable of correctly inferring the actual number of human subject groups in a modeled dataset, and limiting knowledge transfer only to subjects belonging to the same group (wherein knowledge transferability is more likely).",
    "present_kp": [
      "preference learning",
      "multitask learning",
      "dirichlet process",
      "automatic relevance determination"
    ],
    "absent_kp": [
      "nonparametric models"
    ]
  },
  {
    "title": "about a system of anti-periodic trigonometric functions.",
    "abstract": "in this paper we introduce the so-called second kind trigonometric system, which is a useful tool for the representation of 2? 2 ? anti-periodic functions. using these functions we study a sequence of trigonometric polynomials, which are bi-orthogonal in the szeg?s sense. we study the usual topics in the theory of fourier series and we present a new connection with the orthogonal polynomials (op) on the unit circle and some useful properties like: recurrence relations, kernel representations and a favards type theorem.",
    "present_kp": [
      "recurrence relations"
    ],
    "absent_kp": [
      "trigonometric orthogonal functions",
      "bi-orthogonality",
      "orthogonal polynomials on the unit circle",
      "verblunsky parameters"
    ]
  },
  {
    "title": "behavior of (111) grains during the thermal treatment of copper film studied in situ by electron back-scatter diffraction.",
    "abstract": "an annealed cu blanket film was investigated in situ at high temperature using electron back-scatter diffraction (ebsd). the primary aim of the experiment was to study the changes in the (111) texture in the cu film where the microstructure was already stabilized by previous annealing treatment. two separate investigations were carried out at the same location of the film for better statistical reliability of data. it was found that the (111) planes got increasingly inclined to the specimen surface with increasing temperature. additionally, a change in the strength of {111}?110? and {111}?112? texture components was observed with increasing temperature. absence of these phenomena in freestanding cu film indicates the impact of substrate on the behavior of (111) grains. the effect of substrate on the peculiar behavior of the (111) grains has been explained by a model which describes the contribution of both dislocations and diffusion to the observed phenomenon. the tilting of the (111) grains is discussed with reference to the recently reported bauschinger effect in the cu films.",
    "present_kp": [
      "cu film",
      "texture",
      "ebsd",
      "in situ"
    ],
    "absent_kp": [
      "oim",
      "xrd"
    ]
  },
  {
    "title": "approximately fair cost allocation in metric traveling salesman games.",
    "abstract": "a traveling salesman game is a cooperative game g = ( n, c(d)). here n, the set of players, is the set of cities ( or the vertices of the complete graph) and cd is the characteristic function where d is the underlying cost matrix. for all s. n, define cd( s) to be the cost of a minimum cost hamiltonian tour through the vertices of s. {0} where 0 is not an element of. n is called as the home city. define core( g) = {x. k vertical bar(n vertical bar) : x( n) = cd( n) and. s. n, x( s) = cd( s)} as the core of a traveling salesman game g. okamoto ( discrete appl. math. 138: 349 - 369, 2004) conjectured that for the traveling salesman game g = ( n, cd) with d satisfying triangle inequality, the problem of testing whether core( g) is empty or not is np-hard. we prove that this conjecture is true. this result directly implies the np-hardness for the general case when d is asymmetric. we also study approximately fair cost allocations for these games. for this, we introduce the cycle cover games and show that the core of a cycle cover game is non-empty by finding a fair cost allocation vector in polynomial time. for a traveling salesman game, let is an element of- core( g) = {x is an element of k vertical bar (n vertical bar) : x( n) >= c(d)( n) and. for all s subset of n, x( s) 1. by viewing an approximate fair cost allocation vector for this game as a sum of exact fair cost allocation vectors of several related cycle cover games, we provide a polynomial time algorithm demonstrating the non-emptiness of the log2(vertical bar n vertical bar - 1)- approximate core by exhibiting a vector in this approximate core for the asymmetric traveling salesman game. we improve it further by finding a (4/3 log(3)(vertical bar n vertical bar)+ c)- approximate core in polynomial time for some constant c. we also show that there exists an is an element of(0) > 1 such that it is np-hard to decide whether is an element of(0)- core( g) is empty or not.",
    "present_kp": [
      "fair cost allocations",
      "traveling salesman game",
      "approximate fair cost allocation"
    ],
    "absent_kp": [
      "cooperative games",
      "combinatorial optimization"
    ]
  },
  {
    "title": "least square fitting with one explicit parameter less.",
    "abstract": "it is shown that whenever the multiplicative normalization of a fitting function is not known, least square fitting by ?2 ? 2 minimization can be performed with one parameter less than usual by converting the normalization parameter into a function of the remaining parameters and the data. program title: fitm1 catalogue identifier: aeyg_v1_0 program summary url:<url> program obtainable from: cpc program library, queens university, belfast, n. ireland licensing provisions: standard cpc licence, <url> no. of lines in distributed program, including test data, etc.: 354261 no. of bytes in distributed program, including test data, etc.: <phone> distribution format: tar.gz programming language: fortran 77 with standard extensions (tested with g95 on a mac). computer: any which supports a fortran 77 compatible compiler. operating system: any with a fortran 77 compatible compiler. ram: 1 mbyte classification: 4.9. nature of problem: least square minimization when one of the free parameters is the multiplicative normalization of the fitting function. solution method: conversion of the normalization constant into a function of the other parameters and the data, resulting into one explicit fitting parameter less. running time: less than 1s on modern pcs",
    "present_kp": [
      "fitting",
      "least square fitting"
    ],
    "absent_kp": [
      "curve fitting"
    ]
  },
  {
    "title": "geosheet: a distributed visualization tool for geometric algorithms.",
    "abstract": "geosheet (version 1.0) is an interactive visualization tool for visualizing geometric algorithms in distributed environments. it provides features such as interactive visualization of program states for debugging, high-level graphical input/output manipulation facilities for geometric objects, reuse of existing data structures and algorithms implementation, and more importantly distributed executions on heterogeneous machines at different sites. to minimize development effort of the tool we make use of existing software packages available in public domain. specifically we extend xfig with a message-driven interface and a socket-based interprocess communication (ipc) mechanism. this extended-xfig is the backbone of this version of the tool. object-oriented programming methodology is used to construct the visualization interface. by deriving from traditional data type and algorithm libraries, our abstract geoobject representation super-classes are easy to use, easy to construct, and highly portable. although geosheet is not restricted to a particular application domain or any programming language, this release only contains geometric algorithm implementations in c++ and leda. we hope that the geometric algorithm designers will find it useful when they develop their algorithms.",
    "present_kp": [
      "visualization",
      "distributed environment"
    ],
    "absent_kp": [
      "geometric computing",
      "objected-oriented programming",
      "x windows"
    ]
  },
  {
    "title": "efficient abstractions for gpgpu programming.",
    "abstract": "general purpose (gp)gpu programming demands to couple highly parallel computing units with classic cpus to obtain a high performance. heterogenous systems lead to complex designs combining multiple paradigms and programming languages to manage each hardware architecture. in this paper, we present tools to harness gpgpu programming through the high-level ocaml programming language. we describe the spoc library that allows to handle gpgpu subprograms (kernels) and data transfers between devices. we then present how spoc expresses gpgpu kernel: through interoperability with common low-level extensions (from cuda and opencl frameworks) but also via an embedded dsl for ocaml. using simple benchmarks as well as a real world hpc software, we show that spoc can offer a high performance while efficiently easing development. to allow better abstractions over tasks and data, we introduce some parallel skeletons built upon spoc as well as composition constructs over those skeletons.",
    "present_kp": [
      "gpgpu",
      "dsl",
      "ocaml",
      "parallel skeletons"
    ],
    "absent_kp": [
      "parallel abstractions"
    ]
  },
  {
    "title": "current induced along horizontal wire above an imperfectly conducting half-space.",
    "abstract": "the boundary element/exponential approximation technique for calculating the loaded straight wire horizontally located above a dissipative half-space is presented. the influence of a lossy ground is taken into account via sommerfeld integrals appearing within the kernel of the electric field integral equation for thin wire. these integrals are computed by means of the exponential approximation technique. the resulting integral equation for loaded wire above an imperfect earth is solved by the boundary element method. numerical results are obtained for current distribution along a resistively loaded dipole antenna and along a transmission line of a finite length.",
    "present_kp": [
      "sommerfeld integrals",
      "electric field integral equation",
      "boundary element method"
    ],
    "absent_kp": []
  },
  {
    "title": "access time minimization for distributed multimedia applications.",
    "abstract": "the problem of minimizing the access time of a requested multimedia (mm) document on a network based environment is addressed. a generalized version of this problem is formulated and retrieval strategies that minimize the access time of the user-requested mm document from a pool of mm servers are proposed. to this end, we design single-installment and multi-installment mm document retrieval strategies, through which the minimization of access time can be carried out. the main idea is to utilize more than one mm server in downloading the requested document. each server assumes the responsibility of uploading a predetermined portion of the entire document in a particular order. single- and multi-installment strategies differ in the number of disjoint document pieces each server sends to the client. we first introduce a directed flow graph (dfg) model to represent the retrieval process and generate a set of recursive equations using this dfg. then, we derive closed-form solutions for the portions of the mm document downloaded from the various servers and the corresponding access time. we present rigorous analysis for these two strategies and show their performance under mpeg-i and mpeg-ii video streams playback rates. their behavior under different network bandwidths is also examined, revealing in-depth information about their expected performance. we also show that in the case of a multi-installment strategy, the access time can be completely controlled by fine tuning the number of installments. since the number of installments is software tunable, the adaptive nature of the strategies to different channel bandwidths is also demonstrated. important trade-off studies with respect to the number of servers involved in the retrieval process and the number of installments are presented. in the case of a heterogeneous network employing a single-installment strategy, we prove that the access time is independent of the server sequence used. illustrative examples are provided for ease of understanding.",
    "present_kp": [
      "bandwidth",
      "access time"
    ],
    "absent_kp": [
      "video distribution",
      "sequencing",
      "multi-installments",
      "retrieval schedule"
    ]
  },
  {
    "title": "cross correlation and deconvolution of noise signals in randomly layered media.",
    "abstract": "it is known that cross correlation of waves generated by noise sources, propagating in an unknown medium and recorded by a sensor array, can provide information about the medium. in this paper the medium is a three-dimensional small-scale randomly layered medium with slow macroscopic variations. the main objective here is to set forth a framework for analysis of cross correlations of waves generated by noise sources and propagating in such a medium and, moreover, to use this framework to design estimators for macroscale medium features. the noise sources are located at the bottom of a random medium slab and generate a random wave field that is scattered by the rapid random fluctuations of the medium and then recorded at the surface. taking into account the pressure release boundary conditions at the surface, this situation corresponds to the so-called daylight configuration. the analysis is carried out in the asymptotic framework where the typical wavelength is small compared to the scale of the macroscopic variations of the background medium and large compared to the decoherence length of the microscopic random fluctuations of the medium. it is shown that the cross correlation of the waves recorded at the surface contains statistically stable information about the macroscopic background medium.",
    "present_kp": [
      "noise sources"
    ],
    "absent_kp": [
      "passive sensor imaging",
      "random media"
    ]
  },
  {
    "title": "using centrality indices in ant systems.",
    "abstract": "lately, much attention has been posited on evolutionary strategies that bring together self-organizing systems and nature selection inspired methods. among these, ant colony optimization algorithms have been suggested by the foraging behaviour of real ants. they can solve any optimization problem involving complex and heterogenous nodes, so these algorithms have been used to obtain solutions for many real-world problems. this paper presents some conclusions on introducing centrality indices as heuristics in ant systems.",
    "present_kp": [
      "ant colony optimization",
      "centrality"
    ],
    "absent_kp": [
      "swarm inteligence"
    ]
  },
  {
    "title": "optimal policies in continuous time inventory control models with limited supply.",
    "abstract": "in this paper, we deal with the problem of a fixed number of units of a certain perishable commodity over a continuous time horizon. airline seats, hotel rooms, advertising space in newspapers, and some seasonal products that must be sold before the end of the season are examples of such commodities that cannot be carried over and are not storable for consumers. this paper considers such a problem of continuous time perishable inventory control by applying semi-markov decision processes over a finite time horizon. it is shown that there is an optimal policy that is simple and stationary. furthermore, some analytical properties of this optimal policy and its value function are explored under certain specific assumptions.",
    "present_kp": [
      "inventory control",
      "semi-markov decision"
    ],
    "absent_kp": [
      "perishable products",
      "yield management",
      "contraction mapping"
    ]
  },
  {
    "title": "freerec: an anonymous and distributed personalization architecture.",
    "abstract": "we present and evaluate freerec, an anonymous decentralized peer-to-peer architecture, designed to bring personalization while protecting the privacy of its users. freerecs decentralized approach makes it independent of any entity wishing to collect personal data about users. at the same time, its onion-routing-like gossip-based overlay protocols effectively hide the association between users and their interest profiles without affecting the quality of personalization. the core of freerec consists of three layers of overlay protocols: the bottom layer, rps, consists of a standard random peer sampling protocol ensuring connectivity; the middle layer, prps, introduces anonymity by hiding users behind anonymous proxy chains, providing mutual anonymity; finally, the top clustering layer identifies for each anonymous user, a set of anonymous nearest neighbors. we demonstrate the effectiveness of freerec by building a decentralized and anonymous content dissemination system. our evaluation by simulation, our planetlab experiments, and our probabilistic analysis show that freerec effectively decouples users from their profiles without hampering the quality of personalized content delivery.",
    "present_kp": [
      "gossip"
    ],
    "absent_kp": [
      "distributed system",
      "recommendation system",
      "anonymous system",
      " network design and communication",
      " distributed systems"
    ]
  },
  {
    "title": "visual speech synthesis by morphing visemes.",
    "abstract": "we present miketalk, a text-to-audiovisual speech synthesizer which converts input text into an audiovisual speech stream. miketalk is built using visemes, which are a small set of images spanning a large range of mouth shapes. the visemes are acquired from a recorded visual corpus of a human subject which is specifically designed to elicit one instantiation of each viseme. using optical flow methods, correspondence from every viseme to every other viseme is computed automatically. by morphing along this correspondence, a smooth transition between viseme images may be generated. a complete visual utterance is constructed by concatenating viseme transitions. finally, phoneme and timing information extracted from a text-to-speech synthesizer is exploited to determine which viseme transitions to use, and the rate at which the morphing process should occur. in this manner, we are able to synchronize the visual speech stream with the audio speech stream, and hence give the impression of a photorealistic talking face.",
    "present_kp": [
      "morphing",
      "optical flow",
      "speech synthesis"
    ],
    "absent_kp": [
      "computer vision",
      "machine learning",
      "facial modelling",
      "facial animation",
      "lip synchronization"
    ]
  },
  {
    "title": "an extended growing self-organizing map for selection of clusters in sensor networks.",
    "abstract": "sensor networks consist of wireless enabled sensor nodes with. limited energy. as sensors could be deployed in a large area, data transmitting and receiving art, energy consuming operations. one of the methods to save energy is to reduce the communication distance of each node by grouping them in to clusters. each cluster will have a cluster-head (ch), which will communicate with all the other nodes of that cluster and transmit the data to the remote base station. in this paper, we propose an extension to growing self-organizing map (gsom) and describe the use of evolutionary computing technique to cluster wireless sensor nodes and to identify the cluster-heads. we compare the proposed method with clustering solutions based on genetic algorithm (ga), an extended version of particle swarm optimisation (pso) and four general purpose clustering algorithms. this could help to discover the clusters to reduce the communication energy, used to transmit data when exact locations of all sensors are known and computational resources are centrally available. this method is useful in the applications where sensors are deployed in a controlled environment and are not moving. we have derived an energy minimisation model that is used as a criterion for clustering. the proposed method can also be used as a design tool to study and analyze the cluster formation for a given node placement.",
    "present_kp": [
      "self-organizing map",
      "evolutionary computing",
      "sensor networks"
    ],
    "absent_kp": [
      "energy optimisation"
    ]
  },
  {
    "title": "dynamic finite element model of oscillatory brushes.",
    "abstract": "in this work, the concept of oscillatory brushes is studied. the main aims are to develop a generic, dynamic finite element model of oscillatory brushes and to study the effects of brush oscillations on the dynamics and performance of a cup-shaped brush, which may be used for street sweeping. the model entails a transient nonlinear analysis, involving three-dimensional large deflections and contact between bristles and surface and between bristles. an exponential friction model is assumed, and the damping generated by internal friction and bristlebristle contact is modelled as rayleigh damping. existing experimental results on gutter brushes are used to validate the model and determine the coefficient of friction for bristlesurface interaction. the model is also validated through another finite element model derived by the authors. the rayleigh damping coefficients and the normal contact stiffness for bristlesurface interaction have been obtained by means of experimental tests. the model is applied to study the behaviour of a horizontal gutter brush with a bristle mount orientation angle of 128. in order to assess brushing performance, a number of performance criteria are proposed. it is concluded that brush oscillations significantly affect brush dynamics and tend to increase bristle tip velocities and brushing forces. according to the model and the performance criteria defined, brush oscillations may improve sweeping effectiveness for certain ranges of frequency of oscillation.",
    "present_kp": [
      "gutter brushes",
      "sweeping effectiveness"
    ],
    "absent_kp": [
      "finite element method ",
      "transient analysis",
      "contact elements",
      "inertia loads"
    ]
  },
  {
    "title": "enhancing ontology-based antipattern detection using bayesian networks.",
    "abstract": "antipatterns provide information on commonly occurring solutions to problems that generate negative consequences. the antipattern ontology has been recently proposed as a knowledge base for sparse, an intelligent system that can detect the antipatterns that exist in a software project. however, apart from the plethora of antipatterns that are inherently informal and imprecise, the information used in the antipattern ontology itself is many times imprecise or vaguely defined. for example, the certainty in which a cause, symptom or consequence of an antipattern exists in a software project. taking into account probabilistic information would yield more realistic, intelligent and effective ontology-based applications to support the technology of antipatterns. however, ontologies are not capable of representing uncertainty and the effective detection of antipatterns taking into account the uncertainty that exists in software project antipatterns still remains an open issue. bayesian networks (bns) have been previously used in order to measure, illustrate and handle antipattern uncertainty in mathematical terms. in this paper, we explore the ways in which the antipattern ontology can be enhanced using bayesian networks in order to reinforce the existing ontology-based detection process. this approach allows software developers to quantify the existence of an antipattern using bayesian networks, based on probabilistic knowledge contained in the antipattern ontology regarding relationships of antipatterns through their causes, symptoms and consequences. the framework is exemplified using a bayesian network model of 13 antipattern attributes, which is constructed using bntab, a plug-in developed for the protege ontology editor that generates bns based on ontological information.",
    "present_kp": [
      "bayesian networks",
      "ontology",
      "antipatterns",
      "antipattern detection"
    ],
    "absent_kp": []
  },
  {
    "title": "on the convex formulation of area for slicing floorplans.",
    "abstract": "we proved that the area of a compact slicing floorplan is convex. the proof is done without any assumption on constant area. cases with and without empty space are included in the proof. layouts with symmetry and proximity constraints are analyzed.",
    "present_kp": [
      "slicing floorplan"
    ],
    "absent_kp": [
      "layout for analog circuits",
      "area optimization",
      "convexity of area"
    ]
  },
  {
    "title": "a scaling theory for fully-depleted, surrounding-gate mosfets: including effective conducting path effect.",
    "abstract": "a scaling theory for fully-depleted surrounding-gate (sg) mosfets is derived, which gives a basic idea how the effective conducting path affects the scaling theory. by investigating the subthreshold conducting phenomenon of sg mosfets, the effective conducting path effect (ecpe) is employed to obtain the natural length ?4 which is relevant to the scaling equation. with various substrate concentrations, the minimum channel potential ?deff,min ? d eff , min induced by effective conducting path shows the novel scaling factor ?4. compared to conventional scaling rule, our model accounts for doping effect and hence provides a unified scaling rule for fully-depleted sg soi mosfets.",
    "present_kp": [
      "sg mosfets",
      "effective conducting path effect",
      "scaling factor",
      "natural length"
    ],
    "absent_kp": [
      "subthreshold swing"
    ]
  },
  {
    "title": "the application of workflow technology in semantic b2b integration.",
    "abstract": "workflow management systems (wfmss) are often used in context of b2b integration as a base technology to implement business-to-business (b2b) integration processes across enterprises. in this context the notion of \"distributed inter-organizational workflows\" is introduced to indicate the collaboration of enterprises on a process level. this notion requires a thorough examination presented in this article since wfmss were not designed with inter-enterprise distribution as one of the design goals. at a closer look, the proposed use of wfmss in context of b2b integration is often very naive and inappropriate. consequently it does not address the real requirements found in enterprises. enterprises do not share common workflow definitions, let alone common workflow instance execution state and have no intent to do so due to competitive knowledge protection. furthermore, trading partner specific business rules within enterprises are not accounted for leading to an unwanted \"explosion\" of workflow definitions. this article clarifies the notion of \"distributed inter-organizational workflows\" as well as private and public processes. based on this definition, the appropriate use of wfmss is shown in context of an overall b2b integration solution that allows enterprises to protect their competitive knowledge while participating in b2b integration.",
    "present_kp": [
      "private and public process",
      "business rules",
      "workflow management"
    ],
    "absent_kp": [
      "business-to-business integration",
      "transformation",
      "message exchange patterns",
      "b2b standards"
    ]
  },
  {
    "title": "proportional lot sizing and scheduling: some extensions.",
    "abstract": "this contribution generalizes the work of drexl and haase about the so-called proportional lot sizing and scheduling problem which was published in 1995. while the early paper considered single-level cases only, the paper at hand describes multilevel problems, i.e., items are interconnected via a directed network of acyclic precedence constraints, it provides mixed-integer programs for several important extensions which differ in the allocation of resources. a generic solution method is presented, and following the preceding paper, a randomized regret-based sampling method is tested. a computational study proves that, even for the multilevel case which is far more complex than the single-level problem, promising results are obtained.",
    "present_kp": [
      "scheduling"
    ],
    "absent_kp": [
      "multilevel lot sizing",
      "plsp",
      "precedence networks",
      "random sampling"
    ]
  },
  {
    "title": "real-time application mapping for many-cores using a limited migrative model.",
    "abstract": "many-core platforms are an emerging technology in the real-time embedded domain. these devices offer various options for power savings, cost reductions and contribute to the overall system flexibility, however, issues such as unpredictability, scalability and analysis pessimism are serious challenges to their integration into the aforementioned area. the focus of this work is on many-core platforms using a limited migrative model (( lmm )). ( lmm ) is an approach based on the fundamental concepts of the multi-kernel paradigm, which is a promising step towards scalable and predictable many-cores. in this work, we formulate the problem of real-time application mapping on a many-core platform using ( lmm ), and propose a three-stage method to solve it. an extended version of the existing analysis is used to assure that derived mappings (i) guarantee the fulfilment of timing constraints posed on worst-case communication delays of individual applications, and (ii) provide an environment to perform load balancing for e.g. energy/thermal management, fault tolerance and/or performance reasons.",
    "present_kp": [
      "limited migrative model"
    ],
    "absent_kp": [
      "real-time systems",
      "embedded systems",
      "multiprocessors",
      "many-core systems",
      "worst-case analysis"
    ]
  },
  {
    "title": "a treatment of the quantum partial entropies in the atom-field interaction with a class of schrodinger cat states.",
    "abstract": "this paper is an enquiry into the circumstances under which entropy and subentropy methods can give an answer to the question of quantum entanglement in the composite state. using a general quantum dynamical system, we obtain the analytical solution when the atom initially starts from its excited state and the field in different initial states. different features of the entanglement are investigated when the field is initially assumed to be in a coherent state, an even coherent state (schrodinger cate state), and a statistical mixture of coherent states. our results show that the setting of the initial state and the stark shift play important roles in the evolution of the sub-entropies and entanglement.",
    "present_kp": [
      "entropy",
      "entanglement"
    ],
    "absent_kp": [
      "schrodinger's cat",
      "multi-photon processes"
    ]
  },
  {
    "title": "a hybrid discrete differential evolution algorithm for the no-idle permutation flow shop scheduling problem with makespan criterion.",
    "abstract": "this paper presents a hybrid discrete differential evolution (hdde) algorithm for the no-idle permutation flow shop scheduling problem with makespan criterion, which is not so well studied. the no-idle condition requires that each machine must process jobs without any interruption from the start of processing the first job to the completion of processing the last job. a novel speed-up method based on network representation is proposed to evaluate the whole insert neighborhood of a job permutation and employed in hdde, and moreover, an insert neighborhood local search is modified effectively in hdde to balance global exploration and local exploitation. experimental results and a thorough statistical analysis show that hdde is superior to the existing state-of-the-art algorithms by a significant margin.",
    "present_kp": [
      "scheduling",
      "differential evolution",
      "speed-up",
      "insert neighborhood",
      "local search"
    ],
    "absent_kp": [
      "no-idle flow shop"
    ]
  },
  {
    "title": "retrieval of images using rich-region descriptions.",
    "abstract": "retrieval of images from databases using their visual features is a challenging and important problem. while the technical problem shares some aspects of image analysis with image understanding, the goal is not to obtain a correct interpretation of the image but to enhance the recall and precision of retrieval. the dominant visual features of an image depend on subjective interpretations and can vary from user to user. we present a technique to improve recall in region-based retrieval; the method is based upon a family of representations of images called 'rich-region descriptions'. we show in a simple experiment how this kind of representation can improve the flexibility allowed to users in obtaining desired results. we also discuss issues related to the user interface for segmentation and query systems. in this subject, the paper extends a previous work.",
    "present_kp": [
      "segmentation",
      "user interface",
      "rich-region description"
    ],
    "absent_kp": [
      "content query",
      "image database",
      "regions",
      "matching",
      "similarity",
      "distance"
    ]
  },
  {
    "title": "morphological color image simplification by saturation-controlled regional levelings.",
    "abstract": "this paper deals with color image simplification using levelings. this class of connected filters suppresses details but preserves the contours of the remaining structures or objects. as the notion of \"color structure\" is not trivial, the formulation of morphological operators for color images involves many open issues. the principle choice of a well-defined color space is crucial and it is proposed to work on a luminance/saturation/hue representation defined by the norm l(1). a family of morphological color operators is then introduced using the classical formulation with total orderings by means of lexicographic cascades. in this framework, a methodology for color image simplification is introduced, which takes advantage of a saturation-controlled combination of the chromatic and the achromatic (or the spectral and the spatio-geometric) components. more precisely, it is based on the application of a color leveling to each significant region, specifically adapted to the nature (chromatic/achromatic) of the region and which needs an initial image partition into the homogenous regions. experimental results illustrate the performance of the new developed algorithms.",
    "present_kp": [
      "color image",
      "leveling"
    ],
    "absent_kp": [
      "mathematical morphology",
      "lum/sat/hue",
      "color connected filtering",
      "morphological color segmentation",
      "lexicographical ordering"
    ]
  },
  {
    "title": "a program for accurate solutions of two-electron atoms.",
    "abstract": "we present a comprehensible computer program capable of treating non-relativistic ground and excited states for a two-electron atom having infinite nuclear mass. an iterative approach based on the implicitly restarted arnoldi method (iram) is employed. the hamiltonian matrix is never explicitly computed. instead the action of the hamiltonian operator on discrete pair functions is implemented. the finite difference method is applied and subsequent extrapolations gives the continuous grid result. the program is written in c and is highly optimized. all computations are made in double precision. despite this relatively low degree of floating point precision (48 digits are not uncommon), the accuracy in the results can reach about 10 significant figures. both serial and parallel versions are provided. the parallel program is particularly suitable for shared memory machines such as the sun starcat series. the serial version is simple to compile and should run on any platform.",
    "present_kp": [
      "mpi",
      "shared memory"
    ],
    "absent_kp": [
      "two-electron schrodinger equation",
      "iterative methods"
    ]
  },
  {
    "title": "a novel forwarding algorithm using prediction and optimization theory over multipath network.",
    "abstract": "in this work, a novel data forwarding algorithm based on prediction method and optimization theory is proposed for improving the quality of service (qos) in real-time applications, in particular over the multipath network. the proposed prediction method is based on the time-series models of observed trends over all available paths at each pre-determined time. this predictive information is then used as an input for the forwarding decision. the forwarding algorithm is meant to minimize the packet loss rate by adjusting the forwarding rate of packets over each path for next prediction period by pso (particle swarm optimization). to avoid solutions that operate near the capacity of the links and shift flows to less utilized links where they can increase more freely, the optimization problem also introduce the cost function. numerical experiments with packet loss rate-sensitive traffic show that the optimal objective converges quickly to the optimal values. it demonstrates that running the proposed algorithm over multipath network can be more efficient and more flexible than equal cost multipath protocol.",
    "present_kp": [
      "forwarding algorithm",
      "optimization theory",
      "prediction"
    ],
    "absent_kp": [
      "multipath networking"
    ]
  },
  {
    "title": "formal and graphical annotations for digital objects.",
    "abstract": "this paper presents graphical tools that facilitate manual building of formal semantic annotations for digital objects. these tools are intended to be integrated in digital object (do) management systems requiring semantic metadata (e.g. precise indexation, comment, categorization, certification). we define a multidimensional graphical model of semantic annotations that specifically allows contextualization of annotations, and we propose a methodology for building such annotations. the graph-based formalism used (derived from conceptual graphs), provides graphical representations that users can easily understand, and are furthermore logically founded. a graphical generic api implementing elements of the sg family has been developed. cogui consists of three specialized tools: a tool for defining a cg ontology, another for creating annotations based on a cg ontology, and finally a tool that uses the cogitant platform, for querying an annotated do base.",
    "present_kp": [
      "semantic annotation",
      "conceptual graphs"
    ],
    "absent_kp": [
      "knowledge representation"
    ]
  },
  {
    "title": "influence of proceedings papers on citation impact in seven sub-fields of sustainable energy research 20052011.",
    "abstract": "this paper analyses the following seven sub-fields of sustainable energy research with respect to the influence of proceedings papers on citation patterns across citing and cited document types, overall sub-field and document type impacts and citedness: the wind power, renewable energy, solar and wave energy, geo-thermal, bio-fuel and bio-mass energy sub-fields. the analyses cover peer reviewed research and review articles as well as two kinds of proceeding papers from conferences published 20052009 in (a) book series or volumes and (b) special journal issues excluding meeting abstracts cited 20052011 through web of science. central findings are: the distribution across document types of cited versus citing documents is highly asymmetric. predominantly proceedings papers from both proceeding volumes as well as published in journals cite research articles (6076%). largely, journal-based proceedings papers are cited rather than papers published in book series or volumes and have field impacts corresponding to research articles. with decreasing proceedings paper dominance in research fields the ratio of proceeding paper volumes over journal-based proceedings papers decreases significantly and the percentage of proceedings papers in journals citing journal-based proceedings papers over all publications citing journal-based proceedings papers decreases significantly (from 26.3% in wind power to 4% in bio fuel). further, the segment of all kinds of proceedings papers (the combined proceedings paper types) citing all proceedings papers over all publications citing all kinds of proceedings papers decreases significantly (from 36.1% in wind power to 11.3% in bio fuel). simultaneously the field citedness increases across the seven research fields. the distribution of citations from review articles shows that novel knowledge essentially derives directly from research articles (5372%)to a much less extent from proceedings publications published in journals (913%).",
    "present_kp": [
      "document types",
      "proceedings papers",
      "research articles",
      "review articles",
      "citation impact",
      "citedness",
      "sustainable energy research",
      ""
    ],
    "absent_kp": [
      "renewable resources"
    ]
  },
  {
    "title": "comparative proteomic analysis of antibiotic-sensitive and insensitive isolates of orientia tsutsugamushi.",
    "abstract": "scrub typhus, caused by infection with orientia tsutsugamushi, is probably the most common severe rickettsial disease. early diagnosis followed by treatment with antibiotics such as doxycycline or chloramphenicol usually quickly decreases fever in patients, and they often recover well from other symptoms of the disease. however, poorly responsive cases have been reported from northern thailand and southern india. in order to identify protein factors that may be partially responsible for differential drug sensitivity of isolates of orientia, we compared the protein profiles of doxycycline sensitive (karp) versus (vs.) insensitive (afsc4 and afsc7) isolates. tryptic peptides from both total water-soluble proteins and from protein spots separated by 2d-page were analyzed using lc-ms/ms. the identity of each protein was established using the published genomic sequence of boryong strain o. tsutsugamushi. the profiles of protein released into water from these isolates were quite different. there were 10 proteins detected only in afsc4, 3 only in karp, and 1 only in afsc7. additionally, there were 2 proteins not detected only in afsc4, 4 not found only in karp, and 3 not found only in afsc-7. a comparison of 2d-page protein profiles of drug sensitive strain versus (vs.) insensitive isolates has led to the identification of 14 differentially expressed or localized proteins, including elongation factor ts and tu, dna-directed rna polymerase ?-subunit, atp synthase ?-subunit, and several hypothetical proteins. these data confirm the tremendous proteomic diversity of isolates of orientia and suggest that drug insensitivity in this species may arise from multiple mechanisms.",
    "present_kp": [
      "orientia tsutsugamushi",
      "proteomic"
    ],
    "absent_kp": [
      "antibiotic sensitive"
    ]
  },
  {
    "title": "risk assessment for one-counter threads.",
    "abstract": "threads as contained in a thread algebra are used for the modeling of sequential program behavior. a thread that may use a counter to control its execution is called a 'one-counter thread'. in this paper the decidability of risk assessment (a certain form of action forecasting) for one-counter threads is proved. this relates to cohen's impossibility result on virus detection. our decidability result follows from a general property of the traces of one-counter threads: if a state is reachable from some initial state, then it is also reachable along a path in which all counter values stay below a fixed bound that depends only on the initial and final counter value. a further consequence is that the reachability of a state is decidable. these properties are based on a result for omega-one counter machines by rosier and yen.",
    "present_kp": [
      "thread algebra",
      "reachability",
      "risk assessment"
    ],
    "absent_kp": [
      "one-counter systems"
    ]
  },
  {
    "title": "a survey of soa technologies in ngn network architectures.",
    "abstract": "the soa (service oriented architecture) paradigm has been driving the definition of important technologies in the telecommunication field for the last decade, as it fosters the deployment of reusable components to support efficient seamless services across different networks. most of these technologies have focused on the definition of high-level network service interfaces and functionalities, such as parlay-x and ims, guaranteeing the success of the soa model in the telco market after that of web applications. some works proposing the adoption of this model in the lower network layers have also been proposed, but they have not proved to have the same impact yet. this paper intends to provide the reader with a survey of the works that have been proposed in this field by following the ngn (next generation network) functional architecture as defined by the itu-t. specifically, we focus on the service and transport strata of the ngn model and, within each of these, we highlight the main logic functions that have been the subject of investigation according to a service-oriented approach. from the literature review, it can be seen that most of the works fall in service stratum, mostly because of the need for an easy integration of distributed service components at this layer. only a few papers focus on the transport functionalities, where the benefits of applying the soa paradigm still need to be investigated.",
    "present_kp": [],
    "absent_kp": [
      "service oriented architecture ",
      "web services",
      "next generation networks "
    ]
  },
  {
    "title": "a linear-time algorithm to find independent spanning trees in maximal planar graphs.",
    "abstract": "given a graph g: a designated vertex r and a natural number k, we wish to iind k \"independent\" spanning trees of g rooted at r, that is, k spanning trees such that, for any vertex rr, the k paths connecting r and v in the k trees are internally disjoint in g. in this paper we give a linear-time algorithm to find i; independent spanning trees in a k-connected maximal planar graph rooted at any designated vertex.",
    "present_kp": [
      "graph",
      "algorithm",
      "independent spanning trees"
    ],
    "absent_kp": []
  },
  {
    "title": "genetically optimized fuzzy polynomial neural networks with fuzzy set-based polynomial neurons.",
    "abstract": "in this paper, we propose and investigate a new category of neurofuzzy networksfuzzy polynomial neural networks (fpnn) endowed with fuzzy set-based polynomial neurons (fspns) we develop a comprehensive design methodology involving mechanisms of genetic optimization, and genetic algorithms (gas) in particular. the conventional fpnns developed so far are based on the mechanisms of self-organization, fuzzy neurocomputing, and evolutionary optimization. the design of the network exploits the fspns as well as the extended group method of data handling (gmdh). let us stress that in the previous development strategies some essential parameters of the networks (such as the number of input variables, the order of the polynomial, the number of membership functions, and a collection of the specific subset of input variables) being available within the network are provided by the designer in advance and kept fixed throughout the overall development process. this restriction may hamper a possibility of developing an optimal architecture of the model. the design proposed in this study addresses this issue. the augmented and genetically developed fpnn (gfpnn) results in a structurally optimized structure and comes with a higher level of flexibility in comparison to the one we encounter in the conventional fpnns. the ga-based design procedure being applied at each layer of the fpnn leads to the selection of the most suitable nodes (or fspns) available within the fpnn. in the sequel, two general optimization mechanisms are explored. first, the structural optimization is realized via gas whereas the ensuing detailed parametric optimization is carried out in the setting of a standard least square method-based learning. the performance of the gfpnn is quantified through experimentation in which we use a number of modeling benchmarkssynthetic and experimental data being commonly used in fuzzy or neurofuzzy modeling. the obtained results demonstrate the superiority of the proposed networks over the models existing in the references.",
    "present_kp": [
      "genetically optimized fuzzy polynomial neural networks ",
      "genetic algorithms ",
      "group method of data handling "
    ],
    "absent_kp": [
      "fuzzy polynomial neuron ",
      "fuzzy set-based polynomial neuron ",
      "self-organizing network",
      "multi-layer perceptron "
    ]
  },
  {
    "title": "contact end resistance test structure applied for nanocontact measurements.",
    "abstract": "this paper presents an accurate method and test structure to evaluate ohmic nanocontacts. the procedure permits the characterization of the contact resistance between standard-in-microtechnology interconnect lines and a certain nanoscaled-in-width object. it is based on the adaption of the classic contact end resistance structure used for the measurement of the contact end resistance in microelectronic technologies. after the theoretical model study, the methodology is validated for the single-walled carbon nanotube case.",
    "present_kp": [
      "nanocontact",
      "contact resistance",
      "test structure"
    ],
    "absent_kp": []
  },
  {
    "title": "robust object tracking using least absolute deviation.",
    "abstract": "the representation error is modelled as a laplacian distribution. we derive our new ladlasso model based on a bayesian map estimate. ladlasso model is robust to outliers. the number of optimisation variable in the new model reduces greatly. we use admm algorithm to solve the new optimisation problem.",
    "present_kp": [
      "object tracking",
      "least absolute deviation"
    ],
    "absent_kp": [
      "sparse representation"
    ]
  },
  {
    "title": "dtrab: combating against attacks on encrypted protocols through traffic-feature analysis.",
    "abstract": "the unbridled growth of the internet and the network-based applications has contributed to enormous security leaks. even the cryptographic protocols, which are used to provide secure communication, are often targeted by diverse attacks. intrusion detection systems (idss) are often employed to monitor network traffic and host activities that may lead to unauthorized accesses and attacks against vulnerable services. most of the conventional misuse-based and anomaly-based idss are ineffective against attacks targeted at encrypted protocols since they heavily rely on inspecting the payload contents. to combat against attacks on encrypted protocols, we propose an anomaly-based detection system by using strategically distributed monitoring stubs (mss). we have categorized various attacks against cryptographic protocols. the mss, by sniffing the encrypted traffic, extract features for detecting these attacks and construct normal usage behavior profiles. upon detecting suspicious activities due to the deviations from these normal profiles, the mss notify the victim servers, which may then take necessary actions. in addition to detecting attacks, the mss can also trace back the originating network of the attack. we call our unique approach dtrab since it focuses on both detection and traceback in the ms level. the effectiveness of the proposed detection and traceback methods are verified through extensive simulations and internet datasets.",
    "present_kp": [],
    "absent_kp": [
      "computer security",
      "encrypted protocol ",
      "intrusion detection system "
    ]
  },
  {
    "title": "improved video categorization from text metadata and user comments.",
    "abstract": "we consider the task of assigning categories (e.g., howto/cooking, sports/basketball, pet/dogs) to youtube videos from video and text signals. we show that two complementary views on the data -- from the video and text perspectives -- complement each other and refine predictions. the contributions of the paper are threefold: (1) we show that a text-based classifier trained on imperfect predictions of the weakly supervised video content-based classifier is not redundant; (2) we demonstrate that a simple model which combines the predictions made by the two classifiers outperforms each of them taken independently; (3) we analyse such sources of text information as video title, description, user tags and viewers' comments and show that each of them provides valuable clues to the topic of the video.",
    "present_kp": [],
    "absent_kp": [
      "natural language processing",
      "video classification"
    ]
  },
  {
    "title": "inhibitory connections in the assembly neural network for texture segmentation.",
    "abstract": "a neural network with assembly organization is described. this assembly network is applied to the problem of texture segmentation in natural scenes. the network is partitioned into several subnetworks: one for each texture class. hebb's assemblies are formed in the subnetworks during the process of training the excitatory connections. also, a structure of the inhibitory connections is formed in the assembly network during a separate training process. the inhibitory connections result in inhibitory interactions between different subnetworks. computer simulation of the network has been performed. experiments show that an adequately trained assembly network with inhibitory connections is more efficient than without them.",
    "present_kp": [
      "excitatory connections",
      "inhibitory connections",
      "subnetworks",
      "texture segmentation"
    ],
    "absent_kp": [
      "connection matrix",
      "negative features",
      "neural assemblies",
      "neurons"
    ]
  },
  {
    "title": "high occupancy vehicle lane performance assessment through operational, environmental impacts and cost-benefit analyses.",
    "abstract": "high occupancy vehicle (hov) lanes are in operation in the u.s. for more than 30 years and used as a tool to alleviate urban freeway congestion. for new hov projects a need exists to study the feasibility of implementation as well as assess their potential operational and environmental impacts prior to deployment. equally important is to obtain a clear picture of expected cost and benefits of available options in order to determine the most effective strategy for implementation. this paper reports on a study that was undertaken to determine the need for and impact from the potential deployment of hov lanes in birmingham, alabama. to meet the study objectives, a detailed alternatives analysis and cost-benefit analysis were performed using traffic software integrated system (tsis) and integrated development assessment system (idas) respectively. three different scenarios and a total of ten options were considered to quantify the operational, environmental, and economic impacts of hov lanes on traffic operations. the paper provides background information on the models used, data gathered, assumptions made, and outputs obtained. a detailed description of the analysis and results is also presented.",
    "present_kp": [
      "traffic operations",
      "environmental impacts",
      "hov"
    ],
    "absent_kp": [
      "high occupancy vehicle lanes",
      "cost-benefits analysis",
      "traffic management"
    ]
  },
  {
    "title": "local systemic intervention.",
    "abstract": "this paper reviews local systemic intervention (lsi). lsi results from learning about total systems intervention (tsi). learning followed application of tsi and by putting tsi through a postmodern critique. lsi is a complementarist approach. it encourages diversity through local decision making about the relevance of designs, consideredness of decisions and astuteness of judgements. it encourages developing discourse around these three centres of learning and designing action accordingly. there is a regular stream of applications of lsi in many types of organisations.",
    "present_kp": [
      "systems",
      "organisation",
      "total systems intervention",
      "local systemic intervention"
    ],
    "absent_kp": [
      "management"
    ]
  },
  {
    "title": "clustering by sorting potential values (cspv): a novel potential-based clustering method.",
    "abstract": "a novel clustering method called clustering by sorting potential values (cspv) is proposed. the clustering is done in an efficient tree-growing fashion based on both the distances and the hypothetical potential values produced from the distribution of all the data points. the method is simple but is shown to be very effective in identifying different kinds of clusters. it outperforms four popular clustering methods in most of our experiments and is the only one that works for all the six studied data sets. moreover, it is designed as a generic method which can be easily applied to different clustering problems.",
    "present_kp": [
      "clustering"
    ],
    "absent_kp": [
      "potential field",
      "spatial distribution",
      "distance matrix",
      "pattern recognition"
    ]
  },
  {
    "title": "active learning for automatic classification of software behavior.",
    "abstract": "a program's behavior is ultimately the collection of all its executions. this collection is diverse, unpredictable, and generally unbounded. thus it is especially suited to statistical analysis and machine learning techniques. the primary focus of this paper is on the automatic classification of program behavior using execution data. prior work on classifiers for software engineering adopts a classical batch-learning approach. in contrast, we explore an active-learning paradigm for behavior classification. in active learning, the classifier is trained incrementally on a series of labeled data elements. secondly, we explore the thesis that certain features of program behavior are stochastic processes that exhibit the markov property, and that the resultant markov models of individual program executions can be automatically clustered into effective predictors of program behavior. we present a technique that models program executions as markov models, and a clustering method for markov models that aggregates multiple program executions into effective behavior classifiers. we evaluate an application of active learning to the efficient refinement of our classifiers by conducting three empirical studies that explore a scenario illustrating automated test plan augmentation.",
    "present_kp": [
      "empirical studies",
      "scenario",
      "plan",
      "markov models",
      "collect",
      "stochastic process",
      "software behavior",
      "paper",
      "model",
      "statistical analysis",
      "software engineering",
      "classification",
      "software",
      "machine learning",
      "method",
      "test",
      "behavior",
      "aggregate",
      "data",
      "active learning",
      "refine",
      "feature",
      "effect",
      "cluster"
    ],
    "absent_kp": [
      "applications",
      "efficiency",
      "automation",
      "evaluation",
      "exploration",
      "software testing",
      "classifiation"
    ]
  },
  {
    "title": "mathematical modeling of chemical oil-soluble transport for water control in porous media.",
    "abstract": "high water-cut is a long-standing problem in the upstream petroleum industry. typically one-fourth of the produced fluids from oil wells worldwide are hydrocarbons and the remaining is water. self-selective in-situ gel formation is a new potential technology to decrease the production of water from oil reservoirs. in this method an oil-soluble chemical is being injected in the reservoir. the chemical, which in this case is tetra-methyl-ortho-silicate or tetramethoxysilane (tmos) reacts with water and ultimately results in the formation of a semi-rigid gel in the water phase. due to this gelation, the relative permeabilities of the formation to water and oil change in favor of the oil phase; therefore the ultimate effect of this gelation is a reduction of the water production rate from the reservoir. the subject of this paper was to model the flow of tmos in a core, including the mass transfer of tmos from oil phase to the water phase, and the occurring chemical reaction in the water phase.",
    "present_kp": [
      "tetra-methyl-ortho-silicate or tetramethoxysilane ",
      "gelation",
      "porous media"
    ],
    "absent_kp": [
      "water cut",
      "numerical modeling"
    ]
  },
  {
    "title": "virtual scan chains reordering using a ram-based module for high test compression.",
    "abstract": "scan chain reordering could be used for test compression by making the corresponding test set more easily compressed. however, it may adversely affect scan chain routing by creating very long routing path and modifying signal delays. the paper proposes a virtual scan chain reorder technique which targets on test compression. the approach simply uses a ram-based module to control the orders of scan cells in circuits. to achieve high test compression, the scan cells can be virtually arranged into any order for different compression schemes. it does not do any real modification to scan chains, and has no any scan chain routing costs. experimental results show that the proposed method can raise the compression ratio efficiently.",
    "present_kp": [
      "scan chain reorder",
      "test compression"
    ],
    "absent_kp": [
      "scan chain partition",
      "virtually reorder"
    ]
  },
  {
    "title": "fidelity of network simulation and emulation: a case study of tcp-targeted denial of service attacks.",
    "abstract": "in this article, we investigate the differences between simulation and emulation when conducting denial of service (dos) attack experiments. as a case study, we consider low-rate tcp-targeted dos attacks. we design constructs and tools for emulation testbeds to achieve a level of control comparable to simulation tools. through a careful sensitivity analysis, we expose difficulties in obtaining meaningful measurements from the deter, emulab, and wail testbeds with default system settings. we find dramatic differences between simulation and emulation results for dos experiments. our results also reveal that software routers such as click provide a flexible experimental platform, but require understanding and manipulation of the underlying network device drivers. our experiments with commercial cisco routers demonstrate that they are highly susceptible to the tcp-targeted attacks when ingress/egress ip filters are used.",
    "present_kp": [
      "simulation",
      "emulation",
      "testbeds",
      "tcp",
      "denial of service attacks"
    ],
    "absent_kp": [
      "congestion control",
      "low-rate tcp-targeted attacks"
    ]
  },
  {
    "title": "a fully-coupled upwind discontinuous galerkin method for incompressible porous media flows: high-order computations of viscous fingering instabilities in complex geometry.",
    "abstract": "we present a new approach to the simulation of viscous fingering instabilities in incompressible, miscible displacement flows in porous media. in the past, high resolution computational simulations of viscous fingering instabilities have always been performed using high-order finite difference or fourier-spectral methods which do not posses the flexibility to compute very complex subsurface geometries. our approach, instead, by means of a fully-coupled nonlinear implementation of the discontinuous galerkin method, possesses a fundamental differentiating feature, in that it maintains high-order accuracy on fully unstructured meshes. in addition, the proposed method shows very low sensitivity to mesh orientation, in contrast with classical finite volume approximation used in porous media flow simulations. the robustness and accuracy of the method are demonstrated in a number of challenging computational problems.",
    "present_kp": [
      "viscous fingering",
      "discontinuous galerkin method",
      "porous media flows"
    ],
    "absent_kp": []
  },
  {
    "title": "algorithms for processing k-closest-pair queries in spatial databases.",
    "abstract": "this paper addresses the problem of finding the k closest pairs between two spatial datasets (the so-called, k closest pairs query, k-cpq), where each dataset is stored in an r-tree. there are two different techniques for solving this kind of distance-based query. the first technique is the incremental approach, which returns the output elements one-by-one in ascending order of distance. the second one is the non-incremental alternative, which returns the k elements of the result all together at the end of the algorithm. in this paper, based on distance functions between two mbrs in the multidimensional euclidean space, we propose a pruning heuristic and two updating strategies for minimizing the pruning distance, and use them in the design of three non-incremental branch-and-bound algorithms for k-cpq between spatial objects stored in two r-trees. two of those approaches are recursive following a depth-first searching strategy and one is iterative obeying a best-first traversal policy. the plane-sweep method and the search ordering are used as optimization techniques for improving the naive approaches. besides, a number of interesting extensions of the k-cpq (k-self-cpq, semi-cpq, k-fpq (the k-farthest pairs query), etc.) are discussed. an extensive performance study is also presented. this study is based on experiments performed with real datasets. a wide range of values for the basic parameters affecting the performance of the algorithms is examined in order to designate the most efficient algorithm for each setting of parameter values. finally, an experimental study of the behavior of the proposed k-cpq branch-and-bound algorithms in terms of scalability of the dataset size and the k value is also included.",
    "present_kp": [
      "spatial databases",
      "branch-and-bound algorithms",
      "r-tree"
    ],
    "absent_kp": [
      "query processing",
      "distance join",
      "i/o and response time performance"
    ]
  },
  {
    "title": "particle swarm optimisation for discrete optimisation problems: a review.",
    "abstract": "in many optimisation problems, all or some of decision variables are discrete. solving such problems are more challenging than those problems with pure continuous variables. among various optimisation techniques, particle swarm optimisation (pso) has demonstrated more promising performance in tackling discrete optimisation problems. in pso, basic variants are merely applicable to continuous problems. so, appropriate strategies should be adopted for enabling them to be applicable to discrete problems. this paper analyses all strategies adopted in pso for tackling discrete problems and discusses thoroughly about pros and cons of each strategy.",
    "present_kp": [
      "particle swarm optimisation",
      "optimisation"
    ],
    "absent_kp": [
      "discrete environments"
    ]
  },
  {
    "title": "visual analysis for textual relationships in digital forensic evidence.",
    "abstract": "we present a visual analytics framework for exploring the textual relationships in computer forensics. based on a task analysis study performed with practitioners, our tool addresses the inefficiency of searching for related text documents on a hard drive. our framework searches both allocated and unallocated sectors for text and performs some pre-analysis processing; this information is then presented via a visualization that displays both the frequency of relevant terms and their location on the disk. we also present a case study that demonstrates our framework's operation, and we report on an informal evaluation conducted with forensics analysts from the mississippi state attorney general's office and national forensics training center.",
    "present_kp": [
      "computer forensics",
      "visualization",
      "visual analytics"
    ],
    "absent_kp": [
      "treemaps",
      "tag clouds"
    ]
  },
  {
    "title": "the virtual wall approach to limit cycle avoidance for unmanned ground vehicles.",
    "abstract": "robot navigation in unknown and very cluttered environments constitutes one of the key challenges in unmanned ground vehicle (ugv) applications. navigational limit cycles can occur when navigating (ugvs) using behavior-based or other reactive algorithms. limit cycles occur when the robot is navigating towards the goal but enters an enclosure that has its opening in a direction opposite to the goal. the robot then becomes effectively trapped in the enclosure. this paper presents a solution named the virtual wall approach (vwa) to the limit cycle problem for robot navigation in very cluttered environments. this algorithm is composed of three stages: detection, retraction, and avoidance. the detection stage uses spatial memory to identify the limit cycle. once the limit cycle has been identified, a labeling operator is applied to a local map of the obstacle field to identify the obstacle or group of obstacles that are causing the deadlock enclosure. the retraction stage defines a waypoint for the robot outside the deadlock area. when the robot crosses the boundary of the deadlock enclosure, a virtual wall is placed near the endpoints of the enclosure to designate this area as off-limits. finally, the robot activates a virtual sensor so that it can proceed to its original goal, avoiding the virtual wall and obstacles found on its way. simulations, experiments, and analysis of the vwa implemented on top of a preference-based fuzzy behavior system demonstrate the effectiveness of the proposed method.",
    "present_kp": [
      "robot navigation",
      "cluttered environments"
    ],
    "absent_kp": [
      "local minima"
    ]
  },
  {
    "title": "dynamic parts scheduling in multiple job shop cells considering intercell moves and flexible routes.",
    "abstract": "aiming at the problem of scheduling with flexible processing routes and exceptional parts that need to visit machines located in multiple job shop cells, a pheromone based approach (pba) using multi-agent is presented in this paper, in which various types of pheromone inspired by ant colony optimization (aco) are adopted as the basis of negotiation among agents. by removing redundant routes and constructing coalition agents, communication cost and negotiation complexity are reduced, and more importantly, the global performance of scheduling is improved. the performance of the pba is evaluated via experiments with respect to the mean flow time, maximum completion time, mean tardiness, ratio of tardy parts, and ratio of intercell moves. computational results show that compared with various heuristics, the pba has significant advantages with respect to the performance measures considered in this paper.",
    "present_kp": [
      "intercell move",
      "flexible route",
      "pheromone"
    ],
    "absent_kp": [
      "part scheduling",
      "multi-agent system"
    ]
  },
  {
    "title": "renal cell carcinomas in von hippel-lindau disease; tumor detection and management.",
    "abstract": "twenty-seven renal cell carcinomas (rccs) found in one family affected with von hippel-lindau disease were examined using ultrasound (us), ct, mri and angiography. the sensitivity of the tumor detection using different imaging modalities was evaluated by macroscopic pathology (solid or cystic) and size (exceeding 2cm in diameter or not). in 18 of the rcc's exceeding 2cm in diameter (eight solid and ten cystic), all lesions were detected on us, ct, and mri. however, on angiography, solid rccs were detected in 88%, and cystic rccs were detected in 60%. in nine rccs less than 2cm in diameter (seven solid and two cystic), solid rccs were detected in 86% on us, 86% on ct, 80% on mri, and 43% on angiography, but cystic rccs were detected in 50% on only ct and mri. from the pathologic correlation, even renal simple cystic lesions in vhl are considered premalignant lesions and they had better be removed if the residual renal function after surgery is preserved. in case of the observation, they should be followed carefully using thin slice thickness dynamic ct to discover the wall irregularity, septation and irregular contour.",
    "present_kp": [
      "von hippel-lindau disease",
      "renal cell carcinoma",
      "dynamic ct"
    ],
    "absent_kp": [
      "dynamic mri"
    ]
  },
  {
    "title": "optimal register assignment to loops for embedded code generation.",
    "abstract": "abstract: one of the challenging tasks in code generation for embedded systems is register assignment. when more live variables than registers exist, some variables are necessarily accessed from data memory. because loops are typically executed many times and are often time-critical, good register assignment in loops is exceedingly important, since accessing data memory can degrade performance. the issue of finding an optimal register assignment to loops, one which minimizes the number of spills between registers and memory, has been open for some time. in this paper, we address this issue and present an optimal, but exponential, algorithm which assigns registers to loop bodies such that the resulting spill code is minimal. we also show that a heuristic modification performs as well as the exponential approach on typical loops from scientific code.",
    "present_kp": [
      "live variables",
      "performance",
      "paper",
      "minimal",
      "live",
      "code",
      "assignment",
      "optimal register assignment",
      "data",
      "scientific code",
      "heuristic modification",
      "algorithm",
      "loops",
      "critic",
      "embedded code generation",
      "code generation",
      "embedded systems"
    ],
    "absent_kp": [
      "optimisation",
      "addressing",
      "real-time systems",
      "timing",
      "storage allocation",
      "variability",
      "abstraction",
      "optimality",
      "program control structures",
      "memorialized",
      "exponential algorithm",
      "heuristics",
      "minimal spill code",
      "data memory access",
      "embedding"
    ]
  },
  {
    "title": "iprailintellectual property reuse-based analog ic layout automation.",
    "abstract": "this paper presents a computer-aided design tool, iprail, which automatically retargets existing analog layouts for technology migration and new design specifications. the reuse-based methodology adopted in iprail utilizes expert designer knowledge embedded in analog layouts. iprail automatically extracts analog layout intellectual properties as templates, incorporates new technology design rules and device sizes, and generates fully functional layouts. this is illustrated by retargeting two practical operational amplifier layouts from the tsmc 0.25?m cmos process to the tsmc 0.18?m cmos process. while manual re-design is known to take days to weeks, iprail only takes minutes and achieves comparable circuit performances.",
    "present_kp": [],
    "absent_kp": [
      "analog integrated circuit design",
      "analog layout automation",
      "layout symmetry",
      "analog synthesis and optimization"
    ]
  },
  {
    "title": "topic dynamics in weibo: a comprehensive study.",
    "abstract": "the tremendous development of online social media has changed peoples life fundamentally in recent years. weibo, a twitter-like service in china, has attracted more than 500 million users in less than 5years and produces more than 100 million chinese tweets everyday. in these massive tweets, different user interests and daily trends are reflected by different topics. to our best knowledge, a systematic investigation of topic dynamics in weibo is still missing. aiming at filling this vital gap, we try to comprehensively disclose the topic dynamics from the perspective of time, geography, demographics, emotion, retweeting and correlation. an incremental learning framework is first established to probe more than 200 million streaming tweets and an interaction network constituted by around 90,000 active users. many interesting patterns are then revealed, which could provide insights for topic-related applications in online social media, such as user profiling, event detection, trend tracking or content recommendation.",
    "present_kp": [
      "topic dynamics",
      "weibo"
    ],
    "absent_kp": [
      "topic classification",
      "topic patterns",
      "topic correlation"
    ]
  },
  {
    "title": "method-specific dynamic compilation using logistic regression.",
    "abstract": "determining the best set of optimizations to apply to a program has been a long standing problem for compiler writers. to reduce the complexity of this task, existing approaches typically apply the same set of optimizations to all procedures within a program, without regard to their particular structure. this paper develops a new method-specific approach that automatically selects the best optimizations on a per method basis within a dynamic compiler. our approach uses the machine learning technique of logistic regression to automatically derive a predictive model that determines which optimizations to apply based on the features of a method. this technique is implemented in the jikes rvm java jit compiler. using this approach we reduce the average total execution time of the specjvm98 benchmarks by 29%. when the same heuristic is applied to the dacapo+ benchmark suite, we obtain an average 33% reduction over the default level o2 setting.",
    "present_kp": [
      "task",
      "machine learning",
      "structure",
      "method",
      "logistic regression",
      "benchmark",
      "procedure",
      "dynamic",
      "reduction",
      "java",
      "jikes rvm",
      "compilation",
      "dynamic compilation",
      "complexity",
      "paper",
      "feature"
    ],
    "absent_kp": [
      "optimality",
      "compiler optimization",
      "timing",
      "heuristics",
      "prediction model"
    ]
  },
  {
    "title": "new quadratic solidshell elements and their evaluation on linear benchmark problems.",
    "abstract": "this paper is concerned with the development of a new family of solidshell finite elements. this concept of solidshell elements is shown to have a number of attractive computational properties as compared to conventional three-dimensional elements. more specifically, two new solidshell elements are formulated in this work (a fifteen-node and a twenty-node element) on the basis of a purely three-dimensional approach. the performance of these elements is shown through the analysis of various structural problems. note that one of their main advantages is to allow complex structural shapes to be simulated without classical problems of connecting zones meshed with different element types. these solidshell elements have a special direction denoted as the thickness, along which a set of integration points are located. reduced integration is also used to prevent some locking phenomena and to increase computational efficiency. focus will be placed here on linear benchmark problems, where it is shown that these solidshell elements perform much better than their counterparts, conventional solid elements.",
    "present_kp": [
      "solidshell finite elements",
      "locking phenomena",
      "reduced integration",
      "benchmark problems",
      ""
    ],
    "absent_kp": [
      "mixed variational principle",
      "rank deficiency"
    ]
  },
  {
    "title": "schedulability analysis of edf-scheduled embedded real-time systems with resource sharing.",
    "abstract": "earliest deadline first (edf) is the most widely studied optimal dynamic scheduling algorithm for uniprocessor real-time systems. in the existing literature, however, there is no complete exact analysis for edf scheduling when both resource sharing and release jitter are considered. since resource sharing and release jitter are important characteristics of embedded real-time systems, a solid theoretical foundation should be provided for edf scheduled systems. in this paper, we extend traditional processor demand analysis to let arbitrary deadline real-time tasks share non-preemptable resources and suffer release jitter. a complete and exact schedulability analysis for edf scheduled systems is provided. this analysis is incorporated into qpa (quick processor-demand analysis) which provides an efficient implementation of the exact test.",
    "present_kp": [
      "resource sharing",
      "schedulability analysis",
      "scheduling",
      "earliest deadline first"
    ],
    "absent_kp": [
      "algorithms",
      "design",
      "reliability",
      "embedded and real-time systems",
      "algorithms",
      "control and reliability"
    ]
  },
  {
    "title": "computing the minimum enclosing sphere of free-form hypersurfaces in arbitrary dimensions.",
    "abstract": "the problem of computing the minimum enclosing sphere (mes) of a point set is a classical problem in computational geometry. as an lp-type problem, its expected running time on the average is linear in the number of points. in this paper, we generalize this approach to compute the minimum enclosing sphere of free-form hypersurfaces, in arbitrary dimensions. this paper makes the bridge between discrete point sets (for which indeed the results are well-known) and continuous curves and surfaces, showing that the general solution for the former can be adapted for the latter. to compute the mes of a pair of hypersurfaces, each one having a contact point (a point at which the sphere touches the hypersurface), antipodal constraints are employed. for more than a pair, equidistance constraints along with tangency constraints are applied. these constraints yield a finite set of solution points which are used to identify the minimum enclosing sphere. the algorithm uses the lp-characteristic of the problem to process the input set. furthermore, an optimization procedure that uses the convex hull of sampled points from the hypersurfaces is also described. finally, results from our implementation are presented.",
    "present_kp": [
      "minimum enclosing sphere",
      "free-form hypersurfaces"
    ],
    "absent_kp": [
      "minimum enclosing circle",
      "lp-type problems"
    ]
  },
  {
    "title": "on some classes of nonstationary parametric processes.",
    "abstract": "in this paper we investigate nonstationary stochastic processes that are characterized by temporal- and spectral-domain parameters with the aim of determining when temporal and spectral parameterizations exist simultaneously. we begin by examining the large class of purely nondeterministic nonstationary stochastic processes generated by passing white noise through a general linear time-varying filter. then four subclasses of nonstationary parametric processes are studied: (1) the rational class; (2) the rational adjoint class; (3) the well-known arma class; and (4) the arma adjoint class. for each of these classes, we give membership conditions on the green's function. these conditions are used to determine when minimum-order parameterizations are unique. next, we use these results to give precise conditions under which a unique minimum-order process is a member of one or more of these classes. although these conditions are quite restrictive, examples are included to show that these conditions do not apply to nonunique minimum-order parameterizations.",
    "present_kp": [
      "nonstationary",
      "time-varying",
      "arma",
      "rational",
      "parametric"
    ],
    "absent_kp": [
      "model building",
      "spectral density"
    ]
  },
  {
    "title": "a physically based trunk soft tissue modeling for scoliosis surgery planning systems.",
    "abstract": "one of the major concerns of scoliotic patients undergoing spinal correction surgery is the trunk's external appearance after the surgery. this paper presents a novel incremental approach for simulating postoperative trunk shape in scoliosis surgery. preoperative and postoperative trunk shapes data were obtained using three-dimensional medical imaging techniques for seven patients with adolescent idiopathic scoliosis. results of qualitative and quantitative evaluations, based on the comparison of the simulated and actual postoperative trunk surfaces, showed an adequate accuracy of the method. our approach provides a candidate simulation tool to be used in a clinical environment for the surgery planning process.",
    "present_kp": [
      "scoliosis"
    ],
    "absent_kp": [
      "biomedical modeling",
      "soft tissue deformation modeling",
      "interactive surgical planning systems",
      "thin-plate splines",
      "energy minimization"
    ]
  },
  {
    "title": "using multiple offspring sampling to guide genetic algorithms to solve permutation problems.",
    "abstract": "the correct choice of an evolutionary algorithm, a genetic representation for the problem being solved (as well as their associated variation operators) and the appropriate values for the parameters of the algorithm is a hard task and it is often considered as an optimization problem itself. in this contribution, we propose a new theoretical formalism, called multiple offspring sampling (mos). this new technique combines different evolutionary approaches taking advantage of the benefits provided by each of them. mos dynamically balances the participation of different mechanisms to spawn the new offspring population, according to the benefits provided by each of them in previous generations. this approach evaluates multiple offspring generation methods (for example different coding strategies), and configures appropriate sampling sizes. this formalism has been applied to a well-known permutation problem, the traveling salesman problem (tsp). the results on several instances of this problem show that most of the combined techniques outperform the results obtained by single ones.",
    "present_kp": [
      "traveling salesman problem",
      "multiple offspring sampling",
      "genetic algorithms",
      "permutation problems"
    ],
    "absent_kp": [
      "hybrid evolutionary methods"
    ]
  },
  {
    "title": "maximal linear embedding for dimensionality reduction.",
    "abstract": "over the past few decades, dimensionality reduction has been widely exploited in computer vision and pattern analysis. this paper proposes a simple but effective nonlinear dimensionality reduction algorithm, named maximal linear embedding (mle). mle learns a parametric mapping to recover a single global low-dimensional coordinate space and yields an isometric embedding for the manifold. inspired by geometric intuition, we introduce a reasonable definition of locally linear patch, maximal linear patch (mlp), which seeks to maximize the local neighborhood in which linearity holds. the input data are first decomposed into a collection of local linear models, each depicting an mlp. these local models are then aligned into a global coordinate space, which is achieved by applying mds to some randomly selected landmarks. the proposed alignment method, called landmarks-based global alignment (lga), can efficiently produce a closed-form solution with no risk of local optima. it just involves some small-scale eigenvalue problems, while most previous aligning techniques employ time-consuming iterative optimization. compared with traditional methods such as isomap and lle, our mle yields an explicit modeling of the intrinsic variation modes of the observation data. extensive experiments on both synthetic and real data indicate the effectivity and efficiency of the proposed algorithm.",
    "present_kp": [
      "dimensionality reduction",
      "maximal linear patch",
      "landmarks-based global alignment"
    ],
    "absent_kp": [
      "manifold learning"
    ]
  },
  {
    "title": "balancing bounded treewidth circuits.",
    "abstract": "we use algorithmic tools for graphs of small treewidth to address questions in complexity theory. for our main construction, we prove that multiplicatively disjoint arithmetic circuits of size n o(1) and treewidth k can be simulated by bounded fan-in arithmetic formulas of depth o(k 2logn). from this we derive an analogous statement for syntactically multilinear arithmetic circuits, which strengthens the central theorem of m.mahajan and b.v.r. rao (proc. 33rd international symposium on mathematical foundations of computer science, vol.5162, pp.455466, 2008). we show our main construction has the following three applications: bounded width arithmetic circuits of size n o(1) can be balanced to depth o(logn), provided chains of iterated multiplication in the circuit are of length o(1). boolean bounded fan-in circuits of size n o(1) and treewidth k can be simulated by bounded fan-in formulas of depth o(k 2logn). this strengthens in the non-uniform setting the known inclusion that sc0?nc1. we demonstrate treewidth restricted cases of directed-reachability and circuit value problem that can be solved in logdcfl.",
    "present_kp": [
      "arithmetic circuits",
      "bounded treewidth",
      "circuit value problem"
    ],
    "absent_kp": [
      "boolean circuits",
      "depth reduction"
    ]
  },
  {
    "title": "swgdt: a sliding window-based genotype dependence testing tool for genome-wide susceptibility gene scan.",
    "abstract": "we developed a sliding window-based genotype dependence testing tool swgdt. swgdt can be applied for genome-wide susceptibility gene scan. we identified a novel susceptibility gene tacr1 for kbd. swgdt can help to identify novel causal genes of complex diseases.",
    "present_kp": [
      "sliding window",
      "swgdt"
    ],
    "absent_kp": [
      "genotype independence analysis",
      "genome-wide association studies"
    ]
  },
  {
    "title": "buffering-deflection tradeoffs in optical burst switching.",
    "abstract": "a very important issue in optical burst switching (obs) networks is the excessive burst drop when no suitable network resources are found during path reservation. in this study, a network scenario is evaluated in which awg-based optical nodes are used as burst router nodes within the optical network. the two classical solutions to solve the burst contentions on the channels outgoing from the node are considered, that is, either based on buffering within the node, or by exploiting deflection routing. a performance evaluation is carried out to evaluate and compare these solutions for different network topologies with different node and traffic parameters. our main contribution is to set numerical tradeoffs between burst deflection through the network and buffering in the node, so that a guidance in optical network design is provided where node buffering is inherently technologically limited.",
    "present_kp": [
      "optical burst switching",
      "deflection routing",
      "buffering-deflection tradeoff"
    ],
    "absent_kp": [
      "arrayed waveguide grating"
    ]
  },
  {
    "title": "simultaneous process mean and variance monitoring using artificial neural networks.",
    "abstract": "control chart patterns (ccps) can be employed to determine the behavior of a process. hence, ccp recognition is an important issue for an effective process-monitoring system. artificial neural networks (anns) have been applied to ccp recognition tasks and promising results have been obtained. it is well known that mean and variance control charts are usually implemented together and that these two charts are not independent of each other, especially for the individual measurements and moving range (x-r(m),) charts. ccps on the mean and variance charts can be associated independently with different assignable causes when corresponding process knowledge is available. however, ann-based ccp recognition models for process mean and variance have mostly been developed separately in the literature with the other parameter assumed to be under control. little attention has been given to the use of anns for monitoring the process mean and variance simultaneously. this study presents a real-time ann-based model for the simultaneous recognition of both mean and variance ccps. three most common ccp types, namely shift, trend, and cycle, for both mean and variance are addressed in this work. both direct data and selected statistical features extracted from the process are employed as the inputs of anns. the numerical results obtained using extensive simulation indicate that the proposed model can effectively recognize not only single mean or variance ccps but also mixed ccps in which mean and variance ccps exist concurrently. empirical comparisons show that the proposed model performs better than existing approaches in detecting mean and variance shifts, while also providing the capability of ccp recognition that is very useful for bringing the process back to the in-control condition. a demonstrative example is provided.",
    "present_kp": [
      "artificial neural networks",
      "control charts"
    ],
    "absent_kp": [
      "feature extraction",
      "pattern recognition",
      "statistical process control"
    ]
  },
  {
    "title": "sensitivity-analysis based method in single-robot cells cost-effective design and optimization.",
    "abstract": "the paper considers the design of an economically efficient robotic cell, parallel workstations at each production stage, gripper type of the robot, and the layout, which affects the travel time of the robot. this proposed method is successfully employed in the real-life engineering cases by abb engineers. its effectiveness are fully demonstrated by these established engineering scenarios.",
    "present_kp": [
      "robotic cell",
      "sensitivity-analysis",
      "cost-effective design"
    ],
    "absent_kp": [
      "productivity"
    ]
  },
  {
    "title": "adaptive robust stabilisation for a class of uncertain nonlinear time-delay dynamical systems.",
    "abstract": "the problem of adaptive robust stabilisation is considered for a class of uncertain nonlinear dynamical systems with multiple time-varying delays. it is assumed that the upper bounds of the nonlinear delayed state perturbations are unknown and that the time-varying delays are any non-negative continuous and bounded functions which do not require that their derivatives have to be less than one. in particular, it is only required that the nonlinear uncertainties, which can also include time-varying delays, are bounded in any non-negative nonlinear functions which are not required to be known for the system designer. for such a class of uncertain nonlinear time-delay systems, a new method is presented whereby a class of continuous memoryless adaptive robust state feedback controllers with a rather simpler structure is proposed. it is also shown that the solutions of uncertain nonlinear time-delay systems can be guaranteed to be uniformly exponentially convergent towards a ball which can be as small as desired. finally, as an application, an uncertain nonlinear time-delay ecosystem with two competing species is given to demonstrate the validity of the results.",
    "present_kp": [
      "time-delay systems",
      "time-varying delays"
    ],
    "absent_kp": [
      "uncertainty",
      "adaptive control",
      "robust control",
      "uniform boundedness",
      "exponential convergence",
      "ecological systems"
    ]
  },
  {
    "title": "deterministic parallel random-number generation for dynamic-multithreading platforms.",
    "abstract": "existing concurrency platforms for dynamic multithreading do not provide repeatable parallel random-number generators. this paper proposes that a mechanism called pedigrees be built into the runtime system to enable efficient deterministic parallel random-number generation. experiments with the open-source mit cilk runtime system show that the overhead for maintaining pedigrees is negligible. specifically, on a suite of 10 benchmarks, the relative overhead of cilk with pedigrees to the original cilk has a geometric mean of less than 1%. we persuaded intel to modify its commercial c/c++ compiler, which provides the cilk plus concurrency platform, to include pedigrees, and we built a library implementation of a deterministic parallel random-number generator called dotmix that compresses the pedigree and then \"rc6-mixes\" the result. the statistical quality of dotmix is comparable to that of the popular mersenne twister, but somewhat slower than a nondeterministic parallel version of this efficient and high-quality serial random-number generator. the cost of calling dotmix depends on the \"spawn depth\" of the invocation. for a naive fibonacci calculation with n = 40 that calls dotmix in every node of the computation, this \"price of determinism\" is about a factor of 2.3 in running time over the nondeterministic mersenne twister, but for more realistic applications with less intense use of random numbers - such as a maximal-independent-set algorithm, a practical samplesort program, and a monte carlo discrete-hedging application from quantlib - the observed \"price\" was at most 2 1%, and sometimes much less. moreover, even if overheads were several times greater, applications using dotmix should be amply fast for debugging purposes, which is a major reason for desiring repeatability.",
    "present_kp": [
      "cilk",
      "determinism",
      "dynamic multithreading",
      "pedigree",
      "random-number generator"
    ],
    "absent_kp": [
      "algorithms",
      "performance",
      "theory",
      "nondeterminism",
      "parallel computing"
    ]
  },
  {
    "title": "characterizing and comparing the evolution of the major global economies in information and communication technologies.",
    "abstract": "we compare status of the ict industry of the six major global economies in ict. the usa is clearly the top global player in ict. the eu is the largest economy of the world, and the least ict-specialised economy of all six major ict economies. ict berd in china remains low but its ict berd intensity grows strongly. of all six analysed economies, taiwan and korea are the most ict-specialised.",
    "present_kp": [
      "ict industry"
    ],
    "absent_kp": [
      "r&d investment",
      "patenting",
      "employment",
      "digital agenda for europe"
    ]
  },
  {
    "title": "a fractional step lattice boltzmann method for simulating high reynolds number flows.",
    "abstract": "a fractional step lattice boltzmann scheme is presented to greatly improve the stability of the lattice boltzmann method (lbm) in modelling incompressible flows at high reynolds number. this method combines the good features of the conventional lbm and the fractional step technique. through the fractional step, the flow at an extreme case of infinite reynolds number (inviscid flow) can be effectively simulated. in addition, the non-slip boundary condition can be directly implemented.",
    "present_kp": [
      "lattice boltzmann method",
      "fractional step"
    ],
    "absent_kp": [
      "high reynolds numbers"
    ]
  },
  {
    "title": "reliability-based code-search algorithms for maximum-likelihood decoding of block codes.",
    "abstract": "efficient code-search maximum-likelihood decoding algorithms, based on reliability information, are presented for binary linear block codes, the codewords examined are obtained via encoding. the information set utilized for encoding comprises the positions of those columns of a generator matrix g of the code which, for a given received sequence, constitute the most reliable basis for the column space of g, substantially reduced computational complexity of decoding is achieved by exploiting the ordering of the positions within this information set. the search procedures do not require memory; the codeword to be examined is constructed from the previously examined codeword according to a fixed rule, consequently, the search algorithms are applicable to codes of relatively large size. they are also conveniently modifiable to achieve efficient nearly optimum decoding of particularly large codes.",
    "present_kp": [
      "block codes",
      "maximum-likelihood decoding",
      "code-search"
    ],
    "absent_kp": [
      "ordered basis",
      "ordered information set",
      "partial ordering"
    ]
  },
  {
    "title": "simultaneous topology, shape and sizing optimisation of a three-dimensional slender truss tower using multiobjective evolutionary algorithms.",
    "abstract": "this paper presents an integrated design technique to carry out simultaneous topology, shape and sizing optimisation of a three-dimensional truss structure. design objectives include mass, compliance, natural frequencies, frequency response function (frf), and force transmissibility (ft). the pareto fronts are explored by using: strength pareto evolutionary algorithm (spea2), population-based incremental learning (pbil), and archived multiobjective simulated annealing (amosa). the results obtained from using the optimisers are compared based upon the hypervolume (hv) and generational distance (gd). it is shown that pbil is the best for optimising compliance and natural frequency, while spea2 is superior when dealing with frf and ft.",
    "present_kp": [
      "multiobjective evolutionary algorithms",
      "frequency response function"
    ],
    "absent_kp": [
      "truss optimisation",
      "topology optimisation",
      "structural vibration"
    ]
  },
  {
    "title": "a cost-effective programmable environment for developing environmental decision support systems1.",
    "abstract": "this paper presents the trace system, a software platform for the development of environmental decision support systems, which is currently under development and will be available by late 1998. the main objective of the trace technology is to support the user in the real-time management of environmental emergencies, by rapidly assessing a crisis situation and building an effective intervention plan, which exploits the available operational resources at best. the first part of the paper presents the conceptual framework of the trace system, including user and application modelling methodologies, tools and techniques. next the design framework is presented, including the architecture of the system and a selection of relevant modules (namely the resource manager, the dependency manager and the constraint reasoner). the trace approach is then illustrated by a pilot application that will support decision making in the case of wind-throw management in romanian forests. the related works are considered within the context of the other relevant research projects.",
    "present_kp": [
      "decision support systems"
    ],
    "absent_kp": [
      "environmental systems",
      "task analysis",
      "constraint reasoning",
      "resource allocation"
    ]
  },
  {
    "title": "hierarchical dynamic slicing.",
    "abstract": "dynamic slicing is a widely used technique for program analysis, debugging, and comprehension. however, the reported slice is often too large to be inspected by the programmer. in this work, we address this deficiency by hierarchically applying dynamic slicing at various levels of granularity. the basic observation is to divide a program execution trace into \"phases\", with data/control dependencies inside each phase being suppressed. only the inter-phase dependencies are presented to the programmer. the programmer then zooms into one of these phases which is further divided into sub-phases and analyzed. we also discuss how our ideas can be used to augment debugging methods other then slicing (such as \"fault localization\", a recently proposed trace comparison method for software debugging).",
    "present_kp": [
      "debugging",
      "dynamic slicing"
    ],
    "absent_kp": [
      "phase detection"
    ]
  },
  {
    "title": "a high-brightness diffractive stereoscopic display technology.",
    "abstract": "in this research, the diffractive blazed grating was proposed to produce the stereoscopic effects that were traditionally generated by barrier grating through parallax barrier method. three wavelengths, 596nm, 554nm and 450nm, were chosen as the main light waves for three sub-pixels, rgb. because the diffractive optical element was directly attached to the color filter of a mpeg-4 player (motion picture experts group four player, mpeg-4 player) panel, the stereoscopic image remained high brightness. since lithography would be the most possible manufacturing method, multilevel approximation was to proceeded during simulation. from the simulated results of optical simulation software, lighttools, the rgb lights moved parallelly and consequently avoiding cross-talk.",
    "present_kp": [
      "blazed grating",
      "diffractive optical element"
    ],
    "absent_kp": [
      "autostereoscopy"
    ]
  },
  {
    "title": "when is a function securely computable.",
    "abstract": "a subset of a set of terminals that observe correlated signals seek to compute a function of the signals using public communication. it is required that the value of the function be concealed from an eavesdropper with access to the communication. we show that the function is securely computable if and only if its entropy is less than the capacity of a new secrecy generation model, for which a single-letter characterization is provided.",
    "present_kp": [],
    "absent_kp": [
      "aided secret key",
      "balanced coloring lemma",
      "function computation",
      "maximum common function",
      "omniscience",
      "secret key capacity",
      "secure computability"
    ]
  },
  {
    "title": "virtual data space load balancing for irregular applications.",
    "abstract": "load balancing is a key issue in the development of parallel algorithms with irregular structures. existing load balancing systems each support only one specific programming paradigm and thus are of limited use. the system vds presented here allows concurrent use of various paradigms such as fork-join, weighted tasks, and static dags (directed acyclic graphs that are known in advance). the system provides visual performance evaluation tools to facilitate the efficient application of the system. vds supports various communication interfaces including pvm and mpi. thus, vds-applications can be run on architectures ranging from workstation clusters to massively parallel systems.",
    "present_kp": [
      "load balancing",
      "irregular applications"
    ],
    "absent_kp": [
      "programming environment",
      "multithreaded computations",
      "dag-scheduling"
    ]
  },
  {
    "title": "multi-core and many-core shared-memory parallel raycasting volume rendering optimization and tuning.",
    "abstract": "given the computing industry trend of increasing processing capacity by adding more cores to a chip, the focus of this work is tuning the performance of a staple visualization algorithm, raycasting volume rendering, for shared-memory parallelism on multi-core cpus and many-core gpus. our approach is to vary tunable algorithmic settings, along with known algorithmic optimizations and two different memory layouts, and measure performance in terms of absolute runtime and l2 memory cache misses. our results indicate there is a wide variation in runtime performance on all platforms, as much as 254% for the tunable parameters we test on multi-core cpus and 265% on many-core gpus, and the optimal configurations vary across platforms, often in a non-obvious way. for example, our results indicate the optimal configurations on the gpu occur at a crossover point between those that maintain good cache utilization and those that saturate computational throughput. this result is likely to be extremely difficult to predict with an empirical performance model for this particular algorithm because it has an unstructured memory access pattern that varies locally for individual rays and globally for the selected viewpoint. our results also show that optimal parameters on modern architectures are markedly different from those in previous studies run on older architectures. in addition, given the dramatic performance variation across platforms for both optimal algorithm settings and performance results, there is a clear benefit for production visualization and analysis codes to adopt a strategy for performance optimization through auto-tuning. these benefits will likely become more pronounced in the future as the number of cores per chip and the cost of moving data through the memory hierarchy both increase.",
    "present_kp": [
      "performance optimization",
      "auto-tuning",
      "multi-core cpu",
      "many-core gpu"
    ],
    "absent_kp": [
      "parallel volume rendering"
    ]
  },
  {
    "title": "toward smarter health and well-being: an implicit role for networking and information technology.",
    "abstract": "the number of people afflicted with chronic illnesses such as obesity, cancer, and diabetes has soared in recent years, generating new kinds of disparate medical challenges. in turn, these challenges have resulted in skyrocketing costs, preventable deaths, and medical malpractice claims. for example, studies have found nearly half of all us patients receive inadequate care each year, over 2 million are harmed by hospital-acquired infections, and over 1 million suffer disabling complications during surgery even though half of these are thought to be avoidable. these challenges have catalyzed the beginning of a transformation in care delivery, from a health-care system that focuses on disease to one that must look after patients including individuals and communities. to provide proper treatment to those who are chronically ill, the health-care system has to support improved individuals' health-related practices prior to the manifestation of disease; ensure that a range of practitioners can deliver quality clinical care during the onset of an illness; and facilitate patient-provider-family partnerships during post-disease wellness care. as we describe in this review, key advances in networking and information technologies spanning patient monitoring, data visualization and decision making, robotics and computer vision for diagnosis and surgery, social networking for fostering community-based support systems, and so on stand to drive forward these changes. together with social scientists, systems engineers, medical practitioners, and computer scientists, information systems scholars can help alter the nature of care delivery well into the twenty-first century, ultimately contributing to a system that is much more safe, effective, reliable, and timely than it is today.",
    "present_kp": [
      "patient monitoring",
      "data visualization",
      "robotics and computer vision",
      "social networking"
    ],
    "absent_kp": [
      "health information technology",
      "smart health and well-being"
    ]
  },
  {
    "title": "foreign dna integrationperturbations of the genomeoncogenesis.",
    "abstract": "we have been interested in the consequences of foreign dna insertion into established mammalian genomes and have initially studied this problem in adenovirus type 12 (ad12)-transformed cells or in ad12-induced hamster tumors. since integrates are frequently methylated de novo, it appears that they might be modified by an ancient defense mechanism against foreign dna. in cells transgenic for the dna of ad12 or for the dna of bacteriophage ?, changes in cellular methylation and transcription patterns have been observed. thus, the insertion of foreign dna can have important functional consequences that are not limited to the site of foreign dna insertion. these findings appear to be relevant also for tumor biology and for the interpretation of data derived from experiments with transgenic organisms. for most animals, the main portal of entry for foreign dna is the gastrointestinal tract. large amounts of foreign dna are regularly ingested with the supply of nutrients. starting in 1987/1988, we have been investigating the fate of orally administered foreign dna in mice. naked dna of bacteriophage m13 and the cloned gene for the green fluorescent protein (gfp) of aequorea victoria have been used as test molecules. moreover, the plant-specific gene for the ribulose-1,5-bisphosphate carboxylase (rubisco) has been followed in mice after feeding soybean leaves. at least transiently, food-ingested dna can be traced to different organs and, after transplacental transfer, to fetuses and newborns. there is no evidence for germ line transmission or for the expression of orally administered gfp dna.",
    "present_kp": [
      "genome",
      "foreign dna insertion",
      "ad12",
      "tumor"
    ],
    "absent_kp": [
      "dna methylation"
    ]
  },
  {
    "title": "learning from the future of component repositories.",
    "abstract": "we consider quality issues that can arise in future evolutions of software repositories. we define a class of such issues that is amenable to automatic verification. two relevant instances of such a class are identified: outdated and challenged components. we validate our findings on real-world repositories.",
    "present_kp": [],
    "absent_kp": [
      "component repository",
      "quality assurance",
      "software evolution",
      "speculative analysis",
      "open source"
    ]
  },
  {
    "title": "general evolutionary theory of information production processes and applications to the evolution of networks.",
    "abstract": "evolution of information production processes (ipps) can be described by a general transformation function for the sources and for the items. it generalises the fellmanjakobsson transformation which only works on the items. in this paper the dual informetric theory of this double transformation, defined by the rank-frequency function, is described by, e.g. determining the new size-frequency function. the special case of power law transformations is studied thereby showing that a lotkaian system is transformed into another lotkaian system, described by a new lotka exponent. we prove that the new exponent is smaller (larger) than the original one if and only if the change in the sources is smaller (larger) than that of the items. applications to the study of the evolution of networks are given, including cases of deletion of nodes and/or links but also applications to other fields are given.",
    "present_kp": [
      "evolution",
      "ipp",
      "information production process",
      "lotka",
      "network"
    ],
    "absent_kp": [
      "zipf"
    ]
  },
  {
    "title": "a hybrid algorithm for automatic segmentation of slowly moving objects.",
    "abstract": "segmentation of moving objects in video sequences is a basic task in many applications. however, it is still challenging due to the semantic gap between the low-level visual features and the high-level human interpretation of video semantics. compared with segmentation of fast moving objects, accurate and perceptually consistent segmentation of slowly moving objects is more difficult. in this paper, a novel hybrid algorithm is proposed for segmentation of slowly moving objects in video sequence aiming to acquire perceptually consistent results. firstly, the temporal information of the differences among multiple frames is employed to detect initial moving regions. then, the gaussian mixture model (gmm) is employed and an improved expectation maximization (em) algorithm is introduced to segment a spatial image into homogeneous regions. finally, the results of motion detection and spatial segmentation are fused to extract final moving objects. experiments are conducted and provide convincing results.",
    "present_kp": [
      "gmm"
    ],
    "absent_kp": [
      "moving object segmentation",
      "spatio-temporal information",
      "frame difference",
      "fusing operation"
    ]
  },
  {
    "title": "finite element analysis of the influence of loading rate on a model of the full lumbar spine under dynamic loading conditions.",
    "abstract": "despite an increase in the number of experimental and numerical studies dedicated to spinal trauma, the influence of the rate of loading or displacement on lumbar spine injuries remains unclear. in the present work, we developed a bio-realistic finite element model (fem) of the lumbar spine using a comprehensive geometrical representation of spinal components and material laws that include strain rate dependency, bone fracture, and ligament failure. the fem was validated against published experimental data and used to compare the initiation sites of spinal injuries under low (ld) and high (hd) dynamic compression, flexion, extension, anterior shear, and posterior shear. simulations resulted in forcedisplacement and moment-angular rotation curves well within experimental corridors, with the exception of ld flexion where angular stiffness was higher than experimental values. such a discrepancy is attributed to the initial toe-region of the ligaments not being included in the material law used in the study. spinal injuries were observed at different initiation sites under ld and hd loading conditions, except under shear loads. these findings suggest that the strain rate dependent behavior of spinal components plays a significant role in load-sharing and failure mechanisms of the spine under different loading conditions.",
    "present_kp": [
      "bone fracture",
      "dynamic load",
      "finite element model",
      "lumbar spine"
    ],
    "absent_kp": [
      "ligament tear",
      "experimental validation"
    ]
  },
  {
    "title": "speech enhancement based on undecimated wavelet packet-perceptual filterbanks and mmsestsa estimation in various noise environments.",
    "abstract": "in this paper, we proposed a new speech enhancement system, which integrates a perceptual filterbank and minimum mean square errorshort time spectral amplitude (mmsestsa) estimation, modified according to speech presence uncertainty. the perceptual filterbank was designed by adjusting undecimated wavelet packet decomposition (uwpd) tree, according to critical bands of psycho-acoustic model of human auditory system. the mmsestsa estimation (modified according to speech presence uncertainty) was used for estimation of speech in undecimated wavelet packet domain. the perceptual filterbank provides a good auditory representation (sufficient frequency resolution), good perceptual quality of speech and low computational load. the mmsestsa estimator is based on a priori snr estimation. a priori snr estimation, which is a key parameter in mmsestsa estimator, was performed by using decision directed method. the decision directed method provides a trade off between noise reduction and signal distortion when correctly tuned. the experiments were conducted for various noise types. the results of proposed method were compared with those of other popular methods, wiener estimation and mmselog spectral amplitude (mmselsa) estimation in frequency domain. to test the performance of the proposed speech enhancement system, three objective quality measurement tests (snr, segsnr and itakurasaito distance (isd)) were conducted for various noise types and snrs. experimental results and objective quality measurement test results proved the performance of proposed speech enhancement system. the proposed speech enhancement system provided sufficient noise reduction and good intelligibility and perceptual quality, without causing considerable signal distortion and musical background noise.",
    "present_kp": [
      "speech enhancement"
    ],
    "absent_kp": [
      "undecimated wavelet packet transform",
      "perceptual filter bank",
      "minimum mean square errorshort time spectral amplitude estimation"
    ]
  },
  {
    "title": "local bandwidth selectors for deconvolution kernel density estimation.",
    "abstract": "we consider kernel density estimation when the observations are contaminated by measurement errors. it is well-known that the success of kernel estimators depends heavily on the choice of a smoothing parameter called the bandwidth. a number of data-driven bandwidth selectors exist, but they are all global. such techniques are appropriate when the density is relatively simple, but local bandwidth selectors can be more attractive in more complex settings. we suggest several data-driven local bandwidth selectors and illustrate via simulations the significant improvement they can bring over a global bandwidth.",
    "present_kp": [
      "data-driven bandwidth",
      "measurement errors",
      "smoothing parameter"
    ],
    "absent_kp": [
      "contaminated data",
      "ebbs",
      "errors-in-variables",
      "kernel smoothing",
      "plug-in"
    ]
  },
  {
    "title": "multipoint boundary value problems by differential quadrature method.",
    "abstract": "this paper extends the application of the differential quadrature method (dqm) to high order (greater than or equal to 3(rd)) ordinary differential equations with the boundary conditions specified at multiple points (greater than or equal to three different points). explicit weighting coefficients for higher order derivatives have been derived using interpolating trigonometric polynomials. a three-point, linear third-order differential equation governing the shear deformation of sandwich beams is examined. two examples of four-point nonlinear fourth-order systems are also presented. accurate results are obtained for the example problems. since boundary conditions are usually specified only at two extreme ends and not at intermediate boundary points, the present work opens new areas of application of the dqm.",
    "present_kp": [
      "differential quadrature method",
      "multipoint boundary value problem"
    ],
    "absent_kp": [
      "generalized collocation method",
      "collocation",
      "frechet derivative",
      "numerical method"
    ]
  },
  {
    "title": "a new addition formula for elliptic curves over gf(2(n)).",
    "abstract": "in this paper, we propose a new addition formula in projective coordinates for elliptic curves over gf (2'). the new formula speeds up the elliptic curve scalar multiplication by reducing the number of field multiplications, this was achieved by rewriting the elliptic curve addition formula, the complexity analysis shows that the new addition formula speeds up the addition in projective coordinates by about 10-2 percent, which leads to enhanced scalar multiplication methods for random and koblitz curves.",
    "present_kp": [
      "elliptic curves",
      "projective coordinates",
      "scalar multiplication"
    ],
    "absent_kp": [
      "public-key cryptography",
      "point addition"
    ]
  },
  {
    "title": "improved algorithms for two single machine scheduling problems.",
    "abstract": "in this paper, we investigate two single machine scheduling problems. the first problem addresses a class of the two-stage scheduling problems in which the first stage is job production and the second stage is job delivery. for the case that jobs are processed on a single machine and delivered by a single vehicle to one customer area, with the objective of minimizing the time when all jobs are completed and delivered to the customer area and the vehicle returns to the machine, an approximation algorithm with a worst-case ratio of 5/3 is known and no approximation can have a worst-case of 3/2 unless p = np. we present an improved approximation algorithm with a worst-case ratio of 53/35, which only leaves a gap of 1/70. the second problem is a single machine scheduling problem subject to a period of maintenance. the objective is to minimize the total completion time. the best known approximation algorithm has a worst-case ratio of 20/17. we present a polynomial time approximation scheme.",
    "present_kp": [
      "scheduling",
      "delivery",
      "worst-case ratio"
    ],
    "absent_kp": []
  },
  {
    "title": "learning juntas.",
    "abstract": "we consider a fundamental problem in computational learning theory: learning an arbitrary boolean function which depends on an unknown set of k out of n boolean variables. we give an algorithm for learning such functions from uniform random examples which runs in time roughly (n k ) ?/(? + 1) , where ? < 2.376 is the matrix multiplication exponent. we thus obtain the first polynomial factor improvement on the naive n k time bound which can be achieved via exhaustive search. our algorithm and analysis exploit new structural properties of boolean functions.",
    "present_kp": [
      "learning",
      "juntas"
    ],
    "absent_kp": [
      "fourier",
      "uniform distribution",
      "relevant variables"
    ]
  },
  {
    "title": "object recognition using wavelets, l-g graphs and synthesis of regions.",
    "abstract": "this paper presents a recipe of methodologies for object recognition using wavelets, local-global (l-g) graphs and a region synthesis approach. in particular, the wavelets rearrange the shape of an object for reaching a desirable size, the l-g graphs represent the shape, color, texture and location of each image region obtained by an image segmentation method and the synthesis of the regions that compose an object is achieved by synthesizing their graph representations under certain neighborhood criteria. the synthesis of the regions is based on the l-g modeling that compares a set of l-g-based object models sitting in an l-g database. the methodology is accurate for objects existed in the database and it has the ability of \"learning\" new l-g patterns associated with objects that do not exist in the database. illustrative examples are also provided.",
    "present_kp": [
      "object recognition",
      "wavelets",
      "graphs",
      "region synthesis"
    ],
    "absent_kp": []
  },
  {
    "title": "a computer architecture for highly parallel signal processing.",
    "abstract": "a computer of unusual architecture is described that achieves highly parallel operation through use of a data-flow program representation. the machine is especially suited for signal processing computations such as waveform generation, modulation, and filtering, in which a group of operations must be performed once for each sample of the signals being processed. the difficulties of processor switching and memory/processor interconnection arising in attempts to adapt von neuman type computers for parallel operation are avoided by an organization in which sections of the machine communicate through transmission of information packets, and delays in packet transmission do not compromise effective utilization of the hardware. the design concept is especially suited to implementation using asynchronous logic and large-scale integrated circuits. application of the concepts to generalized data-flow program languages is under study.",
    "present_kp": [
      "signal processing",
      "computer architecture"
    ],
    "absent_kp": [
      "parallel processing",
      "data flow"
    ]
  },
  {
    "title": "multimodal biomedical image indexing and retrieval using descriptive text and global feature mapping.",
    "abstract": "the images found within biomedical articles are sources of essential information useful for a variety of tasks. due to the rapid growth of biomedical knowledge, image retrieval systems are increasingly becoming necessary tools for quickly accessing the most relevant images from the literature for a given information need. unfortunately, article text can be a poor substitute for image content, limiting the effectiveness of existing text-based retrieval methods. additionally, the use of visual similarity by content-based retrieval methods as the sole indicator of image relevance is problematic since the importance of an image can depend on its context rather than its appearance. for biomedical image retrieval, multimodal approaches are often desirable. we describe in this work a practical multimodal solution for indexing and retrieving the images contained in biomedical articles. recognizing the importance of text in determining image relevance, our method combines a predominately text-based image representation with a limited amount of visual information, in the form of quantized content-based visual features, through a process called global feature mapping. the resulting multimodal image surrogates are easily indexed and searched using existing text-based retrieval systems. our experimental results demonstrate that our multimodal strategy significantly improves upon the retrieval accuracy of existing approaches. in addition, unlike many retrieval methods that utilize content-based visual features, the response time of our approach is negligible, making it suitable for use with large collections.",
    "present_kp": [
      "image indexing"
    ],
    "absent_kp": [
      "multimodal image retrieval",
      "clustering"
    ]
  },
  {
    "title": "dynamic synchronous/asynchronous replication.",
    "abstract": "online, remote, data replication is critical for today's enterprise it organization. availability of data is key to the success of the organization. a few hours of downtime can cost from thousands to millions of dollars with increasing frequency, companies are instituting disaster recovery plans to ensure appropriate data availability in the event of a catastrophic failure or disaster that destroys a site (e.g. flood, fire, or earthquake). synchronous and asynchronous replication technologies have been available for a long period of time. synchronous replication has the advantage of no data loss, but due to latency, synchronous replication is limited by distance and bandwidth. asynchronous replication on the other hand has no distance limitation, but leads to some data loss which is proportional to the data lag. we present a novel method, implemented within emc recover-point, which allows the system to dynamically move between these replication options without any disruption to the i/o path. as latency grows, the system will move from synchronous replication to semi-synchronous replication and then to snapshot shipping. it returns to synchronous replication as more bandwidth is available and latency allows.",
    "present_kp": [],
    "absent_kp": [
      "management",
      "remote replication"
    ]
  },
  {
    "title": "the impact of genetic variation in comt and bdnf on resting-state functional connectivity.",
    "abstract": "genetic imaging techniques allow investigation of the mechanisms by which genetic variants influence brain structure and function. the default mode network (dmn) is characterized by a default state of neuronal activity in the brain that is linked to core processes of human cognition. this study examined the role of catechol-o-methyl transferase (comt) and brain-derived neurotrophic factor (bdnf) polymorphisms on functional connectivity between brain areas. twenty-three healthy volunteers underwent a resting-state functional magnetic resonance imaging scan and genotyping of comt and bdnf single nucleotide polymorphisms (snps). a resting-state functional connectivity map was created using the posterior cingulate cortex (pcc) as a seed region. the val/val homozygote group of the comt val158met snp showed significantly greater dmn connectivity in the medial and superior frontal gyri and cerebellum compared with the met allele carrier group. for the bdnf val66met snp, connectivity between the pcc and precuneus was stronger in the val/val homozygote group than in the met allele carrier group. different patterns of dmn connectivity related to bdnf and comt snps were observed in this study. these findings suggest interaction between genes and functional connectivity in the brain and indicate that altered functional connectivity may be an endophenotype of cognitive vulnerability.",
    "present_kp": [
      "default mode network",
      "brain-derived neurotrophic factor",
      "catechol-o-methyl transferase"
    ],
    "absent_kp": [
      "resting-state connectivity",
      "functional neuroimaging",
      "genetic polymorphism"
    ]
  },
  {
    "title": "multiobjective controller design handling human preferences.",
    "abstract": "trends in controller design point to the integration of several objectives to achieve new performances. moreover, it is easy to set the controller design problem as an optimization problem. therefore, future improvements are likely to be based on the adequate formulation and resolution of the multiobjective optimization problem. the multiobjective optimization strategy called physical programming provides controller designers with a flexible tool to express design preferences with a physical meaning. for each objective (settling time, overshoot, disturbance rejection, etc.) preferences are established through categories such as desirable, tolerable, unacceptable, etc. to which numerical values are assigned. the problem is normalized and converted to a single-objective optimization problem but normally it results in a multimodal problem very difficult to solve. genetic algorithms provide an adequate solution to this type of problems and open new possibilities in controller design and tuning.",
    "present_kp": [
      "multiobjective optimization",
      "controller design"
    ],
    "absent_kp": [
      "robust control"
    ]
  },
  {
    "title": "compression of sar raw data through range focusing and variable-rate trellis-coded quantization.",
    "abstract": "there is an ever-growing interest in the compression of sar data because of the huge resources they require for storage and transmission. this is especially true for spaceborne sensors, given the limited capacity of the downlink channel. unfortunately, sar data lack the useful properties on which compression algorithms rely; indeed, these are present in the focused images, but focusing is too complex for on-board implementation at this time. in [11], we proposed to perform on the satellite only the low-complexity range focusing, which increases the data correlation and better concentrates their energy. these properties were then exploited by adopting a variable-rate vector quantizer, with a clear performance improvement with respect to reference techniques. however, vector quantization (vq) is too complex for actual on-board implementation, and therefore, here we replace vq with trellis-coded vq. to limit complexity, only small vectors are used, which reduces vq's ability to exploit data dependencies; on the other hand, trellis coding allows one to encode large blocks of data at once, and to obtain a better partition of the input space. experiments on real sar data show that the overall performance is comparable to that of [11], but the complexity is much lower, making on-board implementation possible.",
    "present_kp": [
      "compression",
      "raw data"
    ],
    "absent_kp": [
      "synthetic aperture",
      "trellis coded quantization"
    ]
  },
  {
    "title": "optimization of sylvite transformation into arcanite using experimental design methodology.",
    "abstract": "this study is an application of the experimental design methodology for optimizing a potassium sulfate synthesis reaction. the latter is a two-stage reaction through an intermediate product (schoenite: k2so4 center dot mgso4 center dot 6h(2)o). to determine optimal experimental conditions of the first stage, we have conducted a fractional factorial design and a central composite one. the optimal conditions of the second stage were determined only by means of a fractional factorial design. several physico-chemical techniques were used to implement this study, namely potentiometry, complexometry, gravimetry and x-ray diffraction. this work has showed that this double decomposition reaction, when performed under the determined optimal conditions, gives good quality potassium sulfate (purity more than 95%) with a maximal yield.",
    "present_kp": [
      "experimental design methodology"
    ],
    "absent_kp": [
      "k2so4 synthesis",
      "process optimization"
    ]
  },
  {
    "title": "conversions between three methods for representing 3d surface textures under arbitrary illumination directions.",
    "abstract": "representing the appearances of surfaces illuminated from different directions has long been an active research topic. while many representation methods have been proposed, the relationships and conversion between different representations have been less well researched. these relationships are important, as they provide (a) an insight as to the different capabilities of the surface representations, and (b) a means by which they may be converted to common formats for computer graphic applications. in this paper, we introduce a single mathematical framework and use it to express three commonly used surface texture relighting representations: surface gradients (gradient), polynomial texture maps (ptm) and eigen base images (eigen). the framework explicitly reveals the relations between the three methods, and from this we propose a set of conversion methods. we use 26 rough surface textures illuminated from 36 directions for our experiments and perform both quantitative and qualitative assessments to evaluate the conversion methods. the quantitative assessment uses a normalized root-mean-squared error as metric to compare the original images and those produced by proposed representation methods. the qualitative assessment is based on psychophysical experiments and non-parametric statistics. the results from the two assessments are consistent and show that the original eigen representation produces the best performance. the second best performances are achieved by the original ptm representation and the conversion between polynomial texture maps (ptm) and eigen base images (eigen), while the performances of other representations are not significantly different.",
    "present_kp": [
      "gradient",
      "polynomial texture map",
      "eigen",
      "relighting",
      "conversion methods"
    ],
    "absent_kp": []
  },
  {
    "title": "a novel clonal selection algorithm and its application to traveling salesman problem.",
    "abstract": "the clonal selection algorithm (csa) is employed by the natural immune system to define the basic features of an immune response to an antigenic stimulus. in the immune response, according to burnet's clonal selection principle, the antigen imposes a selective pressure on the antibody population by allowing only those cells which specifically recognize the antigen to be selected for proliferation and differentiation. however ongoing investigations indicate that receptor editing, which refers to the process whereby antigen receptor engagement leads to a secondary somatic gene rearrangement event and alteration of the receptor specificity, is occasionally found in affinity maturation process. in this paper, we extend the traditional csa approach by incorporating the receptor editing method, named recsa, and applying it to the traveling salesman problem. thus, both somatic hypermutation (hm) of clonal selection theory and receptor editing (re) are utilized to improve antibody affinity. simulation results and comparisons with other general algorithms show that the recsa algorithm can effectively enhance the searching efficiency and greatly improve the searching quality within reasonable number of generations.",
    "present_kp": [
      "clonal selection algorithm",
      "receptor editing",
      "traveling salesman problem",
      "affinity"
    ],
    "absent_kp": [
      "somatic maturation"
    ]
  },
  {
    "title": "recursive program optimization through inductive synthesis proof transformation.",
    "abstract": "the research described in this paper involved developing transformation techniques that increase the efficiency of the original program, the source, by transforming its synthesis proof into one, the tar-get, which yields a computationally more efficient algorithm. we describe a working proof transformation system that, by exploiting the duality between mathematical induction and recursion, employs the novel strategy of optimizing recursive programs by transforming inductive proofs. we compare and contrast this approach with the more traditional approaches to program transformation and highlight the benefits of proof transformation with regards to search, correctness, automatability, and generality.",
    "present_kp": [
      "proof transformation",
      "induction"
    ],
    "absent_kp": [
      "synthesis proofs"
    ]
  },
  {
    "title": "role-based access control in ambient and remote space.",
    "abstract": "in the era of ubiquitous computing and world-wide data transfer mobility, as an innovative aspect of professional activities, imposes new andcomplex problems of mobile and distributed access to information,services, and on--line negotiations for this purpose. this paperrestricts itself to presenting a distributed and location--dependentrbac approach which is multi--layered. also an adapted form ofadministration nets is presented whichallows the scheduling of distributed on--line processes for automatedlocation--dependent negotiating procedures, and for provingtheir correctness. examples are discussed in some detail.",
    "present_kp": [],
    "absent_kp": [
      "petri-nets",
      "spatial",
      "work-flow"
    ]
  },
  {
    "title": "on the security of the okamoto-tanaka id-based key exchange scheme against active attacks.",
    "abstract": "as far as the knowledge of authors, the rigorous security of okamoto-tanaka identity-based key exchange scheme nas shown in [4] for the first time since its invention. however, the analysis deals with only the passive attack. in this paper, we give several models of active attacks against the scheme and show the rigorous security of the scheme in these models. we prove several relationships among attack models, including that (1) breaking the scheme in one attack model is equivalent to breaking the rsa public-key cryptosystem and (2) breaking the scheme in another attack model is equivalent to breaking the diffie-hellman key exchange scheme over z(n). the difference of the complexity stems from the difference of the timing of dishonest party's sending out and receiving messages.",
    "present_kp": [
      "active attack"
    ],
    "absent_kp": [
      "okamoto-tanaka key exchange scheme",
      "id-based scheme",
      "known-key attack",
      "forward secrecy"
    ]
  },
  {
    "title": "a study of low-cost, robust assistive listening system based on uhf wireless technology.",
    "abstract": "in this paper, we describe the development of a low cost wireless uhf broadcasting system prototype. it is designed for a short range application, with up to ten receivers, such as one used in a classroom for hearing impaired students. this system is intended to be a low cost alternative to an existing fm system, with additional capability in security with an encryption function. additionally a multimode function including one-to-one, one-to-many and broadcast is added. test results show that the system operating range is approximately ten metres, and the sound quality is close to telephone quality. the broadcasting mode and system range fit the requirements for small classroom use.",
    "present_kp": [
      "wireless"
    ],
    "absent_kp": [
      "assistive technology",
      "rf",
      "als"
    ]
  },
  {
    "title": "description and verification of an aquatic optics monte carlo model.",
    "abstract": "an open source aquatic optics monte carlo (aomc) model is described and evaluated. the open source nature of the aomc software allows for a complete peer review process. the aomc model simulates apparent optical properties commonly measured in aquatic systems. these include spectral or non-spectral radiance, upwelling and downwelling planar and scalar irradiances. the model output can be in a non-gridded or gridded format with the latter used to simulate a radiometric image. in gridded mode, the user may specify a bottom target whose reflecting characteristics are independent from the surrounding water bottom boundary. in both gridded and non-gridded mode, the model inputs consist of depth dependent concentrations of the various components comprising the simulated aquatic environment, their inherent optical properties, and environmental characteristics such as skylight distribution and optical properties of the aquatic bottom boundary. the model is evaluated by comparing its output to: (1) a test of the models closure, (2) an evaluation of a known relationship, (3) the comparison with results from other published numerical models, and (4) the simulation of a spectral pivot-point. the results of the evaluation indicate good agreement between the model and the anticipated outcome.",
    "present_kp": [
      "monte carlo model",
      "radiance",
      "irradiance",
      "aomc"
    ],
    "absent_kp": [
      "aquatic or hydrologic optics"
    ]
  },
  {
    "title": "a droplet in a stationary electric field.",
    "abstract": "water droplets on insulating material influence strongly the aging process of the material. the shape of the droplets signifies the state of the aging material. the present paper discusses a numerical procedure to calculate the droplet shape in an electric field generated by a constant voltage. a combined stationary solution of the droplet shape and the electric field is searched for. this leads to a free boundary value problem which is solved by an iterative method. the typical shapes of the droplets are shown for several voltages. special care is taken to the singularities of the electric field in the triple points.",
    "present_kp": [
      "stationary electric field",
      "droplets",
      "free boundary value problem"
    ],
    "absent_kp": [
      "insulators",
      "corner points",
      "numerical analysis"
    ]
  },
  {
    "title": "an efficient quantum search engine on unsorted database.",
    "abstract": "we consider the problem of finding one or more desired items out of an unsorted database. patel has shown that if the database permits quantum queries, then mere digitization is sufficient for efficient search for one desired item. the algorithm, called factorized quantum search algorithm, presented by him can locate the desired item in an unsorted database using o((log_4n)) queries to factorized oracles. but the algorithm requires that all the attribute values must be distinct from each other. in this paper, we discuss how to make a database satisfy the requirements, and present a quantum search engine based on the algorithm. our goal is achieved by introducing auxiliary files for the attribute values that are not distinct, and converting every complex query request into a sequence of calls to factorized quantum search algorithm. the query complexity of our algorithm is o((log_4n)) for most cases.",
    "present_kp": [
      "quantum search algorithm"
    ],
    "absent_kp": [
      "quantum search model",
      "quantum computation",
      "database search"
    ]
  },
  {
    "title": "finite element modeling of borehole heat exchanger systems part 1. fundamentals.",
    "abstract": "single borehole heat exchanger (bhe) and arrays of bhe are modeled by using the finite element method. the first part of the paper derives the fundamental equations for bhe systems and their finite element representations, where the thermal exchange between the borehole components is modeled via thermal transfer relations. for this purpose improved relationships for thermal resistances and capacities of bhe are introduced. pipe-to-grout thermal transfer possesses multiple grout points for double u-shape and single u-shape bhe to attain a more accurate modeling. the numerical solution of the final 3d problems is performed via a widely non-sequential (essentially non-iterative) coupling strategy for the bhe and porous medium discretization. four types of vertical bhe are supported: double u-shape (2u) pipe, single u-shape (1u) pipe, coaxial pipe with annular (cxa) and centred (cxc) inlet. two computational strategies are used: (1) the analytical bhe method based on eskilson and claesson's (1988) solution, (2) numerical bhe method based on al-khoury et al.'s (2005) solution. the second part of the paper focusses on bhe meshing aspects, the validation of bhe solutions and practical applications for borehole thermal energy store systems.",
    "present_kp": [
      "borehole heat exchanger",
      "thermal resistances"
    ],
    "absent_kp": [
      "finite elements",
      "local problem",
      "static condensation"
    ]
  },
  {
    "title": "improving fairness in a wred-based diffserv network: a fluid-flow approach.",
    "abstract": "the diffserv architecture has been proposed as a scalable approach for upgrading the internet, adding service differentiation functionalities. however, several aspects of this architecture still have to be analyzed and solved. for this reason, network designers need to be provided with tools which are able to estimate the average behavior of a diffserv network with a high level of accuracy and in a short time. in this paper a fluid-flow model of a diffserv network supporting assured forwarding per-hop behavior (phb) and loaded with to flows is proposed. at the edge of the network, two rate three color markers (trtcm) are employed as profile meters, while within the network core routers implement a weighted red (wred) buffer management mechanism. in order to demonstrate the high accuracy of the proposed model, a comparison between model and simulation results is performed, taking into account not just a bottleneck link, but a complex network topology. the proposed analytical framework is then used to analyze the impact of several factors on the fair sharing of network resources between traffic aggregates with the same traffic profile, and to achieve some guidelines for wred parameter settings with the aim of reducing unfairness.",
    "present_kp": [
      "diffserv architecture",
      "wred",
      "fairness"
    ],
    "absent_kp": [
      "tcp",
      "fluid-flow performance analysis"
    ]
  },
  {
    "title": "automatic segmentation of corpus collasum using gaussian mixture modeling and fuzzy c means methods.",
    "abstract": "this paper presents a comparative study of the success and performance of the gaussian mixture modeling and fuzzy c means methods to determine the volume and cross-sectionals areas of the corpus callosum (cc) using simulated and real mr brain images. the gaussian mixture model (gmm) utilizes weighted sum of gaussian distributions by applying statistical decision procedures to define image classes. in the fuzzy c means (fcm), the image classes are represented by certain membership function according to fuzziness information expressing the distance from the cluster centers. in this study, automatic segmentation for midsagittal section of the cc was achieved from simulated and real brain images. the volume of cc was obtained using sagittal sections areas. to compare the success of the methods, segmentation accuracy, jaccard similarity and time consuming for segmentation were calculated. the results show that the gmm method resulted by a small margin in more accurate segmentation (midsagittal section segmentation accuracy 98.3% and 97.01% for gmm and fcm); however the fcm method resulted in faster segmentation than gmm. with this study, an accurate and automatic segmentation system that allows opportunity for quantitative comparison to doctors in the planning of treatment and the diagnosis of diseases affecting the size of the cc was developed. this study can be adapted to perform segmentation on other regions of the brain, thus, it can be operated as practical use in the clinic.",
    "present_kp": [
      "gaussian mixture modeling",
      "fuzzy c means",
      "corpus collasum"
    ],
    "absent_kp": [
      "image segmentation"
    ]
  },
  {
    "title": "an enhanced thermal face recognition method based on multiscale complex fusion for gabor coefficients.",
    "abstract": "the recognition of thermal face is a very promising strategy in biometrics. it is invariant to illumination, robust to pose and immune to forgery. however, thermal face image consist of face heat energy and face counter information mainly, and it makes lower discrimination for inter-class. in this paper, an enhanced thermal face recognition approach based on multiscale complex fusion for gabor coefficients (mcfg) is proposed. initially, the complex gabor jet descriptor (cgjd) is acquired based on the block mean and standard deviation generated from the magnitude, phase, real and imaginary parts of gabor coefficients. then, the complex lda (clda) algorithm and feature level fusion are implemented on multiscale gabor coefficients to reduce the dimension and enhance the discrimination. experiments conducted on two thermal face databases nvie and iris, which have some challenging thermal face images, show that the proposed thermal face recognition approach significantly outperforms the state-of-the-art approaches.",
    "present_kp": [
      "thermal face recognition",
      "multiscale",
      "complex gabor jet descriptor",
      "complex lda",
      "feature level fusion"
    ],
    "absent_kp": []
  },
  {
    "title": "an intrusion detection model based upon intrusion detection markup language (idml).",
    "abstract": "due to the rapid growth of networked computer resources and the increasing importance of related applications, intrusions which threaten the infrastructure of these applications have are critical problems. in recent years, several intrusion detection systems designed to identify and detect possible intrusion behaviors. in this work, an intrusion detection model is proposed to for building an intrusion detection system which can solve problems involved in building an intrusion detection systems, including pattern representation. computability, performance, extendibility and maintenance problems. in this model, idml is first designed to express intrusion patterns, and these patterns are transformed into intrusion pattern state machines. once the intrusion pattern state machines are obtained, the corresponding intrusion detection mechanism that can use these state machines to detect intrusions is designed. to evaluate the performance of our model, an idml-based intrusion detection experimental system based upon this architecture has been implemented.",
    "present_kp": [
      "intrusion detection",
      "intrusion pattern",
      "idml"
    ],
    "absent_kp": [
      "xml",
      "finite state machine"
    ]
  },
  {
    "title": "tcp with adaptive pacing for multihop wireless networks.",
    "abstract": "in this paper, we introduce a novel congestion control algorithm for tcp over multihop ieee 802.11 wireless networks implementing rate-based scheduling of transmissions within the tcp congestion window. we show how a tcp sender can adapt its transmission rate close to the optimum using an estimate of the current 4-hop propagation delay and the coefficient of variation of recently measured round-trip times. the novel tcp variant is denoted as tcp with adaptive pacing (tcp-ap). opposed to previous proposals for improving tcp over multihop ieee 802.11 networks, tcp-ap retains the end-to-end semantics of tcp and does neither rely on modifications on the routing or the link layer nor requires cross-layer information from intermediate nodes along the path. a comprehensive simulation study using ns-2 shows that tcp-ap achieves up to 84% more goodput than tcp newreno, provides excellent fairness in almost all scenarios, and is highly responsive to changing traffic conditions.",
    "present_kp": [
      "ieee 802.11 wireless networks"
    ],
    "absent_kp": [
      "end-to-end congestion control",
      "performance evaluation"
    ]
  },
  {
    "title": "what do transition organizations lack to be innovative.",
    "abstract": "purpose - innovativeness is probably the most effective way for organizations in the transition economies to improve their competitiveness. the purpose of this research is to find those factors, which significantly contribute to the innovativeness of organizations in the oldest market-based economies but they have not been developed in the transition organizations. design/methodology/approach - a range of recently published works, which aimed to provide different theoretical findings and best practice of innovative economies highlighted organizational culture, entrepreneurship and market orientation as the most important factors influencing the organizational innovation intensity and sustained competitive advantage. these factors and their relationships together with the characteristics of the transition economies were applied to put hypotheses about their impact on innovation intensity in the transition organizations. the structural equation modeling was used to test these hypotheses on the sample of 214 slovenian organizations. findings - the comparison of both innovation models, i.e. one significant for the innovative organizations, and the other found for the transition organizations showed that innovative organizational culture and market orientation have been the most important missing factors preventing the transition organizations from being innovative and thus achieving the sustained competitive advantage. research limitations/implications - data collection was limited to one country, i.e. slovenia. practical implications; - the results obtained show that organizations cannot be innovative if all the most important factors influencing the innovation capability are not equally developed. originality/value - the missing factors and relations which have been the main obstacles for the transition organizations to be more innovative are found by benchmarking based on the structural equation modeling.",
    "present_kp": [
      "innovation",
      "competitive advantage"
    ],
    "absent_kp": [
      "cybernetics",
      "transition management"
    ]
  },
  {
    "title": "perspectives on the performance of supply chain systems: the effects of attitude and assimilation.",
    "abstract": "the introduction of information systems into industry to enhance operational efficiency is a common business strategy. introducing such information systems should be expected to enhance employee satisfaction if the systems work as expected. based on previous studies of supply chain management (scm) and using institution theory, and technology use and acceptance models, this study applies the lens of attitude and assimilation to explore employee behavior toward using scm systems after implementation, as well as how employees' attitude and assimilation impact on the performance of the supply chain. collecting data from employees in the relevant departments from selected companies through a survey and applying sem in the analysis, the study finds that the employee's perceived ease of use, perceived usefulness, and the subjective norm (theory of reasoned action) have a positive impact on their attitudes toward their use of the scm systems. furthermore, it was found that top management's positive beliefs and participation also have a positive impact on assimilation and scm performance. finally, it was found that attitudes toward the use of scm systems impact upon information performance but not in other outcomes of scm operations. implications for research and practice are raised. this study contributes to the body of knowledge by adding to our understanding of the relationships among users' attitude, assimilation, and business performance at the post-adoption stage within the context of scm systems.",
    "present_kp": [
      "post-adoption",
      "attitude and assimilation",
      "supply chain management"
    ],
    "absent_kp": [
      "information systems performance"
    ]
  },
  {
    "title": "test set embedding for deterministic bist using a reconfigurable interconnection network.",
    "abstract": "we present a new approach for deterministic built-in self-test (bist) in which a reconfigurable interconnection network (rin) is placed between the outputs of a pseudorandom pattern generator and the scan inputs of the circuit under test (cut). the rin, which consists only of multiplexer switches, replaces the phase shifter that is typically used in pseudorandom bist to reduce correlation between the test data bits that are fed into the scan chains. the connections between the linear-feedback shift-register (lfsr) and the scan chains can be dynamically changed (reconfigured) during a test session. in this way, the rin is used to match the lfsr outputs to the test cubes in a deterministic test set. the control data bits used for reconfiguration ensure that all the deterministic test cubes are embedded in the test patterns applied to the cut. the proposed approach requires very little hardware overhead, only a modest amount of cpu time, and fewer control bits compared to the storage required for reseeding techniques or for hybrid bist. moreover, as a nonintrusive bist solution, it does not require any circuit redesign and has minimal impact on circuit performance.",
    "present_kp": [],
    "absent_kp": [
      "embedded core testing",
      "system-on-a-chip  testing",
      "test application time",
      "test-data volume"
    ]
  },
  {
    "title": "brightness preserving image contrast enhancement using weighted mixture of global and local transformation functions.",
    "abstract": "transformation functions utilizing the global information content of an input image have been long serving contrast enhancement by stretching the dynamic range of intensity levels other transformation functions focus on local information content to correct image details such as edges and texture in this paper an effective method for image contrast enhancement is presented with a mapping function which is a mixture of global and local transformation functions that improve both the brightness and fine details of the input image the final mapping function incorporates a local intensity-pair distribution generated expansion function from each image block to control the enhancement of image details' that the global transformation function alone may fail to improve contribution from the global transformation function preserves the overall image brightness and contrast stretching experiments show that the proposed method competes well with the crating enhancement methods' both subjectively and quantitatively",
    "present_kp": [
      "contrast enhancement",
      "global transformation function",
      "local transformation function"
    ],
    "absent_kp": [
      "intensity pair"
    ]
  },
  {
    "title": "statistical inference for the geometric distribution based on -records.",
    "abstract": "new inferential procedures for the geometric distribution, based on -records, are developed. maximum likelihood and bayesian approaches for parameter estimation and prediction of future records are considered. the performance of the estimators is compared with those based solely on record-breaking data by means of monte carlo simulations, concluding that the use of -records is clearly advantageous. an example using real data is also discussed.",
    "present_kp": [
      " -records ",
      "geometric distribution"
    ],
    "absent_kp": [
      "maximum likelihood estimation",
      "bayes estimation",
      "empirical bayes",
      "maximum likelihood prediction",
      "bayes prediction"
    ]
  },
  {
    "title": "data envelopment analysis with discrete-valued inputs and outputs.",
    "abstract": "the conventional data envelopment analysis (dea) measures the relative efficiencies of a set of decision making units, which has real-valued inputs and outputs. in many real occasions, there are cases in which some inputs and/or outputs must only take some particularised real or integer values. these values are referred to as discrete data. the need to deal with discrete-valued data in dea naturally occurs when we use real-valued or integer-valued dea, but the reduced inputs and outputs of an inefficient decision making unit are restricted to some particular real or integer values. in this case, axioms such as unbounded ray, convexity and free disposability as discussed in the conventional dea are violated. in this paper, an alternative extension of dea in the case of discrete-valued data has been faced. an axiomatic foundation is proposed to define the new production possibility set. in order to exemplify the method, a case of 25 iranian gas companies' activities are illustrated.",
    "present_kp": [
      "data envelopment analysis",
      "discrete-valued data"
    ],
    "absent_kp": [
      "efficiency",
      "mixed integer linear program",
      "msc: "
    ]
  },
  {
    "title": "shifting-level process as a lrd video traffic model and related queuing results.",
    "abstract": "recently it has been reported that variable bit rate (vbr) video traffic exhibits long-range dependence (lrd). various processes have been proposed for modeling traffic with lrd and analyzing its effects on network performance. however, in the previous models it is not possible to identify the effects of short- and long-term correlation of video traffic on queuing performance, and thus many seemingly contradictory arguments on the importance of lrd in ver video traffic can be found in the literature. in this paper, we present a video traffic model based on the shifting-level (sl) process. we observe that the autocorrelation function (acf) of an empirical video trace is accurately captured by a shifting-level process with compound correlation (slcc): an exponential decay for small lags and a hyperbolic one for large lags. especially, we present a parameter matching algorithm for video traffic. the continuous-time first-order discrete auto-regressive (c-dar(1)) model, which is a short-range dependent (srd) video traffic model, can be considered a kind of slcc process with an exponential correlation term only. thus, comparing the queuing performances of the c-dar(1) model and the slcc with that of a real video trace, it is possible to identify the effects of srd and lrd in ver video traffic on queuing performance. from simulation results, we find that lrd may have a significant effect on queuing behavior under heavy traffic loads and large buffer conditions.",
    "present_kp": [
      "shifting-level process",
      "compound correlation",
      "long-range dependence",
      "queuing behavior"
    ],
    "absent_kp": [
      "vbr video traffic model",
      "short-range dependence"
    ]
  },
  {
    "title": "robust camera pose estimation using 2d fiducials tracking for real-time augmented reality systems.",
    "abstract": "augmented reality (ar) deals with the problem of dynamically and accurately align virtual objects with the real world. among the used methods, vision-based techniques have advantages for ar applications, their registration can be very accurate, and there is no delay between the motion of real and virtual scenes. however, the downfall of these approaches is their high computational cost and lack of robustness. to address these shortcomings we propose a robust camera pose estimation method based on tracking calibrated fiducials in a known 3d environment, the camera location is dynamically computed by the orthogonal iteration algorithm. experimental results show the robustness and the effectiveness of our approach in the context of real-time ar tracking.",
    "present_kp": [
      "augmented reality",
      "camera pose estimation",
      "fiducials tracking"
    ],
    "absent_kp": [
      "computer vision"
    ]
  },
  {
    "title": "variations on a theme by schalkwijk and kailath.",
    "abstract": "schalkwijk and kailath (1966) developed a class of block codes for gaussian channels with ideal feedback for which the probability of decoding error decreases as a second-order exponent in block length for rates below capacity. this well-known but surprising result is explained and simply derived here in terms of a result by elias (1956) concerning the minimum mean-square distortion achievable in transmitting a single gaussian random variable over multiple uses of the same gaussian channel. a simple modification of the schalkwijk-kailath scheme is then shown to have an error probability that decreases with an exponential order which is linearly increasing with block length. in the infinite bandwidth limit, this scheme produces zero error probability using bounded expected energy at all rates below capacity. a lower bound on error probability for the finite bandwidth case is then derived in which the error probability decreases with an exponential order which is linearly increasing in block length at the same rate as the upper bound.",
    "present_kp": [
      "block codes",
      "error probability",
      "feedback"
    ],
    "absent_kp": [
      "additive memoryless gaussian noise channel",
      "reliability",
      "schalkwijk-kailath encoding scheme"
    ]
  },
  {
    "title": "the case for simple, visible cache coherency.",
    "abstract": "the shared memory research community has proposed many complex communication protocols that aim to eliminate specific performance bottlenecks, while still providing an easy-to-use communication interface. although tailored protocols can eliminate some bottlenecks that arise in real applications, removing the cause of the bottleneck through software optimizations and bug fixes is cheaper to implement, faster to fix (once found), and requires no additional support by the hardware beyond a simple shared memory interface. in fact, in our experience, the choice of coherence protocol is much less important than providing an efficient hardware feedback that indentifies the source of the problem. future cache-coherence research should focus efforts on illuminating memory system behavior, providing smarter tools to identify bottlenecks, and helping to eliminate them through software optimizations.",
    "present_kp": [
      "coherence protocol",
      "performance bottlenecks",
      "software optimization"
    ],
    "absent_kp": [
      "specomp2001",
      "shared-memory multiprocessor",
      "cc-numa",
      "flash"
    ]
  },
  {
    "title": "a theoretical study of hypothalamo-pituitary-adrenocortical axis dynamics.",
    "abstract": "abstract: does the hypothalamo-pituitary-adrenocortical (hpa) axis itself generate oscillations? the affirmative answer to this question is commonly assumed, because a regular daily rhythm of its hormones is observed. we offer another plausible explanation of the origin of this pattern: hpa just responds to an external pacemaker. a qualitative mathematical model is constructed wherein all the terms in the equations are physicochemically interpretable. linear stability analysis shows that this system does not generate oscillations. computer simulation yields oscillations that are the system's response to an external pulsing activator, implying that the observed pattern does not have to be an intrinsic property of this system.",
    "present_kp": [
      "stability analysis"
    ],
    "absent_kp": [
      "hypothalamo-pituitary-adrenocortical  axis",
      "circadian rhythm",
      "nonlinear dynamics"
    ]
  },
  {
    "title": "gesture-based interaction with voice feedback for a tour-guide robot.",
    "abstract": "we describe a guide robot with advanced humanrobot interaction. the robot includes a laser scanner, a range camera and a rgb camera. we describe the human detection, tracking, recognition and interaction capabilities. its quality in use is evaluated with over 20 users at real world environments. the robot participates in four events with successful results.",
    "present_kp": [
      "tour-guide robot",
      "human detection"
    ],
    "absent_kp": [
      "gesture interface",
      "human-robot interaction",
      "gesture recognition",
      "human following",
      "human identification",
      "human tracking"
    ]
  },
  {
    "title": "automated marsh-like classification of celiac disease in children using local texture operators.",
    "abstract": "automated classification of duodenal texture patches with histological ground truth in case of pediatric celiac disease is proposed. the classical focus of classification in this context is a two-class problem: mucosa affected by celiac disease and unaffected duodenal tissue. we extend this focus and apply classification according to a modified marsh scheme into four classes. in addition to other techniques used previously for classification of endoscopic imagery, we apply local binary pattern (lbp) operators and propose two new operator types, one of which adapts to the different properties of wavelet transform subbands. the achieved results are promising in that operators based on lbp turn out to achieve better results compared to many other texture classification techniques as used in earlier work. specifically, the proposed wavelet-based lbp scheme achieved the best overall accuracy of all feature extraction techniques considered in the two-class case and was among the best in the four-class scheme. results also show that a classification into four classes is feasible in principle however when compared to the two-class case we note that there is still room for improvement due to various reasons discussed.",
    "present_kp": [
      "celiac disease",
      "lbp",
      "children"
    ],
    "absent_kp": [
      "computer-aided classification",
      "endoscopy",
      "marsh classification"
    ]
  },
  {
    "title": "decomposition-based logic synthesis for pal-based cplds.",
    "abstract": "the paper presents one concept of decomposition methods dedicated to pal-based cplds. the proposed approach is an alternative to the classical one, which is based on two-level minimization of separate single-output functions. the key idea of the algorithm is to search for free blocks that could be implemented in pal-based logic blocks containing a limited number of product terms. in order to better exploit the number of product terms, two-stage decomposition and bdd-based decomposition are to be used. in bdd-based decomposition methods, functions are represented by reduced ordered binary decision diagrams (robdds). the results of experiments prove that the proposed solution is more effective, in terms of the usage of programmable device resources, compared with the classical ones.",
    "present_kp": [
      "decomposition",
      "bdd",
      "cpld"
    ],
    "absent_kp": [
      "technology mapping",
      "logic optimization"
    ]
  },
  {
    "title": "online resource for theoretical study of hydration of biopolymers.",
    "abstract": "an online resource has been developed for the theoretical study of hydration of biopolymers by the rism (reference interaction site model) method, deriving from the integral equation theory of liquids. the online resource is based upon original software developed by the authors and includes all steps in studying a biopolymer with a given spatial structure and force field. it prepares the input data and carries out the rism calculation yielding the atom-atom correlation functions of the biopolymer with water as solvent. from these functions the algorithm finds atomic partial contributions to the hydration free energy using various free energy expressions from integral equation theory. the calculated results are automatically recorded in a database, and become available on the website as tables of partial thermodynamic quantities. in addition, the website displays an interactive 3d model of a given molecule, the atoms of which can be painted in different colors in accordance with their partial contributions to the thermodynamic quantity chosen by the user. the user can interactively choose atoms on this molecule and their correlation functions will be displayed. the aim of our work was to develop and present a publicly-accessible resource on the basis of original software which could be used for scientific and educational purposes.",
    "present_kp": [
      "rism",
      "hydration",
      "free energy",
      "biopolymers"
    ],
    "absent_kp": [
      "online computations"
    ]
  },
  {
    "title": "a modular and parameterized presentation of pregroup calculus.",
    "abstract": "the concept of pregroup was introduced by lambek for natural language analysis, with a close link to non-commutative linear logic. we reformulate the pregroup calculus so as to extend it by composition with other logics and calculi. the cut elimination property and the decidability property of the sequent calculus proposed in the article are shown. properties of composed calculi are also discussed.",
    "present_kp": [
      "cut elimination"
    ],
    "absent_kp": [
      "pregroups",
      "lambek categorial grammars",
      "logic functor",
      "logic component"
    ]
  },
  {
    "title": "products of mealy-type fuzzy finite state machines.",
    "abstract": "we introduce seven ways of constructing products for mealy-type fuzzy finite state machines as well as the concept of covering. the properties of transition functions and output functions are discussed. furthermore, we investigate the covering properties and mutual relationship with regard to these products. we also prove that the covering relationship holds in the product of factor machines.",
    "present_kp": [],
    "absent_kp": [
      "algebra",
      "fuzzy finite automata",
      "mealy-type fuzzy finite automata",
      "cascade product",
      "wreath product"
    ]
  },
  {
    "title": "automatic code generation and solution estimate for object-oriented embedded software.",
    "abstract": "this work tailors an alloy model translator to java code and an estimate tool for physical resources optimization into a design-flow. experimental results show distinct implementation strategies only varying data structures used in generated java code.",
    "present_kp": [
      "alloy",
      "java",
      "code generation"
    ],
    "absent_kp": [
      "design space exploration",
      "software automation",
      "embedded systems",
      "modeling languages"
    ]
  },
  {
    "title": "three-dimensional structure determination from common lines in cryo-em by eigenvectors and semidefinite programming.",
    "abstract": "the cryo-electron microscopy reconstruction problem is to find the three-dimensional (3d) structure of a macromolecule given noisy samples of its two-dimensional projection images at unknown random directions. present algorithms for finding an initial 3d structure model are based on the \"angular reconstitution\" method in which a coordinate system is established from three projections, and the orientation of the particle giving rise to each image is deduced from common lines among the images. however, a reliable detection of common lines is difficult due to the low signal-to-noise ratio of the images. in this paper we describe two algorithms for finding the unknown imaging directions of all projections by minimizing global self-consistency errors. in the first algorithm, the minimizer is obtained by computing the three largest eigenvectors of a specially designed symmetric matrix derived from the common lines, while the second algorithm is based on semidefinite programming (sdp). compared with existing algorithms, the advantages of our algorithms are five-fold: first, they accurately estimate all orientations at very low common-line detection rates; second, they are extremely fast, as they involve only the computation of a few top eigenvectors or a sparse sdp; third, they are nonsequential and use the information in all common lines at once; fourth, they are amenable to a rigorous mathematical analysis using spectral analysis and random matrix theory; and finally, the algorithms are optimal in the sense that they reach the information theoretic shannon bound up to a constant for an idealized probabilistic model.",
    "present_kp": [
      "cryo-electron microscopy",
      "angular reconstitution",
      "semidefinite programming"
    ],
    "absent_kp": [
      "random matrices",
      "semicircle law",
      "rotation group so",
      "tomography"
    ]
  },
  {
    "title": "research themes and trends in health information systems.",
    "abstract": "objectives: the health information systems (his) field is characterized as being associated with health care and information systems. drawing on several disciplines, a body of knowledge has come together that help define the core internal structure of the field. this study attempts to identify the emerging sub-fields using the bibliometric technique of author-cocitation analysis. methods. co-citation data for members of editorial boards of several health information systems journals for the period of <phone> was collected and analyzed (n = 166). we performed numerous multivariate analyses, including cluster analysis, factor analysis and multidimensional scaling to extract the sub-fields. results. our findings indicate the presence of several strong sub-fields, including his evaluation, communication and e-health, and clinical dss. in addition, we identified other sub-fields that are distinct but still emerging, such as adoption, outcome and policy, and use and impact of his. the study also confirms the existence of several historical sub-fields and contrasts technology-oriented sub-fields with management-oriented sub-fields. topics on the periphery of his provide links to other disciplines as well. conclusions. the study provides a unique perspective on the field of his, and the results indicate opportunities for further research that explores collaborations and social networks among the sub-fields.",
    "present_kp": [
      "field",
      "sub-field"
    ],
    "absent_kp": [
      "author co-citation analysis ",
      "bibliometrics",
      "health information system "
    ]
  },
  {
    "title": "a standardized blood sampling scheme in quantitative fdg-pet studies.",
    "abstract": "quantitative estimation of brain glucose metabolism (rcmrglc) with positron emission tomography and fluorodeoxyglucose involves arterial blood sampling to estimate the delivery of radioactivity to the brain. usually, for an intravenous injection of 30 s duration, an accurate input curve requires a frequency of one sample every 5 s or less to determine the peak activity in arterial plasma during the first 2 min after injection. in this work, 13 standardized sampling times were shown to be sufficient to accurately define the input curve. this standardized input curve was subsequently fitted by a polynomial function for its rising part and by spectral analysis for its decreasing part. using the measured, the standardized, and the fitted input curves, rcmrglc was estimated in 32 cerebral regions of interest in 20 normal volunteers, comparison of rcmrglc values obtained with the measured and the fitted input curves showed that both procedures gave consistent results, with a maximal relative error in mean rcmrglc of 1% when using the autoradiographic method and 2% using kinetic analysis of dynamic data, this input-curve-fitting technique, which is not dependent on the peak time occurrence, allows an accurate determination of the input-curve shape from reduced sampling schemes.",
    "present_kp": [
      "blood sampling",
      "fdg",
      "input curve",
      "pet"
    ],
    "absent_kp": []
  },
  {
    "title": "reynolds stress modelling of rectangular open-channel flow.",
    "abstract": "a reynolds stress model for the numerical simulation of uniform 3d turbulent open-channel flows is described. the finite volume method is used for the numerical solution of the flow equations and transport equations of the reynolds stress components. the overall solution strategy is the simpler algorithm, and the power-law scheme is used to discretize the convection and diffusion terms in the governing equations. the developed model is applied to a flow at a reynolds number of 77000 in a rectangular channel with a width to depth ratio of 2. the simulated mean flow and turbulence structures are compared with measured and computed data from the literature. the computed flow vectors in the plane normal to the streamwise direction show a small vortex, called inner secondary currents, located at the juncture of the sidewall and the free surface as well as the free surface and bottom vortices. this small vortex causes a significant increase in the wall shear stress in the vicinity of the free surface. a budget analysis of the streamwise vorticity is carried out. it is found that both production terms by anisotropy of reynolds normal stress and by reynolds shear stress contribute to the generation of secondary currents.",
    "present_kp": [
      "open-channel flows",
      "turbulence",
      "reynolds stress model",
      "secondary currents",
      "inner secondary currents",
      "budget analysis"
    ],
    "absent_kp": []
  },
  {
    "title": "loss of phase-locking in non-weakly coupled inhibitory networks of type-i model neurons.",
    "abstract": "synchronization of excitable cells coupled by reciprocal inhibition is a topic of significant interest due to the important role that inhibitory synaptic interaction plays in the generation and regulation of coherent rhythmic activity in a variety of neural systems. while recent work revealed the synchronizing influence of inhibitory coupling on the dynamics of many networks, it is known that strong coupling can destabilize phase-locked firing. here we examine the loss of synchrony caused by an increase in inhibitory coupling in networks of type-i morrislecar model oscillators, which is characterized by a period-doubling cascade and leads to mode-locked states with alternation in the firing order of the two cells, as reported recently by maran and canavier (j comput nerosci, 2008) for a network of wang-buzski model neurons. although alternating-order firing has been previously reported as a near-synchronous state, we show that the stable phase difference between the spikes of the two morrislecar cells can constitute as much as 70% of the unperturbed oscillation period. further, we examine the generality of this phenomenon for a class of type-i oscillators that are close to their excitation thresholds, and provide an intuitive geometric description of such leap-frog dynamics. in the morrislecar model network, the alternation in the firing order arises under the condition of fast closing of k?+? channels at hyperpolarized potentials, which leads to slow dynamics of membrane potential upon synaptic inhibition, allowing the presynaptic cell to advance past the postsynaptic cell in each cycle of the oscillation. further, we show that non-zero synaptic decay time is crucial for the existence of leap-frog firing in networks of phase oscillators. however, we demonstrate that leap-frog spiking can also be obtained in pulse-coupled inhibitory networks of one-dimensional oscillators with a multi-branched phase domain, for instance in a network of quadratic integrate-and-fire model cells. finally, for the case of a homogeneous network, we establish quantitative conditions on the phase resetting properties of each cell necessary for stable alternating-order spiking, complementing the analysis of goel and ermentrout (physica d 163:191216, 2002) of the order-preserving phase transition map.",
    "present_kp": [
      "synchronization",
      "inhibitory network",
      "synaptic inhibition",
      "phase resetting"
    ],
    "absent_kp": [
      "non-weak coupling",
      "non-synchronous dynamics",
      "type-i excitability",
      "leader switching",
      "spike-time response"
    ]
  },
  {
    "title": "an application of shepwm technique in a cascade multilevel inverter.",
    "abstract": "purpose - in this paper, a new application of the selected harmonic elimination pulse width modulation (shepwm) technique used in the cascade multilevel inverter topology which is formed by series connections of one-phase bridge type inverters (h-bridge) is introduced. the advantage of the shepwm technique is its ability to operate in low switching frequency that makes it suitable for high power applications. design/methodology/approach - first, the switching angles are calculated using constrained optimization technique. by using these switching angles, the fundamental harmonic can be controlled and the selected harmonics can be eliminated. then, using these calculated switching angles, a set of equation is formed which calculate the switching angles with respect to the modulation index. the switching angles at any modulation index can be easily obtained by solving the equation set. in this study, this equation set has been solved online using dspace ds1103 controller board. using this technique, three-phase voltages have been obtained from a five-level cascade inverter. these voltages are applied to an induction motor. findings - the simulation results are verified by the experimental results. the results show that selected harmonics can be eliminated and an ac voltage with variable amplitude and frequency can be obtained using the proposed technique. originality/value - this paper presents a new application of the (shepwm) technique for multilevel inverters.",
    "present_kp": [
      "harmonics"
    ],
    "absent_kp": [
      "electric converters",
      "optimization techniques"
    ]
  },
  {
    "title": "a simple and robust three-dimensional cracking-particle method without enrichment.",
    "abstract": "a new robust and efficient approach for modeling discrete cracks in meshfree methods is described. the method is motivated by the cracking-particle method (rabczuk t., belytschko t., international journal for numerical methods in engineering, 2004) where the crack is modeled by a set of cracked segments. however, in contrast to the above mentioned paper, we do not introduce additional unknowns in the variational formulation to capture the displacement discontinuity. instead, the crack is modeled by splitting particles located on opposite sides of the associated crack segments and we make use of the visibility method in order to describe the crack kinematics. we apply this method to several two- and three-dimensional problems in statics and dynamics and show through several numerical examples that the method does not show any mesh orientation bias.",
    "present_kp": [
      "meshfree methods"
    ],
    "absent_kp": [
      "cohesive crack model",
      "dynamic fracture"
    ]
  },
  {
    "title": "stability of small-scale uav helicopters and quadrotors with added payload mass under pid control.",
    "abstract": "the application of rotorcraft to autonomous load carrying and transport is a new frontier for unmanned aerial vehicles (uavs). this task requires that hovering vehicles remain stable and balanced in flight as payload mass is added to the vehicle. if payload is not loaded centered or the vehicle properly trimmed for offset loads, the robot will experience bias forces that must be rejected. in this paper, we explore the effect of dynamic load disturbances introduced by instantaneously increased payload mass and how those affect helicopters and quadrotors under proportional-integral-derivative flight control. we determine stability bounds within which the changing mass-inertia parameters of the system due to the acquired object will not destabilize these aircraft with this standard flight controller. additionally, we demonstrate experimentally the stability behavior of a helicopter undergoing a range of instantaneous step payload changes.",
    "present_kp": [
      "unmanned aerial vehicle",
      "quadrotor",
      "helicopter"
    ],
    "absent_kp": [
      "flight stability",
      "aircraft dynamics"
    ]
  },
  {
    "title": "a multiscale discontinuous galerkin method for convectiondiffusionreaction problems.",
    "abstract": "we provide a general framework of multiscale discontinuous galerkin methods developed in buffa etal. (2006), hughes etal. (2006) for general second-order partial differential equations. we establish stability of the method and prove the error estimates.",
    "present_kp": [
      "discontinuous galerkin method",
      "multiscale discontinuous galerkin method",
      "convectiondiffusionreaction problem"
    ],
    "absent_kp": []
  },
  {
    "title": "three-dimensional thinning algorithms on graphics processing units and multicore cpus.",
    "abstract": "three-dimensional curve skeletons are a very compact representation of three-dimensional objects with many uses and applications in ?elds such as computer graphics, computer vision, and medical imaging. an important problem is that the calculation of the skeleton is a very time-consuming process. thinning is a widely used technique for calculating the curve skeleton because of the properties it ensures and the ease of implementation. in this paper, we present parallel versions of a thinning algorithm for e?cient implementation in both graphics processing units and multicore cpus. the parallel programming models used in our implementations are compute uni?ed device architecture (cuda) and open computing language (opencl). the speedup achieved with the optimized parallel algorithms for the graphics processing unit achieves 106.24x against the cpu single-process version and more than 19x over the cpu multithreaded version.",
    "present_kp": [
      "thinning algorithm",
      "cuda",
      "opencl"
    ],
    "absent_kp": [
      "3d curve skeleton"
    ]
  },
  {
    "title": "scif-iris framework: a framework to facilitate interoperability in supply chains.",
    "abstract": "one approach that allows improving the collaboration among all the enterprises within a supply chain is interoperability. interoperability allows the enterprises in the supply chain to collaborate in an efficient manner while preserving their own identities and their own ways of doing business through mechanisms that act as facilitators. however, there are few real practical examples of supply chain interoperability that can be used as a reference. in this paper, we present a framework that can facilitate supply chain interoperability and an example of how it can be applied to a food supply chain.",
    "present_kp": [
      "interoperability"
    ],
    "absent_kp": [
      "collaborative network"
    ]
  },
  {
    "title": "global pairwise sequence alignment through mixed-integer linear programming: a template-free approach.",
    "abstract": "the problem of protein sequence alignment is the main starting point for biological analysis of genomic information. a new approach to global pairwise sequence alignment is proposed. this approach is formulated in a mathematically rigorous way, as a mixed integer linear optimization (milp) problem. not only does the proposed formulation guarantee the identification of the global optimal alignment, but it also allows for a complete rank-ordered list of any number of user specified top ranking pairwise alignments and for the incorporation of restraints based on restrictions necessary to maintain biological function. this approach has been applied to the alignment of transmembrane helices and serine proteases to demonstrate its utility and advantages over other algorithms.",
    "present_kp": [
      "global pairwise sequence alignment"
    ],
    "absent_kp": [
      "mixed-integer linear optimization",
      "theoretical guarantee",
      "rank-ordered list of alignments"
    ]
  },
  {
    "title": "the sociological concept of autopoiesis - biological and philosophical basics and governance relevance.",
    "abstract": "purpose - to explore the sociological concept of autopoiesis (n. luhmann), investigate its interdisciplinary roots and demonstrate its practical relevance. design/methodology/approach - the biological concept of autopoiesis (h. maturana/f. varela) and the philosophical concept of meaning (e. husserl) are first discussed with respect to their contribution to the development of the sociological concept of autopoiesis. the autopoietic mechanism of three different social systems is then described, and the practical relevance of the sociological concept of autopoiesis demonstrated using the example of governance. findings - the scientific positioning of the sociological approach to autopoiesis is two-fold. on the one hand, it is firmly rooted in the scientific tradition and, on the other, its originality is determined by the adaptation and new combination of existing concepts. although this adaptation-combination process has provoked some criticism, the result does matter because it enriches the theoretical and empirical analysis which we use to explain the dynamics of modem societies. practical implications - the application of the sociological concept of autopoiesis to politics gives new insights into the opportunities and barriers of governance processes. originality/value - positioning of the sociological concept of autopoiesis within the scientific tradition and its application (beyond metaphorical usage) as an analytical tool.",
    "present_kp": [
      "social systems",
      "governance"
    ],
    "absent_kp": []
  },
  {
    "title": "general be approach for three-dimensional dynamic fracture analysis.",
    "abstract": "a general mixed boundary element approach for three-dimensional dynamic fracture mechanics problems is presented in this paper. a mixed traction-displacement integral equation formulation in the frequency domain is used. the hypersingular and strongly singular kernels are regularized by analytical transformations yielding an easy to implement be approach. nine-node quadrilateral and six-node triangular continuous quadratic elements are used for external boundaries and crack surfaces. the crack front elements have their mid node at one quarter of the element length allowing for a proper representation of the crack surface displacement. the present approach is intended for the frequency domain analysis of fracture mechanics problems of any general 3d geometry; i.e. boundless or bounded regions, single or multiple, surface or internal cracks. transient dynamic problems are studied using the fft algorithm. the numerical results presented show the robustness and accuracy of the approach which requires a reasonable number of elements and degrees of freedom.",
    "present_kp": [],
    "absent_kp": [
      "boundary elements",
      "three-dimensional cracks",
      "dynamics",
      "hypersingular integral equations"
    ]
  },
  {
    "title": "optimal design of axis symmetric linear oscillatory actuator using genetic algorithm.",
    "abstract": "the design optimization method of a linear oscillatory actuator (loa) using a genetic algorithm (ga) is presented. in order to optimize the dynamic performance of the loa, the motion analysis of the loa is coupled with the magnetic field analysis using the finite element method (fem). the bubble system is also adopted for the mesh regeneration to protect distorted meshes. it was found that the dynamic performances were greatly improved through ga optimization.",
    "present_kp": [
      "finite element method",
      "design optimization"
    ],
    "absent_kp": [
      "genetic algorithms",
      "actuators"
    ]
  },
  {
    "title": "an evaluation scheme for assessing the worth of automatically generated design alternatives.",
    "abstract": "this paper introduces a tool called the designer preference modeler (dpm) that analyzes the designers decision making during concept evaluation, and constructs a designer preference model to be used for evaluation of automatically generated design alternatives. the method is based on establishing an interaction between a designer and a computational synthesis tool during conceptual design. the synthesis software generates design alternatives using a catalog of design knowledge formulated as grammar rules which describe how electromechanical designs are built. dpm carefully selects a set from these alternatives and presents it to the designer for evaluation. the designers evaluations are translated into a preference model that is subsequently used to search the solution space for best designs. application of the method to the design of a consumer product shows dpms range of capabilities.",
    "present_kp": [],
    "absent_kp": [
      "concept generation",
      "design automation",
      "design selection"
    ]
  },
  {
    "title": "gaussian random number generators.",
    "abstract": "rapid generation of high quality gaussian random numbers is a key capability for simulations across a wide range of disciplines. advances in computing have brought the power to conduct simulations with very large numbers of random numbers and with it, the challenge of meeting increasingly stringent requirements on the quality of gaussian random number generators (grng). this article describes the algorithms underlying various grngs, compares their computational requirements, and examines the quality of the random numbers with emphasis on the behaviour in the tail region of the gaussian probability density function.",
    "present_kp": [
      "random numbers",
      "gaussian",
      "simulation"
    ],
    "absent_kp": [
      "normal"
    ]
  },
  {
    "title": "time-integration of a hypoplastic constitutive equation in finite element modelling.",
    "abstract": "this paper presents the frame of a new method for the time-integration of a granular material's constitutive equation in finite element modelling. the constitutive equation considered here is a hypoplastic model developed by wu. the paper compares the theoretical and numerical performances of the new method with two existing strategies for the integration of hypoplastic constitutive equations.",
    "present_kp": [
      "integration"
    ],
    "absent_kp": [
      "finite element method",
      "hypoplasticity",
      "stability"
    ]
  },
  {
    "title": "toward an explicit construction of nonlinear codes exceeding the tsfasman-vladut-zink bound.",
    "abstract": "we consider asymptotically good nonlinear codes recently introduced by xing. the original definition of these codes relies on a nonconstructive averaging argument. in this correspondence, it is first shown that in some cases, the codes can be constructed without using any averaging arguments. we then introduce an alternative construction of the codes, based on the union of a geometric goppa code and its cosets. in some cases, the problem of explicitly describing the codes reduces to the problem of explicitly describing certain n elements of the relevant function field, where n is the code length. moreover, the number of finite-field operations required to construct these n elements after the construction of the generator matrix of the geometric goppa code is of the order of n(3).",
    "present_kp": [
      "nonlinear codes"
    ],
    "absent_kp": [
      "asymptotic bounds",
      "function fields"
    ]
  },
  {
    "title": "parametrizing del pezzo surfaces of degree 8 using lie algebras.",
    "abstract": "for a del pezzo surface of degree 8 given over the rationals we decide whether there is a rational parametrization of the surface and construct one in the affirmative case. we define and use the lie algebra of the surface to reach the aim.",
    "present_kp": [
      "del pezzo surface",
      "parametrization",
      "lie algebra"
    ],
    "absent_kp": []
  },
  {
    "title": "the rough set theory and applications.",
    "abstract": "this paper presents a comprehensive review of the available literature on applications of the rough set theory. concepts of the rough set theory are discussed for approximation, dependence and reduction of attributes, decision tables and decision rules. the applications of rough sets are discussed in pattern recognition, information processing, business and finance, industry, environment engineering, medical diagnosis and medical data analysis, system fault diagnosis and monitoring and intelligent control systems. development trends and future efforts are outlined. an extensive list of references is also provided to encourage interested readers to pursue further investigations.",
    "present_kp": [
      "set theory"
    ],
    "absent_kp": [
      "decision theory",
      "approximation theory"
    ]
  },
  {
    "title": "a data-driven study of image feature extraction and fusion.",
    "abstract": "feature analysis is the extraction and comparison of signals from multimedia data, which can subsequently be semantically analyzed. feature analysis is the foundation of many multimedia computing tasks such as object recognition, image annotation, and multimedia information retrieval. in recent decades, considerable work has been devoted to the research of feature analysis. in this work, we use large-scale datasets to conduct a comparative study of four state-of-the-art, representative feature extraction algorithms: color-texture codebook (ct), sift codebook, hmax, and convolutional networks (convnet). our comparative evaluation demonstrates that different feature extraction algorithms enjoy their own advantages, and excel in different image categories. we provide key observations to explain where these algorithms excel and why. based on these observations, we recommend feature extraction principles and identify several pitfalls for researchers and practitioners to avoid. furthermore, we determine that in a large training dataset with more than 10,000 instances per image category, the four evaluated algorithms can converge to the same high level of category-prediction accuracy. this result supports the effectiveness of the data-driven approach. finally, based on learned clues from each algorithms confusion matrix, we devise a fusion algorithm to harvest synergies between these four algorithms and further improve class-prediction accuracy.",
    "present_kp": [
      "image annotation",
      "image feature",
      "fusion"
    ],
    "absent_kp": [
      "web-scale"
    ]
  },
  {
    "title": "deno: a decentralized, peer-to-peer object-replication system for weakly connected environments.",
    "abstract": "this paper presents the design, implementation, and evaluation of the replication framework of deno, a decentralized, peer-to-peer object-replication system targeted for weakly connected environments. deno uses weighted voting for availability and pair-wise, epidemic information flow for flexibility. this combination allows the protocols to operate with less than full connectivity, to easily adapt to changes in group membership, and to make few assumptions about the underlying network topology. we present two versions of deno's protocol that differ in the consistency levels they support. we also propose security extensions to handle a class of malicious actions that involve misrepresentation of protocol information. deno has been implemented and runs on top of linux and win32 platforms. we use the deno prototype to characterize the performance of the deno protocols and extensions. our study reveals several interesting results that provide fundamental insight into the benefits of decentralization and the mechanics of epidemic protocols.",
    "present_kp": [
      "epidemic protocols",
      "voting"
    ],
    "absent_kp": [
      "data replication",
      "peer-to-peer systems",
      "weak consistency"
    ]
  },
  {
    "title": "pmcnoc: a pipelining multi-channel central caching network-on-chip communication architecture design.",
    "abstract": "with the de facto transformation of technology into nano-technology, more and more functional components can be embedded on a single silicon die, thus enabling high degree pipelining operations such as those required for multimedia applications. in recent years, system-on-chip designs have migrated from fairly simple single processor and memory designs to relatively complicated systems with multiple processors, on-chip memories, standard peripherals, and other functional blocks. the communication between these ip blocks is becoming the dominant critical system path and performance bottleneck of system-on-chip designs. network-on-chip architectures, such as virtual channel (2004), black-bus (2004), pirate (2004), aethereal (2005), and vichar (2006) architectures, emerged as promising solutions for future system-on-chip communication architecture designs. however, these existing architectures all suffer from certain problems, including high area cost and communication latency and/or low network throughput. this paper presents a novel network-on-chip architecture, pipelining multi-channel central caching, to address the shortcomings of the existing architectures. by embedding a central cache into every switch of the network, blocked head packets can be removed from the input buffers and stored in the caches temporally, thus alleviating the effect of head-of-line and deadlock problems and achieving higher network throughput and lower communication latency without paying the price of higher area cost. experimental results showed that the proposed architecture exhibits both hardware simplicity and system performance improvement compared to the existing network-on-chip architectures.",
    "present_kp": [
      "system-on-chip",
      "network-on-chip",
      "on-chip communication architecture",
      "multi-channel central caching",
      "communication latency",
      "throughput"
    ],
    "absent_kp": []
  },
  {
    "title": "irreversible statistical mechanics from reversible motion: q2r automata.",
    "abstract": "q2r cellular automats are a nice pedagogical example for addressing a century-old question discussed at the 20th iupap statistical physics conference in paris last july: how is thermodynamic irreversibility, as seen in entropy increase, compatible with microscopic reversibility, as in newton's equations of motion? the boltzmann-zermelo argument says that the times for returning to the low-entropy initial state of a large system are far longer than the age of the universe. how can we test this assertion? molecular dynamics in its usual form is inadequate to address this question, since arithmetic rounding errors as well as discretization of space and time preclude return to the exact initial configuration, monte carlo simulations of ising models avoid such errors but require random numbers and are therefore not reversible in the usual sense. the q2r update rules of cellular automata for microcanonical ising models are reversible and the system returns exactly to the initial configuration after an exponentially long time. computer simulation of q2r cellular automata help us to understand some very old fundamental problems. this property will br reviewed, as well well as unexplained critical exponents obtained in large-scale simulations.",
    "present_kp": [
      "entropy"
    ],
    "absent_kp": [
      "teaching",
      "recurrence",
      "ergodicity",
      "dynamical scaling"
    ]
  },
  {
    "title": "simulation of complex pharmacokinetic models in microsoft excel.",
    "abstract": "with the arrival of powerful personal computers in the office numerical methods are accessible to everybody. simulation of complex processes therefore has become an indispensible tool in research and education. in this paper microsoft excel is used as a platform for a universal differential equation solver. the software is designed as an add-in aiming at a minimum of required user input to perform a given task. four examples are included to demonstrate both, the simplicity of use and the versatility of possible applications. while the layout of the program is admittedly geared to the needs of pharmacokineticists, it can be used in any field where sets of differential equations are involved. the software package is available upon request.",
    "present_kp": [
      "differential equations",
      "simulation"
    ],
    "absent_kp": [
      "pharmacokinetics",
      "vba"
    ]
  },
  {
    "title": "multi-location production and delivery with job selection.",
    "abstract": "we study a problem of coordinating multi-location production with limited delivery capability and the option to select jobs. each job has to be completed and delivered to a central warehouse, or customer, before its due-date to avoid incurring a job-dependent penalty. a single vehicle, capable of carrying an unlimited number of jobs, is available to transport processed jobs to the warehouse. the traveling times to and from the different production sites and the delivery costs are location-dependent. we assume equal processing time jobs and location dependent production speed. we develop several properties of an optimal scheduling and delivery policy, and show that the problem can be solved by reduction to a shortest-path problem in a corresponding network. the overall computational effort is o ( n 2 m 2 + 4 log n ) (where n and m are the number of jobs and the number of machines, respectively) by the application of the directed acyclic graph (dag) method. we test the algorithm numerically and show that the algorithm finds an optimal solution in reasonable time. for various special cases the computational effort reduces substantially.",
    "present_kp": [],
    "absent_kp": [
      "uniform machines",
      "multi-locations",
      "batch delivery",
      "weighted number of late jobs"
    ]
  },
  {
    "title": "group attention control for communication robots with wizard of oz approach.",
    "abstract": "this paper describes a group attention control (gac) system that enables a communication robot to simultaneously interact with many people. gac is based on controlling social situations and indicating explicit control to unify all purposes of attention. we implemented a semi-autonomous gac system into a communication robot that guides visitors to exhibits in a science museum and engages in free-play interactions with them. the gac system's effectiveness was demonstrated in a two-week experiment in the museum. we believe these results will allow us to develope interactive humanoid robots that can interact effectively with groups of people.",
    "present_kp": [
      "group attention control"
    ],
    "absent_kp": [
      "science museum robot",
      "field trial",
      "commutation robot",
      "human-robot interaction"
    ]
  },
  {
    "title": "optimization of setup times in the furniture industry.",
    "abstract": "the aim of this paper is to develop a scheduling policy oriented towards minimizing setup times in the made-to-order furniture industry. the task is treated as a dynamic job shop scheduling problem, with the exception that customers orders collected over aspecified period of time are combined into aproduction plan and released together. asimulation of a production flow based on technological routes of real subassemblies was performed. the proposed method of calculating a setup time eliminates the need to determine machine setup time matrices. among the tested priority rules the best performance was observed in the case of the hierarchical rule that combines similar setup, the earliest due date and the shortest processing time. this rule allowed the setup time per operation to be reduced by 58% compared to a combination of the earliest due date with the shortest setup and processing time rule and by over 70% compared to the single shortest processing time rule.",
    "present_kp": [
      "furniture"
    ],
    "absent_kp": [
      "dispatching rule",
      "dynamic scheduling",
      "sequence-dependent setup"
    ]
  },
  {
    "title": "a power-optimized widely-tunable 5-ghz monolithic vco in a digital soi cmos technology on high resistivity substrate.",
    "abstract": "this paper describes the design and technology optimization of power-efficient monolithic vcos with wide tuning range. four 5-ghz lc-tank vcos were fabricated in a 0.12-?m soi cmos technology that was not enhanced for rf applications. high and regular resistivity substrates were used, as were single-layer and multiple-layer copper inductors. using a new figure-of-merit (fomt) that encompasses power dissipation, phase noise and tuning range, our best vco has an fomt of -189 dbc/hz. the measured frequency tuning range is 22 % and the phase noise is -126 dbc/hz at 1 mhz offset for 4.5-ghz. oscillation was achieved at 5.4-ghz at a minimum power consumption of 500 ?w.",
    "present_kp": [
      "vco",
      "soi cmos",
      "high resistivity substrate"
    ],
    "absent_kp": [
      "phase noisefom",
      "low power",
      "rf design"
    ]
  },
  {
    "title": "pinning a stochastic neural network to the synchronous state.",
    "abstract": "in this paper, the asymptotic stability of the pinning synchronous solution of stochastic neural networks with and without time-delays is analyzed. the delays are time-varying, and the uncertainties are norm-bounded that enter into all the parameters of network and control. the aim of this paper is not only to establish easily verifiable conditions under which the pinning synchronous solution of stochastic neural network is globally asymptotically stable but also to give a feasible way to offset the limitation of network itself in order to reach synchronization. in addition, a specific neurobiological network is also introduced, and some numerical examples are provided to illustrate the applicability of the proposed criteria.",
    "present_kp": [
      "synchronization",
      "stochastic neural network"
    ],
    "absent_kp": [
      "pinning mechanism"
    ]
  },
  {
    "title": "local interaction and non-local coordination in agent social law diffusion.",
    "abstract": "there are always two kinds of forms in agent social law diffusion, which are local interaction and non-local coordination. this paper provides an integrated model to make a tradeoff between local diffusion and non-local diffusion. in the local interaction, each agent often interacts with a small set of social neighbors and imitates the locally most authoritative law; in the non-local coordination, agents may coordinate with others that are not in the neighboring places for some tasks, and will negotiate about their social laws according to the coordination strategy. with the presented model, the diffusions between local interaction and non-local coordination can be harmonized, and the diffusion impacts of different strategies in non-local coordination can also be addressed well.",
    "present_kp": [
      "social law",
      "local interaction"
    ],
    "absent_kp": [
      "multi-agent",
      "agent coordination"
    ]
  },
  {
    "title": "a framework for performance measurement in the e-business environment.",
    "abstract": "the advent of networked economy calls for new understanding of business, and it is evidenced by the visible trend of traditional businesses either migrating to e-business or expanding to embrace electronic commerce. constant change in the environment means continually evolving strategies, new products, new processes and new technologies to adopt. e-business metrics are needed to measure performance with the firm's strategic focus in mind, and they must go beyond the web metrics that are discussed in the usual electronic commerce context. the basic objective of this paper is to present a framework for developing performance measurement metrics in the e-business environment. the proposed framework, designed by incorporating the balanced scorecard methodology with existing taxonomies of e-business models and the theories behind them, is intended to enable firms to develop new metrics that are needed to implement e-business strategies and tactics.",
    "present_kp": [
      "e-business strategies",
      "performance measurement",
      "e-business metrics",
      "balanced scorecard"
    ],
    "absent_kp": [
      "taxonomy of business models",
      "nebic theory",
      "c-suite issues",
      "p-suite issues"
    ]
  },
  {
    "title": "a deteriorating inventory model with time-varying demand and shortage-dependent partial backlogging.",
    "abstract": "recently, chu et al. presented the necessary condition of the existence and uniqueness of the optimal solution of padmanabhan and vrat. however, they included neither the purchase cost nor the cost of lost sales into the total cost. in this paper, we complement the shortcoming of their model by adding not only the cost of lost sales but also the non-constant purchase cost, and then extend their model from a constant demand function to any log-concave demand function. we also provide a simple solution procedure to find the optimal replenishment schedule. further, we use a couple of numerical examples to illustrate the results and conclude with suggestions for future research.",
    "present_kp": [
      "inventory",
      "time-varying demand",
      "partial backlogging"
    ],
    "absent_kp": [
      "deteriorating items"
    ]
  },
  {
    "title": "decision making in stock trading: an application of promethee.",
    "abstract": "the key issue for decision making in stock trading is selection of the right stock at the right time. in order to select the superior stocks (alternatives) for investment, a finite number of alternatives have to be ranked considering several and sometimes conflicting criteria. therefore, we are faced with a special multicriteria decision-making problem. the purpose of this paper is to develop a decision-making model for selecting superior stocks in stock exchange and a model is provided in order to structure this problem. the proposed model is structured around two pillars: industry evaluation and company evaluation. the preference ranking organization method for enrichment evaluation (promethee) has been used for solving the problem. the model has been applied at tehran stock exchange (tse) as a real case and a survey from the experts in order to determine the effective criteria for industry evaluation and company evaluation has been conducted.",
    "present_kp": [
      "industry evaluation",
      "company evaluation",
      "promethee",
      "tehran stock exchange "
    ],
    "absent_kp": [
      "multicriteria decision making"
    ]
  },
  {
    "title": "fuzzy logic-based optimization for redundant manipulators.",
    "abstract": "redundant manipulators have more degrees of freedom (dof) than the dof of the task space. this implies that the number of joint position variables is greater than the number of variables specifying the task. the problem of solving the kinematic equations for the joint variables is underspecified unless additional equations/constraints are introduced to obtain a well-posed problem, i.e., the redundancy is resolved. the redundancy resolution can be based on the kinematic or the dynamic equations of the manipulator. in this paper, a dynamic level redundancy resolution is proposed. the manipulator dynamical model in the joint space is first transformed to a reduced-order model in the pseudovelocity space. the elements of the foregoing transformation matrix indirectly determine the contribution of each joint to the total motion. these elements are selected using two fuzzy logic-based methods so as to minimize the instantaneous manipulator power: 1) in the velocity method, a nullspace vector in the velocity relationship between the two spaces is determined by imposing a constraint on the continuity of the joint velocities at the time instant when the elements of the transformation matrix experience a discontinuity and 2) in the torque method, an alternative approach introduced to reduce the computational complexity, the changes in the transformation matrix are made continuous with respect to time by the appropriate choice of a nullspace vector in the joint torque expression. the applications of these two methods to resolve the redundancy are illustrated by simulations. the methods are discussed with regard to their computational efficiency and are compared with other redundancy resolution approaches.",
    "present_kp": [
      "manipulator",
      "redundancy"
    ],
    "absent_kp": [
      "dynamics",
      "fuzzy logic optimization"
    ]
  },
  {
    "title": "an online trained fuzzy neural network controller to improve stability of power systems.",
    "abstract": "the purpose of this paper is to improve the stability in a power system using a new intelligent controller. this controller is an online trained fuzzy neural network controller (otfnnc) in which adaptive learning rates derived by the lyapunov stability are employed to guarantee the convergence of the proposed controller. during the online control process, the identification of system is not necessary, because of learning ability of the proposed controller. one of the proposed controller features is robustness to different operating conditions and disturbances. moreover, the prony method is used to obtain the exponential damping of power system oscillations in this paper.the test power system is a two-area four-machine system power. the simulation results show that the oscillations are satisfactorily damped out by the otfnnc. the proposed approach is effective to mitigate power system oscillations and improve the stability. literature review show that no method is proposed to compute the damping of power system oscillation if adaptive and online controllers like fuzzy and neural network controller are utilized for damping power system oscillations. in this paper, the damping rate of power system oscillations is estimated by the prony method.",
    "present_kp": [
      "fuzzy neural network",
      "adaptive learning rates",
      "prony method"
    ],
    "absent_kp": [
      "power system stabilizer ",
      "intelligent control system"
    ]
  },
  {
    "title": "how to steer the it outsourcing provider development and validation of a reference framework of it outsourcing steering processes.",
    "abstract": "it executives entering into information technology (it) outsourcing arrangements seek various strategic, economic, and technological benefits. however, although several cases of it outsourcing are considered successful, cases of failure can also be observed. problems and challenges associated with it outsourcing often not only relate to the strategic decision whether or not to outsource, but to the operational level as well. especially organizations with little experience of implementing larger it outsourcing programs face problems with the steering of external outsourcing providers. in this paper, we propose a reference framework that structures the required processes for an effective steering of it outsourcing relationships. the research is based on the design science paradigm in information systems research. in a first step, we derive a framework from related literature and knowledge in this particular area. we then undertake extensive fieldwork, including expert interviews and field studies to evaluate our framework and to develop it further. the suggested framework proves to be a viable instrument to support the systematic analysis of current processes and the definition of suitable target processes for the steering of it outsourcing programs. this paper's primary contribution therefore lies in providing an applicable instrument for practitioners as well as in extending the existing body of knowledge on it outsourcing governance.",
    "present_kp": [
      "it outsourcing",
      "steering processes",
      "it outsourcing governance",
      "design science"
    ],
    "absent_kp": []
  },
  {
    "title": "thermal conductivity and heat transport properties of nitrogen-doped graphene.",
    "abstract": "emd simulation based on gk method was employed to study the tc of n-graphene. a drastic decline in tc of graphene observed at very low n-doped concentration. tc of n-graphene approaches to each other as nitrogen concentration increases. tc of n-graphene is much less sensitive to temperature compared with graphene.",
    "present_kp": [
      "nitrogen-doped graphene",
      "thermal conductivity"
    ],
    "absent_kp": [
      "equilibrium molecular dynamics simulation",
      "greenkubo method",
      "heat current autocorrelation function"
    ]
  },
  {
    "title": "a polynomial time optimal diode insertion/routing algorithm for fixing antenna problem.",
    "abstract": "antenna problem is a phenomenon of plasma induced gateoxide degradation. it directly affects manufacturability of vlsicircuits, especially in deep-submicron technology using high density plasma. diode insertion is a very effective way to solve thisproblem. ideally diodes are inserted directly under the wires thatviolate antenna rules. but in today's high-density vlsi layouts,there is simply not enough room for iounder-the-wirel. diode insertion for all wires. thus it is necessary to insert many diodesat legal ieoff-wirel- locations and extend the antenna-rule violating wires to connect to their respective diodes. previously onlysimple heuristic algorithms were available for this diode insertionand routing problem. in this paper, we show that the diode insertion and routing problem for an arbitrary given number of routinglayers can be optimally solved in polynomial time. our algorithmguarantees to find a feasible diode insertion and routing solutionwhenever one exists. moreover, we can guarantee to find a feasible solution to minimize a cost function of the form alphacdot l + betacdot nwhere cdot l is the total length of extension wires andcdot n is the total number of vias on the extension wires. experimental results showthat our algorithm is very efficient.",
    "present_kp": [
      "heuristic algorithm",
      "affect",
      "polynomial",
      "manufacturability",
      "layout",
      "routing",
      "algorithm",
      "paper",
      "routing algorithm",
      "effect",
      "cost",
      "rules"
    ],
    "absent_kp": [
      "efficiency",
      "experimentation",
      "technologies",
      "optimality",
      "functional",
      "extensibility",
      "timing",
      "connection"
    ]
  },
  {
    "title": "the impact of accounting for special methods in the measurement of object-oriented class cohesion on refactoring and fault prediction activities.",
    "abstract": "class cohesion is a key attribute that is used to assess the design quality of a class, and it refers to the extent to which the attributes and methods of the class are related. typically, classes contain special types of methods, such as constructors, destructors, and access methods. each of these special methods has its own characteristics, which can artificially affect the class cohesion measurement. several metrics have been proposed in the literature to indicate class cohesion during high- or low-level design phases. the impact of accounting for special methods in cohesion measurement has not been addressed for most of these metrics. this paper empirically explores the impact of including or excluding special methods on cohesion measurements that were performed using 20 existing class cohesion metrics. the empirical study applies the metrics that were considered to five open-source systems under four different scenarios, including (1) considering all special methods, (2) ignoring only constructors, (3) ignoring only access methods, and (4) ignoring all special methods. this study empirically explores the impact of including special methods in cohesion measurement for two applications of interest to software practitioners, including refactoring and predicting faulty classes. the results of the empirical studies show that the cohesion values for most of the metrics considered differ significantly across the four scenarios and that this difference significantly affects the refactoring decisions, but does not significantly affect the abilities of the metrics to predict faulty classes.",
    "present_kp": [
      "class cohesion",
      "cohesion metric",
      "special methods",
      "refactoring",
      "fault prediction"
    ],
    "absent_kp": [
      "object-oriented design",
      "class quality"
    ]
  },
  {
    "title": "the relevance of autoantigen source and cutoff definition in antichromatin (nucleosome) antibody immunoassays.",
    "abstract": "abstract: in the last few years, several reports have shown that chromatin (nucleosome) represents the main autoantigen-immunogen in systemic lupus erythematosus (sle) and that specific antibodies are an important marker of the disease. to verify the clinical sensitivity and specificity of antinucleosome autoantibodies (anuas), we evaluated three elisa immunoassay methods using different autoantigen preparations: quanta lite chromatin, medizym anti-nucleo, and nucleosome igg elisa. we compared the results with those obtained using two elisa assays for determining anti-native dna (anti-ndna) antibodies: axis-shield and elia dsdna. we tested sera from 321 patients: 101 with sle and 220 controls48 with infectious diseases; 73 with autoimmune rheumatic disease (20 with rheumatoid arthritis, 30 with systemic sclerosis, and 23 with primary sjgren's syndrome), and 99 healthy subjects. using the manufacturer-recommended cutoff, the sensitivity for the three kits was 69%, 78%, and 74%, and specificity was 100%, 94.6%, and 95.0%, respectively. using the cutoff corresponding to 95% specificity, the sensitivity of the methods for the anua assay was 86%, 77%, and 74%higher than obtained with the two elisa methods for anti-ndna (65% and 64%). this study demonstrates that (1) the commercial reagents employed in clinical laboratories for anua detection show good sensitivity and high specificity; (2) anuas are more sensitive than anti-ndna antibodies for diagnosing sle; and (3) different solid-phase antigen preparations and methods used to define cutoff levels may affect a test's clinical performance.",
    "present_kp": [
      "autoantigen source",
      "cutoff",
      "systemic lupus erythematosus "
    ],
    "absent_kp": [
      "antichromatin antibodies",
      "antinucleosome antibodies",
      "anti-native dna antibodies",
      "receiver operating characteristic  curves"
    ]
  },
  {
    "title": "alert correlation: severe attack prediction and controlling false alarm rate tradeoffs.",
    "abstract": "alert correlation plays an increasingly crucial role in nowadays computer security infrastructures. it is particularly needed for coping with the huge amounts of alerts which are daily triggered by intrusion detection systems (idss), fire-walls, etc. while the use of multiple idss, security tools and complementary approaches is fundamental and highly recommended in order to improve the overall detection rates, this however inevitably causes huge amounts of alerts most of which are redundant and false alarms making the manual analysis of these triggered alerts time-consuming and inefficient. this paper addresses three important issues related to predicting severe attacks (attacks with high dangerousness levels) by analyzing inoffensive and preparatory attacks. i) firstly, we address the issue of preprocessing alerts reported by the multiple detection tools in order to eliminate the redundant and irrelevant alerts and format them so that they can be analyzed by a severe attack prediction model. ii) then, we propose a novel prediction model based on a bayesian network multi-net allowing on one hand to better model the severe attacks and on the other hand handle the reliability of idss when predicting severe attacks. iii) finally, we provide a flexible and efficient approach especially designed to limit the false alarm rates by controlling the confidence of the prediction model. the main benefits of our approach is an integrated model guaranteeing very promising prediction/false alarm rate tradeoffs with minimum expert intervention. our experimental studies are carried out on a real and representative alert corpus generated by the de facto network-based ids snort, and show very interesting performances regarding the tradeoffs between the prediction rates and the corresponding false alarm ones.",
    "present_kp": [
      "alert correlation",
      "severe attack prediction"
    ],
    "absent_kp": [
      "idss' reliability",
      "bayesian multi-nets",
      "reasoning with uncertain evidence",
      "reject option"
    ]
  },
  {
    "title": "diagnosability of star graphs with missing edges.",
    "abstract": "in this paper, we study the system diagnosis on an n-dimensional star under the comparison model. following the concept of local diagnosability [3], the strong local diagnosability property [7] is discussed; this property describes the equivalence of the local diagnosability of a node and its degree. we prove that an n-dimensional star has this property, and it keeps this strong property even if there exist n - 3 missing edges in it.",
    "present_kp": [
      "star graph",
      "local diagnosability",
      "strong local diagnosability property"
    ],
    "absent_kp": [
      "comparison diagnosis model",
      "mm* diagnosis model",
      "extended star structure"
    ]
  },
  {
    "title": "a novel methodology for the design of application-specific instruction-set processors (asips) using a machine description language.",
    "abstract": "the development of application-specific instruction-set processors (asip) is currently the exclusive domain of the semiconductor houses and core vendors. this is due to the fact that building such an architecture is a difficult task that requires expertise in different domains: application software development tools, processor hardware implementation, and system integration and verification. this article presents a retargetable framework for asip design which is based on machine descriptions in the lisa language. from that, software development tools can be generated automatically including high-level language c compiler, assembler, linker, simulator, and debugger frontend. moreover, for architecture implementation, synthesizable hardware description language code can be derived, which can then be processed by standard synthesis tools. implementation results for a low-power asip for digital video broadcasting terrestrial acquisition and tracking algorithms designed with the presented methodology will be given. to show the quality of the generated software development tools, they are compared in speed and functionality with commercially available tools of state-of-the-art digital signal processor and muc architectures.",
    "present_kp": [],
    "absent_kp": [
      "architecture exploration",
      "machine description languages",
      "system-on-chip"
    ]
  },
  {
    "title": "on the greatest solutions to weakly linear systems of fuzzy relation inequalities and equations.",
    "abstract": "in this paper we study systems of fuzzy relation inequalities and equations of the form u circle v-i <= v-i circle u (i is an element of i), where u is an unknown and v-i (i is an element of i) are given fuzzy relations, the dual systems v-i circle u <= u circle v-i (i is an element of i), their conjunctions, the systems of the form u circle v-i = v-i circle (i is an element of i), and certain special types of these systems. we call them weakly linear systems. for each weakly linear system, with a complete residuated lattice as the underlying structure of truth values, we prove the existence of the greatest solution, and we provide an algorithm for computing the greatest solution, which works whenever the underlying complete residuated lattice is locally finite. otherwise, we determine some sufficient conditions under which the algorithm works. the algorithm is iterative, and each its single step can be viewed as solving of a particular linear system. weakly linear systems emerged from the fuzzy automata theory, but we show that they also have important applications in other fields, e.g. in the concurrency theory and social network analysis.",
    "present_kp": [
      "fuzzy relations",
      "fuzzy relation inequalities"
    ],
    "absent_kp": [
      "fuzzy relation equations",
      "residuals of fuzzy relations",
      "complete residuated lattices",
      "fuzzy quasi-orders",
      "fuzzy equivalences",
      "regular fuzzy relations",
      "post-fixed points"
    ]
  },
  {
    "title": "revisiting the algebra of petri net processes under the collective token philosophy.",
    "abstract": "revisiting the view of \"petri nets as monoids\" suggested by meseguer and montanari, we give a direct proof of the well-known result that the class of best/devillers processes, which represents the behavior of petri nets under the collective token semantics, has a sound and complete axiomatization in terms of symmetric monoidal categories. using membership equational logic for the axiomatization, we prove the result by an explicit construction of a natural isomorphism between suitable functors. our interest in the collective token semantics is motivated by earlier work on the use of rewriting logic as a uniform framework for different petri net classes, especially including high-level petri nets, where individuality of tokens can be already expressed at the system level.",
    "present_kp": [
      "best/devillers processes",
      "collective token philosophy",
      "membership equational logic",
      "rewriting logic"
    ],
    "absent_kp": [
      "place/transition nets"
    ]
  },
  {
    "title": "m-musics: an intelligent mobile music retrieval system.",
    "abstract": "accurate voice humming transcription and efficient indexing and retrieval schemes are essential to a large-scale humming-based audio retrieval system. although much research has been done to develop such schemes, their performance in terms of precision, recall, and f-measure, among all similarity metrics, are still unsatisfactory. in this paper, we propose a new voice query transcription scheme. it considers the following features: note onset detection using dynamic threshold methods, fundamental frequency (f0) acquisition of each frame, and frequency realignment using k-means. we use a popularity-adaptive indexing structure called frequently accessed index (fai) based on frequently queried tunes for indexing purposes. in addition, we propose a semi-supervised relevance feedback and query reformulation scheme based on a genetic algorithm to improve retrieval efficiency. in this paper, we extend our efforts to mobile multimedia environments and develop a mobile audio retrieval system. experiments show our system performs satisfactory in wireless mobile multimedia environments.",
    "present_kp": [
      "relevance feedback"
    ],
    "absent_kp": [
      "content-based audio retrieval",
      "mobile platform",
      "signal processing"
    ]
  },
  {
    "title": "a biomems chip with integrated micro electromagnet array towards bio-particles manipulation.",
    "abstract": "a novel biomems chip towards the manipulation of micro/nano magnetic particles is proposed. electromagnets array and microfluidic structure are fabricated on a compact chip, without any post-fabrication process. a universal optimization method for the design of a planar microcoil array. a new tubeless method to introduce fluid onto mems chip.",
    "present_kp": [
      "planar microcoil array",
      "magnetic particles",
      "manipulation"
    ],
    "absent_kp": [
      "bio-mems chip",
      "integrated electromagnets"
    ]
  },
  {
    "title": "spectral shape of doubly-generalized ldpc codes: efficient and exact evaluation.",
    "abstract": "this paper analyzes the asymptotic exponent of the weight spectrum for irregular doubly-generalized ldpc (d-gldpc) codes. in the process, an efficient numerical technique for its evaluation is presented, involving the solution of a 4 x 4 system of polynomial equations. the expression is consistent with previous results, including the case where the normalized weight or stopping set size tends to zero. the spectral shape is shown to admit a particularly simple form in the special case where all variable nodes are repetition codes of the same degree, a case which includes tanner codes; for this case it is also shown how certain symmetry properties of the local weight distribution at the cns induce a symmetry in the overall weight spectral shape function. finally, using these new results, weight and stopping set size spectral shapes are evaluated for some example generalized and doubly-generalized ldpc code ensembles.",
    "present_kp": [
      "doubly-generalized ldpc codes",
      "spectral shape",
      "weight distribution"
    ],
    "absent_kp": [
      "irregular code ensembles",
      "stopping set size distribution"
    ]
  },
  {
    "title": "ictcp: incast congestion control for tcp in data-center networks.",
    "abstract": "transport control protocol (tcp) incast congestion happens in high-bandwidth and low-latency networks when multiple synchronized servers send data to the same receiver in parallel. for many important data-center applications such as mapreduce and search, this many-to-one traffic pattern is common. hence tcp incast congestion may severely degrade their performances, e.g., by increasing response time. in this paper, we study tcp incast in detail by focusing on the relationships between tcp throughput, round-trip time (rtt), and receive window. unlike previous approaches, which mitigate the impact of tcp incast congestion by using a fine-grained timeout value, our idea is to design an incast congestion control for tcp (ictcp) scheme on the receiver side. in particular, our method adjusts the tcp receive window proactively before packet loss occurs. the implementation and experiments in our testbed demonstrate that we achieve almost zero timeouts and high goodput for tcp incast.",
    "present_kp": [
      "data-center networks",
      "incast congestion",
      "tcp"
    ],
    "absent_kp": []
  },
  {
    "title": "pacap inhibits oxidative stress-induced activation of map kinase-dependent apoptotic pathway in cultured cardiomyocytes.",
    "abstract": "the present article investigated the effect of pituitary adenylate cyclase-activating polypeptide (pacap) on oxidative stress-induced apoptosis in neonatal rat cardiomyocytes. our results show that pacap decreased the ratio of apoptotic cells following h2o2 treatment. pacap also diminished the activity of apoptosis signal-regulating kinase. these effects of pacap were counteracted by the pacap antagonist pacap6-38. in summary, our results show that pacap is able to attenuate oxidative stress-induced cardiomyocyte apoptosis and suggest that its cardioprotective effect is mediated through inhibition of the map kinase-dependent apoptotic pathway",
    "present_kp": [
      "pacap",
      "apoptosis",
      "cardiomyocyte",
      "apoptosis signal-regulating kinase"
    ],
    "absent_kp": []
  },
  {
    "title": "an adaptive routing algorithm for wk-recursive topologies.",
    "abstract": "this paper presents an easy and straightforward routing algorithm for wk-recursive topologies. the algorithm, based on adaptive routing, takes advantage of the geometric properties of such topologies. once a source node s and destination node d have been determined for a message communication, they characterize, at some level i, two virtual nodes hl-vn(s-d) and hl-vn(d-s) that respectively contain s but not d and d but not s. such virtual nodes characterize other n-d-2 (where n-d is the node degree for a fixed topology) virtual nodes hl-vn(i-sd) of the same level that contain neither s nor d. consequently, it is possible to locate n-d-2 triangles whose vertices are these virtual nodes with property to share the same path, called the self-routing path, directly connecting hl-vn(s-d) to hl-vn(d-s). when the self-routing path is unavailable to transmit a message from s to d because of deadlock, fault, and congestion conditions, the routing strategy can follow what we call the triangle rule to deliver it. the proposed communication scheme has the advantage that 1) it is the same for all three conditions; 2) each node of a wk-recursive network, to transmit messages, does not require any information about their presence or location. furthermore, this routing algorithm is able to tolerate up to [n-d(n-d-3)/2 + 1]n-d(l)-1/n-d-1 faulty links.",
    "present_kp": [
      "routing strategy"
    ],
    "absent_kp": [
      "interconnection network topology",
      "scalability",
      "deadlock freedom",
      "communication overhead",
      "fault tolerancy"
    ]
  },
  {
    "title": "linking business and requirements engineering: is solution planning a missing activity in software product companies.",
    "abstract": "a strong link between strategy and product development is important, since companies need to select requirements for forthcoming releases. however, in practice, connecting requirements engineering (re) and business planning is far from trivial. this paper describes the lessons learned from four software product companies that have recognized the need for more business-oriented long-term planning. the study was conducted using the action research approach. we identified five practices that seem to strengthen the link between business decisions and re. these are (1) explicating the planning levels and time horizons; (2) separating the planning of products business goals from r&d resource allocation; (3) planning open-endedly with a pre-defined rhythm; (4) emphasizing whole-product thinking; and (5) making solution planning visible. to support whole-product thinking and solution planning, we suggest that companies create solution concepts. the purpose of the solution concept is to provide a big picture of the solution and guide re activities.",
    "present_kp": [
      "solution planning",
      "solution concept"
    ],
    "absent_kp": [
      "market-driven requirements engineering",
      "roadmapping",
      "long-term product planning"
    ]
  },
  {
    "title": "polygon interpolation for serial cross sections.",
    "abstract": "in this paper, a new technique for contour interpolation between slices is presented. we assumed that contour interpolation is equivalent to the interpolation of a polygon that approximates the object shape. the location of each polygon vertex is characterized by a set of parameters. polygon interpolation can be performed on these parameters. these interpolated parameters are then used to reconstruct the vertices of the new polygon. finally, the contour is approximated from this polygon using a cubic spline interpolation. this new technique takes into account the shape, the translation, the size, and the orientation of the object's contours. a comparison with regular shape-based interpolation is made on several object contours. the preliminary results show that this new method yields a better contour and is computationally more efficient than shape-based interpolation. this technique can be applied to gray-level images too. the interpolation result of an mr image does not show artifact of intermediate substance commonly seen in a typical linear gray-level interpolation.",
    "present_kp": [
      "contour interpolation",
      "shape-based interpolation"
    ],
    "absent_kp": [
      "image interpolation",
      "polygon approximation"
    ]
  },
  {
    "title": "wkc-owa, a new neat-owa operator to aggregate information in democratic decision problems.",
    "abstract": "in all decision making processes, the use of aggregation operators is necessary. democratic decision problems cannot be considered a usual decision task due to the subjectivity of human' opinions and the complexity of social environments. to model these processes, the concept of work committee is used, where every citizen's opinion is represented in heterogeneous groups and aggregated to obtain a representative final opinion for the problem. in these environments, traditional operators don't represent the concept of discussion groups used in social models producing inadequate aggregations for these types of problems. the purpose of this paper is to present a new owa operator where the weights are a function of the aggregate values in order to model the work committee and aggregate the citizens' opinions in democratic decision problems.",
    "present_kp": [
      "owa operator",
      "neat-owa"
    ],
    "absent_kp": [
      "aggregation information",
      "group decision making",
      "participative democracy",
      "e-democracy"
    ]
  },
  {
    "title": "finding the maximum eigenvalue of essentially nonnegative symmetric tensors via sum of squares programming.",
    "abstract": "finding the maximum eigenvalue of a tensor is an important topic in tensor computation and multilinear algebra. recently, for a tensor with nonnegative entries (which we refer it as a nonnegative tensor), efficient numerical schemes have been proposed to calculate its maximum eigenvalue based on a perronfrobenius-type theorem. in this paper, we consider a new class of tensors called essentially nonnegative tensors, which extends the concept of nonnegative tensors, and examine the maximum eigenvalue of an essentially nonnegative tensor using the polynomial optimization techniques. we first establish that finding the maximum eigenvalue of an essentially nonnegative symmetric tensor is equivalent to solving a sum of squares of polynomials (sos) optimization problem, which, in its turn, can be equivalently rewritten as a semi-definite programming problem. then, using this sum of squares programming problem, we also provide upper and lower estimates for the maximum eigenvalue of general symmetric tensors. these upper and lower estimates can be calculated in terms of the entries of the tensor. numerical examples are also presented to illustrate the significance of the results.",
    "present_kp": [
      "symmetric tensors",
      "maximum eigenvalue",
      "sum of squares of polynomials",
      "semi-definite programming problem"
    ],
    "absent_kp": []
  },
  {
    "title": "ausgewhlte technologische entwicklungen in der hochspannungstechnik.",
    "abstract": "die wende in der energiepolitik stellt die hochspannungstechnik als schlsseltechnologie fr eine nachhaltige, ausfallsichere, umweltfreundlichere und effiziente energieversorgung vor neue herausforderungen. dieser artikel beleuchtet ausgewhlte technologische entwicklungen der hochspannungstechnik. besondere bedeutung kommt dabei der entwicklung in der isolierstofftechnik, z. b. neue, umweltfreundliche gase oder gasgemische, biologische oder synthetische isolierflssigkeiten sowie neuen systemen von feststoffisolierungen zu. einfluss auf diese entwicklungen nimmt auch die nanotechnologie im bereich der nanodielectrics bzw. der nanofluids. zu diesen entwicklungen kommen auch die neuen mglichkeiten in der messtechnik und der diagnostik, die wesentlich fr die qualitt der systeme sind, wie etwa die teilentladungsdiagnostik oder die messung der elektrischen leitfhigkeit der verschiedenen isolierstoffe.",
    "present_kp": [
      "teilentladungsdiagnostik",
      "nanotechnologie"
    ],
    "absent_kp": [
      "alternative isolierstoffe",
      "elektrostatische aufladungsneigung ",
      "alterungsverhalten",
      "alternative insulating materials",
      "electrostatic charging tendency ",
      "partial discharge  diagnostics",
      "ageing behaviour",
      "nanotechnology"
    ]
  },
  {
    "title": "a real world object modeling method for creating simulation environment of real-time systems.",
    "abstract": "most real-time embedded control software feature complex interactions with asynchronous inputs and environment objects, and a meaningful simulation of a real-time control software specification requires realistic simulation of its environment. two problems that need to be addressed in the simulation of a target software system and its environment: first, integration and simulation of the specifications of a target software system and its artificial environment are often performed too late in the lifecycle to provide any significant value. second, real world objects in the environment usually have spatial characteristics (form) such as shape, motion, etc. that must be specified for simulation, and there is no method to express these spatial characteristics at various levels of abstraction that are adequate for the required simulation fidelity. to address these problems, we have developed a method that supports incremental specification and simulation of both the target software system and its environmental objects. the method includes: (1) a specification method for behavior, function, and form integrated objects; (2) form specification primitives that abstract common spatial characteristics of real world objects, their typical spatial relations, and spatial interactions; and (3) a methodology that refines, verifies, and validates behavior, function, and form specification of both the real-time embedded control software and its environment in a systematic and incremental manner. the proposed specification, verification, and validation method has been applied to a robot control system example to demonstrate its effectiveness and usefulness.",
    "present_kp": [
      "real-time control software",
      "simulation",
      "verification",
      "validation"
    ],
    "absent_kp": [
      "requirement specification"
    ]
  },
  {
    "title": "subtle facial expression recognition using motion magnification.",
    "abstract": "this paper proposes a novel method for subtle facial expression recognition that uses motion magnification to transform subtle expressions into corresponding exaggerated ones. motion magnification consists of four steps: first, active appearance model (aam) fitting extracts 70 facial feature points in the face image sequence. second, the face image sequence is aligned using the three feature points (two eyes and nose tip). third, the motion vectors of 27 feature points are estimated using the feature point tracking method. finally, exaggerated facial expressions are obtained by magnifying the motion vectors of the 27 feature points. after motion magnification, the exaggerated facial expressions are recognized as follows: first, the shape and appearance features are obtained by projecting the exaggerated facial expression image to the aam shape and appearance model. second, support vector machines (svm) are used to classify shape and appearance features. experimental results show that proposed subtle facial recognition rate is 88.125% for the 80 facial expression images in the sfed2007 database.",
    "present_kp": [
      "subtle facial expression recognition",
      "motion magnification",
      "feature point tracking"
    ],
    "absent_kp": [
      "motion estimation",
      "active appearance models"
    ]
  },
  {
    "title": "cooperative scheduling mechanism for large-scale peer-to-peer computing systems.",
    "abstract": "over recent years, peer-to-peer (p2p) systems have become an important part of internet. millions of users have been attracted to their structures and services. p2p computing is a distributed computing paradigm that uses internet to connect thousands, or even millions, of users into a single large virtual computer based on the sharing of computational resources. one of the most critical aspects to the design of p2p computing systems is the development of scheduling techniques to manage the computational resources efficiently and in a scalable way. this paper proposes a cooperative scheduling mechanism with a two-level topology designed to work on large-scale distributed computing p2p systems. our main contribution is proposing three criteria that only use local information to schedule tasks thus providing scalability to the overall scheduling system. by setting up these three criteria, the system can be easily adapted to work efficiently with very different kinds of distributed applications. the extensive experimentation carried out justifies the importance of good scheduling in such heterogeneous systems, but also emphasizes the importance of having a scheduling algorithm capable of being adapted to the requirements of different kinds of application.",
    "present_kp": [
      "p2p computing"
    ],
    "absent_kp": [
      "super-peer",
      "overlays",
      "distributed and centralized scheduling"
    ]
  },
  {
    "title": "position control of shape memory alloy actuator based on the generalized prandtl-ishlinskii inverse model.",
    "abstract": "hysteresis and significant nonlinearities in the behavior of shape memory alloy (sma) actuators encumber effective utilization of these actuator. due to these effects, the position control of sma actuators has been a great challenge in recent years. literature review of the research conducted in this area shows that using the inverse of the phenomenological hysteresis models can compensate the hysteresis of these actuators effectively. but, inverting some of these models, such as preisach model, is numerically a complex task. however, the generalized prandtl-ishlinskii model is analytically invertible, and therefore can be implemented conveniently as a feedforward controller for compensating hysteresis nonlinearities effects in sma actuators. in this paper a feedforward-feedback controller is used to control the tip deflection of a large deflected flexible beam actuated by an sma actuator wire. the feedforward part of the control system is based on the generalized prandtl-ishlinskii inverse model while a conventional proportional-integral feedback controller is added to the feedforward controller to increase the accuracy together with eliminating the steady state error in position control process. experimental results show that the proposed controller performs well in terms of achieving small overshoot and undershoot for square wave tracking as well as small tracking errors for sinusoidal trajectory. it has also great capability for tracking hysteresis minor loops.",
    "present_kp": [
      "position control",
      "shape memory alloy actuator",
      "inverse model",
      "generalized prandtl-ishlinskii model"
    ],
    "absent_kp": []
  },
  {
    "title": "conjunction and disjunction operations for digital fuzzy hardware.",
    "abstract": "fuzzy systems have been explored in diverse application fields which require reaching fuzzy inferences at high computer rates. to accomplish this task, fuzzy hardware is the best choice. at inference engine, conjunction and disjunction operations play a very important role for decision making. common operations in existing fuzzy hardware are minimum, maximum, algebraic product and probabilistic sum. in order to extend the applicability of existing fuzzy hardware, it is necessary to consider a wider range of operations. it is even desirable to have configurable circuits which take advantage of hardware resources. this work presents the hardware implementation of configurable circuits for the realization of diverse fuzzy t-norm and t-conorm operations. resultant circuits are low hardware resource consumers which makes them efficient to be used as add-in modules for existing fuzzy hardware in fpga or asic. comparative results are presented showing the advantages of these circuits.",
    "present_kp": [
      "fuzzy system",
      "conjunction",
      "disjunction",
      "t-norm",
      "t-conorm"
    ],
    "absent_kp": [
      "digital hardware"
    ]
  },
  {
    "title": "the branching problem in generalized power solutions to differential equations.",
    "abstract": "generalized power asymptotic expansions of solutions to differential equations that depend on parameters are investigated. the changing nature of these expansions as the parameters of the model cross critical values is discussed. an algorithm to identify these critical values and generate the generalized power series for distinct families of solutions is presented, and as an application the singular behavior of a cosmological model with a nonlinear dissipative fluid is obtained. this algorithm has been implemented in the computer algebra system maple.",
    "present_kp": [
      "generalized power series"
    ],
    "absent_kp": [
      "nonlinear ordinary differential equations",
      "symbolic computation",
      "cosmological models"
    ]
  },
  {
    "title": "the role of evaluation-driven rejection in the successful exploration of a conceptual space of stories.",
    "abstract": "evaluation processes are a basic component of creativity. they guide not only the pure judgement about a new artefact but also the generation itself, as creators constantly evaluate their own work. this paper proposes a model for automatic story generation based on the evaluation of stories. a model of how quality in stories is evaluated is presented, and two possible implementations of the generation guided by this evaluation are shown: exhaustive space exploration and constrained exploration. a theoretical model and its implementation are explained and validation of the evaluation function through comparison with human criteria is described.",
    "present_kp": [
      "story generation",
      "evaluation of stories"
    ],
    "absent_kp": [
      "reader model",
      "conceptual space exploration"
    ]
  },
  {
    "title": "inelastic behavior modelling of concrete in low and high strain rate dynamics.",
    "abstract": "this work deals with response modeling of concrete for dynamic loading. as in statics one has to account for substantial difference of inelastic response in tension and compression, the anisotropy of the response induced by complex cracking patterns and the need of irreversible deformation due to frictional sliding or non-closing cracks. on the top of that, in dynamics, we also have to handle the hardening or softening phenomena which explain a particular hysteretic response for a given cyclic loading as well as the strain rate effects. the latter should further be addressed separately for high as opposed to low strain rates. the main goal of this work is to develop the concrete constitutive model capable of reproducing the salient features experimentally observed. we present one theoretical development for the constitutive model of concrete at low strain rates. the same kind of developments are then carried out for high strain dynamic behavior. both chosen models belong to the class of coupled plasticity damage models briefly presented.",
    "present_kp": [
      "concrete",
      "dynamics",
      "damage",
      "plasticity"
    ],
    "absent_kp": []
  },
  {
    "title": "a geometric construction of iterative functions of order three to solve nonlinear equations.",
    "abstract": "in this paper we consider a geometric construction of iteration functions of order three to develop cubically convergent iterative methods for solving nonlinear equations. this construction can be applied to any iteration function of order two to develop an iteration function of order three. some examples are given of deriving several third-order iteration methods, and several numerical results follow to illustrate the performance of the derived methods.",
    "present_kp": [
      "iterative methods",
      "iteration function",
      "nonlinear equations"
    ],
    "absent_kp": [
      "newtons method",
      "order of convergence",
      "root-finding"
    ]
  },
  {
    "title": "a template-based baseball video scene classification using efficient playfield segmentation.",
    "abstract": "in this paper, we present an effective and efficient framework for baseball video scene classification. the results of scene classification can be able to provide the ground for baseball video abstraction and high-level event extraction. in general, most conventional approaches are shot-based, which shot change detection and key-frame extraction are necessary prerequisite procedures. on the contrary, we propose a frame-based approach. in our scene classification framework, an efficient playfield segmentation technique is proposed, and then the reduced field maps are utilized as scene templates. because the shot change detection and the key-frame extraction are not required in proposed method, the new framework is very simple and efficient. the experimental results have demonstrated that the effectiveness of our proposed framework for baseball videos scene classification, and it can be easily extended the template-based approach to other kinds of sports videos.",
    "present_kp": [
      "scene classification",
      "baseball video",
      "key-frame",
      "playfield segmentation"
    ],
    "absent_kp": []
  },
  {
    "title": "a study on the sudden death and sudden birth of entanglement in an open system.",
    "abstract": "in this paper, we consider the entanglement dynamics of two cavities interacting with independent reservoirs. when the cavity entanglement suddenly disappeared, the reservoir entanglement suddenly and necessarily appears. we study the effect of purity of initial entangled state of two cavities on the entanglement evolution, and acquire that the purity of initial entangled state of two cavities can control the apparition time of the entanglement sudden death and the entanglement sudden birth. also, we find that the conditions on the apparition of the entanglement sudden death and the entanglement sudden birth can be generalized when the initial entangled state of cavities is not pure, which is a complement to the result in the paper for the pure case.",
    "present_kp": [
      "entanglement dynamics",
      "entanglement sudden death ",
      "purity"
    ],
    "absent_kp": []
  },
  {
    "title": "rapid development of scalable scientific software using a process oriented approach.",
    "abstract": "scientific applications are often not written with multiprocessing, cluster computing or grid computing in mind. this paper suggests using python and pycsp to structure scientific software through communicating sequential processes. three scientific applications are used to demonstrate the features of pycsp and how networks of processes may easily be mapped into a visual representation for better understanding of the process workflow. we show that for many sequential solutions, the difficulty in implementing a parallel application is removed. the use of standard multi-threading mechanisms such as locks, conditions and monitors is completely hidden in the pycsp library. we show the three scientific applications: knn, stochastic minimum search and mcstas to scale well on multi-processing, cluster computing and grid computing platforms using pycsp.",
    "present_kp": [
      "communicating sequential processes",
      "grid computing",
      "python"
    ],
    "absent_kp": [
      "many-core"
    ]
  },
  {
    "title": "supervised automatic evaluation for summarization with voted regression model.",
    "abstract": "the high quality evaluation of generated summaries is needed if we are to improve automatic summarization systems. although human evaluation provides better results than automatic evaluation methods, its cost is huge and it is difficult to reproduce the results. therefore, we need an automatic method that simulates human evaluation if we are to improve our summarization system efficiently. although automatic evaluation methods have been proposed, they are unreliable when used for individual summaries. to solve this problem, we propose a supervised automatic evaluation method based on a new regression model called the voted regression model (vrm). vrm has two characteristics: (1) model selection based on corrected aic to avoid multicollinearity, (2) voting by the selected models to alleviate the problem of overfitting. evaluation results obtained for tsc3 and duc2004 show that our method achieved error reductions of about 1751% compared with conventional automatic evaluation methods. moreover, our method obtained the highest correlation coefficients in several different experiments.",
    "present_kp": [
      "automatic summarization",
      "automatic evaluation",
      "regression model"
    ],
    "absent_kp": [
      "text summarization challenge",
      "document understanding conference"
    ]
  },
  {
    "title": "an analytical model for evaluating utilization of tcp reno.",
    "abstract": "this paper presents an analytical model for tcp reno. for this model an algorithm is derived to calculate the utilization and packet drop rate. the accuracy of the model is verified by comparing the calculated results versus simulation results. these results show that the tcp reno is superior to another version(tahoe) by having higher percentage of utilization and lower percentage of packet dropping rate.",
    "present_kp": [
      "utilization",
      "tcp reno"
    ],
    "absent_kp": [
      "fast recovery",
      "congestion control",
      "markovian model"
    ]
  },
  {
    "title": "fast payment schemes for truthful mechanisms with verification.",
    "abstract": "in this paper we study optimization problems with verifiable one-parameter selfish agents introduced by auletta et al.. our goal is to allocate load among the agents, provided that the secret data of each agent is a single positive real number: the cost they incur per unit load. in such a setting the payment is given after the load completion, therefore if a positive load is assigned to an agent, we are able to verify if the agent declared to be faster than she actually is. we design truthful mechanisms when the agents' type sets are upper-bounded by a finite value. we provide a truthful mechanism that is c . (1 + epsilon)-approximate if the underlying algorithm is c-approximate and weakly-monotone. moreover, if type sets are also discrete, we provide a truthful mechanism preserving the approximation ratio of its algorithmic part. our results improve the existing ones which provide truthful mechanisms dealing only with finite type sets and do not preserve the approximation ratio of the underlying algorithm. finally, we give applications for our payment schemes. firstly, we give a full characterization of the q parallel to c-max problem by using our techniques. even if our payment schemes need upper-bounded type sets, every instance of q parallel to c-max can be \"mapped\" into an instance with upper-bounded type sets preserving the approximation ratio. in conclusion, we turn our attention to binary demand games. in particular, we show that the minimum radius spanning tree admits an exact truthful mechanism with verification achieving time (and space) complexity of the fastest centralized algorithm for it. this contrasts with a recent truthful mechanism for the same problem which pays a linear factor with respect to the complexity of the fastest centralized algorithm. such a result is extended to several binary demand games studied in literature.",
    "present_kp": [
      "mechanisms with verification"
    ],
    "absent_kp": [
      "algorithmic mechanism design",
      "one-parameter agents"
    ]
  },
  {
    "title": "a novel approach for decreasing cvt transients in distance protection using artificial neural network.",
    "abstract": "this paper presents the design of a novel method for improvement of the operation of distance relays during capacitive voltage transformer transients using artificial neural network. the proposed module uses voltage and current signals to learn the hidden relationship existing in the input patterns. simulation studies are preformed and the influence of changing system parameters, such as fault resistance and source impedance is studied. details of the design procedure and the results of performance studies with the proposed relay are given in the paper. performance studies results show that the proposed algorithm decreases the effects of cvt transients and is fast and accurate.",
    "present_kp": [
      "artificial neural network"
    ],
    "absent_kp": [
      "cvt transients and distance protection"
    ]
  },
  {
    "title": "reversal effect of low-intensity ultrasound on adriamycin-resistant human hepatoma cells in vitro and in vivo.",
    "abstract": "the aim of this study was to determine whether the ultrasound, with a dosage that did not lead to acute and delayed inhibition, could potentiate the cytotoxicity of adriamycin to human hepatoma resistant cell line hepg2 both in vitro and in vivo. in order to determine whether low-intensity ultrasound could reverse resistance in adriamycin-resistant human hepatoma cell line hepg2/adm in vitro, cells were subjected to a variable level of ultrasound. the results showed that survival rates were decreased in groups in which ultrasound and adriamycin were exerted. the same effect of low-intensity ultrasound was also observed in the xenograft tumor experiment. furthermore, transmission electron microscope revealed the cavitation effects play the determining role in accelerating transmembrane transportation, suggesting that low-intensity ultrasound altered the cell membrane thus resulting in change in adriamycin uptake into hepg2/adm cells. further investigation of the underlying mechanisms showed that the reversal effect of low-intensity ultrasound on adriamycin-resistant human hepatoma cells was attributed to the downregulation of the multidrug resistant genes, mdr1 and mrp1, in both mrna and protein expression. in addition, downregulation of mdr1 and mrp1 was detected in hepg2/adm xenograft tumor treated with adriamycin and ultrasound. these observations suggest the use of ultrasound could increase cytotoxicity attributable to adriamycin in chemoresistant human hepatoma cancer cells. ultrasound is a promising therapeutic modality for refractory hepatoma patients.",
    "present_kp": [
      "ultrasound",
      "adriamycin-resistant",
      "mdr",
      "hepatoma",
      "hepg2"
    ],
    "absent_kp": []
  },
  {
    "title": "robust pca methods for complete and missing data.",
    "abstract": "in this paper, we consider and introduce methods for robust principal component analysis (pca), including also cases where there are missing values in the data. pca is a widely applied standard statistical method for data preprocessing, compression, and analysis. it is based on the second-order statistics of the data and is optimal for gaussian data, but it is often applied to data sets having unknown or other types of probability distributions. pca can be derived from minimization of the mean-square representation error or maximization of variances under orthonormality constraints. however, these quadratic criteria are sensitive to outliers in the data and long-tailed distributions, which may considerably degrade the results given by pca. we introduce robust methods for estimation of both the pca eigenvectors directly or the pca subspace spanned by them. experimental results show that our methods provide often better results than standard pca when outliers are present in the data. furthermore, we extend our methods to incomplete data with missing values. the problems arising in such cases have several features typical for nonlinear models.",
    "present_kp": [
      "principal component analysis",
      "missing values"
    ],
    "absent_kp": [
      "robustness",
      "gradient algorithms",
      "imputation method",
      "and nearest neighbor method"
    ]
  },
  {
    "title": "new approximation bounds for lpt scheduling.",
    "abstract": "we provide new bounds for the worst case approximation ratio of the classic longest processing time (lpt) heuristic for related machine scheduling (q parallel to c(max)). for different machine speeds, lpt was first considered by gonzalez et al. (siam j. comput. 6(1): 155-166, 1977). the best previously known bounds originate from more than 20 years back: dobson (siam j. comput. 13(4): 705-716, 1984), and independently friesen (siam j. comput. 16(3): 554-560, 1987) showed that the worst case ratio of lpt is in the interval (1.512, 1.583), and in (1.52, 1.67), respectively. we tighten the upper bound to 1+ root 3/3 approximate to 1.5773, and the lower bound to 1.54. although this improvement might seem minor, we consider the structure of potential lower bound instances more systematically than former works. we present a scheme for a job-exchanging process, which, repeated any number of times, gradually increases the lower bound. for the new upper bound, this systematic method together with a new idea of introducing fractional jobs, facilitated a proof that is surprisingly simple, relative to the result. we present the upper-bound proof in parameterized terms, which leaves room for further improvements.",
    "present_kp": [
      "related machine scheduling",
      "lpt"
    ],
    "absent_kp": [
      "approximation algorithms"
    ]
  },
  {
    "title": "genetically identical irises have texture similarity that is not detected by iris biometrics.",
    "abstract": "as the standard iris biometric algorithm sees them, the left and right irises of the same person are as different as irises of unrelated people. similarly, in terms of iris biometric matching, the eyes of identical twins are as different as irises of unrelated people. the left and right eyes of an individual or the eyes of identical twins are examples of genetically identical irises. in experiments with human observers viewing pairs of iris images acquired using an iris biometric system, we have found that there is recognizable similarity in the left and right irises of an individual and in the irises of identical twins. this result suggests that iris texture analysis different from that performed in the standard iris biometric algorithm may be able to answer questions that iris biometrics cannot answer.",
    "present_kp": [
      "genetically identical irises",
      "texture analysis"
    ],
    "absent_kp": [
      "ocular biometrics",
      "iris recognition",
      "periocular recognition",
      "monozygotic twins"
    ]
  },
  {
    "title": "learning horn definitions: theory and an application to planning.",
    "abstract": "a horn definition is a set of horn clauses with the same predicate in all head literals. in this paper, we consider learning non-recursive, first-order horn definitions from entailment. we show that this class is exactly learnable from equivalence and membership queries. it follows then that this class is pac learnable using examples and membership queries. finally, we apply our results to learning control knowledge for efficient planning in the form of goal-decomposition rules.",
    "present_kp": [
      "horn definition",
      "horn clause",
      "planning",
      "control knowledge",
      "queries"
    ],
    "absent_kp": [
      "horn program",
      "horn sentence",
      "learning from entailment",
      "pac-learning"
    ]
  },
  {
    "title": "advancing projections of phytoplankton responses to climate change through ensemble modelling.",
    "abstract": "we examined the performance of multiple lake and reservoir ecosystem models. the mean of all models was the best predictor of phytoplankton in a temperate lake relative to any individual model. future climate warming scenarios predict increases in total phytoplankton biomass. climate warming will facilitate higher yields of cyanobacteria relative to nutrient levels.",
    "present_kp": [
      "future climate",
      "cyanobacteria"
    ],
    "absent_kp": [
      "water resources",
      "ecosystem modelling"
    ]
  },
  {
    "title": "variational data assimilation using targetted random walks.",
    "abstract": "the variational approach to data assimilation is a widely used methodology for both online prediction and for reanalysis. in either of these scenarios, it can be important to assess uncertainties in the assimilated state. ideally, it is desirable to have complete information concerning the bayesian posterior distribution for unknown state given data. we show that complete computational probing of this posterior distribution is now within the reach in the offline situation. we introduce a markov chainmonte carlo (mcmc) method which enables us to directly sample from the bayesian posterior distribution on the unknown functions of interest given observations. since we are aware that these methods are currently too computationally expensive to consider using in an online filtering scenario, we frame this in the context of offline reanalysis. using a simple random walk-type mcmc method, we are able to characterize the posterior distribution using only evaluations of the forward model of the problem, and of the model and data mismatch. no adjoint model is required for the method we use; however, more sophisticated mcmc methods are available which exploit derivative information. for simplicity of exposition, we consider the problem of assimilating data, either eulerian or lagrangian, into a low reynolds number flow in a two-dimensional periodic geometry. we will show that in many cases it is possible to recover the initial condition and model error (which we describe as unknown forcing to the model) from data, and that with increasing amounts of informative data, the uncertainty in our estimations reduces.",
    "present_kp": [],
    "absent_kp": [
      "uncertainty quantification",
      "probabilistic methods",
      "stochastic problems",
      "transport",
      "incompressible flow",
      "partial differential equations"
    ]
  },
  {
    "title": "exploring organizational expansion modes and their associated communication system requirements: consolidation and complementation.",
    "abstract": "the expectation of firms following aggressive growth strategies is that firms expanding geographically will acquire extensive economic advantages unavailable to more conservative competitors. growth-minded firms also believe that by integrating info-communication technology (ict) within their regional infrastructures, distribution channels, and marketing approaches, they can achieve lower costs or enhanced differentiation within a broad scope of operations. empirical evidence shows that many organizational expansions have been implemented utilizing two major expansion modes, consolidation and complementation. this research examines the modal properties of these approaches in terms of their configurational characteristics, economic rationales and managerial requirements. viewed from a top-down perspective the modal properties further suggest the organization's management support system requirements. this study also identifies and explores the corresponding communication system requirements, especially for key dispersion-related communication configurations and systems. the desired practical effect of this research would be an increased awareness of and an incentive for commercial and industrial enterprises to include communication system requirements in their adopted expansion mode deliberations.",
    "present_kp": [
      "consolidation",
      "complementation",
      "communication system requirements"
    ],
    "absent_kp": [
      "organizational growth and expansion"
    ]
  },
  {
    "title": "generalized spatio-chromatic diffusion.",
    "abstract": "in this paper, a framework for diffusion of color images is presented. the method is based on the theory of thermodynamics of irreversible transformations which provides a suitable basis for designing correlations between the different color channels. more precisely, we derive an equation for color evolution which comprises a purely spatial diffusive term and a nonlinear term that depends on the interactions among color channels over space. we apply the proposed equation to images represented in several color spaces, such as rgb, cielab, opponent colors, and ihs.",
    "present_kp": [
      "color images"
    ],
    "absent_kp": [
      "scale-space",
      "vector-valued diffusion"
    ]
  },
  {
    "title": "multistage group-blind adaptive multiuser detector for reverse link in direct-sequence cdma systems.",
    "abstract": "we develop a novel multistage group-blind multiuser detection technique for reverse link in multi-cell direct sequence (ds) code-division multiple access (cdma) systems. this approach fully utilizes all known users' information, performs interference cancellation (ic) to suppress the interferers within the cell, and exploits blind adaptive decision-directed (dd) mmse detector to suppress the inter-cell interference. this process can be made iterative and in the multistage process, the tentative decision bits from dd-mmse detectors can be fed back to the ic part to make interference regeneration more accurate. a simplified rls (srls) algorithm is derived particularly for dd-mmse detectors and steady-state sir performance of the srls algorithm is analyzed. it is shown that the proposed detector with srls algorithm is sufficient to perform accurate multiuser detection and has significant performance gain over that of the pure blind detectors and the non-blind detectors for the given scenario. moreover, compared with the conventional rls algorithm, the srls algorithm has lower computational complexity and the convergence speed is faster. we also develop two group-blind channel estimation methods to estimate the effective signature waveforms in multipath fading channel. it is seen that the group-blind channel estimation methods using updated received signal correlation matrix can effectively combat the mismatch problem faced by pure blind detectors.",
    "present_kp": [
      "code-division multiple access",
      "multiuser detection",
      "blind detector"
    ],
    "absent_kp": [
      "direct sequence spread spectrum",
      "interference suppression"
    ]
  },
  {
    "title": "emergent rough set data analysis.",
    "abstract": "purpose - the rough set concept is a new mathematical approach to imprecision, vagueness and uncertainty. this paper introduces the emergent computational paradigm and discusses its applicability and potential in rough set theory. design/methodology/approach - a conceptual discussion and approach are taken. findings - for accepting a system is displaying an emergent behavior, the system should be constructed by describing local elementary interactions between components in different ways of describing global behavior and properties of the running system over a period of time. the proposals of an emergent computation structure for implementing basic rough sets theory operators are also given in this paper. originality/value - the results will have an important impact on the development of new methods for knowledge discovery in databases, in particular for development of algorithmic methods for pattern extraction from data.",
    "present_kp": [
      "set theory",
      "data analysis"
    ],
    "absent_kp": [
      "cybernetics"
    ]
  },
  {
    "title": "enhancement of dft-calculations at petascale: nuclear magnetic resonance, hybrid density functional theory and car-parrinello calculations.",
    "abstract": "one of the most promising techniques used for studying the electronic properties of materials is based on density functional. theory (off) approach and its extensions. dft has been widely applied in traditional solid state physics problems where periodicity and symmetry play a crucial role in reducing the computational workload. with growing compute power capability and the development of improved oft methods, the range of potential applications is now including other scientific areas such as chemistry and biology. however, cross disciplinary combinations of traditional solid-state physics, chemistry and biology drastically improve the system complexity while reducing the degree of periodicity and symmetry. large simulation cells containing of hundreds or even thousands of atoms are needed to model these kind of physical systems. the treatment of those systems still remains a computational challenge even with modern supercomputers. in this paper we describe our work to improve the scalability of quantum espresso for treating very large cells and huge numbers of electrons. to this end we have introduced an extra level of parallelism, over electronic bands, in three kernels for solving computationally expensive problems: the sternheimer equation solver (nuclear magnetic resonance, package qe-gipaw), the fock operator builder (electronic ground-state, package pwscf) and most of the car-parrinello routines (car-parrinello dynamics, package cp). final benchmarks show our success in computing the nuclear magnetic response (nmr) chemical shift of a large biological assembly, the electronic structure of defected amorphous silica with hybrid exchange-correlation functionals and the equilibrium atomic structure of height porphyrins anchored to a carbon nanotube, on many thousands of cpu cores.",
    "present_kp": [
      "car-parrinello",
      "nmr"
    ],
    "absent_kp": [
      "parallelization",
      "k-points",
      "plane waves dft",
      "exact exchange"
    ]
  },
  {
    "title": "characteristics of an envelope model for laser-plasma accelerator simulation.",
    "abstract": "simulation of laser-plasma accelerator (lpa) experiments is computationally intensive due to the disparate length scales involved. current experiments extend hundreds of laser wavelengths transversely and many thousands in the propagation direction, making explicit pic simulations enormously expensive and requiring massively parallel execution in 3d. simulating the next generation of lpa experiments is expected to increase the computational requirements yet further, by a factor of 1000. we can substantially improve the performance of lpa simulations by modeling the envelope evolution of the laser field rather than the field itself. this allows for much coarser grids, since we need only resolve the plasma wavelength and not the laser wavelength, and therefore larger timesteps can be used. thus an envelope model can result in savings of several orders of magnitude in computational resources. by propagating the laser envelope in a galilean frame moving at the speed of light, dispersive errors can be avoided and simulations over long distances become possible. the primary limitation to this envelope model is when the laser pulse develops large frequency shifts, and thus the slowly-varying envelope assumption is no longer valid. here we describe the model and its implementation, and show rigorous benchmarks for the algorithm, establishing second-order convergence and correct laser group velocity. we also demonstrate simulations of lpa phenomena such as self-focusing and meter-scale acceleration stages using the model.",
    "present_kp": [
      "plasma accelerator",
      "pic",
      "envelope model"
    ],
    "absent_kp": [
      "laser-plasma acceleration",
      "laser wakefield acceleration"
    ]
  },
  {
    "title": "multiple asset replacement analysis under variable utilization and stochastic demand.",
    "abstract": "the economic life of an asset is dependent on a variety of factors, including deterioration and obsolescence. while obsolescence is generally a result of changes external to the asset, such as technological change, deterioration is generally a result of how the asset is utilized over its lifetime. if multiple assets are available to meet demand and the assets must not continually operate at maximum capacity, then a decision-maker may have some control over asset utilization patterns by allocating workload. these utilization patterns directly impact operating costs and salvage values and thus have a strong influence on the optimal replacement time of the assets. in this paper, we examine asset replacement decisions, based on age and cumulative utilization, under variable periodic utilization with multiple, parallel assets under various cost and demand assumptions. we provide an efficient optimal solution procedure through the use of stochastic dynamic programming, illustrate a threshold optimal policy under common cost assumptions and provide a method to easily examine solutions for the two-asset case. extensions to the n-asset case are also discussed.",
    "present_kp": [
      "replacement ",
      "asset utilization",
      "dynamic programming"
    ],
    "absent_kp": []
  },
  {
    "title": "flexible-routing anonymous networks using optimal length of ciphertext.",
    "abstract": "we present an efficient hybrid mix scheme that provides both routing flexibility and the optimal length of ciphertext. although it is rather easy to embed routing information in the ciphertext, and a scheme that provides the optimal length of ciphertext is already known, it is not a trivial task to achieve both properties all at the same time. a critical obstacle for providing the optimal length of ciphertext is the session-key encapsulation header in a ciphertext that carries the encrypted session-key to each router, which linearly increases according to the number of intermediate routers. we solve this problem by improving the previously reported hybrid mix scheme such that the resulting scheme benefits from routing flexibility with a constant length of such headers. our basic scheme is only secure against honest, but curious intermediate routers. therefore, we further address the robustness issue to prevent malicious behavior by incorporating and improving an existing efficient approach based on the message authentication code.",
    "present_kp": [
      "hybrid mix"
    ],
    "absent_kp": [
      "mix-net",
      "onion routing",
      "anonymity"
    ]
  },
  {
    "title": "toward cognitive support for owl justifications.",
    "abstract": "justifications are the dominant form of explanation for entailments of owl ontologies, with popular owl ontology editors, such as protg 4, providing justification-based explanation facilities. a justification is a minimal subset of an ontology which is sufficient for an entailment to hold; they correspond to the premises of a proof. unlike proofs, however, justifications do not articulate how their axioms support the entailment. we frequently observe that ontology developers find certain justifications difficult to work with; and while in some cases the sources of difficulty are obvious (such as a large number of axioms), we do not have a good general understanding of what makes justifications easy or difficult for ontology users. in this paper, we present an approach to determining the cognitive complexity of justifications for entailments of owl ontologies. we describe an exploratory study which forms the basis for a cognitive complexity model that predicts the complexity of owl justifications, and present the results of validating that model via experiments involving owl users. this is concluded by an investigation into strategies owl users apply to support them in understanding justifications. our contributions include an evaluation of the cognitive complexity model, new insights into the complexity of justifications for entailments of owl ontologies, a significant corpus with novel analyses of justifications suitable for experimentation, and an experimental protocol suitable for model validation and refinement.",
    "present_kp": [
      "explanation"
    ],
    "absent_kp": [
      "semantic web",
      "description logics",
      "web ontology language",
      "ontology debugging"
    ]
  },
  {
    "title": "shared cache architectures for decision support systems.",
    "abstract": "in this paper we evaluate two shared-cache architectures for small-scale multiprocessors. we vary shared cache sizes from 8mb to 1gb, under various block sizes, cache organizations and sizes, and strategies for io transactions. we use 12 bus trace samples obtained during the execution of a 100gb tpc-h on an eight-way multiprocessor. to deal with the cold-start misses at the beginning of each sample, we identify the sure misses which are known to be misses in the full trace. the difference between the total number of misses and the number of sure misses is the zone of uncertainty, which may be hits or misses in the full trace. it turns out that the zone of uncertainty is small enough in most cases that useful conclusions can be drawn. our conclusions are that a single-cluster configuration with a shared cacheeven a very small onecan be very effective for tpc-h. we also show that the coherence traffic between shared caches in a multiple cluster system is very high in the context of tpc-h.",
    "present_kp": [
      "tpc-h"
    ],
    "absent_kp": [
      "cache memory",
      "trace-driven simulation",
      "cold-start bias",
      "io strategy"
    ]
  },
  {
    "title": "first-order system least squares and the energetic variational approach for two-phase flow.",
    "abstract": "this paper develops a first-order system least-squares (fosls) formulation for equations of two-phase flow. the main goal is to show that this discretization, along with numerical techniques such as nested iteration, algebraic multigrid, and adaptive local refinement, can be used to solve these types of complex fluid flow problems. in addition, from an energetic variational approach, it can be shown that an important quantity to preserve in a given simulation is the energy law. we discuss the energy law and inherent structure for two-phase flow using the allencahn interface model and indicate how it is related to other complex fluid models, such as magnetohydrodynamics. finally, we show that, using the fosls framework, one can still satisfy the appropriate energy law globally while using well-known numerical techniques.",
    "present_kp": [
      "energetic variational approach",
      "algebraic multigrid",
      "first-order system least squares",
      "nested iteration"
    ],
    "absent_kp": [
      "multiphase flow"
    ]
  },
  {
    "title": "task pool teams: a hybrid programming environment for irregular algorithms on smp clusters.",
    "abstract": "clusters of symmetric multiprocessors (smps) are popular platforms for parallel programming since they provide large computational power for a reasonable price. for irregular application programs with dynamically changing computation and data access behavior, a flexible programming model is needed to achieve efficiency. in this paper we propose task pool teams as, a hybrid parallel programming environment to realize irregular algorithms on clusters of smps. task pool teams combine task pools on single cluster nodes by an explicit message passing layer. they offer load balance together with multi-threaded, asynchronous communication. appropriate communication protocols and task pool implementations are provided and accessible by an easy-to-use application programmer interface. as application examples we present a branch and bound algorithm and the hierarchical radiosity algorithm.",
    "present_kp": [
      "irregular algorithms",
      "hybrid programming",
      "smp clusters"
    ],
    "absent_kp": []
  },
  {
    "title": "new bounds on the average information rate of secret-sharing schemes for graph-based weighted threshold access structures.",
    "abstract": "a secret-sharing scheme is a protocol by which a dealer distributes shares of a secret key among a set of n participants in such a way that only qualified subsets of participants can reconstruct the secret key from the shares they received, while unqualified subsets have no information about the secret key. the collection of all qualified subsets is called the access structure of this scheme. the information rate (resp. average information rate) of a secret-sharing scheme is the ratio between the size of the secret key and the maximum size (resp. average size) of the shares. in a weighted threshold scheme, each participant has his or her own weight. a subset is qualified if and only if the sum of the weights of participants in the subset is not less than the given threshold. morillo et al. considered the schemes for weighted threshold access structure that can be represented by graphs called k-weighted graphs. they characterized this kind of access structures and derived a result on the information rate. in this paper, we deal with the average information rate of the secret-sharing schemes for these structures. two sophisticated constructions are presented, each of which has its own advantages and both of them perform very well when n/k is large.",
    "present_kp": [
      "secret-sharing scheme",
      "access structure",
      "weighted threshold access structure"
    ],
    "absent_kp": [
      "optimal information rate",
      "optimal average information rate",
      "complete multipartite covering"
    ]
  },
  {
    "title": "implicitly-multithreaded processors.",
    "abstract": "this paper proposes the implicitly-multithreaded (imt) architecture to execute compiler-specified speculative threads on to a modified simultaneous multithreading pipeline. imt reduces hardware complexity by relying on the compiler to select suitable thread spawning points and orchestrate inter-thread register communication. to enhance imt's effectiveness, this paper proposes three novel microarchitectural mechanisms: (1) resource- and dependence-based fetch policy to fetch and execute suitable instructions, (2) context multiplexing to improve utilization and map as many threads to a single context as allowed by availability of resources, and (3) early threadinvocation to hide thread start-up overhead by overlapping one thread's invocation with other threads' execution.we use spec2k benchmarks and cycle-accurate simulation to show that an microarchitecture-optimized imt improves performance on average by 24% and at best by 69% over an aggressive superscalar. we also compare imt to two prior proposals, tme and dmt, for speculative threading on an smt using hardware-extracted threads. our best imt design outperforms a comparable tme and dmt on average by 26% and 38% respectively.",
    "present_kp": [
      "communication",
      "point",
      "policy",
      "use",
      "simulation",
      "benchmark",
      "design",
      "performance",
      "pipeline",
      "context",
      "simultaneous multithreading",
      "paper",
      "microarchitecture",
      "select",
      "thread",
      "processor",
      "availability",
      "architecture",
      "instruction",
      "multiplexing",
      "complexity",
      "hardware",
      "multithreading",
      "effect",
      "resource"
    ],
    "absent_kp": [
      "dependencies",
      "speculating",
      "compilation"
    ]
  },
  {
    "title": "a better beta for the h measure of classification performance.",
    "abstract": "we propose a modified standard distribution for the h measure in the case of unbalanced datasets. we emphasize the importance of the h measure as a coherent alternative to the auc. we introduce severity ratios to facilitate problem-specific application of the h measure.",
    "present_kp": [
      "auc",
      "h measure"
    ],
    "absent_kp": [
      "supervised classification",
      "classifier performance",
      "roc curve"
    ]
  },
  {
    "title": "indoor inter-robot distance measurement in collaborative systems.",
    "abstract": "this paper focuses on the problem of autonomous distance calculation between multiple mobile robots in collaborative systems. we propose and discuss two distinct methods, specifically developed under important design and functional constraints, such as the speed of operation, accuracy, energy and cost efficiency. moreover, the methods are designed to be applied to indoor robotic systems and are independent of fixed landmarks. the measurement results, performed on the core-tx case study, show that the proposed solutions meet the design requirements previously specified.",
    "present_kp": [
      "collaborative system",
      "distance measurement",
      "mobile robots"
    ],
    "absent_kp": [
      "indoor communication",
      "sonar"
    ]
  },
  {
    "title": "testing whether a digraph contains h-free k-induced subgraphs.",
    "abstract": "a subgraph induced by k vertices is called a k-induced subgraph. we prove that determining if a digraph g contains h-free k-induced subgraphs is omega(n-2)-evasive. then we construct an is an element of-tester to test this property. (an e-tester fora property pi is guaranteed to distinguish, with probability at least 2/3, between the case of g satisfying pi and the case of g being is an element of-far from satisfying pi.)the query complexity of the e-tester is independent of the size of the input digraph. an (e,)is an element of-tester fora property pi is an e-tester for pi that is furthermore guaranteed to accept with probability at least 2/3 any input that is delta-close to satisfying pi. this paper presents an (is an element of, delta)-tester for whether a digraph contains h-free k-induced subgraphs.",
    "present_kp": [],
    "absent_kp": [
      "directed graphs",
      "property testing",
      "randomized algorithm"
    ]
  },
  {
    "title": "brain fingerprinting: a comprehensive tutorial review of detection of concealed information with event-related brain potentials.",
    "abstract": "brain fingerprinting (bf) detects concealed information stored in the brain by measuring brainwaves. a specific eeg event-related potential, a p300-mermer, is elicited by stimuli that are significant in the present context. bf detects p300-mermer responses to words/pictures relevant to a crime scene, terrorist training, bomb-making knowledge, etc. bf detects information by measuring cognitive information processing. bf does not detect lies, stress, or emotion. bf computes a determination of information present or information absent and a statistical confidence for each individual determination. laboratory and field tests at the fbi, cia, us navy and elsewhere have resulted in 0% errors: no false positives and no false negatives. 100% of determinations made were correct. 3% of results have been indeterminate. bf has been applied in criminal cases and ruled admissible in court. scientific standards for bf tests are discussed. meeting the bf scientific standards is necessary for accuracy and validity. alternative techniques that failed to meet the bf scientific standards produced low accuracy and susceptibility to countermeasures. bf is highly resistant to countermeasures. no one has beaten a bf test with countermeasures, despite a $100,000 reward for doing so. principles of applying bf in the laboratory and the field are discussed.",
    "present_kp": [
      "brain fingerprinting",
      "p300-mermer",
      "p300",
      "event-related potential",
      "detection of concealed information"
    ],
    "absent_kp": []
  },
  {
    "title": "unsupervised image segmentation evaluation and refinement using a multi-scale approach.",
    "abstract": "in this study, a multi-scale approach is used to improve the segmentation of a high spatial resolution (30cm) color infrared image of a residential area. first, a series of 25image segmentations are performed in definiens professional 5 using different scale parameters. the optimal image segmentation is identified using an unsupervised evaluation method of segmentation quality that takes into account global intra-segment and inter-segment heterogeneity measures (weighted variance and morans i, respectively). once the optimal segmentation is determined, under-segmented and over-segmented regions in this segmentation are identified using local heterogeneity measures (variance and local morans i). the under- and over-segmented regions are refined by (1) further segmenting under-segmented regions at finer scales, and (2) merging over-segmented regions with spectrally similar neighbors. this process leads to the creation of several segmentations consisting of segments generated at three different segmentation scales. comparison of single- and multi-scale segmentations shows that identifying and refining under- and over-segmented regions using local statistics can improve global segmentation results.",
    "present_kp": [
      "multi-scale segmentation",
      "image segmentation evaluation"
    ],
    "absent_kp": [
      "object-based image analysis",
      "under-segmentation",
      "over-segmentation"
    ]
  },
  {
    "title": "thermal annealing effect on electrical properties of metal nitride gate electrodes with hafnium oxide gate dielectrics in nano-metric mos devices.",
    "abstract": "electrical properties of hafnium oxide (hfo2) gate dielectric with various metal nitride gate electrodes, i.e., tantalum nitride (tan), molybdenum nitride (mon), and tungsten nitride (wn), were studied over a range of hfo2 thicknesses, e.g., 2.510nm, and post-metal annealing (pma) temperatures, e.g., 600c to 800c. the work function of the nitride gate electrode was dependent on the material and the post-metal annealing (pma) temperature. the scanning transmission electron microscopy technique is used to observe the effect of pma on the interfacial gate dielectric thickness. after high-temperature annealing, the metal nitride gates were suitable for nmos. at the same pma temperature, the oxide-trapped charges increased and the interface state densities decreased with the increase of the hfo2 thickness for tan and wn gate electrodes. however, for mon gate electrode the interface state density is almost independent of film thickness. therefore, dielectric properties of the hfo2 high-k film depend not only on the metal nitride gate electrode material but also the post-metal annealing condition as well as the film thickness. during constant voltage stress of the mos capacitors, an increase in the time-dependent gate leakage current is also observed.",
    "present_kp": [
      "hfo2",
      "metal nitride gates"
    ],
    "absent_kp": [
      "nano-electronic device applications",
      "high dielectrics materials"
    ]
  },
  {
    "title": "the prime factor non-binary discrete fourier transform and use of crystal_router as a general purpose communication routine.",
    "abstract": "we have implemented one of the fast fourier transform algorithms, the prime factor algorithm (pfa), on the hypercube. on sequential computers, the pfa and other discrete fourier transforms (dft) such as the winograd algorithm (wfa) are known to be very efficient. however, both algorithms require full data shuffling and are thus challenging to any distributed memory parallel computers. we use a concurrent communication algorithm, called the crystal_router for communicating shuffled data. we will show that the speed gained in reduced arithmetic compared to binary fft is sufficient to overcome the extra communication requirement up to a certain number of processors. beyond this point the standard cooley-tukey fft algorithm has the best performance. we comment briefly on the application of the dft to signal processing in synthetic aperture radar (sar).",
    "present_kp": [
      "processor",
      "communication",
      "point",
      "use",
      "factor",
      "performance",
      "data",
      "general",
      "algorithm",
      "hypercube",
      "distributed",
      "signal processing"
    ],
    "absent_kp": [
      "requirements",
      "applications",
      "efficiency",
      "computation",
      "concurrency",
      "standardization",
      "memorialized",
      "parallel computation",
      "transformation"
    ]
  },
  {
    "title": "investigating and exploiting the bias of the weighted hypervolume to articulate user preferences.",
    "abstract": "optimizing the hypervolume indicator within evolutionary multiobjective optimizers has become popular in the last years. recently, the indicator has been generalized to the weighted case to incorporate various user preferences into hypervolume-based search algorithms. there are two main open questions in this context: (i) how does the specified weight influence the distribution of a fixed number of points that maximize the weighted hypervolume indicator? (ii) how can the user articulate her preferences easily without specifying a certain weight distribution function? in this paper, we tackle both questions. first, we theoretically investigate optimal distributions of ? points that maximize the weighted hypervolume indicator. second, based on the obtained theoretical results, we propose a new approach to articulate user preferences within biobjective hypervolume-based optimization in terms of specifying a desired density of points on a predefined (imaginary) pareto front. within this approach, a new exact algorithm based on dynamic programming is proposed which selects the set of ? points that maximizes the (weighted) hypervolume indicator. experiments on various test functions show the usefulness of this new preference articulation approach and the agreement between theory and practice.",
    "present_kp": [
      "hypervolume indicator",
      "preference articulation"
    ],
    "absent_kp": []
  },
  {
    "title": "exploring overlapping clusters using dynamic re-scaling and sampling.",
    "abstract": "until recently, the aim of most text-mining work has been to understand major topics and clusters. minor topics and clusters have been relatively neglected even though they may represent important information on rare events. we present a novel method for exploring overlapping clusters of heterogeneous sizes, which is based on vector space modeling, covariance matrix analysis, random sampling, and dynamic re-weighting of document vectors in massive databases. our system addresses a combination of difficult issues in database analysis, such as synonymy and polysemy, identification of minor clusters, accommodation of cluster overlap, automatic labeling of clusters based on their document contents, and the user-controlled trade-off between speed of computation and quality of results. we conducted implementation studies with new articles from the reuters and la times trec data sets and artificially generated data with a known cluster structure to demonstrate the effectiveness of our system.",
    "present_kp": [
      "covariance matrix analysis",
      "random sampling",
      "vector space modeling"
    ],
    "absent_kp": [
      "clustering",
      "text mining",
      "dimensional reduction",
      "latent semantic indexing ",
      "principal component analysis "
    ]
  },
  {
    "title": "median-pump wavelength assignment scheme for optical networks with parametric wavelength conversion.",
    "abstract": "this paper proposes a wavelength assignment scheme for optical networks with parametric wavelength conversion to minimize the number of wavelength converters when the number of wavelengths is given. wavelength conversion is needed to reduce the number of wavelengths required in the network, since the number of wavelengths is limited. a parametric wavelength converter (pwc) can be used to reduce the number of wavelength converters because of its multiple wavelengths conversion function. pwc uses the pump wavelength to define original and converted wavelengths. the setting of the pump wavelength affects the converted wavelengths. thus, the number of transmission wavelengths depends on the position of the pump wavelength. the proposed scheme is a heuristic scheme that considers the position of pump wavelength. an output wavelength that sets the pump wavelength near to the middle of the transmission wavelength band is selected. the proposed scheme designs each pwc to maximize the number of wavelength conversions supported. numerical results with our examined networks indicate up to 37% reduction in the total number of converters required, compared to a scheme that the position of pump wavelength is not considered. moreover, the reduction is 42% reduction compared to an optical network with single channel wavelength converters.",
    "present_kp": [
      "wavelength assignment",
      "wavelength conversion"
    ],
    "absent_kp": [
      "optical packet switching"
    ]
  },
  {
    "title": "computable values can be classical.",
    "abstract": "in programming languages of universal power, the computational integers must be distinguished from the classical integers because of the divergent integer. even the equational theory corresponding to evaluation of integer expressions is distinct from the theory of classical integers, and classical reasoning about computational integers yields inconsistencies. we show that there exist programming languages, actually extensions of the polymorphic lambda calculus, that have tremendous computing power and yet whose computational integers, or any other algebraically specified abstract data type, coincide with their classical counterpart. in particular, the equational theory of the programming language is a conservative extension of the theory of the underlying base types as given by algebraic data type specifications.",
    "present_kp": [
      "reasoning",
      "abstract data type",
      "values",
      "language",
      "computation",
      "types",
      "express",
      "polymorphic",
      "data",
      "theory",
      "lambda calculus",
      "power",
      "evaluation",
      "programming language"
    ],
    "absent_kp": [
      "extensibility"
    ]
  },
  {
    "title": "finite volume methods for unidirectional dispersive wave models.",
    "abstract": "we extend the framework of the finite volume method to dispersive unidirectional water wave propagation in one space dimension. in particular, we consider a kdvbbm-type equation. explicit and implicitexplicit rungekutta-type methods are used for time discretizations. the fully discrete schemes are validated by direct comparisons to analytic solutions. invariants' conservation properties are also studied. main applications include important nonlinear phenomena such as dispersive shock wave formation, solitary waves, and their various interactions.",
    "present_kp": [
      "finite volume method",
      "solitary waves"
    ],
    "absent_kp": [
      "nonlinear dispersive waves",
      "unidirectional propagation",
      "water waves"
    ]
  },
  {
    "title": "information technology and the transformation of industries: three research perspectives.",
    "abstract": "it is often claimed that information technology has the potential to transform entire industries. however, we find that very little is research has been conducted at the industry level. moreover, the small amount of research that has been conducted on it and industries has been based largely on just one perspective of industries. given the scale and potential impact of the changes that are happening at an industry level, we believe a concerted effort is needed to study this phenomenon. we propose a research agenda for studying it and industries. we suggest three research perspectives for studying it and industries: an economic perspective, an institutional perspective, and a socio-cultural perspective. just as is research that addresses these aspects at the organizational level has grown in recent years and contributed to our understanding of is, so in this paper we argue that a similar broadening, as well as more studies, are needed at the industry level of analysis. we provide an example from the real estate industry to illustrate the usefulness of the three research perspectives.",
    "present_kp": [
      "information technology",
      "economic perspective",
      "institutional perspective",
      "real estate industry"
    ],
    "absent_kp": [
      "industry transformation",
      "social/cultural perspective"
    ]
  },
  {
    "title": "information-theoretic comparisons of cellular multiple-access systems with bandwidth-dependent fading considerations.",
    "abstract": "the coding performances of generic ds-cdma, fdma and hybrid f/tdma systems with equivalent operating conditions are analysed and compared based on two information-theoretic performance measures, namely, channel capacity and probability of codeword decoding error. statistical signal models for the different systems are developed, which, among other practical considerations, incorporate the effects of multicell operation, raised-cosine bandlimiting and multipath fading statistics which vary with the signalling bandwidth used. the results obtained provide insights into some fundamental mechanisms which contribute to the relative spectral efficiency among different multiple-access systems.",
    "present_kp": [
      "multipath fading",
      "spectral efficiency"
    ],
    "absent_kp": [
      "cellular mobile radio",
      "multiple access",
      "information theory"
    ]
  },
  {
    "title": "multilevel monte carlo method for parabolic stochastic partial differential equations.",
    "abstract": "we analyze the convergence and complexity of multilevel monte carlo discretizations of a class of abstract stochastic, parabolic equations driven by square integrable martingales. we show under low regularity assumptions on the solution that the judicious combination of low order galerkin discretizations in space and an eulermaruyama discretization in time yields mean square convergence of order one in space and of order1/2 in time to the expected value of the mild solution. the complexity of the multilevel estimator is shown to scale log-linearly with respect to the corresponding work to generate a single path of the solution on the finest mesh, resp. of the corresponding deterministic parabolic problem on the finest mesh.",
    "present_kp": [
      "multilevel monte carlo",
      "stochastic partial differential equations",
      ""
    ],
    "absent_kp": [
      "stochastic finite element methods",
      "stochastic parabolic equation",
      "multilevel approximations"
    ]
  },
  {
    "title": "multi-spectral image analysis and classification of melanoma using fuzzy membership based partitions.",
    "abstract": "the sensitivity and specificity of melanoma diagnosis can be improved by adding the lesion depth and structure information obtained from the multi-spectral, trans-illumination images to the surface characteristic information obtained from the epi-illumination images. wavelet transform based bi-modal channel energy features obtained from the images are used in the analysis. methods using both crisp and fuzzy membership based partitioning of the feature space are evaluated. for this purpose, the adwat classification method that uses crisp partitioning is extended to handle multi-spectral image data. also, multi-dimensional fuzzy membership functions with gaussian and bell profiles are proposed for classification. results show that the fuzzy membership functions with bell profile are more effective than the extended adwat method in discriminating melanoma from dysplastic nevus.",
    "present_kp": [
      "melanoma",
      "dysplastic nevus",
      "fuzzy membership function"
    ],
    "absent_kp": [
      "nevoscope",
      "tree-structured wavelet transform"
    ]
  },
  {
    "title": "the use of explicit filters in large eddy simulation.",
    "abstract": "explicit filtering is considered as a means of controlling the numerical errors that result when finite-difference methods are used in large eddy simulation (les). the notion that the finite-difference expressions themselves act as an effective filter is shown to be false for three-dimensional simulations performed on nonuniform meshes. for consistency, the nonlinear terms in the navier-stokes equations should be filtered explicitly at each time step in order to insure that the spectral content of the solution remains fixed at the desired filter level. the explicit filtering operation allows a separation between the filter size and the mesh spacing and can be used to control the impact of the numerical errors. numerical tests of the explicit filtering approach in turbulent channel flow are used to investigate the effectiveness of the explicit filtering approach and to assess its associated cost. explicit filtering is shown to improve the computed results, but this improvement comes at a rather high computational cost. the explicitly-filtered approach is also compared with straightforward mesh refinement as an alternative means of improving the computed results. mesh refinement is also seen to increase the accuracy of the simulation but some traces of numerical error appear to persist in the solution.",
    "present_kp": [
      "large eddy simulation",
      "filtering"
    ],
    "absent_kp": [
      "subgrid-scale modelling",
      "turbulence",
      "numerical simulation"
    ]
  },
  {
    "title": "criminalising hacking tools.",
    "abstract": "making the sale, possession and distribution of the tools of hacking a criminal offence has obvious attractions. but many such tools are dual use and new laws run the risk of significantly inhibiting the activities of investigators, incident responders, penetration testers and academics. recent uk attempts at framing such a law are discussed in order to show the broader problems of policy and wording.",
    "present_kp": [
      "hacking tools",
      "uk"
    ],
    "absent_kp": [
      "legislation",
      "cybercrime treaty",
      "system administration",
      "computer misuse act"
    ]
  },
  {
    "title": "counter machines.",
    "abstract": "in this paper we study the computational power of counters. two different notions of a counter occur in the literature. one, which we call a two-way counter, is common in automata theory and is provided with the following operations: increment and decrement by 1 and test for zero. another notion was used in schematology and logics of programs. the corresponding set of operations comes from the simplest algebraic structure of positive integers with increment by 1, reset to 0 and comparison of two counter values. there is no decrementation, therefore we call it a one-way counter. it is known that two two-way counters are enough to simulate a turing machine. we present the same result for the one-way model: two one-way counters can simulate each turing machine. we also discuss some consequences of this result for the hierarchy of program schemes. o 1999 published by elsevier science b.v.",
    "present_kp": [
      "counter machine",
      "program scheme"
    ],
    "absent_kp": [
      "theory of computation",
      "program equivalence"
    ]
  },
  {
    "title": "optimal channel rate allocation for multimedia communication over fading wireless channels.",
    "abstract": "we consider the coding specifics of wireless broadcast terminals in hybrid multimedia communication systems, those consisting of a set of wireless and wired clients. in particular, we propose an information theoretically motivated framework for optimal multi-rate communication of multimedia contents, generically coded for a packet switched network, over fading wireless channels. the results in this paper however, extent themselves to any other wireless digital broadcast system as well. under idealized conditions, for both fast and slow fading scenarios, the optimal rate allocations can be found by solving a convex optimization problem. we then devise a systematic way to find a near optimal solution for non-ideal, practical scenarios. finally, we demonstrate the validity of our approach through simulations of an image streaming system that uses rate compatible convolutional codes to transmit multiple-description coded packets at the wireless access point.",
    "present_kp": [
      "wireless"
    ],
    "absent_kp": [
      "multi rate",
      "source-channel coding"
    ]
  },
  {
    "title": "asymptotic analysis of pollution filtration through thin random fissures between two porous media.",
    "abstract": "we describe the asymptotic behaviour of a filtration problem from a contaminated porous medium to a non-contaminated porous medium through thin vertical fissures of fixed height h > 0, of random thinness of order epsilon and which are epsilon-periodically distributed. we compute the limit velocity of the flow and the limit flux of pollutant at the interfaces between the two porous media and the intermediate one.",
    "present_kp": [
      "porous medium",
      "fissures",
      "asymptotic analysis"
    ],
    "absent_kp": [
      "pollution filtration problem",
      "darcy's law",
      "gamma-convergence"
    ]
  },
  {
    "title": "a topologically based non-minimum distance routing algorithm.",
    "abstract": "in this paper, a unique topologically based non-minimum distance routing algorithm is presented. it offers both maximal completion rates and minimal via usage. the algorithm is based on a topological transformation of a significant subset of the pc routing problem: a broad channel encompassing a full row of dips and all routes within it. this transformation eliminates the need to consider detailed path shapes during routing. the routing problem is transformed into a permutation of nets. it is coupled with a backtrack search to produce a powerful routing algorithm. the search is conducted in a planar environment.",
    "present_kp": [
      "distance",
      "routing",
      "shape",
      "algorithm",
      "paper",
      "routing algorithm",
      "permutation",
      "search",
      "minimal"
    ],
    "absent_kp": [
      "environments",
      "completeness"
    ]
  },
  {
    "title": "multicriteria fuzzy classification procedure procftn: methodology and medical application.",
    "abstract": "in this paper, we introduce a new classification procedure for assigning objects to predefined classes, named procftn. this procedure is based on a fuzzy scoring function for choosing a subset of prototypes, which represent the closest resemblance with an object to be assigned. it then applies the majority-voting rule to assign an object to a class. we also present a medical application of this procedure as an aid to assist the diagnosis of central nervous system tumours. the results are compared with those obtained by other classification methods, reported on the same data set, including decision tree, production rules, neural network, k nearest neighbor, multilayer perceptron and logistic regression. our results are very encouraging and show that the multicriteria decision analysis approach can be successfully used to help medical diagnosis.",
    "present_kp": [
      "classification",
      "scoring function",
      "medical diagnosis"
    ],
    "absent_kp": [
      "multicriteria decision aid",
      "fuzzy sets",
      "fuzzy binary relations",
      "astrocytic tumour"
    ]
  },
  {
    "title": "unitary torus model for conical mirror based catadioptric system.",
    "abstract": "we propose a new model for conical mirror based catadioptric systems. definition of the unitary torus to model this non-central system. definition of the conical fundamental matrix. motion computation across two views from the conical fundamental matrix. the proposal is illustrated with simulations and real experiments.",
    "present_kp": [],
    "absent_kp": [
      "imaging geometry",
      "omnidirectional camera",
      "non-central catadioptric system",
      "epipolar geometry"
    ]
  },
  {
    "title": "binary partition tree as an efficient representation for image processing, segmentation, and information retrieval.",
    "abstract": "this paper discusses the interest of binary partition trees as a region-oriented image representation, binary partition trees concentrate in a compact and structured representation a set of meaningful regions that can be extracted from an image, they offer a multiscale representation of the image and define a translation invariant 2-connectivity rule among regions, as shown in this paper, this representation can be used for a large number of processing peals such as filtering, segmentation, information retrieval and visual browsing. furthermore, the processing of the tree representation leads to very efficient algorithms, finally, for some applications, it may be interesting to compute the binary partition tree once and to store it for subsequent use for various applications, in this context, the last section of the paper will show that the amount of bits necessary to encode a binary partition tree remains moderate.",
    "present_kp": [
      "browsing",
      "information retrieval",
      "partition tree",
      "segmentation"
    ],
    "absent_kp": [
      "connected operators",
      "mathematical morphology",
      "nonlinear filtering",
      "object recognition",
      "pruning strategy",
      "region adjacency graphs"
    ]
  },
  {
    "title": "educational software to design shafts and analyze them by fem.",
    "abstract": "in this article, an educational computer application to dimension shafts is presented, safety factors according to several classic theories can be obtained by indicating shaft dimensions and particular discontinuities, as grooves or holes. specifying the desired security factor, necessary dimensions can be calculated. one of the advantages of the educational software developed is that text files created by it can be directly imported to the fem software ansys so as to analyze stress distribution of the shaft.",
    "present_kp": [
      "educational software"
    ],
    "absent_kp": [
      "shaft design",
      "finite elements"
    ]
  },
  {
    "title": "specifying and animating facial signals for discourse in embodied conversational agents.",
    "abstract": "people highlight the intended interpretation of their utterances within a larger discourse by a diverse set of non-verbal signals. these signals represent a key challenge for animated conversational agents because they are pervasive, variable, and need to be coordinated judiciously in an effective contribution to conversation. in this paper, we describe a freely available cross-platform real-time facial animation system, ruth, that animates such high-level signals in synchrony with speech and lip movements. ruth adopts an open, layered architecture in which fine-grained features of the animation can be derived by rule from inferred linguistic structure, allowing us to use ruth, in conjunction with annotation of observed discourse, to investigate the meaningful high-level elements of conversational facial movement for american english speakers.",
    "present_kp": [
      "facial animation",
      "embodied conversational agents"
    ],
    "absent_kp": []
  },
  {
    "title": "a 1.5 approximation algorithm for embedding hyperedges in a cycle.",
    "abstract": "the problem of minimum congestion hypergraph embedding in a cycle (mchec) is to embed the hyperedges of a hypergraph as adjacent paths around a cycle, such that the maximum congestion over any physical link in the cycle is minimized. the problem is np-complete in general, but solvable in polynomial time when all hyperedges contain exactly two vertices. in this paper, we first formulate the problem as an integer linear program (ilp). then, a solution with approximation bound of 1: 5(opt + 1) is presented by using a clockwise (2/3)-rounding algorithm, where opt denotes the optimal value of maximum congestion. to our knowledge, this is the best approximation bound known for the mchec problem.",
    "present_kp": [
      "approximation algorithm",
      "hypergraph",
      "np-complete"
    ],
    "absent_kp": [
      "integer linear programming",
      "lp-relaxation"
    ]
  },
  {
    "title": "particle filter based information-theoretic active sensing.",
    "abstract": "this work addresses the task of active sensing, or information-seeking control of mobile sensor platforms. formulation of a control objective in terms of information gain allows mobile sensors to be both autonomous and easily reconfigurable to include a variety of sensor and target models. tracking a moving target using a camera mounted on a fixed-wing unmanned aircraft is considered, but the control formulation is not specific to this choice of sensor or estimation task. a control formulation is developed which minimizes the entropy of an estimate distribution over a receding horizon subject to stochastic non-linear models for both the target motion and sensors. previous similar work has been restricted to either a stationary target, a horizon of length one, or gaussian estimates. the prediction of conditional entropy is shown to be inherently complex, and a computationally efficient sequential monte carlo method is developed. the entropy prediction depends on this monte carlo method as well as a novel approach for entropy calculation in the context of particle filtering. these methods are verified through simulation and post-processing of experimental flight data.",
    "present_kp": [
      "particle filter",
      "active sensing",
      "unmanned aircraft"
    ],
    "absent_kp": [
      "receding horizon control"
    ]
  },
  {
    "title": "analysis of pre-computed partition top method for range top-k queries in olap data cubes.",
    "abstract": "in decision support systems, having knowledge on the top k values is more informative and crucial than the maximum value. unfortunately, the naive method involves high computational cost and the existing methods for range-max query are inefficient if applied directly. in this paper, we propose a pre-computed partition top method (ppt) to partition the data cube and pre-store a number of top values for improving query performance. the main focus of this study is to find the optimum values for two parameters, i.e., the partition factor ( b ) and the number of pre-stored values ( r ), through analytical approach. a cost function based on poisson distribution is used for the analysis. the analytical results obtained are verified against simulation results. it is shown that the ppt method outperforms other alternative methods significantly when proper b and r are used.",
    "present_kp": [
      "analysis",
      "top-k ",
      "olap data cube"
    ],
    "absent_kp": [
      "range query"
    ]
  },
  {
    "title": "a characterization of interval-valued residuated lattices.",
    "abstract": "as is well-known, residuated lattices (rls) on the unit interval correspond to left-continuous t-norms. thus far, a similar characterization has not been found for rls on the set of intervals of [0, 1], or more generally, of a bounded lattice l. in this paper, we show that the open problem can be solved if it is restricted, making only a few simple and intuitive assumptions, to the class of interval-valued residuated lattices (ivrls). more specifically, we derive a full characterization of product and implication in ivrls in terms of their counterparts on the base rl to this aim, we use triangle algebras, a recently introduced variety of rls that serves as an equational representation of ivrls.",
    "present_kp": [
      "residuated lattices",
      "triangle algebras"
    ],
    "absent_kp": [
      "interval-valued fuzzy set theory"
    ]
  },
  {
    "title": "institutionalization of software product line: an empirical investigation of key organizational factors.",
    "abstract": "a good fit between the person and the organization is essential in a better organizational performance. this is even more crucial in case of institutionalization of a software product line practice within an organization. employees participation, organizational behavior and management contemplation play a vital role in successfully institutionalizing software product lines in a company. organizational dimension has been weighted as one of the critical dimensions in software product line theory and practice. a comprehensive empirical investigation to study the impact of some organizational factors on the performance of software product line practice is presented in this work. this is the first study to empirically investigate and demonstrate the relationships between some of the key organizational factors and software product line performance of an organization. the results of this investigation provide empirical evidence and further support the theoretical foundations that in order to institutionalize software product lines within an organization, organizational factors play an important role.",
    "present_kp": [
      "software product line",
      "organizational behavior"
    ],
    "absent_kp": [
      "empirical software engineering",
      "organizational theory",
      "organizational management"
    ]
  },
  {
    "title": "impending brain informatics research from web intelligence perspective.",
    "abstract": "web intelligence (wi)-based portal techniques (e.g. the wisdom web, data mining, multi-agent, and data/knowledge grids) will provide a new powerful platform for brain sciences. new understanding and discovery of the human intelligence models in brain sciences (e.g. cognitive science, neuroscience, brain informatics) will yield new wi research and development. in this paper, we briefly investigate three high-impact research issues as well as present a case study, to demonstrate the potentials of brain informatics (bi) research from wi perspective.",
    "present_kp": [
      "brain informatics ",
      "web intelligence "
    ],
    "absent_kp": [
      "data mining centric multi-layer grid",
      "multi-aspect data analysis"
    ]
  },
  {
    "title": "building a virtual framework for networked reconfigurable hardware and software objects.",
    "abstract": "a virtual framework that uses both hardware and software reconfigurable objects is presented. the new framework supports both networked hardware and software reconfiguration. in such a virtual framework, the networked reconfiguration users only need to develop a single service description targeted on a single hardware and software platform. the service is able to write once, and run everywhere. that facilitates the new service deployment and maintenance. we present the components, design flow and implementation aspects of the virtual framework.",
    "present_kp": [
      "networked reconfiguration"
    ],
    "absent_kp": [
      "platform-independence",
      "computer architecture",
      "design methodology"
    ]
  },
  {
    "title": "accurate and efficient evaluation of failure probability for partial different equations with random input data.",
    "abstract": "several computational challenges arise when evaluating the failure probability of a given system in the context of risk prediction or reliability analysis. when the dimension of the uncertainties becomes high, well established direct numerical methods can not be employed because of the curse-of-dimensionality. many surrogate models have been proposed with the aim of reducing computational effort. however, most of them fail in computing an accurate failure probability when the limit state surface defined by the failure event in the probability space lacks smoothness. in addition, for a stochastic system modeled by partial differential equations (pdes) with random input, only a limited number of the underlying pdes (order of a few tens) are affordable to solve in practice due to the considerable computational effort, therefore preventing the application of many numerical methods especially for high dimensional random inputs. in this work we develop hybrid and goal-oriented adaptive reduced basis methods to tackle these challenges by accurately and efficiently computing the failure probability of a stochastic pde. the curse-of-dimensionality is significantly alleviated by reduced basis approximation whose bases are constructed by goal-oriented adaptation. moreover, an accurate evaluation of the failure probability for pde system with solution of low regularity in probability space is guaranteed by the certified a posteriori error bound for the output approximation error. at the end of this paper we suitably extend our proposed method to deal with more general pde models. finally we perform several numerical experiments to illustrate its computational accuracy and efficiency.",
    "present_kp": [
      "reduced basis method",
      "goal-oriented adaptation",
      "partial differential equations",
      "random input data"
    ],
    "absent_kp": [
      "failure probability evaluation",
      "model order reduction"
    ]
  },
  {
    "title": "how personal task management differs across individuals.",
    "abstract": "summarizes individual differences in ptm by grouping people into three groups: diyers, make-doers, & adopters. assesses generalizability & extends the understanding of the above categorization. extends the understanding of the above user categories. individuals show varying tendencies toward adopting, make-doing, and diying in ptm. offers implications for the design of personalizable ptm tools.",
    "present_kp": [
      "personal task management",
      "individual differences"
    ],
    "absent_kp": [
      "behavioral differences",
      "personalization"
    ]
  },
  {
    "title": "implementation of h-infinity control algorithms for sensor-constrained mechatronic systems. using low-cost microcontrollers.",
    "abstract": "this paper introduces a novel method which is intended to assist in the design and implementation of optimal h-infinity (h.) algorithms in low-cost mechatronic applications. the particular problem considered is position control in a situation where there are both sensor-related uncertainties (caused by low-resolution sensors) and limited computational resources. the first part of the method presented in this paper describes how to design the h.. algorithm based on dynamic features of the sensor. the second part of the method involves finding a suitable numerical controller representation in order to reduce memory and cpu load. evaluation of the method is based on empirical studies using three industrial sensors employed in a sub-acted robot. results for a classic proportional integral derivative (pid) controller are included, in order to provide, comparisons with the h-infinity approach. in the empirical evaluation, the pid implementation shows marginal stability when the low-resolution sensor is employed; by contrast, the h. implementation is found to remain stable in the same circumstances.",
    "present_kp": [
      "h-infinity",
      "low-resolution",
      "mechatronic"
    ],
    "absent_kp": [
      "cost-constrained",
      "embedded system",
      "position control sensor",
      "proportional integral derivative  controller"
    ]
  },
  {
    "title": "mixture design applied to the formulation of hydrotropes for liquid detergents.",
    "abstract": "in a previous study, we presented a new method for preparing a mixture of sodium toluenesulfonates (sts), sodium xylenesulfonates (sxs), sodium benzenesulfonate (sbs) and na2so4 by sulfonating the btx fraction of a tunisian natural gas. such mixtures can be used as a hydrotrope agent for concentrated liquid detergents. in the present work, we performed a mixture design in order to study the effect of each of these four components on the clear point and the viscosity of a liquid detergent, and therefore, to determine the conditions allowing to improve the effectiveness of the hydrotrope. twenty-eight combinations of the 4 components out of 51 candidate points are selected by the nemrod-w software according to the d-optimal criterion to fit two polynomial models. the statistical study shows that the fitted models were adequate to describe the clear point and the viscosity responses. optimal conditions allowing to lower the two responses are then looking for by examining the response surface both as a contour plot, and as three-dimensional surface plot and the response trace. we prove that na2so4 exhibits a harmful negative effect, while sxs and sts exhibit, respectively, a strong and moderate positive effect on both clear point and viscosity responses. as expected, sbs has a harmful effect on the two responses but the magnitude of this effect is lesser than that predicted by the preliminary experiments carried out with sbs alone. this phenomenon is explained by the formation of heteroassociation between sbs, sts and sxs similar to what is found in surfactants. the effectiveness of the hydrotrope, obtained by sulfonation of the btx fraction of the tunisian natural gas, is really improved by removing sulfates either by adding lime to precipitate gypsum, or isopropanol to reduce the solubility of na2so4.",
    "present_kp": [
      "mixture design",
      "detergent",
      "hydrotrope",
      "btx",
      "clear point",
      "viscosity"
    ],
    "absent_kp": []
  },
  {
    "title": "open innovation in an enterprise 3.0 framework: three case studies.",
    "abstract": "nowadays, especially after the recent financial downturn, companies are looking for much more efficient and creative business processes. they need to place better solutions in the market in a less time with less cost. there is a general intuition that communication and collaboration, especially mixed with web 2.0 approach within companies and ecosystems, can boost the innovation process with positive impacts on business indicators. open innovation within an enterprise 2.0 context is a one of the most chosen paradigm for improving the innovation processes of enterprises, based on the collaborative creation and development of ideas and products. the key feature of this new paradigm is that the knowledge is exploited in a collaborative way flowing not only among internal sources, i.e. r&d departments, but also among external ones as other employees, customers, partners, etc. in this paper we show how an ontology-based analysis of plain text can provide a semantic contextualization of content support tasks, such as finding semantic distance between contents, and can help in creating relations between people with shared knowledge and interests. along this paper we will present the results obtained by the adoption of this technology in a large corporate environment like bankinter, a financial institution, telefonica i+d, an international telecommunication firm and repsol, a major oil company in spain.",
    "present_kp": [
      "enterprise 2.0",
      "open innovation"
    ],
    "absent_kp": [
      "semantic web",
      "semantic technologies"
    ]
  },
  {
    "title": "bio-inspired deployment of software over distributed systems.",
    "abstract": "this paper presents a middleware system for multi-agents on a distributed system as a general test-bed for bio-inspired approaches. the middleware is unique to other approaches, including distributed object systems, because it can maintain and migrate a dynamic federation of multiple agents on different computers. it enables each agent to explicitly define its own deployment policy as a relocation between the agent and another agent. this paper describes a prototype implementation of the middleware built on a java-based mobile agent system and its practical applications that illustrates the utility and effectiveness of the approach in real distributed systems.",
    "present_kp": [
      "agent",
      "distributed system",
      "bio-inspired approach",
      "policy"
    ],
    "absent_kp": [
      "self-organization"
    ]
  },
  {
    "title": "mso logics for weighted timed automata.",
    "abstract": "we aim to generalize buchi's fundamental theorem on the coincidence of recognizable and mso-definable languages to a weighted timed setting. for this, we investigate weighted timed automata and show how we can extend wilke's relative distance logic with weights taken from an arbitrary semiring. we show that every formula in our logic can effectively be transformed into a weighted timed automaton, and vice versa. the results indicate the robustness of weighted timed automata and may also be used for specification purposes.",
    "present_kp": [
      "weighted timed automata"
    ],
    "absent_kp": [
      "weighted mso logic",
      "real-time systems"
    ]
  },
  {
    "title": "preventing cancer by targeting abnormally expressed self-antigens: muc1 vaccines for prevention of epithelial adenocarcinomas.",
    "abstract": "prophylactic vaccines based on tumor-associated antigens (taas) have elicited concerns due to their potential toxicity. because taas are considered self-antigens, the prediction is that such vaccines will induce autoimmunity. while this has been observed in melanoma, where an antitumor immune response leads to vitiligo, autoimmunity has almost never been seen following vaccination with numerous other taas. we hypothesized that antigen choice determines outcome and have been working to identify taas whose expression differs between normal and tumor tissue, and thus could elicit antitumor immunity without autoimmunity. studies on the epithelial taa muc1 have revealed that, compared to muc1 on normal cells, tumors, premalignant lesions, and noncancerous pathologies affecting epithelial cells express abnormal muc1, which is not a self-antigen but rather an abnormal disease-associated antigen (daa). this distinction, which can be made for many known taas, has broad implications for the design and acceptance of preventative cancer vaccines.",
    "present_kp": [
      "muc1",
      "cancer vaccines"
    ],
    "absent_kp": [
      "immunosurveillance",
      "tumor immunology",
      "tumor antigens"
    ]
  },
  {
    "title": "type dependencies for logic programs using aci-unification.",
    "abstract": "this paper presents a new notion of typing for logic programs which generalizes the notion of directional types. the generation of tl pe dependencies for a logic program is fully automatic with respect to a given domain of types. the analysis method is based on a novel combination of program abstraction and aci-unification which is shown to be correct and optimal. type dependencies are obtained by abstracting programs, replacing concrete terms by their types, and evaluating the meaning of the abstract programs using a standard semantics for logic programs enhanced by aci-unification. this approach is generic and can be used with any standard semantics. the method is both theoretically clean and easy to implement using general purpose tools. the proposed domain of types is condensing which means that analyses can be carried out in both top-down or bottom-up frameworks with no loss of precision for goal-independent analyses. the proposed method has been fully implemented within a bottom-up approach and the experimental results are promising.",
    "present_kp": [
      "logic programs"
    ],
    "absent_kp": [
      "aci unification",
      "type analysis"
    ]
  },
  {
    "title": "on the asymptotic performance of threshold-based acquisition systems in multipath fading channels.",
    "abstract": "the asymptotic performance of timing. acquisition systems having fixed dwell time in multipath fading channels is investigated. the detrimental effect of the multipath channel fading on the acquisition performance is isolated by considering the asymptotic performance as the average signal-to-noise ratio (snr) increases. it is found that for any threshold such that the average probability of false alarm is less than a given tolerance, the channel fading results in a lower bound on the asymptotic average probability of miss which is nontrivial for a variety of fading scenarios. a threshold-based direct-sequence spread-spectrum signal acquisition system is considered and it is found that the detrimental effect of channel fading on asymptotic acquisition performance, albeit nontrivial, is not very significant. the asymptotic acquisition performance of two threshold-based acquisition schemes for ultra-wideband (uwb) signals with time-hopping (th) spreading are also evaluated and compared. for both schemes, the detrimental effect of the channel fading on the asymptotic acquisition performance turns out to be significant.",
    "present_kp": [
      "acquisition",
      "asymptotic performance",
      "fading channels",
      "ultra-wideband "
    ],
    "absent_kp": [
      "code-division multiple access "
    ]
  },
  {
    "title": "choquet boundaries and efficiency.",
    "abstract": "this research paper is devoted to establish the coincidence between choquet boundaries and a new type of approximate efficient points sets in ordered hausdorff locally convex spaces, being based on the first result established by us concerning such a property as this for pareto-type efficient points sets and the corresponding choquet boundaries of non-empty compact sets, with respect to appropriate convex cones of real, increasing and continuous functions. thus, the main result represents a strong connection between two great fields of mathematics: the axiomatic theory of potential and vector optimization. the present study contains also important relationships concerning strong optimization and approximate efficiency, interesting examples, pertinent remarks and some open problems.",
    "present_kp": [
      "approximate efficiency"
    ],
    "absent_kp": [
      "choquet boundary",
      "altomare projection",
      "full nuclear cone"
    ]
  },
  {
    "title": "the software accessibility of human-computer interfacesiso technical specification 16071.",
    "abstract": "this paper describes the recently published technical specification iso 16071 from the international organisation for standardisation (iso), along with the process through which the document has been developed. iso ts 16071 contains guidelines on designing accessible software. this paper also relates the activities within iso to other ongoing standardisation activities, within, for example, w3c and etsi. scope, contents, guidelines and the definition of accessibility in iso 16071 are discussed in relation to other definitions. finally, the process of turning the technical specification (ts) into an international standard (is) is discussed.",
    "present_kp": [
      "iso 16071",
      "standardisation",
      "technical specification"
    ],
    "absent_kp": [
      "ansi/hfes",
      "international standards organisation",
      "w3cwai guidelines"
    ]
  },
  {
    "title": "electromagnetic system navigating tunneling robots.",
    "abstract": "this paper describes a newly developed navigation system for tunneling robots. the system consists of a remote position-determination technique and an automatic direction control technique. the position of a robot traveling underground is measured by our proposed magnetic sensor sheet which is spread over the ground above the construction site. the sensor sheet contains an array of magnetometer boards which are especially designed to be 1.3mm thick and highly sensitive. each magnetometer in the sheet is a planar coil made using a printed-circuit-board photofabrication process and includes built-in amplifier and filter circuits. using the measured values, the robot direction controller generates control commands through an optimal regulator and a state observer with a kalman filter. experiments proved that the system determines the robot position accurately and constructs tunnels precisely.",
    "present_kp": [
      "tunneling robot",
      "magnetometer",
      "kalman filter",
      "optimal regulator"
    ],
    "absent_kp": [
      "electromagnetism"
    ]
  },
  {
    "title": "an agent based approach for exception handling in e-procurement management.",
    "abstract": "e-procurement has become an important function of enterprise information systems. the process of e-procurement includes the automatic definition of product requirements, search and selection for suppliers, negotiation and contracting with suppliers. however, the adoption of e-procurement encounters various exceptions from internal and external environments such as sharply increased demand, delivery delay and inventory failure. in this paper, we have proposed an agent and web service based architecture for exception handling in e-procurement. agent technology is applied to deal with the complex, dynamic, and distributed e-procurement process, while web service technology is applied to provide scalability and interoperability. in this architecture, different tasks in the e-procurement process, such as searching, negotiating, supplier selection, contracting, monitoring, and exception handling, are assigned to different agents, which are wrapped as web services. a set of rules for detecting and handling the exceptions is defined based on a basic inventory model and a genetic algorithm is utilized for supplier selection. to evaluate our exception handling approach, we have developed a prototype system, through which a simulation has been conducted to verify the effectiveness of our approach.",
    "present_kp": [
      "e-procurement",
      "exception handling",
      "web service"
    ],
    "absent_kp": [
      "intelligent agents"
    ]
  },
  {
    "title": "interface grammars for modular software model checking.",
    "abstract": "we propose an interface specification language based on grammars for modular software model checking. in our interface specification language, component interfaces are specified as context free grammars. an interface grammar for a component specifies the sequences of method invocations that are allowed by that component. using interface grammars one can specify nested call sequences that cannot be specified using interface specification formalisms that rely on finite state machines. moreover, our interface grammars allow specification of semantic predicates and actions, which are java code segments that can be used to express additional interface constraints. we have built an interface compiler that takes the interface grammar for a component as input and generates a stub for that component. the resulting stub is a table-driven parser generated from the input interface grammar. invocation of a method within the component becomes the lookahead symbol for the stub/parser. the stub/parser uses a parser stack, the lookahead, and a parse table to guide the parsing. the semantic predicates and semantic actions that appear in the right hand sides of the production rules are executed when they appear at the top of the stack. we conducted a case study by writing an interface grammar for the enterprise javabeans (ejb) persistence interface. using our interface compiler we automatically generated an ejb stub using the ejb interface grammar. we used the jpf model checker to check ejb clients using this automatically generated ejb stub. our results show that ejb clients can be verified efficiently using our approach.",
    "present_kp": [
      "parser",
      "enterprise",
      "input",
      "specification language",
      "context",
      "software model checking",
      "modular",
      "interfaces",
      "model",
      "constraint",
      "writing",
      "state machine",
      "grammar",
      "component",
      "model checking",
      "interface grammars",
      "product",
      "persistence",
      "method",
      "formalism",
      "semantic",
      "code",
      "rules",
      "sequence",
      "action",
      "parsing"
    ],
    "absent_kp": [
      "modular verification",
      "case studies",
      "segmentation",
      "compilation"
    ]
  },
  {
    "title": "effect of flow disturbances remaining at the beginning of diastole on intraventricular diastolic flow and colour m-mode doppler echocardiograms.",
    "abstract": "a computational model of the fluid dynamics of intraventricular flow was used to investigate the importance of the effects of flow disturbances existing within the left ventricle (lv) at the onset of diastole on a diastolic flow field. the simulation started with a quiescent flow state; it continued for a number of cardiac cycles to obtain a cyclically repeatable flow. after the flow became periodic, the initial diastolic flow was not quiescent. flow disturbances, remnants of a systolic flow, were present within the lv. nevertheless, they faded away during an acceleration phase of diastole and almost ceased by the end of this phase. consequently, a flow field during a deceleration phase of diastole, characterised by the formation of a vortex ring, was hardly affected by the initial flow disturbances. the propagation velocity of a colour m-mode doppler echocardiogram obtained by scanning velocity along the lv long axis was 0.58 m s(-1) in the case where diastolic flow was initially quiescent and 0.56 m s(-1) in the case where flow disturbances existed at the beginning of diastole. these results indicated that the colour m-mode doppler echocardiographic technique captures flow dynamics produced purely by ventricular expansion, with little influence from initial diastolic flow disturbances.",
    "present_kp": [
      "left ventricle",
      "colour m-mode doppler echocardiogram",
      "diastole"
    ],
    "absent_kp": [
      "blood flow",
      "computational fluid dynamics",
      "systole"
    ]
  },
  {
    "title": "a multi-criteria methodology to evaluate the optimal location of a multifunctional railway portal on the railway network.",
    "abstract": "multifunctional railway portals improves safety of a railway infrastructure. tccs monitors real time convoys transits, to detect specific faults. proposed ahp approach evaluates locations to install a tccs on a railway sections. main criteria to assess ranking of eligible sites-track layout, safety, technical feat.",
    "present_kp": [
      "multifunctional railway portal"
    ],
    "absent_kp": [
      "optimal location problem",
      "multicriteria decision problems",
      "analytic hierarchy process ",
      "dangerous goods transport"
    ]
  },
  {
    "title": "the effects of light-induced reduction of the photosystem ii reaction center.",
    "abstract": "accumulation of reduced pheophytin a (pheo-d1) in photosystem ii reaction center (psii rc) under illumination at low redox potential is accompanied by changes in absorbance and circular dichroism spectra. the temperature dependences of these spectral changes have the potential to distinguish between changes caused by the excitonic interaction and temperature-dependent processes. we observed a conformational change in the psii rc protein part and changes in the spatial positions of the psii rc pigments of the active d1 branch upon reduction of pheo-d1 only in the case of high temperature (298 k) dynamics. the resulting absorption difference spectra of psii rc models equilibrated at temperatures of 77 k and 298 k were highly consistent with our previous experiments in which light-induced bleaching of the psii rc absorbance spectrum was observable only at 298 k. these results support our previous hypothesis that pheo-d1 does not interact excitonically with the other chlorins of the psii rc, since the reduced form of pheo-d1 causes absorption spectra bleaching only due to temperature-dependent processes.",
    "present_kp": [
      "photosystem ii",
      "reaction center",
      "pheophytin",
      "absorption spectra"
    ],
    "absent_kp": [
      "exciton interactions",
      "molecular dynamics"
    ]
  },
  {
    "title": "zonal methods for the parallel execution of range-limited n-body simulations.",
    "abstract": "particle simulations in fields ranging from biochemistry to astrophysics require the evaluation of interactions between all pairs of particles separated by less than some fixed interaction radius. the applicability of such simulations is often limited by the time required for calculation, but the use of massive parallelism to accelerate these computations is typically limited by inter-processor communication requirements. recently, snir and shaw independently introduced two distinct methods that offer asymptotic reductions in the amount of data transferred between processors. in the present paper, we show that these schemes represent special cases of a more general class of methods, and introduce several new algorithms in this class that offer practical advantages over all previously described methods for a wide range of problem parameters. we also show that several of these algorithms approach an approximate lower bound on inter-processor data transfer.",
    "present_kp": [],
    "absent_kp": [
      "molecular simulation",
      "molecular dynamics",
      "parallel computing",
      "n-body problem",
      "pairwise particle interactions"
    ]
  },
  {
    "title": "performance analysis of direct n-body algorithms for astrophysical simulations on distributed systems.",
    "abstract": "we discuss the performance of direct summation codes used in the simulation of astrophysical stellar systems on highly distributed architectures. these codes compute the gravitational interaction among stars in an exact way and have an o(n2) scaling with the number of particles. they can be applied to a variety of astrophysical problems, like the evolution of star clusters, the dynamics of black holes, the formation of planetary systems, and cosmological simulations. the simulation of realistic star clusters with sufficiently high accuracy cannot be performed on a single workstation but may be possible on parallel computers or grids. we have implemented two parallel schemes for a direct n-body code and we study their performance on general purpose parallel computers and large computational grids. we present the results of timing analyzes conducted on the different architectures and compare them with the predictions from theoretical models. we conclude that the simulation of star clusters with up to a million particles will be possible on large distributed computers in the next decade. simulating entire galaxies however will in addition require new hybrid methods to speedup the calculation.",
    "present_kp": [
      "performance analysis",
      "grids"
    ],
    "absent_kp": [
      "n-body codes",
      "parallel algorithms"
    ]
  },
  {
    "title": "three dimensional experimental and numerical multiscale analysis of a fatigue crack.",
    "abstract": "a full three dimensional study of a fatigue crack in cast iron is presented. this analysis involves combining various tools, namely, synchrotron x-ray microtomography of an in situ experiment, image acquisition and treatment, 3d volume correlation to measure 3d displacement fields, extraction of the crack geometry, extended digital volume correlation to account for the crack displacement discontinuity, crack modeling in an elastic material exploiting the actual crack geometry, and finally estimation of stress intensity factors. all these different tasks are based on specific multiscale approaches.",
    "present_kp": [
      "volume correlation",
      "fatigue crack",
      "x-ray microtomography"
    ],
    "absent_kp": [
      "multiscale analyses",
      "sif estimates",
      "level sets",
      "multi-grid",
      "x-fem"
    ]
  },
  {
    "title": "a pgm framework for recursive modeling of players in simple sequential bayesian games.",
    "abstract": "we consider the situation where two agents try to solve each their own task in a common environment. in particular, we study simple sequential bayesian games with unlimited time horizon where two players share a visible scene, but where the tasks (termed assignments) of the players are private information. we present an influence diagram framework for representing simple type of games, where each player holds private information. the framework is used to model the analysis depth and time horizon of the opponent and to determine an optimal policy under various assumptions on analysis depth of the opponent. not surprisingly, the framework turns out to have severe complexity problems even in simple scenarios due to the size of the relevant past. we propose two approaches for approximation. one approach is to use limited memory influence diagrams (limids) in which we convert the influence diagram into a set of bayesian networks and perform single policy update. the other approach is information enhancement, where it is assumed that the opponent in a few moves will know your assignment. empirical results are presented using a simple board game.",
    "present_kp": [
      "sequential bayesian games",
      "influence diagrams",
      "limids"
    ],
    "absent_kp": [
      "multiple agents",
      "recursive modeling method"
    ]
  },
  {
    "title": "the design of the cade-13 atp system competition.",
    "abstract": "running a competition for automated theorem proving (atp) systems is a difficult and arguable venture. however, the potential benefits of such an event by far outweigh the controversial aspects. the motivations for running the cade-13 atp system competition were to contribute to the evaluation of atp systems, to stimulate atp research and system development, and to expose atp systems to researchers both within and outside the atp community. this article identifies and discusses the issues that determine the nature of such a competition. choices and motivated decisions for the cade-13 competition, with respect to the issues, are given.",
    "present_kp": [
      "automated theorem proving",
      "competition",
      "design"
    ],
    "absent_kp": []
  },
  {
    "title": "bioontoverb: a top level ontology based framework to populate biomedical ontologies from texts.",
    "abstract": "the semantic web can be conceived as an extension of the current web where information is given well-defined meaning. in this scenario ontologies are crucial since they provide meaning and facilitate the search for contents and information. ontology population is a knowledge acquisition activity used to transform data sources into instance data. the instantiation of ontologies with new knowledge is an important step towards the provision of valuable ontology-based services. in this paper, we present a methodology to be used for ontology population. for it, top level ontologies that define the basic semantic relations in biomedical domains are mapped onto semantic role labelling resources, where every semantic role defines the role of a verbal argument in the event expressed by the verb. the modular architecture employed in our work gives the system a high versatility, as resources have been developed separately and they can be easily adapted to most biomedical domain ontologies.",
    "present_kp": [
      "biomedical ontologies",
      "semantic web",
      "ontology population",
      "semantic role",
      "knowledge acquisition"
    ],
    "absent_kp": []
  },
  {
    "title": "incompressible sph method based on rankine source solution for violent water wave simulation.",
    "abstract": "with wide applications, the smoothed particle hydrodynamics method (abbreviated as sph) has become an important numerical tool for solving complex flows, in particular those with a rapidly moving free surface. for such problems, the incompressible smoothed particle hydrodynamics (isph) has been shown to yield better and more stable pressure time histories than the traditional sph by many papers in literature. however, the existing isph method directly approximates the second order derivatives of the functions to be solved by using the poisson equation. the order of accuracy of the method becomes low, especially when particles are distributed in a disorderly manner, which generally happens for modelling violent water waves. this paper introduces a new formulation using the rankine source solution. in the new approach to the isph, the poisson equation is first transformed into another form that does not include any derivative of the functions to be solved, and as a result, does not need to numerically approximate derivatives. the advantage of the new approach without need of numerical approximation of derivatives is obvious, potentially leading to a more robust numerical method. the newly formulated method is tested by simulating various water waves, and its convergent behaviours are numerically studied in this paper. its results are compared with experimental data in some cases and reasonably good agreement is achieved. more importantly, numerical results clearly show that the newly developed method does need less number of particles and so less computational costs to achieve the similar level of accuracy, or to produce more accurate results with the same number of particles compared with the traditional sph and existing isph when it is applied to modelling water waves.",
    "present_kp": [
      "sph",
      "isph",
      "violent water waves"
    ],
    "absent_kp": [
      "meshless method",
      "isph_r",
      "free surface flow",
      "wave impact"
    ]
  },
  {
    "title": "restricted and swap common superstring: a multivariate algorithmic perspective.",
    "abstract": "in several areas, for example in bioinformatics and in ai planning, the shortest common superstring problem (scs) and variants thereof have been successfully applied for string comparison. in this paper we consider two variants of scs recently introduced, namely restricted common superstring (rcs) and swapped common superstring (srcs). in rcs we are given a set (s) of strings and a multiset (mathcal {m}) of symbols, and we look for an ordering (mathcal {m}_o) of (mathcal {m}) such that the number of input strings which are substrings of (mathcal {m}_o) is maximized. in srcs we are given a set (s) of strings and a text (mathcal {t}), and we look for a swap ordering (mathcal {t}_o) of (mathcal {t}) (an ordering of (mathcal {t}) obtained by swapping only some pairs of adjacent symbols) such that the number of input strings which are substrings of (mathcal {t}_o) is maximized. in this paper we propose a multivariate algorithmic analysis of the complexity of the two problems, aiming at determining how different parameters influence the complexity of the two problems. we consider as interesting parameters the size of the solutions (that is the number of input strings contained in the computed superstring), the maximum length of the given input strings, the size of the alphabet over which the input strings range. first, we give two fixed-parameter algorithms, where the parameter is the size of the solution, for srcs and lrcs (the rcs problem restricted to strings of length bounded by a parameter (ell )). furthermore, we complement these results by showing that srcs and lrcs do not admit a polynomial kernel unless (np subseteq conp/poly). then, we show that srcs is apx-hard even when the input strings have length bounded by a constant (equal to (10)) or are over a binary alphabet.",
    "present_kp": [
      "shortest common superstring",
      "string comparison",
      "algorithms"
    ],
    "absent_kp": [
      "parameterized complexity",
      "apx-hardness"
    ]
  },
  {
    "title": "background adjustment for dna microarrays using a database of microarray experiments.",
    "abstract": "dna microarrays have become an indispensable technique in biomedical research. the raw measurements from microarrays undergo a number of preprocessing steps before the data are converted to the genomic level for further analysis. background adjustment is an important step in preprocessing. estimating background noise has been challenging because background levels vary a lot from probe to probe, yet there are limited observations on each probe. most current methods have used the empirical bayes approach to borrow information across probes on the same array. these approaches shrink the background estimate for either the entire sample or probes sharing similar sequence structures. in this article, we present a solution that is truly probe specific by using a database of large number of microarray experiments. information is borrowed across samples and background noise is estimated for each probe individually. the ability to obtain probe specific background distributions allows us to extend the dynamic range of gene expression levels. we illustrate the improvement in detecting gene expression variation on two datasets: a latin square spike-in experiment from affymetrix and an estrogen receptor experiment with biological replicates. an r package dbrma implementing our method can be obtained from the authors.",
    "present_kp": [
      "background adjustment",
      "gene expression",
      "preprocessing"
    ],
    "absent_kp": [
      "high-density oligonucleotide microarrays"
    ]
  },
  {
    "title": "designing interaction protocols using noughts and crosses type games.",
    "abstract": "interaction management is concerned with the protocols that govern structured interactive activities among multiple users or agents in networked collaborative environments. it is an important aspect of networked software in many application domains such as online meetings, online groupware and online games. however, there is limited support in most programming languages and programming environments for implementing interaction management. high-level features, such as interaction protocols and management policies, are usually hard coded by skilled network programmers, who are often scarce in many applications such as e-learning. in this paper, we present an abstraction of various collaborative applications in the form of the noughts and crosses game and its variations. we examine the needs in these games for programming interaction protocols, and propose a comprehensive collection of program constructs for supporting interaction. we report our efforts for incorporating these new constructs into jacie (java-based authoring language for collaborative interactive environments), an existing scripting language designed to support rapid prototyping and implementation of collaborative applications. we demonstrate, through variations of the noughts and crosses game and an on-line bridge game, the usefulness of these language constructs.",
    "present_kp": [
      "collaborative applications",
      "interaction protocols",
      "rapid prototyping"
    ],
    "absent_kp": [
      "net-centric computing",
      "authoring tools",
      "scripting languages"
    ]
  },
  {
    "title": "samac: a cross-layer communication protocol for sensor networks with sectored antennas.",
    "abstract": "wireless sensor networks have been used to gather data and information in many diverse application settings. the capacity of such networks remains a fundamental obstacle toward the adaptation of sensor network systems for advanced applications that require higher data rates and throughput. in this paper, we explore potential benefits of integrating directional antennas into wireless sensor networks. while the usage of directional antennas has been investigated in the past for ad hoc networks, their usage in sensor networks bring both opportunities as well as challenges. in this paper, sectored-antenna medium access control (samac), an integrated cross-layer protocol that provides the communication mechanisms for sensor network to fully utilize sectored antennas, is introduced. simulation studies show that samac delivers high energy efficiency and predictable delay performance with graceful degradation in performance with increased load.",
    "present_kp": [
      "sensor networks",
      "sectored antennas",
      "directional antennas"
    ],
    "absent_kp": [
      "scheduling",
      "cross-layer protocols",
      "tdma",
      "csma/ca"
    ]
  },
  {
    "title": "the relative distance of key point based iris recognition.",
    "abstract": "iris recognition has received increasing attention in recent years as a reliable approach to human identification. this paper makes an attempt to analyze the local feature structure of iris texture information based on the relative distance of key points. when preprocessed, the annular iris is normalized into a rectangular block. multi-channel 2-d gabor filters are used to capture the iris texture. in every filtered sub-image, we extract the points that can represent the local texture most effectively in each channel. the barycenter of these points in each channel is called the key point and a group of key points are obtained. then, the distance between the center of key points of each sub-image and every key point is called relative distance, which is regarded as the iris feature vector. iris feature matching is based on the euclidean distance. experimental results on public and private databases show that the performance of the proposed method is encouraging.",
    "present_kp": [
      "iris recognition",
      "key point",
      "relative distance",
      "gabor filter"
    ],
    "absent_kp": []
  },
  {
    "title": "business process outsourcing: an event study on the nature of processes and firm valuation.",
    "abstract": "business process outsourcing (bpo) is a phenomenon that is rapidly increasing in both incidence and importance. this study empirically examines the value proposition of bpo with respect to the nature of the processes being outsourced. using the event study methodology, we employ the value chain (vc) position and existing ownership of a business process as our primary independent variables, and the stock abnormal return in response to the bpo announcement as the dependent performance variable in our research model. the study was conducted on 298 bpo announcements from 1998 to 2005. results support the argument that outsourcing is valuable for both primary and supportive business processes. however, we found that bpo announcements on primary processes yield higher abnormal returns than supportive processes. although existing process ownership was not found to be a powerful differentiator for bpo performance, its interaction with vc position provides important insights into the timing of outsourcing. the evidence suggests that internal cultivation of processes is important for bpo success, particularly when bpo is applied to primary processes.",
    "present_kp": [
      "business process outsourcing",
      "value chain",
      "process ownership",
      "event study"
    ],
    "absent_kp": []
  },
  {
    "title": "a computational study on the quadratic knapsack problem with multiple constraints.",
    "abstract": "the quadratic knapsack problem (qkp) has been the subject of considerable research in recent years. despite notable advances in special purpose solution methodologies for qkp, this problem class remains very difficult to solve. with the exception of special cases, the state-of-the-art is limited to addressing problems of a few hundred variables and a single knapsack constraint. in this paper we provide a comparison of quadratic and linear representations of qkp based on test problems with multiple knapsack constraints and up to eight hundred variables. for the linear representations, three standard linearizations are investigated. both the quadratic and linear models are solved by standard branch-and-cut optimizers available via cplex. our results show that the linear models perform well on small problem instances but for larger problems the quadratic model outperforms the linear models tested both in terms of solution quality and solution time by a wide margin. moreover, our results demonstrate that qkp instances larger than those previously addressed in the literature as well as instances with multiple constraints can be successfully and efficiently solved by branch and cut methodologies.",
    "present_kp": [
      "knapsack problem",
      "branch-and-cut",
      "linearization"
    ],
    "absent_kp": [
      "0-1 quadratic programming",
      "mixed integer quadratic program"
    ]
  },
  {
    "title": "scaling replica maintenance in intermittently synchronized mobile databases.",
    "abstract": "to avoid the high cost of continuous connectivity, a class of mobile applications employs replicas of shared data that are periodically updated. updates to these replicas are typically performed on a client-by-client basis--that is, the server individually computes and transmits updates to each client--limiting scalability. by basing updates on replica groups (instead of clients), however, update generation complexity is no longer bound by client population size. clients then download updates of pertinent groups. proper group design reduces redundancies in server processing, disk usage and bandwidth usage, and dimininishes the tie between the complexity of updating replicas and the size of the client population. in this paper, we expand on previous work done on group design, include a detailed i/o cost model for update generation, and propose a heuristic-based greedy algorithm for group computation. experimental results with an adapted commercial replication system demonstrate a significant increase in overall scalability over the client-centric approach.",
    "present_kp": [
      "mobile databases"
    ],
    "absent_kp": [
      "intermittent synchronization",
      "distributed databases"
    ]
  },
  {
    "title": "on fuzzification of some concepts of graphs.",
    "abstract": "in this paper, we will be interested by the extension of the concepts of internal stability, external stability, not external domination and some of their combination to fuzzy graphs. fuzzy sets and fuzzy logical operators are used as basis tools to model these concepts.",
    "present_kp": [
      "fuzzy graphs"
    ],
    "absent_kp": [
      "fuzzy operators",
      "non dominated fuzzy sets",
      "fuzzy kernels",
      "fuzzy g-kernel"
    ]
  },
  {
    "title": "linearised euclidean shortening flow of curve geometry.",
    "abstract": "the geometry of a space curve is described in terms of a euclidean invariant frame field, metric, connection, torsion and curvature. here the torsion and curvature of the connection quantify the curve geometry. in order to retain a stable and reproducible description of that geometry, such that it is slightly affected by non-uniform protrusions of the curve, a linearised euclidean shortening flow is proposed. (semi)-discretised versions of the flow subsequently physically realise a concise and exact (semi-)discrete curve geometry. imposing special ordering relations the torsion and curvature in the curve geometry can be retrieved on a multi-scale basis not only for simply closed planar curves but also for open, branching, intersecting and space curves of non-trivial knot type. in the context of the shortening flows we revisit the maximum principle, the semi-group property and the comparison principle normally required in scale-space theories. we show that our linearised flow satisfies an adapted maximum principle, and that its green's functions possess a semi-group property. we argue that the comparison principle in the case of knots can obstruct topological changes being in contradiction with the required curve simplification principle. our linearised flow paradigm is not hampered by this drawback; all non-symmetric knots tend to trivial ones being infinitely small circles in a plane. finally, the differential and integral geometry of the multi-scale representation of the curve geometry under the flow is quantified by endowing the scale-space of curves with an appropriate connection, and calculating related torsion and curvature aspects. this multi-scale modern geometric analysis forms therewith an alternative for curve description methods based on entropy scale-space theories.",
    "present_kp": [
      "frame field",
      "metric",
      "connection",
      "torsion",
      "curvature",
      "scale-space"
    ],
    "absent_kp": [
      "linearised shortening flow",
      "similarity jet"
    ]
  },
  {
    "title": "an fpga based coprocessor for glcm and haralick texture features and their application in prostate cancer classification.",
    "abstract": "grey level co-occurrence matrix (glcm), one of the best known tool for texture analysis, estimates image properties related to second-order statistics. these image properties commonly known as haralick texture features can be used for image classification, image segmentation, and remote sensing applications. however, their computations are highly intensive especially for very large images such as medical ones. therefore, methods to accelerate their computations are highly desired. this paper proposes the use of programmable hardware to accelerate the calculation of glcm and haralick texture features. further, as an example of the speedup offered by programmable logic, a multispectral computer vision system for automatic diagnosis of prostatic cancer has been implemented. the performance is then compared against a microprocessor based solution.",
    "present_kp": [
      "glcm",
      "haralick texture features"
    ],
    "absent_kp": [
      "fpgas",
      "multispectral images",
      "medical image classification"
    ]
  },
  {
    "title": "supervised reranking for web image search.",
    "abstract": "visual search reranking that aims to improve the text-based image search with the help from visual content analysis has rapidly grown into a hot research topic. the interestingness of the topic stems mainly from the fact that the search reranking is an unsupervised process and therefore has the potential to scale better than its main alternative, namely the search based on offline-learned semantic concepts. however, the unsupervised nature of the reranking paradigm also makes it suffer from problems, the main of which can be identified as the difficulty to optimally determine the role of visual modality over different application scenarios. inspired by the success of the \"learning-to-rank\" idea proposed in the field of information retrieval, we propose in this paper the \"learning-to-rerank\" paradigm, which derives the reranking function in a supervised fashion from the human-labeled training data. although supervised learning is introduced, our approach does not suffer from scalability issues since a unified reranking model is learned that can be applied to all queries. in other words, a query-independent reranking model will be learned for all queries using query-dependent reranking features. the query-dependent reranking feature extraction is challenging since the textual query and the visual documents have different representation. in this paper, 11 lightweight reranking features are proposed by representing the textual query using visual context and pseudo relevant images from the initial search result. the experiments performed on two representative web image datasets demonstrate that the proposed learning-to-rerank algorithm outperforms the state-of-the-art unsupervised reranking methods, which makes the learning-to-rerank paradigm a promising alternative for robust and reliable web-scale image search.",
    "present_kp": [
      "supervised reranking",
      "visual search reranking"
    ],
    "absent_kp": [
      "learning to rerank"
    ]
  },
  {
    "title": "new crosstalk avoidance codes based on a novel pattern classification.",
    "abstract": "the crosstalk delay associated with global on-chip interconnects becomes more severe in deep submicrometer technology, and hence can greatly affect the overall system performance. based on a delay model proposed by sotiriadis et al., transition patterns over a bus can be classified according to their delays. using this classification, crosstalk avoidance codes (cacs) have been proposed to alleviate the crosstalk delays by restricting the transition patterns on a bus. in this paper, we first propose a new classification of transition patterns, and then devise a new family of cacs based on this classification. in comparison to the previous classification, our classification has more classes and the delays of its classes do not overlap, both leading to more accurate control of delays. our new family of cacs includes some previously proposed codes as well as new codes with reduced delays and improved throughput. thus, this new family of cacs provides a wider variety of tradeoffs between bus delay and efficiency. finally, since our analytical approach to the classification and cacs treats the technology-dependent parameters as variables, our approach can be easily adapted to a wide variety of technologies.",
    "present_kp": [
      "crosstalk avoidance codes",
      "delay",
      "interconnects"
    ],
    "absent_kp": []
  },
  {
    "title": "lpfml: a wk xml schema for linear and integer programming.",
    "abstract": "t here are numerous modeling systems for generating linear programs and numerous solvers for optimizing them. however, it is often impossible for modelers to combine their preferred modeling system with their preferred solver. current modeling systems use their own proprietary model-instance formats that various solvers have been adapted to recognize. the existence of all of these formats suggests that one way to encourage modeling-system and solver compatibility is to use a standard representation of a problem instance. such a standard must be simple to manipulate and validate, be able to express instance-specific and vendor-specific information, and promote the integration of optimization software with other software. in this paper we present lpfml, an xml schema for representing linear-programming (lp) instances. in addition, we provide open-source c++ libraries that simplify the exchange of problem-instance and solution information between modeling systems and solvers. we show how our system is used to enable previously unavailable language-solver connections and how our design improves on the state of the art under three different scenarios relevant to communication between solvers and modeling systems.",
    "present_kp": [
      "xml"
    ],
    "absent_kp": [
      "linear programming",
      "information systems"
    ]
  },
  {
    "title": "high-order algorithms for vascular flow modelling.",
    "abstract": "this paper presents the development of spectral/hp high-order elements for vascular flows with particular attention to surface reconstruction and high-order mesh generation. using ideas from computer visualization we apply a technique of constructing smooth implicit functions to reconstruct incomplete sectional imaging data from magnetic resonance imaging. using this as a starting point we outline techniques to discretize and computationally solve the newtonian flow within these geometries using high-order spectral/hp element methods. finally we demonstrate the application of these ideas to anatomically correct and model distal bypass grafts.",
    "present_kp": [
      "high-order mesh generation"
    ],
    "absent_kp": []
  },
  {
    "title": "a mean-value performance analysis of a new multiprocessor architecture.",
    "abstract": "this paper presents a preliminary performance analysis of a new large-scale multiprocessor: the wisconsin multicube . a key characteristic of the machine is that it is based on shared buses and a snooping cache coherence protocol. the organization of the shared buses and shared memory is unique and non-hierarchical. the two-dimensional version of the architecture is envisioned as scaling to 1024 processors. we develop an approximate mean-value analysis of bus interference for the proposed cache coherence protocol. the model includes fcfs scheduling at the bus queues with deterministic bus access times, and asynchronous memory write-backs and invalidation requests. we use our model to investigate the feasibility of the multiprocessor, and to study some initial system design issues. our results indicate that a 1024-processor system can operate at 75 - 95% of its peak processing power, if the mean time between cache misses is larger than 1000 bus cycles (i.e. 50 microseconds for 20 mhz buses; 25 microseconds for 40 mhz buses). this miss rate is not unreasonable for the cache sizes specified in the design, which are comparable to main memory sizes in existing multiprocessors. we also present results which address the issues of optimal cache block size, optimal size of the two-dimensional multicube, the effect of broadcast invalidations on system performance, and the viability of several hardware techniques for reducing the latency for remote memory requests.",
    "present_kp": [
      "value",
      "scheduling",
      "use",
      "scale",
      "large-scale",
      "analysis",
      "performance",
      "design",
      "queue",
      "paper",
      "model",
      "access",
      "processor",
      "cache coherence",
      "shared memory",
      "hierarchic",
      "organization",
      "cache",
      "latency",
      "system design",
      "broadcast",
      "size",
      "architecture",
      "mean",
      "process",
      "power",
      "version",
      "hardware",
      "effect",
      "multiprocessor",
      "performance analysis",
      "interference"
    ],
    "absent_kp": [
      "approximation",
      "addressing",
      "timing",
      "writing",
      "sharing",
      "systems",
      "optimality",
      "asynchronicity",
      "memorialized",
      "blocking"
    ]
  },
  {
    "title": "the variants of the harmony search algorithm: an overview.",
    "abstract": "the harmony search (hs) algorithm is a relatively new population-based metaheuristic optimization algorithm. it imitates the music improvisation process where musicians improvise their instruments' pitch by searching for a perfect state of harmony. since the emergence of this algorithm in 2001, it attracted many researchers from various fields especially those working on solving optimization problems. consequently, this algorithm guided researchers to improve on its performance to be in line with the requirements of the applications being developed. these improvements primarily cover two aspects: (1) improvements in terms of parameters setting, and (2) improvements in terms of hybridizing hs components with other metaheuristic algorithms. this paper presents an overview of these aspects, with a goal of providing useful references to fundamental concepts accessible to the broad community of optimization practitioners.",
    "present_kp": [
      "harmony search",
      "metaheuristic optimization"
    ],
    "absent_kp": []
  },
  {
    "title": "an empirical study of optimizations in yogi.",
    "abstract": "though verification tools are finding industrial use, the utility of engineering optimizations that make them scalable and usable is not widely known. despite the fact that several optimizations are part of folklore in the communities that develop these tools, no rigorous evaluation of these optimizations has been done before. we describe and evaluate several engineering optimizations implemented in the yogi property checking tool, including techniques to pick an initial abstraction, heuristics to pick predicates for refinement, optimizations for interprocedural analysis, and optimizations for testing. we believe that our empirical evaluation gives the verification community useful information about which optimizations they could implement in their tools, and what gains they can realistically expect from these optimizations.",
    "present_kp": [
      "testing"
    ],
    "absent_kp": [
      "software model checking",
      "directed testing",
      "abstraction refinement"
    ]
  },
  {
    "title": "stochastic iteration for non-diffuse global illumination.",
    "abstract": "this paper presents a single-pass, view-dependent method to solve the rendering equation, using a stochastic iterational scheme where the transport operator is selected randomly in each it ration. the requirements of convergence are given for the general case. to demonstrate the basic idea a very simple, continuous random transport operator is examined, which gives back the light tracing algorithm incorporating russian roulette. then, a new mixed continuous and finite-element based iteration method is proposed, which uses ray-bundles to transfer the radiance in a single random direction. the resulting algorithm is fast, it provides initial results in seconds and accurate solutions in minutes and does not suffer from the error accumulation problem and the high memory demand of other finite-element and hierarchical approaches.",
    "present_kp": [
      "rendering equation"
    ],
    "absent_kp": [
      "global radiance",
      "monte-carlo integration",
      "light-tracing",
      "global ray-bundle tracing"
    ]
  },
  {
    "title": "an expert system using rough sets theory for aided conceptual design of ship's engine room automation.",
    "abstract": "more and more complicated conceptual design of ship's engine room (cdser) heavily depends oil designers' engineering knowledge and existing ship data. to achieve intelligent design at the initial ship design stage, many researchers have made much significant progress in this field, however, most of them only focused oil how to find tile similar constructed ships. at present, how to utilize these existing data remains all untouched topic. in order to make good use of the existing data and reduce the dependence oil designers' experience, a novel system named expert system for aided conceptual design of ship's engine room automation (esacd), is elaborated ill this study. with the support of the constructed ship data warehouse system, two core subsystems configuration selection assistant (csa) and design scheme decision assistant (dsda) are included in esacd. a promising approach integrating fuzzy c-means algorithm (fcm) and rough sets theory (rst) to extract configuration rules from the stored data is adopted in csa. according to engineers' proposals, rst is utilized to reason knowledge in incomplete scheme information systems for getting design scheme rules in dsda, which are useful suggestions for engineers to get better schemes at this stage. finally, tile validity and necessity of this interactive expert system are demonstrated through the cdser of a new 50,000 dwt handymax bulk carrier. it is proved that esacd call efficiently facilitate rapid and intelligent design ill cdser, and reduce the cost of a new ship design.",
    "present_kp": [
      "conceptual design of ship's engine room ",
      "rough sets theory "
    ],
    "absent_kp": [
      "discretization",
      "configuration of engine room",
      "design schemes"
    ]
  },
  {
    "title": "a novel approach to smart multi-cell radio resource management based on load gradient calculations.",
    "abstract": "this paper presents a novel methodology for capturing the coupling between the different cells in both the uplink and downlink directions in a wideband code division multiple access (wcdma) scenario. it is based on the definition and computation of the gradient of the uplink cell load factor and the downlink transmitted power, which are the two main parameters that reflect the actual cell load in the two link directions. the paper shows that the gradient is able to capture the relevant information about the spatial distribution of traffic, which has an impact on cell performance. the proposed methodology is also used as the basis for defining and evaluating new radio resource management (rrm) strategies that operate at a multi-cell level.",
    "present_kp": [
      "wcdma",
      "radio resource management"
    ],
    "absent_kp": [
      "cellular systems",
      "spatial traffic distribution"
    ]
  },
  {
    "title": "kernels, regularization and differential equations.",
    "abstract": "many common machine learning methods such as support vector machines or gaussian process inference make use of positive definite kernels, reproducing kernel hilbert spaces, gaussian processes, and regularization operators. in this work these objects are presented in a general, unifying framework and interrelations are highlighted. with this in mind we then show how linear stochastic differential equation models can be incorporated naturally into the kernel framework. and vice versa, many kernel machines can be interpreted in terms of differential equations. we focus especially on ordinary differential equations, also known as dynamical systems, and it is shown that standard kernel inference algorithms are equivalent to kalman filter methods based on such models. in order not to cloud qualitative insights with heavy mathematical machinery, we restrict ourselves to finite domains, implying that differential equations are treated via their corresponding finite difference equations.",
    "present_kp": [
      "positive definite kernel",
      "differential equation",
      "gaussian process",
      "reproducing kernel hilbert space"
    ],
    "absent_kp": []
  },
  {
    "title": "an integrated model for the latency and steady-state throughput of tcp connections.",
    "abstract": "most tcp connections in today's internet transfer data on the order of only a few kilobytes. such tcp transfers are very short and spend most of their time in the slow start phase. thus the underlying assumptions made by steady-state models cease to hold, making them unsuitable for modeling finite flows. in this paper, we propose an accurate model for estimating the transfer times of tcp flows of arbitrary size. our model gives a more accurate estimation of the transfer times than those predicted by cardwell et al. to model finite flows. the main features of our work are the modeling of timeouts and slow start phases which occur anywhere during the transfer and a more accurate model for the evolution of the cwnd in the slow start phase. additionally, the proposed model can also model the steady-state throughput of tcp connections. the model is verified using web based measurements of real life tcp connections. we also introduce an empirical model which allows a better \"feel\" of tcp latency and the nature of its dependence on loss probabilities and window limitation. finally, the paper investigates the effect on window limitation and packet size on tcp latency.",
    "present_kp": [
      "tcp connections",
      "latency",
      "steady-state throughput",
      "internet"
    ],
    "absent_kp": []
  },
  {
    "title": "a method for representing 3d tree objects using chain coding.",
    "abstract": "we describe a method for representing 3d (three-dimensional) tree objects by means of a chain code. these 3d tree objects correspond to natural existing 3d tree structures, such as: blood vessels, plants, live trees, and so on. thus, trees are digitalized and represented by a notation called the unique tree descriptor. the unique tree descriptor is invariant under translation and rotation. furthermore, this descriptor is starting vertex normalized via the unique path in the tree. also, it is possible to obtain the mirror image of any tree with ease. this unique tree descriptor preserves the shape of trees (and the shape of their branches), allows us to know their geometrical and topological properties. to determine if two 3d tree objects have the same shape, it is only necessary to see if their descriptors are equal. in this manner, graph comparisons and tree searches are eliminated. also, the proposed tree descriptor is a good tool for storing of 3d tree objects. finally, in order to prove our method for representing 3d tree objects, we obtain some tree descriptors of objects on real images.",
    "present_kp": [
      "3d tree objects",
      "3d tree structures",
      "unique tree descriptor",
      "chain coding"
    ],
    "absent_kp": [
      "3d discrete branches",
      "3d tree representation"
    ]
  },
  {
    "title": "a fast binary feedback-based distributed adaptive carrier synchronisation for transmission among clusters of disconnected iot nodes in smart spaces.",
    "abstract": "we propose a transmission scheme among groups of disconnected iot devices in a smart space. in particular, we propose the use of a local random search implementation to speed up the synchronisation of carriers for distributed adaptive transmit beamforming. we achieve a sharp bound on the asymptotic carrier synchronisation time which is significantly lower than for previously proposed carrier synchronisation processes. also, we consider the impact of environmental conditions in smart spaces on this synchronisation process in simulations and a case study.",
    "present_kp": [
      "smart spaces",
      "transmission scheme",
      "carrier synchronisation"
    ],
    "absent_kp": [
      "collaborative",
      "sensor networks"
    ]
  },
  {
    "title": "circulant preconditioners for indefinite toeplitz systems.",
    "abstract": "in recent papers circulant preconditioners were proposed for ill-conditioned hermitian toeplitz matrices generated by 2pi-periodic continuous functions with zeros of even order. it was show that the spectra of the preconditioned matrices are uniformly bounded except for a finite number of outliers and therefore the conjugate gradient method, when applied to solving these circulant preconditioned systems, converges very quickly. i this paper, we consider indefinite toeplitz matrices generated by 2pi-periodic continuous functions with zeros of odd order. in particular, we show that the singular values of the preconditioned matrices are essentially bounded. numerical results are presented to illustrate the fast convergence of cgne, minres and qmr methods.",
    "present_kp": [
      "indefinite toeplitz systems"
    ],
    "absent_kp": [
      "banded matrices",
      "preconditioned conjugate-gradient-type method",
      "circulant matrices"
    ]
  },
  {
    "title": "effects of melatonin in age-related macular degeneration.",
    "abstract": "age-related macular degeneration (amd) is the leading cause of severe visual loss in aged people. melatonin has been shown to have the capacity to control eye pigmentation and thereby regulate the amount of light reaching the photoreceptors, to scavenge hydroxyradicals and to protect retinal pigment epithelium (rpe) cells from oxidative damage. therefore, it is reasonable to think that the physiological decrease of melatonin in aged people may be an important factor in rpe dysfunction, which is a well known cause for initiation of amd. our purpose is to explore a new approach to prevent or treat amd. we began case control study with a follow-up of 6 to 24 months. one hundred patients with amd were diagnosed and 3 mg melatonin was given orally each night at bedtime for at least 3 months. both dry and wet forms of amd were included. fifty-five patients were followed for more than 6 months. at 6 months of treatment, the visual acuity had been kept stable in general. though the follow up time is not long, this result is already better than the otherwise estimated natural course.1,2 the change of the fundus picture was remarkable. only 8 eyes showed more retinal bleeding and 6 eyes more retinal exudates. the majority had reduced pathologic macular changes. we conclude that the daily use of 3 mg melatonin seems to protect the retina and to delay macular degeneration. no significant side effects were observed.",
    "present_kp": [
      "melatonin",
      "retina",
      "age-related macular degeneration",
      "amd",
      "macula"
    ],
    "absent_kp": [
      "aging",
      "fundus disease"
    ]
  },
  {
    "title": "an analysis of the channel utilization for call admission control in voice over wlan.",
    "abstract": "in this paper, we present an analytical method for effectively evaluating the channel utilization of voice calls in voice over wlan systems. the method is under realistic assumptions general enough to deal with various cases of mixed traffic sources and transmission conditions. the accuracy and effectiveness of our method are validated through experiments. the method provided a basis for a prototype call-admission-control system for vowlan, in an attempt to enable an operational vowlan system with quality-of-service support.",
    "present_kp": [
      "voice over wlan",
      "call admission control",
      "channel utilization"
    ],
    "absent_kp": []
  },
  {
    "title": "dynamic weather forecaster: results of the testing of a collaborative, on-line educational platform for weather forecasting.",
    "abstract": "the online dynamic weather forecaster is an open, collaborative application available now to high-school and college instructors across the united states who would like to easily incorporate weather forecasting in their instruction. the application consists of a set of 13 questions that allow students to submit forecasts that cover most of the parameters used by professional weather forecasters. submissions are automatically validated against weather parameters and graded. we tested the impact of the application on the learning of 199 undergraduate students in an introductory meteorology course in spring 2008. students who begin forecasting early in the semester and continue to do so throughout the semester are statistically significantly more successful in the course than students who start late or complete a low number of forecasts. college, year in school, and gender were not significant predictors of success. students found the application easy to use, and 92.3% of them found it at least somewhat helpful as they learned about the weather. through the use of the dwf, students also experience first-hand that uncertainty is a critical part of weather forecasting and of scientific studies in general. with sufficient interest from potential users outside the usa, the dwf platform could easily be expanded to include global weather data.",
    "present_kp": [
      "weather forecasting",
      "education"
    ],
    "absent_kp": [
      "collaborative platform",
      "introductory science learning"
    ]
  },
  {
    "title": "combined neurostimulation and neuroimaging in cognitive neuroscience: past, present, and future.",
    "abstract": "modern neurostimulation approaches in humans provide controlled inputs into the operations of cortical regions, with highly specific behavioral consequences. this enables causal structurefunction inferences, and in combination with neuroimaging, has provided novel insights into the basic mechanisms of action of neurostimulation on distributed networks. for example, more recent work has established the capacity of transcranial magnetic stimulation (tms) to probe causal interregional influences, and their interaction with cognitive state changes. combinations of neurostimulation and neuroimaging now face the challenge of integrating the known physiological effects of neurostimulation with theoretical and biological models of cognition, for example, when theoretical stalemates between opposing cognitive theories need to be resolved. this will be driven by novel developments, including biologically informed computational network analyses for predicting the impact of neurostimulation on brain networks, as well as novel neuroimaging and neurostimulation techniques. such future developments may offer an expanded set of tools with which to investigate structurefunction relationships, and to formulate and reconceptualize testable hypotheses about complex neural network interactions and their causal roles in cognition.",
    "present_kp": [
      "transcranial magnetic stimulation"
    ],
    "absent_kp": [
      "state-dependence",
      "effective connectivity",
      "causal inference",
      "eeg",
      "fmri",
      "mrs",
      "computational neurostimulation"
    ]
  },
  {
    "title": "self-healing reconfigurable logic using autonomous group testing.",
    "abstract": "a group-testing-based fault resolution is incorporated into sram-based reconfigurable field programmable gate arrays (fpgas) to provide an evolvable hardware system with self-healing and self-organizing properties. the proposed approach employs adaptive group testing techniques to autonomously maintain fpga resource viability information as an organic means of transient and permanent fault resolution. reconfigurability of the sram-based fpga is leveraged to locate faulty logic resources which are successively excluded by group testing using alternate device configurations. this simplifies the system architects role to definition of functionality using a high-level hardware description language (hdl) and system-level performance vs. availability operating point. system availability, throughput, and mean time to isolate faults are monitored and maintained using an observercontroller model. the proposed group testing method operates on the output response produced for real-time operational inputs, which eliminates the need for dedicated test vectors. the proposed system was demonstrated using a data encryption standard (des) core on 4-input and 6-input lut-based xilinx fpga models. with a single simulated stuck-at fault, the system identifies a completely validated replacement configuration within a few test stages. results also include approaches for optimizing group size, resource redundancy, and availability. the approach demonstrates a readily-implemented yet robust organic hardware application that features a high degree of autonomous self-control.",
    "present_kp": [
      "group testing",
      "evolvable hardware"
    ],
    "absent_kp": [
      "autonomous systems",
      "reconfigurable architectures",
      "reliable systems",
      "organic computing"
    ]
  },
  {
    "title": "integrating recurrent som with wavelet-based kernel partial least square regressions for financial forecasting.",
    "abstract": "this study implements a novel expert system for financial forecasting. in the first stage, wavelet analysis transforms the input space of raw data to a time-scale feature space suitable for financial forecasting, and then a recurrent self-organizing map (rsom) algorithm is used for partitioning and storing temporal context of the feature space. in the second stage, multiple kernel partial least square regressors (as local models) that best fit partitioned regions are constructed for final forecasting. compared with neural networks, pure svms or traditional garch models, the proposed model performs best. the root-mean-squared forecasting errors are significantly reduced.",
    "present_kp": [
      "recurrent self-organizing map",
      "wavelet analysis"
    ],
    "absent_kp": [
      "kernel method",
      "support vector machine",
      "hybrid model"
    ]
  },
  {
    "title": "spatio-temporal data mining for typhoon image collection.",
    "abstract": "our research aims at discovering useful knowledge from the large collection of satellite images of typhoons using data mining approaches. we first introduce the creation of the typhoon image collection that consists of around 34,000 typhoon images for the northern and southern hemisphere, providing the medium-sized, richly-variational and quality-controlled data collection suitable for spatio-temporal data mining research. next we apply several data mining approaches for this image collection. we start with spatial data mining, where principal component analysis is used for extracting basic components and reducing dimensionality, and it revealed that the major principal components describe latitudinal structures and spiral bands. moreover, clustering procedures give the \"birds-eye-view\" visualization of typhoon cloud patterns. we then turn to temporal data mining, including state transition rules, but we demonstrate that it involves intrinsic difficulty associated with the nonlinear dynamics of the atmosphere, or chaos. finally we briefly introduce our system imet (image mining environment for typhoon analysis and prediction), which is designed for the intelligent and efficient searching and browsing of the typhoon image collection.",
    "present_kp": [
      "spatio-temporal data mining",
      "typhoon image collection",
      "principal component analysis",
      "state transition rules"
    ],
    "absent_kp": [
      "typhoon data mining",
      "self-organizing map"
    ]
  },
  {
    "title": "lagrangian relaxation and enumeration for solving constrained shortest-path problems.",
    "abstract": "the constrained shortest-path problem (cspp) generalizes the standard shortest-path problem by adding one or more path-weight side constraints. we present a new algorithm for cspp that lagrangianizes those constraints, optimizes the resulting lagrangian function, identifies a feasible solution, and then closes any optimality gap by enumerating near-shortest paths, measured with respect to the lagrangianized length. \"near-shortest\" implies epsilon-optimal, with a varying c that equals the current optimality gap. the algorithm exploits a variety of techniques: a new path-enumeration method; aggregated constraints; preprocessing to eliminate edges that cannot form part of an optimal solution; \"reprocessing\" that reapplies preprocessing steps as improved solutions are found; and, when needed, a \"phase-i procedure\" to identify a feasible solution before searching for an optimal one. the new algorithm is often an order of magnitude faster than a state-of-the-art label-setting algorithm on singly constrained randomly generated grid networks. on multiconstrained grid networks, road networks, and networks for aircraft routing the advantage varies but, overall, the new algorithm is competitive with the label-setting algorithm.",
    "present_kp": [
      "near-shortest paths",
      "lagrangian relaxation",
      "label-setting algorithm"
    ],
    "absent_kp": [
      "constrained shortest paths",
      "path enumeration"
    ]
  },
  {
    "title": "adaptive bolus chasing computed tomography angiography: control scheme and experimental results.",
    "abstract": "in this paper, a new adaptive bolus-chasing control scheme is proposed to synchronize the bolus peak in a patient's vascular system and the imaging aperture of a computed tomography (ct) scanner. the proposed control scheme is theoretically evaluated and experimentally tested on a modified siemens somatom volume zoom ct scanner. the first set of experimental results is reported on bolus-chasing ct angiography using realistic bolus dynamics, real-time ct imaging and adaptive table control with physical vasculature phantoms. the data demonstrate that the proposed control approach tracks the bolus propagation well, and clearly outperforms the constant-speed scheme that is the current clinical standard.",
    "present_kp": [
      "adaptive bolus chasing",
      "computed tomography angiography"
    ],
    "absent_kp": []
  },
  {
    "title": "effect of magnetic reynolds number on the two-dimensional hydromagnetic flow around a cylinder.",
    "abstract": "numerical experiments have been conducted to study the effect of magnetic reynolds number on the steady, two-dimensional, viscous, incompressible and electrically conducting flow around a circular cylinder. besides usual reynolds number re, the flow is governed by the magnetic reynolds number r(m), and alfven number beta. the flow and magnetic field are uniform and parallel at large distances from the cylinder. the pressure poisson equation is solved to find the pressure fields in the entire flow region. the effects of the magnetic field and electrical conductivity on the recirculation bubble, drag coefficient, standing vortex and pressure are presented and discussed. for low interaction parameter (n < 1), the suppression of the flow-separation is nearly independent of the conductivity of the fluid, whereas for large interaction parameters, the conductivity of the fluid strongly influences the control of flow-separation.",
    "present_kp": [
      "magnetic reynolds number",
      "flow-separation",
      "drag coefficient"
    ],
    "absent_kp": [
      "flow control",
      "magnetohydrodynamics",
      "full mhd"
    ]
  },
  {
    "title": "semantic-aware multi-tenancy authorization system for cloud architectures.",
    "abstract": "cloud computing is an emerging paradigm to offer on-demand it services to customers. the access control to resources located in the cloud is one of the critical aspects to enable business to shift into the cloud. some recent works provide access control models suitable for the cloud; however there are important shortages that need to be addressed in this field. this work presents a step forward in the state-of-the-art of access control for cloud computing. we describe a high expressive authorization model that enables the management of advanced features such as role-based access control (rbac), hierarchical rbac (hrbac), conditional rbac (crbac) and hierarchical objects (ho). the access control model takes advantage of the logic formalism provided by the semantic web technologies to describe both the underlying infrastructure and the authorization model, as well as the rules employed to protect the access to resources in the cloud. the access control model has been specially designed taking into account the multi-tenancy nature of this kind of environment. moreover, a trust model that allows a fine-grained definition of what information is available for each particular tenant has been described. this enables the establishment of business alliances among cloud tenants resulting in federation and coalition agreements. the proposed model has been validated by means of a proof of concept implementation of the access control system for openstack with promising performance results.",
    "present_kp": [
      "authorization system",
      "cloud computing",
      "multi-tenancy",
      "trust model",
      "semantic web"
    ],
    "absent_kp": []
  },
  {
    "title": "managing a strategic source of innovation: online users.",
    "abstract": "the study provides insights of complementarities between external sourcing of knowledge and necessary internal competences. three types of online innovation tools are identified. disclosure competence, appropriation competence, and integration competence effectively implement online innovation tools. a framework and a case study aligns online innovation tools with the three management competences and relevant practices.",
    "present_kp": [
      "innovation",
      "management competence",
      "online innovation tool"
    ],
    "absent_kp": [
      "implementation"
    ]
  },
  {
    "title": "iterative interstream interference cancellation for mimo hspa plus system.",
    "abstract": "in this paper, we propose an iterative interstream interference cancellation technique for system with frequency selective multiple-input multiple-output (mimo) channel. our method is inspired by the fact that the cancellation of the interstream interference can be regarded as a reduction in the magnitude of the interfering channel. we show that, as iteration goes on, the channel experienced by the equalizer gets close to the single input multiple output (simo) channel and, therefore, the proposed simo-like equalizer achieves improved equalization performance in terms of normalized mean square error. from simulations on downlink communications of 2 x 2 mimo systems in high speed packet access universal mobile telecommunications system standard, we show that the proposed method provides substantial performance gain over the conventional receiver algorithms.",
    "present_kp": [
      "high speed packet access ",
      "multiple-input multiple-output "
    ],
    "absent_kp": [
      "iterative interference cancelation",
      "linear minimum mean square error  equalization"
    ]
  },
  {
    "title": "an improved configuration for radio over fiber transmission with remote local-oscillator delivery by using two dual-mach-zehnder modulators in parallel.",
    "abstract": "a simple configuration for millimeter-wave fiber-wireless transmission, with remote local-oscillator (lo) delivery from the central office, both for the uplink and for the downlink, and a simple, cost-effective, base-station solution is proposed. under the assumption of using commercially available components and a conventional single-mode fiber (with dispersion of 17 ps/nm/km at 1.55 mum), our numerical results show that, with a laser linewidth of 150 mhz, a laser power of 0 dbm and an optical gain of only 6 db, it is possible to transmit, without repeaters, data rates of 622 mbit/s across about 18 km at a bit-error-rate of 10(-9). by increasing the optical gain to 24 db, the link length can be increased to approximately 67 km for a laser linewidth of 75 mhz and to 78 km for a laser linewidth 1 mhz.",
    "present_kp": [
      "radio over fiber",
      "mach-zehnder modulator"
    ],
    "absent_kp": [
      "millimeter-wave communication",
      "optical single-side-band modulation with carrier",
      "remote local oscillator delivery"
    ]
  },
  {
    "title": "modeling dialogue with mixed initiative in design space exploration.",
    "abstract": "exploration with a generative formalism must necessarily account for the nature of interaction between humans and the design space explorer. established accounts of design interaction are made complicated by two propositions in woodbury and burrow's keynote on design space exploration. first, the emphasis on the primacy of the design space as an ordered collection of partial designs (version, alternatives, extensions). few studies exist in the design interaction literature on working with multiple threads simultaneously. second, the need to situate, aid, and amplify human design intentions using computational tools. although specific research and practice tools on amplification (sketching, generation, variation) have had success, there is a lack of generic, flexible, interoperable, and extensible representation to support amplification. this paper addresses the above, working with design threads and computer-assisted design amplification through a theoretical model of dialogue based on grice's model of rational conversation. using the concept of mixed initiative, the paper presents a visual notation for representing dialogue between designer and design space formalism through abstract examples of exploration tasks and dialogue integration.",
    "present_kp": [
      "exploration",
      "generation",
      "mixed initiative"
    ],
    "absent_kp": [
      "design spaces",
      "navigation"
    ]
  },
  {
    "title": "simultaneous topology and sizing optimization of a water distribution network using a hybrid multiobjective evolutionary algorithm.",
    "abstract": "numerical scheme for multiobjective simultaneous topology and sizing design of pipe networks. network repairing technique for dealing with an illegitimate network topology. new hybrid multiobjective evolutionary algorithm. performance comparison based on a hypervolume indicator.",
    "present_kp": [
      "pipe network"
    ],
    "absent_kp": [
      "hybrid differential evolution",
      "population-based incremental learning",
      "topology optimization",
      "water distribution systems"
    ]
  },
  {
    "title": "critical infrastructure protection using secrecy a discrete simultaneous game.",
    "abstract": "present a simultaneous game model for critical infrastructure protection. optimally allocate limited resources to protect critical infrastructures. prove pure-strategy nash equilibrium solution does not exist. describe approach for identifying a mixed-strategy nash equilibrium solution. used ieee rts 96 test system as illustration of approach.",
    "present_kp": [
      "critical infrastructure protection",
      "simultaneous game",
      "secrecy"
    ],
    "absent_kp": [
      "intentional attack",
      "information"
    ]
  },
  {
    "title": "a mixture of hmm, ga, and elman network for load prediction in cloud-oriented data centers.",
    "abstract": "the rapid growth of computational power demand from scientific, business, and web applications has led to the emergence of cloud-oriented data centers. these centers use pay-as-you-go execution environments that scale transparently to the user. load prediction is a significant cost-optimal resource allocation and energy saving approach for a cloud computing environment. traditional linear or nonlinear prediction models that forecast future load directly from historical information appear less effective. load classification before prediction is necessary to improve prediction accuracy. in this paper, a novel approach is proposed to forecast the future load for cloud-oriented data centers. first, a hidden markov model (hmm) based data clustering method is adopted to classify the cloud load. the bayesian information criterion and akaike information criterion are employed to automatically determine the optimal hmm model size and cluster numbers. trained hmms are then used to identify the most appropriate cluster that possesses the maximum likelihood for current load. with the data from this cluster, a genetic algorithm optimized elman network is used to forecast future load. experimental results show that our algorithm outperforms other approaches reported in previous works.",
    "present_kp": [
      "cloud computing",
      "load prediction",
      "hidden markov model",
      "genetic algorithm",
      "elman network"
    ],
    "absent_kp": []
  },
  {
    "title": "automating the analysis of problem-solving activities in learning environments: the co-lab case study.",
    "abstract": "the analysis of problem-solving activities carried out by students in learning settings involves studying the students' actions and assessing the solutions they have created. this analysis constitutes an ideal starting point to support an automatic intervention in the student activity by means of feedback or other means to help students build their own knowledge. in this paper, we present a model-driven framework to facilitate the automation of this problem-solving analysis and of providing feedback. this framework includes a set of authoring tools that enable software developers to specify the analysis process and its intervention mechanisms by means of visual languages. the models specified in this way are computed by the framework in order to create technological support to automate the problem-solving analysis. the use of the framework is illustrated thanks to a case study in the field of system dynamics where problem-solving practices are analysed.",
    "present_kp": [
      "analysis of problem-solving activities",
      "visual languages"
    ],
    "absent_kp": [
      "computer-supported learning environments",
      "model-driven development"
    ]
  },
  {
    "title": "on linear balancing sets.",
    "abstract": "let n be an even positive integer and f be the field gf(2). a word in f(n) is called balanced if its hamming weight is n/2. a subset c subset of f(n) is called a balancing set if for every word y is an element of f(n) there is a word x is an element of c such that y + x is balanced. it is shown that most linear subspaces of fn of dimension slightly larger than 3/2 log(2)n are balancing sets. a generalization of this result to linear subspaces that are \"almost balancing\" is also presented. on the other hand, it is shown that the problem of deciding whether a given set of vectors in f(n) spans a balancing set, is np-hard. an application of linear balancing sets is presented for designing efficient error-correcting coding schemes in which the codewords are balanced.",
    "present_kp": [
      "balancing sets"
    ],
    "absent_kp": [
      "balanced codewords",
      "linear codes",
      "covering of hamming space"
    ]
  },
  {
    "title": "an adaptive audio watermarking based on the singular value decomposition in the wavelet domain.",
    "abstract": "this paper presents a secure, robust, and blind adaptive audio watermarking algorithm based on singular value decomposition (svd) in the discrete wavelet transform domain using synchronization code. in our algorithm, a watermark is embedded by applying a quantization-index-modulation process on the singular values in the svd of the wavelet domain blocks. the watermarked signal is perceptually similar to the original audio signal and gives high quality output. experimental results show that the hidden watermark data is robust to additive noise, resampling, low-pass filtering, requantization, mp3 compression, cropping, echo addition, and denoising. performance analysis of the proposed scheme shows low error probability rates. the data embedding rate of the proposed scheme is 45.9 bps. the proposed scheme has high payload and superior performance against mp3 compression compared to the earlier audio watermarking schemes.",
    "present_kp": [
      "audio watermarking",
      "discrete wavelet transform ",
      "singular value decomposition ",
      "synchronization code"
    ],
    "absent_kp": [
      "quantization index modulation "
    ]
  },
  {
    "title": "modelling the semivariograms and cross-semivariograms required in downscaling cokriging by numerical convolutiondeconvolution.",
    "abstract": "a practical problem of interest in remote sensing is to increase the spatial resolution of a coarse spatial resolution image by fusing the information of that image with another fine spatial resolution image (from the same sensor or from sensors on different satellites). thus, the problem is how to introduce spatial detail into a coarse spatial resolution image (decrease the pixel size) such that it is coherent with the spectral information of the image. cokriging provides a geostatistical solution to the problem and has several interesting advantages: it is a sound statistical method by being unbiased and minimizing a prediction variance (c.f. ad hoc procedures), it takes into account the effect of pixel size, and also autocorrelation in each image as well as the cross-correlation between images, it may be extended to incorporate extra information from other sources and it provides an estimation of the uncertainty of the final predictions. when formulating the cokriging system, semivariograms and cross-semivariograms (or covariances and cross-covariances) appear, some of which cannot be estimated from data directly. cross-variograms between different variables as well as cross-semivariograms between different supports for the same variable are required. the problem is solved by using linear systems theory in which any variable for any pixel size is seen as the output of a linear system when the input is the same variable on a point support. in remote-sensing applications, the linear system is specified by the point-spread function (or impulse response) of the sensor. linear systems theory provides the theoretical relations between the different semivariograms and cross-semivariograms. overall, one must ensure that the whole set of covariances and cross-covariances is positive-definite and models must be estimated for non-observed semivariograms and cross-semivariograms. the models must also be realistic, taking into account, for example, the parabolic behaviour close to the origin presented in regularized semivariograms and cross-semivariograms. the solution proposed is to find by numerical deconvolution a positive-definite set of point covariances and cross-covariances and then any required model may be obtained by numerical convolution of the corresponding point model. the first step implies several numerical deconvolutions where some model parameters are fixed, while others are estimated using the available experimental semivariograms and cross-semivariograms, and some goodness-of-fit measure. the details of the proposed procedure are presented and illustrated with an example from remote sensing.",
    "present_kp": [
      "cokriging",
      "downscaling"
    ],
    "absent_kp": [
      "area-to-point prediction",
      "change of support",
      "image fusion"
    ]
  },
  {
    "title": "bond graph modelling and simulation of multidisciplinary systems - an introduction.",
    "abstract": "this paper introduces a graphical, computer aided modelling methodology that is particularly suited for the concurrent design of multidisciplinary systems, viz. of engineering systems with mechanical, electrical, hydraulic or pneumatic components, including interactions of physical effects from various energy domains. following the introduction, bond graph modelling of multibody systems, as an example of an advanced topic, is briefly addressed in order to demonstrate the potential of this powerful approach to modelling multidisciplinary systems. it is shown how models of multibody systems including flexible bodies can be built in a systematic manner.",
    "present_kp": [
      "bond graph modelling",
      "multidisciplinary systems",
      "multibody systems"
    ],
    "absent_kp": [
      "object-oriented physical systems modelling",
      "computational causality",
      "forms of mathematical models"
    ]
  },
  {
    "title": "wave hierarchies in continua with scalar microstructure in the plane and spherical symmetry.",
    "abstract": "in this paper, we consider the balance equations for a continuum with scalar microstructure and propose a general approach to investigate nonlinear wave propagation by means of an asymptotic approach within the theoretical context of wave hierarchies. the evolution equations that are obtained for the leading terms of the asymptotics allow for the description of various levels of wave motion in agreement with the different scales involved in the modelling of continua with microstructure. both the one-dimensional planar and the axisymmetric spherical cases are considered in a unified way, and various examples (immiscible mixtures of perfect fluids, granular materials, liquid with bubbles) are discussed in order to illustrate the procedure.",
    "present_kp": [
      "continua with microstructure",
      "wave hierarchies",
      "evolution equations"
    ],
    "absent_kp": [
      "progressive waves"
    ]
  },
  {
    "title": "a distributed and interoperable object-oriented support for safe e-commerce transactions.",
    "abstract": "business via internet is becoming popular. a number of organizations doing business in the traditional way are extending themselves to do business over the web. most business-to-business dealings are done through value added networks but for general consumer-to-business dealings, the internet provides a powerful base. however, customer confidence in internet commerce needs to be further strengthened before large scale internet purchasing and selling becomes a reality. while security standards have been established through efficient cryptographic techniques to ensure that network communications are not intercepted, the lack of trust between the endpoint parties involved in an e-commerce transaction remains an obstacle for a smooth completion of the transaction. this is because transacting parties need to trust each other before making any commitment. in this paper, we show how trust can be provided automatically through a network of trust service providers (tsp). the solution we propose allows both customers and merchants to deal with each other confidently through trusted intermediaries which role is to guarantee the goods delivery to the customer and the payment to the merchant for issuing those goods. the transactions are conducted atomically and transparently. that is, a transaction process does not experience any off-line transition delay at the intermediaries and the transacting parties deal virtually directly with each other. this solution requires building a trust web based on a network of tsps, which we implemented in the form of distributed corba objects.",
    "present_kp": [
      "trust",
      "corba"
    ],
    "absent_kp": [
      "electronic-commerce",
      "distributed computing",
      "search",
      "java"
    ]
  },
  {
    "title": "probabilistic multi-task learning for visual saliency estimation in video.",
    "abstract": "in this paper, we present a probabilistic multi-task learning approach for visual saliency estimation in video. in our approach, the problem of visual saliency estimation is modeled by simultaneously considering the stimulus-driven and task-related factors in a probabilistic framework. in this framework, a stimulus-driven component simulates the low-level processes in human vision system using multi-scale wavelet decomposition and unbiased feature competition; while a task-related component simulates the high-level processes to bias the competition of the input features. different from existing approaches, we propose a multi-task learning algorithm to learn the task-related \"stimulus-saliency\" mapping functions for each scene. the algorithm also learns various fusion strategies, which are used to integrate the stimulus-driven and task-related components to obtain the visual saliency. extensive experiments were carried out on two public eye-fixation datasets and one regional saliency dataset. experimental results show that our approach outperforms eight state-of-the-art approaches remarkably.",
    "present_kp": [
      "visual saliency",
      "probabilistic framework",
      "multi-task learning"
    ],
    "absent_kp": [
      "visual search tasks"
    ]
  },
  {
    "title": "a webquest framework to improve the study of deadlock and process synchronization.",
    "abstract": "the impact of the internet on society also affects learning at university. students use not only printed books and their own notes, but also the information available on the net. webquests are learning tools that help the students use the internet, but under the supervision of the lecturer, who has previously selected the most interesting sites to visit. an experience of using webquests with first year computer science students is shown, as well as the good results obtained both in the improvement of examination results and in the positive attitude of the students when using webquests.",
    "present_kp": [
      "webquests"
    ],
    "absent_kp": [
      "e-learning",
      "education technologies"
    ]
  },
  {
    "title": "implementation and evaluation of a statistical framework for nerve conduction study reference range calculation.",
    "abstract": "nerve conduction studies (ncs) play a central role in the clinical evaluation of neuropathies. their clinical utilization depends on reference ranges that define the expected parameter values in disease-free individuals. in this paper, a statistical framework is proposed and described in detail for deriving ncs parameter reference ranges. the bootstrap technique is used to identify demographic and physiologic covariates that influence the ncs measurements. multi-variate linear regression is used to improve the accuracy and effectiveness of ncs interpretation by reducing parameter variance. non-linear mappings are used to transform parameters into a gaussian distribution in order to minimize the influence of outliers. modeling of heteroscedasticity observed in this and other studies leads to more sensible normal limits for several parameters. the proposed reference range method is automated using the matlab programming language. data from a large sample of healthy subjects are used to establish reference ranges for 24 commonly measured ncs parameters. all but three parameters follow gaussian distributions in their respective transformed domains. excluding the distal motor latency difference between median and ulnar nerves, the reduction of the parameter variance as a result of regression in the transform domain is greater than 50% for all f-wave latency parameters and at least 10% for all other ncs parameters. subject age is found to influence normal limits of all but one parameter and height has a statistically significant impact on all but three parameters. these reference range specifications provide clinicians with an alternative to developing their own reference ranges as long as their ncs techniques are consistent with those described in this paper. the proposed method should also be applicable to reference range development for other ncs techniques and physiological measurements.",
    "present_kp": [
      "reference range",
      "normal limit",
      "nerve conduction study"
    ],
    "absent_kp": [
      "statistical method"
    ]
  },
  {
    "title": "triage: a practical solution or admission of failure.",
    "abstract": "an experienced investigator, digital forensic examiner, and academic reflects on the strengths and weaknesses of the use of triage. the author argues that the current practice, while a practical necessity, is a failure of the forensic process and software. it is suggested that triage be re-imagined as a formal process that can be measured for efficiency and efficacy.",
    "present_kp": [
      "software"
    ],
    "absent_kp": [
      "forensic triage",
      "backlog",
      "information security",
      "ediscovery",
      "digital forensic process"
    ]
  },
  {
    "title": "hybrid bfoapso algorithm for automatic generation control of linear and nonlinear interconnected power systems.",
    "abstract": "suitable objective function selection is very important for controller design. an objective function using itae, damping ratio and settling times is proposed. the concept is applied to design an hbfoapso based pi controller for agc system. linear and nonlinear interconnected power system models are considered. simulation results show better performance than pso, bfoa, ga, crazypso and anfis approaches.",
    "present_kp": [
      "automatic generation control "
    ],
    "absent_kp": [
      "multi-area power system",
      "proportional-integral  controller",
      "bacteria foraging optimization algorithm ",
      "particle swarm optimization "
    ]
  },
  {
    "title": "h? state feedback controller design for continuous-time ts fuzzy systems in finite frequency domain.",
    "abstract": "this paper is concerned with finite frequency h? state feedback controller designs for continuous-time takagisugeno (ts) fuzzy systems. a parallel distributed compensation (pdc) controller is designed in finite frequency domain. with the aid of the generalized kalmanyakubovichpopov (gkyp) lemma, the design problem is formulated as solving a set of linear matrix inequalities (lmis). simulation examples illustrate that the finite frequency design approach proposed in this paper receives better results than those of existing full frequency approaches.",
    "present_kp": [
      "ts fuzzy system",
      "finite frequency",
      "h? state feedback controller"
    ],
    "absent_kp": [
      "gkyp lemma"
    ]
  },
  {
    "title": "neurocomputing strategies for solving reliability-robust design optimization problems.",
    "abstract": "purpose - this paper, by taking randomness and uncertainty of structural systems into account aims to implement a combined reliability-based robust design optimization (rrdo) formulation. the random variables to be considered include the cross section dimensions, modulus of elasticity, yield stress, and applied loading. the rrdo problem is to be formulated as a multi-objective optimization problem where the construction cost and the standard deviation of the structural response are the objectives to be minimized. design/methodology/approach - the solution of the optimization problem is performed with the non-dominant cascade evolutionary algorithm with the weighted tchebycheff metric, while the probabilistic analysis required is carried out with the monte carlo simulation method. despite the computational advances, the solution of a rrdo problem for real-world structures is extremely computationally demanding and for this reason neurocomputing estimations are implemented. findings - the obtained estimates with the neural network predictions are shown to be very satisfactory in terms of accuracy for performing this type of computation. furthermore, the present numerical results manage to achieve a reduction in computational time up to four orders of magnitude, for low probabilities of violation, compared to the conventional procedure making thus feasible the reliability-robust design optimization of realistic structures under probabilistic constraints. originality/value - the novel parts of the present work include the implementation of neurocomputing strategies in rrdo problems for reducing the computational cost and the comparison of the results given by rrdo and robust design optimization formulations, where the significance of taking into account probabilistic constraints is emphasized.",
    "present_kp": [],
    "absent_kp": [
      "neural nets",
      "optimization techniques",
      "structural design",
      "structural engineering"
    ]
  },
  {
    "title": "the power of amnesia: learning probabilistic automata with variable memory length.",
    "abstract": "we propose and analyze a distribution learning algorithm for variable memory length markov processes. these processes can be described by a subclass of probabilistic finite automata which we name probabilistic suffix automata (psa). though hardness results are known for learning distributions generated by general probabilistic automata, we prove that the algorithm we present can efficiently learn distributions generated by psas. in particular, we show that for any target psa, the kl-divergence between the distribution generated by the target and the distribution generated by the hypothesis the learning algorithm outputs, can be made small with high confidence in polynomial time and sample complexity. the learning algorithm is motivated by applications in human-machine interaction. here we present two applications of the algorithm. in the first one we apply the algorithm in order to construct a model of the english language, and use this model to correct corrupted text. in the second application we construct a simple stochastic model for e. coli dna.",
    "present_kp": [
      "learning distributions",
      "probabilistic automata"
    ],
    "absent_kp": [
      "markov models",
      "suffix trees",
      "text correction"
    ]
  },
  {
    "title": "organization of invalidation reports for energy-efficient cache invalidation in mobile environments.",
    "abstract": "in a wireless environment, mobile clients often cache frequently accessed data to reduce contention on the limited wireless bandwidth. however, it is difficult for clients to ascertain the validity of their cache content because of their frequent disconnection. one promising cache invalidation approach is the bit-sequences scheme that organizes invalidation reports as a set of binary bit sequences with an associated set of timestamps. the report is periodically broadcast by the server to clients listening to the communication channel. while the approach has been shown to be effective, it is not energy efficient as clients are expected to examine the entire invalidation report. in this paper, we reexamine the bit-sequences method and study different organizations of the invalidation report to facilitate clients to selectively tune to the portion of the report that are of interest to them. this allows the clients to minimize the power consumption when invalidating their cache content. we conducted extensive studies based on a simulation model. our study shows that, compared to the bit-sequences approach, the proposed schemes are not only equally effective in salvaging the cache content but are more efficient in energy utilization.",
    "present_kp": [
      "disconnection",
      "bit-sequences",
      "cache invalidation"
    ],
    "absent_kp": [
      "mobile computing",
      "access time",
      "energy consumption"
    ]
  },
  {
    "title": "prediction of heterogeneous differential genes by detecting outliers to a gaussian tight cluster.",
    "abstract": "heterogeneously and differentially expressed genes (hdeg) are a common phenomenon due to bio-logical diversity. a hdeg is often observed in gene expression experiments (with two experimental conditions) where it is highly expressed in a few experimental samples, or in drug trial experiments for cancer studies with drug resistance heterogeneity among the disease group. these highly expressed samples are called outliers. accurate detection of outliers among hdegs is then desirable for dis- ease diagnosis and effective drug design. the standard approach for detecting hdegs is to choose the appropriate subset of outliers to represent the experimental group. however, existing methods typically overlook hdegs with very few outliers.",
    "present_kp": [
      "cancer",
      "outlier",
      "differentially expressed genes"
    ],
    "absent_kp": [
      "microarray"
    ]
  },
  {
    "title": "the use of cart and multivariate regression trees for supervised and unsupervised feature selection.",
    "abstract": "feature selection is a valuable technique in data analysis for information-preserving data reduction. this paper describes classification and regression trees (cart) and multivariate regression trees (mrt)-based approaches for both supervised and unsupervised feature selection. the well-known cart method allows to perform supervised feature selection by modeling one response variable (y) by some explanatory variables (x). the recently proposed cart extension, mrt can handle more than one response variable (y). this allows to perform a supervised feature selection in the presence of more than one response variable. for unsupervised feature selection, where no response variables are available, we propose auto-associative multivariate regression trees (aamrt) where the original variables (x) are not only used as explanatory variables (x), but also as response variables (y=x). since (aa)mrt is grouping the objects into groups with similar response values by using explanatory variables, this means that the variables are found which are most responsible for the cluster structure in the data. we will demonstrate how these approaches can improve (the detection of) the cluster structure in data and bow they can be used for knowledge discovery.",
    "present_kp": [
      "supervised",
      "unsupervised",
      "feature selection",
      "cart",
      "mrt",
      "aamrt",
      "auto-associative",
      "multivariate regression trees"
    ],
    "absent_kp": [
      "clustering"
    ]
  },
  {
    "title": "investigating business-it alignment through multi-disciplinary goal concepts.",
    "abstract": "the alignment of information technology (it) with business strategies is optimal when harmony exists between organizational and system goals. empirical evidence reveals that effective strategic alignment leads to superior financial performance for organizations. this observation has spurred extensive research into business-it alignment. yet, the issue of alignment remains a top concern for cios. we argue that the parochial view undertaken by past research into business-it alignment is a probable cause for continuing system failures. furthermore, strategic alignment research is limited and devoid within the requirements engineering discipline. in this paper, we highlight existing shortfalls of research in business-it alignment, and bring to light insights that may be offered by other disciplines to augment this field. subscribing to a multi-disciplinary perspective, we develop a goal-based framework that incorporates goals from various literatures in investigating business-it alignment. one of the novelties of our proposed framework lies in its delineation between goals that have been originally assigned to stakeholders versus those interpreted by stakeholders. additionally, the framework includes constructs at the strategic level for supporting the rationale of strategic level goals. we tested the usefulness and usability of this methodology in an organization with a newly developed information system founded on strategic business goals and reported lessons learnt from both researchers and practitioners perspectives.",
    "present_kp": [
      "requirements engineering",
      "business-it alignment"
    ],
    "absent_kp": [
      "goal-based frameworks"
    ]
  },
  {
    "title": "proportional fair scheduling with superposition coding in a cellular cooperative relay system.",
    "abstract": "many works have tackled on the problem of throughput and fairness optimization in cellular cooperative relaying systems. considering firstly a two-user relay broadcast channel, we design a scheme based on superposition coding (sc) which maximizes the achievable sum-rate under a proportional fairness constraint. unlike most relaying schemes where users are allocated orthogonally, our scheme serves the two users simultaneously on the same time-frequency resource unit by superposing their messages into three sc layers. the optimal power allocation parameters of each sc layer are derived by analysis. next, we consider the general multi-user case in a cellular relay system, for which we design resource allocation algorithms based on proportional fair scheduling exploiting the proposed sc-based scheme. numerical results show that the proposed algorithms allowing simultaneous user allocation outperform conventional schedulers based on orthogonal user allocation, both in terms of throughput and proportional fairness. these results indicate promising new directions for the design of future radio resource allocation and scheduling algorithms.",
    "present_kp": [
      "cellular relay system",
      "superposition coding",
      "proportional fair scheduling"
    ],
    "absent_kp": [
      "cooperative communication"
    ]
  },
  {
    "title": "privacy-preserving computation of bayesian networks on vertically partitioned data.",
    "abstract": "traditionally, many data mining techniques have been designed in the centralized model in which all data is collected and available in one central site. however, as more and more activities are carried out using computers and computer networks, the amount of potentially sensitive data stored by business, governments, and other parties increases. different parties often wish to benefit from cooperative use of their data, but privacy regulations and other privacy concerns may prevent the parties from sharing their data. privacy-preserving data mining provides a solution by creating distributed data mining algorithms in which the underlying data need not be revealed. in this paper, we present privacy-preserving protocols for a particular data mining task: learning a bayesian network from a database vertically partitioned among two parties. in this setting, two parties owning confidential databases wish to learn the bayesian network on the combination of their databases without revealing anything else about their data to each other. we present an efficient and privacy-preserving protocol to construct a bayesian network on the parties' joint data.",
    "present_kp": [
      "bayesian networks",
      "privacy-preserving data mining"
    ],
    "absent_kp": [
      "data privacy"
    ]
  },
  {
    "title": "ontology localization.",
    "abstract": "international organizations (e.g., fao, who, etc.) are increasingly expressing the need for multilingual ontologies for different purposes, e.g., ontology-based multilingual machine translation, multilingual information retrieval. however, most of the ontologies built so far have mainly english or another natural language as basis. since multilingual ontology building is a very expensive and time-consuming undertaking, we propose methods for guiding users in the localization of ontologies, and provide tools for supporting the process. the main contributions of this paper are: i) the description of a generic ontology localization activity and a methodology for guiding in the localization of ontologies; ii) the description of a tool built according to the guidelines proposed for an automatic localization of ontologies; and iii) a set of experiments used to evaluate the methodological and technological aspects of the ontology localization activity.",
    "present_kp": [
      "ontology localization",
      "multilingual ontologies"
    ],
    "absent_kp": []
  },
  {
    "title": "supports for transparent object-migration in pdes systems.",
    "abstract": "it is well known that parallel discrete event simulation systems may suffer, in terms of delivered performance, from imbalance of the computational load. in case of conservative synchronization we may experience cpu under-utilization and/or excessive communication overhead. on the other hand, for the optimistic paradigm we may even have rollback thrashing effects, with a consequent reduction of the percentage of productive (ie not rolled back) work carried out. this paper presents the design of a global memory management architecture supporting application-transparent migration of simulation objects whose state is scattered across dynamically allocated memory chunks. our approach is based on a non-intrusive background protocol that provides each instance of the simulation kernel with information on the current mapping of the virtual address space of all the other instances. dynamic memory requests by the application layer are then locally mapped onto virtual-address ranges that maximize the likelihood of being portable onto the address space of a remote kernel instance. in this way, independently of the load-balancing trigger (or policy), we maximize the likelihood that a desirable migration across a specific couple of kernels can actually take place due to compliance of the corresponding source/destination address spaces. we have integrated the global memory manager within the rome optimistic simulator (root-sim), namely a run-time environment based on the optimistic synchronization paradigm which automatically and transparently parallelizes the execution of event-handler-based simulation programs conforming to ansi-c. further, we provide a contribution in the direction of widening load-balancing schemes for optimistic simulation systems by defining migration triggers and selection policies for the objects to be migrated on the basis of memory usage patterns. an experimental assessment of the architecture and of memory-oriented load balancing is also provided.",
    "present_kp": [
      "discrete event simulation",
      "load balancing",
      "memory management",
      "optimistic synchronization"
    ],
    "absent_kp": [
      "parallel simulation",
      "performance optimization"
    ]
  },
  {
    "title": "experiences in building and operating epost, a reliable peer-to-peer application.",
    "abstract": "peer-to-peer (p2p) technology can potentially be used to build highly reliable applications without a single point of failure. however, most of the existing applications, such as file sharing or web caching, have only moderate reliability demands. without a challenging proving ground, it remains unclear whether the full potential of p2p systems can be realized.to provide such a proving ground, we have designed, deployed and operated a p2p-based email system. we chose email because users depend on it for their daily work and therefore place high demands on the availability and reliability of the service, as well as the durability, integrity, authenticity and privacy of their email. our system, epost, has been actively used by a small group of participants for over two years.in this paper, we report the problems and pitfalls we encountered in this process. we were able to address some of them by applying known principles of system design, while others turned out to be novel and fundamental, requiring us to devise new solutions. our findings can be used to guide the design of future reliable p2p systems and provide interesting new directions for future research.",
    "present_kp": [
      "point",
      "applications",
      "design",
      "direct",
      "experience",
      "group",
      "paper",
      "email",
      "failure",
      "participant",
      "place",
      "availability",
      "research",
      "systems",
      "system design",
      "peer-to-peer",
      "users",
      "file sharing",
      "reliability",
      "process",
      "privacy",
      "future"
    ],
    "absent_kp": [
      "electronic mail",
      "authentication",
      "addressing",
      "technologies",
      "operability",
      "web caches",
      "decentralized systems",
      "integrability"
    ]
  },
  {
    "title": "simulation-verification: biting at the state explosion problem.",
    "abstract": "simulation and verification are the two conventional techniques for the analysis of specifications of real-time systems. while simulation is relatively inexpensive in terms of execution time, it only validates the behavior of a system for one particular computation path. on the other hand, verification provides guarantees over the entire set of computation paths of a system. but is, in general. very expensive due to the state-space explosion problem. in this paper, we introduce a new technique: simulation-verification combines the best of both worlds by synthesizing an intermediate analysis method. this method uses simulation to limit the generation of a computation graph to that set of computations consistent with the simulation. this limited computation graph, called a simulation-verification graph, can be one or more orders of magnitude smaller than the full computation graph. a tool, xsvt, is described which implements simulation-verification graphs. three paradigms for using the new technique are proposed. the paper illustrates the application of the proposed technique via an example of a robot controller for a manufacturing assembly line.",
    "present_kp": [
      "real-time systems",
      "specification",
      "verification",
      "simulation"
    ],
    "absent_kp": [
      "formal methods",
      "timing constraints",
      "modechart",
      "requirements analysis"
    ]
  },
  {
    "title": "process flowsheet superstructures: structural multiplicity and redundancy part i: basic gdp and minlp representations.",
    "abstract": "structural multiplicity has a significant effect on the solution of an minlp model for process synthesis problems. the optimization model may also have built-in redundancy that cannot always be directly derived from the multiplicity of the superstructure. a basic gdp representation (bgr) involving logical relations is defined, and can be constructed by applying a standard natural representation of the process. basic minlp representation (bmr) is defined by transforming the logical relations to algebraic ones. minlp representation (mr) is defined through a fixed form of bmr. equivalency and representativeness of mr-s in general form can be analyzed by reducing them to their bmrs. bmr can be automatically generated, and can serve as a reference representation. binary and continuous multiplicity of mr are defined. if the supergraph, i.e. the graph representing the superstructure, is structurally redundant (i.e. there are isomorphic graphs amongst their subgraphs) then bmr has binary multiplicity. conversely, the structural redundancy of the graph does not follow from the binary multiplicity of its bmr. different kinds of multiplicity and redundancy measures of the minlp representation will be defined in part 11 of this series in order to help inventing tools for decreasing their detrimental effect. alternative minlp representations will there be defined, constructed, and compared from the viewpoint of ideality, minimality, and solution properties.",
    "present_kp": [
      "multiplicity",
      "superstructure",
      "redundancy",
      "minlp",
      "representation",
      "process synthesis"
    ],
    "absent_kp": []
  },
  {
    "title": "developing compact course timetables with optimized student flows.",
    "abstract": "a course timetable has an impact on the student flows between consecutive lectures. large flows lead to congestion and large travel times between lectures. therefore, we look at minimizing the travel time between consecutive lectures. flows are modeled in a detailed way, using insights from pedestrian traffic models. using a decomposition approach, our model can be solved with a standard ip solver.",
    "present_kp": [
      "student flow"
    ],
    "absent_kp": [
      "scheduling",
      "timetabling",
      "university course timetabling problem",
      "integer programming"
    ]
  },
  {
    "title": "(mathfrak{sl}(2)) operators and markov processes on branching graphs.",
    "abstract": "we present a unified approach to various examples of markov dynamics on partitions studied by borodin, olshanski, fulman, and the author. our technique generalizes kerovs operators which first appeared in okounkov (random matrix models and their applications, pp.407420, cambridge university press, cambridge, 2001), and also stems from the study of duality of graded graphs in fomin (j. algebr. comb., 3(4):357404, 1994).",
    "present_kp": [
      "markov processes on branching graphs"
    ],
    "absent_kp": [
      "linear operators on graphs",
      "young graph",
      "z-measures on partitions",
      "meixner symmetric functions",
      "ewenspitmans partition structures"
    ]
  },
  {
    "title": "adaptive fuzzy backstepping approach for temperature control of continuous stirred tank reactors.",
    "abstract": "in this paper, an adaptive fuzzy controller, based on backstepping technique, has been proposed for temperature control of a general class of continuous stirred tank reactors (cstrs), using the observability concept, an adaptive fuzzy controller using temperature measurement has been designed. a fuzzy logic system is used to estimate the concentration dependent terms and other unknown system parameters appearing in the control law. it is shown that the closed loop system is asymptotically stable and influences of minimum approximation error and external disturbance on the l(2) norm of the output tracking error can be attenuated arbitrarily. the performance of the proposed controller has been examined for temperature control of an unstable reactor and its effectiveness has been demonstrated through computer simulation.",
    "present_kp": [
      "backstepping technique"
    ],
    "absent_kp": [
      "adaptive control",
      "fuzzy inference systems",
      "process control"
    ]
  },
  {
    "title": "security versus convenience? an experimental study of user misperceptions of wireless internet service quality.",
    "abstract": "this paper demonstrates that consumers make incorrect inferences about security/convenience tradeoff. we find the evidence that consumers tend to infer unobservable security quality from observable convenience and that their inferences are not always correct. in four studies, we examine user perceptions of wireless internet service quality, with an aim to understand consumers' irrational choice of a dominated product over a dominant option. our results indicate that consumers make inference in security from convenience using a zero-sum heuristic and that they believe in improving security in return for losing convenience. in a choice setting, we empirically show that security perception, as well as convenience, influences consumers' product choices, contradicting the common view of existing literature that convenience is the sole driver of consumer choice. our findings show that spontaneous and extensive education of consumers about security makes a modest impact on their inference making.",
    "present_kp": [
      "security/convenience tradeoff",
      "inference making"
    ],
    "absent_kp": [
      "information security",
      "security engineering"
    ]
  },
  {
    "title": "mpeg video encryption algorithms.",
    "abstract": "multimedia data security is important for multimedia commerce. previous cryptography studies have focused on text data. the encryption algorithms developed to secure text data may not be suitable to multimedia applications because of the large data size and real time constraint. for multimedia applications, light weight encryption algorithms are attractive. we present four fast mpeg video encryption algorithms. these algorithms use a secret key to randomly change the sign bits of discrete cosine transform (dct) coefficients and/or the sign bits of motion vectors. the encryption is accomplished by the inverse dct (idct) during the mpeg video decompression processing. these algorithms add a small overhead to mpeg codec. software implementations are fast enough to meet the real time requirement of mpeg video applications. the experimental results show that these algorithms achieve satisfactory results. they can be used to secure video-on-demand, video conferencing, and video email applications.",
    "present_kp": [
      "multimedia data security",
      "mpeg video encryption",
      "mpeg codec"
    ],
    "absent_kp": []
  },
  {
    "title": "a nonlinear shooting method for two-point boundary value problems.",
    "abstract": "we study a new nonlinear shooting method for solving two-point boundary value problems and show numerical experiments with various initial velocity conditions. we discuss and analyze the numerical solutions which are obtained by the shooting method.",
    "present_kp": [
      "shooting method",
      "initial velocity"
    ],
    "absent_kp": [
      "self-adjoint",
      "critical value",
      "generalized newton's method"
    ]
  },
  {
    "title": "field emission properties of zno nanosheets grown on a si substrate.",
    "abstract": "vertical zno nanosheets are synthesized on a si substrate. zno nanosheet-based field emission devices were fabricated. the fe performance of a zno nanosheet device is attributable to the unique surface morphology of the nanosheets.",
    "present_kp": [
      "zno nanosheet",
      "field emission"
    ],
    "absent_kp": []
  },
  {
    "title": "idsolver: a general purpose solver for n th-order integro-differential equations.",
    "abstract": "many mathematical models of complex processes may be posed as integro-differential equations (ide). many numerical methods have been proposed for solving those equations, but most of them are ad hoc thus new equations have to be solved from scratch for translating the ide into the framework of the specific method chosen. furthermore, there is a paucity of general-purpose numerical solvers that free the user from additional tasks. here we present a general-purpose matlab solver that has the above features. we have chosen to use a numerical quadrature algorithm combined with an accurate and efficient ode solverboth within a matlab environmentto construct a routine (idsolver) capable of solving a wide variety of ide of arbitrary order, including the volterra and fredholm ide, variable limits on the integral, and non-linear ide. the solver performs successive relaxation iterations until convergence is achieved. the user has to define a kernel, limits of integration and a forcing function, then launch the routine and get accurate results by tuning in a single tolerance parameter, as described below for several numerical examples. we have found, by solving several numerical examples from the literature, that the method is robust, fast and accurate. program title: idsolver catalogue identifier: aequ_v1_0 program summary url:<url> program obtainable from: cpc program library, queens university, belfast, n. ireland licensing provisions: gnu general public license no. of lines in distributed program, including test data, etc.: 372 no. of bytes in distributed program, including test data, etc.: 3435 distribution format: tar.gz programming language: matlab 2011b. computer: pc, macintosh. operating system: windows, osx, linux. ram: 1gb (1,073,741,824 bytes). classification: 4.3, 4.11. nature of problem: to solve a wide variety of integro-differential equations (ide) of arbitrary order, including the volterra and fredholm ide, variable limits on the integral, and non-linear ide. solution method: an efficient lobatto quadrature, a robust and accurate ivp matlabs solver routine, and a recipe for combining old and new estimates that is equivalent to a successive relaxation method. running time: the solver may take several seconds to execute.",
    "present_kp": [
      "integro-differential equation",
      "successive relaxation method",
      "matlab"
    ],
    "absent_kp": [
      "iterative method"
    ]
  },
  {
    "title": "modelling spatio-temporal movement of tourists using finite markov chains.",
    "abstract": "this paper presents a novel method for modelling the spatio-temporal movements of tourists at the macro-level using markov chains methodology. markov chains are used extensively in modelling random phenomena which results in a sequence of events linked together under the assumption of first-order dependence. in this paper, we utilise markov chains to analyse the outcome and trend of events associated with spatio-temporal movement patterns. a case study was conducted on phillip island, which is situated in the state of victoria, australia, to test whether a stationary discrete absorbing markov chain could be effectively used to model the spatio-temporal movements of tourists. the results obtained showed that this methodology can indeed be effectively used to provide information on tourist movement patterns. one significant outcome of this research is that it will assist park managers in developing better packages for tourists, and will also assist in tracking tourists' movements using simulation based on the model used. all tights reserved.",
    "present_kp": [
      "modelling",
      "finite markov chains",
      "movements",
      "tourists"
    ],
    "absent_kp": [
      "spatial and temporal"
    ]
  },
  {
    "title": "wet paper codes and the dual distance in steganography.",
    "abstract": "in 1998 crandall introduced a method based on coding theory to secretly embed a message in a digital support such as an image. later, in 2005, fridrich et al. improved this method to minimize the distortion introduced by the embedding; a process called wet paper. however, as previously emphasized in the literature, this method can fail during the embedding step. here we find sufficient and necessary conditions to guarantee a successful embedding, by studying the dual distance of a linear code. since these results are essentially of combinatorial nature, they can be generalized to systematic codes, a large family containing all linear codes. we also compute the exact number of embedding solutions and point out the relationship between wet paper codes and orthogonal arrays.",
    "present_kp": [
      "steganography",
      "wet paper code",
      "dual distance"
    ],
    "absent_kp": [
      "error-correcting code"
    ]
  },
  {
    "title": "boundary element method homogenization of the periodic linear elastic fiber composites.",
    "abstract": "the paper presented is devoted to the boundary element method based homogenization of the periodic transversely isotropic linear elastic fiber-reinforced composites. the composite material under consideration has deterministically defined elastic properties while its components are perfectly bonded. to have a good comparison with the fem-based computational techniques used previously, the additional finite element discretization is presented and compared numerically against bem homogenization implementation on the example of engineering glassepoxy composite. the homogenization method proposed has rather general characteristics and, as it is shown, can be easily extended on n-component composites. on the contrary, we can consider and homogenize the heterogeneous media with randomly defined material properties using monte-carlo simulation technique or second order perturbation second probabilistic moment approach.",
    "present_kp": [
      "homogenization",
      "boundary element method"
    ],
    "absent_kp": [
      "effective modules method",
      "finite element method",
      "periodic composite materials",
      "stochastic perturbation approach"
    ]
  },
  {
    "title": "lsm: a layer subdivision method for deformable object matching.",
    "abstract": "a deformation technique is a method to deform any part of, or an entire object, into a desired shape. existing deformation methods take a lot of computational cost to represent smoothness correctly due to the constraints caused by differential coefficients of high degree. thus, it is very difficult to find a general solution. in this paper we propose a lsm (layered subdivision method) that integrates a controlling mechanism, surface deformation, and mesh refinement processing 3d modeling and free-form deformable object matching. the proposed method is considerably more efficient and robust when compared to the existing method of free-form surface, because of the computation of the reference points of deformation edge using geometry of free-form surface. this approach can be applied to automatic inspection of nurbs models and object recognition.",
    "present_kp": [
      "mesh refinement",
      "subdivision"
    ],
    "absent_kp": [
      "free-form deformation",
      "geometric modeling"
    ]
  },
  {
    "title": "minimum phase noise of an lc oscillator: determination of the optimal operating point of the active part.",
    "abstract": "in this paper, we describe an original method for determining the optimal operating point of the active part (transistor) of an lc oscillator leading to the minimum phase noise for given specifications in terms of power consumption, oscillation frequency and for given devices (i.e., transistor and resonator). the key point of the proposed method is based on the use of a proper lc oscillator architecture providing a fixed loaded quality factor for different operating points of the active part within the oscillator. the feedback network of this architecture is made of an lc resonator with coupling transformers. in these conditions, we show that it is possible to easily change the operating point of the amplifier, through the determination of the turns ratio of those transformers, and observe its effect on phase noise without modifying the loaded quality factor of the resonator. the optimal operating point for minimum phase noise is then extracted from nonlinear simulations. once this optimal behavior of the active part is known and by associating the previous lc resonator, a design of an lc oscillator or vco with an optimal phase noise becomes possible. the conclusions of the presented simulation results have been widely used to design and implement a fully integrated lc differential vco on a 0.35 ? m bicmos sige process.",
    "present_kp": [
      "operating point"
    ],
    "absent_kp": [
      "class-c",
      "cyclostationary noise",
      "power added",
      "oscillators"
    ]
  },
  {
    "title": "geographical recommendation method using user's interest model based on map operation and category selection.",
    "abstract": "digital map services and search services for geographical information are widely available on the internet. users can retrieve suitable geographical information using these digital maps through certain map operations and category selections. however, when a map region includes a large amount of information, it can be difficult for users to find specific geographical information. in addition, a user's interest in a region or a category may change. therefore, we propose a method for recommending geographical information to users on the basis of their map operation or category selection history. we develop a model for determining a user's interest and use it to recommend suitable regions and categories.",
    "present_kp": [
      "recommendation",
      "digital map"
    ],
    "absent_kp": [
      "gir",
      "user operation"
    ]
  },
  {
    "title": "evaluation of hierarchical clustering algorithms for document datasets.",
    "abstract": "fast and high-quality document clustering algorithms play an important role in providing intuitive navigation and browsing mechanisms by organizing large amounts of information into a small number of meaningful clusters. in particular, hierarchical clustering solutions provide a view of the data at different levels of granularity, making them ideal for people to visualize and interactively explore large document collections.in this paper we evaluate different partitional and agglomerative approaches for hierarchical clustering. our experimental evaluation showed that partitional algorithms always lead to better clustering solutions than agglomerative algorithms, which suggests that partitional clustering algorithms are well-suited for clustering large document datasets due to not only their relatively low computational requirements, but also comparable or even better clustering performance. we present a new class of clustering algorithms called constrained agglomerative algorithms that combine the features of both partitional and agglomerative algorithms. our experimental results showed that they consistently lead to better hierarchical solutions than agglomerative or partitional algorithms alone.",
    "present_kp": [
      "requirements",
      "quality",
      "collect",
      "computation",
      "performance",
      "paper",
      "navigation",
      "evaluation",
      "role",
      "hierarchic",
      "document clustering",
      "play",
      "partitional clustering",
      "experimental evaluation",
      "hierarchical clustering",
      "data",
      "algorithm",
      "feature",
      "class",
      "cluster"
    ],
    "absent_kp": [
      "browse",
      "informal",
      "exploration",
      "organization",
      "visualization",
      "experimentation",
      "agglomerative clustering",
      "documentation"
    ]
  },
  {
    "title": "artificial intelligence as a discursive practice: the case of embodied software agent systems.",
    "abstract": "in this paper, i explore some of the ways in which artificial intelligence (ai) is mediated discursively. i assume that ai is informed by an ancestral dream to reproduce nature by artificial means. this dream drives the production of cyborg discourse, which hinges on the belief that human nature (especially intelligence) can be reduced to symbol manipulation and hence replicated in a machine. cyborg discourse, i suggest, produces ai systems by rhetorical means; it does not merely describe ai systems or reflect a set of prevailing attitudes about technology. to support this argument, i analyse a set of research articles about an embodied conversational agent called the real estate agent (rea). the articles about rea mobilise a set of rhetorical strategies that systematically downplay the systems artificiality and bolster its humanlike qualities. within the context of the dream of ai to produce humanlike machines, and given our strong bias for human-human interaction, the designers claim to reas humanness in their research articles, as i argue in the final section of this paper, needs little justification.",
    "present_kp": [
      "discourse",
      "rhetoric"
    ],
    "absent_kp": [
      "embodiment",
      "metaphor",
      "software agents"
    ]
  },
  {
    "title": "magad-bfs: a learning method for beta fuzzy systems based on a multi-agent genetic algorithm.",
    "abstract": "this paper proposes a learning method for beta fuzzy systems (bfs) based on a multiagent genetic algorithm. this method, called multi-agent genetic algorithm for the design of bfs has two advantages. first, thanks to genetic algorithms (ga) efficiency, it allows to design a suitable and precise model for bfs. second, it improves the ga convergence by reducing rule complexity thanks to the distributed implementation by multi-agent approach. dynamic agents interact to provide an optimal solution in order to obtain the best bfs reaching the balance interpretability-precision. the performance of the method is tested on a simulated example.",
    "present_kp": [
      "beta fuzzy systems",
      "learning",
      "genetic algorithms"
    ],
    "absent_kp": [
      "multi-agent systems",
      "distributed genetic algorithms"
    ]
  },
  {
    "title": "minimax reference point approach and its application for multiobjective optimisation.",
    "abstract": "in multiobjective optimisation, one of the most common ways of describing the decision makers preferences is to assign targeted values (goals) to conflicting objectives as well as relative weights and priority levels for attaining the goals. in linear and convex decision situations, traditional goal programming provides a pragmatic and flexible manner to cater for the above preferences. in certain real world decision situations, however, multiobjective optimisation problems are non-convex. in this paper, a minimax reference point approach is developed which is capable of handling the above preferences in non-convex cases. the approach is based on ?-norm formulation and can accommodate both preemptive and non-preemptive goal programming. a strongly non-linear multiobjective ship design model is presented and fully examined using the new approach. this simulation study is aimed to illustrate the implementation procedures of the approach and to demonstrate its potential application to general multiobjective optimisation problems.",
    "present_kp": [
      "multiobjective optimisation",
      "reference point approach",
      "goal programming"
    ],
    "absent_kp": [
      "minimax formulation",
      "computer aided design"
    ]
  },
  {
    "title": "border gateway protocol (bgp) and traceroute data workshop report.",
    "abstract": "on monday, 22 august 2011, caida hosted a one-day workshop to discuss scalable measurement and analysis of bgp and traceroute topology data, and practical applications of such data analysis including tracking of macroscopic censorship and filtering activities on the internet. discussion topics included: the surprisingly stability in the number of bgp updates over time; techniques for improving measurement and analysis of inter-domain routing policies; an update on colorado state's bgpm on instrumentation; using bgp data to improve the interpretation of traceroute data, both for real-time diagnostics (e.g., as traceroute) and for large-scale topology mapping; using both bgp and traceroute data to support detection and mapping infrastructure integrity, including different types of of filtering and censorship; and use of bgp data to analyze existing and proposed approaches to securing the interdomain routing system. this report briefly summarizes the presentations and discussions that followed.",
    "present_kp": [
      "routing",
      "topology",
      "data analysis",
      "censorship",
      "filtering"
    ],
    "absent_kp": [
      "internet measurement techniques",
      "validation"
    ]
  },
  {
    "title": "interactive content presentation based on expressed emotion and physiological feedback.",
    "abstract": "in this technical demonstration, we showcase an interactive content presentation (icp) system that integrates media-expressed-emotion-based composition, user-perceived preference feedback, and interactive digital art creation. icp harmonizes the browsing of multimedia contents by presenting them in the form of music videos (photos, blog articles with accompanied music) based on their expressed emotion similarity. icp facilitates content browsing by automatically and dynamically selecting the media to be played next in real time, responding to user's preference feedback measured from physiological signals. in addition, icp enhances the enjoyments of content browsing by incorporating interactive digital art creation. icp achieves these goals by properly integrating recent researches on media-expressed emotion classification,cross-media composition, and physiological signal processing.",
    "present_kp": [
      "cross-media composition",
      "interactive digital art creation",
      "media-expressed emotion",
      "user-perceived preference feedback"
    ],
    "absent_kp": []
  },
  {
    "title": "the effect of speech melody on voice quality.",
    "abstract": "this paper explores whether a speaker's voice quality, defined as the perceived timbre of someone's speech, changes as a function of variation in speech melody. analyses are based on several productions of the vowel 'a', provided with different intonation patterns. it appears that in general fundamental frequency covaries with the 'strength relationship' between the first two harmonics (h1h2). that relationship determines the voice quality to some extent, and is often claimed to reflect open quotient. however, correlating the h1h2 measure to parameters of the lf-model reveals that both the open quotient and the skewness of the glottal pulse have an impact on the lower part of the harmonic spectrum.",
    "present_kp": [
      "voice quality",
      "intonation",
      "h1h2",
      "glottal pulse"
    ],
    "absent_kp": []
  },
  {
    "title": "recognition of communication signal types using genetic algorithm and support vector machines based on the higher order statistics.",
    "abstract": "automatic recognition of communication signal type plays an important role in various applications. most of the existing recognizers can only identify a few types of communication signal. this paper presents a novel intelligent technique that identifies a variety of digital signal types. here, a hierarchical support vector machine based structure is proposed as the multiclass classifier. a proper set of the higher order moments (up to eighth) and higher order cumulants (up to eighth) are proposed as the effective features for recognizing of the digital communication signal. a genetic algorithm is used for selecting the suitable parameters of support vector machines. this idea improves the performance of the recognizer, efficiently. simulation results show that the proposed recognizer has a high success rate for recognition of the different modulations even at very low snrs.",
    "present_kp": [
      "support vector machine",
      "higher order statistics"
    ],
    "absent_kp": [
      "communication signal type recognition",
      "pattern recognition",
      "artificial intelligence",
      "model selection"
    ]
  },
  {
    "title": "joint learning and weighting of visual vocabulary for bag-of-feature based tissue classification.",
    "abstract": "automated classification of tissue types of region of interest (roi) in medical images has been an important application in computer-aided diagnosis (cad). recently, bag-of-feature methods which treat each roi as a set of local features have shown their power in this field. two important issues of bag-of-feature strategy for tissue classification are investigated in this paper: the visual vocabulary learning and weighting, which are always considered independently in traditional methods by neglecting the inner relationship between the visual words and their weights. to overcome this problem, we develop a novel algorithm, joint-vivo, which learns the vocabulary and visual word weights jointly. a unified objective function based on large margin is defined for learning of both visual vocabulary and visual word weights, and optimized alternately in the iterative algorithm. we test our algorithm on three tissue classification tasks: classifying breast tissue density in mammograms, classifying lung tissue in high-resolution computed tomography (hrct) images, and identifying brain tissue type in magnetic resonance imaging (mri). the results show that joint-vivo outperforms the state-of-art methods on tissue classification problems.",
    "present_kp": [
      "tissue classification",
      "visual vocabulary"
    ],
    "absent_kp": [
      "bag-of-features",
      "visual word weighting"
    ]
  },
  {
    "title": "a hybrid dynamic forecast model for analyzing celebrity endorsement effects on consumer attitudes.",
    "abstract": "this study investigates the time-varying effects of celebrity endorsements on consumer purchase attitudes toward promoted products using a novel dynamic hierarchical multi-attribute attitude forecast model. the induced direct and indirect effects via constructs of product attributes and net product value are then incorporated into the proposed conceptual model, which is formulated with a discrete-time nonlinear stochastic system. an empirical study of product categories of sport shoes and note book computers demonstrates the feasibility of the proposed methodology. the analytical results demonstrate the capability of the proposed model to forecast consumer attitudes toward a promoted product and reveal the potential heterogeneity in patterns of attitude changes characterized by product attributes, price, and endorser performance as perceived by consumers. furthermore, we infer that celebrity endorsement can significantly influence consumer purchase attitudes via both direct and indirect effects through product-attribute construct.",
    "present_kp": [
      "dynamic forecast model",
      "nonlinear stochastic system",
      "celebrity endorsement"
    ],
    "absent_kp": [
      "fuzzy analytical hierarchy process"
    ]
  },
  {
    "title": "learning a hierarchical image manifold for web image classification.",
    "abstract": "image classification is an essential task in content-based image retrieval. however, due to the semantic gap between low-level visual features and high-level semantic concepts, and the diversification of web images, the performance of traditional classification approaches is far from users' expectations. in an attempt to reduce the semantic gap and satisfy the urgent requirements for dimensionality reduction, high-quality retrieval results, and batch-based processing, we propose a hierarchical image manifold with novel distance measures for calculation. assuming that the images in an image set describe the same or similar object but have various scenes, we formulate two kinds of manifolds, object manifold and scene manifold, at different levels of semantic granularity. object manifold is developed for object-level classification using an algorithm named extended locally linear embedding (elle) based on intra- and inter-object difference measures. scene manifold is built for scene-level classification using an algorithm named locally linear submanifold extraction (llse) by combining linear perturbation and region growing. experimental results show that our method is effective in improving the performance of classifying web images.",
    "present_kp": [
      "web image classification",
      "image manifold",
      "semantic granularity",
      "distance measure"
    ],
    "absent_kp": [
      "manifold learning"
    ]
  },
  {
    "title": "impact of identifier-locator split mechanism on ddos attacks.",
    "abstract": "the semantic overload of ip address, representing not only the identifiers of nodes but also the locators of nodes, is one of the fundamental reasons for hindering the development of current internet. therefore, the identifier-locator split mapping network which separates the two functions has become one of the federating themes for future internet architecture. however, ddos attacks are still in existence in this network. in this paper, we use the attack traffic to discuss and compare the effects of ddos attacks on the current internet and the identifier-locator split mapping network. the numerical and simulation analyses show that the identifier-locator split mapping network alleviates ddos attacks more effectively compared with the current internet.",
    "present_kp": [
      "identifier-locator split",
      "ddos attacks"
    ],
    "absent_kp": [
      "network security",
      "lisp"
    ]
  },
  {
    "title": "coupling of finite element method with material point method by local multi-mesh contact method.",
    "abstract": "as a lagrangian particle method, the material point method (mpm) has the potential to model extreme deformation of materials, where the traditional finite element method (fem) often encounters mesh distortion and element entanglement which lead to numerical difficulties. however, fem is more accurate and efficient than mpm for problems with small deformation. it is therefore desirable to model the body with extreme deformation by mpm and the body with small deformation by fem, respectively. in this paper, a method to handle the contact interaction between the mpm body and the fem body is proposed, which is implemented on the background grid of mpm. by this method, fem is coupled with mpm and a hexahedral element is incorporated into our 3d explicit mpm code mpm3d. several numerical examples, including plate impact, sphere rolling, perforation of thick plate, and fluidstructure interaction problems, are studied and the numerical results are in good agreement with analytical solution and results available in the literature. the coupling of fem and mpm offers advantages of both fem and mpm.",
    "present_kp": [
      "material point method",
      "finite element method",
      "extreme deformation",
      "contact"
    ],
    "absent_kp": []
  },
  {
    "title": "increasing the crowd's capacity to create: how alternative generation affects the diversity, relevance and effectiveness of generated ads.",
    "abstract": "crowds can generate ideas by searching for new designs. a model for such crowd-based search is proposed consisting of three major forces: the problem domain, the actors, and the process. one particular process that can perform such search is that described by human based genetic algorithms, in which crowds are responsible for creating, modifying, and combining designs. this study looks at one aspect of the process: the alternative generation algorithm. three systems were built that performed greenfield, modification and combination-based alternative generation. these were compared in an experiment involving 2220 participants who played different roles in creating and evaluating advertisements. the results favor the modification system. this suggests for domains like advertising, crowd-based design systems should encourage a series of modifications of initial ideas. for designers of other crowd-based systems in other problem domains, this study suggests that both modification and combination processes should be tested and their ratio of use adjusted according to the results obtained, much as the ratio of mutation and crossover are adjusted in genetic algorithms.",
    "present_kp": [
      "human based genetic algorithms",
      "advertisement",
      "design"
    ],
    "absent_kp": [
      "creativity",
      "crowdsourcing",
      "evolutionary computing"
    ]
  },
  {
    "title": "new cancer targets emerging from studies of the von hippel-lindau tumor suppressor protein.",
    "abstract": "inactivation of the von hippel-lindau tumor suppressor protein (pvhl) causes the most common form of kidney cancer. pvhl is part of a complex that polyubiquitinates the alpha subunit of the heterodimeric transcription factor hif. in the presence of oxygen, hif1? is prolyl hydroxylated by egln1 (also called phd2); this modification recruits pvhl, which then targets hif1? for proteasomal degradation. in hypoxic or pvhl-defective cells, hif1? accumulates, binds to hif1?, and transcriptionally activates genes such as vegf. vegf inhibitors and mtor inhibitors, which indirectly affect hif, are now approved for the treatment of kidney cancer. egln1 is a 2-oxoglutaratedependent dioxygenase; such enzymes can be inhibited with drug-like small molecules and egln1 inhibitors are currently being tested for the treatment of anemia. egln2 (phd1) and egln3 (phd3), which are egln1 paralogs, appear to play hif-independent roles in cell proliferation and apoptosis, respectively, and are garnering interest as potential cancer targets. a number of jmjc-containing proteins, including rbp2 and plu-1, are 2-oxoglutaratedependent dioxygenases that demethylate histones. preclinical data suggest that inhibition of rbp2 or plu-1 would suppress tumor growth.",
    "present_kp": [
      "cancer",
      "tumor suppressor protein",
      "transcription factor"
    ],
    "absent_kp": []
  },
  {
    "title": "maintenance strategies for routing indexes.",
    "abstract": "query processing in large-scale unstructured p2p networks is a crucial part of operating such systems. in order to avoid expensive flooding of the network during query processing so-called routing indexes are used. each peer maintains such an index for its neighbors. it provides a compact representation (data summary) of data accessible via each neighboring peer. an important problem in this context is to keep these data summaries up-to-date without paying high maintenance costs. in this paper, we investigate the problem of maintaining distributed data summaries in p2p-based environments without global knowledge and central instances. based on a classification of update propagation strategies, we discuss several approaches to reduce maintenance costs and present results from an experimental evaluation.",
    "present_kp": [
      "routing indexes",
      "distributed data summaries"
    ],
    "absent_kp": [
      "unstructured p2p systems",
      "index maintenance",
      "peer data management systems"
    ]
  },
  {
    "title": "the economics of natural language interfaces: natural language processing technology as a scarce resource.",
    "abstract": "this paper discusses appropriate application areas for natural language interfaces (nlis) to databases. this requires comparing nlis with competing approaches, including other user-friendly interfaces, and training of users with less user-friendly interfaces. also, since nli technology is still limited, users may need to learn how to use nlis themselves. this suggests that nli popularity may snowball at some point, as users become familiar with nlis. we use a simple prototype nli to illustrate when nlis can achieve flexibility unattainable by simpler interfaces. currently existing commercial nlis and application-specific customization are also discussed.",
    "present_kp": [
      "natural language interfaces"
    ],
    "absent_kp": [
      "database management systems",
      "network externalities",
      "economics of information systems"
    ]
  },
  {
    "title": "improvement in demand-controlled ventilation simulation on multi-purposed facilities under an occupant based ventilation standard.",
    "abstract": "the objective of this paper was to find an effective way of improving demand-controlled ventilation (dcv) simulation performed under an occupant based ventilation standard established in many countries. two attractive dcv approaches, co2dcv and rfiddcv, were applied to dcv simulations for a theoretical public assembly space served by a dedicated outdoor air system (doas) with an enthalpy recovery device. a numerical model for predicting the real-time occupant number, required ventilation amount, co2 and formaldehyde (hcho) concentrations under given conditions was developed using a commercial equation solver program. the current ventilation standard used in korea was applied as a case of occupant based ventilation standards. it was found that the current standard might cause unstable dcv simulation results, especially under co2dcv. this is because the ventilation rate (per person) indicated in the standard is the sum of the outdoor air required to remove or dilute air contaminants generated by both occupants and the buildings themselves, and not a pure function of occupant numbers. finally, it makes dcv control unstable when ventilation flow is regulated only by the number of occupants. in order to solve this problem, the current occupant based ventilation standard was modified as a form of ashrae standard 62.1-2007 showing good applicability to various dcv approaches. it was found that this modification enhances applicability of the current ventilation standard to co2dcv significantly and can maintain acceptable hcho concentrations during the entire time of operation. fan energy reduction can also be expected from dcv operations.",
    "present_kp": [
      "demand-controlled ventilation",
      "dedicated outdoor air system",
      "co2",
      "rfid"
    ],
    "absent_kp": [
      "indoor air quality"
    ]
  },
  {
    "title": "locality and parallelism optimization for dynamic programming algorithm in bioinformatics.",
    "abstract": "dynamic programming has been one of the most efficient approaches to sequence analysis and structure prediction in biology. however, their performance is limited due to the drastic increase in both the number of biological data and variety of the computer architectures. with regard to such predicament, this paper creates excellent algorithms aimed at addressing the challenges of improving memory efficiency and network latency tolerance for nonserial polyadic dynamic programming where the dependences are nonuniform. by relaxing the nonuniform dependences, we proposed a new cache oblivious scheme to enhance its performance on memory hierarchy architectures. moreover we develop and extend a tiling technique to parallelize this nonserial polyadic dynamic programming using an alternate block-cyclic mapping strategy for balancing the computational and memory load, where an analytical parameterized model is formulated to determine the tile volume size that minimizes the total execution time and an algorithmic transformation is used to schedule the tile to overlap communication with computation to further minimize communication overhead on parallel architectures. the numerical experiments were carried out on several high performance computer systems. the new cache-oblivious dynamic programming algorithm achieve 2-10 speedup and the parallel tiling algorithm with communication-computation overlapping shows a desired potential for fine-grained parallel computing on massively parallel computer systems.",
    "present_kp": [
      "tiling",
      "locality",
      "dynamic programming",
      "cache-oblivious",
      "parallelism"
    ],
    "absent_kp": []
  },
  {
    "title": "a rule-based system for automatic decidability and combinability.",
    "abstract": "we present a many-sorted schematic superposition calculus for non-unit theories. the calculus is presented as a rule-based system. the implementation automatically checks termination for some theories of interest. combinability of signature-disjoint theories can also be checked.",
    "present_kp": [
      "superposition"
    ],
    "absent_kp": [
      "decision procedures",
      "schematic saturation"
    ]
  },
  {
    "title": "ldshake support for team-based learning design.",
    "abstract": "existing tooling hardly supports the integral design of learning environments. a learning design tool should support team formation, design exchange and co-creation. the ldshake environment proves to support the design of learning. the ldshake environment appears to provide affordances for devising research ideas.",
    "present_kp": [
      "learning design"
    ],
    "absent_kp": [
      "networked teams",
      "learning and innovation",
      "sharing",
      "co-editing",
      "research design"
    ]
  },
  {
    "title": "graph structured program evolution with automatically defined nodes.",
    "abstract": "currently, various automatic programming techniques have been proposed and applied in various fields. graph structured program evolution (grape) is a recent automatic programming technique with graph structure. this technique can generate complex programs automatically. in this paper, we introduce the concept of automatically defined functions, called automatically defined nodes (adn), in grape. the proposed grape program has a main program and several subprograms. we verified the effectiveness of adn through several program evolution experiments, and report the results of evolution of recursive programs using grape modified with adn.",
    "present_kp": [
      "automatically defined function",
      "automatic programming",
      "recursive program",
      "graph structured program evolution"
    ],
    "absent_kp": [
      "genetic algorithm",
      "graph-based genetic programming",
      "evolutionary algorithm",
      "genetic programming"
    ]
  },
  {
    "title": "self-adaptive simulated binary crossover for real-parameter optimization.",
    "abstract": "simulated binary crossover (sbx) is a real-parameter recombinationoperator which is commonly used in the evolutionary algorithm (ea) literature. the operatorinvolves a parameter which dictates the spread of offspring solutionsvis-a-vis that of the parent solutions. in all applications of sbx sofar, researchers have kept a fixed value throughout a simulation run. in this paper, we suggest a self-adaptive procedure of updating theparameter so as to allow a smooth navigation over the functionlandscape with iteration. some basic principles of classicaloptimization literature are utilized for this purpose. the resultingeas are found to produce remarkable and much better results comparedto the original operator having a fixed value of the parameter. studieson both single and multiple objective optimization problems are madewith success.",
    "present_kp": [
      "simulated binary crossover",
      "real-parameter optimization"
    ],
    "absent_kp": [
      "self-adaptation",
      "recombination operator"
    ]
  },
  {
    "title": "extracting and sharing knowledge from medical texts.",
    "abstract": "in recent years, we have been developing a new framework for acquiring medical knowledge from encyclopedic texts. this framework consists of three major parts. the first part is an extended high-level conceptual language (called hlcl 1.1) for use by knowledge engineers to formalize knowledge texts in an encyclopedia. the other part is an hlcl 1.1 compiler for parsing and analyzing the formalized texts into knowledge models. the third part is a set of domain-specific ontologies for sharing knowledge.",
    "present_kp": [
      "high-level conceptual language"
    ],
    "absent_kp": [
      "encyclopedia of china",
      "knowledge acquisition",
      "knowledge compilation",
      "io-model"
    ]
  },
  {
    "title": "a construction of rational manifold surfaces of arbitrary topology and smoothness from triangular meshes.",
    "abstract": "given a closed triangular mesh, we construct a smooth free-form surface which is described as a collection of rational tensor-product and triangular surface patches. the surface is obtained by a special manifold surface construction, which proceeds by blending together geometry functions for each vertex. the transition functions between the charts, which are associated with the vertices of the mesh, are obtained via subchart parameterization.",
    "present_kp": [
      "manifold surface"
    ],
    "absent_kp": [
      "geometric continuity",
      "smooth free-form rational surface",
      "arbitrary topological genus"
    ]
  },
  {
    "title": "efficient control of epidemics over random networks.",
    "abstract": "motivated by the modeling of the spread of viruses or epidemics with coordination among agents, we introduce a new model generalizing both the basic contact model and the bootstrap percolation. we analyze this percolated threshold model when the underlying network is a random graph with fixed degree distribution. our main results unify many results in the random graphs literature. in particular, we provide a necessary and sufficient condition under which a single node can trigger a large cascade. then we quantify the possible impact of an attacker against a degree based vaccination and an acquaintance vaccination. we define a security metric allowing to compare the different vaccinations. the acquaintance vaccination requires no knowledge of the node degrees or any other global information and is shown to be much more efficient than the uniform vaccination in all cases.",
    "present_kp": [
      "vaccination",
      "epidemics",
      "random graphs"
    ],
    "absent_kp": []
  },
  {
    "title": "providing dos resistance for signature-based broadcast authentication in sensor networks.",
    "abstract": "recent studies have demonstrated that it is feasible to perform public key cryptographic operations on resource-constrained sensor platforms. however, the significant energy consumption introduced by public key operations makes any public key-based protocol an easy target of denial-of-service (dos) attacks. for example, if digital signature schemes such as ecdsa are used directly for broadcast authentication without further protection, an attacker can simply broadcast fake messages and force the receiving nodes to perform a huge number of unnecessary signature verifications, eventually exhausting their battery power. this paper shows how to mitigate such dos attacks when digital signatures are used for broadcast authentication in sensor networks. specifically, this paper first presents two filtering techniques, the group-based filter and the key chain-based filter, to handle the dos attacks against signature verification. both methods can significantly reduce the number of unnecessary signature verifications when a sensor node is under dos attacks. this paper then combines these two filters and proposes a hybrid solution to further improve the performance.",
    "present_kp": [
      "sensor networks",
      "broadcast authentication",
      "dos attacks"
    ],
    "absent_kp": [
      "design",
      "security",
      "algorithms",
      "security"
    ]
  },
  {
    "title": "system structure identification and adaptive control of a seismic isolator test rig.",
    "abstract": "a system structure identification is performed. a non-linear first order model of the seismic isolator test rig is derived. a parameter sensitivity analysis is carried out. a non-linear adaptive control is designed.",
    "present_kp": [
      "seismic isolator"
    ],
    "absent_kp": [
      "mechatronics",
      "shaking table",
      "hydraulic actuator modeling",
      "vibration control",
      "parameter identification"
    ]
  },
  {
    "title": "an agent system for managing uncertainty in the integration of spatio-environmental data.",
    "abstract": "recent applications in environmental systems have necessitated the integration of data from multiple, heterogeneous sources. the integration process involves challenges related to issues of uncertainty and imprecision associated with both the data and the process itself. while the handling of uncertainty in geographical information systems (gis) has been a focal point of research in recent years, the additional challenges of dealing with multiple data sources and types, as well as specific fields of analysis, lead to much more complex situations. in this paper, we present a framework for the use of fuzzy mobile agents to address these additional challenges from the standpoint of large-scale environmental systems.",
    "present_kp": [],
    "absent_kp": [
      "fuzzy databases",
      "information retrieval",
      "large-scale systems",
      "geography"
    ]
  },
  {
    "title": "extending the applicability of the o-ring theory to proteindna complexes.",
    "abstract": "the o-ring theory is a well-established theory for proteinprotein interfaces. lacks confirmation for other protein-based systems. molecular dynamics simulations of several protein-dna complexes. high ?sasa values for hot-spots and fewer water molecules around them. the work confirms the applicability of the o-ring theory to protein-dna interfaces.",
    "present_kp": [
      "dna",
      "sasa",
      "o-ring"
    ],
    "absent_kp": [
      "proteindna interaction",
      "alanine-scanning mutagenesis"
    ]
  },
  {
    "title": "tap expression level in tumor cells defines the nature and processing of mhc class i peptides for recognition by tumor-specific cytotoxic t lymphocytes.",
    "abstract": "we identified that the antigen preprocalcitonin (ppct) is recognized on a human lung carcinoma by a cytotoxic t lymphocyte clone derived from autologous tumor-infiltrating lymphocytes. the antigenic peptide ppct1625 is encoded by the gene calcitonin-related polypeptide alpha (calca), which codes for ct and is overexpressed in several lung carcinomas compared with normal tissues. the ppct peptide is derived from the c-terminal region of the signal peptide and is processed independently of proteasomes and the transporter associated with antigen processing (tap)1/tap2 heterodimeric complexes. instead, processing occurs within the endoplasmic reticulum by a novel mechanism involving signal pepsidase (sp) and signal peptide peptidase (spp). although lung cancer cells bearing the ppct1625 epitope displayed low levels of tap, restoration of tap expression by interferon (ifn)-? treatment or by tap1/tap2 gene transfer inhibited ppct antigen presentation. thus, the ppct1625 human tumor epitope requires low tap expression for efficient presentation. these results indicate that emerging sp-generated peptides represent alternative t cell targets that permit cytotoxic t lymphocytes to destroy tap-impaired tumors, a process that helps to overcome tumor escape from cd8+ t cell immunity. additionally, our data suggest that ppct is a promising candidate for cancer immunotherapy.",
    "present_kp": [
      "lung cancer",
      "cytotoxic t lymphocytes",
      "antigen processing"
    ],
    "absent_kp": [
      "tumor-associated antigens"
    ]
  },
  {
    "title": "an affine transformation invariance approach to cell tracking.",
    "abstract": "accurate and robust methods for automatically tracking rolling leukocytes facilitate inflammation research as leukocyte motion is a primary indicator of inflammatory response in the microvasculature. this paper reports on an affine transformation invariance approach we proposed to track rolling leukocyte in intravital microscopy image sequences. the method is based on the affine transformation invariance property, which enables the accommodation of linear affine transformations (translation, rotation, and/or scaling) of the target, and a particle filter that overcomes the effect of image clutter. in our data set of 50 sequences, we compared the new approach with an active contour tracking method and a monte carlo tracker. with the manual tracking result provided by an operator as the reference, the root mean square errors for the active contour tracking method, the monte carlo tracker and the affine transformation invariance approach were 0.95?m, 0.79?m and 0.74?m, respectively, and the percentage of frames tracked were 72%, 75% and 89%, respectively. the affine transformation invariance approach demonstrated more robust (being able to successfully locate target leukocyte in more frames) and more accurate (lower root mean square error) tracking performance. we also separately studied the ability of the affine transformation invariance approach to track a dark target leukocyte and a bright target leukocyte by using the number of frames tracked as an evaluation measure. dark target leukocyte possesses similar image intensity to the background, making it difficult to be located. in 20 sequences where the target leukocyte was dark, the affine transformation invariance approach tracked more frames in 18 sequences and fewer frames in 2 sequences when compared with the active contour tracking method. in comparison with the monte carlo tracker, the affine invariance method tracked more frames in 9 sequences, the same number of frames in 7 sequences and fewer frames in 4 sequences. in tracking a bright target leukocyte in 30 sequences, the affine transformation invariance approach demonstrated superior performance in 7 sequences and inferior performance in 1 sequence when compared with the active contour tracking method. it outperformed the monte carlo tracker in 15 sequences and underperformed in 1 sequence.",
    "present_kp": [
      "linear affine transformation",
      "cell tracking",
      "particle filter"
    ],
    "absent_kp": [
      "in vivo",
      "leukocyte rolling velocity"
    ]
  },
  {
    "title": "improving routing performance in wireless ad hoc networks using cross-layer interactions.",
    "abstract": "this article presents a combined layer two and three control loop, which allows prediction of link breakage in wireless ad hoc networks. the method monitors the physical layer transmission mode on layer two and exploits the gained knowledge at layer three. the mechanism bases on link adaptation, which is used in ieee 802.11a wlan to select the transmission mode according to the link quality. the process of link adaptation contains information that is useful to predict link stability and link lifetime. after introducing the ieee 802.11a medium access control (mac) and phy layer, we present insight to the ieee 802.11a link adaptation behaviour in multi-hop ad hoc networks. the link adaptation algorithm presented here is derived from auto rate fallback (arf) algorithm. we survey the performance gain of two newly developed route adaptation approaches exploding the prediction results. one approach is early route rearrangement (erra) that starts a route reconstruction procedure before link breakage. hence, an alternative route is available before connectivity is lost. early route update (eru) is a complementing approach that enhances this process, by communications among routing nodes surrounding the breaking link. the delay caused by route reconstruction can be significantly reduced if prediction and either of our new route discovery processes is used.",
    "present_kp": [
      "ad hoc network",
      "link adaptation",
      "erra",
      "eru"
    ],
    "absent_kp": [
      "ad hoc routing",
      "aodv",
      "link breakage prediction",
      "iponair",
      "ieee 802.11 mac"
    ]
  },
  {
    "title": "optimizing data aggregation for cluster-based internet services.",
    "abstract": "large-scale cluster-based internet services often host partitioned datasets to provide incremental scalability. the aggregation of results produced from multiple partitions is a fundamental building block for the delivery of these services. this paper presents the design and implementation of a programming primitive-data aggregation call (dac)-to exploit partition parallelism for cluster-based internet services. a dac request specifies a local processing operator and a global reduction operator, and it aggregates the local processing results from participating nodes through the global reduction operator. applications may allow a dac request to return partial aggregation results as a tradeoff between quality and availability. our architecture design aims at improving interactive responses with sustained throughput for typical cluster environments where platform heterogeneity and software/hardware failures are common. at the cluster level, our load-adaptive reduction tree construction algorithm balances processing and aggregation load across servers while exploiting partition parallelism. inside each node, we employ an event-driven thread pool design that prevents slow nodes from adversely affecting system throughput under highly concurrent workload. we further devise a staged time-out scheme that eagerly prunes slow or unresponsive servers from the reduction tree to meet soft deadlines, we have used the dac primitive to implement several applications: a search engine document retriever, a parallel protein sequence matcher, and an online parallel facial recognizer. our experimental and simulation results validate the effectiveness of the proposed optimization techniques for reducing response time, improving throughput, and gracefully handling server unresponsiveness. we also demonstrate the ease-of-use of the dac primitive and the scalability of our architecture design.",
    "present_kp": [
      "design",
      "response time",
      "throughput"
    ],
    "absent_kp": [
      "algorithms",
      "experimentation",
      "performance",
      "reliability",
      "scalable data aggregation",
      "load-adaptive tree formation",
      "cluster-based network services",
      "fault tolerance"
    ]
  },
  {
    "title": "symplectic partitioned rungekutta methods with minimal phase-lag.",
    "abstract": "symplectic partitioned rungekutta (sprk) methods with minimal phase-lag are derived. specifically two new symplectic methods are constructed of second and third order with fifth phase-lag order. the methods are tested on the numerical integration of hamiltonian problems and the schrdinger equation.",
    "present_kp": [
      "symplectic partitioned rungekutta methods",
      "hamiltonian problems",
      "schrdinger equation",
      "phase-lag"
    ],
    "absent_kp": []
  },
  {
    "title": "an instant semantics acquisition system of live soccer video with application to live event alert and on-the-fly language selection.",
    "abstract": "automatic indexing of recorded sports video is a hard problem, even more so for live sports video, though a lot of advances have been achieved in the recent years. this paper presents a semi-automatic instant semantics generation system of live soccer video and its two applications: live event alert and instant semantics embedding. a key technical challenge of building the proposed system is how to acquire the desired semantics in a very short time lag. to confront this challenge, we develop a three-channel approach to accurately and quickly acquire semantics. the hree channels are (1) developing the tool for inputting gamelog, (2) signal communication for getting commands of referee and director, and (3) video analysis for finding event boundary. the proposed system requires only a very small amount of manual work to operate gamelog acquisition tool using an icon-based interface for entering event log. experimental results show that our system can quickly obtain the semantics of live soccer video, alert live events with a very small latency, and embed semantics into broadcasting video with a short time lag.",
    "present_kp": [
      "instant semantics acquisition",
      "sports video",
      "gamelog",
      "on-the-fly language selection",
      "live event alert"
    ],
    "absent_kp": [
      "icon-based input"
    ]
  },
  {
    "title": "impacts of handset bundling on mobile data usage: the case of finland.",
    "abstract": "promotion of the adoption of new services has emerged as a possible driver for the regulation of handset bundling and subsidies. handset bundling, however, has complex implications not only on mobile data service adoption, the focus of this research, but also more broadly on the mobile market dynamics. due to the complexity, regulators have difficulties in anticipating the possible resulting impacts. using a case study, expert interviews, and usage measurements as research method, an empirical framework was constructed to make the service usage impacts more explicit. the framework enables the identification of the regulator's steering options and their qualitative impacts. results are based on observations before and after the change of law on handset bundling in the finnish market. according to the findings, handset bundling regulation is a possible but risky tool for steering the market.",
    "present_kp": [
      "handset bundling",
      "service adoption",
      "regulation"
    ],
    "absent_kp": [
      "subsidy",
      "competition"
    ]
  },
  {
    "title": "characterization on simultaneous approximation for left gamma quasi-interpolants in l-p spaces.",
    "abstract": "m.w.muller gave the gamma quasi-interpolants and obtained approximation equivalence theorems with w(phi)(2r)(f, t)(p). in this paper we obtain characterizations on simultaneous approximation for left gamma quasi-interpolants with w(phi)(2r)(f, t)(p).",
    "present_kp": [
      "quasi-interpolants",
      "simultaneous approximation",
      "characterization"
    ],
    "absent_kp": [
      "gamma operator",
      "modulus of smoothness"
    ]
  },
  {
    "title": "correctness of gossip-based membership under message loss.",
    "abstract": "due to their simplicity and effectiveness, gossip-based membership protocols have become the method of choice for maintaining partial membership in large peer-to-peer systems. a variety of gossip-based membership protocols were proposed. some were shown to be effective empirically, lacking analytic understanding of their properties. others were analyzed under simplifying assumptions, such as lossless and delayless network. it is not clear whether the analysis results hold in dynamic networks, where both nodes and network links can fail. in this paper we try to bridge this gap. we first enumerate the desirable properties of a gossip-based membership protocol, such as view uniformity, independence, and load balance. we then propose a simple send & forget protocol, and show that even in the presence of message loss, it achieves the desirable properties.",
    "present_kp": [
      "peer-to-peer",
      "membership",
      "gossip"
    ],
    "absent_kp": [
      "random sampling"
    ]
  },
  {
    "title": "identifying the global core-periphery structure of science.",
    "abstract": "while there is a consensus that there is a core-periphery structure in the global scientific enterprise, there have not been many methodologies developed for identifying this structure. this paper develops a methodology by looking at the differences in the power law structure of article outputs and degree centrality distributions of countries. this methodology is applied to five different scientific fields: astronomy and astrophysics, energy and fuels, nanotechnology and nanosciences, nutrition, and oceanography. this methodology uncovers a two-tiered power law structure that exists in all examined fields. the core-periphery structure that is unique to each field is characterized by the cores size, minimum degree, and exponent of its power law distribution. stark differences are identified between technology and non-technology intensive scientific fields.",
    "present_kp": [
      "core-periphery structure"
    ],
    "absent_kp": [
      "power law analysis",
      "network centrality",
      "global science"
    ]
  },
  {
    "title": "structure-activity study of thiazides by magnetic resonance methods (nqr, nmr, epr) and dft calculations.",
    "abstract": "the paper presents a comprehensive analysis of the relationship between the electronic structure of thiazides and their biological activity. the compounds of interest were studied in solid state by the resonance methods nuclear quadrupole resonance (nqr), nuclear magnetic resonance (nmr) and electron paramagnetic resonance (epr) and quantum chemistry (ab inito and dft) methods. detailed parallel analysis of the spectroscopic parameters such as quadrupole coupling constant (qcc) nqr chemical shift (?), chemical shift anisotropy (csa), asymmetry parameter (?), nmr and hyperfine coupling constant (a), epr was performed and the electronic effects (polarisation and delocalisation) were revealed and compared. biological activity of thiazides has been found to depend on many factors, but mainly on the physico-chemical properties whose assessment was possible on the basis of electron density determination in the molecules performed by experimental and theoretical methods.",
    "present_kp": [
      "structure-activity",
      "thiazides",
      "nmr",
      "nqr",
      "epr",
      "dft"
    ],
    "absent_kp": []
  },
  {
    "title": "dictionary learning with incoherence and sparsity constraints for sparse representation of nonnegative signals.",
    "abstract": "this paper presents a method for learning an overcomplete, nonnegative dictionary and for obtaining the corresponding coefficients so that a group of nonnegative signals can be sparsely represented by them. this is accomplished by posing the learning as a problem of nonnegative matrix factorization (nmf) with maximization of the incoherence of the dictionary and of the sparsity of coefficients. by incorporating a dictionary-incoherence penalty and a sparsity penalty in the nmf formulation and then adopting a hierarchically alternating optimization strategy, we show that the problem can be cast as two sequential optimal problems of quadratic functions. each optimal problem can be solved explicitly so that the whole problem can be efficiently solved, which leads to the proposed algorithm, i.e., sparse hierarchical alternating least squares (shals). the shals algorithm is structured by iteratively solving the two optimal problems, corresponding to the learning process of the dictionary and to the estimating process of the coefficients for reconstructing the signals. numerical experiments demonstrate that the new algorithm performs better than the nonnegative k-svd (nn-ksvd) algorithm and several other famous algorithms, and its computational cost is remarkably lower than the compared algorithms.",
    "present_kp": [
      "dictionary learning",
      "sparse representation",
      "nonnegative matrix factorization ",
      "hierarchical alternating least squares ",
      "quadratic function"
    ],
    "absent_kp": [
      "overcomplete dictionary"
    ]
  },
  {
    "title": "multi-agent learning in recommender systems for information filtering on the internet.",
    "abstract": "recommender systems (rs), allow users to share information about items they like or dislike and obtain, in a timely fashion, recommendations based on predictions about unseen items (physical or information goods and/or services). in this process, users' preferences are considered to be the learning target functions. we study agent-based recommender systems (ars) under the scope of online learning in multi-agent systems (mas). this approach models the problem as a pool of independent cooperative predictor agents, one per each user (the masters) in the system, in situations in which each agent (the learners) faces a sequence of trials, with a prediction to make in every step, eventually getting the correct value from its master. each learner is willing to discover the degree of similarity among the target function of its master and those of other agents' masters (i.e. preference similarity). the agent uses this information for the calculation of its own prediction task, the goal being to make as few mistakes as possible. a simple, yet effective method is introduced in order to construct a compound algorithm for each agent by combining memory-based individual prediction and online weighted-majority voting. we give a theoretical mistake bound for this algorithm that is closely related to the total loss of the best predictor agent in the pool. finally, we conduct some experiments obtaining results that empirically support these ideas and theories.",
    "present_kp": [
      "information filtering",
      "recommender systems",
      "online learning",
      "mas"
    ],
    "absent_kp": []
  },
  {
    "title": "a comparative study of high resolution schemes for solving population balances in crystallization.",
    "abstract": "this article demonstrates the applicability and usefulness of high resolution finite volume schemes for the solution of population balance equations (pbes) in crystallization processes. the population balance equation is considered to be a statement of continuity. it tracks the change in particle size distribution as particles are born, die, grow or leave a given control volume. in the population balance models, the one independent variable represents the time, the other(s) are \"property coordinate(s)\", e.g. the particle size in the present case. they typically describe the temporal evolution of the number density functions and have been used to model various processes. these include crystallization, polymerization, emulsion and cell dynamics. the high resolution schemes were originally developed for compressible fluid dynamics. the schemes resolve sharp peaks and shock discontinuities on coarse girds, as well as avoid numerical diffusion and numerical dispersion. the schemes are derived for general purposes and can be applied to any hyperbolic model. here, we test the schemes on the one-dimensional population balance models with nucleation and growth. the article mainly concentrates on the re-derivation of a high resolution scheme of koren which is then compared with other high resolution finite volume schemes. the numerical test cases reported in this paper show clear advantages of high resolutions schemes for the solution of population balances.",
    "present_kp": [
      "population balance models",
      "high resolution schemes",
      "crystallization",
      "nucleation and growth"
    ],
    "absent_kp": [
      "distributed parameter systems",
      "hyperbolic conservation laws"
    ]
  },
  {
    "title": "image compression for fast wavelet-based subregion retrieval.",
    "abstract": "in an image browsing environment there is need for progressively viewing image subregions at various resolutions. we describe a storage scheme that accomplishes good image compression, while supporting fast image subregion retrieval. we evaluate analytically and experimentally the compression performance of our algorithm. we also provide results on the speed of the algorithm to demonstrate its effectiveness, and present an extension to a client/server environment.",
    "present_kp": [
      "image compression",
      "wavelet",
      "subregion retrieval"
    ],
    "absent_kp": [
      "digital library",
      "quadtrees",
      "huffman code"
    ]
  },
  {
    "title": "on the potential advantages of exploiting behavioural information for contract-based service discovery and composition.",
    "abstract": "the importance of service contracts providing a suitably synthetic description of software services is widely accepted. while different types of information - ranging from extra-functional properties to ontological annotations to behavioural descriptions - have been proposed to be included in service contracts, no widely accepted de facto standard has yet emerged for describing service contracts, except for signature information. the lack of a de facto standard is inhibiting large scale deployment of techniques and tools supporting enhanced discovery and composition of services. in this paper we discuss the potentially huge advantages of exploiting behavioural information for service discovery and composition, and relate them to the cost of generating such information and to the needed trade-off between expressiveness and cost and value of analysing such information. on such ground, we also discuss the potential suitability of some well-known modelling approaches to become the de facto standard to represent service behaviour in contracts, also in view of contextual factors (such as required know-how and current employment).",
    "present_kp": [],
    "absent_kp": [
      "service-oriented computing",
      "service descriptions",
      "behaviour information"
    ]
  },
  {
    "title": "intelligent pervasive middleware for context-based and localized telematics services.",
    "abstract": "telematics is arguably the next-wave in mobile computing: with most cars already equipped with multiple embedded computing platforms, we shall witness the development of a variety of mobile services and applications with significant commercial potential. telematics will only become a commercial reality when the underlying architecture is able to address significant concerns related to the security and privacy of telematics data, and is able to provide context information from and to a large number of mobile data sources in a scalable and device-independent manner. a telematics platform should utilize existing internet components and technologies but cannot rely exclusively on these, especially since mobile commerce applications in the telematics environment impose specific requirements on the relationships between various services and data providers. in this paper we describe how we are developing an open standards telematics platform based on the ts-pwlan wireless service environment and the telematics resource manager middleware. our design employs existing web service interfaces coupled with novel technology for connecting to these through a wireless gateway. our middleware acts as a common substrate for building and deploying a wide range of telematics applications. we describe how several of these applications are currently being built on our infrastructure.",
    "present_kp": [
      "telematics"
    ],
    "absent_kp": [
      "pervasive computing"
    ]
  },
  {
    "title": "bootstrap for neural model selection.",
    "abstract": "bootstrap techniques (also called resampling computation techniques) have introduced new advances in modeling and model evaluation (principles of neural model identification, selection and adequacy, springer, new york, 1999). using resampling methods to construct a series of new samples which are based on the original data set, allows to estimate the stability of the parameters. properties such as convergence and asymptotic normality can be checked for any particular observed data set. in most cases, the statistics computed on the generated data sets give a good idea of the confidence regions of the estimates. in this paper, we debate on the contribution of such methods for model selection, in the case of feedforward neural networks. the method is described and compared with the leave-one-out resampling method. the effectiveness of the bootstrap method, versus the leave-one-out method, is checked through a number of examples.",
    "present_kp": [
      "bootstrap",
      "model selection"
    ],
    "absent_kp": [
      "multilayer perceptron"
    ]
  },
  {
    "title": "a new perspective for neural networks: application to a marketing management problem.",
    "abstract": "over the last few years, connectionism or neural networks (nn) have successfully been applied to a wide range of areas and have demonstrated their capabilities in solving complex problems. current indications show that these techniques are very important and rapidly developing areas of research and applications, particularly, in the area of data mining for knowledge discovery. one particular neural network model, the back-propagation (bp) algorithm, has performed very well in this regard and it is now accepted as a reliable method for data mining. however, these models have their shortcomings. the major difficulty lies in the fact that the relationships between specific variables and the neural network results are, at best, difficult to explain. this article presents an innovative but simple method for using nn to understand the pattern/outcome correlation to interpret a cause and effect relationship. a comparative analysis and experimental results are also presented to show the validity of the proposed scheme.",
    "present_kp": [
      "neural networks",
      "data mining"
    ],
    "absent_kp": [
      "sensitivity analysis",
      "cart",
      "logistic regression"
    ]
  },
  {
    "title": "fixtag: an algorithm for identifying and tagging fixations to simplify the analysis of data collected by portable eye trackers.",
    "abstract": "video-based eye trackers produce an output video showing where a subject is looking, the subject's point-of-regard (por), for each frame of a video of the scene. this information can be extremely valuable, but its analysis can be overwhelming. analysis of eye-tracked data from portable (wearable) eye trackers is especially daunting, as the scene video may be constantly changing, rendering automatic analysis more difficult. a common way to begin analysis of por data is to group these data into fixations. in a previous article, we compared the fixations identified (i.e., start and end marked) automatically by an algorithm to those identified manually by users (i.e., manual coders). here, we extend this automatic identification of fixations to tagging each fixation to a region-of-interest (roi). our fixation tagging algorithm, fixtag, requires the relative 3d positions of the vertices of rois and calibration of the scene camera. fixation tagging is performed by first calculating the camera projection matrices for keyframes of the scene video (captured by the eye tracker) via an iterative structure and motion recovery algorithm. these matrices are then used to project 3d roi vertices into the keyframes. a por for each fixation is matched to a point in the closest keyframe, which is then checked against the 2d projected roi vertices for tagging. our fixation tags were compared to those produced by three manual coders tagging the automatically identified fixations for two different scenarios. for each scenario, eight rois were defined along with the 3d positions of eight calibration points. therefore, 17 tags were available for each fixation: 8 for rois, 8 for calibration points, and 1 for \"other.\" for the first scenario, a subject was tracked looking through products on four store shelves, resulting in 182 automatically identified fixations. our automatic tagging algorithm produced tags that matched those produced by at least one manual coder for 181 out of the 182 fixations (99.5% agreement). for the second scenario, a subject was tracked looking at two posters on adjoining walls of a room. our algorithm matched at least one manual coder's tag for 169 fixations out of 172 automatically identified (98.3% agreement).",
    "present_kp": [
      "fixations",
      "portable",
      "wearable"
    ],
    "absent_kp": [
      "algorithms",
      "eye tracking",
      "coding"
    ]
  },
  {
    "title": "an iterative linearized optimization technique for non-linear ill-posed problems applied to cardiac activation time imaging.",
    "abstract": "a promising approach for the solution of the electrocardiographic inverse problem is the calculation of the cardiac activation sequence from body surface potential (bsp) mapping data. here, a two-fold regularization scheme is applied in order to stabilize the inverse solution of this intrinsically ill-posed problem. the solution of the inverse problem is defined by the minimum of a non-linear cost function. the l-curve method can be applied for regularization parameter determination. solving the optimization problem by a newton-like method, the l-curve may be of pronged shape. then a numerically unique determination of the optimal regularization parameter will become difficult. this problem can be avoided applying an iterative linearized algorithm. it is shown that activation time imaging due to temporal and spatial regularization is stable with respect to large model errors. even neglecting cardiac anisotropy in activation time imaging results in an acceptable inverse solution.",
    "present_kp": [],
    "absent_kp": [
      "inverse problems",
      "medical imaging"
    ]
  },
  {
    "title": "a learning algorithm for fault tolerant feedforward neural networks.",
    "abstract": "a new learning algorithm is proposed to enhance fault tolerance ability of the feedforward neural networks. the algorithm focuses on;he links (weights) that may cause errors at the output when they are open faults. the relevances of the synaptic weights to the output error (i.e. the sensitivity of the output error to the weight fault) are estimated in each training cycle of the standard backpropagation using the taylor expansion of the output around fault-free weights. then the weight giving the maximum relevance is decreased. the approach taken by the algorithm described in this paper is to prevent the weights from having large relevances. the simulation results indicate that the network trained with the proposed algorithm do have significantly better fault tolerance than the network trained with the standard backpropagation algorithm. the simulation results show that the fault tolerance and the generalization abilities are improved.",
    "present_kp": [
      "feedforward neural network",
      "learning algorithm",
      "open faults"
    ],
    "absent_kp": [
      "relevance of synaptic weights",
      "essential link"
    ]
  },
  {
    "title": "an information system for drug prescription and distribution in a public hospital.",
    "abstract": "with 773 beds and 3696 employees, the hospital das cl nicas da faculdade de medicina de ribeiro preto (hcfmrp)-universidade de so paulo, is one of the largest medical institutions in latin america. the complete process of prescribing and distributing medication at hcfmrp involves the following stages: prescribing (physicians); ordering (infirmary); separating and dispensing (main drugstore); verifying and administering (infirmary). this was a manual process, normally taking place in the morning. bottlenecks were inevitable and the risk of errors was elevated. an information system (is) was devised and implemented with a view to controlling such problems. this article addresses the construction of this system and reports on a survey in which different groups of users have evaluated the project.",
    "present_kp": [],
    "absent_kp": [
      "hospital administration",
      "information systems",
      "materials administration",
      "medication storage",
      "electronic prescription"
    ]
  },
  {
    "title": "investigating prefix propagation through active bgp probing.",
    "abstract": "to devise effective network engineering strategies and to assess the quality of upstream providers, network operators would greatly benefit from the knowledge of which internet paths might be traversed by the traffic flows entering their networks in the case of network faults or when traffic engineering measures are used. however, current methodologies do not provide this information. this paper presents methodologies to discover alternate paths that might be selected in the presence of network faults or different routing policies and to deduce the routing policies of other operators. the techniques are validated through extensive experimentation on the internet.",
    "present_kp": [],
    "absent_kp": [
      "networking",
      "internet probing",
      "interdomain routing",
      "border gateway protocol ",
      "internet service provider "
    ]
  },
  {
    "title": "timeliness-accuracy balanced collection of dynamic context data.",
    "abstract": "in the future, we are likely to see a tremendous need for context-aware applications which adapt to available context information such as physical surroundings, network, or system conditions. we aim to provide a fundamental support for these applications - a real-time context information collection service. this service delivers the right context information to the right user at the right time. the complexity of providing the real-time context information service arises from 1) the dynamically changing status of information sources, 2) the diverse user requirements in terms of data accuracy and service latency, and 3) constantly changing system conditions. in this paper, we take into consideration these dynamics and focus on addressing the trade-offs between timeliness, accuracy, and cost for information collection in distributed real-time environments. we propose a middleware-based approach to enable a judicious composition of services for accuracy-aware scheduling and cost-aware database maintenance. specifically, we characterize the problem in terms of quality-of-service satisfaction (qossat), quality-of-data satisfaction (qodsat), and cost. we propose a middleware framework for the real-time information collection process, where the information mediator coordinates and facilitates communication between information sources and consumers. we design a family of algorithms for real-time request scheduling, request servicing, and directory service maintenance to be implemented at the mediator to support qossat and qodsat. our studies indicate that the composition of our proposed scheduling algorithm and directory service maintenance policy can improve the overall efficiency of the system. we also observe that the proposed policies perform very well as the system scales in the number of information sources and consumer requests.",
    "present_kp": [
      "real-time"
    ],
    "absent_kp": [
      "distributed systems",
      "context-sensitive middleware",
      "quality of service",
      "quality of data"
    ]
  },
  {
    "title": "module placement for fault-tolerant microfluidics-based biochips.",
    "abstract": "microfluidics-based biochips are soon expected to revolutionize clinical diagnosis, dna sequencing, and other laboratory procedures involving molecular biology. most microfluidic biochips today are based on the principle of continuous fluid flow and they rely on permanently etched microchannels, micropumps, and microvalves. we focus here on the automated design of \"digital\" droplet-based microfluidic biochips. in contrast to conventional continuous-flow systems, digital microfluidics offers dynamic reconfigurability; groups of cells in a microfluidics array can be reconfigured to change their functionality during the concurrent execution of a set of bioassays. we present a simulated annealing-based technique for module placement in such biochips. the placement procedure not only addresses chip area, but also considers fault tolerance, which allows a microfluidic module to be relocated elsewhere in the system when a single cell is detected to be faulty. simulation results are presented for case studies involving the polymerase chain reaction and multiplexed in vitro clinical diagnostics.",
    "present_kp": [
      "design",
      "module placement",
      "microfluidics",
      "biochips"
    ],
    "absent_kp": [
      "algorithms",
      "performance",
      "reliability",
      "physical design automation"
    ]
  },
  {
    "title": "on discretization errors and subgrid scale model implementations in large eddy simulations.",
    "abstract": "we analyze the impact of discretization errors on the performance of the smagorinsky model in large eddy simulations (les). to avoid difficulties related to solid boundaries, we focus on decaying homogeneous turbulence. it is shown that two numerical implementations of the model in the same finite volume code lead to significantly different results in terms of kinetic energy decay, time evolutions of the viscous dissipation and kinetic energy spectra. in comparison with spectral les results, excellent predictions are however obtained with a novel formulation of the model derived from the discrete navierstokes equations. we also highlight the effect of discretization errors on the measurement of physical quantities that involve scales close to the grid resolution.",
    "present_kp": [
      "large eddy simulation",
      "smagorinsky model"
    ],
    "absent_kp": [
      "finite volume method",
      "homogeneous isotropic turbulence"
    ]
  },
  {
    "title": "fast instruction cache modeling for approximate timed hw/sw co-simulation.",
    "abstract": "approximate timed co-simulation has been proposed as a fast solution for system modeling in early design steps. this co-simulation technique enables the simulation of systems at speeds close to functional execution, while considering timing effects. as a consequence, system performance estimations can be obtained early, enabling efficient design space exploration and system refinement. to achieve fast simulation speeds, first the sw code is pre-annotated with time information and then it is natively executed, performing what is called native-based co-simulation. to obtain sufficiently accurate performance estimations, the effect of the system components must be considered. among them, processor caches are really important, as they have a strong impact on the overall system performance. however, no efficient techniques for cache modeling in native-based co-simulation have been proposed. previous works considering caches apply slow cache models based on tag search, similar to iss-based models. this solution slows down the simulation speed, greatly reducing the efficiency of native based co-simulations. in this paper, a high-level instruction cache model is proposed, along with the required instrumentation for native simulation. this model allows the designer to obtain cache hit/miss rate estimations with simulation speeds very close to native execution. results present a speed-up of two orders of magnitude with respect to iss and one order of magnitude regarding previous approaches in native simulation. miss rate estimation error remains below 5%.",
    "present_kp": [
      "performance estimation",
      "cache modeling"
    ],
    "absent_kp": [
      "electronic system level"
    ]
  },
  {
    "title": "guided latent space regression for human motion generation.",
    "abstract": "in the present work, we describe a mathematical model to generate human-like motion trajectories in space. we use linear regression in a latent space to find the model parameters from a set of demonstration examples. the learning procedure requires a relevant set of similar examples. the apprehended models encode both the typical shapes of motion and their variability towards specific boundary conditions (bc). we will show the added value of encoding both properties in a unique model and we apply this ability to common problems of error compensation and target tracking. the models allow us to describe human motion using expansion-function series (efs), thus avoiding typical stability issues that arise in the use of differential equation models. to cope with variable scenarios, we show two specific algorithms that morph and adapt the evolution trajectory. in analogy to splines, the efs preserve an analytical structure on which we develop the optimisation steps. in such a way, we managed to combine multiple single segments into complex motions that preserve continuity and may simultaneously optimise other criteria. in the present work, after having analysed similar tools, we present the basic model and its features. then we develop a robust tool to gather the model from examples, and to achieve real-time trajectory adaptation. the achieved results will be analysed through an experimental analysis on data collected in a ball catching experiment.",
    "present_kp": [
      "trajectory adaptation"
    ],
    "absent_kp": [
      "motion models",
      "trajectory generation",
      "programming by demonstration",
      "robust regression"
    ]
  },
  {
    "title": "influence of shear deformation and rotary inertia on nonlinear free vibration of a beam with pinned ends.",
    "abstract": "the method of multiple scales is used to analyze the nonlinear vibrations of a beam with pinned ends. the formulation incorporates the effects of the transverse shear deformation as well as the rotary inertia on the large amplitude vibration behaviour. a uniformly valid higher second-order perturbation solution is obtained. the predictions of nonlinear frequencies at different beam parameters are given. the influences of shear and rotary inertia are significant in the case of moderately thick and short beams undergoing large amplitude vibrations.",
    "present_kp": [
      "shear deformation",
      "rotary inertia",
      "nonlinear vibrations",
      "beams"
    ],
    "absent_kp": []
  },
  {
    "title": "modeling lane-changing behavior in a connected environment: a game theory approach.",
    "abstract": "models lane changing behavior in connected environment using game theory concepts. formulates 2-person non-zero-sum non-cooperative game under complete information. model parameters estimated using ngsim vehicle trajectories. implemented in simulation framework to evaluate effect of connectivity.",
    "present_kp": [
      "lane-changing",
      "game theory"
    ],
    "absent_kp": [
      "simulated moments",
      "fictitious play"
    ]
  },
  {
    "title": "evolutionary approaches to solve three challenging engineering tasks.",
    "abstract": "three applications of evolutionary algorithms, namely the optimization of the nuclear core reload design, the synthesis of multi-layer optical coatings, and the synthesis of chemical engineering plants, are presented in this paper. the examples demonstrate the applicability of the evolutionary approach to solve complex real-world problems. these problems are often mixed-integer, variable-dimensional and multi-criteria optimization problems. additionally, instead of an objective function, given in a closed form, complex simulation models are used as an objective function, thus prohibiting success by means of classical analysis. although standard evolutionary algorithms are not able to solve the three complex tasks presented here, enhanced evolutionary algorithms (eas) clearly demonstrate their potential to do so. through the three examples, we show the necessary ingredients for tackling real-world optimization problems. in addition to adequate representations and appropriate evolutionary operators, one needs to integrate expert knowledge and heuristic operators to guide the search. finally, parallel eas are needed to improve the reliability and speed up of evolutionary algorithms.",
    "present_kp": [
      "evolutionary algorithms",
      "chemical engineering plants"
    ],
    "absent_kp": [
      "engineering applications",
      "nuclear reactors",
      "multilayer optical coatings"
    ]
  },
  {
    "title": "the use of static column ram as a memory hierarchy.",
    "abstract": "the static column ram devices recently introduced offer the potential for implementing a direct-mapped cache on-chip with only a small increase in complexity over that needed for a conventional dynamic ram memory system. trace-driven simulation shows that such a cache can only be marginally effective if used in the obvious way. however it can be effective in satisfying the requests from a processor containing an on-chip cache. the scram cache is more effective if the processor cache handles both instructions and data.",
    "present_kp": [
      "processor",
      "cache",
      "use",
      "device",
      "dynamic",
      "memory system",
      "instruction",
      "data",
      "direct",
      "complexity",
      "effect"
    ],
    "absent_kp": [
      "trace driven simulation",
      "memory hierarchies"
    ]
  },
  {
    "title": "parallel and distributed algorithms: laboratory assignments in joyce/linda.",
    "abstract": "a nsf ili grant funded development of parallel and distributed laboratories for an undergraduate course in parallel algorithms. the laboratory assignments explored various parallel and distributed architectures, and paradigms. these assignments were implemented using the joyce/linda language. the joyce/linda software was also utilized to develop parallel and distributed laboratory assignments for courses in data structures, operating systems and computer networks. the parallel and distributed algorithms course examined both theoretical and practical areas of study.",
    "present_kp": [
      "software",
      "distributed algorithms",
      "computer network",
      "parallel",
      "parallel algorithm",
      "architecture",
      "language",
      "operating system",
      "data structures",
      "assignment",
      "distributed",
      "practical",
      "laboratory"
    ],
    "absent_kp": [
      "visualization",
      "developer",
      "parallel computing"
    ]
  },
  {
    "title": "quantum chemical study of the intermediate complex required for iron-mediated reactivity and antimalarial activity of dispiro-1,2,4-trioxolanes.",
    "abstract": "quantum chemical methods were used to obtain a structure for the peroxideiron intermediate complex required for the inner-sphere reduction of dispiro-1,2,4-trioxolane antimalarials. investigation of this biologically important interaction with iron(ii) allows further understanding of the mechanisms of action and clearance of this promising new class of fully synthetic peroxide antimalarials. uhf, b3lyp and b3lyp//mp2 calculations were undertaken to provide structural and energetic information about the coordination complex of iron(ii) with five representative trioxolanes, ranging in both iron-mediated reactivity and antimalarial activity. significant energy differences were observed between the conformational isomers of these trioxolanes, indicating the importance of steric interactions between the iron complex ligands and the trioxolane substituents. these calculations may explain the slower iron-mediated reaction rates of trioxolanes that preferentially adopt a conformation that sterically shields the peroxide bond. the relationship between antimalarial activity and accessibility of the peroxide bond to iron has also been demonstrated for these trioxolanes.",
    "present_kp": [
      "trioxolane",
      "iron",
      "antimalarial"
    ],
    "absent_kp": [
      "quantum chemistry",
      "density functional methods"
    ]
  },
  {
    "title": "universal design for learning: meeting the challenge of individual learning differences through a neurocognitive perspective.",
    "abstract": "the traditional one-size-fits-all approach to curriculum denies the vast individual differences in learning strengths, challenges, and interests. the focus of this article is a novel approach, called universal design for learning, to addressing the challenge of individual learner differences. cognitive science research suggests the joint action of three broad sets of neural networks in cognition and learning: one that recognizes patterns, one that plans and generates patterns, and one that determines which patterns are important. these networks, referred to in this paper as recognition, strategic, and affective networks, are subject to individual differences that impact how individual students learn. this paper describes these networks and how the universal design for learning framework makes use of this networks-based perspective to structure the consideration of individual learner differences and guide the design of a flexible, technology-rich curriculum that provides rich options for meeting diverse student needs.",
    "present_kp": [
      "cognition and learning",
      "universal design for learning"
    ],
    "absent_kp": [
      "disabilities",
      "brain",
      "educational technology"
    ]
  },
  {
    "title": "energy efficient joint data aggregation and link scheduling in solar sensor networks.",
    "abstract": "solar sensor nodes equipped with micro-solar subsystems 111 provide a novel approach to harvest ambient energy, which partially alleviated the energy-limitation in traditional wireless sensor networks. however, it also poses new challenges that the amounts of energy harvested by nodes are dynamic and unbalanced among them thus network life cannot be necessarily prolonged if no well-designed energy-scheduling is adopted. herein, we present an algorithm to construct energy-efficient data aggregation tree (edat) based on a maximum-weighted connected dominating set (macds). the edat aims to prolong network life by minimizing differences in energy consumption among sensor nodes. here we assume that the amount of harvested energy randomly and uniformly distributes in the interval [h-min, h-max], where h-min and h-max are respectively the maximum and the minimum of h. the total energy consumption difference of an edat is at most where 5 (h) over tildes(2)/n-1, where (h) over tilde = vertical bar[h-min, h-max]vertical bar and s is the dominating set and it the total number of nodes. furthermore, we designed a link-scheduling algorithm to minimize the number of time slots necessary for scheduling all links in the whole network based on edat. the number of time slots is bounded in the interval [4 delta - 1,2/k(q)(2)], where l(k) is determined by hops k; q = max {delta, left perpendicularh(max)/h(min)right perpendicular}, and delta and delta are respectively the minimal and maximal degree of the network. we found the most efficient work period is not determined by the node with the minimum harvested energy but by the one with the minimum where d is the node's degree in the edat. we determine the necessary condition required for every node to have sufficient energy to support consecutive operation.",
    "present_kp": [
      "cds",
      "link scheduling"
    ],
    "absent_kp": [
      "energy harvesting",
      "solar node"
    ]
  },
  {
    "title": "incorporating angular information into parametric models.",
    "abstract": "the problem is considered of fitting a parametrically defined model in two or three dimensions to observed data, when angular information about the measured data points is available. gauss-newton methods based on correct separation of variables are developed. some numerical results axe included.",
    "present_kp": [
      "measured data",
      "parametric models",
      "angular information"
    ],
    "absent_kp": [
      "least squares fit",
      "conics"
    ]
  },
  {
    "title": "multilingual and cross-domain temporal tagging.",
    "abstract": "extraction and normalization of temporal expressions from documents are important steps towards deep text understanding and a prerequisite for many nlp tasks such as information extraction, question answering, and document summarization. there are different ways to express (the same) temporal information in documents. however, after identifying temporal expressions, they can be normalized according to some standard format. this allows the usage of temporal information in a term- and language-independent way. in this paper, we describe the challenges of temporal tagging in different domains, give an overview of existing annotated corpora, and survey existing approaches for temporal tagging. finally, we present our publicly available temporal tagger heideltime, which is easily extensible to further languages due to its strict separation of source code and language resources like patterns and rules. we present a broad evaluation on multiple languages and domains on existing corpora as well as on a newly created corpus for a language/domain combination for which no annotated corpus has been available so far.",
    "present_kp": [
      "temporal information",
      "temporal tagger"
    ],
    "absent_kp": [
      "named entity recognition",
      "named entity normalization",
      "timex2",
      "timex3"
    ]
  },
  {
    "title": "image time series processing for agriculture monitoring.",
    "abstract": "image time series analysis is of increasing relevance for environmental monitoring. dedicated tools are needed to process remote sensing image time series. spirits is free software to process image time series for crop monitoring. spirits has a user-friendly interface and is extensively documented.",
    "present_kp": [
      "remote sensing",
      "environmental monitoring"
    ],
    "absent_kp": [
      "early warning",
      "yield forecasting system",
      "crop and vegetation monitoring"
    ]
  },
  {
    "title": "analyzing e-governance mainstays on municipalities websites.",
    "abstract": "e-governance aims to provide citizens with a high quality of government. it covers services, information delivery and interactive community / government communication. this goal can be achieved by adopting the ict (information and communication technologies) tools in the government websites' design and content. this communication channel allows a redefinition of the traditional role played by each one of the actors in the relationship. the government as provider of: services, information, transparency and interactive communication. the citizens acting as active subjects with their government, using services, receiving information, controlling the government's decisions and returning feedback to them. this feedback includes opinions, complaints and suggestions delivered by the websites' interactive tools. this paper analyses local government websites and verifies if the adoption of tools provided by icts regarding national and international rules of design and contents, contributes to achieve a high level of: e-democracy, e-services, e-transparency and active and passive communication. all of them key concepts and mainstays to improve e-governance quality.",
    "present_kp": [
      "e-democracy",
      "local government",
      "transparency",
      "e-services",
      "e-governance"
    ],
    "absent_kp": [
      "municipality",
      "active and passive communicaton"
    ]
  },
  {
    "title": "two constructions of permutation arrays.",
    "abstract": "in this correspondence, two new constructions of permutation arrays are given. a number of examples to illustrate the constructions are also provided.",
    "present_kp": [
      "permutation arrays"
    ],
    "absent_kp": [
      "bounds",
      "code constructions"
    ]
  },
  {
    "title": "using bottom-up design techniques in the synthesis of digital hardware from abstract behavioral descriptions.",
    "abstract": "this paper reports on a new method for using bottom-up design information in the synthesis of integrated circuits from abstract behavioral descriptions. there are two important ways in which this method differs from traditional top-down synthesis techniques. first, it draws on a newly developed procedural database to collect detailed information on the physical and logical properties of the primitives available for building the design. second, it uses a different method for representing and organizing knowledge about a design that makes possible estimates of physical placement and wiring in the analysis of that design, even at the abstract register-transfer level. this allows a more accurate evaluation of candidate register-transfer designs without doing a full logic-level or transistor-level layout. it also leads to a simple method for systematically exploring the space of possible designs in order to find the one that best meets the designer's objectives and constraints.",
    "present_kp": [
      "design techniques",
      "order",
      "method",
      "space",
      "analysis",
      "behavior",
      "collect",
      "design",
      "layout",
      "object",
      "placement",
      "physical",
      "logic",
      "synthesis",
      "hardware",
      "knowledge",
      "paper",
      "integrated circuit",
      "constraint",
      "database",
      "evaluation"
    ],
    "absent_kp": [
      "digitize",
      "organization",
      "abstraction",
      "procedure",
      "informal",
      "exploration"
    ]
  },
  {
    "title": "the founding of the netscape user experience group.",
    "abstract": "netscape communications is a company that has grown faster than any other software company in history. although the design effort at netscape has evolved greatly, the initial experience of bringing design into an organization in hypergrowth provided some valuable lessons in the creation of a successful design organization.",
    "present_kp": [],
    "absent_kp": [
      "usability testing",
      "visual design",
      "organizations",
      "human factors"
    ]
  },
  {
    "title": "maximum relative margin and data-dependent regularization.",
    "abstract": "leading classification methods such as support vector machines (svms) and their counterparts achieve strong generalization performance by maximizing the margin of separation between data classes. while the maximum margin approach has achieved promising performance, this article identifies its sensitivity to affine transformations of the data and to directions with large data spread. maximum margin solutions may be misled by the spread of data and preferentially separate classes along large spread directions. this article corrects these weaknesses by measuring margin not in the absolute sense but rather only relative to the spread of data in any projection direction. maximum relative margin corresponds to a data-dependent regularization on the classification function while maximum absolute margin corresponds to an l(2) norm constraint on the classification function. interestingly, the proposed improvements only require simple extensions to existing maximum margin formulations and preserve the computational efficiency of svms. through the maximization of relative margin, surprising performance gains are achieved on real-world problems such as digit, text classification and on several other benchmark data sets. in addition, risk bounds are derived for the new formulation based on rademacher averages.",
    "present_kp": [
      "support vector machines"
    ],
    "absent_kp": [
      "kernel methods",
      "large margin",
      "rademacher complexity"
    ]
  },
  {
    "title": "view planning for automated three-dimensional object reconstruction and inspection.",
    "abstract": "laser scanning range sensors are widely used for high-precision, high-density three-dimensional (3d) reconstruction and inspection of the surface of physical objects. the process typically involves planning a set of views, physically altering the relative object-sensor pose, taking scans, registering the acquired geometric data in a common coordinate frame of reference, and finally integrating range images into a nonredundant model. efficiencies could be achieved by automating or semiautomating this process. while challenges remain, there are adequate solutions to semiautomate the scan-register-integrate tasks. on the other hand, view planning remains an open problem-that is, the task of finding a suitably small set of sensor poses and configurations for specified reconstruction or inspection goals. this paper surveys and compares view planning techniques for automated 3d object reconstruction and inspection by means of active, triangulation-based range sensors.",
    "present_kp": [
      "view planning",
      "range images",
      "object reconstruction"
    ],
    "absent_kp": [
      "algorithms",
      "design",
      "measurement",
      "performance",
      "object inspection"
    ]
  },
  {
    "title": "virtual integration of sensor observation data.",
    "abstract": "virtual observation data integration vs. prevailing data warehouse approaches. flexible combination of mediator/wrapper architecture with ogc swe interfaces. in situ and remote static and mobile sensors producing vector and raster data. multi-thread implementation to leverage current hardware multicore architectures. under validation in two spanish public meteorological and oceanographic agencies.",
    "present_kp": [
      "observation data",
      "data integration"
    ],
    "absent_kp": [
      "sos",
      "sensor web",
      "interoperability",
      "sensor data"
    ]
  },
  {
    "title": "modelling urban development with cellular automata incorporating fuzzy-set approaches.",
    "abstract": "this is the first part of a two-paper series on the development and application of a cellular automata model of urban development using geographic information systems (gis) and fuzzy-set approaches. under the paradigm of fuzzy-set theory, a cellular automata model of urban development was developed based on an understanding of the logistic trend of urban development processes. the model assigns membership of urban areas to multiple states of urban development using a fuzzy membership function. the transition rules based on linguistic variables are applied to represent the non-deterministic nature of urban development controls. by implementing the model in a raster based gis format, experimental scenarios of development of a virtual city under realistic conditions are presented. experimental application of the model to an artificial city produced realistic results and demonstrated the model was theoretically feasible and valid. further work is needed to calibrate the model when applying it to simulate actual urban development. in the second part of the two-paper series, spatio-temporal simulations of urban development in sydney, australia, from 1971 to 1996 will be demonstrated and discussed.",
    "present_kp": [
      "urban development",
      "cellular automata",
      "fuzzy-set",
      "gis"
    ],
    "absent_kp": [
      "fuzzy-logic-control"
    ]
  },
  {
    "title": "object-oriented subspace analysis for airborne hyperspectral remote sensing imagery.",
    "abstract": "an object-oriented mapping approach based on subspace analysis of airborne hyperspectral images was investigated in this paper. hyperspectral features were extracted based on subspace learning approaches, in order to reduce the redundancy of spectral space and extract the characteristic images for the further object-oriented classification. in this paper, three kinds of spectral feature extraction (fe) methods were utilized to obtain the subspace of airborne hyperspectral data: (1) unsupervised fe, such as pca (principal component analysis), ica (independent component analysis) and mnf (maximum noise fraction); (2) supervised fe, e.g. dbfe (decision boundary feature extraction), dafe (discriminant analysis feature extraction) and nwfe (nonparametric weighted feature extraction); and (3) linear mixture analysis. afterwards, the extracted subspace features were fed into the object-based classification system. the fnea (fractal net evolution approach) was utilized to extract objects from the subspace images and svm (support vector machines) was then used to classify the object-based features. experiments were conducted on two airborne hyperspectral datasets: (1) the aviris dataset over the northwest indiana's pine with 220 spectral bands (agricultural region), and (2) the rosis dataset over pavia university, northern of italy with 102 spectral bands (urban region). results revealed that the proposed object-based approach could give significantly higher accuracies than the traditional pixel-based subspace classification.",
    "present_kp": [
      "hyperspectral images",
      "subspace analysis",
      "feature extraction"
    ],
    "absent_kp": [
      "object-oriented analysis",
      "texture"
    ]
  },
  {
    "title": "reynolds stress modelling of jet and swirl interaction inside a gas turbine combustor.",
    "abstract": "influences of the inlet swirl levels on the interaction between the dilution air jets and the swirling cross-flow to the interior flow field inside a gas turbine combustor were investigated numerically by reynolds stress transport model (rstm). due to the intense swirl and jet interaction, a high level of swirl momentum is transported to the centreline and hence, an intense vortex core is formed. the strength of the centreline vortex core was found to depend on the inlet swirl levers. for the higher swirling inlet, the decay of the swirling motion causes strong streamline variation of pressure; and consequently leads to an elevated level of deceleration of its axial velocity. predictions contrasted with measurements indicate that the stress model reproduces the flow correctly and is able to reflect the influences of inlet swirl levels on the interior flow structure.",
    "present_kp": [
      "swirl",
      "gas turbine combustor"
    ],
    "absent_kp": [
      "three-dimensional flow",
      "turbulence modelling",
      "incompressible fluid"
    ]
  },
  {
    "title": "arbitrary viewpoint image synthesis for real-time processing system using multiple image sensors.",
    "abstract": "we have studied arbitrary viewpoint image synthesis using multiple sensors based on image-based rendering, in which the depth estimation is required for generating precise virtual images. major problems in this estimation process include the increase of computational efforts and the decrease in performance for occluded regions. to solve these difficulties, we improve our depth estimation process by using the multi-resolution method and by weighting similar pixels to the target pixel in the block matching. furthermore, we incorporate the proposed method to our prototype hardware system using the cmos image sensors we have originally developed. experimental results show that our proposed method and prototype system are efficient and effective compared to the previous work and that arbitrary viewpoint images are synthesized favorably.",
    "present_kp": [
      "cmos image sensor",
      "arbitrary viewpoint image"
    ],
    "absent_kp": [
      "fpga",
      "object shape estimation",
      "real-time synthesis"
    ]
  },
  {
    "title": "intelligent backstepping control for wheeled inverted pendulum.",
    "abstract": "the adaptive output recurrent cerebellar model articulation control (aorcmac) is an adaptive system with simple computation, good generalization capability and fast learning property. the proposed aorcmac has superior capability to the conventional cerebellar model articulation controller (cmac) in efficient learning mechanism and dynamic response. in this study, an intelligent backstepping tracking control system is proposed for wheeled inverted pendulums (wips) with unknown system dynamics and external disturbance. in this control system, an aborcmac is used to copy an ideal backstepping control (ibc), and a compensated controller is designed to compensate for difference between the ibc law and aorcmac. moreover, all adaptation laws of the proposed system are derived based on the lyapunov stability analysis, the taylor linearization technique, so that the stability of the closed-loop system can be guaranteed.",
    "present_kp": [
      "wheeled inverted pendulum",
      "backstepping tracking control",
      "compensated control",
      "output recurrent cerebellar model articulation control"
    ],
    "absent_kp": []
  },
  {
    "title": "situated robot learning for multi-modal instruction and imitation of grasping.",
    "abstract": "a key prerequisite to make user instruction of work tasks by interactive demonstration effective and convenient is situated multi-modal interaction aiming at an enhancement of robot learning beyond simple low-level skill acquisition. we report the status of the bielefeld gravis-robot system that combines visual attention and gestural instruction with an intelligent interface for speech recognition and linguistic interpretation to allow multi-modal task-oriented instructions. with respect to this platform, we discuss the essential role of learning for robust functioning of the robot and sketch the concept of an integrated architecture for situated learning on the system level. it has the long-term goal to demonstrate speech-supported imitation learning of robot actions. we describe the current state of its realization to enable imitation of human hand postures for flexible grasping and give quantitative results for grasping a broad range of everyday objects.",
    "present_kp": [
      "interactive demonstration",
      "imitation",
      "learning",
      "architecture",
      "grasping"
    ],
    "absent_kp": []
  },
  {
    "title": "pareto-based evolutionary computational approach for wireless sensor placement.",
    "abstract": "wireless sensor networks (wsns) have become increasingly appealing in recent years for the purpose of data acquisition, surveillance, event monitoring, etc. optimal positioning of wireless sensor nodes is an important issue for small networks of relatively expensive sensing devices. for such networks, the placement problem requires that multiple objectives be met. these objectives are usually conflicting, e.g. achieving maximum coverage and maximum connectivity while minimizing the network energy cost. a flexible algorithm for sensor placement (flex) is presented that uses an evolutionary computational approach to solve this multiobjective sensor placement optimization problem when the number of sensor nodes is not fixed and the maximum number of nodes is not known a priori. flex starts with an initial population of simple wsns and complexifies their topologies over generations. it keeps track of new genes through historical markings, which are used in later generations to assess two networks compatibility and also to align genes during crossover. it uses pareto-dominance to approach pareto-optimal layouts with respect to the objectives. speciation is employed to aid the survival of gene innovations and facilitate networks to compete with similar networks. elitism ensures that the best solutions are carried over to the next generation. the flexibility of the algorithm is illustrated by solving the device/node placement problem for different applications like facility surveillance, coverage with and without obstacles, preferential surveillance, and forming a clustering hierarchy.",
    "present_kp": [
      "wireless sensor network",
      "coverage",
      "connectivity"
    ],
    "absent_kp": [
      "optimal node placement",
      "multiobjective optimization"
    ]
  },
  {
    "title": "converse duality in nonlinear programming with cone constraints.",
    "abstract": "the purpose of this paper is to establish various converse duality results for nonlinear programming with cone constraints and its four dual models introduced by chandra and abha.",
    "present_kp": [
      "converse duality",
      "nonlinear programming"
    ],
    "absent_kp": [
      "pseudo-invexity",
      "positive definiteness"
    ]
  },
  {
    "title": "asymptotic results for batch-variance methods in simulation output analysis.",
    "abstract": "we consider the hatching methods for estimating the variance of the sample variance based on steady-state correlated data. intensive research has been devoted to the problem of estimating the variance of the sample, mean, but little to the sample variance when the desired performance measure is the population variance. the batch-variance estimator (for the variance of the sample variance) is a function of the batch variances. which are the sample variances of the hatched data. by viewing the sample variance as a sample mean of squared terms. we show that the asymptotic results for the batch-variance and batch-mean estimators are analogous in two ways. first. both. have the same convergence rates. second, whether batch means or batch variances are employed, a single rule applies to both multipliers in the asymptotic formula. the constant multipliers are the same. and the other multipliers depend on the data properties, which are analogous for batch variances and batch means with squared terms. we prove these results analytically for the nonoverlapping batch-variance method and argue that they can be extended to cover the overlapping batch-variance method.",
    "present_kp": [
      "batch means",
      "batch variances",
      "sample variance",
      "simulation output analysis"
    ],
    "absent_kp": [
      "optimal batch size"
    ]
  },
  {
    "title": "a context-aware approach based on self-organizing maps to study web-users' tendencies from their behaviour.",
    "abstract": "in the context of a highly volatile web of uneven quality, the identification of content deemed valuable by end users is of paramount importance. where page content undergoes rapid change, this issue is particularly challenging. web browsing activity represents a unique source of context by which the value of web pages can be determined via an assessment of individual user interactions, such as scrolling, clicking, saving and so forth. over time, this data set forms a pattern of activity which can be mined for meaning. in this paper we present an approach to web content, based on kohonen mapping, used to generate a topological model of users' behaviour over web-pages. each web-document can thus be represented as a semantic map built by adopting unsupervised techniques where similar users' behaviour are mapped close together, with identification of information stability emerging as a by product of the identification of similarity in user activity over content. in this model, the more similar the outputs of the map for each user who has endorsed a web-page, the more the web site is considered current or in context with changing information. we illustrate the potential application of this approach to our ongoing work in social search.",
    "present_kp": [
      "social search",
      "self-organizing map",
      "web"
    ],
    "absent_kp": [
      "distributed systems",
      "kohonen maps",
      "context-awareness",
      "computational trust"
    ]
  },
  {
    "title": "a linear time algorithm for computing a most reliable source on a tree network with faulty nodes.",
    "abstract": "given an unreliable communication network, we seek for a node which maximizes the expected number of nodes that are reachable from it. such a node is called a most reliable source (mrs) of the network. in communication networks, failures may occur to both links and nodes. previous studies have considered the case where each link has an independent operational probability, while the nodes are immune to failures. in practice, however, failures may happen to the nodes as well, including both transmitting fault and receiving fault. recently, another variant of the mrs problem is studied, where all links are immune to failures and each node has an independent transmitting probability and receiving probability, and an o(n(2)) time algorithm is presented for computing an mrs on tree networks with n nodes. in this paper, we present a faster algorithm for this problem, with a time complexity of o(n).",
    "present_kp": [
      "most reliable source",
      "transmitting probability",
      "receiving probability"
    ],
    "absent_kp": []
  },
  {
    "title": "controllable adiabatic manipulation of the qubit state.",
    "abstract": "we propose a scheme which implements a controllable change of the state of the target spin qubit in such a way that both the control and the target spin qubits remain in their ground states. the interaction between the two spins is mediated by an auxiliary spin, which can transfer to its excited state. our scheme suggests a possible relationship between the gate and adiabatic quantum computation.",
    "present_kp": [
      "adiabatic quantum computation"
    ],
    "absent_kp": [
      "gate quantum computation"
    ]
  },
  {
    "title": "a new bidirectional heteroassociative memory encompassing correlational, competitive and topological properties.",
    "abstract": "in this paper, we present a new recurrent bidirectional model that encompasses correlational, competitive and topological model properties. the simultaneous use of many classes of network behaviors allows for the unsupervised learning/categorization of perceptual patterns (through input compression) and the concurrent encoding of proximities in a multidimensional space. all of these operations are achieved within a common learning operation, and using a single set of defining properties. it is shown that the model can learn categories by developing prototype representations strictly from exposition to specific exemplars. moreover, because the model is recurrent, it can reconstruct perfect outputs from incomplete and noisy patterns. empirical exploration of the models properties and performance shows that its ability for adequate clustering stems from: (1) properly distributing connection weights, and (2) producing a weight space with a low dispersion level (or higher density). in addition, since the model uses a sparse representation (k-winners), the size of topological neighborhood can be fixed, and no longer requires a decrease through time as was the case with classic self-organizing feature maps. since the models learning and transmission parameters are independent from learning trials, the model can develop stable fixed points in a constrained topological architecture, while being flexible enough to learn novel patterns.",
    "present_kp": [
      "bidirectional heteroassociative memory",
      "topological architecture",
      "unsupervised learning",
      "categorization"
    ],
    "absent_kp": [
      "competitive learning",
      "self-organizing map"
    ]
  },
  {
    "title": "a service re-design methodology for multi-channel adaptation.",
    "abstract": "many available services have been designed for a single-channel world, web and internet typically. in a real world scenario, an ever-growing number of users take advantage of different kinds of communication channels and devices. in this paper, we propose a methodology to formalize the re-design process of these services to support multi-channel access in different contexts. the methodology considers the channel characteristics, the location of users and the context of use to characterize the new services. it has been tested in a case study: the italian national project of identification and registration of bovine animals.",
    "present_kp": [
      "context of use",
      "methodology",
      "service re-design"
    ],
    "absent_kp": [
      "location awareness",
      "multi-channel application",
      "qualities of services"
    ]
  },
  {
    "title": "value-at-risk-based two-stage fuzzy facility location problems.",
    "abstract": "reducing risks in location decisions when coping with imprecise information is critical in supply chain management so as to increase competitiveness and profitability. in this paper, a two-stage fuzzy facility location problem with value-at-risk (var), called var-fflp, is proposed, which results in a two-stage fuzzy zero-one integer programming problem. some properties of the var-fflp, including the value of perfect information (vpi), the value of fuzzy solution (vfs), and the bounds of the fuzzy solution, are discussed. since the fuzzy parameters of the location problem are represented in the form of continuous fuzzy variables, the determination of var is inherently an infinite-dimensional optimization problem that cannot be solved analytically. therefore, a method based on the discretization of the fuzzy variables is proposed to approximate the var. the approximation approach converts the original problem into a finite-dimensional optimization problem. a pertinent convergence theorem for the approximation approach is proved. subsequently, by combining the simplex algorithm, the approximation approach, and a mechanism of genotype-phenotype-mutation-based binary particle swarm optimization (gpm-bpso), a hybrid gpm-bpso algorithm is being exploited to solve the var-fflp. a numerical example illustrates the effectiveness of the hybrid gpm-bpso algorithm and shows its enhanced performance in comparison with the results obtained by other approaches using genetic algorithm (ga), tabu search (ts), and boolean bpso (b-bpso).",
    "present_kp": [
      "binary particle swarm optimization ",
      "facility location",
      "fuzzy variable",
      "genetic algorithm ",
      "tabu search ",
      "value-at-risk "
    ],
    "absent_kp": [
      "approximate approach"
    ]
  },
  {
    "title": "ecloudrfid a mobile software framework architecture for pervasive rfid-based applications.",
    "abstract": "in the last few years, rfid technology has become a stimulating and rapidly expanding area of research and development. the technology's ability to precisely identify objects at low cost and without line of sight creates new and emerging opportunities for rfid applications that could become an integral part of our daily lives. mobile rfid services for the internet of things can be created using rfid as an enabling technology in mobile devices. this pervasiveness enables embedded systems to interact with the user continuously, providing data from their sensors and responding to requests from the users too. humans, devices and things are both the content providers and users of these mobile services. mobile rfid services can be either provided on mobile devices applications as stand-alone services or combined with legacy and end-to-end systems. however, to develop and deploy this kind of system, it is necessary for an effective software development infrastructure to be able to deal with the restrictions and limitations imposed by related business areas, rfid and pervasive applications. this work presents a software framework architecture for mobile devices that aims to facilitate the development process of embedded rfid applications and the integration process of business applications and epc network instances. the framework provides for applications a common communication interface to abstract different devices and reading protocols as well as functions to process and distribute data.",
    "present_kp": [
      "internet of things",
      "mobile devices"
    ],
    "absent_kp": [
      "middleware rfid",
      "embedded framework",
      "pervasive computing"
    ]
  },
  {
    "title": "a note on exact and heuristic algorithms for the identical parallel machine scheduling problem.",
    "abstract": "a recent paper (davidovi? etal., j. heuristics, 18:549569, 2012) presented a bee colony metaheuristic for scheduling independent tasks to identical processors, evaluating its performance on a benchmark set of instances from the literature. we examine two exact algorithms from the literature, the former published in 1995, the latter in 2008 (and not cited by the authors). we show that both such algorithms solve to proven optimality all the considered instances in a computing time that is several orders of magnitude smaller than the time taken by the new algorithm to produce an approximate solution.",
    "present_kp": [
      "identical parallel machine scheduling",
      "exact algorithms"
    ],
    "absent_kp": [
      "metaheuristic algorithms"
    ]
  },
  {
    "title": "off-line optimization and control for real time integration of a three-phase hydrogenation catalytic reactor.",
    "abstract": "in order to develop and test the integration procedure, in this paper a real time process integration involving the optimization and control of the process is presented, in this case, with the two-layer approach. the used optimization algorithms were levenberg-marquardt and sqp that solve a non-linear least square problem subject to bounds on the variables. the two-layer approach is a hierarchical control structure where an optimization layer calculates the set points and manipulated variables to the advanced controller, which is based on the dynamic matrix control with constraints (qdmc). the non-isothermal dynamic model of the three-phase slurry catalytic reactor with appropriate solution procedure was utilized in this work (vasco de toledo, e. c., santana, p. l., maciel, m. r. w., & maciel filho, r. (2001). dynamic modeling of a three-phase catalytic slurry reactor. chemical engineering science, 56, 6055-6061). the model consists on mass and heat balance equations for the catalyst particles as well as for the bulk phases of gas and liquid. the model was used to describe the dynamic behavior of hydrogenation reaction of o-cresol to obtain 2-methil-cyclohexanol, in the presence of a catalyst ni/sio2.",
    "present_kp": [
      "two-layer approach",
      "hydrogenation",
      "dynamic modeling",
      "optimization",
      "advanced controller"
    ],
    "absent_kp": [
      "simulation"
    ]
  },
  {
    "title": "a workflow engine-driven soa-based cooperative computing paradigm in grid environments.",
    "abstract": "the grid has been proposed as a promising service-oriented platform for increasingly complex cooperative computing. the platforms of service-oriented grids are often web-based where participants collaborate to achieve a common goal by sharing scarce web-based computational/ computing resources (wbcr). to share the wbcr effectively is a challenging problem in boundary-spanning grid environments, particularly when these resources are subject to both static and dynamic usage. to set up the certificate-based usage policy described in this paper, we first explore a workflow engine-driven soa-based resource access control mechanism. then, aiming at setting up a cooperative computing paradigm from the resource sharing perspective, an infrastructure derived from a specific project of soa&edscce (soa-based&engine-driven structured cooperative computing environment) is proposed for promoting its cooperative computing in grid environment based on the control disciplines and the wbcr usage policy. the main contributions of this paper are twofold: 1) a workflow engine-driven soa-based wbcr sharing mechanism is presented in accordance with to the certificate-based usage policy; and 2) a specific infrastructure of cooperative computing is put forward for the collaboration based on the wbcr sharing mechanism.",
    "present_kp": [
      "workflow engine",
      "cooperative computing",
      "grid",
      "certificate"
    ],
    "absent_kp": [
      "service-oriented architecture "
    ]
  },
  {
    "title": "a new method for inverse electromagnetic casting problems based on the topological derivative.",
    "abstract": "the inverse electromagnetic casting problem consists in looking for a suitable set of electric wires such that the electromagnetic field induced by an alternating current passing through them makes a given mass of liquid metal acquire a predefined shape. in this paper we propose a new method for the topology design of such inductors. the inverse electromagnetic casting problem is formulated as an optimization problem, and topological derivatives are considered in order to locate new wires in the right position. several numerical examples are presented showing that the proposed technique is effective to design suitable inductors.",
    "present_kp": [
      "topological derivatives",
      "electromagnetic casting"
    ],
    "absent_kp": [
      "topological asymptotic analysis",
      "inverse problem"
    ]
  },
  {
    "title": "translation and scale invariants of legendre moments.",
    "abstract": "by convention, the translation and scale invariant functions of legendre moments are achieved by using a combination of the corresponding invariants of geometric moments. they can also be accomplished by normalizing the translated and/or scaled images using complex or geometric moments. however, the derivation of these functions is not based on legendre polynomials. this is mainly due to the fact that it is difficult to extract a common displacement or scale factor from legendre polynomials. in this paper, we introduce a new set of translation and scale invariants of legendre moments based on legendre polynomials. the descriptors remain unchanged for translated, elongated, contracted and reflected non-symmetrical as well as symmetrical images. the problems associated with the vanishing of odd-order legendre moments of symmetrical images are resolved. the performance of the proposed descriptors is experimentally confirmed using a set of binary english, chinese and latin characters. in addition to this, a comparison of computational speed between the proposed descriptors and the aforesaid geometric moments-based method is also presented.",
    "present_kp": [
      "legendre moments",
      "translation and scale invariants",
      "geometric moments"
    ],
    "absent_kp": [
      "image normalization",
      "aspect ratio invariants"
    ]
  },
  {
    "title": "an (h^1)-galerkin mixed finite element method for time fractional reactiondiffusion equation.",
    "abstract": "in this article, an (h^1)-galerkin mixed finite element (mfe) method for solving time fractional reactiondiffusion equation is presented. the optimal time convergence order (o(vardelta t^{2-alpha })) and the optimal spatial rate of convergence in (h^1) and (l^2)-norms for variable (u) and its gradient (sigma ) are derived. moreover, some numerical results are shown to support our theoretical analysis.",
    "present_kp": [
      "time fractional reactiondiffusion equation",
      ""
    ],
    "absent_kp": [
      "-galerkin mixed method",
      "fractional derivative",
      "optimal convergence rate",
      "a priori error estimates"
    ]
  },
  {
    "title": "fuzzy systems design: direct and indirect approaches.",
    "abstract": "a systematic classification of the data-driven approaches for design of fuzzy systems is given in the paper. the possible ways to solve this modelling and identification problem are classified on the basis of the optimisation techniques used for this purpose. one algorithm for each of the two basic categories of design methods is presented and its advantages and disadvantages are discussed. both types of algorithms are self-learning and do not require interaction during the process of fuzzy model design. they perform adaptation of both the fuzzy model structure (rule-base) and the parameters. the indirect approach exploits the dual nature of takagi-sugeno (ts) models and is based on recently introduced recursive clustering combined with kalman filtering-based procedure for recursive estimation of the parameter of the local sub-models. both algorithms result in finding compact and transparent fuzzy models. the direct approach solves the optimisation problem directly, while the indirect one decomposes the original problem into on-line clustering and recursive estimation problems and finds a sub-optimal solution in real-time. the later one is computationally very efficient and has a range of potential applications in real-time process control, moving images recognition, autonomous systems design etc. it is extended in this paper for the case of multi-input-multi-output (mimo systems). both approaches have been tested with real data from an engineering process.",
    "present_kp": [
      "on-line clustering"
    ],
    "absent_kp": [
      "fuzzy models design",
      "takagi-sugeno and mamdani fuzzy models",
      "recursive least squares estimation",
      "genetic algorithms"
    ]
  },
  {
    "title": "analysis of zero-inflated clustered count data: a marginalized model approach.",
    "abstract": "min and agresti (2005) proposed random effect hurdle models for zero-inflated clustered count data with two-part random effects for a binary component and a truncated count component. in this paper, we propose new marginalized models for zero-inflated clustered count data using random effects. the marginalized models are similar to dobbie and welshs (2001) model in which generalized estimating equations were exploited to find estimates. however, our proposed models are based on a likelihood-based approach. a quasi-newton algorithm is developed for estimation. we use these methods to carefully analyze two real datasets.",
    "present_kp": [
      "hurdle models",
      "random effects",
      "quasi-newton"
    ],
    "absent_kp": [
      "zip models"
    ]
  },
  {
    "title": "logic minimization for large-scale networks based on multi-signal implications.",
    "abstract": "this paper presents a novel implication-based method for logic minimization in large-scale, multi-level networks. it significantly reduces network size through repeated addition and removal of redundant subnetworks, utilizing multisignal implications and relationships among these implications. these are handled on a transitive implication graph, proposed ill this paper, which offers the practical use of implications for logic minimization. the proposed method holds great promise for the achievement of an interactive logic design environment for large-scale networks.",
    "present_kp": [
      "logic minimization",
      "implication",
      "implication graph"
    ],
    "absent_kp": [
      "logic synthesis"
    ]
  },
  {
    "title": "a curve evolution approach for unsupervised segmentation of images with low depth of field.",
    "abstract": "in this paper, we describe a novel algorithm for unsupervised segmentation of images with low depth of field (dof). first of all, a multi-scale reblurring model is used to detect the object of interest (ooi) in saliency space. then, to determine the boundary of ooi, an active contour model based on hybrid energy function is proposed. in this model, a global energy item related with the saliency map is adopted to find the global minimum, and a local energy term regarding the low dof image is used to improve the segmentation precision. in addition, an adaptive parameter is attached to this model to balance the weight of global and local energy. furthermore, an unsupervised curve initialization method is designed to reduce the number of evolution iterations. finally, we conduct experiments on various low dof images, and the results demonstrate the high robustness and precision of the proposed approach.",
    "present_kp": [
      "low depth of field",
      "active contour model",
      "curve evolution"
    ],
    "absent_kp": [
      "image segmentation",
      "unsupervised initialization"
    ]
  },
  {
    "title": "a dynamic model of optimal investment in research and development with international knowledge spillovers.",
    "abstract": "we consider a two-country endogenous growth model where an economic follower absorbs part of the knowledge generated in a leading country. to solve a suitably defined infinite horizon dynamic optimization problem an appropriate version of the pontryagin maximum principle is developed. the properties of optimal controls and the corresponding optimal trajectories are characterized by the qualitative analysis of the solutions of the hamiltonian system arising through the implementation of the pontryagin maximum principle.",
    "present_kp": [
      "knowledge spillovers",
      "infinite horizon",
      "pontryagin maximum principle"
    ],
    "absent_kp": [
      "optimal economic growth",
      "transversality conditions"
    ]
  },
  {
    "title": "tap control for headphones without sensors.",
    "abstract": "a tap control technique for headphones is proposed. a simple circuit is used to detect tapping of the headphone shell by using the speaker unit in the headphone as a tap sensor. no additional devices are required in the headphone shell and cable, so the user can use their favorite headphones as a controller while listening music. a prototype is implemented with several calibration processes to compensate the differences in headphones and users' tapping actions. tests confirm that the user can control a music player by tapping regular headphones.",
    "present_kp": [
      "headphones",
      "tap"
    ],
    "absent_kp": [
      "input device",
      "wearable"
    ]
  },
  {
    "title": "topical locality in the web.",
    "abstract": "most web pages are linked to others with related content . this idea, combined with another that says that text in, and possibly around, html anchors describe the pages to which they point , is the foundation for a usable world-wide web. in this paper, we examine to what extent these ideas hold by empirically testing whether topical locality mirrors spatial locality of pages on the web. in particular, we find that the likelihood of linked pages having similar textual content to be high; the similarity of sibling pages increases when the links from the parent are close together; titles, descriptions, and anchor text represent at least part of the target page; and that anchor text may be a useful discriminator among unseen child pages. these results show the foundations necessary for the success of many web systems, including search engines, focused crawlers, linkage analyzers, and intelligent web agents.",
    "present_kp": [
      "spatial",
      "agent",
      "systems",
      "search engine",
      "test",
      "web",
      "web page",
      "text",
      "links",
      "similarity",
      "paper",
      "locality"
    ],
    "absent_kp": [
      "contention",
      "usability",
      "intelligence",
      "world wide web"
    ]
  },
  {
    "title": "modified constrained learning algorithms incorporating additional functional constraints into neural networks.",
    "abstract": "in this paper, two modified constrained learning algorithms are proposed to obtain better generalization performance and faster convergence rate. the additional cost terms of the first algorithm are selected based on the first-order derivatives of the activation functions of the hidden neurons and the second-order derivatives of the activation functions of the output neurons, while the additional cost terms of the second one are selected based on the first-order derivatives of the activation functions of the output neurons and the second-order derivatives of the activation functions of the hidden neurons. in the course of training, the additional cost terms of the proposed algorithms can penalize the input-to-output mapping sensitivity and the high frequency components simultaneously so that the better generalization performance can be obtained. finally, theoretical justifications and simulation results are given to verify the efficiency and effectiveness of our proposed learning algorithms.",
    "present_kp": [
      "constrained learning algorithm",
      "generalization performance",
      "convergence rate",
      "mapping sensitivity",
      "high frequency components"
    ],
    "absent_kp": []
  },
  {
    "title": "dynamic environment robot path planning using hierarchical evolutionary algorithms.",
    "abstract": "the problem of path planning deals with the computation of an optimal path of the robot, from source to destination, such that it does not collide with any obstacle on its path. in this article we solve the problem of path planning separately in two hierarchies. the coarser hierarchy finds the path in a static environment consisting of the entire robotic map. the resolution of the map is reduced for computational speedup. the finer hierarchy takes a section of the map and computes the path for both static and dynamic environments. both the hierarchies make use of an evolutionary algorithm for planning. both these hierarchies optimize as the robot travels in the map. the static environment path is increasingly optimized along with generations. hence, an extra setup cost is not required like other evolutionary approaches. the finer hierarchy makes the robot easily escape from the moving obstacle, almost following the path shown by the coarser hierarchy. this hierarchy extrapolates the movements of the various objects by assuming them to be moving with same speed and direction. experimentation was done in a variety of scenarios with static and mobile obstacles. in all cases the robot could optimally reach the goal. further, the robot was able to escape from the sudden occurrence of obstacles.",
    "present_kp": [
      "evolutionary algorithms",
      "robot path planning"
    ],
    "absent_kp": [
      "dynamic obstacles",
      "hierarchical genetic algorithms",
      "robotics"
    ]
  },
  {
    "title": "detecting phenotype-specific interactions between biological processes from microarray data and annotations.",
    "abstract": "high throughput technologies enable researchers to measure expression levels on a genomic scale. however, the correct and efficient biological interpretation of such voluminous data remains a challenging problem. many tools have been developed for the analysis of go terms that are over-or under-represented in a list of differentially expressed genes. however, a previously unexplored aspect is the identification of changes in the way various biological processes interact in a given condition with respect to a reference. here, we present a novel approach that aims at identifying such interactions between biological processes that are significantly different in a given phenotype with respect to normal. the proposed technique uses vector-space representation, svd-based dimensionality reduction, differential weighting, and bootstrapping to asses the significance of the interactions under the multiple and complex dependencies expected between the biological processes. we illustrate our approach on two real data sets involving breast and lung cancer. more than 88 percent of the interactions found by our approach were deemed to be correct by an extensive manual review of literature. an interesting subset of such interactions is discussed in detail and shown to have the potential to open new avenues for research in lung and breast cancer.",
    "present_kp": [
      "phenotype-specific interactions",
      "biological processes"
    ],
    "absent_kp": [
      "microarrays",
      "gene ontology",
      "single value decomposition"
    ]
  },
  {
    "title": "computer simulation of production systems for woven fabric manufacture.",
    "abstract": "this paper reports a research and development of a suite of generic software program entitled texsim (textile simulator). the software is mainly intended to create simulation models of weaving of production systems without any programming and automatically performs the simulation study and produces results to understand the stochastic behaviour of the system as well as to analyze the system performances to solve the real life weaving production management problems. the 'texsim' reads the input parameters from the user in an on-line session through its user-interface, written in fortran'77, and interactively uses witness, a manufacturing simulation package containing the basic simulation model building blocks, and creates the simulation model in accordance with the user's specifications and conducts the simulation experiments and produces results. the objective is to focus on the practicality and simplicity of simulation model building of a weaving production system with a readily available suite of user-friendly program texsim within few minutes without expertise and back ground of simulation technique and the knowledge of computer simulation programming as well as the skill of handling of commercial simulation package. it also highlights the importance of use of computer simulation technique as a modern, powerful and flexible management analysis tool in weaving factories. textile engineers and technologists, particularly the managers who have no background of simulation can take full advantages of the use of simulation technique to analyze their present complex weaving production systems, rather than using the conventional analytical rule of thumb methods, to help the management to plan, design and operate their systems in an efficient manner to improve the manufacturing productivity. texsim also facilitates the scheduling of production within the factory through simulation.",
    "present_kp": [
      "suite of generic software",
      "simulation",
      "system performance",
      "witness"
    ],
    "absent_kp": [
      "os/2 environment",
      "model library file"
    ]
  },
  {
    "title": "optimization of range checking.",
    "abstract": "an analysis is given for optimizing run-time range checks in regions of high execution frequency. these optimizations are accomplished using strength reduction, code motion and common subexpression elimination. test programs, using the above optimizations, are used to illustrate run-time improvements.",
    "present_kp": [
      "reduction",
      "program",
      "test",
      "motion",
      "region",
      "analysis",
      "code"
    ],
    "absent_kp": [
      "timing",
      "optimality"
    ]
  },
  {
    "title": "new autoantibodies in pediatric opsoclonusmyoclonus syndrome.",
    "abstract": "opsoclonusmyoclonus syndrome (oms) is a rare neurologic disorder comprising the main symptoms of eye-movement disturbances, muscle jerks, and severe ataxia. in children and adults, some cases are associated with a tumor as a paraneoplastic syndrome, whereas in children the paraneoplastic form is almost exclusively associated with neuroblastoma. the detection of autoantibodies in some oms sera led to the hypothesis that the syndrome is of autoimmune origin. beside autoantibodies against intracellular proteins, such as anti-hu, ?-enolase, and khsrp, specific binding of autoantibodies to the surface of neuroblastoma cells and cerebellar granular neurons have been found. antiproliferative and proapoptotic effects of these autoantibodies on neuroblastoma cell lines were noted as well. these results support the concept of a humoral autoimmune process in the pathogenesis of oms",
    "present_kp": [
      "autoantibodies",
      "neuroblastoma",
      "opsoclonusmyoclonus syndrome "
    ],
    "absent_kp": []
  },
  {
    "title": "a visual shape descriptor using sectors and shape context of contour lines.",
    "abstract": "this paper describes a visual shape descriptor based on the sectors and shape context of contour lines to represent the image local features used for image matching. the proposed descriptor consists of two-component feature vectors. first, the local region is separated into sectors and their gradient magnitude and orientation values are extracted; a feature vector is then constructed from these values. second, local shape features are obtained using the shape context of contour lines. another feature vector is then constructed from these contour lines. the proposed approach calculates the local shape feature without needing to consider the edges. this can overcome the difficulty associated with textured images and images with ill-defined edges. the combination of two-component feature vectors makes the proposed descriptor more robust to image scale changes, illumination variations and noise. the proposed visual shape descriptor outperformed other descriptors in terms of the matching accuracy: 14.525% better than sift, 21% better than pca-sift, 11.86% better than gloh, and 25.66% better than the shape context.",
    "present_kp": [
      "shape context",
      "image matching"
    ],
    "absent_kp": [
      "local descriptor",
      "object recognition",
      "image retrieval"
    ]
  },
  {
    "title": "from modulated hebbian plasticity to simple behavior learning through noise and weight saturation.",
    "abstract": "synaptic plasticity is a major mechanism for adaptation, learning, and memory. yet current models struggle to link local synaptic changes to the acquisition of behaviors. the aim of this paper is to demonstrate a computational relationship between local hebbian plasticity and behavior learning by exploiting two traditionally unwanted features: neural noise and synaptic weight saturation. a modulation signal is employed to arbitrate the sign of plasticity: when the modulation is positive, the synaptic weights saturate to express exploitative behavior; when it is negative, the weights converge to average values, and neural noise reconfigures the networks functionality. this process is demonstrated through simulating neural dynamics in the autonomous emergence of fearful and aggressive navigating behaviors and in the solution to reward-based problems. the neural model learns, memorizes, and modifies different behaviors that lead to positive modulation in a variety of settings. the algorithm establishes a simple relationship between local plasticity and behavior learning by demonstrating the utility of noise and weight saturation. moreover, it provides a new tool to simulate adaptive behavior, and contributes to bridging the gap between synaptic changes and behavior in neural computation.",
    "present_kp": [
      "adaptive behavior",
      "learning"
    ],
    "absent_kp": [
      "computational models",
      "neural plasticity",
      "neuromodulation"
    ]
  },
  {
    "title": "investigation of the substrate activation mechanism and electroless nip coating ductility and adhesion.",
    "abstract": "the effects of different factors: concentration of sn and pd ions, temperature, ph, additives, surface roughness, time of treatment on sensitization, activation and electroless deposition were investigated. the adsorption and desorption of tin and palladium ions have been investigated using the method of radioactive isotopes and x-ray photoelectron spectroscopy. a mechanism of sensitization and activation was established. the established characters of ductility and adhesion between the electrolessly deposited nip alloy and dielectrics depending on annealing temperature were explained on the basis of the changes in the alloy structure and phase transformations. the proposed methods of metallization are widely used in electronics and instrument-making. as a result, au, ag and pd were adequately replaced with the alloys of non-precious metals, usage of toxic substances was eliminated and the technology was significantly simplified. a new resistless technology is proposed.",
    "present_kp": [
      "electroless deposition",
      "sensitization",
      "activation",
      "ductility",
      "resistless technology"
    ],
    "absent_kp": [
      "microelectronics"
    ]
  },
  {
    "title": "model checking conditional csl for continuous-time markov chains.",
    "abstract": "in this paper, we consider the model-checking problem of continuous-time markov chains (ctmcs) with respect to conditional logic. to the end, we extend continuous stochastic logic introduced in aziz et al. (2000) to conditional continuous stochastic logic (ccsl) by introducing a conditional probabilistic operator. ccsl allows us to express a richer class of properties for ctmcs. based on a parameterized product obtained from the ctmc and an automaton extracted from a given ccsl formula, we propose an approximate model checking algorithm and analyse its complexity.",
    "present_kp": [
      "continuous-time markov chains",
      "continuous stochastic logic",
      "conditional logic"
    ],
    "absent_kp": [
      "formal methods",
      "probabilistic systems"
    ]
  }
]