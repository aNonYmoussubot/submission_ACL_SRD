[
  {
    "text": "is x11 cross-linux-platform and free from platform quirks?. does x11 work across all linux operating systems? if i code a application that has an x11 window, will it look and operate the same across linux os's or will i have different tweaks i need to perform?is there an alternative to x11 that is in c/c++ and low-level cross linux platform? *i'm aware of qt and gtk. what is the lowest level linux graphical system/windowing system?",
    "present_kp": [
      "linux",
      "x11"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how can one measure contributions to a project?. has anyone ever come across this problem? when you have a team of developers working on a project, how can you measure their contributions to said project? is there a formal way of doing it? number of commits? number of bug fixes? lines committed? or maybe on a ticket basis? i'm trying to think of a workflow where a group of developers can work seamlessly interchanging tasks and at the end of the month, getting paid for they contributions (or merit if you will).i hope this question is not entirely out of place here, if it is, feel free to close it.",
    "present_kp": [
      "contribution"
    ],
    "absent_kp": [
      "workflows",
      "measurement"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "key_read: type mismatch: encoding error. seeing very strange error on newly install centos7, i didn't create or touch any ssh-key. ~/.ssh/ directory is also empty. [john@server1 ~]$ ssh server1key_read: type mismatch: encoding errorkey_read: type mismatch: encoding errorjohn@server1's password:if i try ssh localhost i am not getting that error. what will be the issue? update:[john@server1 ~]$ ssh -vvv server1openssh_6.4, openssl 1.0.1e-fips 11 feb 2013debug1: reading configuration data /etc/ssh/ssh_configdebug1: /etc/ssh/ssh_config line 49: applying options for *debug2: ssh_connect: needpriv 0debug1: connecting to server1 [10.1.1.10] port 22.debug1: connection established.debug3: incorrect rsa1 identifierdebug3: could not load /home/john/.ssh/id_rsa as a rsa1 public keydebug1: identity file /home/john/.ssh/id_rsa type 1debug1: identity file /home/john/.ssh/id_rsa-cert type -1debug1: identity file /home/john/.ssh/id_dsa type -1debug1: identity file /home/john/.ssh/id_dsa-cert type -1debug1: identity file /home/john/.ssh/id_ecdsa type -1debug1: identity file /home/john/.ssh/id_ecdsa-cert type -1debug1: enabling compatibility mode for protocol 2.0debug1: local version string ssh-2.0-openssh_6.4debug1: remote protocol version 2.0, remote software version openssh_6.4debug1: match: openssh_6.4 pat openssh*debug2: fd 3 setting o_nonblockdebug3: load_hostkeys: loading entries for host server1 from file /home/john/.ssh/known_hostsdebug3: load_hostkeys: found key type ecdsa in file /home/john/.ssh/known_hosts:2debug3: load_hostkeys: loaded 1 keysdebug3: load_hostkeys: loading entries for host server1 from file /etc/ssh/ssh_known_hostskey_read: type mismatch: encoding errordebug3: load_hostkeys: loaded 0 keys...... omitted some output...debug2: kex_parse_kexinit: none,<email> kex_parse_kexinit: none,<email> kex_parse_kexinit:debug2: kex_parse_kexinit:debug2: kex_parse_kexinit: first_kex_follows 0debug2: kex_parse_kexinit: reserved 0debug2: mac_setup: found <email> kex: server->client aes128-ctr <email> nonedebug2: mac_setup: found <email> kex: client->server aes128-ctr <email> nonedebug1: sending ssh2_msg_kex_ecdh_initdebug1: expecting ssh2_msg_kex_ecdh_replydebug1: server host key: ecdsa 45:9e:70:1d:89:49:d9:dd:ed:df:4b:b0:56:6e:11:31debug3: load_hostkeys: loading entries for host server1 from file /home/john/.ssh/known_hostsdebug3: load_hostkeys: found key type ecdsa in file /home/john/.ssh/known_hosts:2debug3: load_hostkeys: loaded 1 keysdebug3: load_hostkeys: loading entries for host server1 from file /etc/ssh/ssh_known_hostskey_read: type mismatch: encoding errordebug3: load_hostkeys: loaded 0 keysdebug3: load_hostkeys: loading entries for host 10.1.1.10 from file /home/john/.ssh/known_hostsdebug3: load_hostkeys: found key type ecdsa in file /home/john/.ssh/known_hosts:2debug3: load_hostkeys: loaded 1 keysdebug3: load_hostkeys: loading entries for host 10.1.1.10 from file /etc/ssh/ssh_known_hostskey_read: type mismatch: encoding errordebug3: load_hostkeys: loaded 0 keysdebug1: host 'server1' is known and matches the ecdsa host key.debug1: found key in /home/john/.ssh/known_hosts:2debug1: ssh_ecdsa_verify: signature correctas per request here is the file output. $cat /etc/ssh/ssh_known_hostsserver1,server1,server1.example.com,10.1.1.10 ssh-dss aaaab3nzac1yc2eaaaadaqabaaabaqcjezfdesyp4xtjslnxevg0arhpaddsmfumo/lbuoet0p31qanbfs3lvvc4ep/ziipjuifzdaket3kb+4zmioiwr2po67c9ddy4ztasvozv1kl7eihkyxnijimxhymrm+mqbtibjww5nb9srff/tqsbanicxqmzzyco1yo7b95xzr5fko3lle8mr5lvuxwmnlzeu/+9vw69rxwbl+jnrjt2ydv61h23bsl3rez9zvpemvgf+dkgqxdbp9ao2gftwlvx96e2/enmwy2a/2kulb9twkgt7gi5vucep1ia4eshid9wxxhjn/iuw3k/vfzqsdtvizg72dqkkpaberxgj83v",
    "present_kp": [
      "ssh"
    ],
    "absent_kp": [
      "linux",
      "key authentication"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what's an elegant and oop way to create a tree from arrays and render it as a nested ul. i have a series of arrays which represent file system paths, so each next value is actually a directory deeper, for example:var a1 = [desktop, pictures, summer 2011];is the equivalent of desktop|-pictures |-summer 2011i'm trying to find an elegant way to:flatten/merge all the different arrays i have to come up with one object/dictionary/multi-dimensional-array.parse the result and render a nested ul (html list) to the page which represents the hierarchy correctly.write what i already have in a more 'oop way'..i already have a working version:<!doctype html><html> <head></head> <body onload=init()> <ul id=tree></ul> <script> var a = [ desktop, folder1, innerfolder1 ]; var b = [ desktop, folder1, inner 2 ]; var c = [ documents, folder 2, innerfolder1, even deepper ]; var d = [ something else, folder1, inner 2 ]; var all = [a, b, c, d]; var tree = {}; var ul = document.getelementbyid(tree); function init(){ for ( var i = 0; i < all.length; i++ ){ addtotree(tree, all[i] ); } createlist(tree, ul); } //function from <url> function addtotree(tree, array) { for (var i = 0, length = array.length; i < length; i++) { tree = tree[ array[i] ] = tree[ array[i] ] || {}; } } function createlist( obj, _pushto ){ for ( attr in obj ){ var _doihavechildren = function ( ){ for(var prop in obj[attr]) { if (obj[attr].hasownproperty(prop)) { return true; } } return false; } var li = document.createelement(li); li.id = attr.tostring(); li.innerhtml = attr.tostring(); if ( tree.hasownproperty(attr) ){ ul.appendchild(li); } else { _pushto.appendchild(li); } if ( _doihavechildren() === true ){ var ul2 = document.createelement(ul); li.appendchild(ul2); } createlist( obj[attr], ul2 ); } } </script> </body></html>thanks!edit:i'm starting from strings which represent the paths i.e. desktop/pictures/summer 2011 which are broken to arrays.",
    "present_kp": [],
    "absent_kp": [
      "javascript",
      "object oriented"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "not receiving notifications when getting likes on a comment on a page's post on facebook. when i receive likes on my comment that i posted on the post of a page, i don't receive notifications. where can i change these settings?",
    "present_kp": [
      "facebook"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "privacy concern to track every user actions in your website?. we are building a system that track every user (logged in) actions, e.g. click, pageview, duration etc and store into a database.so all the user activities will be known and we are using them to create information our crm so that we know more about our user, e.g. which products they have visited and how often they view that page.my questions:are there any privacy concern since we identify the user, not just analytics.is it legal in countries such as eu?what thing we need to pay attention to the data we gathered?",
    "present_kp": [
      "analytics",
      "privacy",
      "crm"
    ],
    "absent_kp": [
      "security",
      "ecommerce"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "apt-get : no installation candidate for libxul-dev. when i'm installing libxul-dev i end with this error message:$ sudo apt-get install libxul-devreading package lists... donebuilding dependency tree reading state information... donepackage libxul-dev is not available, but is referred to by another package.this may mean that the package is missing, has been obsoleted, oris only available from another sourcee: package libxul-dev has no installation candidatehow can i solve this?sources.list",
    "present_kp": [
      "apt"
    ],
    "absent_kp": [
      "ubuntu",
      "software installation",
      "package management"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "$! is used for?. while going through:info coreutils 'dd invocation'i came across:dd if=/dev/zero of=/dev/null count=10mb & pid=$!what is $! used for?",
    "present_kp": [],
    "absent_kp": [
      "shell"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "logging using microsoft account. i would like to create wordpress website with forum where people will be able to login with microsoft account. i would like to do this to authorize their gamertags from xbox live. is that even possible?",
    "present_kp": [
      "wordpress",
      "microsoft"
    ],
    "absent_kp": [
      "authentication"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to make sure facebook knows that you're a publisher, not a brand?. the past couple facebook edgerank updates have penalized blatant advertisers & linkbait publishers like upworthy, making it more difficult for many brand pages to get organic distribution. and it appears it will soon get even more difficult to achieve organic reach, especially for brand pages.how does facebook determine what pages are owned by brands, and which are owned by publishers of substantive content. are pages manually reviewed and added to a whitelist? or is it all algorithmically determined.and is there anything a page manager can do to ensure his/her posts are viewed by facebook as content and not an ad?",
    "present_kp": [
      "facebook"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "pcie bus error when booting archiso and when using wifi-menu. i'm trying to install arch linux on a acer spin 5 laptop. i'm booting the latest archiso from a usb-stick in uefi mode and even before the system has fully started these errors appear during the boot sequence:[...] pcieport 0000:00:1c.0: pcie bus error: severity=corrected, type=physical layer, id=00e0(receiver id)[...] pcieport 0000:00:1c.0: device [8086:9d16] error status/mask=<phone>/<phone>[...] pcieport 0000:00:1c.0: [0] receiver error[...] pcieport 0000:00:1c.0: pcie bus error: severity=corrected, type=physical layer, id=00e0(receiver id)[...] pcieport 0000:00:1c.0: device [8086:9d16] error status/mask=<phone>/<phone>[...] pcieport 0000:00:1c.0: [0] receiver error (first)and lscpi tells me that 0000:00:1c.0 belongs topci bridge: intel corporation sunrise point-lp pci express root port #7 (rev f1)these errors also appear (sometimes) when using wifi-menu to connect to my wifi. sometimes this error does not occur at all, and sometimes it's spamming my shell.some times the error code also is [12] replay timer timeout and sometimes [6] bad tlp, but i don't know what it depends on.does someone know what might cause this error and how to fix it?it's very annoying and hindering me from installing arch.",
    "present_kp": [
      "linux",
      "arch linux",
      "pci",
      "acer"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "problem when trying a simple loop in bash. #!bin/sha=0while[$a -lt 50]doecho $aa='expr $a+1'donei get infinite echos of expr $a+1. what am i doing wrong?",
    "present_kp": [
      "bash"
    ],
    "absent_kp": [
      "shell script"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "specializing std::hash for std::array. i want to have an std::unordered_map which uses arrays as keys. as long as a type has an operator== and an std::hash it can be used as a key.here is my std::hash<std::array>template<class t, size_t n> struct std::hash<std::array<t, n>> { auto operator() (const std::array<t, n>& key) const { std::hash<t> hasher; size_t result = 0; for(size_t i = 0; i < n; ++i) { result = result * 31 + hasher(key[i]); // ?? } return result; }};i copied the multiply by 31 from somewhere. an example here uses << and ^. i don't know which is better or how to apply the cppreference example to an array.// cppreference exampletemplate<> struct hash<s> { typedef s argument_type; typedef std::size_t result_type; result_type operator()(argument_type const& s) const { result_type const h1 ( std::hash<std::string>{}(s.first_name) ); result_type const h2 ( std::hash<std::string>{}(s.last_name) ); return h1 ^ (h2 << 1); // or use boost::hash_combine (see discussion) } };it seems to work. all of the code compiles and the return values for std::hash<key_type> seem ok to me.using key_type = std::array<int, 2>;using hash_type = std::hash<key_type>;using map_type = std::unordered_map<key_type, int>;map_type map; // compileshash_type hasher; // compilesint main() { // test return values of std::hash<key_type> for(int y = -2; y < 3; ++y) { for(int x = -2; x < 3; ++x) { std::cout << hasher({x, y}) << ; } std::cout << std::endl; }}the values printed from this code are:184467440737095<phone>737095<phone>73709551614 29 60 184467440737095<phone>737095<phone>73709551615 30 61 184467440737095<phone>73709551585 0 31 62 184467440737095<phone>73709551586 1 32 63 184467440737095<phone>73709551587 2 33 64i'm not sure what makes a good hash function for std::unordered_map, so i'd appreciate any feedback on how to make this hash function better.",
    "present_kp": [
      "array"
    ],
    "absent_kp": [
      "c++",
      "hash map"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "website traffic - with no money (backlinks, spam, likes, tweets). i read a lot of things around the web.i saw people selling 4k backlinks and social real users tweets, likes, sharing links on facebook, twitter, instagram etc..now, assuming you have opened a site which is on beta stage and users needs invitations to signupassuming you are not an administrator of any huge facebook or social community (facebook groups, meetup groups, forums, chats etc..),assuming you don't have much money to spend for a huge campaign on paying ads,assuming you want to stay out from venture capitalists, angels and so on (at least to check if you can do it without external/third parties help)so...is trusted spam, or paying backlinks, the only way you can go for getting some traffic to your website?if not can you describe how can you promote your website on the web excluding all the points above?",
    "present_kp": [
      "backlinks",
      "traffic"
    ],
    "absent_kp": [
      "advertising",
      "website promotion"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "cross-browser double form submit prevention. we developed a potential solution for the double form submit prevention and we need some review on it. to be able to execute this code on asp.net we needed to add our function call directly into the onsubmit event of the form. this cannot be handle by jquery because asp.net use a dopostback function and called form.submit(). if onsubmit attribute of the form is empty then it wil not execute the code. we don't want to depend on a bloq-ui or disabled button actions.this is our form tag <form id=form1 runat=server onsubmit=return preventdoublesubmit(event);>and this is our javascript that handle the double submit prevention//double submit preventionsvar _preventdoublesubmit = false;function preventdoublesubmit(e) { if (_preventdoublesubmit) { return canceldoublesubmit(e); } else { _preventdoublesubmit = true; return true; }}function canceldoublesubmit(e) { if (!e) { e = window.event; } if (e.returnvalue != undefined) { e.returnvalue = false; } if (e.cancelbubble != undefined) { e.cancelbubble = true; } if (e.stoppropagation) { e.stoppropagation(); } if (e.stopimmediatepropagation) { e.stopimmediatepropagation(); } if (e.preventdefault) { e.preventdefault(); } return false;}//end - double submit preventionany review on this we be appreciated.",
    "present_kp": [
      "javascript",
      "asp.net"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "matplotlib: display y value as a marker. i created a plot on which the y value is visible with the marker.what's your opinion ? i wonder if i could have done something simpler. (code should run in a jupyter notebook)%matplotlib inlinefig = plt.figure()axes = fig.add_axes((1,1,1,1)) x = np.array([3,3.2,3.5,3.8,3.9,5,5.5,5.8,5.9,5])y = np.array([1,1,1,1,1,2,2,2,2,2])markers = ['^', 'o']for k, m in enumerate(markers): i = (y == k+1) flat = [1 for val in y[i]] axes.scatter(x[i], flat, marker=m)axes.set_xlim([2,7])axes.set_ylim([0,3])the result:",
    "present_kp": [
      "matplotlib"
    ],
    "absent_kp": [
      "python"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "ida proximity viewer not finding obvious paths?. using ida 6.2 (and also with ida 6.4), i'm trying out the proximity viewer to find the path between 2 functions as described at the hexblog post here.using the xrefs from/to (old option) it shows the clear path: allocatevolume -> volumesortcmp -> comparevolumecomponents as shown in the screenshot belowapart from the add name and hide childs options not existing in the context menu (as described in the blog) of the proximity browser as seen in the screenshot belowthe find path menu does list comparevolumecomponents in the dialog that opens (so it has some knowledge of what is reachable). however when i press search i expected a nice clean graph (as again shown in the blog and added as reference below) showing only the the 3 relevant nodes, but instead nothing seems to change to the proximity browser layout as i still see 30 something nodes. hexblog condensed find path example resultmy resultis the proximity viewer malfunctioning or are my expectations off? or am i doing something wrong here?",
    "present_kp": [
      "ida"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "caustics: methods to render?. what are the recommended ways to render caustics? i am aware bidirectional path tracing and photon mapping can do this. could someone give a brief overview of photon mapping and a good resource of where to learn about implementation?",
    "present_kp": [
      "photon mapping",
      "caustics"
    ],
    "absent_kp": [
      "pathtracing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why the difference between tomcat and tomcat7 regarding servlet mapping and defaults?. with the following configuration:<servlet-mapping> <servlet-name>default</servlet-name> <url-pattern>*.css</url-pattern></servlet-mapping><servlet-mapping> <servlet-name>default</servlet-name> <url-pattern>*.html</url-pattern></servlet-mapping>then both tomcat7 and tomcat6 serves the following url just fine: <url> if i reduce the configuration to just mention the css pattern:<servlet-mapping> <servlet-name>default</servlet-name> <url-pattern>*.css</url-pattern></servlet-mapping>then tomcat6 works, but tomcat7 fails with: the requested resource () is not available.why is there a difference here?",
    "present_kp": [
      "tomcat"
    ],
    "absent_kp": [
      "java",
      "localhost"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "protect memory from a potentially seg faulting function call. how can one safely call a function that might segfault without corrupting the stack or the heap?these so questions cover using signal handlers and setjmp.h to regain control.coming back to life after segmentation violationbest practices for recovering from a segmentation faultthey neglect the likely memory corruption that occurs prior to a seg fault.what strategies can be used to isolate the memory space of a function and its caller?this is just a curiosity question, there's no specific problem i'm trying to solve. let's just suppose we're programming something that absolutely cannot crash -- pacemaker, mars orbiter, nuclear launch control, take your pick. we've already thoroughly unit tested all our code and formally proven its correctness. for bureaucratic reasons we have to use c++ and linux.i was trying to sketch this out with clone(). my idea was to run the function with as much isolation as possible and pass data back and forth by squirreling it away at the bottom of the child's stack.is that reasonable or is there a better way to do this?",
    "present_kp": [
      "c++",
      "linux"
    ],
    "absent_kp": [
      "error handling"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is it possible to have a better sql editor for phpmyadmin?. i am studying database systems and using mysql and phpmyadmin.the editor that comes with phpmyadmin is plain editor that does not highlight any special syntax like for example: gedit in ubuntu.can i change the editor?",
    "present_kp": [
      "database",
      "phpmyadmin"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "developing a multistep form. i implemented a multistep form with the method described by ryan bates in ep217 but i had some weird behavior when refreshing or moving between the steps acts_as_good_style (old but still good) has a tip redirect when moving on that lead me to the change in the code that follows.in a nutshell, it says that if you're in the create action you should not render new (as i was doing).this solved the problem. but i had to manage the case of errors in the form, so i ended up with this code.#profiles_controller.rbdef create # [...] save etc [...] # render if @profile.new_record? # render 'new' # old session[:profile_valid] = @profile.errors.blank? # new redirect_to new_profile_path # new else # [...] endend def new @profile = profile.new(session[:profile_params]) # [...] # rebuild errors (see create) # check false because first time is nil and no error have to be displayed @profile.valid? if session[:profile_valid] == false session[:profile_valid] = trueendwhere in the new action i reload the errors otherwise lost depending on session[:profile_valid], that works fine.but this way doesn't look very good to me and i would appreciate to have your opinion, or how do you manage your controllers in multispets forms?what was the strange behavior? refreshing or going back and forth through the steps sometimes you jump to the wrong page, not clear what the logic is, probably depends on the validations in the model and the params hash.",
    "present_kp": [
      "form"
    ],
    "absent_kp": [
      "ruby",
      "ruby on rails"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what algorithms can be used to predict the outcome of a cricket match?. i am doing a project to predict the outcome of a cricket match, i have the data that states which matches were won by whom for odis. [espn data]which algorithm could be used to predict the outcome of the coming matches?would quadratic regression be a good idea? or would predicting based on probability algorithms such as the markov's algorithm is what is generally used?any other algorithm i should use?so basically, i want to know which algorithm i should use, i will implement in c++ ultimately but i will do it in r or python first.p.s.:-i am a newbie in this field, hence pardon if the question sounds too stupid.i have learnt regression so far in data analytics.",
    "present_kp": [
      "regression"
    ],
    "absent_kp": [
      "machine learning",
      "predictive modeling",
      "linear regression"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "phong shading in opengl: line of light from the center of the world. i implemented a phong shader in glsl, but there is a bug.what you are (supposed) to see down below:a point light source rotating around the center of the world,with a radiance of (0, 100, 0)a textured quad with kd of (1, 0, 0).the problem is:why is there a a green line to the center of the world instead of a green spot light where the quad is lit? (where the center of the point light source is)(btw in this case there would be only a yellow spot of light, because red + green = yellow)vertex shader:#version 330uniform mat4 modelmatrix, viewmatrix, projectionmatrix; uniform vec4 wlipos[5]; // pos of light source uniform vec3 weye; // pos of eyeuniform int lightsize; //number of lightslayout(location = 0) in vec3 vtxpos;layout(location = 1) in vec3 vtxnorm;layout(location = 2) in vec2 vtxuv;out vec2 texcoord;out vec3 wnormal; // normal in world spaceout vec3 wview[5]; // view in world spaceout vec3 wlight[5]; // light dir in world spacevoid main() { gl_position = projectionmatrix * viewmatrix * modelmatrix * vec4(vtxpos, 1); texcoord = vtxuv; vec4 wpos = modelmatrix * vec4(vtxpos, 1); wnormal = (vec4(vtxnorm, 0) * inverse(modelmatrix)).xyz; for(int i = 0; i < lightsize; i++) { wview[i] = weye * wpos.w - wpos.xyz; wlight[i] = wlipos[i].xyz * wpos.w - wpos.xyz * wlipos[i].w; }}fragment shader:#version 330uniform sampler2d samplerunit;uniform vec3 kd, ks, ka; // diffuse, specular, ambient refuniform vec3 la, le[5]; // ambient and point source raduniform float shininess; // shininess for specular refuniform int lightsize;in vec2 texcoord; in vec3 wnormal; // interpolated world sp normalin vec3 wview[5]; // interpolated world sp viewin vec3 wlight[5]; // interpolated world sp illum dirlayout(location = 0) out vec4 fragmentcolor;void main() { vec3 color = vec3(0, 0, 0); for(int i = 0; i < lightsize; i++) { vec3 n = normalize(wnormal); vec3 v = normalize(wview[i]); vec3 l = normalize(wlight[i]); vec3 h = normalize(l + v); float cost = max(dot(n,l), 0); float cosd = max(dot(n,h), 0); color += ka * la + (kd * cost + (ks * pow(cosd, shininess)) * le[i]) / pow(length(wlight[i]), 2); } vec3 gamma = vec3(1.0/2.2); fragmentcolor = vec4(pow(color * vec3(texture2d(samplerunit, texcoord)), gamma), 1);}",
    "present_kp": [
      "opengl",
      "shader",
      "glsl"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to explicitly set the tabname of a new gnome-terminal?. i want to explicity rename a tab in gnome-terminal on startup of the tab. i don't want to use gnome-terminal --title flag as that gets reset by my systems bashrc file after whatever else is supposed to run. i have used this command with success in a normal terminalexport prompt_command=echo -ne ']0;tabname'this command works fine to rename the current tab's name, but when i try to use it in conjunction with gnome-terminal execute command, i am not getting proper output.i have used gnome-terminal --e flag to execute simple commands with success, something like this will bring up a new terminal and echo hey then return to bashgnome-terminal -e bash -c 'echo hey';bashhere is what i am trying note the escaped marks that i addedtabname=export prompt_command=\\echo -ne ']0;tabname'\\gnome-terminal --tab --e bash -c $tabname;bashi always get weird output no matter how i change the quotes, but i think that is where the problem is.",
    "present_kp": [
      "bash"
    ],
    "absent_kp": [
      "shell script",
      "environment variables",
      "quoting",
      "gnome terminal"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "tips for managing internal and external links using wordpress. so i'm looking for ways to optimize my site for user and search engine purposes. i've read several articles and looked at several different plugins. to say the least, i'm thoroughly confused as what are the best practices for managing internal and external links. here is a list of some of my questions:which internal links should be set to nofollow?which external links should be set to nofollow?to what degree does actively managing links contribute to your pr?should you use nofollow blindly on all links in comments?if a link to an external site is broken (404 or whatever), should you nofollow that link? what about noindex?as you can see, lots of questions. i'm hoping that you experienced webmasters can give a newb some best-practice advice.",
    "present_kp": [
      "links"
    ],
    "absent_kp": [
      "seo"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "loading and querying a spark machine learning model outside of spark. i'm currently working on a project where we're building a data pipeline. we have spark setup and have generated models. sadly, loading the model in spark and querying it isn't fast enough for us. what's the most straightforward way of exporting a model and loading it into memory on a local server? i've researched pmml and some libraries and that appears to be one path.",
    "present_kp": [],
    "absent_kp": [
      "apache spark"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "access imagemagick's man page. on my system i have imagemagick and its documentation installed:$ apt-cache pkgnames imagemagickimagemagickimagemagick-6.q16imagemagick-dbgimagemagick-docimagemagick-commoni can access convert's man page, which tells me see also imagemagick(1) if i want to know more.man imagemagick says no manual entry for imagemagick.and finally, man -k imagemagick says:quantize (5) - imagemagick's color reduction algorithm.how do i access imagemagic's man page on my system?additional informationabove i was mistaken.apt-cache pkgnames listed packages are not necessarily available to download, installable or installed (ref. apt-cache(8)).so, my command did not list the installed packages.digging more, i ended up with$ apt list imagemagick* listing... doneimagemagick/xenial-updates,xenial-security,now 8:6.8.9.9-7ubuntu5.4 amd64 [installed]imagemagick-6.q16/xenial-updates,xenial-security,now 8:6.8.9.9-7ubuntu5.4 amd64 [installed,automatic]imagemagick-common/xenial-updates,xenial-updates,xenial-security,xenial-security,now 8:6.8.9.9-7ubuntu5.4 all [installed,automatic]imagemagick-dbg/xenial-updates,xenial-security 8:6.8.9.9-7ubuntu5.4 amd64imagemagick-doc/xenial-updates,xenial-updates,xenial-security,xenial-security 8:6.8.9.9-7ubuntu5.4 allso, apparently, imagemagick-doc was not installed.nevertheless, this package installs the www documentation (/usr/share/doc/imagemagick-doc/www) and not the man one.checking the content of the imagemagick package does not give away what the name of its man page is, or at least, i am not able to figure it out.$ dpkg -l imagemagick | grep man/usr/share/man/usr/share/man/man1/usr/share/man/man1/stream-im6.1.gz/usr/share/man/man1/display-im6.1.gz/usr/share/man/man1/animate-im6.1.gz/usr/share/man/man1/mogrify-im6.1.gz/usr/share/man/man1/composite-im6.1.gz/usr/share/man/man1/montage-im6.1.gz/usr/share/man/man1/import-im6.1.gz/usr/share/man/man1/identify-im6.1.gz/usr/share/man/man1/conjure-im6.1.gz/usr/share/man/man1/convert-im6.1.gz/usr/share/man/man1/compare-im6.1.gzbug reports hereimagemagick: i've opened a bug report here.debian: another bug report here.let's see if someting comes up.",
    "present_kp": [
      "man",
      "imagemagick",
      "documentation"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "opengraph tags and html5 validity. i have a html5 based page, and i inculded the opengraph tags according to it's documentation. also i checked with facebook debug, and it can parse the metadata.but when i use w3c validator, it reports the og tags as error:attribute content not allowed on element meta at this point.<meta property=fb:admins content=.... />attribute content not allowed on element meta at this point.<meta property=og:url content=http://www....>they are all in the <head>.i would need my page be valid html5 and og tags, as well. could you help me giving a hint how can it be achieved?update:the name version also invalid: <meta name='fb:admins' content=''>",
    "present_kp": [
      "facebook",
      "html5"
    ],
    "absent_kp": [
      "validation",
      "facebook graph"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how can i visualize/analyze data that has percentages and numbers across multiple time periods?. i have a large dataset like this:+-----+------+---------+-----------+------------------+-------+-----------------+---------+-------+| mat | cust | qty(ag) | net v(ag) | demand hits (ag) | qty% | demand hits (%) | net v % | month |+-----+------+---------+-----------+------------------+-------+-----------------+---------+-------+| y | a | 200 | 7000 | 200 | 76.9% | 95.7% | 76.9% | 1 || x | a | 100 | 1000 | 10 | 83.3% | 66.7% | 33.3% | 1 || y | c | 50 | 1750 | 8 | 19.2% | 3.8% | 19.2% | 1 || x | b | 20 | 2000 | 5 | 16.7% | 33.3% | 66.7% | 1 || y | b | 10 | 350 | 1 | 3.8% | 0.5% | 3.8% | 1 || y | a | 600 | 21000 | 78 | 86.0% | 86.7% | 86.0% | 2 || x | b | 60 | 6000 | 56 | 0.6% | 50.5% | 0.6% | 2 || x | c | 10000 | <phone> | 45 | 98.9% | 40.5% | 98.9% | 2 || y | b | 98 | 3430 | 12 | 14.0% | 13.3% | 14.0% | 2 || x | a | 50 | 5000 | 10 | 0.5% | 9.0% | 0.5% | 2 |+-----+------+---------+-----------+------------------+-------+-----------------+---------+-------+this data is about sale of materials (mat) to customers (cust). for each mat-cust combination i have the qty they ordered (qty), net value of their orders (net v) and the number of orders (demand hits) and the month (month). i also have percentages of orders, net value, and quantity. for example: in the line one (mat y and cust a) the qty% is 76.9%. this means that 76.9% of the total quantity of material y was ordered by customer a. so if you sum the qty % of mat y-cust a and mat y - cust c and mat y - cust b for month 1 it will add up to a 100%. the same has been done for the other percentages as well. i was looking for a good way to visualize this data (that could run into millions of records even for one month) and also to statistically analyse it to draw any conclusions. any ideas for an exploratory visual and statistical analysis?here are some questions i am thinking about?:are there customers who make up a large portion of the sales of particular items? are there specific mat-cust combos that can be considered critical based on demand hits or net value? can i cluster mat-cust into different categories and treat each category differently based on their unique characteristics? is customer buying patterns repeating across time periods?",
    "present_kp": [],
    "absent_kp": [
      "machine learning",
      "data mining",
      "clustering",
      "statistics",
      "visualization"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "blog.domain.com(domain.com/blog) or another-domain.org. i have a website which is going to show my shareware, another site for my blog. i will post all my thinkings/development tips/design tips/business experience on my blog, but most of them are not related to my shareware. software users may not interested in my blog. in this case, is it still better to host my blog to another domain?thanks.",
    "present_kp": [
      "blog",
      "business"
    ],
    "absent_kp": [
      "domains"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "debug a .dll file within powershell. i have given a .ps1 file which loads a .dll via:import-module .\\decrypter.dllafter that, a call to that module is performed by:get-decrypt( *some base64 encoded string* )only the .dll is given. the dependency walker returns no exported functions. ida pro free shows only one modulemy question:how do i debug this .dll file?kindly regards",
    "present_kp": [
      "dll"
    ],
    "absent_kp": [
      "debugging"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "osx - dd command behaving weirdly. i've been using dd for a couple of years now, most of the time successfully. i usually use it to dd linux images to usb drives. my workflow doing so is as such:convert .iso image to a .img format using hdiutil convert -format udrw output.img input.isounmount the drive using diskutil unmountdisk /dev/diskxdd the image to the drive using dd if=./image.img of=/dev/rdiskx bs=1mlately, and namely since i upgraded to yosemite, dd has been showing some weird behavior. when i specify the block-size flag (bs=1m), it quits after about a minute, showing the default successful write message with the numbers of records in/out and bytes transferred and all that. what you would expect on a successful write. the problem is, it writes just a very small part of the image, then quits.when i do not specify the block-size flag dd behaves as expected, writing the image to the disk in the ordinary fashion. does anyone have any idea why this is happening?",
    "present_kp": [
      "osx",
      "dd"
    ],
    "absent_kp": [
      "command line"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "my parked domain was de-indexed by google - what to do?. i have a question about how to handle my domain. in a nutshell, i bought a domain last year from go daddy. my intention was to launch a real site with this domain and i have spent the last year working on my site. for the last year, i have been using the default go daddy page display for an up and coming site.when i first bought this site, it was indexed by google - you could search for alphabanter and my site would show up on the search result page for google. several months ago, it seemed google de-indexed my domain and if you type alphabanter, my domain no longer shows up on the list of search results. however, if you search for <url>, that's the only way it shows up in the search results for google.anyways, i am about to launch my site for real. however, i don't quite know if i can get my site back into google's index. i have a few questions:1) was my domain permanently penalized by google and removed from their index just because it was a parked domain? i don't believe i have done anything abusive other than using the go daddy default page for almost a year because my site was not ready.2) should i just launch my site, put a few backlinks to my site, and hope that google indexes my site again?3) should i submit my site to google at google submit your contenti assume getting google to reconsider my site is the last option if none of the above works.",
    "present_kp": [
      "google index"
    ],
    "absent_kp": [
      "seo",
      "google search"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "are hdf5 files suitable for git revision control?. i am not familiar with the file format used in hdf5, but i am wondering if hdf5 files are suitable for revision control with git (or for example mercurial or subversion)? i guess what i mean is: are hdf5 files suitable for line-based diff'ing or will git have to treat an hdf5 as one big binary and store an entire copy for each revision?",
    "present_kp": [],
    "absent_kp": [
      "data management",
      "data storage"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "test for existence of multiple files, given by pipe. i have a command that gives me a list of files, one on each line. filenames are normal - no spaces, no need to escape parentheses etc.now i want to pipe that command to something like test -f and return true if and only if all of the files exist. (behaviour with 0 lines can be undefined, i don't really care.)so, something likemake_list_of_files | test -fbut actually working.bashisms are allowed, since i need it in bash.the files are not in the same directory, but they are in subdirectories of a current directory, and the paths have directory names in them, so for exampledir/file1dir/file2dir2/file3",
    "present_kp": [
      "bash",
      "files"
    ],
    "absent_kp": [
      "shell script"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "even distribution through a chain of resources. i'm working on an algorithm which routes tasks through a chain of distributed resources based on a hash (or random number).for example, say you have 10 gateways into a service which distribute tasks to 1000 handlers through 100 queues. 10,000 connected clients are expected to be connected to gateways at any given time (numbers are very general to keep it simple).thats10,000 clients 10 gateways (producers)100 queues 1000 workers/handlers (consumers)the flow of each task is client->gateway->queue->workereach client will have it's own hash/number which is used to route each task from the client to the same worker each time, with each task going through the same gateway and queue each time. yet the algorithm handles distribution evenly, meaning each gateway, queue, and worker will have an even workload.my question is what exactly would this be called? does such a thing already exist? this started off as a dht, but i realized that dhts can't do exactly what i need, so i started from scratch.",
    "present_kp": [],
    "absent_kp": [
      "algorithms",
      "scalability",
      "statistics"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "1 fastcgi sent in stderr: php message: php fatal error: session_start(): failed to initialize storage module:. i installed phpmyadmin on my arch linux box. i then created a sym link named phpmyadmin in my /usr/share/nginx/html/ to /usr/share/webapps/phpmyadmin/. when i visit <url> in my browser i get 500 internal server error. checking in my nginx error log this' what i find:2017/06/13 19:10:42 [error] 4059#4059: *1 fastcgi sent in stderr: php message: php fatal error: session_start(): failed to initialize storage module: files (path: ) in /usr/share/webapps/phpmyadmin/libraries/session.inc.php on line 121 while reading response header from upstream, client: 127.0.0.1, server: localhost, request: get /phpmyadmin/ http/1.1, upstream: fastcgi://unix:/var/run/php-fpm/php-fpm.sock:, host: 127.0.0.1:8080please how do i solve this error? how do i get my phpmyadmin working?if you need my nginx.conf or php.ini let me know. thanks.",
    "present_kp": [
      "arch linux",
      "php",
      "phpmyadmin"
    ],
    "absent_kp": [
      "mariadb"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "create facebook ad for people who like specific group or page (targetting)?. i would like to boost a specific post and target people who liked a group or page that i do not own.can this be done or it's forbidden to prevent spam (or whatever)?i used targeted advertising to create such group, but i was offered to (in/ex)clude people for my own page only.",
    "present_kp": [
      "facebook"
    ],
    "absent_kp": [
      "facebook ads"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to set date with epoch format. i want to set date from a seconds since epoch value, for instance i want to set date with input value <phone>.i read through date -help but not found anything.is there any parameters to do it?",
    "present_kp": [
      "date"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "why is an email failing rfc2822 specifications?. i am sending an email to a maximum of 14 gmail users. they are part of a small group of paid subscribers. in a send to all 14 1 or 2 are normally returned sayingreason: remote host said: 550 5.7.1 rfc 2822 specifications for more informationit is not an email to the same member each time and seems random. has anyone ever expereinced this before?",
    "present_kp": [
      "email",
      "gmail"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "minimum scalar product. here's the problem statement for minimum scalar productyou are given two vectors v1=(x1,x2,...,xn) and v2=(y1,y2,...,yn). the scalar product of these vectors is a single number, calculated as x1y1+x2y2+...+xnyn.suppose you are allowed to permute the coordinates of each vector as you wish. choose two permutations such that the scalar product of your two new vectorsis the smallest possible, and output that minimum scalar product. please review my code.#include<iostream>#include<vector>#include<algorithm>int main() { int t; std::cin >> t; while (t--) { std::size_t n; std::cin >> n; if (n != 0) { int product = 0; std::vector<int> x(n); for (std::size_t x_index = 0; x_index < n; ++x_index) { std::cin >> x[x_index]; } std::vector<int> temp(n); for (std::size_t t_index = 0; t_index < n; ++t_index) { std::cin >> temp[t_index]; } sort(x.begin(),x.end()); sort(temp.begin(),temp.end()); std::vector<int> y(n); for (std::size_t y_index = 0; y_index < n; ++y_index) { y[y_index] = temp[temp.size()-1-y_index]; } for (std::size_t i = 0; i < n; ++i) { product += x[i]*y[i]; } std::cout << product << ' '; } }}how can i make this code better and faster? particularly without using an additional vector? and is there a better solution?",
    "present_kp": [],
    "absent_kp": [
      "c++",
      "c++11",
      "programming challenge"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "getting a value from shared preferences. i use this code to get values from sharedpreferences.public static string getsharedpreferencevalue(context context, string key) { sharedpreferences sp = context.getsharedpreferences( utility.preferences_name, context.mode_private); string value = sp.getstring(key, utility.empty_string); return value;}will this static method keep context (activity) all time and are there memory leaks?is there a better way to get values from sharedpreferences?i'm looking for the best way to do this.",
    "present_kp": [],
    "absent_kp": [
      "java",
      "performance",
      "android"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "opening social media links in a new window. i'm rather new to jquery but managed to get some social links to open in a new controlled window instead of new tab. i have a few functions that do the same thing, and this works, but there is always a better method.i did review the question and answer here and couldn't figure out which would apply to this case.jquery(document).ready(function ($) { $(#ssba_facebook_share).attr('title','opens in a new pop-up'); $(#ssba_facebook_share).click(function(){ w = parseint((screen.width - 600)/2); h = parseint((screen.height - 400)/2); cwin = window.open($(this).attr('href'), 'closewin', 'status=0,toolbar=0,location=0,menubar=0,directories=0,resizable=0,scrollbars=0,height=400,width=600'); cwin.moveto(w,h); return false; }); $(#ssba_twitter_share).attr('title','opens in a new pop-up'); $(#ssba_twitter_share).click(function(){ w = parseint((screen.width - 600)/2); h = parseint((screen.height - 400)/2); cwin = window.open($(this).attr('href'), 'closewin', 'status=0,toolbar=0,location=0,menubar=0,directories=0,resizable=0,scrollbars=0,height=400,width=600'); cwin.moveto(w,h); return false; }); $(#ssba_google_share).attr('title','opens in a new pop-up'); $(#ssba_google_share).click(function(){ w = parseint((screen.width - 600)/2); h = parseint((screen.height - 400)/2); cwin = window.open($(this).attr('href'), 'closewin', 'status=0,toolbar=0,location=0,menubar=0,directories=0,resizable=0,scrollbars=0,height=400,width=600'); cwin.moveto(w,h); return false; });});now, i've attempted a few things to no avail, and one method was this:jquery(document).ready(function ($) { $(#ssba_facebook_share,#ssba_twitter_share,#ssba_google_share).attr('title','opens in a new pop-up'); $(this).click(function(){ w = parseint((screen.width - 600)/2); h = parseint((screen.height - 400)/2); cwin = window.open($(this).attr('href'), 'closewin', 'status=0,toolbar=0,location=0,menubar=0,directories=0,resizable=0,scrollbars=0,height=400,width=600'); cwin.moveto(w,h); return false; });});does anyone have any pointers in this, if possible, with a description of where i went wrong? or even post a link to a question that goes over this in laymen terms? (i've been looking around and found similar posts, which is how i arrived at where i am now, but still having trouble making complete sense of it.)",
    "present_kp": [
      "jquery"
    ],
    "absent_kp": [
      "javascript"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "serialize name/value pairs. i have an array of objects that each have a name/value property. the array can contain multiple objects with the same name property. i want to serialize this array into the form: name:value,value|name:value|name:value,value,valueso basically each property is separated by a | and each value by a ,. the above example is not exactly how i want it to look, just an example of the syntax, results will vary based upon input.here is an example of the array:var objs = [ {name:name1, value: value1}, {name:name4, value: value4}, {name:name3, value: value3}, {name:name2, value: value2}, {name:name2, value: value2}, {name:name2, value: value3}];i wrote some code to perform this serialization, but i wanted to see if anyone could suggest improvements to this code, i know there are some javascript experts on this site and wanted to get their opinion:function serialize(objs){ var out = ; for(var i = 0; i < objs.length; i++){ var propkey = objs[i].name + :; if (out.indexof(propkey) == -1){ out += | + propkey; } var position = out.indexof(propkey) + propkey.length; out = out.substring(0, position) + objs[i].value + , + out.substring(position); } return out.substring(1,out.length-1).replace(/\\,\\|/g,|);}js fiddle: <url> jquery is acceptable.",
    "present_kp": [
      "javascript",
      "jquery"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "downloading all files in a ftp folder and then deleting them. i've created two methods in a class that allow me to download the contents of an ftp folder and if specified then delete them. although the code operates as intended i think it's vastly bloated and i'm looking for direction on how to correct it.of note i believe that there's far too many ftp connections opened in a single file move (three i believe - list directory, download files, delete files). i'm a bit unsure how to refine this.public void downloadfolder(string localfilespath, string remoteftppath, bool deleteafterdownload = false){ remoteftppath = ftp:// + hostname + remoteftppath; var request = (ftpwebrequest)webrequest.create(remoteftppath); request.method = webrequestmethods.ftp.listdirectory; request.credentials = new networkcredential(username, password); request.proxy = null; ftpwebresponse response = (ftpwebresponse)request.getresponse(); stream responsestream = response.getresponsestream(); streamreader reader = new streamreader(responsestream); list<string> directories = new list<string>(); var line = reader.readline(); while (!string.isnullorempty(line)) { directories.add(line); line = reader.readline(); } reader.close(); using (var ftpclient = new webclient()) { ftpclient.credentials = new networkcredential(username, password); for (var i = 0; i <= directories.count - 1; i++) { if (!directories[i].contains(.)) { continue; } var path = remoteftppath + / + directories[i].remove(0, directories[i].indexof(@/) + 1); var transferpath = localfilespath + @\\ + directories[i].replace(@/, @\\); postevent($attempting to download file: {path} to: {transferpath}, debug); try { ftpclient.downloadfile(path, transferpath); postevent($downloaded file: {path} to: {transferpath}, info); postevent($preparing to delete file: {path} , debug); if (deleteafterdownload) { deletefile(path); } } catch (webexception ex) { postevent($error downloading or deleting file {path} to {transferpath}, error); postevent($exception: {ex.message}, error); } catch (exception ex) { postevent($general exception: {ex.message}, error); } } } response.close();}public void deletefile(string remoteftppath){ ftpwebrequest request = (ftpwebrequest)webrequest.create(remoteftppath); request.method = webrequestmethods.ftp.deletefile; request.credentials = new networkcredential(username, password); request.proxy = null; ftpwebresponse response = (ftpwebresponse)request.getresponse(); postevent($deleting file {remoteftppath} returned status {response.statusdescription}, debug); response.close();}here's the class initializer (where my credentials are passed into properties within this class):public ftphelper(string ftphostname, string ftpusername, string ftppassword){ hostname = ftphostname; username = ftpusername; password = ftppassword;}and the postevent method that's referred to here come's from a base class i inherit to this class (unsure if this is bad practice, please let me know if so):public class basehelper{ private eventhandler<baseexceptioneventargs> _onevent; public event eventhandler<baseexceptioneventargs> oneventhandler { add { _onevent += value; } remove { _onevent += value; } } /// <exception cref=exception>a delegate callback throws an exception.</exception> public void postevent(string message, baseexceptioneventargs.exceptionlevel exceptionlevel, exception exception = null) { if (_onevent == null) return; if (exception == null) { var e = new baseexceptioneventargs(message, exceptionlevel); _onevent(this, e); } else { var e = new baseexceptioneventargs(message, exceptionlevel, exception); _onevent(this, e); } }}",
    "present_kp": [],
    "absent_kp": [
      "c#",
      "network file transfer"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "when do environment variables get set in tcsh?. i set a lot of environment variables in my .tcshrc file, using the setenv command.when i needed to have one of these unset today, i moved this file and opened a new terminal (this is all in a gnome graphical environment) expecting environment variables i set in my .tcshrc to no longer be in the env.but some of these variables were still set; where else could they be being set? i know my .login file is empty.are setenv commands more global than i think they are? when i open a new virtual terminal via alt-ctrl-f2 the variables were no longer set.",
    "present_kp": [
      "gnome",
      "tcsh"
    ],
    "absent_kp": [
      "fedora"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "submitting jobs to get 3 nodes running parallel executions. i have a submit script looks like below, it tries to run a large number of instances of csce.py in backgrounds with 3 nodes.... in a laptop, this usually could successfully automatically distribute all the background tasks into 16 cores.... however, i am not sure if in a cluster, it would also automatically distribute the 4*13*9 tasks in 3 nodes (48 cores).#!/bin/bash#sbatch -n 3 # total number of nodes requested (16 cores/node)#sbatch -n 48 # total number of mpi tasks requestedfor simplify in 0.1 0.15 0.2 0.25do for lmbda in 0.5 1 2 5 10 20 50 100 <phone> 2000 5000do for mu in 0.005 0.01 0.05 0.1 0.5 1 5 10 50 do rm eci.outcsce.py --mu $mu --lmbda $lmbda --simplify $simplify --favor-low-energy 0.01 --bias-stable --save-energies lmbda_$lmbda\\_mu_$mu\\_simplify_$simplify\\_ce-energies.dat --save-weights lmbda_$lmbda\\_mu_$mu\\_simplify_$simplify\\_ce-weights.dat --casm-eci-file eci.in lmbda_$lmbda\\_mu_$mu\\_simplify_$simplify\\_eci.out --save-hull lmbda_$lmbda\\_mu_$mu\\_simplify_$simplify\\_ce-hull.dat --preserve-ground-state 10000 2> lmbda_$lmbda\\_mu_$mu\\_simplify_$simplify\\_error 1> lmbda_$lmbda\\_mu_$mu\\_simplify_$simplify\\_output &done done donewait",
    "present_kp": [],
    "absent_kp": [
      "parallelism",
      "batch jobs"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "freebsd 10.2 - is three-way hast replication possible?. i have a pair of freebsd 10.2 servers using hast to keep a volume replicated between them. i'm introducing two new storage nodes that have much newer hardware and will eventually replace the existing servers but i want to introduce replication gradually without disrupting the existing setup.i'm attempting to use a configuration like this:# /etc/hast.conf# hast - highly available storagecompression lzfchecksum crc32timeout 10replication memsyncresource ha0 { # new storage node #1 on zsan1 { local /dev/zvol/vmdata/targets/ha0 remote 10.10.30.11 remote 10.10.30.12 remote 10.10.30.14 } # new storage node #2 on zsan2 { local /dev/zvol/vmdata/targets/ha0 remote 10.10.30.11 remote 10.10.30.12 remote 10.10.30.13 } # old storage node #1 on zfs-primary { local /dev/zvol/tank/targets/ha0 remote 10.10.30.12 remote 10.10.30.13 remote 10.10.30.14 } # old storage node #2 on zfs-secondary { local /dev/zvol/tank/targets/ha0 remote 10.10.30.11 remote 10.10.30.13 remote 10.10.30.14 }}i have created new volumes to test the replication with this configuration on the new servers and the existing ones but when i run hastctl on the primary node it only shows a single 'remoteaddr':root@zsan1:~ # hastctl listha0: role: primary provname: ha0 localpath: /dev/zvol/vmdata/targets/ha0 extentsize: <phone> (2.0mb) keepdirty: 64 remoteaddr: 10.10.30.14 replication: memsync status: degraded workerpid: 7229 dirty: <phone> (3.2gb) statistics: reads: 1649 writes: 26117 deletes: 0 flushes: 27 activemap updates: 1651 local errors: read: 0, write: 0, delete: 0, flush: 0 queues: local: 0, send: 0, recv: 0, done: 0, idle: 255i would appreciate it if someone could help me out here, either what i'm doing is unsupported or i'm doing it wrong. thank you!",
    "present_kp": [
      "freebsd",
      "storage"
    ],
    "absent_kp": [
      "cluster",
      "high avability"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to use mock objects [c++] without passing them as arguments to functions. i'm in the process of integrating a unit testing framework for an existing code base in c++. i've zeroed down on cxxtest, but as it turns out we can use other mocking frameworks (like googlemock) in conjunction with cxxtest too.after reading tutorials on cxxtest (mocking) and googlemock (the infamous turtle example), i have the general idea that you have to define a mock class (using macros etc), declare an object of the mock class and then pass it to the function you are unit testing. now, there are many occurences in the existing code base where it isn't possible to do that.here's an pseudo example to clarify:class ccandidatefortest{ public: bool foo(int a) { canotherclass obj; int b = obj.bar(a+2); if (a == b) { return true; } else { return false; } }}(this is over-simplified, there are also exceptions and the primitive types may also be other objects etc.; but you get the general idea) (also, creation of objects is not always direct and may sometimes use a factory)i want to write a test for the function ccandidatefortest::foo. this method internally creates an object of canotherclass, which i need to mock so that canotherclass::bar returns different values so that different code paths in foo are traversed in unit testing.in a nutshell, the problem is that the function being tested internally creates an object of the class that needs to be mocked - hence passing an instance of the mocked object to the function is not possible.how do i use mocking in such a case? is there a specific mocking framework that makes this possible?",
    "present_kp": [
      "c++",
      "unit testing",
      "mocking"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "systemd-nspawn os container is unusable because i can't set the root password. i combined the detailed instructions from the original blog post, and the more up to date instructions from the man page (using dnf instead of yum).# sudo dnf -y --releasever=24 --installroot=$home/fedora-24 --disablerepo='*' --enablerepo=fedora --enablerepo=updates install systemd passwd dnf fedora-release vim-minimal# sudo systemd-nspawn -d fedora-24spawning container fedora-24 on /home/alan-sysop/fedora-24press ^] three times within 1s to kill container.-bash-4.3# passwdchanging password for user root.new password:retype new password:result:passwd: authentication token manipulation errorand an avc popup, i.e. selinux error. it says passwd is not allowed to unlink (replace) /etc/passwd. one of the suggestions from the troubleshoot button is that i could assign the label passwd_file_t to /etc/passwd.what's wrong, how can i fix it?",
    "present_kp": [
      "fedora",
      "selinux",
      "container"
    ],
    "absent_kp": [
      "systemd nspawn"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "fedora and samba on virtualbox. i have linux fedora 18 installed as a vm guest.trying to share directories from the linux vm guest box to the windows 7 host. (note: virtualbox is installed on windows 7, and the linux virtual machine is created on it).i made my samba share using text on this link:<url> made all proper changes in /etc/samba/smb.conf, added proper user, etc. i even disabled selinux, and flushed the iptables configuration. samba is up and running of course. also, i can ping linux from windows.this is the error i'm getting:error code 0x800704cf the network location cannot be reached. for information about network troubleshooting, see windows helpthere are lots of other resources, like:<url> did what is described there, and still having same problem. in all of the cases, i have same error.can you give me some tips on what i'm doing wrong in this case?",
    "present_kp": [
      "linux",
      "fedora",
      "windows",
      "virtualbox",
      "samba"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "write name of file before a pattern inside that file. i have thousands of similar files and i'd like to write their respective names after a pattern inside them. for example:**file 1's name is nexus0000inside the file there is:>pseudomonas_1matgatccgcttcgagcaggtcggcaaacgctatc>pseudomonas_2mgtgagcttcgagcaggtcggcgagccgctatcand i want to get this:nexus0000>pseudomonas_1matgatccgcttcgagcaggtcggcaaacgctatcnexus0000>pseudomonas_2mgtgagcttcgagcaggtcggcgagccgctatc**file 2's name is nexus0001inside the file there is:>pseudomonas_1matgatccgcttcgagcaggtcggcaaacgctatc>pseudomonas_2mgtgagcttcgagcaggtcggcgagccgctatcand i want to get this:nexus0001>pseudomonas_1matgatccgcttcgagcaggtcggcaaacgctatcnexus0001>pseudomonas_2mgtgagcttcgagcaggtcggcgagccgctatcetc.up to this point, i've only managed to write the name of the file in the first line using: for file in nexus*; do echo $file$$(cat -- $file) > $file; donethanks for the help!",
    "present_kp": [],
    "absent_kp": [
      "linux",
      "bash",
      "bioinformatics"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "reserving database independence using spring jpa. we are planning to use spring boot with jpa for our next project and i am wondering how much flexibility jpa gives in reality. if we start developing using a self-hosted postgresql server and later want to change to amazon rds postgresql or sql server, will we run into a lots of problems? on paper, jpa should serve as an abstraction layer, but if we start to use database specific data types, views, functions/stored procedures we quickly lose the ability to change. generally what is the best practice? use only jpa even if it means performance loss or dive hard into database programming with plsql or t-sql and if platform change is needed, rewrites of those functions needs to happen.another problem is the data types. postgresql for example can be extended with ltree and it provides a good foundation for hierarchical data structures, but to use it, one need to extend jpa to use the ltree data type or solve every interaction with functions.",
    "present_kp": [
      "database",
      "spring",
      "jpa"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "insignificant input variable but high r2. can a linear regression model with a single input variable x with x not significant but the model has a high r2?can this exist? if yes, what are the reasons?",
    "present_kp": [
      "regression",
      "linear regression"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "ssh private-public key map for client. in ssh communication between server and client, client gets authenticated by its private key. i want to know that how does the server knows that which public key belong to client, if the server has more than 1 public key in its authorized_keys file.",
    "present_kp": [
      "ssh"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how do i create a complete new rpm including all dependencies - ansible disconnected package. i need to install ansible on my node and the node doesn't have internet connection. so i had to download all the dependencies and installed the same. now i would like to create a complete package which includes all the dependenciesmain rpm - ansible-2.3.0.0-3.el7.noarch.rpmdependenciespython2-pyvmomi-6.5-1.el7.noarch.rpmpython-crypto-2.0.1-1.el7.rf.x86_64.rpmpython-crypto-2.6.1-1.el7.rf.x86_64.rpmpython-ecdsa-0.11-3.el7.centos.noarch.rpmpython-httplib2-0.7.7-3.el7.noarch.rpmpython-keyczar-0.71c-2.el7.noarch.rpmpython-paramiko-1.15.1-1.el7.noarch.rpmpython-pyasn1-0.1.6-2.el7.noarch.rpmpyyaml-3.10-11.el7.x86_64.rpmsshpass-1.05-5.el7.x86_64.rpmi am listing out the complete list so in case if anyone needs this might help.is there any specific process to create a package ?",
    "present_kp": [
      "rpm"
    ],
    "absent_kp": [
      "linux",
      "rhel",
      "package management",
      "rpmbuild"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to move forward on regression problem. i'm an undergrad interested in machine learning, and i'm playing around with some data in order to get a better understanding of the field.datai'm working with the following data:to give you an idea of what you're looking at (sorry for the unlabeled axes) -- each data point corresponds to a basketball player's points per game (y-values) over a one-week period (x-values).end goali want to predict progress throughout the year, so in order to test/train, i start with the first two points, fit a regression model, and then predict the third point. i then add the actual value of the third point to the set, re-train, and predict the fourth point, etc.issuesas you can see, the data is all over the place, and the predictions are just as messy (sometimes getting up to ~200 points per game, which is totally impossible).i've tested different degrees of linear regression (quadratic, cubic, etc.) and degree=1 is always the best predictor because of how wonky the data is.ideasi have thought of the following ways to get more accurate predictions:smoothing the data, maybe using a moving average or some variantset an upper limit for predictions non-linear regression but outside of smoothing the data, i'm not sure if the rest are even possible in a regression model (upper limit) or applicable to this situation (non-linear).questionare any of the ideas i had above worth pursuing? if not, is there anything i should look into that might help me solve this problem?thanks!",
    "present_kp": [
      "machine learning",
      "regression",
      "linear regression"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "send a command from the console to a running x server. is it possible to send a command from the console, say tty1, to the terminal currently occupied with x (in my case tty7 as i use debian), to tell for example mplayer to play a movie?edit - made a shorthand function with the commands i learned in the answer below:function movie () { orig_tty='fgconsole' chvt 7 display=:0 mplayer -fs $1 > /dev/null 2> /dev/null chvt $orig_tty}",
    "present_kp": [
      "console"
    ],
    "absent_kp": [
      "command line",
      "x11",
      "remote"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to install debian gnu/linux on hp chromebook 14? usb boot doesn't works!. following this guide from the debian wiki, i'm tring to install debian on my hp chromebook 14 (falco).the problem is that, when i choose install in the debian-installer's menu, instead of the installation process i receive again the screen stating that the os verification has been turned off (developer mode).i want to install debian jessie on a sd, booting the debian-installer from a usb stick.",
    "present_kp": [
      "debian"
    ],
    "absent_kp": [
      "debian installer",
      "chrome book"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "exception when logging exception: is it correct to ignore them?. my question is specific to php, but i think it can be useful in other languages.i log into a table all the exception a code can throw:try{ //some code } catch (exception $e) { $log = new log(basename($_server['php_self']), $_session['id'], logtype::exception, $e); try { $log ->addlog(); } catch (exception $e2) { // } }the function addlog insert the exception informations into a database.but (by example if the connection with the database is lost) this function can throw an exception. i can't see what i have to do with this exception except ignoring it.i know ignoring exception is a bad practice, but what can i do in this particular case?if there is no problem to ignore it, is there a better way to indicate that this exception should not be taken into account?",
    "present_kp": [
      "php",
      "logging"
    ],
    "absent_kp": [
      "exceptions",
      "exception handling"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "important debian packages auto-removable. i'm in a situation where apt shows 1000+ packages on my system marked as auto-removable. amongst those are many that i know i need. i've resolved the situation by setting those to manually installed. but that results in almost half the packages in my system showing up as 'manually installed'. i ran into trouble when upgrading my system to the current stable version of debian (apt-get dist-upgrade), and none of the 'manually installed' packages were upgraded. again, i resolved the situation by setting all the 'manually installed' packages to automatically installed. that made the upgrade possible. but now all the upgraded, formerly 'manually installed' packages are again auto-removable. i tried finding not installed meta-packages that through their dependencies would at least reduce the auto-removable list. but without success. - is there no way to get back to a situation where the packages marked as auto-removable are really those i don't need?",
    "present_kp": [
      "apt"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "my first login class in php with pdo and bcrypt. this is the first time using a class. please review this and tell me if it's secure and if it's the right way to do it. the code itself is working, but i have doubts in the way i used all this.// register.php<?php...$db_password = $hash = password_hash($pass, password_bcrypt, array(cost => 10)); .... // store in the 'users' table only the hash of the password provided by user.?>// clases/manageusers.php<?phpinclude_once('connect.php');class manageusers{public $link;public $hash; function __construct(){ $db_connection = new dbconnection(); $this->link = $db_connection->connect(); return $this->link;} public function login($id,$email,$pass,$hash){ $query = $this->link->prepare(select id,username,email,pass from users where email = :email and activated = 'y' limit 1); $query->bindparam(':email', $email, pdo::param_str); $query->execute(); $data = $query->fetch(pdo::fetch_assoc); return $data; if(password_verify($pass,$hash)){ return true; }else{ return false; } }public function logout($id){ $query=$this->link->prepare(update users set verify = '0', activenow = 'n' where id= :id limit 1); $query->bindparam(':id', $id, pdo::param_int); $query->execute(); $countsout = $query->rowcount(); return $countsout; } }?>// login.php<?phpsession_start();error_reporting(e_all);ini_set('display_errors', '1');$errormsg = '';$email = '';$pass = '';$hash=;$id='';$token='';$token = md5(uniqid(rand(),true));if (isset($_post['e'])) { $email = $_post['e'];$pass = $_post['p'];$email = stripslashes($email);$pass = stripslashes($pass);$email = strip_tags($email);$pass = strip_tags($pass);if ((!$email) || (!$pass)) { //header(location: index.php); $errormsg = 'please fill in both fields';} else { include_once 'clases/manageusers.php'; include_once 'clases/connect.php'; $loginusers = new manageusers();$data = $loginusers->login($id,$email,$pass,$hash); $hash=$data['pass']; if(password_verify($pass,$hash) == true){ $db = new dbconnection();// <- new connection to database to update members table $dbh = $db->connect(); // with new login data. i think this is wrong... session_regenerate_id(true); $id=$data['id']; $username=$data['username']; $query = $dbh->prepare(update users set verify= :token, lastlog=now(), activenow='y' where id= :id limit 1); $query->bindparam(':id', $data['id'], pdo::param_int); $query->bindparam(':token', $token, pdo::param_str); $query->execute(); $_session['id'] = $data['id']; $_session['username'] = $data['username']; $_session['token'] = $token; header(location: profile.php?id=$id); exit(); } else { session_destroy(); header(location: login.php); $errormsg = incorrect login data, please try again; }}} ?>//logout.php<?phpsession_start();include_once 'clases/manageusers.php';if(isset ($_session['id']) && isset ($_session['username']) && isset ($_session['token'])){$id=$_session['id']; $lgout = new manageusers(); $loggingout = $lgout->logout($id); if($loggingout > 0){ session_destroy(); session_regenerate_id(true); header ('location: login.php'); }else{ echo 'not logged out'; }}?>",
    "present_kp": [
      "php",
      "pdo"
    ],
    "absent_kp": [
      "object oriented",
      "authentication"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "kernel and qemu : unable to mount root fs error. i am trying to run a distro in the virtual disk image with a custom kernel,so that i can experiment and debug the kernel. i followed this to make a disk image and then install debian to it. now i tried running distro with the following command:-qemu-system-i386 -hda debian.img -kernel ../linux-3.6.11/arch/i386/boot/bzimage -append root=/dev/sda1to my dissappointment it simply gives a kernel panic-not syncing:vfs:unable to mount root fs on unknown-block(8,1). how can i fix the problem?am i on the right path as far as kernel debugging is concerned?",
    "present_kp": [
      "linux",
      "kernel",
      "debugging",
      "qemu"
    ],
    "absent_kp": [
      "linux kernel"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "java installation in rhel. while installing java in redhat i'm facing some problems..i've attached the screen print ..if anyone could help me with it it would be appreciable..thank u.. i unzipped the file using tar command .once i created the softlink for it ,i should be able to see java version and all but which in this case i'm not able to see .. please need help here..",
    "present_kp": [
      "rhel",
      "java"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "cached bootstrap vs custom css. i am debating whether to use an existing framework such as bootstrap which has a decent chance of being cached vs custom css which has zero chance of being cached.which is better from a ux point of view and an seo point of view?",
    "present_kp": [
      "cache"
    ],
    "absent_kp": [
      "css framework"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "can the concatenation of two non-regular languages be regular?. can anyone give an example of two non-regular languages $a, b \\subseteq \\{0, 1\\}^$ for which the language $ab$ is regular?",
    "present_kp": [
      "regular languages"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "creating google forms from google spreadsheet. it is possible to use the formcreator app to create google forms using google spreadsheet. my question: how did app manage to achieve that?",
    "present_kp": [
      "google forms"
    ],
    "absent_kp": [
      "google spreadsheets",
      "google sheets addons"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how do i switch to vi editing mode in readline?. i want to switch to vi editing mode in a readline environment. but i don't want to use 'set -o vi'. i want to temporarily switch using a keyboard shortcut. the man page says i can do this with m-c-j. but that doesn't work for me. i'm using ubuntu and an xterm. doesn't work under gnome-terminal either.",
    "present_kp": [
      "readline"
    ],
    "absent_kp": [
      "bash"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "nvidia linux drivers on dell xps l502x nvidia geforce gt 540m. i have a dell xps l502x with an nvidia geforce gt 540m and i'd like help installing the official drivers off the nvidia site. every time i try i brick my installation and have to re-install. even if i restore my xorg.conf it still has some serious bugs. i'm currently running the drivers from inside the driver manager but i want to use the latest ones on the site.",
    "present_kp": [
      "linux",
      "drivers",
      "nvidia"
    ],
    "absent_kp": [
      "linux mint",
      "proprietary drivers"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "regular expressions within a string in awk using if/then control structure?. problemi want to specifically use awk (gawk, mawk, whatever) in an awk script to sum up the values in field 9 for every record that contains runner__.jpx (the underscores are placeholders for two digits 0-9) in field 6 and good in field 7.sample filefeb 14 11:33:16 ringer a[2388]: runner01.jpx good aa 3feb 14 11:33:32 ringer a[2388]: runner24.jpx good xx 1feb 14 11:33:39 ringer a[1894]: jogger02.jpx good aa 5feb 14 11:33:45 ringer a[2388]: runner99.jpx stop cc 1attempthow could i do this? i tried using an if statement, like:begin { sum = 0}{ if (($6 == runner[0-9][0-9].jpx) && ($7 == good)) sum += $9}end { printf sum}is there way to do this by matching strings in fields using if statements? am i allowed to use regular expressions within the if statement? what is an alternative method to do this? thanks.",
    "present_kp": [
      "awk",
      "mawk"
    ],
    "absent_kp": [
      "shell script",
      "scripting"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "screen dims after xrandr. i have an old p3 laptop with an 800x600 screen on which i've installed wattos 7.5. i know it is now on 10 but 7.5 is the latest one that will fit on a cd. the later ones will only fit on dvd. this is an old machine - it only has a cd reader, it will not boot off usb and it doesn't like microwatt (which still fits on a cd). there is something wrong with the microwatt drivers - the entire screen appears as a 0.25 bar. anyway, it has wattos 7.5.my login screen is very reddish. if i run it from my desktop it is greenish - which is the correct colour. also, for some reason, the system thinks it has a 1024x768 screen. after logging in, all the colours are ok.the obvious answer would have been to ask the wattos site but it is a chicken and egg situation. it wants me to sign in and in order to sign in, i need an invite key. i've got no idea what one of those is. to find out, i'd have to login and ask but i can't login because i haven't got an invite key.before committing the settings in stone, i decided to try them out. first, i set the screen sizexrandr -s 800x600the screen size changes and i can now see the taskbar but the brightness has also changed to half. when i type xrandr -q, i getxrandr: failed to get size of gamma for output defaultfollowed by the resolutionsi suspect it is assuming i have 24 or 32-bit colour but this is an old system so at most 16-bit colour, which possibly explains why it has all gone dim and why my login screen seems to have lost its blue/green component.the questionsi must be looking up the wrong keywords - i can't find how to set the number of colours to 65536 or to tell the system that it has 16-bit colour. all the hits i am getting are how to set console window colours.another lot of searches says gedit /etc/x11/xorg.conf this file doesn't exist on my system. again, i think i'm looking up the wrong keywords. almost all the hits tell me where it is, none of them tell me what has replaced it. how do i make these settings permanent so that my login screen appears in the correct colour and the system knows what size my screen is.edit i've found <url> apparently xorg.conf will be used if created. i'll give that a try later today. the laptop can only stray from a power supply for 5 minutes: after that it shuts down.edit this question is now purely academic - the machine just died.",
    "present_kp": [],
    "absent_kp": [
      "configuration"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "need clarification concerning windows azure. i basically need some confirmation and clarification concerning windows azure with respect to a silverlight application using ria services.in a normal silverlight app that uses ria services you have 2 projects:appapp.web... where app is the default client-side silverlight and app.web is the server-side code where your ria services go.if you create a windows azure app and add a wcf web services role, you get:app (azure project)app.services (wcf services project)in app.services, you add your ria domainservice(s). you would then add another project to this solution that would be the client-side silverlight that accesses the ria services in the app.services project. you then can add the entity model to the app.services or another project that is referenced by app.services (if that division is required for unit testing etc.) and connect that entity model to either a sqlserver db or a sqlazure instance.is this correct?if not, what is the general 'layout' for building an application with the following tiers:ui (silverlight 4)services (ria services)entity/domain (ef 4)data (sql server)",
    "present_kp": [
      "silverlight",
      "azure"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "editing the iptables file on asus firmware. i have downloaded the asus dsl-n14u firmware, extracted the .trx file and located the iptables file, as linked below:<url> know the iptables rules that i need to add. however, i am just a little confused about two things:1) is it possible to edit this file, add the rules, repackage the .trx file and then reload the firmware on the router?2) if #1 is possible, where in the file would i add my custom rules?thanks, just need a little guidance :)",
    "present_kp": [
      "iptables",
      "firmware"
    ],
    "absent_kp": [
      "linux"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "mvc: what is the difference between a model and a service?. why in some frameworks the logic layer is called model whereas in some it is called service. are they different from each other or just different by naming conventions?update 1the reason i'm asking is because in zend framework, a classical mvc framework, everybody uses the concept of model. now i'm learning angularjs and it seems that the word model disappeared and was replaced by the word service.what i noticed is that a service is more like a singleton that can be reused again and again (example: a rest client) whereas a model is more related to the data manipulations coming from the controller in the mvc pattern.",
    "present_kp": [
      "mvc",
      "model",
      "service"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "cpu temperature is higher than usual after installing xubuntu. i have installed xubuntu 14.04 on asus s550cb but the system becomes warmer than usual. so i tried to configure lm-sensors and saw cpu temperature is about 50 degrees. i guessed that some drivers are not installed and runlshw -html > system.html && xdg-open ./system.htmlthen i noticed that 3 drivers are red. does it mean they are not installed? those drivers were:1)id: generic:0description: signal processing controller2) id: serialdescription: smbus3) id: generic:1description: signal processing controllernow if it means that they are not installed, how can i search for them and install them? thanks for your help.",
    "present_kp": [
      "drivers",
      "cpu"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "is dofollow linking from a long list of items allowed by google?. i've just finished writing a list of 190 items. i'd like to make some links of the list dofollow, but i don't know if google still allows dofollow links and how many per article.",
    "present_kp": [
      "google",
      "links",
      "dofollow"
    ],
    "absent_kp": [
      "seo"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "can a mother's untimely death lead to misogyny?. many years ago, in an undergraduate psychology class, we were discussing misogyny, and my professor told us that there was a significantly high number of misogynists who had lost their mothers (specifically that they had died) in their mid to late teens. he said it was because the teen were angry at their mothers for leaving them, but were unable to process or express what they were feeling, so the anger was redirected into an anger towards all women.i have tried to find some mention of this theory elsewhere, but cannot seem to formulate the right google search phrase. is this a theory that still has merit, or has it been disproven? if it is a valid belief, can someone direct me to a book or online documentation which discusses it?",
    "present_kp": [],
    "absent_kp": [
      "cognitive psychology"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "object oriented analysis, missing methodology. i've studied design pattern and oop principles, still feel that there is something missing in most design theories. maybe the fact is that there is not a 'theory' as it is in database design. let me explain with an example.suppose that i want to model different ways to produce orange juice.let say that i start by a class oranges, that models a lot of oranges that will be of a certain variety, of a certain ripeness, they may have been froozen etc., ant these will by my oranges class properties. then i continue by providing orange with a squeeze method. this seems to be correct, because ties togheter the method to the data it applies to, that is the precondition to achieve encapsulation and other valuable oop principles. but quickly it comes out that it has not been a good choiche because there is not just one way to squeeze an orange. it would be preferrable to have a juicer class whose instances contains details of the juicing method: by hand, with an electric juicer, including the pulp, not to mention mixing different varieties of oranges. where the latter opens an whole new cathegory of possibilities that were not available (or not achievable with ease) if each oranges instance would have been processed using its squeeze method.say that we have performed a 'refactoring' of our oo architecture and move to another similar example that will get us to the point.it is now a common practise to use an orm to save class instances to database and get them back later. in what is it similar to the oranges class example? well, here i have a class whose instances may be persisted to a database, that is 'allow a particular process'. but the class itself does not contain the logic for doing so, that actually is in the orm classes. so it seems that the same process of 'externalization' of the oranges example has taken place in this case too.and here is the questiongiven that it is desireable to reduce the number of attempts-and-refactorings to the minimum, what are the questions that a programmer should put in order to determine precociously if a method should be externalized to another class? are all the methods of a class exernalizable or is there a set of core methods that are inherently of the class itself and so not externalizable?ideasas i said in the beginning, i'm not completely clueless, i feel more like that a 'comprehensive' theory is missing.it's easy to put some examples: if i have a class whose instances represent invoices, i will have a method 'calculatetotalamount'. this is clearly an inthrinsic, core method of that class, as oppsed to the methods for saving that invoice to the database that will be 'external' of the class itself. but this is not a 'final answer', but a starting point that serves to demonstrate that 'not all methods are equal', but methods may be divided into cathegories. how many (useful) cathegories can we make, and are there methodologies that put these questions?",
    "present_kp": [
      "object oriented"
    ],
    "absent_kp": [
      "development methodologies"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "a combined if/switch statement syntax with exception handling for a c#-inspired language. it is sometimes necessary to try/catch exceptions inside the if condition, but not the body that follows. in c#, this is really rather cumbersome, requiring locals and code that isn't entirely obvious, at a glance, as to its operation.it is also sometimes necessary to write switch statements, which, unlike if, have more than two possible outcomes, although in c# this requires the use of a fairly unpopular syntax.here's an idea for combining these.merging switch with try/catchfirst, we change the fundamental syntax of switch to be close to that fairly popular replacement, but without the extra curlies:switch (condition) case 1 { } case 2 { } default { }next, we make it possible to catch exceptions in the condition. under the hood, this inserts an appropriate try/catch block only when at least one catch is present:switch (condition) catch filenotfoundexception e { } // use e catch argumentexception { } catch { } case 1 { } case 2 { } default { }now we make it easier to chain several such statements akin to if/else chaining, by moving default one level up, and renaming it to else:switch (condition1) catch filenotfoundexception { } case 1 { } case 2 { }else switch (condition2) case abc { } case def { }else { }and so we have a switch merged with try/catch!combining with 'if'both switch and if now have else clauses, so why not allow you to mix them as you please:if (bool-condition1) { }else if (bool-condition2) { }else switch (string-expr) catch filenotfoundexception { } case thing { }else { }(thanks ach_l!)further improvementsswitching on a list or range of values has been proposed and can be easily incorporated:switch (int-expression) case 1 { } case 2, 5 { } case 10..50 { }else switch (string-expression) case abc { } case def { }else if (boolean-condition3) { }else { }in this case it could be really convenient to be able to reference the value in the condition clause. this can be easily incorporated by allowing one to declare a single variable in this clause the same way c#'s using allows:switch (var z = int-expression) case 2, 5 { return z == 2 ? two : five; } case 10..50 { return (z * 10).tostring(); }benefitsamong other benefits, one can now express a common error-handling scenario concisely, and yet not wtfy:switch (bool-condition) catch filenotfoundexception e { } case true { } case false { }would this solve any real-life coding difficulties you have encountered? are there any tweaks you would apply to this?",
    "present_kp": [
      "syntax"
    ],
    "absent_kp": [
      "language design",
      "language features"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "the line between fantasy and reality. it is already well known that imagining certain things can have a physiological effect on us. for example, imagining an intense situation can cause the heart rate to go up, while imagining a quiet, safe place might have the opposite effect (or maybe that one is simply a result of distracting oneself from whatever may cause unrest).another example would be imagining yourself doing something you don't want to do. from what i understand, this reduces the stress of anticipating work.i wonder, though, what exactly separates this from actually doing what you imagine? what are the differences in the physiological effects? how different is the activation of the brain? (aside from the obvious - for example, imagining something physical happening to you, such as being punched, won't actually cause any bruising)",
    "present_kp": [],
    "absent_kp": [
      "visualization"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how many tf2 servers would this vps be able to run?. i've been renting from a certain company for a while now, and i'm currently thinking of switching to nfoservers because i would get a cheaper deal since i plan to expand the servers further. however it's not going to be many servers, so i was wondering if it would be possible to at least get 3 servers running on nfoservers' two core vps plan?specs (if you're too lazy to open their site):two full, dedicated ht cpu cores (nehalem or better)2048 mb of ram200 gb of raid-protected storage8000 gb of bandwidth transferif not, how many servers will i be able to have?",
    "present_kp": [
      "server"
    ],
    "absent_kp": [
      "gaming"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "upgrade openssl for old version xampp. i am trying to enable tls 1.1 on my xampp apache server, but it seems like the version of openssl is too old for that. (error: sslprotocol: illegal protocol 'tlsv1.1')can anyone help me how i can upgrade openssl? or something easier approach? i need to keep php version as 5.3.1.my configuration is apache/2.2.14 (win32) dav/2 mod_ssl/2.2.14 openssl/0.9.8l mod_autoindex_color php/5.3.1.thank you so much!",
    "present_kp": [
      "openssl",
      "xampp"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "are timeseries databases useful for non-numerical data?. i have a huge time series (about 30 million) of network paths with the following format:timestamp, path, latencythe path is a sequence of ip address, so it can be represented either as a string or an array of integers. currently the data are stored in text files which makes it very slow the analysis and querying of paths. it was suggested to me to use a timeseries database (tsdb), such as influxdb or opentsdb, to store them efficiently, but some background reading i did suggests that tsdbs are appropriate for numerical values. for instance opentsdb mentions:opentsdb is a time series database. a time series is a series of numeric data points of some particular metric over time.is there any optimization i'll gain from using a tsdb instead of a relational db in my case, and generally for timeseries that include non-numerical values?the main queries i plan to do is basically to get all the paths between two timestamps, check if there are path changes, and how this changes affect the lattency. additionally i may need to search for path with specific hops (e.g. select all records where the path includes the ip hop 1.2.3.4), or all the paths with latency over a certain threshold.",
    "present_kp": [
      "database"
    ],
    "absent_kp": [
      "performance",
      "relational database",
      "analytics"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "call center design. imagine you have a call center with three levels of employees: respondent, manager, and director. an incoming telephone call must be first allocated to a respondent who is free. if the respondent cant handle the call, he or she must escalate the call to a manager. if the manager is not free or not able to handle it, then the call should be escalated to a director. design the classes and data structures for this problem. implement a method dispatchcall() which assigns a call to the first available employee.package design;import java.math.biginteger;import java.util.*;import java.util.concurrent.linkedblockingqueue;public class callcenter { employee[] employees; int size = 100; map<employee,phonecall> callmap = new treemap<employee, phonecall>(new comparator<employee>() { @override public int compare(employee o1, employee o2) { return 0; } }); queue<phonecall> calls ; priorityqueue<employee> dispatchqueue; callcenter(){ employees = new employee[size]; calls = new linkedblockingqueue<phonecall>(); dispatchqueue = new priorityqueue<employee>(new comparator<employee>() { @override public int compare(employee o1, employee o2) { if(o1.id > o2.id) return -1; else if(o1.id < o2.id) return 1; else return 0; } }); for(int i = 0;i<size;i++){ if(i==0 || i==1){ employees[i] = new director(i); } else if(i>=2 && i<=6){ employees[i] = new manager(i); }else{ employees[i] = new respondent(i); } dispatchqueue.offer(employees[i]); } } public void dispatchcall(phonecall p){ if(dispatchqueue.isempty()){ calls.offer(p); }else{ callmap.put(dispatchqueue.poll(), p); } } public void endcall(employee emp){ if(callmap.containskey(emp)){ callmap.remove(emp); dispatchqueue.offer(emp); } } public void processcallsqueue(){ while(!calls.isempty()){ if(dispatchqueue.isempty())break; else dispatchcall(calls.poll()); } } public employee getrandomemployee(){ random rn = new random(); int randomposition = rn.nextint(100); return employees[randomposition]; }}class phonecall{ int id; string location; string number; phonecall(){ random rand = new random(); id = rand.nextint(); location = us; number = new biginteger(130,rand).tostring(); }}class employee{ int id; string designation; int priority; //priority is the call priority 1. respondent, 2. manager , 3. director employee(){ } employee(int id){ this.id = id; }}class manager extends employee{ manager(int id){ super(id); designation = manager; priority = 2; }}class director extends employee{ director(int id){ super(id); designation = director; priority = 3; }}class respondent extends employee{ respondent(int id){ super(id); designation=respondent; priority=1; }}is this design too shallow to be done in an interview? any feedback on the design is greatly appreciated.",
    "present_kp": [
      "java"
    ],
    "absent_kp": [
      "object oriented",
      "design patterns"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is it bad to write object oriented c?. i always seem to write code in c that is mostly object oriented, so say i had a source file or something i would create a struct then pass the pointer to this struct to functions (methods) owned by this structure:struct foo { int x;};struct foo* createfoo(); // mallocs foovoid destroyfoo(struct foo* foo); // frees foo and its thingsis this bad practice? how do i learn to write c the proper way.",
    "present_kp": [
      "c"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to decide if a propositional formula is a well formed?. so, i have the following propositional formula:(((p q) s) t)how do i decide if it's well formed?",
    "present_kp": [],
    "absent_kp": [
      "propositional logic"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "pretty tail -f for log files. possible duplicate:how to have tail -f show colored output is there a bash/awk script or utility that pretty formats and colors tail -f from log files? at the moment i am specifically looking at apache log files.",
    "present_kp": [
      "tail"
    ],
    "absent_kp": [
      "software rec",
      "logs"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "rel canonical link tag pointing to the same page. i'm implementing the canonical tag in my page to avoid be penalized with duplicate content flag by search engines. my doubt are the following:if i have a page copy with the canonical tag pointing to original, and in this original page i have the canonical tag pointing to original again, what are the consequences? the thing is, that for me is more easy to generate the tag in all pages, and not only in the copies.can i put the <link rel=canonical href=original /> in anyplace, or should be in the <head> tag.",
    "present_kp": [
      "rel canonical"
    ],
    "absent_kp": [
      "seo",
      "canonical url"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "can't get hello world shell script to run in freebsd. server is running freebsd 9.2.using vim, i wrote the following script called hello:#!/bin/shecho hello worldthen i set it as executable:>chmod 755 hellothen i tried to run it from the command line (while in the same folder where the script was saved):>helloi got this error message:hello: command not found.is there something different i have to do to make an executable script in bsd?",
    "present_kp": [
      "shell script",
      "executable"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "setting up funnel urls with regular expressions. i'm trying to set up google analytics to track my e-commerce checkout page. the urls in the process are as follows:/checkout/[order number]/checkout/[order number]/your_details/checkout/[order number]/review/checkout/[order number]/completei've used the regular expression for the first page (/checkout/[order number]:/checkout/\\d+that works fine, however the problem is i'm struggling to work out what the regular expression should be for the other pages.",
    "present_kp": [
      "google analytics",
      "analytics"
    ],
    "absent_kp": [
      "goal tracking"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "windows service or windows task scheduler?. i am planning to create a utility, which will query the database and store some information (on another table in the database). its a multi-threaded utility and require to run for every 5 or 10 minutes/later may be thrice in a day.i see two options to achieve this in c#/dotnet programming. creating windows service having timer approach inside it.a console program and schedule it using windows task scheduler.which one do you prefer and why?",
    "present_kp": [],
    "absent_kp": [
      ".net"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why does sed replace all occurrences instead of only the first occurrence?. i'm trying to replace the first occurrence in a file using sed:sed -i s/he/he/ dummy.txtinstead of replacing first occurrence, it replaces all the occurrences, even without /g.according to the documentation it should replace the first only. the sed version which is:gnu sed version 4.1.5am i missing anything? or does the behavior differ for different sed implementations?",
    "present_kp": [
      "sed"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to append character to the end of line using sed?. i have the following text:servername: nzrc222total: 8.46 gbserverid: 259695servername: nzrc333total: 50.13 tbserverid: 260582servername: nzrc555total: 9.31 tbserverid: 260956my desired output would be: servername: nzrc222,total: 8.46 gb,serverid: 259695,servername: nzrc333,total: 50.13 tb,serverid: 260582,servername: nzrc555,total: 9.31 tb,serverid: 260956,basically, i want to generate a csv file which can be imported to the excel.when i go with :sed '/\\w.*/ a ,' file i get this:servername: nzrc222,total: 50.13 tb,serverid: 260582,servername: nzrc333,total: 9.31 tb,serverid: 260956,any suggestions? thank you in advance!",
    "present_kp": [
      "sed",
      "csv"
    ],
    "absent_kp": [
      "text processing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "cp/rsync both grind to a halt when copying to usb/micro sd. i'm trying to transfer about 20gb of music from my arch linux laptop to a micro sd card, via a usb reader. the process starts off fine; the first ~50 songs take a combined one second, although from what i've read that's just to do with caching (or something...) and doesn't represent the actual speed. then it goes to what i'd consider a reasonable speed, where each song takes anywhere between one and five seconds (the files are lossless, so maybe ~15mb on average).however, after a few hundred songs, things just slow down completely. a single file will take about five minutes to transfer, and that's a conservative estimate. i left it running overnight and barely any progress had been made!the card is a recently purchased class 10 sandisk, and i've tried using it in both a usb and a regular sd card adapter, so i'd like to try some os-level solutions before i investigate further into the physical side of things. i've also tried all three usb ports on my laptop and they all face the same issue. i've tried using the regular cp -rv ~/music /mnt/sd command, as well as rsync -rvh ~/music /mnt/sd, and the same thing happens with both. if, for testing purposes, i copy to a destination on the same partition then everything's fine, so it's definitely to do with it being on a different partition. i've also tried the suggestion posted here which didn't help.my kernel version is 4.10.9.any ideas?",
    "present_kp": [
      "arch linux",
      "rsync",
      "cp",
      "sd card"
    ],
    "absent_kp": [
      "file copy"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is ethical/unethical while seeking help on the web with programming assignments?. i have used the web and stack overflow extensively during the past month or so in creating my final project for my c# class. i have used so much code that i didn't write myself that i feel i am being unethical by not giving proper credit to the people who helped me; or the websites that have provided excellent examples.is it unethical to publish work which was created by me, even though its hardest problems were solved by other people? should i credit these people for helping me with my assignment? or the web sites which provided examples?",
    "present_kp": [],
    "absent_kp": [
      "ethics",
      "credits",
      "attribution"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "samba - releasing file lock. our (windows) users are starting a program from a shared subdirectory on a debian server. they use samba to access this directory.when we want to release a new version of the program the started files (program file and libraries) are locked. to release these locks we restart the samba service. after restarting all locks are released and the new files can be copied to the directory.the only disadvantage is that the users are losing all the handles to the files in all directories which are served by samba on this server. we are starting keepass from another subdirectory and keepass crashes afterwards.is there a nicer way of releasing the locks (in a single directory) ?is there a way to separate the samba shares ?",
    "present_kp": [
      "debian",
      "windows",
      "samba"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "is the text in my open source project under the gpl?. i have an open source project that used to be under the gpl licence (now it has no licence).in this repository i had many short stories. my intention was for the code to be open source but the stories to be our copyright. when i made the commit i had not considered this. once i realized that this might be an issue i removed the license for all future versions of the code. what i am concerned about now is what licence do people have with these stories? can they do whatever they want with them under the terms of the gpl? if (hypothetically) somebody wanted to write a book or a movie based on these stories would they be allowed to? if they wanted to make a complete copy of the website with all of the stories in it, can they?another issue i have is that i would like to make the code open source again under the gpl or potentially some other licence. if i do this will all the code in-between the 2 versions which are open source, be under any license? for example after i removed the license the first time i continued to add more stories and text that i would not like people to be able to do whatever they want with. now these are all in a database and not part of the source code. but if i make the project open source will they be able to use that content as well?",
    "present_kp": [
      "copyright"
    ],
    "absent_kp": [
      "gpl 3",
      "github"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "relationship between an np-hard problems with the subsets of them?. i am writing a paper. i have a problem and i want to prove that it is an np-hard problem. however, for simplicity, i select a subset from my problem to prove that it is an np-hard problem. although i think that it is reasonable that if a subset of problem is np-hard, it result in the problem being np-hard, but i think writing this sentence in a paper need a reference. i will be thankful if anyone helps and gives me a reference for it.......................more explanations : my problem includes n boxs, b(1) through b(n). every box b(i) has a size called t(i) and t(i)<=t(i+1). now, to prove that my problem is np-hard, i have considered a subset of my problem such that there are arbitrary number of boxes with size t(n). and for this subset i have proved that my problem is np-hard, as a result the problem is np-hard. is this an acceptable assumption?",
    "present_kp": [],
    "absent_kp": [
      "complexity theory",
      "np hard"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "implementation of datatypes in haskell?. in haskell, are datatypes converted to the church encoding i.e. folding the data type. for example, given data n = z | s nin haskell, it can be converted to its church encoding by foldn z z s = zfoldn (s n) z s = s (foldn z s n)where if we do foldn m, we get the church encoding:\\z s . s ( .... s n ... )in proofs and types, girard shows how this works for any inductive datatype. there are two questions i have: (1) is this actually how haskell treats datatypes and (2) what is the equivalent construction for coinductive datatypes.",
    "present_kp": [],
    "absent_kp": [
      "data structures",
      "term rewriting",
      "interactive proof systems"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is .gnupg directory in the home directory?. i found a strange directory in my home directory on linux mint 17.2 cinnamon (though i'm pretty confident it's on all linux distros) called .gnupg. it has no access given to anyone other than root. so i have three questions:what is this directory? what does it contain?why is it placed in the user's home directory yet doesn't give them any access?will it do any harm by just entering the directory as root? (i know that's a stupid question, but i want to be safe)",
    "present_kp": [
      "home"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to uninstall compiled gtk+. a few days ago i compiled gtk+ 3.12 on my ubuntu 14.04 and linux mint 17 (with cinnamon) distros. it messed up the appearance. how can i remove it totally and safely? i didn't change the default installation location when compiling.i also have versions 3.10 and 2.24 (installed by default.)",
    "present_kp": [
      "uninstall"
    ],
    "absent_kp": [
      "gtk3"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "hyperthreading not detected by linux. i have a dell server with two intel xeon e5645 cpus, each cpu has 6 cores, and each core is hyperthreaded (i.e. should be equivalent to two virtual cores). i installed centos 6.2 on this server and it seems to detect only 12 cores (although there should be 24 virtual cores altogether).when i look at /proc/cpuinfo i get for each cpu:cpu cores: 6siblings: 6which seems to indicate that number of cores equals number of virtual cores, or hyperthreading not detected/enabled.when i run dmidecode i can see that ht flag is turned on and i do see the following, which seem to indicate that the bios is configured correctly for hyperthreading.core count: 6core enabled: 6thread count: 12is there some configuration that i am missing in order to make linux detect all virtual cores?",
    "present_kp": [
      "linux",
      "centos",
      "cpu",
      "hyperthreading"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "deploying a package through puppet from url. how can i install a package through puppet by using the url of the .tar.gz of the package on centos 6 client. is there any native support in puppet without installing anything extra on the puppet master.",
    "present_kp": [
      "centos",
      "puppet"
    ],
    "absent_kp": [
      "package management"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "cognito forms. text characters calculation. how to calculate numbers of characters in a text box, and to make sure it is not lesser than or greater than a specific amount of numbers",
    "present_kp": [
      "cognito forms"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "u-boot: changing tty columns and rows for virtual consoles. having installed grub as boot manager allows the operating system (linux) to boot on specific video mode so the tty virtual consoles change its columns and rows (width and height).for example, editing /etc/default/grub (for grub v2) :grub_gfxpayload_linux=1024x768but now i have an embedded linux device: utilite pro from compulab, and i am testing ubuntu 12.04 and kali linux v1.1.1 for it.they both work ok, but seem not to use grub as boot loader. according to the docs, they use u-boot as boot manager. could i do this resolution change for virtual consoles (tty, text mode) for u-boot?i don't need to change the gui video resolution, yet i can accept to change it too if it is needed.",
    "present_kp": [
      "video",
      "tty"
    ],
    "absent_kp": [
      "u boot"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "have a class that holds all the interfaces. i seriously didn't know how to write this in google, there must be an answer lying around. here it goes: i'm using ninject (ioc) and asp.net mvc 5. i have a business layer with a couple of classes. is it ok if i have one class that holds all these classes? like this: public class businesslayer : ibusinesslayer{ public iorganizerlogic organizerlogic { get; } public iparticipantlogic participantlogic { get; } public ipaymentlogic paymentlogic { get; } public itournamentlogic tourmanetlogic { get; } public businesslayer(iorganizerlogic organizerlogic, iparticipantlogic participantlogic, ipaymentlogic paymentlogic, itournamentlogic tournamentlogic) { organizerlogic = organizerlogic; participantlogic = participantlogic; paymentlogic = paymentlogic; tourmanetlogic = tourmanetlogic; }}",
    "present_kp": [
      "asp.net",
      "interface"
    ],
    "absent_kp": [
      "c#"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "cpanel / whm // centos // selinux. the file config indicates selinux mode is target.is it enabled or disabled ?terminal/ssh says 'disabled'.but several behaviours here and there act like it is enabled.thanks.",
    "present_kp": [
      "centos",
      "cpanel"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to use cut and grep command to find data separated by :. in my simple database, it contains text of like, example, soldiers.name:rank:gender:age:years of servicetom corporal:recruit:male:19:2nicole sergeant:corporal:female:30:10daniel recruit:sergeant:male:40:19and my script goes like this:echo enter name: read nameecho enter rank: read rankecho enter gender: read genderecho enter age: read ageecho enter years of service: read yearsgrep $name database.txtif i enter corporal into name, the output will showtom corporal:recruit:male:19:2nicole sergeant:corporal:female:30:10because the string matches under the section name and rank.how do i use delimiter : to categories the search, so if i enter my conditions under rank, it will search just the rank category and not everything else. thanks in advance.and do up my question instead of down voting.",
    "present_kp": [
      "grep",
      "cut"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "computing integer square roots in java - follow-up. (see the previous iteration.)my two previous methods for computing the integer square root of a number \\$n\\$ ran in the \\$\\mathcal{o}(\\sqrt{n})\\$ worst case time. now i have added a method (intsqrt3) that runs in \\$\\mathcal{o}(\\log \\sqrt{n})\\$ time:main.java:import java.util.random;import java.util.function.function;public class main { public static long intsqrt1(long number) { long sqrt = 0l; while ((sqrt + 1) * (sqrt + 1) <= number) { sqrt++; } return sqrt; } public static long intsqrt2(long number) { if (number <= 0l) { return 0l; } long sqrt = 1l; while (4 * sqrt * sqrt <= number) { sqrt *= 2; } while ((sqrt + 1) * (sqrt + 1) <= number) { sqrt++; } return sqrt; } public static long intsqrt3(long number) { if (number <= 0l) { return 0l; } long sqrt = 1l; // do the exponential search. while (4 * sqrt * sqrt <= number) { sqrt *= 2; } long left = sqrt; long right = 2 * sqrt; long middle = 0; // do the binary search over the range that is guaranteed to contain // the integer square root. while (left < right) { middle = left + (right - left) / 2; if (middle * middle < number) { left = middle + 1; } else if (middle * middle > number) { right = middle - 1; } else { return middle; } } // correct the binary search noise. this iterates no more than 3 // times. long ret = middle + 1; while (ret * ret > number) { --ret; } return ret; } public static long intsqrt4(long number) { return (long) math.sqrt(number); } private static void profile(function<long, long> function, long number) { long result = 0l; long starttime = system.nanotime(); for (int i = 0; i < iterations; ++i) { result = function.apply(number); } long endtime = system.nanotime(); system.out.printf(time: %.2f, result: %d. , (endtime - starttime) / 1e6, result); } private static final int iterations = 1_000; private static final long upper_bound = 1_000_000_000_000l; public static void main(string[] args) { long seed = system.nanotime(); random random = new random(seed); long number = math.abs(random.nextlong()) % upper_bound; system.out.println(seed = + seed); system.out.println(number: + number); profile(main::intsqrt1, number); profile(main::intsqrt2, number); profile(main::intsqrt3, number); profile(main::intsqrt4, number); }}the performance figures i get looks like this:seed = 19608492647714number: 54383384696time: 531.18, result: 233202.time: 218.41, result: 233202.time: 1.81, result: 233202.time: 0.43, result: 233202.above, intsqrt3 took 1.81 milliseconds.critique requestis there something i could improve? naming/coding conventions? performance? api design?",
    "present_kp": [
      "java"
    ],
    "absent_kp": [
      "algorithm",
      "reinventing the wheel",
      "numerical methods"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "which graph will be appropriate for the visualization task?. i have some terminal charging values for us and china comes in a pandas dataframe like the following, value country 0 550.0 usa 1 820.0 china 2 835.0 china 3 600.0 usa 4 775.0 china 5 785.0 usa 6 790.0 usa this is the sample data and i have in total 5k+ entries. the data is cleared for the outliers and needs to be visualized. what kind of visualization can i use to plot my data meaningfully?",
    "present_kp": [
      "visualization",
      "data",
      "pandas"
    ],
    "absent_kp": [
      "plotting"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how do i make this c# code more efficient, in terms of fps and time complexity?. image<bgr, byte> frame = _capture.queryframe();image<gray, byte> grayframe = frame.convert<gray, byte>();grayframe._equalizehist();mcvavgcomp[][] facesdetected = grayframe.detecthaarcascade(_faces, 1.1, 1, emgu.cv.cvenum.haar_detection_type.find_biggest_object, new size(20, 20));if (facesdetected[0].length == 1){ mcvavgcomp face = facesdetected[0][0]; #region search roi based on face metric estimation - int32 ycoordstartsearcheyes = face.rect.top + (face.rect.height * 3 / 11); point startingpointsearcheyes = new point(face.rect.x, ycoordstartsearcheyes); size searcheyesareasize = new size(face.rect.width, (face.rect.height * 3 / 11)); rectangle possibleroi_eyes = new rectangle(startingpointsearcheyes, searcheyesareasize); #endregion int widthnav = (frame.width / 10 * 2); int heightnav = (frame.height / 10 * 2); rectangle nav = new rectangle(new point(frame.width / 2 - widthnav / 2, frame.height / 2 - heightnav / 2), new size(widthnav, heightnav)); frame.draw(nav, new bgr(color.lavender), 3); point cursor = new point(face.rect.x + searcheyesareasize.width / 2, ycoordstartsearcheyes + searcheyesareasize.height / 2); grayframe.roi = possibleroi_eyes; mcvavgcomp[][] eyesdetected = grayframe.detecthaarcascade(_eyes, 1.15, 3, emgu.cv.cvenum.haar_detection_type.do_canny_pruning, new size(20, 20)); grayframe.roi = rectangle.empty; if (eyesdetected[0].length != 0) { frame.draw(face.rect, new bgr(color.yellow), 1); foreach (mcvavgcomp eye in eyesdetected[0]) { rectangle eyerect = eye.rect; eyerect.offset(possibleroi_eyes.x, possibleroi_eyes.y); grayframe.roi = eyerect; frame.draw(eyerect, new bgr(color.darkseagreen), 2); frame.draw(possibleroi_eyes, new bgr(color.deeppink), 2); if (nav.left < cursor.x && cursor.x < (nav.left + nav.width) && nav.top < cursor.y && cursor.y < nav.top + nav.height) { linesegment2d cursordraw = new linesegment2d(cursor, new point(cursor.x, cursor.y + 1)); frame.draw(cursordraw, new bgr(color.white), 3); int xcoord = (frame.width * (cursor.x - nav.left)) / nav.width; int ycoord = (frame.height * (cursor.y - nav.top)) / nav.height; cursor.position = new point(xcoord, ycoord); } } } imageboxframe.image = frame;}",
    "present_kp": [
      "c#"
    ],
    "absent_kp": [
      ".net",
      "performance"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "finding the largest product of four consecutive numbers in a grid. project euler #11 asks to find the largest product of four numbers of a grid, where the four numbers occur consecutive to each other vertically, horizontally, or diagonally.here is my solution in python. in addition to the usual code review, i have 2 extra questions (actually confessions of my laziness):would it be better if i compared products as in traditional way instead of using max(list)? like:if (current > max_product): max_product = currentwould it be better if i used proper for loops instead of relying on try? because in certain cases it gives keyerror. because of these two shortcuts, i feel like i have cheated. shall i worry about them or just ignore them?yy = 08 02 22 97 38 15 00 40 00 75 04 05 07 78 52 12 50 77 91 0849 49 99 40 17 81 18 57 60 87 17 40 98 43 69 48 04 56 62 0081 49 31 73 55 79 14 29 93 71 40 67 53 88 30 03 49 13 36 6552 70 95 23 04 60 11 42 69 24 68 56 01 32 56 71 37 02 36 9122 31 16 71 51 67 63 89 41 92 36 54 22 40 40 28 66 33 13 8024 47 32 60 99 03 45 02 44 75 33 53 78 36 84 20 35 17 12 5032 98 81 28 64 23 67 10 26 38 40 67 59 54 70 66 18 38 64 7067 26 20 68 02 62 12 20 95 63 94 39 63 08 40 91 66 49 94 2124 55 58 05 66 73 99 26 97 17 78 78 96 83 14 88 34 89 63 7221 36 23 09 75 00 76 44 20 45 35 14 00 61 33 97 34 31 33 9578 17 53 28 22 75 31 67 15 94 03 80 04 62 16 14 09 53 56 9216 39 05 42 96 35 31 47 55 58 88 24 00 17 54 24 36 29 85 5786 56 00 48 35 71 89 07 05 44 44 37 44 60 21 58 51 54 17 5819 80 81 68 05 94 47 69 28 73 92 13 86 52 17 77 04 89 55 4004 52 08 83 97 35 99 16 07 97 57 32 16 26 26 79 33 27 98 6688 36 68 87 57 62 20 72 03 46 33 67 46 55 12 32 63 93 53 6904 42 16 73 38 25 39 11 24 94 72 18 08 46 29 32 40 62 76 3620 69 36 41 72 30 23 88 34 62 99 69 82 67 59 85 74 04 36 1620 73 35 29 78 31 90 01 74 31 49 71 48 86 81 16 23 57 05 5401 70 54 71 83 51 54 69 16 92 33 48 61 43 52 01 89 19 67 48rows = yy.splitlines()d = {}x = 0for row in rows: row_cels = row.split() y = 0 d[x] = {} for cell in row_cels: d[x].update({y: int(cell)}) y+=1 x+=1def product(a, b, al=hori, r=4): try: res = 1 for xx in xrange(r): if al == hori: # - res *= d[a][b+xx] elif al == verti: # | res *= d[b+xx][a] elif al == dia: # \\ res *= d[a+xx][b+xx] elif al == diarev: # / res *= d[a+xx][19-(b+xx)] return res except: return 0hori = []verti = []dia = []diarev = []for x in xrange(0, 20): for y in xrange(0, 20): hori.append(product(x,y)) verti.append(product(x, y, verti)) dia.append(product(x, y, dia)) diarev.append(product(x, y, diarev))print max(max(hori), max(verti), max(dia), max(diarev))",
    "present_kp": [
      "python"
    ],
    "absent_kp": [
      "programming challenge",
      "matrix"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "can anybody recomend a good seo book?. im currently developing a asp mvc site.is there any book out there for asp mvc seo or just seo in general?",
    "present_kp": [
      "seo"
    ],
    "absent_kp": [
      "asp.net mvc"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "named pipes: several experiments leads to confusion. i've come across various articles and so questions and i am still confused about something that i use on daily basis, but never realized how confusing it can be. i am experimenting with (named) pipes in linux.1sttry was simple: figure out how pipe buffers are working:#1mkfifo /tmp/mypipe#2echo hello world >/tmp/mypipectrl+c#3cat /tmp/mypipeobservation:when i killed echo before cat reads the data nothing was written to pipe (cat keeps running but nothing was read from pipe). i was assuming that when you type producent >named_pipe and you will exit producent then part of data that match pipe buffer size will be written to named_pipe and will remain here until it will be read by consument (now i know that this is not how it works). so what i did next was:2ndtry was to connect consument to other end of pipe:#1mkfifo /tmp/mypipe#2echo hello world >/tmp/mypipe#3cat /tmp/mypipeobservation:cat command displays the hello world message and both processes ends. the interesting discovery here was that during the #2 step ps -elf does not display the echo command. it seems that echo is waiting until somebody will read from pipe and this is explanation why nothing was printed to pipe in my first attempt.3rdtry was to pipe command that will run forever and constantly write to pipe and see what will happened:#1mkfifo /tmp/mypipe#2yes >/tmp/mypipe#3cat /tmp/mypipeobservation:this worked as expected and cat printed out what yes forwarded to pipe. however i have tried to replace cat with tail -f. when i did this then tail did not print anything until the yes command was killed.4thtry is the big mystery:# 1#mkfifo /tmp/mypipe# 2#for i in $(seq 1 10000); do echo -n $i|> /tmp/mypipe; done# 3#for i in $(seq 1 10); do echo ${i}# read:; cat /tmp/mypipe && echo ; doneafter this the 3# command start typing something like that:1# read:1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27|28|29|30|31|32|33|34|35|36|37|38|39|40|41|42|43|44|45|46|47|48|49|50|51|52|53|54|55|56|57|58|59|60|61|62|63|64|65|66|67|68|69|70|71|72|73|74|75|76|77|78|79|80|81|82|83|84|85|86|87|88|89|90|91|92|93|94|95|96|97|98|99|100|101|102|103|104|105|106|107|108|2# read:109|3# read:110|4# read:111|5# read:112|6# read:113|114|115|7# read:116|8# read:117|9# read:118|119|120|121|122|123|124|125|126|127|128|129|130|131|132|133|134|135|136|137|138|139|140|141|142|143|144|145|146|147|148|149|150|151|152|153|154|155|156|157|158|159|160|161|162|163|164|165|166|167|168|169|170|171|172|173|174|175|176|177|178|179|180|181|182|183|184|185|186|187|188|189|190|191|192|193|194|195|196|197|198|199|200|201|202|203|204|205|206|207|208|209|210|211|212|213|214|215|216|217|218|219|220|221|222|223|224|225|226|227|228|229|230|231|232|233|234|235|236|237|238|239|240|241|242|243|244|245|246|247|248|249|250|251|252|253|254|255|256|257|258|259|260|261|262|263|264|265|266|267|268|269|270|271|272|273|274|275|276|277|278|279|280|281|282|283|284|285|286|287|288|289|290|291|292|293|294|295|10# read:296|297|298|299|300|301|302|303|304|305|306|307|308|309|310|311|312|313|314|315|316|317|318|319|320|321|322|323|324|325|326|327|328|329|questions:1st and 2nd try:are the named pipes equivalent to classic | pipes as they areknows e.g. from bash in this particular case?does producent always wait for consument? if yes then what is purpose of pipe buffers? is this behavior known as blocking communication?how does linux know when the consument is connected to pipe and thus when the communication can happen? i've tried lsof named_pipe but it gives me nothing, where is this information stored? i have also try following and result was that cat cannot read from pipe.#1mkfifo /tmp/mypipe#2echo 1 >/tmp/mypipe#3rm /tmp/mypipe#4mkfifo /tmp/mypipe#5cat /tmp/mypipeis typing: producent >/tmp/mypipethe equivalent of typing command | i mean the situation when somebody wants to pipe one command to another but forget to type another command after pipe (ps in this case also did not show first command)?3rd try:what is difference between cat and tail -f in this particular case?4th try:what is going on here? why the chunks of read data are not the exactsize? i was expecting output as:1# read: 1| 2# read: 2| 3# read: 3|ps:also i have tried different order of starting commands (reading first and writing after) but the result was the same.pps:i hope this is clear but:producer = process that writes to pipe.consumer = process that reads from pipe.is this possible explain to guy which has mostly scripting knowledge with bit of c? thank you very much.edit in reply to: joe sewellok clear 2. i understand that both run in parallel, or in other words, following two are not the same:find | lessvsfind > /tmp/file && less /tmp/filemy further observation discovers that, when i run following, hdd is not working seems that it is stopped until less command has enough data to displayfind | lesswhen i hit shifg+g (go to the end of file in less) hdd starts immediately to work and data starts outputting. does this mean that when less command has enough data to display it will somehow tell find to not produce further data? this is what you mean by synchronization? also the amount of data writes to pipe corresponds to it buffer size? i have also noticed that find changes it state (ps aux - stat column) from s+ to d+ after i hit shift+g in lesss interruptible sleep (waiting for an event to complete)d uninterruptible sleep (usually io)+ is in the foreground process group.[wakatana@~] [63 files, 178mb]> ps aux | egrep -w 'less|find'wakatana 6071 0.0 0.0 12736 1088 pts/5 s+ 23:15 0:00 findwakatana 6072 0.0 0.0 7940 928 pts/5 s+ 23:15 0:00 lesswakatana 6183 0.0 0.0 7832 892 pts/6 s+ 23:20 0:00 egrep --color=auto -w less|find[wakatana@~] [63 files, 178mb]> ps aux | egrep -w 'less|find'wakatana 6071 0.0 0.0 12808 1304 pts/5 d+ 23:15 0:00 findwakatana 6072 0.0 0.0 9556 2508 pts/5 s+ 23:15 0:00 lesswakatana 6193 0.0 0.0 7832 892 pts/6 s+ 23:21 0:00 egrep --color=auto -w less|findwho sends this signal, consument to producent? if yes then howconsument know that he is connected to the pipe which already hasproducent (e.g. my example with rm pipe)?ok clearok cleari think that the new lines is not the case that confuses me. based on my previous observations (and you confirmed that: yes, both ends wait for each other.). i was expecting this:i. 1st iteration in 1st loop will write to pipe and because nobody isreading it will wait here.ii. when 2nd loop is issued then the data which were written by 1stloop in 1st iteration will be read, nothing more was written here sonothing more can be read.iii. 2nd loop will wait for next data to be written by 1st loop or(because order no matter) 1st loop will wait until written data willbe read by 2nd loop, and so on and so on.because of this i was expecting that one write will corresponds to one read. i was also verifying if loop is not running so i modified a bit original command to see if something will be printed to stdout even if consument wont be reading, but nothing was printed.for i in $(seq 1 10000); do if [ $(( $i % 5 )) -eq 0 ]; then echo $i; else echo -n $i|> /tmp/mypipe; fi;donesince the writing process isn't sending any newlines, the reader simply reads until it's told it got enough.who will tell consument that he's got enough?in the first case it's probably because the fifo's buffer filled up,how can i fill buffer if communication is blocked (as i described above)?and therefore got flushed through to the reader.what do you mean by this? sorry for my english.while there are ways to make communication asynchronous ...can you please briefly describe what is the difference between asynchronous and synchronous in this case?",
    "present_kp": [
      "pipe",
      "fifo"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "why are my photographs (or videos?) counted against my storage limit?. i'm uploading photos to google storage using picasa. i'm not using full resolution - i leave google to resize them from full resolution. why is it using some of my storage limit? might it be caused by uploaded videos?",
    "present_kp": [],
    "absent_kp": [
      "picasa web albums"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to programmatically add videos to youtube playlist?. i have a list of several youtube urls that i would like to add programmatically to a youtube playlist (in my channel). (actually, i have several target playlists in mind, each getting a subset of the urls in my list.)is there a way i can add these programmatically (e.g. with python or perl, etc.)?p.s., (i imagine realize that there's a way to do this using the youtube api, but my (very limited) understanding of this approach is that it would require me to create and register (with google) a web app, and it would be this web app that would make api calls. if this picture is correct, it looks like a lot of hassle for what i want to do.)edit: by way of clarification:i am familiar with the info given adding a video to a playlist, but, as i explained, i am hoping to find some otherway to do this.i have no problem with registering myself to get a developer key, nor i have any problem with including a developer key with every request; but, in principle, including a developer key in one's requests does not require a webapp (a simple script, or even a command-line one-liner, is perfectly adequate, technically speaking, to this task); therefore, the webapp business seems to me an extraneous artifact of google's registration procedure, one that greatly complicates what would otherwise be a very simple programming task.",
    "present_kp": [
      "youtube",
      "youtube playlist"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "does blogger get a site better rankings for being owned by google?. given that i want to create a blog and purchase my own domain:the fact that blogger is provided by google, may it imply a higher indexing performance, compared both to the other weblog publishing tools (like wordpress, tumblr...) and creating a website fom scratch?",
    "present_kp": [
      "google",
      "indexing",
      "blogger"
    ],
    "absent_kp": [
      "seo",
      "blogspot"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "finding all partitions with filesystems. i'm writing a script that will copy all of the files on a device to a directory. the problem is that some of the devices have multiple partitions and some of the partitions don't have filesystems to copy files from. at the moment, i'm thinking about using lsblk to get a list of partitions on the device and file -s to check for a filesystem on each partition.is there a less brute way to do what i am trying to do?here is information regarding the empty partition problem:# /dev/sdb is a flashdrive with two partitions# /dev/sdb1 has no filesystem# /dev/sdb2 has an ext4 partition$ lsblk -fi...sdb vfat carrier-r c84b-6a72 |-sdb1 vfat carrier-r c84b-6a72 '-sdb2 ext4 carrier-r 33ebb632-68a5-4bf5-bd29-90733af9699e...$ lsblk -ln -o name,fstype...sdb vfatsdb1 vfatsdb2 ext4...# as confirmation, mounting the partition fails$ mount -t auto /dev/sdb1 /mntmount: wrong fs type, bad option, bad superblock on /dev/sdb1...$ dmesg | tail...[ 985.933627] ext4-fs (sdb1): vfs: can't find ext4 filesystem[ 985.935722] ext4-fs (sdb1): vfs: can't find ext4 filesystem[ 985.937603] ext4-fs (sdb1): vfs: can't find ext4 filesystem[ 985.939623] fat-fs (sdb1): invalid media value (0xa7)[ 985.939627] fat-fs (sdb1): can't find a valid fat filesystem",
    "present_kp": [
      "filesystems",
      "partition"
    ],
    "absent_kp": [
      "block device"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "used with permission reusable in a gplv2 derivative work?. the license file of a gplv2 application i am modifying contains a statement saying that certain resources used in the application were used with permission from 3rd parties - such as textual material from a published book and certain graphics.question: in my derivative work based off of this application, am i required to seek permission from those 3rd parties to use those same resources? or does the gplv2 encompass the fact that the use of these resources was already permitted in the original work, and thus the 3rd-party permission flows through to my derivative work?",
    "present_kp": [
      "gpl"
    ],
    "absent_kp": [
      "licensing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "interactively deleting files from a list. i'd like to remove a list of files and be asked for confirmation. the list is in the list.txt file. why the following command doesn't work properly?while read i; do rm -i $i; done < list.txtthe previous command doesn't wait for me but fortunately no file is deleted.i'm using bash.",
    "present_kp": [
      "bash",
      "rm"
    ],
    "absent_kp": [
      "shell",
      "io redirection"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "kgdb not returning control to gdb. i've compiled a kernel with kgdb support and i'm trying to debug it. i have two machines running, a debug machine (running the kgdb kernel) and the machine i'm using to debug it. they are connected via two serial cables. i can operate the debug machine's serial console through ttys0 (on both machines) and i can connect to kgdb with gdb over ttys1 (on both machines).this works fine up until a point: i boot the debug machine; it waits for gdb to connect; gdb connects; i can set breakpoints or whatever using gdb; i tell gdb to continue; and the kernel continues to boot.the problem is when i next hit a breakpoint gdb doesn't seem to realize that a breakpoint has been hit. the kernel stops when it's meant to, but gdb doesn't do anything. it just sits there as if nothing has happened.does anyone know what might cause this?",
    "present_kp": [
      "kernel",
      "gdb"
    ],
    "absent_kp": [
      "linux",
      "linux kernel",
      "debugging"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to fix opera and google chrome?. recently i've changed from mint 17 to linux mint 18. unlike on mint 17, now google chrome border is like it is not selected (light-gray color) and returns [warning:flash/platform/pepper/pep_module.cpp(63)] sandboxedthis error, and has some graphical problems, while playing screen sections are turning black for some seconds.i also installed opera, it doesn't have any visual issues, but terminal shows[<phone>:error:object_proxy.cc(583)] failed to call method: org.freedesktop.dbus.properties.get: object_path= /org/freedesktop/upower: org.freedesktop.dbus.error.invalidargs: no such interface 'org.freedesktop.upower.device'[warning:flash/platform/pepper/pep_module.cpp(63)] sandboxedvector smash protection is enabled.[<phone>:error:ticl-message-validator.cc(212)] registration_digest must be non-empty[<phone>:error:ticl-message-validator.cc(319)] field registration_summary failed validation in { protocol_version: { version: { major_version: 3 minor_version: 2 } } client_token: nwqcvmarvrn/fjkztoaz4a== registration_summary: { num_registrations: 0 registration_digest: } server_time_ms: 0 }[<phone>:error:ticl-message-validator.cc(361)] field header failed validation in { header: { protocol_version: { version: { major_version: 3 minor_version: 2 } } client_token: nwqcvmarvrn/fjkztoaz4a== registration_summary: { num_registrations: 0 registration_digest: } server_time_ms: 0 } }[<phone>:error:protocol-handler.cc(145)] received invalid message: { header: { protocol_version: { version: { major_version: 3 minor_version: 2 } } client_token: nwqcvmarvrn/fjkztoaz4a== registration_summary: { num_registrations: 0 registration_digest: } server_time_ms: 0 } }what does this errors mean and how can they be fixed?",
    "present_kp": [
      "linux mint",
      "chrome",
      "opera"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "using two memory controllers with sparse_mem on at91sam9g45. there are lot posts on this problem in google, but i have tried all the solutions but they are not working for me, so i'm posting here i am working with at91bootstarp 3.5.9 with linux kernel 3.6.9. my problem is that linux does not use the second memory bank (128 mb on ebi_cs1 mapped at 0x20000000).the memory mapping is as follows:0x70000000 => 0xc0000000 (128 mb - ddrsdrc0) 0x20000000 => 0xc8000000 (128 mb - ebi_cs1)as per atmel evaluation (user) board.while booting i see this message: ignoring ram at 70000000-77ffffff (vmalloc region overlap).i have enabled sparsemem and also tried patches posted at:<url> ... -with.htmli have checked & verified my ddr2 initializations in at91bootstrap and i dont see any issues at bootstrap. tested both memory banks, dont see any issues.i have added uboot boot args as:mem= mem=128m@0x20000000 mem=128m@0x70000000 console=ttys0,115200 root=/dev/mmcblk0p2 rw rootdelay=2 rootwait=1when i boot, i am getting following warning and board is getting configured with 128 mb not with 256 mb.ignoring ram at 70000000-77ffffff (vmalloc region overlap).cpu: arm926ej-s [41069265] revision 5 (armv5tej), cr=00053177cpu: vivt data cache, vivt instruction cachemachine: atmel at91sam9m10g45-ekignoring ram at 70000000-77ffffff (vmalloc region overlap).memory policy: ecc disabled, data cache writebackat91: detected soc type: at91sam9g45at91: detected soc subtype: unknownat91: sram at 0x300000 of 0x10000 mapped at 0xfef68000clocks: cpu 400 mhz, master 133 mhz, main 12.000 mhzbuilt 1 zonelists in zone order, mobility grouping on. total pages: 32512kernel command line: mem=128m@0x20000000 mem=128m@0x70000000 console=ttys0,115200 root=/dev/mmcblk0p2 rw rootdelay=2 rootwait=1pid hash table entries: 512 (order: -1, 2048 bytes)dentry cache hash table entries: 16384 (order: 4, 65536 bytes)inode-cache hash table entries: 8192 (order: 3, 32768 bytes)memory: 128mb = 128mb totalmemory: 124424k/124424k available, 6648k reserved, 0k highmemvirtual kernel memory layout: vector : 0xffff0000 - 0xffff1000 ( 4 kb) fixmap : 0xfff00000 - 0xfffe0000 ( 896 kb) vmalloc : 0xc8800000 - 0xff000000 ( 872 mb) lowmem : 0xc0000000 - 0xc8000000 ( 128 mb) modules : 0xbf000000 - 0xc0000000 ( 16 mb) .text : 0xc0008000 - 0xc04d5f18 (4920 kb) .init : 0xc04d6000 - 0xc050022c ( 169 kb) .data : 0xc0502000 - 0xc05345a0 ( 202 kb) .bss : 0xc05345c4 - 0xc055711c ( 139 kb)nr_irqs:16 nr_irqs:16 16at91: 160 gpio irqs in 5 bankssched_clock: 32 bits at 100 hz, resolution 10000000ns, wraps every 4294967286msconsole: colour dummy device 80x30",
    "present_kp": [
      "linux kernel",
      "arm"
    ],
    "absent_kp": [
      "boot loader"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "photos being copied all over the place. we have a rather popular website with plenty of photos. our whole business depends on our content - and the photos are important in this. we invest a lot of time, effort, and money into taking these pictures. on our website we have clear copyright notices, we have the website name and logo in the photos, and we have a photo licensing page which states the prices of licensing our photos. despite all this, our photos are copied by personal and commercial websites alike. we really want to do something about this. we do not want them to take out the photos and leave it at that. we want them to pay for the usage, as we clearly state on our website. now a few questions come to mind:can we legally force them to pay right away? or are we obligated to first write a cease and desist letter?photos are used on websites throughout the world. are there any worldwide rules for this? has anybody experience with doing these things outside of their home country? should we hire a lawyer in any country? or could a local lawyer contact oversees companies directly?",
    "present_kp": [
      "copyright"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "entity-component-system architecture: interaction between systems. i am studying the entity-component-system architecture philosophy. as i have read about it, a typical entity system has:1) entities - which are merely id tags which have a number of components2) components - which contain data on various aspects of an enity that the component is responsible for3) systems - which update relevant components of every entity. say, a rendering system updates the rendering component, or simply saying, draws a picture that is stored in the data of that component. a positional and movement system handles position and movement of each entity who has a corresponding component.these statements follow from this article which in my opition tries to be the most clear and pure in it's statements - but the author did not explain how the interaction between systems should be realized. for example, the rendering system must know the data from the positional component of an entity in order to draw it in a correct position. and so on.so the question is - how should i implement the interaction between the various systems?",
    "present_kp": [
      "architecture",
      "entity",
      "component"
    ],
    "absent_kp": [
      "concepts",
      "message passing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "bridge eth0 and wlan0. i've a computer, c, a router r, and a raspberry pi, p. they are connected:internet <--ethernet--> r <--wlan--> p <--ethernet--> cnow i want c to be able to access the internet.the p has wlan0 and eth0, my first thought was to bridge eth0 and wlan0 but that is not possible due to the nature of wifi i've learned.next approach is to add a dhcp server to p and let c lease an ip number. it works fine and ip route on c gives:10.254.239.0/27 dev eth0 src 10.254.239.13 default via 10.254.239.10 dev eth0and ifconfig on p giveseth0 link encap:ethernet hwaddr b8:27:eb:44:bb:71 inet addr:10.254.239.10 bcast:10.254.239.31 mask:255.255.255.224 inet6 addr: fe80::3206:e7e:fb7e:23d5/64 scope:link up broadcast running multicast mtu:1500 metric:1 rx packets:569 errors:0 dropped:0 overruns:0 frame:0 tx packets:235 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 rx bytes:142936 (139.5 kib) tx bytes:50384 (49.2 kib)lo link encap:local loopback inet addr:127.0.0.1 mask:255.0.0.0 inet6 addr: ::1/128 scope:host up loopback running mtu:65536 metric:1 rx packets:19 errors:0 dropped:0 overruns:0 frame:0 tx packets:19 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 rx bytes:1444 (1.4 kib) tx bytes:1444 (1.4 kib)wlan0 link encap:ethernet hwaddr b8:27:eb:11:ee:24 inet addr:192.168.0.106 bcast:192.168.0.255 mask:255.255.255.0 inet6 addr: fe80::2501:6a8:8bcf:4a40/64 scope:link up broadcast running multicast mtu:1500 metric:1 rx packets:5415 errors:0 dropped:4989 overruns:0 frame:0 tx packets:454 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 rx bytes:815495 (796.3 kib) tx bytes:49230 (48.0 kib)c can ping p but not r, leaving me to believe that p has some routing error.how can i configure p to pass traffic between r and c?ip route on p gives:default via 192.168.0.1 dev wlan0 metric 303 10.254.239.0/27 dev eth0 proto kernel scope link src 10.254.239.10 192.168.0.0/24 dev wlan0 proto kernel scope link src 192.168.0.106 metric 303 also on p$ cat /proc/sys/net/ipv4/ip_forward1",
    "present_kp": [
      "routing",
      "wlan"
    ],
    "absent_kp": [
      "isc dhcpd"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "dual keyboard shortcuts in gnome. i would like to set multiple keyboard shortcuts doing the same thing. my particular example is volume up/down, i would like to retain the standard settings i have (sound/volumeup - xf86audioraisevolume, my laptop dedicated button) and i would like to add a second set (tux+up). how can i do that? thanks a lot.",
    "present_kp": [
      "gnome",
      "keyboard shortcuts",
      "volume"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "within an interface specified by a core component, should it request ireadonlycollection or ienumerable?. in thinking about the principle of be conservative in what you send and liberal in what you accept, i generally try to make my methods receive ienumerable parameters, but emit a ireadonlycollection (except where deferred execution is truly valuable, then i do emit an ienumerable).however, i just realized that when specifying methods in an interface, this may be reversed. is that true?should my interface specifications be telling any implementers i'll give you an ireadonlycollection, but you can just give me any old ienumerable back? (again, if deferred execution makes sense, i would provide an ienumerable as a parameter type instead.) this makes those methods the reverse, accepting a narrow/conservative set but emitting a broad/liberal one. however, this seems right to me because with the dependency inversion, in reality the input arguments when the method is called are really the output arguments of the core system (specifying the interface).i'm just wondering if my thinking is right on this or not. it seems that making the implementer side as easy as possible is really where the benefit is.i'm realizing that part of the issue comes from my concern about not requiring implementers to think deeply about whether deferred execution makes sense or not. knowing when an enumerable will be enumerated can be very important in a system, and with the separation of concerns that an interface can help bring to code, i wouldn't want to be so liberal in what the interface method outputs (which is ultimately an input to the core system) that improperly-deferred ienumerables get thrown around...this is not about speed. not sure why this was raised, but please do not even think for a second about performance implications here. that is not the question.",
    "present_kp": [
      "parameters"
    ],
    "absent_kp": [
      "c#",
      "interfaces",
      "return type"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to view certificate detail that contains more than one certificates. in ubuntu 16.04 os there is /etc/ssl/certs/ca-certificates.crt which has a lot of: -----begin certificate-----xxx-----end certificate-----how can i view what certificates are in the file as individual certificate, one by one?i have tried:openssl x509 -noout -issuer -subject -dates -in ca-certificates.crtbut it looks as there is only one cert in it: issuer= /cn=accvraiz1/ou=pkiaccv/o=accv/c=essubject= /cn=accvraiz1/ou=pkiaccv/o=accv/c=esnotbefore=may 5 09:37:37 2011 gmtnotafter=dec 31 09:37:37 2030 gmtshouldnt there be more? how can i see all of them?",
    "present_kp": [
      "openssl",
      "ssl"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "what units should shannon entropy be measured in?. the only examples i've seen use bits as a measurement of entropy, but all these examples happen to use binary code alphabets. if we wanted to see how well a coding with a code alphabet of length n works, would we measure entropy in units of n?or would it make sense to stay using bits if we're comparing codings with binary and n-length code alphabets?",
    "present_kp": [
      "entropy"
    ],
    "absent_kp": [
      "information theory"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "solving quasilinear/nonlinear equations obtained from the discretization of partial differential equations. when you solve numerically a (system of) linear partial differential equation (pde) as for example lapace's equation $ abla^2arphi = 0$ or poisson's equation $ abla^2arphi = f$ you obtain a linear system of equations that can be solve easily with the tools provided by the theory of linear algebra. conversely, when you solve a quasilinear (system of) pde, as for example a simple convection-difussion problem $ abla^2arphi - ec{v}\\cdot ablaarphi = f$, or a nonlinear (system of) pde, as for example a general convection-difussion problem $ abla\\cdot(\\gamma ablaarphi) - abla(ec{v}arphi) = f$, you obtain a nonlinear system of equations that cannot be solved (directly) with the same tools.one possible solution is the use newton or quasi-newton method, but you must evaluate full or approximate jacobian, which increases the complexity of algorithm. another solution is to use jacobi/gauss-seidel/sor methods, but they do not work unless the system is diagonally dominant and the nonlinearities are small. what methods are used in practice or more often to solve these problems?",
    "present_kp": [
      "pde"
    ],
    "absent_kp": [
      "solver"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to make files unmodifiable and undeletable without 'chattr'?. i (and my collaborators) have a directory tree containing ~160k files (most of them are automatically generated by data gathering instrumentation).these files reside in a system that gets backed up constantly, and therefore, we are not too worried about data loss or corruption, as long as we have the information required to restore from backup when needed.we're much more worried about inadvertent data loss or corruption resulting from user error (especially buggy user-written code), because this means, at best, wasted work until the problem is detected.therefore, we want to make these files undeletable and unmodifiable.unfortunately, neither of us has permission to use chattr on this system, which rules out applying chattr +i to these files.is there some other way, not requiring special permissions, to approximate chattr +i?the rest of this post describes a couple of possibilities we've considered, along with their shortcomings.one possibility would be to apply chmod -r a-w data, where i've used data as shorthand for the root of the directory tree in question.this is fine as a first approximation, but it goes a bit too far, because it renders many operations that we may need to perform occasionally (e.g. consolidating several subdirectories into one)a second possibility would be something likefind data -type f -exec chmod a-w {} \\;this is a bit more flexible, and the files can no longer be modified, but they can still be deleted.",
    "present_kp": [
      "files",
      "permissions"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "rsyslogd taking up all my drive space, only when the service is started. i have configured rsyslogd to write log files to an external drive. when the service is not started, but the drive is mounted, my disk utilization (by df) is low. as soon as i start rsyslogd, the disk utilization creeps up over a minute or so to very high utilization on the main drive (not the drive that it's logging to.) using du to try to find a cache file or something else that might be used by rsyslogd shows nothing. what could be causing this?",
    "present_kp": [
      "rsyslog"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "scraping the names of a whole category with a three-liner code. i've written a script in python using beautifulsoup to parse the name of different coffee shops spreading across 51 pages in yellowpage website. i'm thrilled to see that it works perfectly. except for importing libraries, i used three lines of code to do this. i think this time i've done this errorlessly.here is what i've tried with:import requestsfrom bs4 import beautifulsoup for i in range(1, 52): for title in beautifulsoup(requests.get(<url>}.format(i)).text, lxml).findall(h2,{class:n},a): print(title.text)",
    "present_kp": [
      "python",
      "beautifulsoup"
    ],
    "absent_kp": [
      "python 3.x",
      "web scraping"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is the recommended seo procedure when moving to a site with several subdomains to a new domain?. is there any known seo recommendation for migrating a domain with several subdomains to a new domain name?as google can see subdomains as internal parts of root domains (link), what could be the seo impact if we only migrate the root domain (<url> to <url>) and leave the subdomains unmodified (a.old.com, b.old.com, etc...)?consider the subdomains indexed at google, with moderated inbound links, and with links to root domain. is it worth it to migrate all subdomains or by migrating the root domain (with the proper procedure) will be enough? the goal is to keep our website traffic, while risking as little as possible .i couldn't find any article describing this scenario.",
    "present_kp": [
      "seo",
      "google",
      "subdomain"
    ],
    "absent_kp": [
      "301 redirect",
      "migration"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "does it matter if my meta description is truncated?. so i'm programmatically generating meta descriptions from the first 160 characters of my site's product listing descriptions. (i'm putting a 160 character limit on it. is that still best practice?) does it matter if a word at the end of the description gets truncated? so if the word was, say, webmasters, it might get truncated to webmast. i can make it so only whole words are included by dropping the final truncated word, but some sentences will necessarily be chopped in half. does that also matter?",
    "present_kp": [
      "meta description"
    ],
    "absent_kp": [
      "seo",
      "meta tags"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "implement mlp in tensorflow. there are many resources online about how to implement mlp in tensorflow, and most of the samples do work :) but i am interested in a particular one, that i learned from <url> in which, it uses a cost function defined as follow:$j( heta) = rac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{k} \\left[ -y_k^{(i)} \\log((h_ heta(x^{(i)}))_k - (1 - y_k^{(i)}) \\log(1 - (h_ heta(x^{(i)}))_k ight]$$h_ heta$ is the sigmoid function.and there's my implementation:# one hidden layer mlpx = tf.placeholder(tf.float32, shape=[none, 784])y = tf.placeholder(tf.float32, shape=[none, 10])w_h1 = tf.variable(tf.random_normal([784, 512]))h1 = tf.nn.sigmoid(tf.matmul(x, w_h1))w_out = tf.variable(tf.random_normal([512, 10]))y_ = tf.matmul(h1, w_out)# cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(y_, y)cross_entropy = tf.reduce_sum(- y * tf.log(y_) - (1 - y) * tf.log(1 - y_), 1)loss = tf.reduce_mean(cross_entropy)train_step = tf.train.gradientdescentoptimizer(0.05).minimize(loss)correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))# trainwith tf.session() as s: s.run(tf.initialize_all_variables()) for i in range(10000): batch_x, batch_y = mnist.train.next_batch(100) s.run(train_step, feed_dict={x: batch_x, y: batch_y}) if i % 100 == 0: train_accuracy = accuracy.eval(feed_dict={x: batch_x, y: batch_y}) print('step {0}, training accuracy {1}'.format(i, train_accuracy))i think the definition for the layers are correct, but the problem is in the cross_entropy. if i use the first one, the one got commented out, the model converges quickly; but if i use the 2nd one, which i think/hope is the translation of the previous equation, the model won't converge.",
    "present_kp": [
      "tensorflow"
    ],
    "absent_kp": [
      "machine learning"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "can someone sue me/take my domain?. i have found a great domain that isn't in use, but the .com and .net domains are already taken. there's nothing on the domains though, it just says they are registered with network solutions and are under construction.my question is: if i buy the .org version of the domain, and the .com guys later start a company on that domain, can they sue me or make me change name because it is too similar to their .com domain? should i avoid using domains that have already been registered but with a different ending?",
    "present_kp": [
      "domains"
    ],
    "absent_kp": [
      "legal",
      "registration",
      "purchase"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "sed: replace text keeping digit occurrence. i have some text like:blablabla <b>[intlink id=</b>2204<b> type=page] blalalai want replace it for remove the </b> and <b>, keeping the id=number. so it should result like:blablabla <b>[intlink id=2204 type=page] blalalai try with:sed -i 's@id=</b>[[:digit:]]\\+<b>@id={1}@g' ~/edit.txtblablabla <b>[intlink id={1} type=page] blalalaalso i try with:sed -i 's@id=</b>[[:digit:]]\\+<b>@id=\\1@g' ~/edit.txtbut i get:blablabla <b>[intlink id= type=page] blalalaso, how i can keep the id number text in the regex digit?",
    "present_kp": [
      "sed",
      "replace"
    ],
    "absent_kp": [
      "regular expression",
      "html",
      "trim"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "cross compile arm program to intel. i have searched around for a way to run a program meant for arm processors on an intel computer, but i can only find ways to do the reverse, to compile intel programs for arm. are there any open-source cross-compilers that will allow me to do so? thanks for your help.",
    "present_kp": [
      "compiler",
      "arm",
      "intel"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "treating a 1d data structure as 2d grid. i am working with a native class that represents a 2d image as a 1d array. if you want to change one pixel, for example, you need to now how to derive the index from the x,y coordinates.so, let's say we have a 1d array array1d like this:array1d = [ a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y ]in the context of our program, array1d represents a 2d grid:a b c d ef g h i jk l m n op q r s tu v w x yand we want to perform operations on array1d such as:get the value at x,y coordinates (in this example, 1,2 would give l)get any sub-grid using x,y,width,height (1,2,2,2 would give [l, m, q, r])set the value at any x,y coordinate (etc.)how do we do these?",
    "present_kp": [],
    "absent_kp": [
      "data structures",
      "math",
      "graphics"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "regular expression of a language over {a,b,c} which does not contain substring bbb. i'm trying to figure out how to build a regular expression for a language that doesn't contain substring bbb. the alphabet is {a,b,c}. i'm trying to construct a dfa and convert to help me get the regular expression but still stuck as i found the dfa a bit complicated. i appreciate any help. thanks!!!ok, here is the regular expression i've worked on to help me solve the above question.ac[(acb)* u (acbb)]ac*test:aaaa ccccc acbacbacb aaaaaaa ccccc (ok)aa acbb acbb acbb acbb cccccccc (ok)but how about cab or cabb? i then modified the above expression to:ac[(acb)* u (acbb)* u (cab)* u (cabb)]ac*i'm i heading to the right direction?thanks again!",
    "present_kp": [],
    "absent_kp": [
      "regular languages",
      "regular expressions"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "optimizing jquery twitch tv application. i'm trying to get in the habit of writing better looking code and more efficient code blocks. i wrote this twitch tv application which allows you to add and edit channels and it lets you know when a channel is live. is there anything i can work on to make my code work / look better? html<!doctype html public -//w3c//dtd xhtml 1.0 strict//en <url> xmlns=<url> <title>streaming</title> <link href=style.css rel=stylesheet type=text/css /> <script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js></script> <script type=text/javascript src=streamer.js></script> <script type=text/javascript src=dynamic.js></script></head><body> <div id=container> <div id=nav> <ul id=mainnav> <li id=streaminfo></li> </ul> <ul id=chatanchor> <li> <a onclick=$(this).addclass('activechat');$('#streamchat').removeclass('activechat') href=<url> target=chatframe id=mainchat class=activechat> main chat </a> </li> <li> <a href=javascript: void(0) class=tab id=streamchat onclick=changechat()> stream chat </a> </li> </ul> </div> <div style=clear:both></div> <div id=player> <object type=application/x-shockwave-flash id=twitchtv width=780px height=500px></object> </div> <div id=chat> <iframe name=chatframe frameborder=0 scrolling=no id=chatframe src=<url> </div> <div id=chancontrols> <div id=add> <a href=javascript:void(0)> <img src=add.jpg /> </a> <form id=addchan onsubmit=return addchannel()> streamer name: <input type=text name=title /> streamer channel: <span style=color:#383838; font-style:italic> <url> </span> <input type=text name=url /> <input type=submit id=submit value=verify channel /> <span class=successmsg></span> </form> </div> <div id=edit> <a href=javascript:void(0)> <img src=edit.jpg /> </a> </div> </div> <div id=list> <ul></ul> </div> </div></body></html>jqueryvar streams = new array();var current = -1; // for viewers updatevar twitch = false;var youtube = false;var timer = 60000; // milisecondsvar toggleedit = [editlist, addlist];var togglecounter = 0; var youtubeplayer = '<iframe width=765 height=500 frameborder=0 allowfullscreen id=youtubeplayer></iframe>';var twitchplayer = '<object type=application/x-shockwave-flash id=twitchtv width=780px height=500px></object>';var tubes = [zn7-fvtt16k, ivjvcohdaxs, 1_hklftku5y, l3w2mtxbebg, uctlj692f70, zj2zf9tlg2y, mgvwv0zuphm, 7zskqbt3gq0, yhrxv-40wmu, zimoqhpvfqq, taae7sjahiw, xbzobgfm55w, aenydwbm9qw, mhtd4_ids80, afa-rols8ya, celrlmv9a-s, 7re0-ek6mza, wa4tlcgctg4, 0m0rbapxq2k, vicx-6dmoua, njos57ijf-0, k5a_v0mp_fk, dx_1b0w7hzc, xdj7gvc_dsa, 17cllzuibkq, 0krakxfryq4, gj1mz7kgvf0, -6g6czt7h4, lilu2tez7ky, ehcyajs4cbs, ueipcowy4de, v2xgp5ix8he, drqrs40okne, bbcyg_j3o2q, upxzavmhw8k, 0xcn12uvheq, itvjybdcybi, l7ivsdrbhnc, 51v1vmkuyx0, aqqeg3jygoa, xgk84poeynk, maczn2n6q_i, 9q5pz49r9au, dfm140rju4k, qhbvnmf2t7w, yto-6xg3g2m];var updatetimer = setinterval(getlist, timer); // update the streams list every minute$(window).load(function(){ if(localstorage.length == 0) { var streamer1 = new streamer(gaurdsman bob, guardsmanbob, null, null); var streamer2 = new streamer(siv hd, sivhd, null, null); var streamer3 = new streamer(day9 tv, day9tv, null, null); localstorage.setitem(streamer1.url, json.stringify(streamer1)); localstorage.setitem(streamer2.url, json.stringify(streamer2)); localstorage.setitem(streamer3.url, json.stringify(streamer3)); } getstorage(); getlist();});$(document).ready(function(){ $('#add a').click(function() { var lefty = $(this).next(); lefty.animate({ left: parseint(lefty.css('left'),10) == 150 ? -lefty.width() : 150 }); }); $('#addchan input[name=title]').click(function(){ var title = $('#addchan input[name=title]'); if(title.val() == enter streamer name) { title.val(''); title.css(border-color, ); } }); $('#addchan input[name=url]').click(function(){ var url = $('#addchan input[name=url]'); if(url.val() == enter streamer url) { url.val(''); url.css(border-color, ); } }); $('#edit a').click(function(){ if(togglecounter % 2 == 1) { $(document).off('click', '#list li img'); $(document).off('focusout', '#list input.edittitle'); } toggleedit[togglecounter++%2](); });});function getlist(){ $.post( streams.php, {streams : json.stringify(streams)}, function(data) { streams = $.parsejson(data); $.each(streams, function(index, obj){ $.each(obj, function(key, value){ if(current == -1 && value == 'online') { current = index; return false; } }); }); if(current != -1 && !twitch) build(current); else if(current == -1 && !youtube) randomtube(); if(twitch) updateviewers(); addlist(); } );}function build(index){ var data = '<url> // object data var src = 'hostname=<url>&auto_play=false&start_volume=25&channel='; // flashvars param var changevars = '<param name=flashvars \\ value=hostname=<url>&auto_play=false&start_volume=100&channel='+streams[index].url+'/>'; var params = '<param name=allowfullscreen value=true />' + '<param name=allowscriptaccess value=always />' + '<param name=allownetworking value=all />' + '<param name=movie value=<url> />' + changevars; $(#player).html(twitchplayer); $(#twitchtv).html(params); $(#twitchtv).attr(data, data); if(streams[index].status == 'online') $('#streaminfo').html(<span id=\\streamtitle\\>streamer: + streams[index].title + </span> - <span id=iewers\\> + streams[index].viewers + </span> viewers); else $('#streaminfo').text(streamer: + streams[index].title + - offline); current = index; twitch = true;}function addlist(){ var numoffline = 0; var online = ''; var offline = ''; var curr = ''; for(var i = 0; i < streams.length; i++) { if(i == current && streams[i].status == 'online') { curr = '<li class=item><div class=online></div><a style=color:green href=javascript: void(0) \\ title='+streams[i].title+' stream \\ onclick=changestream($(this).text())>'+streams[i].title+'</a></li>'; } else if(streams[i].status == 'online' && i != current) { online += '<li class=item><div class=online></div><a style=color:green href=javascript: void(0) \\ title='+streams[i].title+' stream \\ onclick=changestream($(this).text())>'+streams[i].title+'</a></li>'; } else { offline += '<li class=item><div class=offline></div><a href=javascript: void(0) \\ title='+streams[i].title+' stream - offline \\ onclick=changestream($(this).text())>'+streams[i].title+'</a></li>'; numoffline++; } } if(numoffline == streams.length) { online += '<li class=item><div class=online></div><a style=color:green href=javascript: void(0) \\ title=random youtube onclick=randomtube()>random youtube</a></li>'; } $('#list ul').html(curr + online + offline);}function editlist(){ clearinterval(updatetimer); var online = ''; var offline = ''; var curr = ''; var previoustext; for(var i = 0; i < streams.length; i++) { if(i == current && streams[i].status == 'online') { curr = '<li class=item><div><img src=greendelete.png /></div> \\ <input type=text value='+streams[i].title+' class=edittitle /></li>'; } else if(streams[i].status == 'online' && i != curr) { online += '<li class=item><div><img src=greendelete.png /></div>\\ <input type=text value='+streams[i].title+' class=edittitle /></li>'; } else { offline += '<li class=item><div><img src=reddelete.png /></div>\\ <input type=text value='+streams[i].title+' class=edittitle /></li>'; } } $('#list ul').html(curr + online + offline); $(document).on('click', '#list li div img', function(){ var find = $(this).parent().next(input).val(); alert(find); var index = findobjbytitle(streams, find); var obj = streams.splice(index, 1); removefromstorage(obj[0]); $(this).parent().parent().remove(); }); $(document).on('focus', '#list input.edittitle', function(){ previoustext = this.value; }); $(document).off('focus', '#list input.edittitle', function(){}); $(document).on('focusout', '#list input.edittitle', function(){ var index = findobjbytitle(streams, previoustext); streams[index].title = this.value; updatestorage(streams[index]); });}function updateviewers(){ if(typeof(streams[current]) != 'undefined') $('#viewers').text(streams[current].viewers);}function changechat(){ var chatsrc = $('#chatframe').attr('src'); var chat = chatsrc.split('channel='); $('#chatframe').attr('src', chat[0]+'channel='+streams[current].url); $('#mainchat').removeclass('activechat'); $('#streamchat').addclass('activechat');}function changestream(find){ var found = false $.each(streams, function(index, obj){ $.each(obj, function(key, value){ if(!found && streams[index].title == find) { build(index); current = index; found = true; return false; } }); }); if($('#streamchat').hasclass('activechat')) changechat();}function randomtube(){ var video = tubes[math.floor(math.random()*tubes.length)]; $('#player').html(youtubeplayer); $('#player iframe').attr(src, <url> video); $.ajax({ url: <url> datatype: jsonp, success: function (data){ $('#streaminfo').html('<a href=javascript: void(0) style=color:#ae0000 title=random video onclick=randomtube()>\\ random video</a> -- ' + data.entry.title.$t); } }); youtube = true;}function addchannel(){ var tempstreamer; var title = $('#addchan input[name=title]'); var url = $('#addchan input[name=url]'); if(title.val() == '' && url.val() == '') { title.css(border-color, red); title.val(enter streamer name); url.css(border-color, red); url.val(enter streamer url); return false; } else if(title.val() == '') { title.css(border-color, red); title.val(enter streamer name); return false; } else if(url.val() == '') { url.css(border-color, red); url.val(enter streamer url); return false; } else { title = title.val(); url = (url.serialize()).split('='); } tempstreamer = new streamer(title, url[1], null, null); $.post( chanexists.php, {streams : tempstreamer}, function(data){ tempstreamer = $.parsejson(data); if(!(tempstreamer.status)) { $('#addchan .successmsg').text(channel does not exist!); $('#addchan .successmsg').css('color', 'red'); } else { var objindex = objexists(streams, tempstreamer); if(objindex == -1) { $('#addchan .successmsg').text(channel added!); $('#addchan .successmsg').css('color', 'green'); localstorage.setitem(tempstreamer.url, json.stringify(tempstreamer)); streams.push(tempstreamer); clearinterval(updatetimer); getlist(); } else { $('#addchan .successmsg').text(channel + streams[objindex].title + already added); $('#addchan .successmsg').css('color', 'red'); } } }); updatetimer = setinterval(getlist, timer); return false;}/*******************************/* the following are storage/* manipulation functions*******************************/function getstorage(){ var tempstream; for(var i = 0; i < localstorage.length; i++) { tempstream = $.parsejson(localstorage.getitem(localstorage.key(i))); streams.push(tempstream); }}function removefromstorage(obj){ var found = false; for(var i = 0; (i < localstorage.length && !found); i++) { tempstream = $.parsejson(localstorage.getitem(localstorage.key(i))); if(tempstream.url = obj.url) { localstorage.removeitem(obj.url); found = true; } }}function updatestorage(obj){ var found = false; for(var i = 0; (i < localstorage.length && !found); i++) { tempstream = $.parsejson(localstorage.getitem(localstorage.key(i))); if(tempstream.url = obj.url) { localstorage.removeitem(obj.url); localstorage.setitem(obj.url, json.stringify(obj)); found = true; } }}/*******************/* streamer object/* and member functions*******************/function streamer(title, url, status, viewers){ this.title=title; this.url=url; this.status=status; this.viewers=viewers;}function objexists(arr, obj){ var i = arr.length; while(i--) { if(arr[i].url == obj.url) return i; } return -1;}function findobjbytitle(arr, title){ var found = false; var i = 0; while(!found) { if(arr[i].title == title) { return i; } i++; }}",
    "present_kp": [
      "javascript",
      "jquery"
    ],
    "absent_kp": [
      "optimization"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "recursion into git diff tree using nodegit. i'm using nodegit to develop the backend for a nodejs app that uses a mix of mongodb and git for persistence. the app has to version documents and be able to present diffs between versions, thus the use of git in the backend.for one of the api endpoints, i needed to get the diff between two commits, so i've develop the express middleware getdiffbetweencommits detailed below.afaik, nodegit does not provide this natively. it just provides with commit.getdiff() which only compares a commit to its parent, but i needed to compare to arbitrary commits.git model is as follows:commit -> patches -> hunks -> modified linesi need to traverse this tree, gather all the modified lines for all the hunks in all the patches, and return.the thing is, all the calls that go deeper into this tree (ie. commit.patches(), patch.hunks(), hunks.lines(), are all async (promises), so instead of simply doing a foreach at each level, i had to recursively visit the patches tree.i created a context object to help me iterate it, maintain cursors and stack state, and call expressjs next() function to pass to the next middleware when all the tree has been traversed and there are no more patches to iterate.exports.getdiffbetweencommits = function (req, res, next) { var commitid1 = req.params.stageid1; var commitid2 = req.params.stageid2; var repo; git.repository.open('/path/to/git/repo') .then(function (repository) { repo = repository; return repo.getcommit(commitid1); }) .then(function (firstcommit) { req.commit1 = firstcommit; return repo.getcommit(commitid2); }) .then(function (commit2) { req.commit2 = commit2; req.gitoptions = null; return; }) .then(function () { return req.commit1.gettree(); }) .then(function (committree) { req.commit1tree = committree; return req.commit2.gettree(); }) .then(function (committree) { req.commit2tree = committree; return; }) .then(function () { return req.commit1tree.diff(req.commit2tree); }) .then(function (diffs) { var diffresult = []; req.diffresult = diffresult; var context = { next: next, diffresult: diffresult, modificationcount: 0, pushpatches: function (patches) { this.patches = patches; this.currentpatch = -1; }, nextpatch: function () { this.currentpatch++; if (this.currentpatch !== this.patches.length) { this.patches[this.currentpatch].hunks().then(processhunks.bind(null, this)); } else { this.next(); } }, pushhunks: function (hunks) { this.hunks = hunks; this.currenthunk = -1; }, nexthunk: function () { this.currenthunk++; if (this.currenthunk !== this.hunks.length) { var hunkheader = this.hunks[this.currenthunk].header().trim(); log(hunkheader.substring(0, hunkheader.length - 1)); this.hunks[this.currenthunk].lines().then(processlines.bind(null, this)); } else { this.nextpatch(); } }, increasemodificationcount: function () { this.modificationcount++; req.modificationcount = this.modificationcount; } }; diffs.patches().then(processpatches.bind(null, context)); });};function processpatches(context, patches) { if (patches.length === 0) { return context.next(); } context.pushpatches(patches); context.nextpatch();}function processhunks(context, hunks) { if (hunks.length === 0) { return context.next(); } context.pushhunks(hunks); context.nexthunk();}function processlines(context, lines) { lines.foreach(function (line) { var diffstring = string.fromcharcode(line.origin()) + line.content().trim(); log(diffstring); var originchar = string.fromcharcode(line.origin()); var diffline = { contents: line.content().trim(), isaddition: originchar === '+', isdeletion: originchar === '-', iscontext: originchar === ' ' }; if (diffline.isaddition || diffline.isdeletion) { context.increasemodificationcount(); } context.diffresult.push(diffline); }); context.nexthunk();}i would like a review of this because although it works, it feels too complex and procedural, so perhaps i'm missing a simpler or more proper solution here.",
    "present_kp": [
      "recursion",
      "promise",
      "git"
    ],
    "absent_kp": [
      "javascript",
      "node.js"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "more efficient alternative that checks if a list can be made a palindrome. for my algorithms and data structures class, i have to write an algorithm that is more efficient in the worst case than the following algorithm:def algo_x(a): i = 0 j = len(a)-1 while i < j: if a[i] != a[j]: k = i + 1 while k < j: if a[i] == a[k]: a[k], a[j] = a[j], a[k] break elif a[j] == a[k]: a[k], a[i] = a[i], a[k] break else: k += 1 if k == j: return false i += 1 j -= 1 return truethis algorithm returns true if the elements of list passed as argument can be manipulated (swapped) in order to create a new list, which, if we read from the left or from the right, has the same order of elements (palindrome). else returns false.for example, this list ['a', 'a', 'b', '2', 'b', 'b', 2] can be ordered so that we have a list with the same elements in the same positions, if we read at the same time from the left and from the right: ['a', 'b', '2', 'b', '2', 'b', a].note that this is my interpretation of the algorithm, they did not tell us what this algorithm was supposed to do.if i am not wrong, this algorithm, in the worst case (and average case), is o(n2), and (n) in the best case.the exercise specifically states that i don't have necessarily to change the list (like in algo_x), but just to return true or false specifying respectively if the list can be made a palindrome or not. we cannot use libraries, but just built-in constructs. we cannot even use slice, for example. this is because we don't know exactly the time complexity of those functions. i will edit my questionto make a better algorithm for the worst case, i thought i could use merge sort, whose time complexity is always n*log(n), plus a loop, which would not make the algorithm worse, asymptotically.this is my merge function for my merge sort function:def merge(a, b): ls = [] a, b = 0, 0 while a < len(a) and b < len(b): if a[a] <= b[b]: ls.append(a[a]) a += 1 else: ls.append(b[b]) b += 1 while a < len(a): ls.append(a[a]) a += 1 while b < len(b): ls.append(b[b]) b += 1 return lsthis is my merge sort function:def merge_sort(a): if len(a) < 2: # basic condition return a l = merge_sort(a[0:len(a)//2]) r = merge_sort(a[len(a)//2:]) return merge(l, r)finally, here's my alternative, which returns true, if the list can be made a palindrome, false otherwise:def better_algo_x(a): if len(a) < 2: return true sorted_a = merge_sort(a) odd_groups = 0 current = sorted_a[0] c = 1 # used to count the number of characters that are equal between them for i in range(1, len(sorted_a)): if current == sorted_a[i]: c += 1 else: if c % 2 == 1: odd_groups += 1 if odd_groups > 1: return false current = sorted_a[i] c = 1 # for the last group of characters if c % 2 == 1: odd_groups += 1 if odd_groups > 1: return false return truei have a few questions:are my functions correct? is my analysis of the time complexity of the algorithm algo_x correct?does my better_algo_x do what it is supposed to do? does it do it in n*log(n) in the worst case?can i still improve it? how?do you know (other) better alternatives for algo_x?of course, some questions might seem silly, but i would like to hear the opinion of some experts. of course, i have tried my algorithms.",
    "present_kp": [
      "algorithms"
    ],
    "absent_kp": [
      "algorithm analysis",
      "python 3.x"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "gnu emacs; does the gui version offer anything more than the ability to have a gui menu?. i've been looking at gnu emacs for a few months now, on and off (mainly off), and i've really only gone as far as testing a few basic things which i especially want in an editor... i'm slowly getting to realize its topography, and it is starting to make (good) sense.... the main thing i've noticed is that it seems to work exactly the same in the x-gui version as it does in the x-terminal version (and i suspect it would be pretty much the same in a non-gui environment... i originally thought that i would feel very uncomfortable working in a non-gui editor, and that has been the case, but the more i dabble in the emacs waters, the less significant that need becomes... so i am now looking at it from the other end of the stick... i'm turning my focus to working primarily in the terminal version.. my question is: aside from the obvious gui-menu (which has turned out to be quite unnecessary), is there any notable difference between the versions (x-gui, x-terminal, and no-gui) ?*",
    "present_kp": [
      "emacs"
    ],
    "absent_kp": [
      "x11"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "top k selection in fixed size priority queue with dynamically changing values. my question is something of a scheduling problem. i'd like to know what algorithm to use to find the top k items in a fixed length queue in which the values of the items change dynamically.basically a long running online top k selectioni imagine that operating systems do this all the time, but not being a systems guy, i don't know what terms to search for. thanks in advance.",
    "present_kp": [
      "scheduling"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "eval is evil: dynamic method calls from named regex groups in python 3. i'm working on a simple dice roller for a python irc bot. the particular game this roller is for uses six different kinds of dice, which i've defined using a simple base class, and created six instances of it, passing a string array with the possible values of each dice.class dicebag(object): def __init__(self, dice: list[str]): self.bag = dice def draw(self, count: int) -> str: return [random.choice(self.bag) for _ in range(0, count)]an example of the instances:proficiency_dice = dicebag(['', 's', 's', 'ss', 'ss', 'a', 'as', 'as', 'as', 'aa', 'aa', '!'])boost_dice = dicebag(['', '', 'aa', 'a', 'sa', 's'])now for a user to actually access this function, they must write a textual dice expression in the irc channel. an example usage:<user> ?roll 2p1b<bot> result: p(a, aa) b(s)so i need a way to quickly convert from the dice expression provided by the user, to call the draw method on the appropriate class.i'm using a regular expression to evaluate the dice expression, so named capture groups seem like the most straightforward way to handle this - at the possible cost of my immortal soul, since i'm eval-ing the group name to the proper instance of dicebag.#user's input comes in as a string on the dice varrex = (r'(?p<setback>\\ds)?' r'(?p<ability>\\da)?' r'(?p<difficulty>\\dd)?' r'(?p<proficiency>\\dp)?')to_roll = re.match(rex, dice).groups() for group in to_roll: if group: # we don't care about the nones dicetype = group.split('')[1] dicecnt = group.split('')[0] # handle the extra rolls later eval(dicetype + _dice + .draw( + dicecnt + ))is there a better, or saner, or more pythonic, or perhaps less evil way i could be handling this use case?",
    "present_kp": [
      "python",
      "regex"
    ],
    "absent_kp": [
      "strings",
      "dynamic loading"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "looking for rating functions. i'm looking for something i would call rating functions.i'm searching for some literature about this concept.i'm not really sure about the terminology, but what i mean should be pretty obvious.a type of function that returns a rating of some input.lets have a function that gets some input and returns a number between 0 and 1 as a rating, where 0 is bad and 1 is great. everything between is well between bad and great depending.lets assume inputs are just numbers$f\\colon \\mathbb{n} \\longrightarrow [0,1]$i would like if i have several rating functions be able to compose them.for example if i have the rating functions $r_1$, $r_2$ i would like to compose both to a new rating function that returns a new rating in dependency to $r_1$ and $r_2$now i'm looking for literature, but was unable to find any.can somebody hint me into the correct direction?the correct name for the concept i'm looking for would be great.editi want to implement various rating functions and want to combine themone functions could bealwaysperfect = (x) -> 1alwaysbad = (x) -> 0isodd = (x) -> x%2distancetoone = (x) -> x = 2 if x is 0 1/abs(x)anyone could implement this functions, but the contract for this functions would be to always return a value between 0 and 1i need to evaluate some data with various evaluation conditions. writing these evaluation seperate small functions and combine them seems to be more clearer than writing one big function that does all the evalauting",
    "present_kp": [],
    "absent_kp": [
      "machine learning",
      "mathematical analysis",
      "data mining"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "are there any hash functions/stateless rngs that do not use xor, but produce good quality visual randomness?. i'm looking for a small function from integers to integers - in a language that only has floats - that can act as a visual rng. normally i would use a function such as the one described here:int cash(int x, int y){ int h = seed + x*374761393 + y*668265263; //all constants are prime h = (h^(h >> 13))*<phone>; return h^(h >> 16);}however, for the thing i'm working on, i have the added restriction of only using arithmetic operations. the operations i have available are: +, *, -, /, %, sign(), abs(), pow(base, exponent), ln(), floor(), as well as basic trig functions. i'm having difficulty finding anything that doesn't rely on having bitwise operations available. does anyone know of a hash function that doesn't use xor, that produces reasonable visual randomness when fed a linear sequence of integers?",
    "present_kp": [
      "hash"
    ],
    "absent_kp": [
      "graphics",
      "pseudo random generators"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "same blog post link appearing in different tag pages. i have each blog post tagged with different terms. each term page lists the tagged post link, so that the link may appear more than once. how does that affect my seo?",
    "present_kp": [
      "seo"
    ],
    "absent_kp": [
      "tags"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to find the most frequent word of each file in a directory?. i need to find the most frequent word of each file in a directory and print it like this :12 my /home/test/file1.txt5 you /home/test/file3.txt7 hello /home/test/file4.txti tried:for tmp in <path> do tr -c '[:alnum:]' '[ *]' < $tmp | sort | uniq -c | sort -nr | head -1 done it doesn't work",
    "present_kp": [],
    "absent_kp": [
      "bash",
      "text processing",
      "files"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "add buffer to hash table. retrievedata is a 3rd party api that takes 3 parameters: a buffer, number of bytes to retrieve, and actual bytes required for the buffer.the buffer returned is a non-null terminated unicode string99% of the time, the buffer is less than 256 bytes, but may be larger and needs to handle this scenarioretrievedata(data) is guaranteed to return the reason for using std::map is the id must later be retrieved based on a stringstd:map<std::wstring, unsigned int> map_columns;unsigned int id;unsigned long pcbactual;char* data;do { retrieveid(&id); // find out how large buffer we need to retrieve data retrievedata(null, 0, &pcbactual); data = new char[pcbactual + 2]; // actually retrieve data retrievedata(data, pcbactual, &pcbactual); data[pcbactual] = ''; data[pcbactual + 1] = ''; std::wstring str((wchar_t*)data); map_columns[str] = id;} while (movenext());",
    "present_kp": [],
    "absent_kp": [
      "c++",
      "c++11"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "if a is mapping-reducible to b and is not mapping-reducible to co-b, is a turing-reducible to co-b?. if $a \\leq_m b$ and $a$ is not mapping reducible to $co ext{-}b$, then $a \\leq_t co ext{-}b$.is this true?my intuition is false even if we can find some special case to make it true such as $a=b=co ext{-}a_{tm}$. however, i still can't find a counterexample. could anyone give me a little hint?",
    "present_kp": [],
    "absent_kp": [
      "computability",
      "reductions"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "help understanding the behaviour of at + x minutes. when i run echo hello | at now + 7 minutesi get the following output - job 2 at 2016-12-11 05:06however when i use bash txt | at now + 7 minutesit starts executing immediately. can someone please explain this behaviour?",
    "present_kp": [
      "at"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "run a command in subfolders. i have a command rdseed -d -r -p -f filename which gives an output of response time and etc for each file in a subfolder. the problem is i want to run this command in all subfolders and the output (response time, pz) must be inside the subfolders. i've tried using ls -1 */*.file type | awk -f '[/]' '{print rdseed -d -r -p -f }' but the result is just it do this in the file in the first subfolder and its was also placed in the parent folder.",
    "present_kp": [
      "awk"
    ],
    "absent_kp": [
      "linux"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "modeling a control polygon for a piecewise spline curve. aimi'm modeling a control polygon for a piecewise spline curve. each sub-spline is defined by a location the spline must pass through, as well as a forward tangent, and a backwards tangent. the control points and their tangents can be freely manipulated.i aim at keeping this flexible, so the forward and backward tangents may or may not be colinear. if the user want to make them colinear, i want to be able to express this as a constraint, that holds true even when the control points and/or its tangents are manipulated.additionally, a control point can be reused in an other piecewise spline, to make a network of them. think train fever's free-form tracklaying system:here is a usage scenario:have a start point a with a forward tangent afwd.have an end point c with a backward tangent cbwd.build a spline out of that control polygon: awfd---->cbwdinsert a mid-point b with two tangents (bfwd/bbwd)require b's tangents to be opposite.build a spline out of that control polygon: awfd---->[bbwd|bfwd]---->cbwdmove the tangent bfwd.-> the tangent bbwd is automatically altered to the oppositebreak the constraint on b's tangents-> the tangent bbwd and bfwd are automatically freedhere's how i did ita controlpoint class, which has location, and references two tangencyconstraint objectsa controlpolygon class, is a collection of controlpoints. has a builder, and can generate an immutable piecewisebezier splinea tangencyconstraint interface that provides a tangent vectoran oppositetangency which is a tangencyconstraint. works in pair, one isthe opposite of the other. is an inner (non-static) class of oppositetangencymanageran oppositetangencymanager class wich manages two oppositetangency. makessure both are opposite, and that if one side wishes to break the constraint, the other is informed and becomes a free tangent.here's the codecontrolpoint.java:public class controlpoint { private final point2d location; private final atomicreference<tangencyconstraint> forwardtangency; private final atomicreference<tangencyconstraint> backwardtangency; public controlpoint(point2d location, point2d backwardtangent, point2d forwardtangent) { this.location = new point2d.double(location.getx(), location.gety()); this.backwardtangency = new atomicreference<>(new absolutetangency(forwardtangent)); this.forwardtangency = new atomicreference<>(new absolutetangency(forwardtangent)); } public point2d getlocation() { return new point2d.double(location.getx(), location.gety()); } public point2d getforwardtangent() { return forwardtangency.get().get(); } public point2d getbackwardtangent() { return backwardtangency.get().get(); } public void moveforwardtangency(point2d dir) { forwardtangency.get().move(dir); } public void imposeoppositetangency(point2d forwarddirection) { oppositetangencymanager.lock(forwardtangency, backwardtangency, forwarddirection); } public void imposeoppositetangency(boolean keepforward) { point2d direction; if (keepforward) { direction = forwardtangency.get().get(); } else { point2d opp = backwardtangency.get().get(); direction = new point2d.double(-opp.getx(), -opp.gety()); } oppositetangencymanager.lock(forwardtangency, backwardtangency, direction); } public void freetangency() { forwardtangency.get().destroy(); backwardtangency.get().destroy(); }}controlpolygon.java:/** a controlpolygon is a collection of {@link controlpoint}s. has a {@link builder}, and can generate an immutable piecewisebezier spline */public class controlpolygon { /** builds a {@link controlpolygon} */ public static class builder { private final list<controlpoint> controlpolygon = new arraylist<>(); public builder(point2d begin) { controlpolygon.add(new controlpoint(begin, new point2d.double(), new point2d.double())); } public builder(point2d begin, point2d forwardtangent) { controlpolygon.add(new controlpoint(begin, new point2d.double(), forwardtangent)); } public builder through(point2d next, point2d backwardtangent, point2d forwardtangent) { controlpolygon.add(new controlpoint(next, backwardtangent, forwardtangent)); return this; } public builder endat(point2d next, point2d backwardtangent) { controlpolygon.add(new controlpoint(next, backwardtangent, new point2d.double())); return this; } public controlpolygon build() { return new controlpolygon(new arraylist<>(controlpolygon)); } } private final list<controlpoint> polygon; private controlpolygon(list<controlpoint> poly) { this.polygon = poly; } public controlpoint getcontrolpoint(int i) { return polygon.get(i); } /** generates an immutable {@link piecewisebezier} curve using this controlpolygon. public piecewisebezier tobezier() { point2d p0 = polygon.get(0).getlocation(); piecewisebezier.builder builder = new piecewisebezier.builder(p0); for (int i = 0; i < polygon.size() - 1; i++) { controlpoint cp1 = polygon.get(i); controlpoint cp2 = polygon.get(i + 1); point2d t1 = cp1.getforwardtangent(); point2d t2 = cp2.getbackwardtangent(); point2d p1 = new point2d.double(p0.getx() + t1.getx(), p0.gety() + t1.gety()); point2d p3 = cp2.getlocation(); point2d p2 = new point2d.double(p3.getx() - t2.getx(), p3.gety() - t2.gety()); builder.addbezier(p1, p2, p3); p0 = p3; // loop } return builder.build(); }}tangencyconstraint.java interface :/** provides a tangent vector which may or may not be externally constrained. an eventual constrain can be destroyed. */public interface tangencyconstraint { point2d get(); void destroy(); void move(point2d dir);}oppositetangencymanager.java:/** manages two {@link oppositetangency} objects so they are always opposite. can be disabled, but never re-enabled. */public class oppositetangencymanager { /** a {@link tangencyconstraint} which is always the opposite of another one. */ public class oppositetangency implements tangencyconstraint { private final atomicreference<tangencyconstraint> referencetome; protected oppositetangency(atomicreference<tangencyconstraint> referencetome) { super(); this.referencetome = referencetome; } @override public point2d get() { return oppositetangencymanager.this.gettangencyforme(this); // don't know the current value, must ask the manager } @override public void destroy() { oppositetangencymanager.this.destroy(); // can't do this myself, must ask the manager } @override public void move(point2d dir) { oppositetangencymanager.this.move(this, dir); // can't do this myself, must ask the manager } } /** creates a constraint on two references to tangents so they stay opposite until it is disabled. */ public static void lock(atomicreference<tangencyconstraint> forward, atomicreference<tangencyconstraint> backward, point2d direction) { new oppositetangencymanager(forward, backward, direction); } private final point2d tangency; // the only place the shared tangent is stored private boolean active = true; // can only become false. private final oppositetangency forwardtangency; private final oppositetangency backwardtangency; /** displace the tangent, but keep the constraint. */ public void move(oppositetangency oppositetangency, point2d dir) { if (oppositetangency == forwardtangency) { tangency.setlocation(dir); } else { tangency.setlocation(-dir.getx(), -dir.gety()); } } private oppositetangencymanager(atomicreference<tangencyconstraint> forward, atomicreference<tangencyconstraint> backward, point2d direction) { super(); this.tangency = direction; this.forwardtangency = new oppositetangency(forward); this.backwardtangency = new oppositetangency(backward); forward.get().destroy(); // must destroy the old ones before! backward.get().destroy(); // must destroy the old ones before! forward.set(forwardtangency); backward.set(backwardtangency); } public void destroy() { if (!active) { return; } active = false; forwardtangency.referencetome.set(new absolutetangency(new point2d.double(tangency.getx(), tangency.gety()))); backwardtangency.referencetome.set(new absolutetangency(new point2d.double(-tangency.getx(), -tangency.gety()))); } protected point2d gettangencyforme(oppositetangency requestingtangency) { if (!active) { return null; // should be handled better } if (requestingtangency == forwardtangency) { return new point2d.double(tangency.getx(), tangency.gety()); } else { return new point2d.double(-tangency.getx(), -tangency.gety()); } }}piecewisebezier.java is way outside the scope. any usual library works.example usage (complies with the scenario above):public static void main(string[] argc) { // 1. have a start point 'a' with a forward tangent 'afwd'. point2d a = new point2d.double(0, 0); point2d afwd = new point2d.double(0, 1.0); // 2. have an end point 'c' with a backward tangent 'cbwd'. point2d c = new point2d.double(10, 0); point2d cbwd = new point2d.double(-0.5, -0.5); // 3. build a spline out of that control polygon: 'awfd---->cbwd' builder builder = new controlpolygon.builder(a, afwd); builder.endat(c, cbwd,); controlpolygon polyac = builder.build(); piecewisebezier curveac = polyac.tobezier(); // 4. insert a mid-point 'b' with two tangents ('bfwd'/'bbwd') point2d b = new point2d.double(5, 5); point2d bbwd = new point2d.double(-1, 0); point2d bfwd = new point2d.double(-1, 0.5); builder.insert(1, b, bbwd, bfwd); // 5. require b's tangents to be opposite. controlpolygon polyabc = builder.build(); polyabc.getcontrolpoint(1).imposeoppositetangency(true); // 6. build a spline out of that control polygon: 'awfd---->[bbwd|bfwd]---->cbwd' piecewisebezier bezierabc = polyabc.tobezier(); // 7. move the tangent 'bwfd'. // -> the tangent 'bbwd' is automatically altered to the opposite polyabc.getcontrolpoint(1).moveforwardtangency(new point2d.double(2,3)); // 9. break the constraint on 'b''s tangents // -> the tangent 'bbwd' and 'bfwd' are automatically freed polyabc.getcontrolpoint(1).freetangency();}what i'm looking fori'm looking for a simpler, more elegant way to activate/deactivate/maintain those constraints than a manager. i feel like this framework is a sledgehammer to crack a nut.the builder is annoying. should i not have made one? how to change the definition of the control polygon easily?my control points are directed (have a forward, and a backward). this prevents me from defining a polygon as abwd--->bbwd at the moment. how can i remedy this? a controlpointreverser?i intend to allow making constraints between two distinct control point's tangents (like ctrla.forceparallel(ctrlb)). i also might introduce constraints on location (like ctrla.sitontopof(ctrlb)) or even curvature. how do you feel about applying the same manager mechanic throughout all these constraints?any complaints on style?",
    "present_kp": [
      "java"
    ],
    "absent_kp": [
      "algorithm",
      "graph",
      "computational geometry"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to open a new terminal from my working folder in solaris 10?. in solaris 8, i could to ctrl+t to open a terminal from the file manager. but in solaris 10, i can't seem to be able to.i try to create a shortcut key by gnome-terminal, but the pwd is always my home directory. i want a way to set up a keyboard shortcuts to be able to use in file browser to open up a terminal that has pwd of the directory i am currently on.",
    "present_kp": [
      "gnome",
      "keyboard shortcuts"
    ],
    "absent_kp": [
      "gnome terminal",
      "working directory"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is there a way to know if apache receives a request from a public ip not local to your country?. i'm looking for ways to know if an apache server receives a request , from another country (e.g germany) considering its a fair request (no spam , or proxies). i want it either to appear this as a separate entry(using dns request) in apache logs? or some kind of alert . can this be possible.",
    "present_kp": [
      "apache"
    ],
    "absent_kp": [
      "logging",
      "geolocation",
      "geotargeting",
      "apache log files"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "does second x is np-complete imply x is np-complete?. second $x$ problem is the problem of deciding the existence of another solution different from some given solution for problem instance. for some $np$-complete problems, the second solution version is $np$-complete (deciding the existence of another solution for the partial latin square completion problem) while for others it is either trivial (second nae sat) or it can not be $np$-complete (second hamiltonian cycle in cubic graphs) under widely believed complexity conjecture. i am interested in the opposite direction. we assume a natural $np$ problem $x$ where there is natural efficient verifier that verifies a natural interesting relation $(x, c)$ where $x$ is an input instance and $c$ is a short witness of membership of $x$ in $x$. all witnesses are indistinguishable to the verifier. the validity of witnesses must be decided by running the natural verifier and it does not have any knowledge of any correct witness ( both examples in the comments are solutions by definition). does second $x$ is np-complete imply $x$ is np-complete for all natural problems $x$?in other words, are there any natural problem $x$ where this implication fails?. or equivalently,is there any natural problem $x$ in $np$ and not known to be $np$-complete but its second $x$ problem is $np$-complete?edit: thanks to marzio's comments, i am not interested in contrived counter-examples. i am only interested in natural and interesting counter-examples for np-complete problems $x$ similar to the ones above. an acceptable answer is either a proof of the above implication or a counter-example second x problem which is defined for natural, interesting, and well known $np$ problem $x$.edit 2: thanks to the fruitful discussion with david richerby, i have edited the question to emphasis that my interest is only in natural problems $x$.edit 3: motivation: first, the existence of such implication may simplify the $np$-completeness proofs of many $np$ problems. secondly, the existence of the implication links the complexity of deciding the uniqueness of solution to the problem of deciding existence of a solution for $np$ problems.",
    "present_kp": [],
    "absent_kp": [
      "cc.complexity theory",
      "unique solution"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "checking whether one string is a permutation of another. i solved the standard write a method to determine if one string is a permutation of the other question with the following code, using xor, map, and reduce:from functools import reducefrom operator import xordef checkpermutation(str1, str2): return not bool(reduce(xor, map(ord, str1 + str2)))the idea is that if the two strings are permutations, the xor sum of the int value of every character of the concatenation of the two strings must be 0.if i write the algorithm out by hand it seems come down to o(n) time and space:def checkpermutation(str1, str2): # map every character in the strings to its integer value map = [] str = str1 + str2 for char in str: map.append(ord(char)) # xor every number that was mapped into a sum sum = map[0] for i in range(1, len(map)): sum ^= map[i] return not bool(sum)essentially i think that's what reduce and map are doing but depending on how they're actually implemented in python, the time and space complexity of two solutions may differ.",
    "present_kp": [
      "python",
      "algorithm",
      "strings",
      "complexity"
    ],
    "absent_kp": [
      "comparative review"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how do keywords in google webmaster tools affect search results?. i've checked the keywords list on the google webmaster tools account for my website (which is a message board), and i noticed that the top keywords are things such as topic and forum. i'm guessing that is because those are the words which appear the most within the homepage.the question is, will this impact search results? i'm guessing yes. if so, can this be solved, and how? i already searched on google and on seo forums without any luck. (when it comes to seo, most info looks like bogus to me. i hope i'm wrong, but meanwhile i'll ask here since it seems a reliable place.)",
    "present_kp": [
      "seo",
      "keywords"
    ],
    "absent_kp": [
      "google search console"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is it true that google will effectively penalise mobile-unfriendly sites from april 21, 2015?. i have heard that google will boost mobile-friendly websites in search results. my website is not mobile-friendly.will my website penalised in the google search results if i have not made the website mobile friendly before april 21st?",
    "present_kp": [
      "google search",
      "mobile"
    ],
    "absent_kp": [
      "seo",
      "penalty"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how do i list all lines after two subsequent patterns are matched up to when they don't. i have files that are used to install files with specific owner:group and permissions. i'll ignore permissions for clarity and the details of installing files. once an owner & group are set to a specific pair, then i want to print a separator line followed by all subsequent lines until either the owner or group changes. if i could print the line numbers with the lines, that would be great too. the pair i'm searching for is owner, group as ownerx, groupy respectively.example (i don't need line numbers, so i'll leave them off)type = d owner = root group = staff mode = 0750 <-ignore. owner & group aren't ownerx & groupy ... <- ignore group = groupy <- owner=root, not ownerx, so still not ownerx & groupy ... <- ignore owner = ownerx <-now, owner=ownerx and group=groupy <- print ----------------- mode = 0750 <- print target = /app_dir/conf <- print target = /app_dir/data <- print owner = dilbert <- stop printing since not ownerx & groupy ... <- ignore group = dogbert ... <- ignore group = groupy ... <- ignore owner = ownerx <- print a separator line type = f <- print mode = 0540 <- print source = [path to compiled binary file in source environment]/file1_ver2 <- print target = [path to a bin directory in the install environment]/file <- print owner = oracle <- stop printing ... <end of file, eof> so, the desired output would be: --------- mode = 0750 target = /app_dir/conf target = /app_dir/data --------- type = f mode = 0540 source = [path to compiled binary file in source environment]/file1_ver2 target = [path to a bin directory in the install environment]/file1 that would help me apply the following fixes:change the 1st mode from 0750 (group is read-only) to 0770.change the 2nd mode from 0540 (group can't execute) to 0550.",
    "present_kp": [
      "sed"
    ],
    "absent_kp": [
      "text processing",
      "awk"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how many tiers should my models have in a db driven web app?. in my experience, the model in mvc is often not really a single layer. i would regularly have backend models and frontend models, where the backend ones would have properties that i want to hide from the ui code, and the frontend ones have derived or composite properties which are more meaningful in the context of the ui.recently, i have started to introduce a third layer in between when database normalization created tables that did not really match the conceptual domain objects anymore. in those projects i have model classes that are equivalent with the db structure, these become components of objects that represent the domain model, and finally these are filtered and amended for displaying information to the user.are there patterns or guidelines for how, when and why to organize the data model across application layers?",
    "present_kp": [],
    "absent_kp": [
      "design"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "understanding this dos partition table and cloning to a new drive. i have a piece of equipment that has a motherboard in it which boots to a linux based operating system. i am interested in being able to clone this one hard drive so that in the case it fails i have a backup plan to keep the equipment working. so far i have been able to mount the hard disk to a different pc running linux, and was able to tar the data off the partitions. however i don't know what to make of two of the partitions- sdb2 and sdb13 which do not show up as ext3 file systems; sdb2 is 0x05 extended, and while sdb13 is 0x83 it does not show it has any file system and i cannot mount it.and i am not sure yet how to handle grub if i use a new hard drive of a different size. i'm looking to find out if what i want to do is even possible... if i have enough information from the followingoutput from fdisk -ldisk /dev/sdb: 500.1 gb, 500107862016 bytes255 heads, 63 sectors/track, 60801 cylinders, total 976773168 sectorsunits = sectors of 1 * 512 = 512 bytessector size (logical/physical): 512 bytes / 4096 bytesi/o size (minimum/optimal): 4096 bytes / 4096 bytesdisk identifier: 0x00066c45 device boot start end blocks id system/dev/sdb1 63 208844 104391 83 linuxpartition 1 does not start on physical sector boundary./dev/sdb2 208845 31262489 <phone>+ 5 extendedpartition 2 does not start on physical sector boundary./dev/sdb5 208908 <phone> <phone> 83 linuxpartition 5 does not start on physical sector boundary./dev/sdb6 <phone> <phone> 200781 83 linuxpartition 6 does not start on physical sector boundary./dev/sdb7 <phone> <phone> 297171 83 linuxpartition 7 does not start on physical sector boundary./dev/sdb8 <phone> <phone> 104391 83 linuxpartition 8 does not start on physical sector boundary./dev/sdb9 <phone> <phone> <phone> 83 linux/dev/sdb10 <phone> 23503094 <phone> 83 linuxpartition 10 does not start on physical sector boundary./dev/sdb11 23503158 24097499 297171 83 linuxpartition 11 does not start on physical sector boundary./dev/sdb12 24097563 24691904 297171 83 linuxpartition 12 does not start on physical sector boundary./dev/sdb13 24691968 31262489 <phone> 83 linuxoutput from sfdisk -d# partition table of /dev/sdbunit: sectors/dev/sdb1 : start= 63, size= 208782, id=83/dev/sdb2 : start= 208845, size= 31053645, id= 5/dev/sdb3 : start= 0, size= 0, id= 0/dev/sdb4 : start= 0, size= 0, id= 0/dev/sdb5 : start= 208908, size= <phone>, id=83/dev/sdb6 : start= <phone>, size= 401562, id=83/dev/sdb7 : start= <phone>, size= 594342, id=83/dev/sdb8 : start= <phone>, size= 208782, id=83/dev/sdb9 : start= <phone>, size= <phone>, id=83/dev/sdb10: start= <phone>, size= <phone>, id=83/dev/sdb11: start= 23503158, size= 594342, id=83/dev/sdb12: start= 24097563, size= 594342, id=83/dev/sdb13: start= 24691968, size= <phone>, id=83i was able to mount the ext3 file systems of sdb 1, 5, 7, 8, 9, 10, 11, 12, and save the contents of each to sdb1.tar, sdb5.tar, and so on.i have also done dd if=/dev/sdb of=./sdb_dd bs=512 count=1 to save the mbr of the drive to a file called sdb_dd.",
    "present_kp": [
      "partition",
      "mbr"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "find all videos by codec (and not by container formats or mime type). how can i get the list of all my videos with the used video codec?i found some intersting commands like mediainfo, ffmpeg or exiftool but they giving a lot of informations only by one video.the most intersting command i found is file and give me this output:$ file *8 mile (2002).avi: riff (little-endian) data, avi, 640 x 272, 23.98 fps, video: xvid, audio: mpeg-1 layer 3 (stereo, 48000 hz)1984 (1984).avi: riff (little-endian) data, avi, 960 x 540, ~24 fps, video: h.264 x.264 or h.264, audio: mpeg-1 layer 3 (stereo, 48000 hz)and i would like an output something like this (to convert next, for sample, only videos with xvid codec):~/movies/8 mile (2002).avi xvid~/movies/1984 (1984).avi h.264nb : i'm on macos sierra (10.12) but i can translate linux commands too",
    "present_kp": [
      "video",
      "codec"
    ],
    "absent_kp": [
      "command line",
      "video encoding"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "running a program that extracts data from websites. i have created a program that searches through every player of a particular online game, visits their information webpage and extracts pieces of information about them (ie. their stats).the problem is that there are several millions of players. by my initial calculations, it may take over 10 days to complete, and will use up over 30gbs of data traffic. this is less than ideal when you have a 40gb monthly allowance and you want to run the program weekly.my question is this. how can i run my program quickly and cheaply. for instance, is it possible to buy some webspace with a webhosting company and run my java program somehow from there? i have seen some webhosting for around $2 per month, which seems pretty reasonable.or is it a webserver i would be after. although they seem rather expensive. i am only doing this for my own interest and wouldn't want to spent more than a few dollars.thanks",
    "present_kp": [
      "webserver",
      "java"
    ],
    "absent_kp": [
      "web hosting"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how do i remove 1 item from an amazon order?. how can i remove just one item from my amazon order? i accidentally ordered 2 copies of a book. i find where i can remove the item, but it completely deletes the item. i just want to remove one of the two. how can i do this?",
    "present_kp": [
      "amazon"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "#p-completeness of the hosoya index. the description from wikipedia mentions that it is #p-complete to compute, but there are methods. what is a layman's explanation to this?",
    "present_kp": [],
    "absent_kp": [
      "time complexity",
      "counting complexity"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "missing kernel headers, but need them to install the wifi driver. i am trying to install kali on my lenovo yoga13, but after formatting the disk the setup failed to install grub because of no internet access (no ethernet, need driver to get wifi to work).so, i decided to compile the wifi driver to complete the setup just to realize i am missing kernel headers. i cannot apt-get install because i do not have net access. is there a way to manually install kernel headers to compile the driver?",
    "present_kp": [
      "kernel"
    ],
    "absent_kp": [
      "drivers",
      "compiling",
      "kali linux"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "linux server as a target for my c code. how to use a linux server as a target for my c code written on a different (windows) machine. including step by step debugging and etc... all that with eclipse env.",
    "present_kp": [
      "c",
      "debugging",
      "eclipse"
    ],
    "absent_kp": [
      "remote"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "calculating rmse and r-squared from the confusion matrix. i have my confusion matrix as c.mat8263 20 392 3826 1443 7 4431 my predicted class labels are ypred and actual labels are ytest. ypred size is 16000*1 and ytest 16000*1.i am trying to calculate the r-squared and rmse. is there a way to directly calculate rmse and r-squared from the confusion matrix?i tried this:rmse = sqrt(immse(ypred, ytest))however, it didn't work.i can use either r or matlab.any advice will be appreciated!",
    "present_kp": [
      "confusion matrix"
    ],
    "absent_kp": [
      "multiclass classification"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "two dimensional matrix-like data type using lists and/or mutable lists. i am trying to think of an implementation of a two dimensional matrix-like data type.normally i would take an array of arrays but i'm bound to a relatively low language level which only provides lists and mutable lists, because this is part of (but not an exercise itself, i could simply use an inefficient solution) a software project at university. in this, we are only allowed to use a specific language level.so of cause i could take a list of mutable lists and, in search of an item in row n and column m, get the m-th mutable list and go throught it until position n. but isn't there a better solution than going through the whole list?",
    "present_kp": [
      "array"
    ],
    "absent_kp": [
      "scheme",
      "matrices",
      "racket"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "calculate difference between numbers in 2 columns row-by-row in a spreadsheet. i have 3 columns in a google docs spreadsheet: a) inventory purchased price, b) inventory sold price, c) difference between a and b (i.e. profit or loss)data is going to be entered manually into columns a and b. how do i make column c to 1) display the calculated amount for each row, and 2) color profitable sales in green and unprofitable in red?",
    "present_kp": [],
    "absent_kp": [
      "google spreadsheets",
      "worksheet function"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "mx record question. is there a way to bypass the dns's mx record? my company uses barracuda for spam filtering and our company's mx record points to barracuda. a product we're trying to build needs to talk tls to the mail server, but to test this i have to create a subdomain that bypasses barracuda and then having to deal with the ssl certs for that. not necessarily a show stopper. but if i could fool something to use our mail server's actual ip address as opposed to the mx, that'd help a lot.any thought, cantor, ideas or anecdotes?",
    "present_kp": [
      "dns"
    ],
    "absent_kp": [
      "email"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "ubiquitous language - conflict between correctness and usability. a core part of domain driven design is the consistent use of a ubiquitous language across the system - in conversations, code, database schema, ui, tests, etc.i'm involved in a project in which there is a well-established domain language already, defined by an international standards organisation.however, the work we're doing is for a public web site, and the 'correct' terms for the domain aren't necessarily how the public typically use and understand them.the compromise we're using at the moment is to use the 'official' terms everywhere, except for in our acceptance criteria which refer to ui components, where we use the informal names.does this seem like a reasonable approach?",
    "present_kp": [],
    "absent_kp": [
      "documentation",
      "domain specific languages"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to enable control key combinations for gnu screen on putty?. i am accessing a linux box via ssh using putty.key combinations work fine when i'm not running screen.however, ctrl key combinations do not work under a screen session.in fact, a ctrl-any key is registered the same as the same key without the ctrl combination.i know this by typing ctrl-v and then a ctrl-key combination to figure out what characters are sent to my terminal. for example, ctrl- (left arrow) gives me ^[[d on screen (screen256-color term). gives me the same result.weird thing is that alt-key combinations work fine.in particular, i would like to get the 'forward-word' and 'backward-word' key bindings working under screen.i have tried modifying .inputrc to work with various terminals. as such, my .inputrc looks something like this:$if term=xterm 'xxx' : forward-word # xxx key gotten from ctrl-v 'xxx' : backward-word$endif$if term=screen-256color......i have also tried various terminals by setting the term of my bash profile, setting term on .screenrc, and setting the putty keyboard terminal mappings. the obvious ones, linux and xterm, don't work. however, i haven't tried every permutation of settings for obvious reasons.additional info:i stand corrected, arrow key combinations are the only combinations that do not work.the distro is rhel 6. .screenrc:term xterm # tried other terms as wellshell -$shell # login shell to reload configscaption string %whardstatus alwayslastline %{b kw}%h %{r}%1' %{w}| %{g}%c %{w}| %{y}%d.%m.%y %{w}| %{g}%l %{w}| %{-b kw}%u %-lw%{= rw}%50> %n%f %t %{-}%+lw%<vbell offdefscrollback 5000putty's default keyboard mode esc [n~. taken from the putty manual:in the default mode, labelled esc [n~, the function keys generate sequences like esc [11~, esc [12~ and so on. this matches the general behaviour of digital's terminals.interestingly, what's actually sent by putty (by following the first answer on <url>), is ^[0d : left-arrow^[[d : ctrl-{left-arrow}since the they are different, i guess putty isn't the issue then?fwiw, i tried changing the application cursor key mode settings on putty, but to no avail. i also tried using tmux, only to encounter the same issue.",
    "present_kp": [
      "terminal",
      "gnu screen",
      "putty"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "does your voice pitch affect your perceived authority?. i heard a claim that people with lower voice pitch are perceived as more credible than people with higher pitch.is there any research on this?",
    "present_kp": [],
    "absent_kp": [
      "social psychology",
      "perception",
      "audition",
      "trust"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "splitting a single row command to two parts so it would be more structured aesthetically. i have the following command which i run from a script. in the script file it is written in this somewhat long row:sudo zip -r /var/www/html/html-$(date +\\%f-\\%t-).zip /var/www/html -x /var/www/html/wp-content/cacheso far so good, but i want to split this command into a few pieces horizontally, like:sudo zip -r /var/www/html/html-$(date +\\%f-\\%t-).zip /var/www/html || -x /var/www/html/wp-content/cachewhere || should come non-executed characters that will use just for aeshtetic splitting of the command to two parts.or maybe even vertically like:sudo zip -r /var/www/html/html-$(date +\\%f-\\%t-).zip /var/www/html -x /var/www/html/wp-content/cachewhat will you say is the best way to achieve that?",
    "present_kp": [],
    "absent_kp": [
      "shell"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "hp deskjet 3050a scanner not working in debian. i have an hp deskjet all-in-one 3050a.in debian testing i installed it through cups and the printer works fine. however i can't say the same about the scanner. i've tried install it using hplip, but it's the same. sane or simple scan detect the scanner but at the time of use it, both say that there was a problem with i/o.the weird part is that i've tried the scanner in two virtual machines (w7 and linux mint 14) hosted in the debian testing, and works fine in both vm's.",
    "present_kp": [
      "debian",
      "scanner"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "why does my finite difference approximation not work?. i am trying to find out the magnitude of the acceleration of my object based on non-uniformly sampled 3d position data. i'm using the standard approximation of the 2nd order derivative on a non-uniform grid (see e.g. here, page 121):$$ f'' pprox 2 \\cdot rac{h^+ f_0 - (h^- + h^+) f_1 + h^- f_2}{h^+ h^- (h^+ + h^-)}$$where $h^-$ is the first sample interval, $h^+$ is the second sample interval, and $f_n$ are the sampled points.i'm trying to implement it in the following way:class computeacceleration{ vector3d prevposition; vector3d prevprevposition; float prevtimestep; float acceleration; // this function gets called every time step with the time step // since the last data input as argument. void update(const float timestep) { float doubletimestep = prevtimestep + timestep; vector3d currentposition = getcurrentposition(); vector3d accelerationvector = prevtimestep * currentposition - doubletimestep * prevposition + timestep * prevprevposition; accelerationvector *= 2 / (doubletimestep * prevtimestep * timestep); acceleration = sqrt(accelerationvector.x * accelerationvector.x + accelerationvector.y * accelerationvector.y+ accelerationvector.z * accelerationvector.z); prevprevposition = prevposition; prevposition = currentposition; }}i can't find the error in this, but when i plot this in real-time, i get a curve that doesn't resemble acceleration at all, but rather speed, in some jerky way. i plotted it against the speed (which i know to be correct), and it looks roughly like this:am i doing something fundamentally wrong here?",
    "present_kp": [
      "finite difference"
    ],
    "absent_kp": [
      "computational physics"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "understand the order of operations for bash parameter expansion. i read a similar example from bash programming book:$ cat indirection #!/usr/bin/env bashset -xnum=1eval ${!num#*:}$ when i execute the script with bash indirection test:echo blah, then how is the last line of the script processed? i guess first the indirection happens so that eval ${!num#*:} becomes eval ${1#*:}? then substring removal takes place and eval ${1#*:} becomes eval echo blah? if yes, then why is eval needed, i.e ${!num#*:} instead of eval ${!num#*:} would provide the same results?",
    "present_kp": [
      "bash"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "activeroommodel. could someone tell me how i can improve this please? it's similar to my other question just another version of it.using system;using system.collections.generic;using system.globalization;using system.linq;using system.text;using system.threading.tasks;namespace sahara.base.game.rooms{ internal class activeroommodel : idisposable { private readonly bool _clubonly; private readonly int _doorx; private readonly int _doory; private readonly double _doorz; private readonly int _doordirection; private string _modelheightmap; private int _modelsizex; private int _modelsizey; private short[,] _squarefloorheight; private byte[,] _squareseatrotation; private squarestate[,] _squarestate; private roommodel _staticmodel; private readonly string _dependantheightmap; public activeroommodel(roommodel staticmodel) { _staticmodel = staticmodel; _doorx = staticmodel.doorx; _doory = staticmodel.doory; _doorz = staticmodel.doorz; _doordirection = staticmodel.doordirection; _modelheightmap = staticmodel.heightmap; _modelsizex = staticmodel.modelsizex; _modelsizey = staticmodel.modelsizey; _clubonly = staticmodel.clubonly; _dependantheightmap = string.empty; _squarestate = new squarestate[_modelsizex, _modelsizey]; _squarefloorheight = new short[_modelsizex, _modelsizey]; _squareseatrotation = new byte[_modelsizex, _modelsizey]; for (var y = 0; y < _modelsizey; y++) { for (var x = 0; x < _modelsizex; x++) { if (x > (staticmodel.modelsizex - 1) || y > (staticmodel.modelsizey - 1)) { _squarestate[x, y] = squarestate.blocked; } else { _squarestate[x, y] = staticmodel.squarestate[x, y]; _squarefloorheight[x, y] = staticmodel.squarefloorheight[x, y]; _squareseatrotation[x, y] = staticmodel.squareseatrotation[x, y]; } } } var floormap = new stringbuilder(); for (var y = 0; y < _modelsizey; y++) { for (var x = 0; x < _modelsizex; x++) { if (x == _doorx && y == _doory) { floormap.append(_doorz > 9 ? ((char)(87 + _doorz)).tostring() : _doorz.tostring()); continue; } if (_squarestate[x, y] == squarestate.blocked) { floormap.append('x'); continue; } double height = _squarefloorheight[x, y]; floormap.append(height > 9 ? ((char)(87 + height)).tostring() : height.tostring()); } floormap.append(convert.tochar(13)); } _dependantheightmap = floormap.tostring(); } public string dependantheightmap => _dependantheightmap; public bool doorcorrect => _doorx <= _squarefloorheight.getupperbound(0) && _doory <= _squarefloorheight.getupperbound(1); public void appendxcordinate() { _modelsizex++; updatesquarearrays(); } public void appendycordinate() { _modelsizey++; updatesquarearrays(); } public void createmap(int x, int y) { _modelsizex = x; _modelsizey = y; updatesquarearrays(); } private void updatesquarearrays() { var newsqstate = new squarestate[_modelsizex + 1, _modelsizey + 1]; var newsqfloorheight = new short[_modelsizex + 1, _modelsizey + 1]; var newsqseatrot = new byte[_modelsizex + 1, _modelsizey + 1]; for (var y = 0; y < _modelsizey; y++) { for (var x = 0; x < _modelsizex; x++) { if (x > (_staticmodel.modelsizex - 1) || y > (_staticmodel.modelsizey - 1)) { newsqstate[x, y] = squarestate.blocked; } else { newsqstate[x, y] = _squarestate[x, y]; newsqfloorheight[x, y] = _squarefloorheight[x, y]; newsqseatrot[x, y] = _squareseatrotation[x, y]; } } } _squarestate = newsqstate; _squarefloorheight = newsqfloorheight; _squareseatrotation = newsqseatrot; } public void dispose() { array.clear(_squarestate, 0, _squarestate.length); _squarestate = null; array.clear(_squarefloorheight, 0, _squarefloorheight.length); _squarefloorheight = null; array.clear(_squareseatrotation, 0, _squareseatrotation.length); _squareseatrotation = null; _staticmodel = null; _modelheightmap = null; } }}roommodel:using system;namespace sahara.base.game.rooms{ internal sealed class roommodel { private readonly bool _clubonly; private readonly int _doorx; private readonly int _doory; private readonly double _doorz; private readonly int _doordirection; private readonly string _modelheightmap; private readonly int _modelsizex; private readonly int _modelsizey; private readonly int _wallheight; private readonly short[,] _squarefloorheight; private readonly byte[,] _squareseatrotation; private readonly squarestate[,] _squarestate; private readonly string _modelfurnimap; private readonly bool _publicpool; private readonly byte[,] _roommodeleffects; public roommodel(bool clubonly, int doorpositionx, int doorpositiony, int wallheight, double doorpositionz, int doordirection, string modelheightmap, string modelfurnimap, string poolmap) { _doorx = doorpositionx; _doory = doorpositiony; _doorz = doorpositionz; _doordirection = doordirection; _wallheight = wallheight; _modelheightmap = modelheightmap.tolower(); _modelfurnimap = modelfurnimap; if (poolmap != string.empty) { _publicpool = true; _roommodeleffects = new byte[_modelsizex, _modelsizey]; } var temporaryheightmap = _modelheightmap.split(convert.tochar(13)); _modelsizex = temporaryheightmap[0].length; _modelsizey = temporaryheightmap.length; _clubonly = clubonly; _squarestate = new squarestate[_modelsizex, _modelsizey]; _squarefloorheight = new short[_modelsizex, _modelsizey]; _squareseatrotation = new byte[_modelsizex, _modelsizey]; for (var y = _modelsizey - 1; y >= 0; y--) { var line = temporaryheightmap[y].replace(environment.newline, string.empty); var x = 0; foreach (var modelsquare in line) { if (modelsquare == 'x') { _squarestate[x, y] = rooms.squarestate.blocked; } else { _squarestate[x, y] = rooms.squarestate.open; _squarefloorheight[x, y] = sahara.getserver().getutility().parsemodelsquare(modelsquare); } x++; } } } public int doorx => _doorx; public int doory => _doory; public double doorz => _doorz; public int doordirection => _doordirection; public string heightmap => _modelheightmap; public int modelsizex => _modelsizex; public int modelsizey => _modelsizey; public bool clubonly => _clubonly; public squarestate[,] squarestate => _squarestate; public short[,] squarefloorheight => _squarefloorheight; public byte[,] squareseatrotation => _squareseatrotation; public bool publicpool => _publicpool; }}squarestate:using system;using system.collections.generic;using system.linq;using system.text;using system.threading.tasks;namespace sahara.base.game.rooms{ public enum squarestate { open = 0, blocked = 1, seat = 2, pool = 3, vip = 4 }}",
    "present_kp": [],
    "absent_kp": [
      "c#"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "using cognito form creator to create order forms with multiple available options. using the cognito form creator there does not seem to be a very straightford way of creating an order form with multiple available items for possible selection; where each available items has different possible options such as sizes and colors. here is an example of my order form that have created using jotform: <url> . i would like to create this same form using cognito including the images. does anyone know how to do it?",
    "present_kp": [],
    "absent_kp": [
      "cognito forms"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "logrotate fails to rotate logs: error setting owner. recently i've noticed that logrotate does not rotate my logs.user1@host:~$ /usr/sbin/logrotate /home/user1/logrotate.conf -v gives me an error:error: error setting owner of /home/logs/mylog.log.1 to uid 10111 and gid 10111: operation not permittederror: error creating output file /var/lib/logrotate/status.tmp: permission denied that gid confuses me, as user1 is only a member of a group with different gid:user1@host:~$ iduid=10111(user1) gid=1001(mygroup) groups=1001(mygroup) however, there's another group called user1, but, as i mentioned, actual user user1 is not its member:user1@host:~$ cat /etc/group | grep user1user1\u274c10111it's something simple here, but i can't see it.update:here's what logrotate.conf looks like:/home/logs/*.log { rotate 7 daily copytruncate compress notifempty }logrotate 3.8.7update 2:user1@host:~$ ls -la /home/logs/-rw-r--r-- 1 user1 mygroup <phone> dec 19 00:58 mylog.log",
    "present_kp": [
      "group",
      "logrotate"
    ],
    "absent_kp": [
      "ubuntu",
      "permissions",
      "users"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "singleton wrapper. feel free to critique this database wrapper which is written as a code example for employers or clients.<?php class database { static function getinstance() { if (self::$instance == null) { self::$instance = new database(); } return self::$instance; } public function connect() { /* if dbconnection already exists, do not make another one. */ if (is_resource($this->dbconnection)) { return true; } $conn = $this->createdbconnection(); if (!$conn){ trigger_error(error connecting to database, e_user_notice); } $this->dbconnection = $conn; $result = mysql_select_db($this->database_name, $conn); if (!$result){ trigger_error(error selecting database, e_user_notice); } }//endfunction public function closeconnection() { /* mysql documentation says closing a connection isn't nessesary since the connection is closed at the end of the script. */ mysql_close($this->dbconnection); } public function executequery($query) { $this->query_result = mysql_query($query); if (!$this->query_result){ trigger_error(error executing database query, e_user_notice); } } public function getrow(){ return mysql_fetch_array($this->query_result); } private function createdbconnection() { if ($_server[server_name] == website.com) { $this->database_name = 'databasename'; return mysql_connect('localhost', 'username', 'password'); //$msdb = mysql_connect(localhost, root, ); //mysql_select_db(test, $msdb) or die(mysql_error()); } } private $dbconnection; private $query_result; private $database_name; static private $instance = null; }?>",
    "present_kp": [
      "php",
      "singleton"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "understanding ext4/jbd2 transactions. i'm trying to debug a bit of trouble i'm having with i/o latency on a system using ext4, but i'm a bit stumped in that i don't understand its journalling as well as i'd like to.what i'm wondering in particular is at what point disk blocks are actually written, both into the journal and for real. my assumption thus far is that, when some metadata operation like, say, a rename(), is done by a user process, the filesystem will ensure that the disk blocks that the operation needs to touch are in the buffer cache, carry out the necessary changes in memory on the cached pages, while creating a jbd2 transaction that records the changed pages, and then, at some later point, write those pages into the journal, and then, only when that is done and at some further arbitrarily later point, write them into the filesystem proper. please do correct me if i'm wrong somewhere in here.assuming i'm somewhat correct, i'm wondering what might trigger the write to the journal to be carried out. is it when some more general fs/vm part of the kernel decides that it's time to flush the dirty pages? is it when there is no more space in the journal to allocate for more transactions? what happens if the kernel needs to reclaim some of the dirty pages from the buffer cache? would that trigger the journal-write and main-write in forced succession?also, what is the standard terminology? when i see a transaction being referred to being committed, what does that mean, more precisely?",
    "present_kp": [
      "ext4"
    ],
    "absent_kp": [
      "linux"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "hidden password is being displayed when invoking the su command?. this is the first time it has happened to me where i am using the su command and it actually displays the password on the terminal and doesn't stay hidden. here is my code snippet:sshpass -p password ssh -q <email> su -lc 'mkdir temp/'code explanation: i am accessing a remote server and trying be root on that server to create a folder. in doing so i have to use the su command and it prompts me for the password. when i enter the password, it gets displayed and doesn't stay hidden.",
    "present_kp": [],
    "absent_kp": [
      "bash"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "make rsync print unicode filenames correctly. i'm using rsync on mac, during file syncing, unicode filenames are not correctly displayed, e.gany ideas?",
    "present_kp": [
      "rsync"
    ],
    "absent_kp": [
      "macintosh"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why has my google analytics goal stopped tracking visits?. i created a goal on google analytics which was working just fine all this time. it looked like this:my urls are dynamic so i chose the begins with. it was working fine until recently. i wanted to see full page urls in my reports. so i created a filter to show the complete page url following this link - https:// support.google.com/analytics/answer/<phone>?hl=ensince then, the goal won't track any visits. in the goal funnel page, should i use the complete page url now like http:// sub.example.com/itinerarybooking instead of /itinerarybooking?edithere's how the new goal details screen looks with the hostname included as part of the url.edit after user nyuen's recommendation of using regex in goal detailsplease find the screenshot below. i have used regex while creating these steps but 3 of the 5 page links will be dynamic in nature. 2 links will be an exact match.actual url -> <url> step -> .*veho.pickyourtrail.com/itinerary/detailsactual url -> <url> step -> .*veho.pickyourtrail.com/itinerarycostactual url -> <url> step -> .*veho.pickyourtrail.com/itinerarybookinggoal step ->",
    "present_kp": [
      "google analytics"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "is there something wrong with my script or is bash much slower than python?. i was testing the speed of bash and python by running a loop 1 billion times.$ cat python.py#!/bin/python# python v3.5i=0;while i<=<phone>: i=i+1;bash code:$ cat bash2.sh#!/bin/bash# bash v4.3i=0while [[ $i -le <phone> ]]dolet i++doneusing the time command i found out that the python code takes just 48 seconds to finish while the bash code took over 1 hour before i killed the script.why is this so? i expected that bash would be faster. is there something wrong with my script or is bash really much slower with this script?",
    "present_kp": [
      "bash"
    ],
    "absent_kp": [
      "python3"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "vms are not created on the same physical server to balance load?. when load reaches a certain level in iaas environemnt, new vm instances are created to reduce load (scalability).my questions that these vms are not created on the same physical server, right?i mean one vm with like 10 users will give the same performance as 2 vms on the same physical server with each vm has 5 users.if vms are created on the same physical server, then how can this balance load?for background, i'm just new to cloud computing programming and i'm trying to understand some of the concepts involved.",
    "present_kp": [
      "scalability",
      "cloud computing",
      "concepts",
      "cloud"
    ],
    "absent_kp": [
      "load balancing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "trying to open grub configuration file during boot process. i am trying to access grub configuration file at boot process (boot load). but unable to do it. i am able to go to grub terminal but when i give below command, it is throwing an error file not foundcommand: configfile (hd0,0)/boot/grub/grub.cfgis there any any command to open grub configuration file during boot?",
    "present_kp": [],
    "absent_kp": [
      "grub2"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "copyright and contributing to an open source project. i'm a little confused by copyright notices on open source projects.let's say that a particular project is covered by a very permissive license, such as mit or bsd. a copyright notice appears from the company that originated the source code.but let's say that the project leader leaves the company, but continues to contribute to the project. should s/he add a new copyright notice, to indicate that portions are copyright from the original company and portions from the author no longer employed by that company?it seems to me that anyone who touches a file essentially invalidates the copyright claims of anyone prior, because the file becomes a new creative expression based on those modifications. how is the copyright supposed to be managed in a legally (and morally) robust manner as more and more people contribute?",
    "present_kp": [
      "copyright"
    ],
    "absent_kp": [
      "licensing",
      "contributor",
      "collaboration"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "the equivalent of a live-cd for a smartphone. is there any way of running an os in a smartphone without installing it on the device? that would be the equivalent of a live-cd for a pc. i intend to test several bsd distributions on a phone, that could be an android or windows.",
    "present_kp": [],
    "absent_kp": [
      "livecd"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "remove headers and contents from a flat file if they are below a specific line count. i've a flat file containing about 10 million lines: query id1 content1 content2 query id2 content3 content4 ... content21 query id3 content22 content23 ... content81any block in the file less than 10 lines should be removed. for example, the first block contains 4 lines (query to content2) and it should be removed. this step need to be done before splitting the blocks into individual files. any suggestion?",
    "present_kp": [],
    "absent_kp": [
      "sed",
      "awk"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "can one use any search engine in order to find a set of keywords on a site (not page)?. the problem i'm trying to solve is that i'd like to buy a set of products online (fyi: topeak f25, f66, f55, iphone drybag, and propack small).for convenience, and to save on shipping costs, i'd like to buy all products from one retailer.i can find each and every aforementioned product at different retailers in the eu, but not all of them at a single one.",
    "present_kp": [
      "search engine"
    ],
    "absent_kp": [
      "shopping"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "you are promoted, you are promoted, we all are promoted!. my latest refactoring for rubberduck is called introduce field - it promotes a local variable to a private field.the three overridden refactor() methods are the members of irefactoring, and are used to start the refactoring sequence. the other methods are the worker methods.overall, i am pretty happy with it, but there are a few things that bother me. the first of these is that refactor(declaration) has a strict requirement for the declaration type, but accepts any declaration, regardless of the type. is there a way to enforce this other than throwing? should i even be throwing here? should i just return instead?removevariable() and removeextracomma() both seem somewhat clunky. is there a cleaner way to do this?public class introducefield : irefactoring{ private readonly ilist<declaration> _declarations; private readonly iactivecodepaneeditor _editor; private declaration _targetdeclaration; private readonly imessagebox _messagebox; public introducefield(rubberduckparserstate parseresult, iactivecodepaneeditor editor, imessagebox messagebox) { _declarations = parseresult.alldeclarations.tolist(); _editor = editor; _messagebox = messagebox; } public void refactor() { if (_targetdeclaration == null) { _messagebox.show(rubberduckui.promotevariable_invalidselection); return; } removevariable(); addfield(); } public void refactor(qualifiedselection selection) { _targetdeclaration = findselection(selection); refactor(); } public void refactor(declaration target) { if (target.declarationtype != declarationtype.variable) { throw new argumentexception(invalid declaration type); } _targetdeclaration = target; refactor(); } private void addfield() { var module = _targetdeclaration.qualifiedname.qualifiedmodulename.component.codemodule; module.insertlines(module.countofdeclarationlines + 1, getfielddefinition()); } private void removevariable() { selection selection; var declarationtext = _targetdeclaration.context.gettext(); var multipledeclarations = hasmultipledeclarationsinstatement(_targetdeclaration); var variablestmtcontext = getvariablestmtcontext(_targetdeclaration); if (!multipledeclarations) { declarationtext = variablestmtcontext.gettext(); selection = getvariablestmtcontextselection(_targetdeclaration); } else { selection = new selection(_targetdeclaration.context.start.line, _targetdeclaration.context.start.column, _targetdeclaration.context.stop.line, _targetdeclaration.context.stop.column); } var oldlines = _editor.getlines(selection); var newlines = oldlines.replace( _ + environment.newline, string.empty) .remove(selection.startcolumn, declarationtext.length); if (multipledeclarations) { selection = getvariablestmtcontextselection(_targetdeclaration); newlines = removeextracomma(_editor.getlines(selection).replace(oldlines, newlines)); } _editor.deletelines(selection); _editor.insertlines(selection.startline, newlines); } private selection getvariablestmtcontextselection(declaration target) { var statement = getvariablestmtcontext(target); return new selection(statement.start.line, statement.start.column, statement.stop.line, statement.stop.column); } private vbaparser.variablestmtcontext getvariablestmtcontext(declaration target) { var statement = target.context.parent.parent as vbaparser.variablestmtcontext; if (statement == null) { throw new missingmemberexception(statement not found); } return statement; } private string removeextracomma(string str) { if (str.count(c => c == ',') == 1) { return str.remove(str.indexof(','), 1); } var significantcharacteraftercomma = false; for (var index = 0; index < str.length; index++) { if (!char.iswhitespace(str[index]) && str[index] != '_' && str[index] != ',') { significantcharacteraftercomma = true; } if (str[index] == ',') { significantcharacteraftercomma = false; } if (!significantcharacteraftercomma && str[index] == ',') { return str.remove(index, 1); } } return str; } private bool hasmultipledeclarationsinstatement(declaration target) { var statement = target.context.parent as vbaparser.variableliststmtcontext; if (statement == null) { return false; } return statement.children.count(i => i is vbaparser.variablesubstmtcontext) > 1; } private string getfielddefinition() { if (_targetdeclaration == null) { return null; } return private + _targetdeclaration.identifiername + as + _targetdeclaration.astypename; } private declaration findselection(qualifiedselection selection) { var target = _declarations .where(item => !item.isbuiltin) .firstordefault(item => item.isselected(selection) && item.declarationtype == declarationtype.variable || item.references.any(r => r.isselected(selection) && r.declaration.declarationtype == declarationtype.variable)); if (target != null) { return target; } var targets = _declarations .where(item => !item.isbuiltin && item.componentname == selection.qualifiedname.componentname && item.declarationtype == declarationtype.variable); foreach (var declaration in targets) { var declarationselection = new selection(declaration.context.start.line, declaration.context.start.column, declaration.context.stop.line, declaration.context.stop.column + declaration.context.stop.text.length); if (declarationselection.contains(selection.selection) || !hasmultipledeclarationsinstatement(declaration) && getvariablestmtcontextselection(declaration).contains(selection.selection)) { return declaration; } var reference = declaration.references.firstordefault(r => r.selection.contains(selection.selection)); if (reference != null) { return reference.declaration; } } return null; }}",
    "present_kp": [
      "rubberduck"
    ],
    "absent_kp": [
      "c#",
      "meta programming"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "repositories, gateways, models and architecture questions. i am working with a laravel project and i am looking for a way to solve the issue of bloated models and cross referencing between them.i had started extracting higher level methods to a repository but this doesn't solve the issue of one method needing to know about another method.for example a task lookup method needs to lookup a slug in another table first. i don't believe i should be placing this code into either model or repository but i'd like a single method to achieve this lookup./models/slug/models/task/repositories/slugrepository/repositories/taskrepositoryi have now started experimenting with adding a gateway/service layer with a higher level methods which can access both of the underlying repositories to complete the task.the task service would depend on the two repositories above./service/taskfindbyslug()i think this will work but i am not sure if i should now let the controllers still access the repository directly or force everything through the service/gateway layer.or perhaps do away with the repositories entirely and let the services access the models directly, (laravel abstracts db access anyway).and on top of it all i want to keep this as simple as possible!can anyone confirm this method as a good choice or not or perhaps suggest an alternative?",
    "present_kp": [
      "architecture",
      "repository",
      "services",
      "laravel"
    ],
    "absent_kp": [
      "design patterns"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "help with this fsa please :(. i have to convert this fsa to regular expression but these are too different than i learned... can you guys help with converting these? plz!",
    "present_kp": [],
    "absent_kp": [
      "finite automata",
      "regular expressions"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "looking for papers and articles on higher-order sequent systems. i am looking for work on systems that are similar to k. dosen's higher-order sequents (sequent systems for modal logic, jsl 50). the only work that i am aware of is recent work by iemhoff and metcalfe (proof theory for admissible rules, annals of pure and applied logic 159 (1-2), 2009).are there other papers on such systems?",
    "present_kp": [
      "proof theory"
    ],
    "absent_kp": [
      "reference request",
      "lo.logic"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is there a method to add multiple properties to htmltextwriterstyle?. i am creating new elements for a webpage at run-time and i have code like this: var dyndiv = new system.web.ui.htmlcontrols.htmlgenericcontrol(div) {id = dyndiv}; dyndiv.style.add(htmltextwriterstyle.backgroundcolor, red); dyndiv.style.add(htmltextwriterstyle.position, absolute; left: 500px; top: 500px); dyndiv.style.add(htmltextwriterstyle.height, 30px); dyndiv.style.add(htmltextwriterstyle.width, 300px); dyndiv.innerhtml = new object; placeholder1.controls.add(dyndiv);is there a shorthand method to adding these value? i.e. something like:dyndiv.style.add(htmltextwriterstyle {backgroundcolor = red, position = absolute; left: 500px; top: 500px, height = 30px, width=300px});or any other easier way that anyone can suggest?",
    "present_kp": [
      "html"
    ],
    "absent_kp": [
      "c#",
      "asp.net"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "are your projects completed on time or past the deadline?. are your projects completed on time? if not, what problems cause you to miss deadlines? how can they be overcome? do your clients understand them?",
    "present_kp": [],
    "absent_kp": [
      "productivity",
      "project management",
      "time management"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "lxc unable to apply profile to container. i'm using lxc on ubuntu ubuntu 14.04.4 lts, and i'm performing the following operations as unprivileged user. $lxc config profile create privileged$lxc config profile set privileged security.privileged truei'm attempting to apply this profile to my container $ lxc profile apply my-lxc-ct privileged error: not foundany idea on why i'm unable to apply this profile?",
    "present_kp": [
      "lxc"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "circuit for finding the index of first zero entry in a binary string. finding the index of first zero entry in a binary string:input: binary string ($0$'s and $1$'s)output: index of first zero entrycan you give a circuit for finding the index of first zero entry in a binary string with minimum depth and polynomial many processors? 1) is this problem in $ac_0$?2) is this problem in $nc_1$?can point me at references?",
    "present_kp": [],
    "absent_kp": [
      "circuits",
      "digital circuits",
      "sequential circuit"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "async queue processing. i have a queue that needs to try to process when anything updates in it (add, remove or update).however, i don't want any of the callers to wait while the processing is happening (either for the processing to happen or while the processing is happening).this is what i came up with:private static readonly semaphoreslim asynclock = new semaphoreslim(1);private async void processqueue(){ // lock this up so that one thread at a time can get through here. // others will do an async await until it is their turn. await asynclock.waitasync(); try { // offload this to a background thread (so that the ui is not affected) var queueprocessingtask = task.run( () => { var processingstuck = false; while (myqueue.count >= 1 && !processingstuck) { // get the next item var queueitem = myqueue.peek(); // try to process this one. (ie dostuff) processingstuck = processqueueitem(queueitem); // if we processed successfully, then we can dequeue the item if (!processingstuck) myqueue.dequeue(); } }); task.waitall(queueprocessingtask); } finally { asynclock.release(); } }is my thread and async handling going to ensure that only 1 queueitem at a time can ever be in the works?and will this avoid using resources from the ui thread?",
    "present_kp": [
      "async await"
    ],
    "absent_kp": [
      "c#",
      "task parallel library"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "can we always reduce the weights of a weighted graph to rationals and preserve equality relationships?. let $g = (q, \\delta, w)$ be a finite weighted graph with $\\delta: q imes q$ and $w: q imes q o \\mathbb{r}^{+}$. is it the case that there always exist a function $w': q imes q o \\mathbb{q}^{+}$ such that $ orall s, s' \\subseteq q, orall s, s' \\in s$:$$\\sum_{s'' \\in s'}{w(s, s'')} = \\sum_{s'' \\in s'}{w(s', s'')} \\leftrightarrow \\sum_{s'' \\in s'}{w'(s, s'')} = \\sum_{s'' \\in s'}{w'(s', s'')}$$?i believe, at least for the finite case, this should be possible and i'm trying to prove this constructively by finding $w'$.so let $\\epsilon \\in \\mathbb{q}^{+}$ be a rational such that $ orall s_0,s_1,s_2,s_3 \\in q, \\epsilon < |w(s_0, s_1) - w(s_2, s_3)|$ whenever that difference isn't $0$.obviously if we just round every non-rational weight in a grid with spacing $\\epsilon$ we will not preserve equalities and i believe it'll be quite difficult to restore the exact relations.so my idea is to round one weight at a time, and then fix the equalities that get broken by modifying only non-rational weights and repeating the process until there are only rational weights.so start by taking two subsets $s, s' \\subseteq q$ and take $s \\in s$ and $s'' \\in s'$ such that $w(s, s'') otin \\mathbb{q}$.now we can round this number to the nearest rational of the form $k\\cdot \\epsilon$.let $e = w(s, s'') - w'(s, s'')$ is the error we introduced.now if there was an $s' \\in s$ and an $s'''$ such that $w(s', s''') = w(s, s'')$ then we modify that weight in the same manner, and thus the iff condition still holds for $s$ and $s'$.otherwise for $s' \\in s$ we must have that some (more than one) weight $w(s', s''') otin \\mathbb{q}$ and we can subtract $e$ from it and preserve the condition for $s$ and $s'$ with respect to $s$ and $s'$.however we have now changed some other weight, and thus while we may have preserved the equality between $s$ and $s'$, we may have broken it for other subsets. so we have to consider every subset that is either a subset or superset of $s$ and $s'$, verify if we have broken that iff condition, and if so we must find an other weight and change it, and repeat the process until it we find an equilibrium. but does this actually happen?my question is:does there exist such $w'$ or am i trying to build a non-existent function?if $w'$ exists, is the above approach feasible in some way, or should i completely change strategy in order to find $w'$? do you know any particular trick that can help in devising $w'$?",
    "present_kp": [],
    "absent_kp": [
      "graph theory",
      "simulation",
      "weighted graphs"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what does it mean for the json interchange format to have a license?. a recent flurry of comments on a jshint issue alerted me to the fact that the json data-interchange format has a license.the content of this license can be debated elsewhere.what i'm unclear on is what it means for json (or any other data-interchance format) to have a license.if an a library simply consumes and parses json, does it have to include this license? i understand that using a library that contains this license would require a user to include it. i'm talking about writing a library from scratch.what if i have to use json because a third party service only communicates via json. if i'm serializing and deserializing data according to the rfc, do i still need to refer to the license?",
    "present_kp": [
      "json"
    ],
    "absent_kp": [
      "licensing",
      "legal"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "mimicking a bluetooth disconnection. i've written a program to control a bluetooth device. i'm trying to test cases when the bluetooth disconnects, i.e. if its out of range.physically taking the device out of range is one possibility, but its quite cumbersome and i have to go outside my office to achieve this.what can i do to trigger a disconnection? is there, for example, an interferer i can setup, say with an android phone, that would make the connection drop? or limit the bluetooth transmit power? any other possibilities?",
    "present_kp": [],
    "absent_kp": [
      "testing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "which child process will inherit threads of parent process?. what happens when one process has child threads and a child process, will child process inherit all child threads of parent process?assume os is linux. let it be java threading model.",
    "present_kp": [],
    "absent_kp": [
      "multithreading",
      "operating systems"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "put command result in variable within makefile target. i am trying to install something in tomcat webapp. this is beginning of my install target:tomcat='locate --regex ^(/var/lib/tomcat[0-9]{1,2}/webapps/[^/]+/)appname\\.html$ -l 1 | tr -d 'echo tomcat: $tomcat# if the string is empty (no path matched) or the path does not exists (that should never really happen)# terminateif [ -z $tomcat ] || [ ! -f $tomcat ]; then echo application not found on filesystem. exit 404fihowever this is the output:tomcat='locate --regex ^(/var/lib/tomcat[0-9]{1,2}/webapps/[^/]+/)appname\\.html -l 1 | tr -d '/bin/sh: 1: syntax error: unterminated quoted stringmakefile:77: recipe for target 'install' failedmake: *** [install] error 2everyone else claims that you can use ' (backtick) to assign command stdout output into variable. i even used tr -d to delete all newline characters, may they appear. and the code works flawlessly in shell:xxxxx@debianvirtualbox:~$ tomcat='locate --regex ^(/var/lib/tomcat[0-9]{1,2}/webapps/[^/]+/)appname\\.html$ -l 1 | tr -d 'xxxxx@debianvirtualbox:~$ echo $tomcat/var/lib/tomcat8/webapps/websight/appname.htmlwhat else is there to fix?",
    "present_kp": [
      "make"
    ],
    "absent_kp": [
      "bash"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "any online r console?. i am looking for an online console for the language r. like i write the code and the server should execute and provide me with the output.similar to the website datacamp.",
    "present_kp": [
      "r"
    ],
    "absent_kp": [
      "statistics"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "nfc-list not showing device. i have plugged in my nexus 4 via usb and i'm running nfc-list on kali linux 2.0.but when running the command i only receive 'no device found'. is the nexus 4 not compatible with nfc-list??",
    "present_kp": [
      "kali linux"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "i accidentally removed my ufw allow rules (ssh and ftp rules) and then logged out of my server, am now locked out. my environment: aws ec2 (vmware workstation 11) for use as development sandbox environment. it's not crucial, i just have some work i don't want to re do for a client. is there anyway to change those rules from the outside?",
    "present_kp": [],
    "absent_kp": [
      "amazon web services",
      "amazon ec2"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "any idea what nt referrer means?. i'm getting a lot of referrer headers that are just nt. there's no other information (it's just that string, not a url). has anyone come across this referrer string before? does it correspond to a known user agent or spider?",
    "present_kp": [
      "referrer"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "getting insights data from linkedin and twitter. is there any way to get insights data from twitter and linkedin company/brand pages as facebook provides?data like:negative feedback;comment/wall posts;likes, shares, follows, re-tweets etcand insights like:monthly active users;page views;user demographics etc",
    "present_kp": [
      "linkedin",
      "data"
    ],
    "absent_kp": [
      "twitter api"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why doesn't greedy approach work for following problem but the solution provided in the editorial does?. problem statement:alexa has two stacks of nonnegative integers, stack a = [a0,a1, . . . ,an_1] and stackb = [b0, b1, . . . ,bm_1] where index 0 denotes the top of the stack. alexa challenges nick to play thefollowing game:in each move, nick can remove one integer from the top of either stack a or stack b. nick keeps a running sum of the integers he removes from the two stacks. nick is disqualified from the game if, at any point, his running sum becomes greater than some integer given at the beginning of the game.nick's fine/score is the total number of integers he has removed from the two stacks.given a, b, and m for g games, find the maximum possible score nick can achieve (i.e., the maximumnumber of integers he can remove without being disqualified)link to the problem: <url> solution(greedy approach):pick(pop) the smallest element between the one present on top of the stack a and the other present on top of the stack b. increment sum by the value of popped element and total(number of ints) by 1.repeat 1 until either sum becomes greater than asked or one of the two stacks get empty.if one of the two stacks get empty then pop ints from the other stack until the current sum exceeds the asked limit.editorial solution:this problem can be solved in linear time. we'll begin by taking as many integers as possible from stack a without exceeding the sum. once we've done this we'll start taking integers from b, but whenever the sum becomes larger than the limit, we'll put integers back into stack a. make sure to update the answer (the number of integers) as the traversal through stack b takes place. break the loop when you have put back all integers that was taken from a and it's not possible to take any more integers from b.please explain where am i getting wrong and what is the intuition behind the editorial solution?",
    "present_kp": [
      "stack"
    ],
    "absent_kp": [
      "algorithms",
      "data structures"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "can iptables be used to convert a single-homed host into a nat server?. given: i have a machine (hosta) with only one nic which has internet connectivity. i have another machine (hostb) with one nic on the same switch. hostb is not configured for internet access yet. hosta has its default gateway and dns servers appropriately configured. ipv4 is being used. oses on the hosts are ubuntu 13 and fedora17.what i want: now, i would like hostb to have internet connectivity, too. is this possible using 'some' combination of iptables, virtual tun/tap devices, or a vpn setup between hosta and hostb, etc? what i already know and can do: currently, i can use an ssh-based socks proxy on hostb (ssh -d 9050 usera@hosta) and route traffic of select 'socksifiable' applications on hostb via this proxy to hosta and beyond. however, sadly, not all applications are socksifiable. now, i know very well that if hosta had 2 nics, i could have used some iptables rules to convert hosta into a gateway that would then route traffic between its nic-1 and nic-2 (where nic-1 would be connected to hostb and nic-2 to internet). but installing another nic in hosta is not feasible for me.ps: i had posted this earlier on superuser.com but got no useful information.edit 1:network informationhost a::> ip addr[...]2: p4p1: <broadcast,multicast,up,lower_up> mtu 1500 qdisc pfifo_fast state up qlen 1000link/ether d4:be:d9:d5:46:05 brd ff:ff:ff:ff:ff:ffinet 192.168.22.9/24 brd 192.168.22.255 scope global p4p1 :> ip routedefault via 192.168.22.254 dev p4p1 proto static192.168.22.0/24 dev p4p1 proto kernel scope link src 192.168.22.9host b::> ip addr[...]2: eth0: <broadcast,multicast,up,lower_up> mtu 1500 qdisc pfifo_fast state up qlen 1000link/ether 30:f9:ed:d9:2e:20 brd ff:ff:ff:ff:ff:ffinet 192.168.22.234/24 brd 192.168.22.255 scope global eth0 :> ip route169.254.0.0/16 dev eth0 scope link metric <phone>.168.22.0/24 dev eth0 proto kernel scope link src 192.168.22.234 metric 1",
    "present_kp": [
      "iptables",
      "nat"
    ],
    "absent_kp": [
      "linux",
      "networking"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "using printf function for repeated patterns. i have codes which append the patterns into text file w.r.t. the user's input as follows;echo -n which stations?read stationawk -v input=$station ' begin { n = split(tolower(input), user) pattern= force %-4s npos 0. .0001 } {print} /<< write after this line >>/ { for (i=1; i<=n; i++) printf pattern, user[i] exit }' ./data > data_2let assume user's input abcd ab12 then commands append below lines;force abcd npos 0. .0001force ab12 npos 0. .0001i need to add epos and upos strings for each input for separate lines as follows (for same inputs as above example);force abcd npos 0. .0001force abcd epos 0. .0001force abcd upos 0. .0001force ab12 npos 0. .0001force ab12 epos 0. .0001force ab12 upos 0. .0001how can i modify the pattern option to append these lines into the data file?",
    "present_kp": [
      "awk",
      "printf"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "decorator pattern in php simple implementation. i am learning decorator design pattern in php. please see what i am doing is correct or wrong, would appreciate your review, feedback, comments, suggestions.<?php/** * notifierservice interface is used as abstraction for notification mechanism * * it can be later implemented as emailnotifierservice, pushnotifierservice, or * any other notification service */interface notifierservice { public function notify();}/** * emailnotifierservice - is email decorator * here the emailsending is implemented, and it can be used wherever it is * required, different unique services that doesn not have common behaviours * are utilised using decorator patterns */class emailnotifierservice implements notifierservice { public function __construct(){ $this->to = '<email>' ;//$toarr; $this->from = '<email>' ;//$from; $this->msg = 'this is a test message';//$msg } /** * [sendmail actual implementation of sending email is put here] * kept simple for demo purposes */ private function sendmail(){ echo sending email to .php_eol; print_r($this->to); echo from: .php_eol; print_r($this->from); echo msg: .php_eol; print_r($this->msg); } /** * [notify - implementation of the interface method - notify] */ public function notify(){ echo notify function called .php_eol; $this->sendmail(); }}class user implements notifierservice{ var $users = []; var $notifierservice; //passing notifierservice object, so that we can utilise its service public function __construct(notifierservice $notifierservice){ echo new user constructed; $this->notifierservice = $notifierservice; } /** * [adduser this function creates a new user, like new user signup, * whenever a new user signup, a email is sent to the website admin] * @param [username] $u username */ public function adduser($u){ $this->users[]=$u; $this->notify();// calls the notification } public function notify(){ $this->notifierservice->notify(); }}// new user is created, emailnotificationservice is wrapped in it// so that whenever a new user is created, admin gets an email$u1 = new user(new emailnotifierservice());$u1->adduser('ramkumar');",
    "present_kp": [
      "php"
    ],
    "absent_kp": [
      "design patterns"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "uploading .dll files to a local computer. i have created a simple application that has grown a little bit. though most likely it won't grow anymore, i have looked back and i realized that it has become a little monster in terms of how many almost identical properties and methods i have created. please let me know how this could be made more manageable etc.purpose of the application**upload dll files to a remote computer. there are three types of files - workflow plugin, archive plugin, processing plugin. after some time, i have extended the app so that for each file type it can also have a config xml file.structureuser controlsto keep things simpler and more reusable, i have created a user control for uploader section, as this will repeat three times.it has:two path boxes (for paths of .dll and xml files) - these are smaller usercontrols with browse/open selected file/display previous files' functionalitiestwo upload buttons - one for .dll and one for xmltwo sets of two labels - to display modified date of local and server file for .dll and xml files - labels can change colorsone 'open folder' button - to open a server folder where a .dll file isa fold/unfold button so that it could be collapseda checkbox for autouploadas a result, this user control has got 17 dependency properties and 4 events:four dependency properties for paths (current path and list of previous paths for two types of files)four dependency properties for labelsfour dependency properties for label brushesa couple of miscellaneous dps for 'auto-uplad' checkbox, visibility, groupbox header etcfour routed events - for all buttons clicksfull xaml on dotnetfiddle, click on the view tabmainwindow.xamlnow, in my mainwindow.xaml file, apart from crap like status bar and menu bar, i need to define three uploadercontrols. and this is where it starts being pretty monstrous (especially if i were to have e.g. 10 types of files to upload)for each of the controls i need a separate set of properties to be bound.four properties for paths, connected with application settingsfour properties for xml file labels and brushes four properties for .dll file labels and brushes apart from that there's a bunch of miscellaneous props for visbility, auto-uplad and event handlers. i am not really concerned about those. <controls:uploadercontrol controlheader=workflow plugin contentvisibility={binding elementname=thisuc, path=workflowuploadervisibility} hidebuttonclick=onhidebuttonclick currentprovidedpath={binding elementname=thisuc, path=currentworkflowpluginpath, updatesourcetrigger=propertychanged, mode=twoway} previouspathscollection={binding elementname=thisuc, path=previousworkflowpluginpathscollection} currentprovidedconfigpath={binding elementname=thisuc, path=currentworkflowpluginconfigpath, updatesourcetrigger=propertychanged, mode=twoway} previousconfigpathscollection={binding elementname=thisuc, path=previousworkflowpluginconfigpathscollection} dockpanel.dock=top openserverfolder=uploadercontrol_onopenserverfolder autouploadafterbuild={binding elementname=thisuc,path=workflowautoupload,mode=twoway} locallastmodified={binding elementname=thisuc, path=localworkflowpluginlastmodified} locallastmodifiedbrush={binding elementname=thisuc, path=localworkflowpluginlastmodifiedbrush} serverlastmodified={binding elementname=thisuc, path=serverworkflowpluginlastmodified} serverlastmodifiedbrush={binding elementname=thisuc, path=serverworkflowpluginlastmodifiedbrush} localconfiglastmodified={binding elementname=thisuc, path=localworkflowpluginconfiglastmodified} localconfiglastmodifiedbrush={binding elementname=thisuc, path=localworkflowconfiglastmodifiedbrush} serverconfiglastmodified={binding elementname=thisuc, path=serverworkflowpluginconfiglastmodified} serverconfiglastmodifiedbrush={binding elementname=thisuc, path=serverworkflowconfiglastmodifiedbrush} uploadpluginclick=uploadbutton_click uploadconfigclick=uploadercontrol_onuploadconfigclick uploadername={binding source={x:static tmsobjectsnames:tmsservicenames.workflow}} /> <controls:uploadercontrol controlheader=archiver plugin contentvisibility={binding elementname=thisuc, path=archiveruploadervisibility} hidebuttonclick=onhidebuttonclick currentprovidedconfigpath={binding elementname=thisuc, path=currentarchiverpluginconfigpath, updatesourcetrigger=propertychanged, mode=twoway} previousconfigpathscollection={binding elementname=thisuc, path=previousarchiverpluginconfigpathscollection} autouploadafterbuild={binding elementname=thisuc,path=archiverautoupload, mode=twoway} currentprovidedpath={binding elementname=thisuc, path=currentarchiverpluginpath, updatesourcetrigger=propertychanged, mode=twoway} openserverfolder=uploadercontrol_onopenserverfolder dockpanel.dock=top previouspathscollection={binding elementname=thisuc, path=previousarchiverpluginpathscollection} locallastmodified={binding elementname=thisuc, path=localarchiverpluginlastmodified} locallastmodifiedbrush={binding elementname=thisuc, path=localarchiverpluginlastmodifiedbrush} serverlastmodified={binding elementname=thisuc, path=serverarchiverpluginlastmodified} serverlastmodifiedbrush={binding elementname=thisuc, path=serverarchiverpluginlastmodifiedbrush} localconfiglastmodified={binding elementname=thisuc, path=localarchiverpluginconfiglastmodified} localconfiglastmodifiedbrush={binding elementname=thisuc, path=localarchiverconfiglastmodifiedbrush} serverconfiglastmodified={binding elementname=thisuc, path=serverarchiverpluginconfiglastmodified} serverconfiglastmodifiedbrush={binding elementname=thisuc, path=serverarchiverconfiglastmodifiedbrush} uploadpluginclick=uploadbutton_click uploadconfigclick=uploadercontrol_onuploadconfigclick uploadername={binding source={x:static tmsobjectsnames:tmsservicenames.archiver}} />c#for all the dependency properties above i have separate properties that are almost identical, except they are for different uploader type (workflow, archive, processing). do i really need it like that? i suppose not, but i don't know how to handle this better.sample properties #region wokrlfow plugin configpathbox //this one will be 'repeated' six times - 3x for different file types and 2x for dll and xmlprivate string _currentworkflowpluginconfigpath = settings.default.currentworkflowpluginconfigpath;public string currentworkflowpluginconfigpath{ get { return _currentworkflowpluginconfigpath; } set { _currentworkflowpluginconfigpath = value; settings.default.currentworkflowpluginconfigpath = value; raisepropertychanged(nameof(currentworkflowpluginconfigpath)); initializeworkflowpluginuploader(); }}private bool _prevworkflowconfigpathseventsubscribed;private observablecollection<string> _previousworkflowpluginconfigpathscollection = settings.default.previousworkflowpluginconfigpathscollection ?? new observablecollection<string>();public observablecollection<string> previousworkflowpluginconfigpathscollection{ get { if (!_prevworkflowconfigpathseventsubscribed) { _previousworkflowpluginconfigpathscollection.collectionchanged += _previousworkflowconfigpathslist_collectionchanged; _prevworkflowconfigpathseventsubscribed = true; } return _previousworkflowpluginconfigpathscollection; } set { }}void _previousworkflowconfigpathslist_collectionchanged(object sender, notifycollectionchangedeventargs e){ raisepropertychanged(nameof(previousworkflowpluginconfigpathscollection)); settings.default.previousworkflowpluginconfigpathscollection = previousworkflowpluginconfigpathscollection;}#endregion #region workflow // this is a section for last modified info along with brushes for coloring. again, these will be 'repeated' six times.private string _localworkflowpluginlastmodified;public string localworkflowpluginlastmodified{ get { return _localworkflowpluginlastmodified; } set { _localworkflowpluginlastmodified = value; raisepropertychanged(nameof(localworkflowpluginlastmodified)); brush local; brush server; colorizelabels(localworkflowpluginlastmodified, out local, serverworkflowpluginlastmodified, out server); localworkflowpluginlastmodifiedbrush = local; serverworkflowpluginlastmodifiedbrush = server; }}private string _serverworkflowpluginlastmodified;public string serverworkflowpluginlastmodified{ get { return _serverworkflowpluginlastmodified; } set { _serverworkflowpluginlastmodified = value; raisepropertychanged(nameof(serverworkflowpluginlastmodified)); brush local; brush server; colorizelabels(localworkflowpluginlastmodified, out local, serverworkflowpluginlastmodified, out server); localworkflowpluginlastmodifiedbrush = local; serverworkflowpluginlastmodifiedbrush = server; }}private brush _localworkflowpluginlastmodifiedbrush;public brush localworkflowpluginlastmodifiedbrush{ get { return _localworkflowpluginlastmodifiedbrush; } set { _localworkflowpluginlastmodifiedbrush = value; raisepropertychanged(nameof(localworkflowpluginlastmodifiedbrush)); }}private brush _serverworkflowpluginlastmodifiedbrush;public brush serverworkflowpluginlastmodifiedbrush{ get { return _serverworkflowpluginlastmodifiedbrush; } set { _serverworkflowpluginlastmodifiedbrush = value; raisepropertychanged(nameof(serverworkflowpluginlastmodifiedbrush)); }}#endregioni think i am handling button click events pretty well with switches, so that all uploaders point to the same handler and then proper object is launched: private async void uploadercontrol_onuploadconfigclick(object sender, routedeventargs e) { fileuploadinfo = preparing upload...; application.current.mainwindow.cursor = cursors.wait; string pluginname = (sender as controls.uploadercontrol).uploadername; switch (pluginname) { case tmsservicenames.workflow: await workflowfileuploader?.uploadconfig(false); break; case tmsservicenames.archiver: await archiverfileuploader?.uploadconfig(false); break; case tmsservicenames.pptservice: await pptfileuploader?.uploadconfig(false); break; } application.current.mainwindow.cursor = cursors.arrow; }i have a big problem with initialization of each of the fileuploader classes.for each of the three types i had to create a separate method: private void initializeworkflowpluginuploader() { if (currentworkflowpluginpath.isexistingfilepath() && servicesmanager.services.containskey(tmsservicenames.workflow)) { iprogress<filewatcherprogress> p = new progress<filewatcherprogress>(x => { localworkflowpluginlastmodified = x.localfileversion ?? localworkflowpluginlastmodified; serverworkflowpluginlastmodified = x.serverfileversion ?? serverworkflowpluginlastmodified; localworkflowpluginconfiglastmodified = x.localconfigfileversion ?? localworkflowpluginconfiglastmodified; serverworkflowpluginconfiglastmodified = x.serverconfigfileversion ?? serverworkflowpluginconfiglastmodified; fileuploadinfo = x.overallinfo ?? fileuploadinfo; if (x.popuptext != null) { launchuploaderpopup(x.popuptitle, x.popuptext); } }); workflowfileuploader = new remotefileuploader(currentworkflowpluginpath, currentworkflowpluginconfigpath, new tmsfolderpaths().workflow, workflowautoupload, showpopupafterautoupload, servicesmanager.services[tmsservicenames.workflow], p); } }i would like to create all these in a single method, where i could pass a bunch of properties as parameters - in this case for example file paths and properties for status labels (localworkflowpluginlastmodified) etc, which are different for each file type.so, summing up, while this is finished and will not grow any further, i would like to do a better structure next time, reduce the number of properties if possible. what would i do if i had to add 5 more file types, and then add 2 more labels to each of them?",
    "present_kp": [
      "c#",
      "xaml"
    ],
    "absent_kp": [
      "wpf"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "br0 causes to drop multicast connection after 5 minutes. i have these interfaces set up on the router (linux machine):br0: flags=4419<up,broadcast,running,promisc,multicast> mtu 1500 inet 192.168.0.3 netmask 255.255.255.0 broadcast 192.168.0.255bridge name bridge id stp enabled interfacesbr0 8000.00156d8591ec no eth0 wlan0eth0: flags=4163<up,broadcast,running,multicast> mtu 1500 inet6 fe80::7271:bcff:feb1:d9cf prefixlen 64 scopeid 0x20<link>eth1: flags=4163<up,broadcast,running,multicast> mtu 1500 inet 10.200.44.147 netmask 255.255.255.128 broadcast 10.200.44.255wlan0: flags=4163<up,broadcast,running,multicast> mtu 1500 inet6 fe80::215:6dff:fe85:91ec prefixlen 64 scopeid 0x20<link>in br0 there are two interfaces (eth0 - local home network and wlan0 - wireless home network). eth1 is internet connection interface and is not part of the br0. i use mumudvb for sending multicast iptv with ip group 239.100.0.1 (udp/1234) and receiving this multicast traffic on a different machine on lan using vlc player. when i configure it to send multicast over eth0 all works well on home lan but i can't join the multicast group from the router itself. that's why i configured it to send multicast over br0 (makes more sense - then i can join the multicast group from the lan eth0 as well as router itself) but after about 5 minutes since join, lan multicast connection drops (without sending any igmp message). but i can join again and it will work for next 5 minutes again, then drop.why is it dropping when sending multicast over br0 and not dropping when sending directly through eth0? am i missing some configuration on the bridge? what can expire during this period? for example stp is disabled for the bridge but it shouldn't affect this?when i join the br0 multicast group from the router itself, it won't drop. only when subscribed from a machine connected to the br0 through eth0.",
    "present_kp": [
      "linux",
      "bridge",
      "multicast"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "google spreadsheets timesheet calculations. i have a timesheet that i use to log my time. it works fine if i log in and log out. it works at the end of the day but doesn't work in all configurations. for example, i want it to work if i only arrive then leave at the end of the day but also show total of hours for the day if i am logging back in after a break.my example shows the configurations that don't work.editupdated example sheet so anyone can use it. what i do is hide all but the current week. then go to the html view and copy+paste that to my payroll person.",
    "present_kp": [
      "google spreadsheets"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to add a string to cursor point of gvim through a shell script. when gvim is opened it saves the cursor position whether it is minimized or not. so i'm curious whether it is possible to add a string starting from the cursor blinking position by running a shell script or through a terminal.",
    "present_kp": [
      "terminal",
      "gvim"
    ],
    "absent_kp": [
      "cursor movement",
      "invocation",
      "persistent state"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "can i use qt open-source for my web-scraping website?. i apologize if i should be understanding this more readily but i'm a little new to this and dont understand the lgpl license. here is the faq for it: <url> am making a website that scrapes data from other websites and organizes it in a nice manner. i plan to try and make money off this website through advertising and also by getting commissions from linking to certain sites. i plan on using server-side code that scrapes the website using pyqt4 (which uses qt). the code scrapes the website and stores the data in a sql database. i then use the sql database to display the webpage.i think it's fine to use the open source version of qt right? i'm not selling or really giving any application away. the qt is being used on server-side code to generate the sql database.and if it is fine to use the open source, do i have to display a link on the website to the source code of the qt i'm using? just trying to fully understand.",
    "present_kp": [
      "lgpl",
      "qt"
    ],
    "absent_kp": [
      "web development",
      "licensing",
      "python"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "print out table with start/end temperatures and step size. i am trying to learn c++ by myself. i looked up a sample question after going through some text. though i would like someone to review my code. i'm basically asking you to break it to show some flaws or some thing i missed. as a beginner i tried exhaustively to improve it and now hit a wall to analyse robustness of the code.problem statement:in this challenge, write a program that takes in three arguments, a start temperature (in celsius), an end temperature (in celsius) and a step size. print out a table that goes from the start temperature to the end temperature, in steps of the step size; you do not actually need to print the final end temperature if the step size does not exactly match. you should perform input validation: do not accept start temperatures less than a lower limit (which your code should specify as a constant) or higher than an upper limit (which your code should also specify). you should not allow a step size greater than the difference in temperatures.#include <iostream>#include <cstdlib>#include <string>#define lower_limit 23.3#define upper_limit 256.3using namespace std;bool isnum(string s){ //check if the string is a number //48 & 57 int len = s.length(); for(int i = 0; i < len; i++) { //cout << s[i] << << int(s[i])<< endl; if(int(s[i])>=48 && int(s[i])<=57) return true; else { return false; break; } }}int main(int argc, char** argv)//the int argc holds the argument count and the argv is a 2-d array4// of characters{ double start,end,step_size; if(argc!=4) { cout<<please enter three intigers<<endl; cout<<celcius <start_temprature> <end_temprature> <step_size><<endl; cout<<last step may not be printed<<endl; } else { //check if all the arguments are intigers if(isnum(argv[1]) && isnum(argv[2]) && isnum(argv[3])) { cout<<argv[1]<<endl; cout<<argv[2]<<endl; cout<<argv[2]<<endl; start = atof(argv[1]); end = atof(argv[2]); step_size = atof(argv[3]); //calculate the table and print. if(start < lower_limit || start >upper_limit) { cout<<the <start_temprature> does not meet the limit requirement (<<lower_limit<<<<c - <<upper_limit<<<<c)<<endl; // the degree symbol to be printed on command line requires utf-8 characters which has the degree symbol and the location is return -1; } if(end >upper_limit || end <lower_limit) { cout<<the <end_temprature> does not meet the requirement (<<lower_limit<<<<c - <<upper_limit<<<<c) <endl; return -1; } if (step_size <1 || step_size >=(upper_limit - lower_limit)) { //zero or negetive stepsize checking cout<<the step_size cannot be negetive, zero or greater than or equal to step_size<<endl; return -1; } if(end<start) //swapping variables if start is greater than end { cout <<swapped! end and start values for simplicity <<endl; double tmp = start; start = end; end = tmp; } cout << start <<start<<endl; cout << end <<end<<endl; cout << step_size <<step_size<<endl; int nend = (int)((end-start)/step_size); cout << nend <<number of iterations<<endl; for(int i = 0; i < nend ;i++) { cout << start << <<c = << ((start*(9/5))+32) << <<f <<endl; start += (double)step_size; } } else cout <<all three input arguments must be positive numbers! <<endl; }}",
    "present_kp": [
      "c++",
      "beginner"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "load balancing multiplie services linux. i have setup multiple servers,one of them is a windows server running team speak.the other is a game server with csgo(counter strike global offensive) and lastly, one running an apache server.i will use my vserver as load balancer.i want to load balance all traffic over the vserver but without redirects, so that clients can connect to my vserver using multiple games and gaming services.it has been suggested to use something like vpn or iptables but didn't get a good answer so i'm asking here..i am already using pound for web services and now i will do the same things for my games and services...how i can do this?another thing that i would like to do is white-list on every server only the vserver, so no client can do an direct connection to the servershere is the actual system:[client]-(ts3)--[ts3 server](different ip)[client]-(apache)--[apache server](different ip)[client]-(game)--[game server](different ip)here is my planned system: (one ip)[client]-(ts3/apache/game)--[vserver]-(apache via pound)-[apache server] |-(ts3 traffic)-[ts3 server] |-(game traffic)-[game server]",
    "present_kp": [
      "linux",
      "load balancing"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "what is the kernel syntax when booting from zfs?. i'm trying to write a grub.cfg completely from scratch that will boot one of several freebsd systems off of a single zfs pool named tank with a set of root file systems named root1, root2, root3.when i look at the official grub2 docs, there is one example in 5.3 menuentry freebsd { insmod zfs search --set=root --label freepool --hint hd0,msdos7 kfreebsd /freebsd@/boot/kernel/kernel kfreebsd_module_elf /freebsd@/boot/kernel/opensolaris.ko kfreebsd_module_elf /freebsd@/boot/kernel/zfs.ko kfreebsd_module /freebsd@/boot/zfs/zpool.cache type=/boot/zfs/zpool.cache set kfreebsd.vfs.root.mountfrom=zfs:freepool/freebsd set kfreebsd.hw.psm.synaptics_support=1}i figured that freepool is likely the zfs pool name which i would replace with tank. i suspect that the x@y syntax is the way to refer to a file y on the file system x of the pool selected by the search. then i would replace this with /root1@/boot/kernel/kernel. sadly, the x@y syntax is undocumented and i don't want to just try with fingers crossed, but rather understand and know what i'm doing. can anyone shed light on this?",
    "present_kp": [
      "freebsd",
      "grub2",
      "zfs"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "are there constraints for functions in structured programming?. i just talking with a colleague (university instructor) who teaches c (fundamental of programming course). he said i won't give score to a student, if he uses i/o (scanf or printf or cin count) in a function (it shows he hasn't understood it well) or if he writes a function which does two things (for example to return the maximum of an array, he shouldn't sort the array and return the first element, however he can use two functions, one for sorting and another for returning the first element ...)... i thought these are related to his teaching method but he claimed they are the principles of functions in structured programming.are there really such constraints and definitions for functions? what are those?if yes, in which topic are they discussed? should they be discussed in a c or c++ teaching course?",
    "present_kp": [
      "functions",
      "definition"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how does facebook know my credit card number?. i just looked at boosting a post on one of my facebook pages. and they already had a credit card number for me. i'm pretty sure i haven't given them my credit card details before, is there another reason they could have my credit card number? for example, i do use gmail to log in to it, and gmail has my credit card number.",
    "present_kp": [
      "facebook"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "using logic to compare absolute values. is there a way to make my code simpler/more pythonic?given three int values (a, b, c), return true if one of b or c is close (differing from a by at most 1), while the other is far, differing from both other values by 2 or more. note: abs(num) computes the absolute value of a number. close_far(1, 2, 10) trueclose_far(1, 2, 3) falseclose_far(4, 1, 3) truedef close_far(a, b, c): if abs(a - b) <= 1 or abs(a - c) <= 1: if abs(c - a) >= 2 <= abs(c - b) or abs(b - a) >= 2 <= abs(b - c): return true return false",
    "present_kp": [
      "python"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how do i make an alias to substitute single word in a piped command?. i use aliases a lot but right now only for use cases like alias i='sudo apt-get install -y'. i often would like to add an alias in the following form:alias cmd='echo [something] >> /path/to/file' where i would like to substitute [something] with what i enter after the cmd.i can obviously create a one-line script,save it somewhere and make an alias to that command but since i only want to substitute only 1 word in a pipe, is there a simpler way to do this?",
    "present_kp": [
      "pipe",
      "alias"
    ],
    "absent_kp": [
      "command line",
      "arguments"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "a game of positioning overlapping circles to maximize travel time between them. i encountered the following game. i'll migrate this as requested.a bug is visiting circles, and an adversary wishes to maximize his travel time.the adversary places a circle on every turn. the bug walks from it's current position directly toward the center of the newest circle, then stops when it encounters the interior of the circle (thus: it doesn't walk if a circle is played covering its location). this is the bug's turn.there are $n$ circles available to the adversary. each subsequent circle has radius less than the previous circle. each circle must intersect the intersection of all previously played circles. that is, all circles must have a common intersection once all are played.edit: the adversary is free to choose the radii of the circles, subject to the constraint that the radii monotonically decrease.questions and answers:is the distance as $n o\\infty$ bounded? a: no, an example of an adversary strategy is given by this answerwhat is the maximum distance the bug must travel over the playing of $n$ circles. a: it grows at $\\theta(\\log(n))$, by the same answer.variant 2: the bug walks directly toward the intersection of the two most recently played circles.update: this variant was addressed, under the assumption that the bug can only remember the last 2 circles played here. the result was again an unbounded distance. what impact does unliminted memory have? i.e., the bug goes to the intersection of all previously played circles. this produced a loose bound of $o(d)$, where $d$ is the diameter of the first circle. obviously it cannot be less than this. see here. the current upper bound was $1000 imes d$. this was obtained by approximating the worst-case path as a tour around progressively smaller circles. it was shown that the bug always makes progress towards the final intersection, thus reducing the next-step distance it must travel.i suspect the distance traveled is a small constant times the circumference of the first circle, but i'm not currently able to provide a good proof.",
    "present_kp": [],
    "absent_kp": [
      "gt.game theory"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to find which package required another package?. i am looking for which no-longer-installed package depended on rsync. an apt-get autoremove now wants to remove rsync so i'm guessing that it was installed as part of a dependency rather than manually, and i'm curious which package depended on it.can i search back in logs for something like x requires y, so i will install y as well? or does it even store which reverse dependency required it, just like it stores that it was not installed by the user?",
    "present_kp": [],
    "absent_kp": [
      "dpkg",
      "dependencies"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to make people new to programming stop asking me questions and distracting me?. i am at secondary school right now and i'm the only one in my class who is experienced with programming. because of that, people are constantly distracting me while i'm writing code to ask me to solve a problem. usually i reply with something like 'i don't know, i never use that' but i don't want to lie to people.another problem is that i became so well known for this that even students from other classes are asking me questions. i find this damn annoying.thirdly, if i solve a problem for them they don't learn anything from it.how can i stop people from asking me programming-related questions in a kind way?",
    "present_kp": [],
    "absent_kp": [
      "productivity",
      "knowledge transfer"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "preventing/blocking from crawling a specific user control of a page. currently, google access/crawls a user control from the page given below-<url> of google crawl - linkas per the above link, we can see that the comment section [i.e. user control] is accessed and crawled and we have to prevent/disallow and block the same.any ideas or suggestion on how to block the same.",
    "present_kp": [],
    "absent_kp": [
      "seo",
      "web crawlers",
      "robots.txt",
      "meta tags"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "minimum weighted arithmetic mean partion?. assume i have some positive numbers $a_1,\\ldots,a_n$ and a number $k \\in \\mathbb{n}$. i want to partition these numbers into exactly $k$ sets $a_1,\\ldots,a_k$ such that the weighted arithmetic mean$$ ext{cost}(a_i,\\ldots,a_k)=\\sum_{i=1}^{k} rac{|a_i|}{n}c(a_i)$$is minimal, where $c(a_i)=\\sum_{a \\in a_i}a$ is simply the sum of all numbers in $a_i$.is there actually a (polynomial) algorithm to do this or is this a (np) hard problem? i tried to reduce it to some np-hard problems but didn't get anywhere, especially because the numbers are nonnegative and thus in an optimal partition big sets need to have smaller weight which seems to be some kind of balancing problem instead of a packing problem (which i am more familiar with).",
    "present_kp": [
      "np"
    ],
    "absent_kp": [
      "optimization",
      "np hard",
      "partitions"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "update linux centos 6.6. i've tried to update my linux server centos6.6 using these command, yum clean all and then yum update, but unfortunately i got these errors.please help me. according to these errors if i use this command yum update --skip-broken, it will be dangerous to my server??? how can i solve these errors.thank you in advance error: package: xine-lib-extras-1.1.21-10.el6.x86_64 (@epel) requires: libmagickcore.so.2()(64bit) removing: imagemagick-6.5.4.7-7.el6_5.x86_64 (@base) libmagickcore.so.2()(64bit) updated by: imagemagick-6.7.2.7-6.el6.x86_64 (base) not founderror: package: php-pecl-igbinary-1.2.1-1.el6.x86_64 (epel) requires: php(zend-abi) = 20090626 installed: php-common-5.5.14-2.el6.remi.x86_64 (@remi-php55) php(zend-abi) = 20121212-64 available: php-common-5.3.3-49.el6.x86_64 (base) php(zend-abi) = 20090626 available: php55w-common-5.5.38-1.w6.x86_64 (webtatic) php(zend-abi) = 20121212-64 available: php56w-common-5.6.30-1.w6.x86_64 (webtatic) php(zend-abi) = 20131226-64 available: php70w-common-7.0.15-1.w6.x86_64 (webtatic) php(zend-abi) = 20151012-64 available: php70w-common-7.0.16-1.w6.x86_64 (webtatic) php(zend-abi) = 20151012-64 available: php70w-common-7.0.17-1.w6.x86_64 (webtatic) php(zend-abi) = 20151012-64 available: php71w-common-7.1.1-1.w6.x86_64 (webtatic) php(zend-abi) = 20160303-64 available: php71w-common-7.1.2-1.w6.x86_64 (webtatic) php(zend-abi) = 20160303-64 available: php71w-common-7.1.3-1.w6.x86_64 (webtatic) php(zend-abi) = 20160303-64error: package: xine-lib-extras-1.1.21-10.el6.x86_64 (@epel) requires: libmagickwand.so.2()(64bit) removing: imagemagick-6.5.4.7-7.el6_5.x86_64 (@base) libmagickwand.so.2()(64bit) updated by: imagemagick-6.7.2.7-6.el6.x86_64 (base) not founderror: package: php-pecl-apcu-4.0.11-2.el6.x86_64 (epel) requires: php(api) = 20090626 installed: php-common-5.5.14-2.el6.remi.x86_64 (@remi-php55) php(api) = 20121113-64 available: php-common-5.3.3-49.el6.x86_64 (base) php(api) = 20090626 available: php55w-common-5.5.38-1.w6.x86_64 (webtatic) php(api) = 20121113-64 available: php56w-common-5.6.30-1.w6.x86_64 (webtatic) php(api) = 20131106-64 available: php70w-common-7.0.15-1.w6.x86_64 (webtatic) php(api) = 20151012-64 available: php70w-common-7.0.16-1.w6.x86_64 (webtatic) php(api) = 20151012-64 available: php70w-common-7.0.17-1.w6.x86_64 (webtatic) php(api) = 20151012-64 available: php71w-common-7.1.1-1.w6.x86_64 (webtatic) php(api) = 20160303-64 available: php71w-common-7.1.2-1.w6.x86_64 (webtatic) php(api) = 20160303-64 available: php71w-common-7.1.3-1.w6.x86_64 (webtatic) php(api) = 20160303-64error: package: php-pecl-igbinary-1.2.1-1.el6.x86_64 (epel) requires: php(api) = 20090626 installed: php-common-5.5.14-2.el6.remi.x86_64 (@remi-php55) php(api) = 20121113-64 available: php-common-5.3.3-49.el6.x86_64 (base) php(api) = 20090626 available: php55w-common-5.5.38-1.w6.x86_64 (webtatic) php(api) = 20121113-64 available: php56w-common-5.6.30-1.w6.x86_64 (webtatic) php(api) = 20131106-64 available: php70w-common-7.0.15-1.w6.x86_64 (webtatic) php(api) = 20151012-64 available: php70w-common-7.0.16-1.w6.x86_64 (webtatic) php(api) = 20151012-64 available: php70w-common-7.0.17-1.w6.x86_64 (webtatic) php(api) = 20151012-64 available: php71w-common-7.1.1-1.w6.x86_64 (webtatic) php(api) = 20160303-64 available: php71w-common-7.1.2-1.w6.x86_64 (webtatic) php(api) = 20160303-64 available: php71w-common-7.1.3-1.w6.x86_64 (webtatic) php(api) = 20160303-64error: package: php-pecl-apcu-4.0.11-2.el6.x86_64 (epel) requires: php(zend-abi) = 20090626 installed: php-common-5.5.14-2.el6.remi.x86_64 (@remi-php55) php(zend-abi) = 20121212-64 available: php-common-5.3.3-49.el6.x86_64 (base) php(zend-abi) = 20090626 available: php55w-common-5.5.38-1.w6.x86_64 (webtatic) php(zend-abi) = 20121212-64 available: php56w-common-5.6.30-1.w6.x86_64 (webtatic) php(zend-abi) = 20131226-64 available: php70w-common-7.0.15-1.w6.x86_64 (webtatic) php(zend-abi) = 20151012-64 available: php70w-common-7.0.16-1.w6.x86_64 (webtatic) php(zend-abi) = 20151012-64 available: php70w-common-7.0.17-1.w6.x86_64 (webtatic) php(zend-abi) = 20151012-64 available: php71w-common-7.1.1-1.w6.x86_64 (webtatic) php(zend-abi) = 20160303-64 available: php71w-common-7.1.2-1.w6.x86_64 (webtatic) php(zend-abi) = 20160303-64 available: php71w-common-7.1.3-1.w6.x86_64 (webtatic) php(zend-abi) = 20160303-64 you could try using --skip-broken to work around the problem you could try running: rpm -va --nofiles --nodigest",
    "present_kp": [
      "centos",
      "yum"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "what makes cdms such as widevine hard to reverse engineer?. according to this pdf, widevine has three security levels. the least secure one, and the one used by chrome on desktops is level 3 in which all decryption is done outside of a trusted execution environment.but in that case, what stops someone from opening the widevine chrome plugin in ida and following the video data until they get to whatever function that decrypts it and then write their own implementation of widevine that just saves the output to a file instead of rendering it?the pdf does say that appropriate measures may be taken to protect the cryptographic information and decrypted content on host operating system, but things like video games also use various protection systems, but these still get cracked with some effort.so, why hasn't widevine been cracked yet?",
    "present_kp": [
      "ida"
    ],
    "absent_kp": [
      "disassembly"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "add another accout in gmail interface. q: what's the work around way to use gmail interface with email handle by ms exchange server?the company i worked for use ms exchange server for the emails, but my workstation is imac and they installed me entourage 2007 which is very poor designed for work flow.i have add my work email account (ex: <email>) to the gmail succesfully, got verify..etc. but it only work half way,scenerio: (works) when i create a new message from gmail interface with <email> and send to <email>, i can recieve it in entourage from my local computer. (not works) but if i create a new message from entourage or ms exchange web server, the message just don't get to the gmail interface.(works) if i create new message from gmail interface and send to my colleuage, she can recieve it and reply, and i can get her replies in gmail interface as well.i believe there is firewall or sort of thing, the challengs i face is without it guy turn off anything for me ( a wanna-be super user), what's the work around way? of coz no forwarding method, coz it will create a mess....thanks.",
    "present_kp": [
      "email",
      "gmail"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "chkconfig on linux mint 12 giving tons of errors. i'm trying to disable some services from starting at boot time on my linux mint 12 laptop.so i installed chkconfig, which has worked great for me before on fedora.however, on linux mint 12, it gives me tons of errors. here is an example, trying to disable the rsync service:$ sudo chkconfig rsync offinsserv: warning: script 'k01acpi-support' missing lsb tags and overridesthe script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'cryptdisks-udev' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'cryptdisks-udev'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'cryptdisks-udev'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'acpid' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'acpid'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'acpid'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'plymouth-upstart-bridge' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'plymouth-upstart-bridge'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'plymouth-upstart-bridge'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'rsyslog' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'rsyslog'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'rsyslog'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'friendly-recovery' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'friendly-recovery'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'friendly-recovery'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'udevtrigger' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'udevtrigger'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'udevtrigger'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'udev-finish' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'udev-finish'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'udev-finish'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'plymouth-stop' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'plymouth-stop'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'plymouth-stop'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'apport' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'apport'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'apport'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'dbus' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'dbus'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'dbus'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'cryptdisks-enable' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'cryptdisks-enable'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'cryptdisks-enable'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'hwclock' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'hwclock'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'hwclock'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'lightdm' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'lightdm'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'lightdm'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'udevmonitor' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'udevmonitor'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'udevmonitor'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'ufw' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'ufw'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'ufw'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'binfmt-support' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'binfmt-support'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'binfmt-support'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'udev' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'udev'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'udev'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'irqbalance' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'irqbalance'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'irqbalance'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'cron' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'cron'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'cron'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'nmbd' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'nmbd'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'nmbd'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'plymouth-splash' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'plymouth-splash'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'plymouth-splash'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'procps' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'procps'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'procps'insserv: warning: script 'acpi-support' missing lsb tags and overridesthe script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'network-manager' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'network-manager'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'network-manager'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'smbd' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'smbd'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'smbd'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'dmesg' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'dmesg'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'dmesg'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'module-init-tools' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'module-init-tools'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'module-init-tools'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'network-interface' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'network-interface'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'network-interface'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'console-setup' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'console-setup'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'console-setup'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'anacron' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'anacron'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'anacron'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'modemmanager' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'modemmanager'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'modemmanager'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'udev-fallback-graphics' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'udev-fallback-graphics'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'udev-fallback-graphics'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'plymouth' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'plymouth'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'plymouth'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'network-interface-security' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'network-interface-security'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'network-interface-security'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'alsa-store' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'alsa-store'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'alsa-store'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'alsa-restore' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'alsa-restore'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'alsa-restore'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'avahi-daemon' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'avahi-daemon'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'avahi-daemon'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'plymouth-log' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'plymouth-log'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'plymouth-log'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'mysql' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'mysql'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'mysql'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'atd' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'atd'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'atd'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'hostname' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'hostname'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'hostname'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'cups' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'cups'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'cups'the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'hwclock-save' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'hwclock-save'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'hwclock-save'insserv: script virtualbox: service vboxdrv already provided!insserv: script virtualbox: service virtualbox already provided!the script you are attempting to invoke has been converted to an upstartjob, but lsb-header is not supported for upstart jobs.insserv: warning: script 'setvtrgb' missing lsb tags and overridesinsserv: default-start undefined, assuming empty start runlevel(s) for script 'setvtrgb'insserv: default-stop undefined, assuming empty stop runlevel(s) for script 'setvtrgb'it seems to have worked when i run:# chkconfig rsync rsync offis it bad to continue to use chkconfig? can anyone suggest an alternate service-managing program, or a way to fix the errors when running chkconfig?",
    "present_kp": [
      "linux mint",
      "upstart",
      "chkconfig"
    ],
    "absent_kp": [
      "command line"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "should tdd be done top-down, bottom-up, or a combination of the two?. when doing test-driven development, should you start at the lowest possible level of abstraction, writing your classes bottom-up, to ensure that you can run unit tests in a timely manner? or should you start somewhat above that level, coding several classes top-down, and afterward, using those classes, code bottom-up?",
    "present_kp": [
      "tdd"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "are python sockets suitable for file synchronization?. i'm working in an organisation with limited funds. they can't afford a business account on dropbox. however, they would find it useful to have all their files synchronized on local machines. i've recently looked at python sockets and it looks as though a custom script could achieve what they're looking for. i'm concerned that it may be too low-level because a library may exist that provides a lot of functionality and it would be a waste of resources to start from scratch. is there a python library/module that would provide what i'm looking for?",
    "present_kp": [
      "python",
      "sockets"
    ],
    "absent_kp": [
      "networking",
      "file storage"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "jquery ui minify. i have a series of web pages that link to the following:jquery-ui-1.8.4.custom.min.jsjquery.ui.widget.jsjquery.ui.core.jsjquery.ui.accordion.jsjquery.ui.selectmenu.jsjquery.ui.button.jsnot every page uses each .js (for example, not every pages uses jquery.ui.button) however i was wondering if it would make more sense to combine all of these files and minify them into a single.js file and include it on every page?",
    "present_kp": [
      "jquery"
    ],
    "absent_kp": [
      "javascript"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "design patterns in swift: chain of responsibility. i'm solving the following problem using the chain of responsibility design pattern in swift:not all mechanics are created equally. some mechanics are more experienced and can do more than others. we need a system where every job is propagated from the least experienced mechanic to the most. this way experienced mechanics that can perform more jobs are not busy with jobs that more junior mechanics can take care of.i would love some feedback on how i can improve this. if it remains true to the definition of chain of responsibility or if there are any swift standard coding convention that i should be following. here is the code, the full repo can be found here: design patterns in swift: chain of responsibility import foundationenum skill: int{ case oilchangeonly = 1, junior, apprentice, mastermechanic}class job{ let minimumskillset: skill let name: string var completed: bool = false init(minimumskillset: skill, name: string){ self.minimumskillset = minimumskillset self.name = name }}class mechanic{ let skill: skill var name: string var isbusy: bool = false init(skill: skill, name: string){ self.skill = skill self.name = name } func performjob(job: job) -> bool{ if job.minimumskillset > self.skill || isbusy{ assert(false, this mechanic is either busy or insufficiently skilled for this job, he should have never been asked to perform it, there is something wrong in the chain of responsibility); }else { isbusy = true print(\\(name) with skill set \\(skill) has started to do \\(job.name)) job.completed = true return true } }}class mechanicskillgroup{ var mechanics: [mechanic] var nextlevel: mechanicskillgroup? var skill: skill init(skill: skill, mechanics: [mechanic], nextlevel: mechanicskillgroup?){ self.mechanics = mechanics self.skill = skill self.nextlevel = nextlevel } func performjoborpassitup(job: job) -> bool{ if (job.minimumskillset > skill || mechanics.filter({$0.isbusy == false}).count == 0){ if let nextlevel = nextlevel{ return nextlevel.performjoborpassitup(job) }else{ print(no one is available to do this job) return false } }else{ if let firstavailablemechanic = mechanics.filter({$0.isbusy == false}).first{ return firstavailablemechanic.performjob(job) } assert(false, this should never be reached since our if-else statement is fully exhaustive. you cannot have both all mechanics busy and an available mechanic within one skill group); } }}class shop{ private var firstmechanics: mechanicskillgroup init(firstmechanics: mechanicskillgroup){ self.firstmechanics = firstmechanics } func performjob(job: job) -> bool{ return firstmechanics.performjoborpassitup(job) }}here is main with setup and some test casesimport foundationvar steve = mechanic(skill: .mastermechanic, name: steve frank)var joe = mechanic(skill: .mastermechanic, name: joe alison)var jack = mechanic(skill: .mastermechanic, name: jack ryan)var brian = mechanic(skill: .mastermechanic, name: drake jin)var mastermechanics = mechanicskillgroup(skill: .mastermechanic, mechanics: [steve, joe, jack, brian], nextlevel: nil)var tyson = mechanic(skill: .apprentice, name: tyson trump)var tina = mechanic(skill: .apprentice, name: tina bernard)var bryan = mechanic(skill: .apprentice, name: bryan tram)var lin = mechanic(skill: .apprentice, name: lin young)var apprenticemechanics = mechanicskillgroup(skill: .apprentice, mechanics: [tyson, tina, bryan, lin], nextlevel: mastermechanics)var ken = mechanic(skill: .junior, name: ken hudson)var matt = mechanic(skill: .junior, name: matt lowes)var sandeep = mechanic(skill: .junior, name: sandeep shenoy)var tom = mechanic(skill: .junior, name: tom berry)var juniormechanics = mechanicskillgroup(skill: .junior, mechanics: [ken, matt, sandeep, tom], nextlevel: apprenticemechanics)var grant = mechanic(skill: .oilchangeonly, name: grant hughes)var larry = mechanic(skill: .oilchangeonly, name: larry white)var bryant = mechanic(skill: .oilchangeonly, name: bryant newman)var reza = mechanic(skill: .oilchangeonly, name: reza shirazian)var laura = mechanic(skill: .oilchangeonly, name: laura lee)var arnold = mechanic(skill: .oilchangeonly, name: arnold shummer)var oilchangeonlyes = mechanicskillgroup(skill: .oilchangeonly, mechanics: [grant], nextlevel: juniormechanics)var shop = shop(firstmechanics: oilchangeonlyes)var jobs = [job(minimumskillset: .junior, name: windshield viper), job(minimumskillset: .apprentice, name: light bulb change), job(minimumskillset: .apprentice, name: battery replacement), job(minimumskillset: .oilchangeonly, name: general oil change), job(minimumskillset: .oilchangeonly, name: general oil change), job(minimumskillset: .oilchangeonly, name: general oil change), job(minimumskillset: .oilchangeonly, name: general oil change), job(minimumskillset: .mastermechanic, name: timing belt replacement), job(minimumskillset: .junior, name: brake pads replacement)]for job in jobs{ shop.performjob(job)}here is output you'd get with this setup:ken hudson with skill set junior has started to do windshield wipertyson trump with skill set apprentice has started to do light bulb changetina bernard with skill set apprentice has started to do battery replacementgrant hughes with skill set oilchangeonly has started to do general oil changematt lowes with skill set junior has started to do general oil changesandeep shenoy with skill set junior has started to do general oil changetom berry with skill set junior has started to do general oil changesteve frank with skill set mastermechanic has started to do timing belt replacementbryan tram with skill set apprentice has started to do brake pads replacementprogram ended with exit code: 9",
    "present_kp": [
      "design patterns",
      "swift"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "personalizing trello invitation message. i'm a member of an irl ong and i invited several to a trello board. but as they don't know what's this trello they put their invitation in the trash.can i change the invitation message to write a personalized message?",
    "present_kp": [
      "trello"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "what can we say about all cycles in graphs (connected undirected graph). i am considering one optimization problem who is known to be np hard in the general setting. but there is application of this problem on the cylces of graph. this problem involves several sets and in this setting each set is the set of nodes of cycle in connected unidrected graph. and all cycles of connected unidrected graphs are the all sets for this problem. this is quite specific setting for the problem and i wonder whether there can be improvements in this specific setting.i would like to work out all the details myself but the question is - is there the general theory of all cycles in grpahs (connected undirected). like - what is the number of cycles, what is the minimum and maximum lenght of them, how much common nodes they can have and so on? mybe there are connections with group theory - e.g. cycle could be some kind of orbit for a group element (in rude language). any such information provides the constraint on the initial problem and therefore - the complexity improvements can be possible to achieve in this specific setting.google gives a lot about hamiltonian and similar specific cycles. my question is about all possible cycles in graph.any references could be helpful. any names for the problems and keywords in this are could be appreciated as well. thanks.",
    "present_kp": [],
    "absent_kp": [
      "cc.complexity theory",
      "graph theory",
      "graph algorithms"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "the joomla 'create a template' tutorial. i've recently downloaded joomla using an instant download option, and now i'm looking to create my own templates. i went to <url> and am trying to follow the tutorial but i've failed at the first hurdle... where it says:first, open the templates directory in your joomla installation. then create a subfolder in it named tutorial_template. all the files of your template will reside in it.the problem is i have no clue how to do this, where is the directory to start with? the only thing i know how to get up is the administrator's screen, in which i have to choose one of the stock templates, which would be great if i didn't care about my own designs...but obviously i'm looking to have my own html and css coding used instead.how do i find where the folders are and how do i open them?",
    "present_kp": [
      "html",
      "css",
      "joomla"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "file containing directory structure. i'm currently doing a ctf challenge and have extracted what i assume is the flag by injecting a relative path into the $filename parameter of a call to the php functionfile_get_contents($filename)that is executed by an apache server on the target system.i got the name of the file the flag was hidden in through a hint at some other point but i was wondering:is there a file that's commonly present on a linux system that holds information about the directory structure and files contained in these directories?i've tried searching through several log files in order to find references to interesting files but i've yet to find a detailed list of file names. i also do not have permission to read entire device files. i do have full control over the $filename parameter but from i gathered, i can only insert absolute or relative paths without wildcards in there.",
    "present_kp": [
      "files",
      "php",
      "directory structure"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "pda for all non-palindromic strings of even length. i had a homework assignment where i had to build a pda over the alphabet $\\{a,b\\}^*$, accepting $l = \\{x \\mid x ext{ is even but not a palindrome}\\}$.i already turned it in, but i know i had it wrong and it's driving me insane that i can't figure out this construction. i tried a cartesian product construction of the following languages and then deselected the accepting states of $l_2$, but i obviously did it wrong:$l_1 = \\{x \\mid x ext{ is even}\\}$$l_2 = \\{xx^r\\}$, where $x^r$ denotes $x$ reversed.i kept running into a problem where it would still accept because palindromes are even and i was basically accepting all even numbers.",
    "present_kp": [],
    "absent_kp": [
      "finite automata",
      "pushdown automata",
      "nondeterminism"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "binding events in constructor. i am trying to get familiar with oop javascript, in particular working with the prototype pattern and would love some pointers/suggestions on how to improve my code.i think i have understood the basics well, and have been able create class instances successfully.i am however, a little unsure on how to work with events when it comes to representing these instances with a ui. until now, i have been binding my events in the constructor before the element gets added to the dom, but am sure this could be improved:function person(data) { var self = this; self.data = data; self.$element = $('<div class=person>' + '<p class=name>' + data.name + '</p>' + '<p class=age>' + data.age + '</p>' + '</div>'); // bind events self.$element.find('.name').click(function () { self.speakname(); }); self.$element.find('.age').click(function () { self.speakage(); }); // render element $('.person-container').append(self.$element);}person.prototype.speakname = function () { alert(this.data.name);};person.prototype.speakage = function () { alert(this.data.age);};ideally, i'd like to be able to build up a relatively complex ui of 'people', with various controls on each person to perform actions related to that person only. apologies for the contrived example, i'm just trying to get the concept right in my head before doing anything more real-world.i've also included a working jsfiddle incase this is easier to work with: <url>",
    "present_kp": [
      "javascript"
    ],
    "absent_kp": [
      "beginner",
      "jquery",
      "object oriented",
      "prototypal class design"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how show attributes which appear in many to many association. as we know a many to many association are shown by two asterisks in both end of association. now i have a association between two entities good and invoice so good and invoice have a many to many association but i want to show the count of each good in each invoice on class diagram. how can i show it?",
    "present_kp": [
      "class diagram"
    ],
    "absent_kp": [
      "uml",
      "modeling"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what to consider before learning a new language for data analysis. i'm currently in the very early stages of preparing a new research-project (still at the funding-application stage), and expect that data-analysis and especially visualisation tools will play a role in this project.in view of this i face the following dilemma: should i learn python to be able to use its extensive scientific libraries (pandas, numpy, scipy, ...), or should i just dive into similar packages of a language i'm already acquainted with (racket, or to a lesser extent scala)?(ideally i would learn python in parallel with using statistical libraries in racket, but i'm not sure i'll have time for both)i'm not looking for an answer to this dilemma, but rather for feedback on my different considerations:my current position is as follows:in favour of python:extensively used librarieswidely used (may be decisive in case of collaboration with others)a lot of online material to start learning itconferences that are specifically dedicated to scientific computing with pythonlearning python won't be a waste of time anywayin favour of a language i already know:it's a way to deepen my knowledge of one language rather than getting superficial knowledge of one more language (under the motto: you should at least know one language really well)it is feasible. both racket and scala have good mathematics and statistics librariesi can start right away with learning what i need to know rather than first having to learn the basicstwo concrete questions:what am i forgetting?how big of a nuisance could the python 2 vs 3 issue be?",
    "present_kp": [
      "python"
    ],
    "absent_kp": [
      "visualization"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "weird escape sequence. i need help identifying what this escape sequences represents. i see this sequence is autogenerating on my server's console, but i'm not sure what is the reason for that.escape sequence:^[[[di've checked this chart of escape sequences as reference: <url> , but haven't found anything matching.",
    "present_kp": [
      "console"
    ],
    "absent_kp": [
      "tty",
      "escape characters"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "the importance of normal forms like chomsky normal form for cfgs. i understand that context-free grammars can be used to represent context-free languages.it might have ambiguities. we also have normal forms like chomsky and greibach normal form. i couldn't understand the need of that. why they are important in the theory of languages? all the textbooks i referred to tell about these normal forms but not telling anything about their importance.",
    "present_kp": [
      "normal forms"
    ],
    "absent_kp": [
      "formal languages",
      "context free",
      "formal grammars"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "unable to mount root fs over nfs. i am attempting to set up the first (of many) raspberry pis running pidora (a fedora remix) to boot from an nfs share. it is my goal to eventually place a dozen or more raspberry pis in the datacenter i use to power several different services for my infrastructure and clients and hopefully replace some of the smaller vps nodes that i am currently using. my configuration in cmdline.txt is: dwc_otg.lpm_enable=0 console=ttyama0,115200 console=tty1 root=/dev/nfs nfsroot=<serverip>:/fake/path,nolock ip=dhcp elevator=deadline rootwaiton the pi, the output i see is:ip-config: got dhcp answer from <router>, my address is <clientip>ip-config: complete:device=eth0, hwaddr=<macaddress>, ipaddr=<clientip>, mask=255.255.255.0, gw=<routerip>host=<clientip>, domain=, nis-domain=(none)bootserver=<routerip>, rootserver=<serverip>, rootpath=nameserver0=<routerip>(it pauses for a bit here)vfs: unable to mount root fs via nfs, trying floppyvfs: cannot open root device nfs or unknown-block(2,0); error -6please append a correct root= boot option; here are the available partitions:.....the export configuration is:/fake/path <clientip>(rw,no_root_squash,insecure,no_subtree_check)on the nfs server (an openvz container), the output i see in the /var/log/messages is:aug 22 23:24:01 vps-4178 rpc.mountd[928]: authenticated mount request from <clientip>:783 for /fake/path (/fake/path)aug 22 23:24:38 vps-4178 rpc.mountd[928]: authenticated mount request from <clientip>:741 for /fake/path (/fake/path)aug 22 23:25:25 vps-4178 rpc.mountd[928]: authenticated mount request from <clientip>:752 for /fake/path (/fake/path)aug 22 23:26:12 vps-4178 rpc.mountd[928]: authenticated mount request from <clientip>:876 for /fake/path (/fake/path)to test, i've made sure i can mount (non-root) from both the pi and another machine and it worked. does anyone have an idea on what could be wrong or how to narrow it down? update:the process has gotten slightly further. the server is still showing the same message, however the client is now saying is not responding, still trying.here is the tcpdump (tcdump -vv 'port not 22') during the process:tcpdump: listening on venet0, link-type linux_sll (linux cooked), capture size 65535 bytes02:27:49.396600 ip (tos 0x0, ttl 50, id 56458, offset 0, flags [df], proto udp (17), length 144) <clienthostname>.<phone> > <serverhostname>.nfs: 116 read fh unknown/01000401d2255200f6209d570c172001aa2f2645000000000000000000000000 4096 bytes @ 51609602:27:49.396694 ip (tos 0x0, ttl 64, id 22318, offset 0, flags [+], proto udp (17), length 1500) <serverhostname>.nfs > <clienthostname>.<phone>: reply ok 1472 read reg 100755 ids 0/0 sz <phone> nlink 1 rdev ffffffff fsid 579d20f6 nodeid 120170c a/m/ctime <phone>.8<phone>8.0<phone>5.20258902:27:49.396700 ip (tos 0x0, ttl 64, id 22318, offset 1480, flags [+], proto udp (17), length 1500) <serverhostname> > <clienthostname>: udp02:27:49.396701 ip (tos 0x0, ttl 64, id 22318, offset 2960, flags [none], proto udp (17), length 1264) <serverhostname> > <clienthostname>: udp02:27:49.396963 ip (tos 0x0, ttl 64, id 34369, offset 0, flags [df], proto udp (17), length 72) <serverhostname>.57067 > <redacted2>.domain: [udp sum ok] 45505+ ptr? <redacted>.in-addr.arpa. (44)02:27:49.400054 ip (tos 0x0, ttl 60, id 0, offset 0, flags [df], proto udp (17), length 121) <redacted2>.domain > <serverhostname>.57067: [udp sum ok] 45505 q: ptr? <redacted>.in-addr.arpa. 1/0/0 <redacted>.in-addr.arpa. ptr <clienthostname>. (93)02:27:49.400289 ip (tos 0x0, ttl 64, id 34372, offset 0, flags [df], proto udp (17), length 73) <serverhostname>.51421 > <redacted2>.domain: [udp sum ok] 15808+ ptr? <redacted3>.in-addr.arpa. (45)02:27:49.401603 ip (tos 0x0, ttl 60, id 0, offset 0, flags [df], proto udp (17), length 115) <redacted2>.domain > <serverhostname>.51421: [udp sum ok] 15808 q: ptr? <redacted3>.in-addr.arpa. 1/0/0 <redacted3>.in-addr.arpa. ptr <redacted2>. (87)02:27:50.496543 ip (tos 0x0, ttl 50, id 56459, offset 0, flags [df], proto udp (17), length 144) <clienthostname>.<phone> > <serverhostname>.nfs: 116 read fh unknown/01000401d2255200f6209d570c172001aa2f2645000000000000000000000000 4096 bytes @ 51609602:27:50.496627 ip (tos 0x0, ttl 64, id 22319, offset 0, flags [+], proto udp (17), length 1500) <serverhostname>.nfs > <clienthostname>.<phone>: reply ok 1472 read reg 100755 ids 0/0 sz <phone> nlink 1 rdev ffffffff fsid 579d20f6 nodeid 120170c a/m/ctime <phone>.8<phone>8.0<phone>5.20258902:27:50.496634 ip (tos 0x0, ttl 64, id 22319, offset 1480, flags [+], proto udp (17), length 1500) <serverhostname> > <clienthostname>: udp02:27:50.496636 ip (tos 0x0, ttl 64, id 22319, offset 2960, flags [none], proto udp (17), length 1264) <serverhostname> > <clienthostname>: udp02:27:52.694985 ip (tos 0x0, ttl 50, id 56460, offset 0, flags [df], proto udp (17), length 144) <clienthostname>.<phone> > <serverhostname>.nfs: 116 read fh unknown/01000401d2255200f6209d570c172001aa2f2645000000000000000000000000 4096 bytes @ 51609602:27:52.695058 ip (tos 0x0, ttl 64, id 22320, offset 0, flags [+], proto udp (17), length 1500) <serverhostname>.nfs > <clienthostname>.<phone>: reply ok 1472 read reg 100755 ids 0/0 sz <phone> nlink 1 rdev ffffffff fsid 579d20f6 nodeid 120170c a/m/ctime <phone>.8<phone>8.0<phone>5.20258902:27:52.695064 ip (tos 0x0, ttl 64, id 22320, offset 1480, flags [+], proto udp (17), length 1500) <serverhostname> > <clienthostname>: udp02:27:52.695066 ip (tos 0x0, ttl 64, id 22320, offset 2960, flags [none], proto udp (17), length 1264) <serverhostname> > <clienthostname>: udp02:27:57.105354 ip (tos 0x0, ttl 50, id 56461, offset 0, flags [df], proto udp (17), length 144) <clienthostname>.<phone> > <serverhostname>.nfs: 116 read fh unknown/01000401d2255200f6209d570c172001aa2f2645000000000000000000000000 4096 bytes @ 51609602:27:57.105451 ip (tos 0x0, ttl 64, id 22321, offset 0, flags [+], proto udp (17), length 1500) <serverhostname>.nfs > <clienthostname>.<phone>: reply ok 1472 read reg 100755 ids 0/0 sz <phone> nlink 1 rdev ffffffff fsid 579d20f6 nodeid 120170c a/m/ctime <phone>.8<phone>8.0<phone>5.20258902:27:57.105456 ip (tos 0x0, ttl 64, id 22321, offset 1480, flags [+], proto udp (17), length 1500) <serverhostname> > <clienthostname>: udp02:27:57.105458 ip (tos 0x0, ttl 64, id 22321, offset 2960, flags [none], proto udp (17), length 1264) <serverhostname> > <clienthostname>: udp02:28:05.914058 ip (tos 0x0, ttl 50, id 56462, offset 0, flags [df], proto udp (17), length 144) <clienthostname>.<phone> > <serverhostname>.nfs: 116 read fh unknown/01000401d2255200f6209d570c172001aa2f2645000000000000000000000000 4096 bytes @ 51609602:28:05.914130 ip (tos 0x0, ttl 64, id 22322, offset 0, flags [+], proto udp (17), length 1500) <serverhostname>.nfs > <clienthostname>.<phone>: reply ok 1472 read reg 100755 ids 0/0 sz <phone> nlink 1 rdev ffffffff fsid 579d20f6 nodeid 120170c a/m/ctime <phone>.8<phone>8.0<phone>5.20258902:28:05.914137 ip (tos 0x0, ttl 64, id 22322, offset 1480, flags [+], proto udp (17), length 1500) <serverhostname> > <clienthostname>: udp02:28:05.914138 ip (tos 0x0, ttl 64, id 22322, offset 2960, flags [none], proto udp (17), length 1264) <serverhostname> > <clienthostname>: udp02:28:07.014579 ip (tos 0x0, ttl 50, id 56463, offset 0, flags [df], proto udp (17), length 144) <clienthostname>.<phone> > <serverhostname>.nfs: 116 read fh unknown/01000401d2255200f6209d570c172001aa2f2645000000000000000000000000 4096 bytes @ 51609602:28:07.014665 ip (tos 0x0, ttl 64, id 22323, offset 0, flags [+], proto udp (17), length 1500) <serverhostname>.nfs > <clienthostname>.<phone>: reply ok 1472 read reg 100755 ids 0/0 sz <phone> nlink 1 rdev ffffffff fsid 579d20f6 nodeid 120170c a/m/ctime <phone>.8<phone>8.0<phone>5.20258902:28:07.014672 ip (tos 0x0, ttl 64, id 22323, offset 1480, flags [+], proto udp (17), length 1500) <serverhostname> > <clienthostname>: udp02:28:07.014674 ip (tos 0x0, ttl 64, id 22323, offset 2960, flags [none], proto udp (17), length 1264) <serverhostname> > <clienthostname>: udp02:28:09.216009 ip (tos 0x0, ttl 50, id 56464, offset 0, flags [df], proto udp (17), length 144) <clienthostname>.<phone> > <serverhostname>.nfs: 116 read fh unknown/01000401d2255200f6209d570c172001aa2f2645000000000000000000000000 4096 bytes @ 51609602:28:09.216102 ip (tos 0x0, ttl 64, id 22324, offset 0, flags [+], proto udp (17), length 1500) <serverhostname>.nfs > <clienthostname>.<phone>: reply ok 1472 read reg 100755 ids 0/0 sz <phone> nlink 1 rdev ffffffff fsid 579d20f6 nodeid 120170c a/m/ctime <phone>.8<phone>8.0<phone>5.20258902:28:09.216107 ip (tos 0x0, ttl 64, id 22324, offset 1480, flags [+], proto udp (17), length 1500) <serverhostname> > <clienthostname>: udp02:28:09.216109 ip (tos 0x0, ttl 64, id 22324, offset 2960, flags [none], proto udp (17), length 1264) <serverhostname> > <clienthostname>: udp",
    "present_kp": [
      "linux",
      "mount",
      "nfs"
    ],
    "absent_kp": [
      "startup",
      "root filesystem"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "building a uefi-secure boot debian-usb. i assume that there already is something where exactly my problem was discussed years ago, but after spending too much time searching i try my luck here, i hope you can help me or atleast show me the right links to get my information from.target: debian with secure boot on usbsome links i base my progress onrodsbooks efi-bootloaders installationrodsbooks efi-bootloaders secureboothttps://wiki.ubuntu.com/securityteam/securebootsteps so far:# download debian8.4-netinstall (official link)wget <url> debnet # extract isobsdtar -c debnet -xf debian-8.4.0-amd64-netinst.isols debnet>autorun.inf doc install pool readme.txt>boot efi install.amd readme.html setup.exe>css firmware isolinux readme.mirrors.html tools>debian g2ldr md5sum.txt readme.mirrors.txt win32-loader.ini>dists g2ldr.mbr pics readme.sourceas described by rodsbooks,i should now rename efi/boot/bootx64.efi to efi/boot/grubx64.efimove shim.efi to efi/boot/bootx64.efimove mokmanager.efi to efi/boot/make some nice magic with keys and signing, tadaa, you're done.to make an bootable usb, i take the following stepssources=debnetoutiso=debian8.4.0.modified.isorelative_isolinuxbin=isolinux/isolinux.binxorriso -as mkisofs \\ -isohybrid-mbr /usr/lib/syslinux/isohdpfx.bin \\ -c boot.cat \\ -b $relative_isolinuxbin \\ -no-emul-boot -boot-load-size 4 -boot-info-table -eltorito-alt-boot \\ -e boot/grub/efi.img -no-emul-boot -isohybrid-gpt-basdat \\ -o $outiso $sourcessudo dd if=debian8.4.0.modified.iso of=/dev/sdbif i had any idea what i was doing before (just getting uefi to run), i would have seen that i use efi.img. so it took some time experimenting with debnet/efi/boot to find that adding or changing of even deleting this directory does not change booting at all. (maybe deleting the directory corrupts the installation - didnt try - but not the booting)so, i finally understood i have to change debnet/boot/grub/efi.img. so here's my main question so far - how do i create this efi.img for my needs?dd if=/dev/zero of=efi.img bs=1k count=3000mkfs.vfat efi.imgsudo mount efi.img img_mountpoint/and copying the bootloaders as described in rodsbooks does not seem to work.if have information for me would be quite nice.",
    "present_kp": [
      "debian",
      "uefi",
      "secure boot"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "can't delete file on mtp device. using libmtp and mtp-tools to copy files from an mtp device(camera) over usb.i can copy files but when i try and delete using the following commandsudo mtp-delfile -n <file id>i get the following errorfailed to delete file:3 error 2: ptp layer error 200e: libmtp_delete_object(): could not delete object. error 2: error 200e: ptp: store read only",
    "present_kp": [
      "usb",
      "mtp",
      "delete"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "psychology behind repeated viewing of certain pictures and songs. often it happens that we like to frequently listen to a particular tune or song or view some pictures or images repeatedly time after time. what are the reasons and psychology behind this?",
    "present_kp": [],
    "absent_kp": [
      "cognitive psychology",
      "perception",
      "music",
      "aesthetics"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to set what field names are displayed in listings?. when i execute a command in ubuntu, which results in a listing, i get results without the field names. example is ls -l or ps l. i am not very experienced and always need to go digging through man pages and online documentation. and the names are quite crypcit already.is there a way to turn on field name listing globally i.e. for all commands?note: actually ps l shows field names, while ls -l does not. it is true that the second is very trivial. however, the question stands - is there a way to overwrite this behaviour?",
    "present_kp": [
      "ls",
      "ps"
    ],
    "absent_kp": [
      "command line",
      "configuration"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is it possible to get a paypal sandbox account without creating a paypal account?. recently paypal changed their sandbox account section. now it asks for a real paypal account to get into the sandbox account. as i don't have a paypal account, is there any way to get back into my sandbox account without creating a paypal account? or are there any demo accounts available?",
    "present_kp": [
      "paypal"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to install a src .rpm. i downloaded a source rmp src.rpm file of audacity when i run [root@09pc148b downloads]# rpmbuild --rebuild audacity-2.1.0-2.fc23.src.rpmthis is the response i am getting flac-devel is needed by audacity-2.1.0-2.el6.i686 jack-audio-connection-kit-devel is needed by audacity-2.1.0-2.el6.i686 ladspa-devel is needed by audacity-2.1.0-2.el6.i686 libid3tag-devel is needed by audacity-2.1.0-2.el6.i686 taglib-devel is needed by audacity-2.1.0-2.el6.i686 libogg-devel is needed by audacity-2.1.0-2.el6.i686 libsndfile-devel is needed by audacity-2.1.0-2.el6.i686 libvorbis-devel is needed by audacity-2.1.0-2.el6.i686 portaudio-devel >= 19-16 is needed by audacity-2.1.0-2.el6.i686 soundtouch-devel is needed by audacity-2.1.0-2.el6.i686 soxr-devel is needed by audacity-2.1.0-2.el6.i686 vamp-plugin-sdk-devel >= 2.0 is needed by audacity-2.1.0-2.el6.i686 wxgtk-devel is needed by audacity-2.1.0-2.el6.i686 libappstream-glib is needed by audacity-2.1.0-2.el6.i686when i run this command [root@09pc148b downloads]# rpm -ivv audacity-2.1.0-2.fc23.src.rpmmy resonse is d: ============== audacity-2.1.0-2.fc23.src.rpmd: loading keyring from pubkeys in /var/lib/rpm/pubkeys/*.keyd: couldn't find any keys in /var/lib/rpm/pubkeys/*.keyd: loading keyring from rpmdbd: opening db environment /var/lib/rpm cdb:mpool:joinenvd: opening db index /var/lib/rpm/packages rdonly mode=0x0d: locked db index /var/lib/rpm/packagesd: opening db index /var/lib/rpm/name rdonly mode=0x0d: read h# 1438 header sanity check: okd: added key gpg-pubkey-6b8d79e6-3f49313d to keyringd: using legacy gpg-pubkey(s) from rpmdbd: expected size: 24389466 = lead(96)+sigs(4292)+pad(4)+data(24385074)d: actual size: 24389466d: audacity-2.1.0-2.fc23.src.rpm: header sha1 digest: ok (6b5705fc00764be7bc14578e1976d33d86ac2a3d)d: added source package [0]d: found 1 source and 0 binary packagesd: expected size: 24389466 = lead(96)+sigs(4292)+pad(4)+data(24385074)d: actual size: 24389466d: installsourcepackage at: psm.c:244: header sha1 digest: ok (6b5705fc00764be7bc14578e1976d33d86ac2a3d)audacity-2.1.0-2.fc23d: ========== directories not explicitly included in package:d: 0 /root/rpmbuild/sources/d: 1 /root/rpmbuild/specs/d: ==========d: fini 100664 1 ( 501, 501) <phone> /root/rpmbuild/sources/audacity-manual-2.1.0.zip;55486cb6 unknownd: fini 100664 1 ( 501, 501) <phone> /root/rpmbuild/sources/audacity-minsrc-2.1.0.tar.xz;55486cb6 unknownd: fini 100644 1 ( 501, 501) 23062 /root/rpmbuild/specs/audacity.spec;55486cb6 unknowngzdio: 2994 reads, 24518844 total bytes in 0.074680 secsd: closed db index /var/lib/rpm/named: closed db index /var/lib/rpm/packagesd: closed db environment /var/lib/rpmwhat is the error how to fix this regardsagxin.j",
    "present_kp": [],
    "absent_kp": [
      "software installation"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "information about links disappeared from webmaster tools. i discovered that all information about links to my site disappeared from google webmaster tools. last time i checked the links to your site page in gwt there was nice list of linking domains and all. but now there is only no data available. there were no changes to the site contents.why could it be? and what can i do to fix this?about a month earlier i found that pr of all my pages dropped by 2 points. may these changes be related?",
    "present_kp": [],
    "absent_kp": [
      "pagerank",
      "google search console",
      "backlinks"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "importdata google sheet csv on google drive. so i'm starting to really get into utilizing google drive to streamline my many personal projects. one of the things i'd love to do is figure out how to download csv files onto my google drive, and have google sheets automatically do an =importdata on any new .csv files in a specific directory.i've come across a number of problems trying to figure this out, but the biggest issue is that putting any kind of file on the google drive doesn't give you a link to the file itself, but rather something like such:<url> doesn't recognize this as a .csv file. how can i go about doing this?",
    "present_kp": [
      "google drive",
      "csv"
    ],
    "absent_kp": [
      "google spreadsheets"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "need to run su command if i am a user with sudo access. i am a user in oracle linux server with sudo access. if i run su command, system prompts for password. when i input my account password, system gives message incorrect password while i login successfully with the same password. why is it happening? need to execute su or sudo command after login if i have sudo access? need to prefix sudo with every command?",
    "present_kp": [
      "sudo",
      "oracle linux"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "if/then/else man page. i would like to know if there is any man page documenting the construction of the most basic script commands like if/then/else, while, for each, and all the relative switches, like -eq, -e, -ge, and so on.",
    "present_kp": [
      "man"
    ],
    "absent_kp": [
      "shell",
      "scripting",
      "documentation"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "where are networkmanager's wifi settings stored?. i remember the days of playing with /etc/$we/wpa_supplicant.conf to try and force a semi-secure network on a debian based system, but now on to xubuntu, i haven't needed to do any *.conf-ing in a while (in terms of networking)as such, i'm interested to know; how do the nm gui's store network information? can this be backed up or exported as a wpa_supplicant.conf file?",
    "present_kp": [
      "ubuntu",
      "networking",
      "wifi",
      "networkmanager",
      "xubuntu"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how do i recover files from a single degraded mdadm raid1 drive? not enough to start the array. given a single raid1 drive in degraded/rebuilding state, can it be force mounted? i'd like to recover all the files before undertaking the dangerous operation of pairing it and rebuilding. as far as i can tell the drive is in perfectly good shape, fully intact. the pair drive is partly failed.if the drive was not in rebuilding state i'd know exactly what to do. here is what i have tried:# mdadm --verbose --assemble /dev/md8 /dev/sdb1 --forcemdadm: looking for devices for /dev/md8mdadm: /dev/sdb1 is identified as a member of /dev/md8, slot 1.mdadm: no uptodate device for slot 0 of /dev/md8mdadm: added /dev/sdb1 to /dev/md8 as 1mdadm: /dev/md8 assembled from 0 drives and 1 rebuilding - not enough to start the array.# cat /proc/mdstat md8 : inactive sdb1[1](s) 976759808 blocks super 1.2 md0 : active raid1 sdc1[0] 976759672 blocks super 1.2 [2/1] [u_]# mdadm --stop /dev/md8mdadm: stopped /dev/md8# mount /dev/sdb1 /mnt/temp2mount: unknown filesystem type 'linux_raid_member'# mount -o ro -t ext3 -b 2048 /dev/sdb1 /mnt/temp1mount: wrong fs type, bad option, bad superblock on /dev/sdb1.# foremost -i /dev/sdb -o /tmp/foo (this results in perfectly good files)in this particular case the foremost command recovers files, so something is definitely on the drive, if i could only get the superblock offset correct.and in this particular case assembling both halves of the array crashes the kernel(!), so that's not a real option anyway (aside from the safety issues).update: added output of mdadm# mdadm --examine /dev/sdb1/dev/sdb1: magic : a92b4efc version : 1.2 feature map : 0x2 array uuid : e00a291e:016bbe47:09526c90:3be48df3 name : ubuntu:0 creation time : wed may 11 12:26:39 2011 raid level : raid1 raid devices : 2 avail dev size : <phone> (931.51 gib 1000.20 gb) array size : <phone> (931.51 gib 1000.20 gb) used dev size : <phone> (931.51 gib 1000.20 gb) data offset : 2048 sectors super offset : 8 sectorsrecovery offset : 0 sectors state : clean device uuid : 41346f44:ccacbbf7:0c17c133:eb7b341f update time : sat apr 13 00:02:08 2013 checksum : 483a0a44 - correct events : 402833 device role : active device 1 array state : aa ('a' == active, '.' == missing)",
    "present_kp": [
      "mdadm"
    ],
    "absent_kp": [
      "data recovery",
      "software raid"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "algorithm for finding similar images. if you go to ffffound! and click on some image you will notice that on the new page, under the image, there is a section called you may like these images. which suggests 10 images that look similar to the original.what would be a good algorithm to achieve this functionality for a collection of images?any documentation, books, etc. related to such algorithms is very appreciated. also algorithms for finding similar images that yield better results than those seen on ffffound! website are also welcome.",
    "present_kp": [],
    "absent_kp": [
      "ds.algorithms",
      "reference request"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "object-oriented brainfuck interpreter. inspired by this question, i decided to give it a try and implemented a brainfuck interpreter myself. it includes various improvements:it's object-oriented and modularunlimited tape sizeit includes a call stack (no seeking for opening parenthesis)it allows stepping and debuggingit throws exceptions on errorstape/// <summary>/// one-sided, infinite, default-initialized tape. </summary>public class tape<t>{ list<t> _tape; int _pos; public tape() { _tape = new list<t>(); _tape.add(default(t)); _pos = 0; } /// <summary> /// currently selected cell's value. </summary> public t value { get { return _tape[_pos]; } set { _tape[_pos] = value; } } /// <summary> /// step one cell to the left. </summary> public void stepleft() { if (_pos == 0) throw new invalidoperationexception(); else _pos--; } /// <summary> /// step one cell to the right. </summary> public void stepright() { _pos++; if (_pos == _tape.count) _tape.add(default(t)); } /// <summary> /// return a full snapshot of the tape without altering anything. </summary> public t[] toarray() { return _tape.toarray(); }}interpreterpublic class brainfuckinterpreter{ stream _program; stream _input; stream _output; tape<byte> _tape; stack<long> _callstack; /// <summary> /// create a new brainfuckinterpreter. </summary> /// <param name=program> /// program to be executed. must be readable and seekable. /// <param name=input> /// must be readable. can be null if the program doesn't take any input. </param> /// <param name=output> /// must be writable. can be null if the program doesn't produce output. </param> public brainfuckinterpreter(stream program, stream input, stream output) { _program = program; _input = input; _output = output; _tape = new tape<byte>(); _callstack = new stack<long>(); } /// <summary> /// run the program until it terminates. </summary> public void run() { while (step()); } /// <summary> /// execute the next command. </summary> /// <returns> /// false if the program terminated. true otherwise. </returns> public bool step() { int command = _program.readbyte(); switch (command) { default: throw new argumentexception(); // invalid command case -1: return false; // end reached case '+': _tape.value++; break; case '-': _tape.value--; break; case ',': int input = _input.readbyte(); if (input == -1) // null-termination instead of error status _tape.value = 0; else _tape.value = (byte)input; break; case '.': _output.writebyte(_tape.value); break; case '>': _tape.stepright(); break; case '<': _tape.stepleft(); break; case '[': _callstack.push(_program.position); break; case ']': if (_tape.value == 0) _callstack.pop(); else _program.position = _callstack.peek(); break; } return true; } /// <summary> /// print the tape for debug purposes. /// format (hex): |xx|xx|xx|...|xx| </summary> public string print() { string result = |; foreach (var n in _tape.toarray()) { result += string.format({0:x2}|, n); } return result; }}main routine (for testing)takes the program directly (not a filename) as first command line argument. prints the tape's contents to stderr after each step.public static class program{ public static void main(string[] args) { stream program = new memorystream(args[0].select(c => (byte)c).toarray()); var bf = new brainfuckinterpreter( program, console.openstandardinput(), console.openstandardoutput() ); while (bf.step()) { console.error.writeline(bf.print()); } }}",
    "present_kp": [
      "interpreter",
      "brainfuck"
    ],
    "absent_kp": [
      "c#"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "my work profile changed. my work profile changed to being formerly employed as an employee health nurse at metro health hospital. i never changed this. could facebook have changed their format and just added this?it usually reads worked at metro health hospital.",
    "present_kp": [
      "facebook"
    ],
    "absent_kp": [
      "privacy"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to decide maintenance cost/terms for freelance work?. i am a javascript programmer and am planning to take freelance projects related to javascript effects for the web. recently a potential client approached me to create a javascript effect for his website. the effect was quite complex and it was not generic. that is, every time the design of the website would change the javascript code would have to be modified.due to this nature of the code, i told the client that i will not be able to provide maintenance for this project. i will be happy to modify the code in the future but i will be doing the cost estimation each time there is a modification. the client was not happy with this. i thought of calculating the probability of the modifications the client may ask per year and multiply the initial estimate by that so that the maintenance becomes free. for example, if the cost of the initial project is $100 and i assume that the client will ask me to modify the code 4 times a year i would charge him $ (100 + 4*100). but i think this would be way too much and the client would deny to pay such a high amount.i have just started freelancing and the cost estimation part related to maintenance confuses me a lot. what is a good way to estimate maintenance cost. also suggestions on maintenance terms like what should be the scope would be useful.",
    "present_kp": [
      "freelancing",
      "maintenance"
    ],
    "absent_kp": [
      "pricing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "using large amounts of if-else statements for playing card numbers. i have a method that uses a bunch of if-else statements, and i am thinking how i could simplify it.public static cardnumber decode(string s) { if(s == null) { return null; } else if(s.equalsignorecase(ace.tostring())) { return ace; } else if(s.equalsignorecase(two.tostring())) { return two; } else if(s.equalsignorecase(three.tostring())) { return three; } else if(s.equalsignorecase(four.tostring())) { return four; } else if(s.equalsignorecase(five.tostring())) { return five; } else if(s.equalsignorecase(six.tostring())) { return six; } else if(s.equalsignorecase(seven.tostring())) { return seven; } else if(s.equalsignorecase(eight.tostring())) { return eight; } else if(s.equalsignorecase(nine.tostring())) { return nine; } else if(s.equalsignorecase(ten.tostring())) { return ten; } else if(s.equalsignorecase(jack.tostring())) { return jack; } else if(s.equalsignorecase(queen.tostring())) { return queen; } else if(s.equalsignorecase(king.tostring())) { return king; } else { return null; }}the whole class is here:public class cardnumber { private final string name; private final int value; public static final cardnumber ace_as_one = new cardnumber(a, 1); public static final cardnumber two = new cardnumber(2, 2); public static final cardnumber three = new cardnumber(3, 3); public static final cardnumber four = new cardnumber(4, 4); public static final cardnumber five = new cardnumber(5, 5); public static final cardnumber six = new cardnumber(6, 6); public static final cardnumber seven = new cardnumber(7, 7); public static final cardnumber eight = new cardnumber(8, 8); public static final cardnumber nine = new cardnumber(9, 9); public static final cardnumber ten = new cardnumber(10, 10); public static final cardnumber jack = new cardnumber(j, 11); public static final cardnumber queen = new cardnumber(q, 12); public static final cardnumber king = new cardnumber(k, 13); public static final cardnumber ace = new cardnumber(a, 14); private cardnumber(string name, int value) { this.name = name; this.value = value; } @override public string tostring() { return name; } public static cardnumber decode(string s) { if(s == null) { return null; } else if(s.equalsignorecase(ace.tostring())) { return ace; } else if(s.equalsignorecase(two.tostring())) { return two; } else if(s.equalsignorecase(three.tostring())) { return three; } else if(s.equalsignorecase(four.tostring())) { return four; } else if(s.equalsignorecase(five.tostring())) { return five; } else if(s.equalsignorecase(six.tostring())) { return six; } else if(s.equalsignorecase(seven.tostring())) { return seven; } else if(s.equalsignorecase(eight.tostring())) { return eight; } else if(s.equalsignorecase(nine.tostring())) { return nine; } else if(s.equalsignorecase(ten.tostring())) { return ten; } else if(s.equalsignorecase(jack.tostring())) { return jack; } else if(s.equalsignorecase(queen.tostring())) { return queen; } else if(s.equalsignorecase(king.tostring())) { return king; } else { return null; } } public int getvalue() { return value; }}",
    "present_kp": [],
    "absent_kp": [
      "java",
      "strings",
      "playing cards"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "extracting data from a word document is too slow. i have a 100page long .docx format document. i'm using a macro written in vbs to extract some information and then just generate a table from them. i iterate through the paragraphs and store the found strings in 3 separate arrays.however, the loop is unreasonably slow. it takes 3 min to complete on a relatively fast computer. can you take a look at it and tell me what causes this slowness?'todo : add checks, exception handling, dynamic user options, probes, style checks, and fix slowness.sub gentable()dim objdoc'''''''''''''''''''''modify these''''''''''''''''dim columnname1, columnname2, columnname3, magicstringcolumnname1 = foo1columnname2 = foo2columnname3 = foo3magicstring = asd321: ' we search for this stringset objdoc = activedocument ' because we run inside of word as macro''''''''''''''''''''''''''''''''''''''''''''''''''const max = 200 ' using fixed sized arrays, mod this if you'll have more than 200 entries.dim vulnerabilityarr(max) ' initializing the arraysdim severityarr(max)dim paragrapharr(max)dim counter ' will count the processed entries in thiscounter = 0dim currparagraph ' will be set in loopdim tmparray() as string ' for splitting for pindex = 1 to objdoc.paragraphs.count ' this loop is slow currparagraph = objdoc.paragraphs(pindex) currparagraph = left(currparagraph, len(currparagraph) - 1) 'remove junk character if instr(1, currparagraph, magicstring) > 0 then ' assuming this string is always present and the other two target data is near it tmparray = split(currparagraph) ' extract level currparagraph = objdoc.paragraphs(pindex - 1) 'assuming the previous paragraph is the vuln. name 'storing the 3 extracted data vulnerabilityarr(counter) = currparagraph severityarr(counter) = tmparray(1) paragrapharr(counter) = objdoc.paragraphs(pindex - 1).range.listformat.liststring ' for some weird reason i cant use currparagraph here counter = counter + 1 ' adjusting index end if next pindexobjdoc.tables.add objdoc.paragraphs(objdoc.paragraphs.count).range, counter, 3, true, trueset objtable = objdoc.tables(objdoc.tables.count) 'select last tableobjtable.cell(1, 1).range.text = columnname1objtable.cell(1, 2).range.text = columnname2objtable.cell(1, 3).range.text = columnname3for rowindex = 0 to counter objtable.cell(rowindex, 1).range.text = paragrapharr(rowindex) objtable.cell(rowindex, 2).range.text = vulnerabilityarr(rowindex) objtable.cell(rowindex, 3).range.text = severityarr(rowindex) next rowindexy = rowindex + 1objtable.autoformat (25)end sub",
    "present_kp": [],
    "absent_kp": [
      "performance",
      "vba",
      "vbscript",
      "ms word"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "relation between lattice and boolean algebra. in discrete math, i have read that lattice is a generalized form of boolean lattice. but those places where boolean algebra is mentioned, they don't tell about lattices (digital logic, binary,...). whether the meet and join is same as and and or in boolean logic? if we are thinking in terms of lattices how you define 1 and 0 = 0? we consider only a two element lattice?",
    "present_kp": [
      "boolean algebra",
      "lattices"
    ],
    "absent_kp": [
      "discrete mathematics"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to efficiently determine when changes occur in event lists. we have a system that stores event information.there is a primary list of events as well as a secondary list, these may overlap.secondary lists are combined with the primary based on 3 main rules.default - only displayed when no events are currentevents - only displayed when primary events are currentoverride - displayed in stead of eventsa new 3rd party system we have to integrate with means we need to get each of the different states throughout the days of what events/secondary events should be showing.eg, from 9-5 these events are current based on the timings/ rules, but then after 5 these events are current based on the timings and rules.a simple exampleprimary eventsevent alpha | 10:30 - 14:30event bravo | 14:00 - 16:00event charlie | 18:30 - 20:00secondary eventsevent zulu | 00:00 - 23:59 | defaultevent yankee | 09:00 - 11:00 | defaultevent xray | 09:00 - 11:30 | eventsevent whiskey | 19:00 - 19:15 | overridejust on this very small example this would result as the following lists that we would need to generate00:00-09:00event zulu09:00-10:30event zuluevent yankee10:30-11:30event alphaevent xray11:30-14:00event alpha14:00-14:30event alphaevent bravo14:30-16:00event bravo16:00-18:30event zulu18:30-19:00event charlie19:00-19:15event whiskey19:15-20:00event charlie20:00-23:59event zuluthis is a tiny example in production there is usually hundreds to thousands of events over many days. at present the only end points deal with this problem dynamically on the fly, this third party system requires we provide the data in this way.it's all written in php, and we have the primary and secondary event lists in an array each.what is the most efficient way of making these lists ?we have considered doing a minute by minute check, but this seems horribly inefficient and i'm hoping there is a better way.",
    "present_kp": [
      "php"
    ],
    "absent_kp": [
      "sorting"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "c# - design to parse and write csv and manipulating data. i've designed the below code for one of the requirement. i have to write different tests for the code and i would need feedback on where i can improve the design. requirement: read the customer details from csv, calculate his average expenses on different items on yearly and write to csv again. read the customer details from csv, calculate his total expenses on faimly and write to csv again.as both the expensecalculator uses csvreaderwriter function, is it better to create the abstract class which implements csvreaderwriter functionality and derive from abstract class or is it better to use interface? public class customer{ public string firstname { get; set; } public string lastname { get; set; } public int expenses { get; set; } // other details}public interface icsvreaderwriter<t>{ ienumerable<t> parsecsv(string filepath); void writecsv(string filepath, ienumerable<customer> customers);}public class readerwriter : icsvreaderwriter<customer>{ public ienumerable<customer> parsecsv(string filepath) { throw new notimplementedexception(); } public void writecsv(string filepath, ienumerable<customer> customers ) { throw new notimplementedexception(); }}public interface iexpensecalculator{ void calculateexpenses(string filepath);}public class itemexpensecalculator : iexpensecalculator{ private readonly icsvreaderwriter<customer> _csvreaderwriter; public itemexpensecalculator(icsvreaderwriter<customer> csvreaderwriter) { this._csvreaderwriter = csvreaderwriter; } public void calculateexpenses(string filepath) { var customers = _csvreaderwriter.parsecsv(filepath); // do manipulation _csvreaderwriter.writecsv(filepath, customers); }}public class familyexpensecalculator: iexpensecalculator{ private readonly icsvreaderwriter<customer> _csvreaderwriter; public familyexpensecalculator(icsvreaderwriter<customer> csvreaderwriter) { this._csvreaderwriter = csvreaderwriter; } public void calculateexpenses(string filepath) { var customers = _csvreaderwriter.parsecsv(filepath); // do manipulation _csvreaderwriter.writecsv(filepath, customers); }}public class program{ private void main() { iexpensecalculator itemexpensecalculator = new itemexpensecalculator(new readerwriter()); itemexpensecalculator.calculateexpenses(@d:\\); iexpensecalculator familyexpensecalculator = new familyexpensecalculator(new readerwriter()); familyexpensecalculator.calculateexpenses(@d:\\); }}",
    "present_kp": [
      "c#"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "set cpu affinity for the specific program?. how can i set cpu affinity for the specific program (say gzip) to always run on specific core or cores (core 1, for example)?i read about taskset, but can it be used before program is actually used and creates a process?",
    "present_kp": [
      "cpu"
    ],
    "absent_kp": [
      "cpu frequency",
      "cpu usage"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "maximize mst(g[s]) over all induced subgraphs g[s] in a metric graph. has this problem been studied before?given a metric undirected graph g (edge lengths satisfy triangle inequality), find a set s of vertices such that mst(g[s]) is maximized, where mst(g[s]) is the minimum spanning tree of the subgraph induced by s. has this problem been studied before? is it np-hard? thanks a lot.",
    "present_kp": [],
    "absent_kp": [
      "ds.algorithms",
      "graph algorithms",
      "np hardness",
      "approximation algorithms"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to upload to a unix server?. im a part of a project group, and one of our tasks is to upload some content to a server. i am completely new to this and have some questions.there are content on different servers(distributed on several servers). my task is to get all the content, and upload them on a single server, and make them available for others, e.g everybody that visits that server(by website) should be able to download the content.how does one do that in a unix server? and how do i make them available for others?i have no experience with servers, but i have experience with java(also java ee). what do i need to learn to be able to do these tasks?ill be getting an account for the server soon, which will give me access to it.please, if the question is in wrong place, or it violates any rules, let me know.thanks.",
    "present_kp": [],
    "absent_kp": [
      "linux",
      "files",
      "webserver",
      "file server"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "using induction to prove that $n^3log^4n=o(n^4)$. i need to prove the following asymptotic relation for the purpose of cacluating a recurrence relation:$$n^3log^4n=o(n^4)$$i tried and failed to do it with induction, which, if possible using basic calculus 1 level math, i would like you to help me with.i am not required to do it with induciton or anything, i just wondered if i can do it.i was able to prove it though, in a differet manner:$$f \\in o(g) \\rightarrow f \\in o(g)$$$$lim_{n o \\infty} rac{n^3log^4n}{n^4}=0$$using l'hpital's rule 4 times, which proves that:$$n^3log^4n=o(n^4)$$by the definitino of $o$.therefore it also follows that $$n^3log^4n=o(n^4)$$for the basis of the induction let $n=1$ and with $c=1$:$$f(1) = 1^3log^4(1) = 0 \\leq 1=1^4 \\cdot 1=c \\cdot g(1)$$from here i think that it would be enough to show that:$$log^4(n) \\leq n$$although i am not sure how to continue from here.",
    "present_kp": [
      "recurrence relation",
      "induction"
    ],
    "absent_kp": [
      "asymptotics"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "determining length of a walk in nondeterministic finite automata with lambda transitions. i am learning about cs theory and specifically nondeterministic finite automata (nfa) right now. in my book i came across a section of text that discussed a way to determine the length of a walk stating specifically that :if a transition graph has a walk labeled $w$, then there is a walk w of length no more than $\\lambda + (1 + \\lambda)|w|$ where $\\lambda$ is the number of $\\lambda$ transitions in the graph.the book does not define $|w|$ which is causing part of my confusion. i'm assuming $|w|$ is the length of the label of the walk. so in this case it would be 1 when going from $q_1 o q_0 o q1$ because $a$ is the only labeled edge.i am trying to understand this concept and how it works because when i have tested out this claim it has not held true.here is the test i did with this nfa$q_1$ is the final state. so assuming you were trying to find the length of the walk $a$, the label would indicate that the length is 1 (e.g. $\\delta^* (q_1, a)$) . however due to the lambdas you actually have $\\lambda \\lambda a$ to go from $q_1 o q_0 o q_1$.this theorem doesn't hold with my math though because it is defined as + (1 + )|w| where is the number of -edges in the graph.since there are two -edges (and it doesn't state whether it means -edges in the walk itself or in the graph in total...) this would then be 2 + (1 + 2)|w|. so thats 2 + 3|w|. this clearly is more than 3, which is the length of q1 -> q1 of a.what am i missing here? any help is greatly appreciated.this comes from peter linz an introduction to formal languages and automata 5th edition.some more information about the argument for this claim:while -edges may be repeated, there is always a walk in which every repeated -edge is separated by an edge labeled with a nonempty symbol. otherwise, the walk contains a cycle labeled , which can be replaced by a simple path without changing the label of the walk.also the book never names this as a theorem or lemma or anything of the sort so it has been very difficult to find online resources about this topic.",
    "present_kp": [
      "finite automata"
    ],
    "absent_kp": [
      "graph theory",
      "nondeterminism"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "node.js dependencies weigh too much. recently i started playing with node.js.now, every node tutorial out there states that you should start withnpm initand then, say you want some standard server framework, say you choose express:npm install expressbut then you'll want many more things you are used to from worlds like asp.net.i talk about template engines (jade) and stylesheet pre-processors (sass).and then they tell you install gulp/grunt! so you can minify and uglify and run the server and so many other things automatically!and that means installing gulp, node-sass, and gulp-sass, and gulp-uglify, and maybe some more really cool stuff (tsd or babel, markdown etc)...but all of those are heavy on your disk and project. don't look for a moment and you can easily find yourself with 100mb+ disk size for that project (which hasn't even started yet!) not to mention 10000+ files since every node module brings its own dependencies, no matter that the same dependency is used by another module. and this is a very hard thing to move anywhere, let alone a web server.am i missing something? i don't think it's possible that so much praise is given to the node environment while such a clear flaw exists. do i expect too much (after all i did try to use many tools at once), is there something trivial known to node veterans to bypass this?",
    "present_kp": [
      "node.js"
    ],
    "absent_kp": [
      "dependency management"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "share google drive folder with submitters of google form. i have a google form that is used as a sign up form. this form has a question requesting emails. it uses the email validation. i have a google drive folder that i want everyone who signs up to be able to view. some of these files, i want to be able to easily set it up so they can edit it. how can i do this?",
    "present_kp": [
      "google drive"
    ],
    "absent_kp": [
      "google spreadsheets",
      "google forms"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is the way to understand someone else's giant uncommented spaghetti code?. possible duplicate:ive inherited 200k lines of spaghetti code what now? i have been recently handled a giant multithreaded program with no comments and have been asked to understand what it does, and then to improve it (if possible). are there some techniques which should be followed when we need to understand someone else's code? or do we straightaway start from the first function call and go on tracking next function calls?c++ (with multi-threading) on linux",
    "present_kp": [
      "c++",
      "linux"
    ],
    "absent_kp": [
      "productivity",
      "code reviews",
      "maintenance"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "does gzip add integrity/crc check to a .tar?. i run commands:tar -cf myarchive.tar mydirectory/gzip myarchive.tarthen i copy the file over a lot of unreliable mediums, and later i unpack it using:tar -xzf myarchive.tar.gzthe fact that i compressed the tar-ball, will that in any way guarantee the integrity, or at least a crc of the unpacked content?",
    "present_kp": [
      "tar",
      "gzip",
      "integrity"
    ],
    "absent_kp": [
      "checksum"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what does 01:00.0 mean with respect to graphics?. when i typed $ lspci -nn | grep vga01:00.0 vga compatible controller [0300]: nvidia corporation g80 [geforce 8800 gts] [10de:0193] (rev a2)i was reminded of this strange string, 01:00.0, which i have seen occasionaly without ever knowing what it wants to tell me. especially the 00.0 part of it. how would you explain to a layman what this string mean?",
    "present_kp": [
      "graphics"
    ],
    "absent_kp": [
      "monitors"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "design pattern for removing default listeners. we have a standard gui control with a bunch of default listeners like that:class ourcontrol extends control { ourcontrol() { addmousewheellistener(new defaultmousewheellistener()); }}the defaults are good for the standard use-cases, but now the customer wants to use their own listeners in some cases. i'm searching for the most elegant way to achieve that.idea 1: (it just feels weird to me, and i don't like to maintain a list of listeners that might be removed)class ourcontrol extends control { mousewheellistener defaultlistener = new defaultmousewheellistener(); ourcontrol() { addmousewheellistener(defaultlistener); } void removedefaultmousewheellistener() { removemousewheellistener(defaultlistener); }}idea 2: (i like this one, even though technically speaking the listener stays where it is; it at least follows the standard bean pattern)class ourcontrol extends control { public boolean enablemousewheel = true; ourcontrol() { addmousewheellistener(new mousewheellistener() { void mousewheelmoved(final mousewheelevent e) { if (enablemousewheel) domagic(); } }); }}what is a good way to achieve this goal?",
    "present_kp": [],
    "absent_kp": [
      "design patterns"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "understanding terms related to 2sat algorithm. recently i am learning about solution of the 2-satiability problem using strongly connected components. there is a theorem related to this problem given below:let $f = q_lx_1 q_2x_2\\ldots q_nx_n c$ be a quantified boolean formula with no free variables, where each $q_i$ is either universal or existential, and $c$ is in conjunctive normal form. that is, $c$ is a conjunction of clauses, each clause is a disjunction of literals, and each literal is either a variable, $x_i$, or the negation of a variable, $x_i$ ($1 \\leq i \\leq n$). theorem 2: the formula $f$ is true if and only if none of the following three conditions holds: 3(i) an existential vertex $u$ is in the same strong component as its complement $u$.3(ii) a universal vertex $u_i$ is in the same strong component as an existential vertex $u_j$ such that $j < i$ (i.e., $x_i$ is not quantified within the scope of $q_i$).3(iii) there is a path from a universal vertex $u$ to another universal vertex $v$. (this condition includes the case that $v = u$.)in the proof they use this definition:we call a vertex universal if the corresponding variable is universally quantified and existential otherwise.but i am not familiar with the terms universal vertex and existential vertex. can any one please explain these terms to me?",
    "present_kp": [],
    "absent_kp": [
      "algorithms",
      "graphs",
      "satisfiability"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is there a way to set today as default due date in trello?. i am using a trello board for my daily plan. i would like all newly added cards to be added with current date as due date. i know i can add cards and afterward i can set the date, i am even using the d shortcut to speed things up. i was wondering if there is a way to automate it even more.",
    "present_kp": [
      "trello"
    ],
    "absent_kp": [
      "trello cards"
    ],
    "domain": "stackexchange"
  },
  {
    "text": ".net - refactoring code. i have inherited and now further develop a large application consisting of an asp.net application, vb6 and vb.net application.the software was poorly written. i am trying to refactor the code as i go along. the changes i am making are not live (they are contained in a folder on my development machine). this is proving to be time consuming and i am doing this along side other work which is the prioritiy.my question is: is this a practical approach or is there a better methodology for refactoring code? i don't have any experience with version control software or source control software and i am wandering if this is what i am missing. i am a sole developer.",
    "present_kp": [
      "asp.net"
    ],
    "absent_kp": [
      "design patterns"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "getting prices and volume. i'm learning f# and have a couple of routines much of whose functionality looks common so i am looking to refactor them together.here are the routines (which for the record i lifted from elsewhere:let prices time id (polling:float) = let sync = system.threading.synchronizationcontext.current let obs = new event<int>() let raiseevent (value:int) = sync.post((fun _ -> obs.trigger(value)), null) let interval = timespan.fromseconds(polling) let rec loop nexttime= async { // generate next value (on the gui thread) do getprices nexttime |> list.filter (fun price -> price.id = id) |> list.iter (fun price -> raiseevent price.price) // wait some short time do! async.sleep(1000) // continue looping do! loop (nexttime.add(interval)) } loop time |> async.start obs.publishand let volumes time id (polling:float) = let sync = system.threading.synchronizationcontext.current let obs = new event<volumedto>() let raiseevent (value:volumedto) = sync.post((fun _ -> obs.trigger(value)), null) let interval = timespan.fromseconds(polling) let rec loop nexttime= async { // generate next value (on the gui thread) do getvolumes nexttime |> list.filter (fun volume -> volume.id = id) |> list.iter (fun volume -> raiseevent volume) // wait some short time do! async.sleep(1000) // continue looping do! loop (nexttime.add(interval)) } loop time |> async.start obs.publishthe external function calls are hopefully reasonable self explanatory retrieving data from a dblet getprices lasttime = createlist<pricedto>((getpricesql lasttime), pricereader)let getvolumes lasttime = createlist<volumedto>((getvolumesql lasttime), volumereader)the types are very simple, but i'm not sure i've got this 'right':type iid = abstract member id : int64type pricedto(id:int64, price:int) = interface iid with member this.id = id member x.id = id member x.price = pricetype volumedto(id:int64, amount:decimal, price:int32) = interface iid with member this.id = id member x.id = id member x.amount = amount member x.price = pricei've played around with something but i'm not at all there as yet:let publish<'a> time id (polling:float) dataretriever = let sync = system.threading.synchronizationcontext.current let obs = new event<'a>() let raiseevent (value:'a) = sync.post((fun _ -> obs.trigger(value)), null) let interval = timespan.fromseconds(polling) let rec loop (nexttime:datetime)= async { // generate next value (on the gui thread) do dataretriever nexttime |> list.filter (fun item -> item.id = id) |> list.iter (fun item -> raiseevent item) // wait some short time do! async.sleep(1000) // continue looping do! loop (nexttime.add(interval)) } loop time |> async.start obs.publishi may well have not quite implemented this correctly. i'm obviously getting a list of prices or volumes filtering them and publishing them. in order to achieve the filtering i've created an interface, but i'm not really happy with this since i'd prefer to keep the dtos as simple as possible. intellisense is telling me that my dataretriever function takes a datetime and returns an 'a list, but i'm not sure i quite want that since i need to cast it to an iid interface for filtering and then back to the underlying dto for publishing.is there a more 'functional' (or perhaps just better) way to do this?",
    "present_kp": [
      "f#"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "fibonacci sequence methods. decided to get a little jdk 8 practice in today, so i built two methods to print out the golden ratio, one uses the stream api the other does not. wanted to request general feedback on how well i implemented these methods (mainly the stream generator,) considering the given task and if there is anything i could have done better.import java.util.arraylist;import java.util.arrays;import java.util.list;import java.util.function.supplier;import java.util.stream.collectors;import java.util.stream.stream;/** * created on 7/5/2016. * */public class fibonacci { public static void main(string[] args) { system.out.printf(fibonacci old: %s%n, string.join(, , fibonacci.calculatefib(10).stream() .map(object::tostring) .collect(collectors.tolist()))); system.out.printf(fibonacci new: %s%n, string.join(, , fibonacci.generatefib() .limit(10) .map(object::tostring) .collect(collectors.tolist()))); } private static list<integer> calculatefib(int fibcount){ list<integer> fibsequence = new arraylist<>(); fibsequence.add(0); fibsequence.add(1); for(int i = 2; i < fibcount; i++){ fibsequence.add(fibsequence.get(i-1) + fibsequence.get(i-2)); } return fibsequence; } private static stream<integer> generatefib() { return stream.generate(new supplier<integer>() { list<integer> list = new arraylist<>(arrays.aslist(0, 1)); int i = 0; @override public integer get() { list.add(list.get(i) + list.get(i+1)); return list.get(i++); } }); }}",
    "present_kp": [
      "java",
      "fibonacci sequence",
      "generator"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "event driven programming in haskell. i'm new to haskell, so this is more a high-level conceptual question. i've read this: <url> and it has this:run :: domain -> [event] -> io ()run dm [] = do events <- uiupdate dm run dm eventsrun _ (eventexit:_) = return ()run dm (e:es) = run (dmupdate dm e) esso uiupdate would generate events in this case.i am trying to understand how this works in an application where you need to push events. an example - say you have a gui where you have a single int counter and three types of event sources:filesystemnetworkhuman interactionfor simplicity, say the counter needs to be increased or decreased whenever any kind of event happens (e.g. new file added or deleted, http call succeeded or failed, human typed on a keyboard or clicked with a mouse).how do you push these events into the event loop? most importantly, i'm not asking for here's how you can do it, but here's how real world haskell applications work. especially if there are different options that are used in practice.",
    "present_kp": [
      "haskell"
    ],
    "absent_kp": [
      "event programming"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "can constraint satisfaction problems be solved with prolog?. is party attendance type of problems solvable in prolog? for example:burdock muldoon and carlotta pinkstone both said they would come if albus dumbledore came. albus dumbledore and daisy dodderidge both said they would come if carlotta pinkstone came. albus dumbledore, burdock muldoon, and carlotta pinkstone all said they would come if elfrida clagg came. carlotta pinkstone and daisy dodderidge both said they would come if falco aesalon came. burdock muldoon, elfrida clagg, and falco aesalon all said they would come if carlotta pinkstone and daisy dodderidge both came. daisy dodderidge said she would come if albus dumbledore and burdock muldoon both came. whom is needs to be persuaded to attend the party in order to ensure that all her invitees attend?i have tried to express this in gnu prolog:attend(bm) :- attend(ad).attend(cp) :- attend(ad).attend(ad) :- attend(cp).attend(dd) :- attend(cp). attend(ad) :- attend(ec).attend(bm) :- attend(ec).attend(cp) :- attend(ec). attend(cp) :- attend(fa).attend(dd) :- attend(fa).attend(bm) :- attend(cp),attend(dd).attend(ec) :- attend(cp),attend(dd).attend(fa) :- attend(cp),attend(dd).attend(dd) :- attend(ad),attend(bm).attend(fa). /* try different seed invitees in order to see if all would attend*//* input:write('invited:'),nl, attend(x),write(x),nl, fail.*/i'm experiencing stack overflow (no pun), and have no knowledge of prolog evaluation, this is why i'm asking.generally speaking, this problem can be cast into boolean cnf satisfaction formula (with 6 boolean variables). therefore, does the prolog perspective have any merit?",
    "present_kp": [
      "prolog"
    ],
    "absent_kp": [
      "logic",
      "constraint programming",
      "logic programming"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "high-resolution finite volume schemes for two phase flow (fields with jumps) literature sources. what other recent sources of literature on this topic would you recommend? this is where i'm starting from: leveque's article: hric schemebut the related articles seem to be a bit dated (some up to 20 years). can anyone with experience with such higher order schemes suggest a direction in which i could search?",
    "present_kp": [
      "finite volume"
    ],
    "absent_kp": [
      "numerics",
      "reference request"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "stroke extraction to recreate chinese characters. i was going through the project lists of andrew ng from his stanford machine learning course and i found a project based on recreation of calligraphic characters. to do so firstly we need to extract strokes from characters and i can only understand it in bits and pieces and i cannot understand at all how to implement it in matlab.i also read the research paper that the stroke extraction method is based on. but i did not understand that too like there were some terms that i could not understand. and not to mention i can't understand just how to implement all the pixel related operations mentioned. i am a beginner in image processing. ss please any help is appreciated. quoting some lines from the paper:we use the direction contribution of the segments to estimate the degree for each pixel on the thick-line image. given a binary image, for each black pixel $p(r,c)$, let $d_k(r,c)$ denote the orientation distance between the pixel and the boundary point along the $k$th quantized orientation, where $k = 1, 2, \\dots, m$, and $m$ is an integer that denotes the quantization number from $0^\\circ$ to $360^\\circ$ . the orientation distance is called point-to-boundary orientation distance (pbod). the distribution of the pbods contains information about the degree of each pixel. we can easily tell that the pbods along the quantized orientations of the branches are much larger than that along other quantized orientation. so, to estimate the degree of each pixel, we need only calculate the number of the crests of the distribution of all the pbods of the pixel. figure 2 shows some cases of the distribution. the resolution of quantization is $3^\\circ$. what is quantisation and orientation distance, and how to implement these in matlab?",
    "present_kp": [
      "image processing"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "unix: what happens if i enter ls -d [2-q]*. would it just ignore the bit after the -d option since you cant arrange characters lexically between 2 and q?",
    "present_kp": [],
    "absent_kp": [
      "shell",
      "wildcards"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "do crawlers add page views on a forum?. i have a mybb forum and i get multiple page views on new threads within 3-5 minutes of posting. i manually submit new urls through google webmaster tools which should if anything account for 1 page view, but not multiple. i don't have active members at the moment. research leads me to believe my 100+ members who never post or respond to admin emails are actually registration bots. so are these page views on my new content from real people or from web crawlers?",
    "present_kp": [
      "forum"
    ],
    "absent_kp": [
      "spam bots"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "should cookies be used in a restful api?. i'm specifically interested in how users perform authorized / authenticated operations on a web api.are authentication cookies compatible with the rest philosophy, and why?",
    "present_kp": [
      "rest"
    ],
    "absent_kp": [
      "web applications",
      "web services"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is policy collapse and what are the causes?. i saw the term policy collapse on the comments of a tutorial for reinforcement learning. i'm guessing that it's referred to as a policy collapse when the policy worsens over training due to a bad hyper-parameter, be it the learning rate, batch size, etc., but i couldn't find anything explaining it in clearly and in detail.",
    "present_kp": [
      "reinforcement learning"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "properties of cat command. a simple question, can i create a file at a location i want & using cat command & without using pipes & and the location is some other place than i where i am currently. (i would appreciate an edit)",
    "present_kp": [
      "cat"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "gvfs-mount won't mount a ssh host specified in config. i am struggling with configuring gvfs-mount to work with a host that is being connected to through a proxy. my config looks like this:host proxyhost user [username] hostname [domain] port [non-standard port]host destination hostname [ip] user [other username] port 22 identityfile [path to private key] forwardagent yes proxycommand ssh proxyhost nc %h %p 2> /dev/nulli am able to connect to it through the terminal - ssh destination works perfectly fine. the first server just asks for password and i'm logged in.but when i try to connect through gvfs-mount (i would like to work using thunar) it prompts for username and password and i have completely no idea what should i provide there. i tried credentials to the proxyhost and it does not work. i just keep being asked 3 times for passoword and then returnederror mounting location: permission deniederror. is there any way that i can make it work?",
    "present_kp": [
      "ssh",
      "mount",
      "thunar",
      "gvfs"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "trim audio file using start and stop times. i have an ffmpeg command to trim audio:ffmpeg -ss 01:43:46 -t 00:00:44.30 -i input.mp3 output.mp3the problem i have with this command is that option -t requires a duration (in seconds) from 01:43:46. i want to trim audio using start/stop times, e.g. between 01:43:46 and 00:01:45.02.is this possible?",
    "present_kp": [
      "audio",
      "ffmpeg",
      "trim"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "what are ./ and ../ directories?. simple question, but i'm not sure where to look and google doesn't respond to periods and slashes.i'm just trying to count the # of files & directories in the current directory (not including subfolders/files) and was trying to differentiate ls -1 | wc -l and ls | wc -l since they seem identical. a site i was looking at said keep in mind that this is also counting the ./ and ../ directories. regarding the one with ls -1, and i'm not sure if that means it includes the directories before or something (which i don't want), but it didn't seem to do that from testing.could someone confirm which one of those would be most adequate for counting # of files & directories in the current directory only (not sub) and what they mean by ./ and ../ directories?",
    "present_kp": [
      "directory"
    ],
    "absent_kp": [
      "filenames"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "create a pie chart with a formula. i'm using google's monthly budget sheet to track my expenses.i need to have a pie chart but i don't want it to be floating around in the sheet and be in a specific cell(s). so i thought a formula like sparkline might be the answer. but is there such a formula? how can i achieve this?",
    "present_kp": [],
    "absent_kp": [
      "worksheet function",
      "google spreadsheets"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "2d game enemy entities factories, probably in need of refactoring. so i'm making a 2d shoot'em up game as a way of learning canvas, and larger-scale programming than what i'm accustomed to. i'm a purely front-end web dev, i can't say i'm very experienced in this.anyway the way i spawn my enemies is like this : at xyz gametime, a spawner object is pushed into the game. this spawner object add various enemies in various configurations, using that type of interface, i hope the code is self explanatory:state.enemies.push(enemies[this.enemytype].add({ pos: [this.pos[0], this.pos[1]], angle: this.angle, rotation: this.enemyrotation, path: this.path}));but i'm not here for game design review so let me talk about what i want advice on.basically you can see my enemy module here: <url> me explain it bit by bit.this is the global config object that manages only the default settings of every enemy type. that is, it doesn't contain any vector/rotation/position infos, because these settings are injected at enemy creation. // config objectvar enemyconfig = { redbomber: { url: 'assets/images/enemy-xs-1.svg', pos: [0, 0], size: [75, 53], speed: 100, hitpoints: 10, rof: 100, score: 100, burst: { amount: 3, delay: 1000, counter: 3 }, }, scout: { url: 'assets/images/scout.png', pos: [0, 0], size: [50, 44], speed: 500, hitpoints: 2, score: 50 }, rotatingplat: { url: 'assets/images/platpart.png', pos: [0, 0], size: [150, 44], speed: 30, hitpoints: 100, rof: 200, score: 250, burst: { amount: 1000, delay: 1000, counter: 1000 }, }, rogueleader: { url: 'assets/images/rogueleader.svg', pos: [0, 0], size: [200, 89], speed: 100, hitpoints: 60, score: 300, rof: 500, }, drone: { url: 'assets/images/drone.svg', pos: [0, 0], size: [50, 62], speed: 300, hitpoints: 5, score: 75 },};next we have a global enemy constructor, managing every enemy type as a base. it contains a mix of the default settings shown above, and the what i call active settings, that is, arguments different for every enemy entity even if they are of the same type (like position). this constructor also have the basic methods and a blank shoot method, because each enemy type has his own implementation (doesn't seem right?).// base enemy constructorvar enemyentity = function(settingsdefault, settingsactive) { // default this.active = true; this.speed = settingsdefault.speed; this.hitpoints = settingsdefault.hitpoints; this.lastfire = date.now(); this.score = settingsdefault.score; this.rof = settingsdefault.rof || null; this.maxhitpoints = settingsdefault.hitpoints; this.rotating = settingsdefault.rotating || null; if (settingsdefault.burst) { this.burst = { amount: settingsdefault.burst.amount, delay: settingsdefault.burst.delay, counter: settingsdefault.burst.counter }; } this.sprite = new sprite({ url: settingsdefault.url, pos: settingsdefault.pos, size: settingsdefault.size, rotated: true }); // active this.angle = settingsactive.angle; this.pos = settingsactive.pos; this.radians = settingsactive.angle * math.pi / 180; this.vector = [math.cos(this.radians) * this.speed, math.sin(this.radians) * this.speed]; this.rotation = settingsactive.rotation || 0; this.path = settingsactive.path || null;};// update methodenemyentity.prototype.shoot = function() {};enemyentity.prototype.update = function(dt) { if (this.outofbounds()) { this.active = false; return; } if (this.path == angular) { paths.angular(this); } this.pos[0] += this.vector[0] * dt; this.pos[1] += this.vector[1] * dt;};enemyentity.prototype.outofbounds = function() { return this.pos[1] > canvas.height || this.pos[0] < 0 || this.pos[0] > canvas.width;};// draw methodenemyentity.prototype.render = function() { canvas.ctx.save(); canvas.ctx.translate(this.pos[0], this.pos[1]); canvas.ctx.translate(this.sprite.size[0] / 2, this.sprite.size[1] / 2); canvas.ctx.rotate(math.pi / 180 * this.rotation); this.sprite.render(canvas.ctx); canvas.ctx.restore();};this part is another constructor, used to create enemy factories. basically its only role is to share an add method, used by every enemy factory.// factory constructorvar enemyfactory = function() {};enemyfactory.prototype.add = function(settings) { return new this.type(settings);};now this is an example of what a specific enemy factory looks like. i put two so you can see how much wet code this is. basically, we first create a constructor, by calling the enemyentity constructor with this, and with the active settings. we also get enemyentity methods with calling it as a prototype. the shoot function is different for each enemy.then we have the enemy factory per se, basically it's constructed from the enemyfactory constructor, and giving the specific enemy constructor as a type, so we then can use myenemy.add(activesettings);// redbombervar redbomber = function(settings) { enemyentity.call(this, enemyconfig.redbomber, settings);};redbomber.prototype = object.create(enemyentity.prototype);redbomber.prototype.shoot = function() { var now = date.now(); // if the enemy can shoot if (this.pos[1] > 0 && this.burst.counter && now - this.lastfire > this.rof) { var x = this.pos[0] + this.sprite.size[0] / 2; var y = this.pos[1] + this.sprite.size[1] / 2; state.ebullets.push(weapons.red.addmissile({x: x, y: y, angle: 90})); state.ebullets.push(weapons.red.addmissile({x: x, y: y, angle: 80})); state.ebullets.push(weapons.red.addmissile({x: x, y: y, angle: 100})); this.burst.counter--; this.lastfire = now; return; } else if (!this.burst.counter && now - this.lastfire > this.burst.delay) { this.burst.counter = this.burst.amount; }};function redbomberfactory () {};redbomberfactory.prototype = new enemyfactory();redbomberfactory.prototype.type = redbomber;var redbomber = new redbomberfactory();// rogue leadervar rogueleader = function(settings) { enemyentity.call(this, enemyconfig.rogueleader, settings);};rogueleader.prototype = object.create(enemyentity.prototype);rogueleader.prototype.shoot = function() { var now = date.now(); // if the enemy can shoot if (this.pos[1] > 0 && now - this.lastfire > this.rof) { var x = this.pos[0]; var y = this.pos[1]; state.ebullets.push(weapons.redray.addmissile({x: x + this.sprite.size[0] * 0.3, y: y + this.sprite.size[1] * 0.6, angle: this.rotation - 90})); state.ebullets.push(weapons.redray.addmissile({x: x + this.sprite.size[0] * 0.7, y: y + this.sprite.size[1] * 0.6, angle: this.rotation - 90})); this.lastfire = now; }};function rogueleaderfactory () {};rogueleaderfactory.prototype = new enemyfactory();rogueleaderfactory.prototype.type = rogueleader;var rogueleader = new rogueleaderfactory();then we exports our factory functionsmodule.exports = { redbomber: redbomber, scout: scout, rotatingplat: rotatingplat, rogueleader: rogueleader, drone: drone};what's the problem then ? as you can see there is a lot of repetition in my code, every new enemy type adds a lot of code, but i only really need the shoot method. though i'm not sure right now how i could manage that better and ask for your advice. any other consideration of the code is of course much appreciated",
    "present_kp": [
      "game",
      "constructor"
    ],
    "absent_kp": [
      "javascript",
      "object oriented",
      "factory method"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how can i find the implementation/source code of an interface in .net?. so i'm disassembling the winnov.amalga.core.session.common.dll, and trying to figure out how the writescriptcommand works.i'm absolutely new to .net, so go easy on me, please.you can find the dll yourself here: link under the binary folder.below is pretty much the only reference i found. edit: from searching through the decompiled dll.using system;namespace winnov.amalga.core{ public interface isession { string configuration { get; set; } sessionstate { get; } timespan duration { get; } string archivebasepath { get; set; } string archivepath { get; } void start(); void stop(); void applypreset(string presetxml); void writescriptcommand(string name, string value); }}",
    "present_kp": [
      "dll",
      ".net"
    ],
    "absent_kp": [
      "decompilation"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "salesforce report that searches long text fields, has customizable view, easy to update, etc. i got really tired of how ugly and inflexible salesforce reports are. so i made this apex/visualforce combo that searches long text fields, makes adding and removing fields a snap, has a customizable view, and is generally easy(er) to work with.this basically does everything by shuttling json strings back and forth between the view and the controller. if your browser doesn't have javascript enabled, you're going to have a bad time.other cool things:works for any custom record type, and any custom field. you specify which in the view; theoretically, there's no need to touch any apex, just good ol' html/visualforce.detects whether or not one of the fields by which you want to search is a picklist. if it is, it gets all the values for the picklist and prints them out in the dom as a multi-select picklist. ditto for checkboxes.it's super-easy to reference fields and make them appear in the view however you want.you can sort results just by clicking on column headers.salesforce has two methods of searching for records: sosl and soql. one's great for searching for text strings; the other one's great for everything else. this apex class figures out automatically which one to use.<apex:page controller =sfreportsplus sidebar =false showheader =true standardstylesheets=true><!--contact <email> with any questions!--> <apex:form> <apex:actionfunction name = lookupfields action ={!lookupfields} oncomplete =renderfields(), $('#loading').slideup(), $('#wrapper').slidedown() rerender =fieldsblock, messageblock ><!--specify the salesforce name of the record type you want to search. i use leads here, but custom record types (and custom fields) are peachy.--> <apex:param name = recordname assignto ={!recordname} value =lead /><!--specify all the fields you want to involve here -- the fields by which you want people to search, and the fields you want to otherwise show in the view.--><!--this is clearly just a json map. the values are the actual salesforce names of the fields. the keys are easier-to-read headers that you create, and that you use to refer to the fields from now on.--><!--if you're getting a ton of null pointer exceptions, it's because you spelled field's name wrong, or referenced a field that doesn't exist. pardon my crappy error handling.--> <apex:param name = fieldsheaderswithnamesjsonin assignto ={!fieldsheaderswithnamesjsonin} value ={ 'id' :'id', 'name' :'name', 'description' :'description', 'where_it_came_from' :'leadsource' } /><!--using the headers you specified above, specify the fields by which you want people to be able to search.--> <apex:param name = fieldstosearchjsonin assignto ={!fieldstosearchjsonin} value =[ 'name', 'where_it_came_from' ] /><!--using the headers you specified above, specify the fields you want to show in the view.--> <apex:param name = fieldstoshowjsonin assignto ={!fieldstoshowjsonin} value =[ 'id', 'name', 'where_it_came_from', 'description' ] /> </apex:actionfunction> </apex:form><!--you'll need to tweak this to reference jquery properly.--> <apex:includescript value={!urlfor($resource.slca2__jquery, 'jquery.min.js')} /> <script type=text/javascript> window.onload = new function(){ lookupfields(); } var fieldstoshow; var fieldstosearch; var fieldswithoptions; function prettify(input){ var output = ; output += input.charat(0).touppercase(); output += input.substr(1).replace(/_/g, ); return output; }//this takes the values from multi-select picklists and concats them into a comma-separated string in hidden text inputs. function selectedtocsv(select){ var values = []; $(select).find(:selected).each(function(index, element){ values.push($(element).val()); }); $(select).prev(input).val(values.join(,)); } function togglefields(){ if($(#and).val().trim() == ){ $(#or, #not).prop(disabled,disabled); }else{ $(#or, #not).prop(disabled,); } } function renderfields(){ window.fieldswithoptions = $.parsejson($(#fieldswithoptionsjsonout).val()); window.fieldstosearch = $.parsejson($(#fieldstosearchjsonout).val()); window.fieldstoshow = $.parsejson($(#fieldstoshowjsonout).val()); window.results; var fieldsinputs = [], fieldsheaders = [], fieldsinputshtml = , fieldsheadershtml = , fieldstosearchhtml = , numcols = 5; for(var x in fieldstosearch){ var fieldheader = fieldstosearch[x], field = fieldswithoptions[fieldheader], output = ; switch(field.type){ case checkbox: output = <input type=\\checkbox\\ id=\\ + field.name + \\ name=\\ + field.name + \\/>; break; case picklist: output = <input type=\\hidden\\ name=\\ + field.name + \\> + <select multiple= rue\\ id=\\ + field.name + \\ onchange=\\selectedtocsv(this)\\>; for(var optionvalue in field.options){ var optionlabel = field.options[optionvalue]; output += <option value=\\ + optionvalue + \\> + optionlabel + </option>; } output += </select>; break; default: output = <input type= ext\\ name=\\ + field.name + \\ />; break; } fieldsinputs.push(<td> + output + </td>); fieldsheaders.push(<th> + prettify(fieldheader) + </th>); } for(var x in fieldsheaders){ var counter = number(x) + 1; fieldsheadershtml += fieldsheaders[x]; fieldsinputshtml += fieldsinputs[x]; if(counter % numcols == 0 || counter >= fieldsheaders.length){ fieldstosearchhtml += <tr> + fieldsheadershtml + </tr> + <tr> + fieldsinputshtml + </tr>; fieldsheadershtml = ; fieldsinputshtml = ; } } $(#fieldstosearch).html(fieldstosearchhtml); } function getnewresults(){ window.results = $.parsejson($(#resultsjson).val()); renderresults(); } function renderresults(){ var output = ; $(#results).html(); for(x in window.results){ var result = window.results[x], template = $(#resulttemplate).html(); output += template.replace(/\\{([a-za-z_])+\\}/g, function(substring){ var varname = substring.substring(1, substring.length - 1); if(varname.indexof(date) > -1){ return result[varname].substring(0, 10); }else if(result[varname] == null){ return ; }else{ return result[varname]; } }); } $(#results).html(output); $('#loading').slideup(); $('#resultswrapper').slidedown(); } var ascordesc = asc; function orderby(orderbythis){ window.results.sort(function(a,b){ if(a[orderbythis] > b[orderbythis]){ return (window.ascordesc == asc? 1 : -1); }else{ return (window.ascordesc == asc? -1 : 1); } }); if(window.ascordesc == asc){ window.ascordesc = desc; }else{ window.ascordesc = asc; } renderresults(); } </script> <style type=text/css> .errors { color:red; } #wrapper * { box-sizing:border-box; } #wrapper table { width:100%; } #wrapper tr>* { vertical-align:top; } #wrapper th { background-color:#fda; white-space:normal; } #fields select, #fields input, #fields textarea { width:100%; } #fields *[colspan] { max-width:none; } #fields td { padding-bottom:30px; } #resultswrapper th[onclick] { cursor:pointer; text-decoration:underline; } .hide{ display:none; } #loading span { display:inline-block; vertical-align:bottom; overflow:hidden; animation: loading 1s ease-in-out 0s infinite alternate; -webkit-animation: loading 1s ease-in-out 0s infinite alternate; } @keyframes loading { 0% { width:5px;} 100% { width:30px; } } @-webkit-keyframes loading { 0% { width:5px; } 100% { width:30px; } } </style> <apex:sectionheader title=sfreportsplus /> <apex:outputpanel id=messageblock> <p class=errors>{!messageout}</p> </apex:outputpanel> <div id=wrapper class=hide> <apex:pageblock id=fieldsblock> <apex:form> <input id = fieldstosearchjsonout value ={!fieldstosearchjsonout} type =hidden /> <input id = fieldstoshowjsonout value ={!fieldstoshowjsonout} type =hidden /> <input id = fieldswithoptionsjsonout value ={!fieldswithoptionsjsonout} type =hidden /> <table id=fields> <tbody id=fieldstosearch></tbody> <tr> <th colspan=2>search for records that contain...</th> <th>any of these words:</th> <th>or any of these:</th> <th>but not any of these:</th> </tr> <tr> <td colspan=2><em>(separate terms with commas, e.g. <kbd>john smith, graphic design, graduate</kbd>)</em></td> <td> <input name =and id =and type =text onkeyup =togglefields() /> </td> <td> <input name =or id =or type =text disabled =disabled /> </td> <td> <input name =not id =not type =text disabled =disabled /> </td> </tr> <tr> <td> <apex:commandbutton value =search action ={!dothesearch} onclick =$('#loading').slidedown() oncomplete =getnewresults() rerender =messageblock, resultsjsonblock /> </td> </tr> </table> </apex:form> </apex:pageblock> <apex:outputpanel id=resultsjsonblock> <input id = resultsjson value ={!resultsjson} type =hidden /> </apex:outputpanel> <div id=resultswrapper class=hide> <apex:pageblock> <table> <thead> <tr> <th></th> <th onclick=orderby('name')>name</th> <th onclick=orderby('where_it_came_from')>lead source</th> </tr> </thead><!--this is where the results actually show up. this <tbody> is a template. for each result, javascript repeats this template in the dom. it expects everything inside curly brackets {blah} to correspond to one of the headers of the record you're looking up.--><!--stuff in curly brackets with an exclamation mark {!blah} still refers to an actual salesforce variable.--> <tbody id=resulttemplate class=hide> <tr> <td> <button onclick=$('#longtext{id}').toggle() type=button>view</button> </td> <td><a href={!$site.baseurl}/{id} target=_blank>{name}</a></td> <td>{where_it_came_from}</td> </tr> <tr id=longtext{id} class=hide> <td colspan=8>{description}</td> </tr> </tbody> <tbody id=results> </tbody> </table> </apex:pageblock> </div> </div> <p id=loading>loading<span>..........</span></p></apex:page>...and the apex class:public with sharing class sfreportsplus{ public string messageout = ''; public string recordname {get; set;} public schema.sobjecttype recordtype; public map<string, schema.sobjectfield> recordfields; public sobject record; public string fieldsheaderswithnamesjsonin {get; set;} public map<string, string> fieldsheaderswithnames; public map<string, map<string, object>> fieldswithoptions = new map<string, map<string, object>>(); public string fieldstosearchjsonin {get; set;} public list<string> fieldstosearch; public string fieldstoshowjsonin {get; set;} public list<string> fieldstoshow; public string getmessageout(){ return messageout; } public string getfieldsheaderswithnamesjsonout(){ return json.serializepretty(fieldsheaderswithnames); } public string getfieldstosearchjsonout(){ return json.serializepretty(fieldstosearch); } public string getfieldstoshowjsonout(){ return json.serializepretty(fieldstoshow); } public string getfieldswithoptionsjsonout(){ return json.serializepretty(fieldswithoptions); } public pagereference lookupfields(){ try{ recordtype = schema.getglobaldescribe().get(recordname); record = recordtype.newsobject(); recordfields = recordtype.getdescribe().fields.getmap(); fieldsheaderswithnames = (map<string, string>) json.deserialize( fieldsheaderswithnamesjsonin.replaceall(''', ''), map<string, string>.class ); fieldstosearch = (list<string>) json.deserialize( fieldstosearchjsonin.replaceall(''', ''), list<string>.class ); fieldstoshow = (list<string>) json.deserialize( fieldstoshowjsonin.replaceall(''', ''), list<string>.class ); for(string fieldlabel : fieldsheaderswithnames.keyset()){ fieldswithoptions.put(fieldlabel, returnfieldoptions(fieldlabel)); } }catch(exception e){ messageout += 'something went wrong getting the search fields: ' + e + '; '; } return null; } public map<string, object> returnfieldoptions(string fieldlabel){ string fieldname = fieldsheaderswithnames.get(fieldlabel); schema.describefieldresult field = recordfields.get(fieldname).getdescribe(); string fieldtype = string.valueof(field.gettype()); map<string, string> fieldoptions = new map<string, string>(); map<string, object> output = new map<string, object>(); if( fieldtype == 'picklist' ||fieldtype == 'multipickist' ){ fieldtype = 'picklist'; for(schema.picklistentry option : field.getpicklistvalues()){ fieldoptions.put(option.getvalue(), option.getlabel()); } }else if( fieldtype == 'date' ||fieldtype == 'time' ){ fieldtype = 'time'; }else if( fieldtype == 'boolean' ){ fieldtype = 'checkbox'; }else{ fieldtype = 'string'; } output.put('name', fieldname); output.put('type', fieldtype); output.put('options', fieldoptions); return output; } public map<string, string> fieldswithinputs {get; set;} public string logicstring = ''; public string conditionalstring = ''; public string fieldstoshowstring = ''; public string querystring; public list<sobject> results = new list<sobject>(); public list<object> resultsout; public string getresultsjson(){ return json.serializepretty(resultsout); } public string getquerystring(){ return querystring; } public pagereference dothesearch(){ messageout = ''; resultsout = new list<object>(); try{ fieldswithinputs = apexpages.currentpage().getparameters(); buildlogicstring(); buildfieldstoshowstring(); buildconditionalstring(); if(fieldswithinputs.get('and') == ''){ results = dosoqlsearch(); }else{ results = dososlsearch(); } for(integer x = 0; x < results.size(); x++){ sobject result = results.get(x); map<string, object> resultout = new map<string, object>(); for(string fieldheader : fieldstoshow){ string fieldname = fieldsheaderswithnames.get(fieldheader); resultout.put(fieldheader, result.get(fieldname)); } resultsout.add(resultout); } }catch(exception e){ messageout += 'something went wrong looking up records: ' + e + '; '; } return null; } public list<sobject> dosoqlsearch(){ querystring = 'select ' + fieldstoshowstring + ' from ' + recordname; if(conditionalstring != ''){ querystring +=' where (' + conditionalstring + ')'; } list<sobject> soql = database.query(querystring); return soql; } public list<sobject> dososlsearch(){ querystring = 'find '(' + logicstring + ')'' + ' returning ' + recordname + ' (' + fieldstoshowstring; if(conditionalstring != ''){ querystring +=' where (' + conditionalstring + ')'; } querystring +=')'; list<list<sobject>> sosl = search.query(querystring); return sosl[0]; } public void buildlogicstring(){ logicstring = '('; logicstring += '(' + fieldswithinputs.get('and').replaceall(',',' and ') + ')'; if(string.isnotblank(fieldswithinputs.get('or'))){ logicstring += ' or (' + fieldswithinputs.get('or').replaceall(',',' or ') + ')'; } logicstring += ')'; if(string.isnotblank(fieldswithinputs.get('not'))){ logicstring += ' and not (' + fieldswithinputs.get('not').replaceall(',',' or ') + ')'; } } public void buildfieldstoshowstring(){ list<string> output = new list<string>(); for(string fieldheader : fieldstoshow){ output.add(fieldsheaderswithnames.get(fieldheader)); } fieldstoshowstring = join(output, ', '); } public void buildconditionalstring(){ list<string> outputslist = new list<string>(); for(string fieldheader : fieldstosearch){ string fieldname = fieldsheaderswithnames.get(fieldheader); if(fieldswithinputs.get(fieldname) == null ||fieldswithinputs.get(fieldname) == '' ){ continue; } string outputstring; string inputstring = fieldswithinputs.get(fieldname); list<string> inputlist = inputstring.split(',', 0); if(inputlist.size() == 1){ outputstring = fieldname + ' = '' + inputstring + '''; }else{ outputstring = fieldname + ' in ('' + join(inputlist,'','') + '')'; } outputslist.add('(' + outputstring + ')'); } conditionalstring = join(outputslist, ' and '); } public string join(list<string> values, string delimeter){ string output = ''; for(integer x = 0; x < values.size(); x++){ output += values.get(x); if(x < values.size() - 1){ output += delimeter; } } return output; }}on github",
    "present_kp": [],
    "absent_kp": [
      "salesforce apex"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "prevent mint from suspending while playing music in spotify. in linux mint 17 i have suspend when inactive for under power management set to 10 minutes.the problem is that the system suspends even when i'm listening to music using spotify.is there any way to prevent this?",
    "present_kp": [
      "linux mint",
      "power management"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "the complexity of properly learning decision trees. where does this paper prove the middle bullet point of its abstract?i have looked through that paper fairly thoroughly.there are three things i want to read how they're getting around.reductions from np problems to sat may have the effectof applying arbitrarily large polynomials to the input size.theorems 2 and 3 and 20 restrict the runtime of their learning algorithms.theorems 2 and 3 and 20 consider randomized algorithms.",
    "present_kp": [
      "decision trees"
    ],
    "absent_kp": [
      "pac learning"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "floating panel over fullscreen windows in tile wm. i want to create kiosk using gnu/linux. i need to show gui one application full screen.i've found that ratpoison suits my needs. but i also want to allow user to see and change keyboard layout and display digital clock on a panel. it's clear that i don't need all 1280 horizontal pixels to display this information. my fullscreen application has main menu and it would be great to use empty space on the right to overlay it with the panel.i've achieved that with those lines in tint2 config:'''panel_position = top right horizontalpanel_size = 150 24strut_policy = nonepanel_layer = top'''it works well with a normal wm (like kwin), but i cannot get the same result with a tile wm (i suppose it's easier to strengthen simple tile wm than a normal one). i've already tried ratpoison, awesome and i3.how can i achieve my goal? i can easily change wm or panel to a different one.",
    "present_kp": [
      "panel",
      "kiosk",
      "fullscreen"
    ],
    "absent_kp": [
      "tiling wm"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "installing ossec on an existing centos 6.4 machine. i have an existing machine running centos 6.4. is it ok to install now the ossec or it's not worth any more as the machine is already running for few months. which is the best mechanism to install because i saw one is from the source and some recommend atomicorp repo? any recommendation please?",
    "present_kp": [
      "centos"
    ],
    "absent_kp": [
      "security"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to make modprobe nf_conntrack_ftp persist a reboot on centos 7 and firewalld?. i'm stumped. i've seen the related questions about editing /etc/sysconfig/iptables-config, but i'm running centos 7 (and webmin/virtualmin), which uses firewalld instead of iptables.how do i get modprobe nf_conntrack_ftp to persist after a reboot? i tried setting up a cron on reboot to issue the command, but that didn't work (and is probably the wrong way to go about it anyway).right now after every reboot i have to login and type in the command manually in order to get ftp connections to work correctly.thanks.",
    "present_kp": [
      "centos",
      "modprobe",
      "firewalld"
    ],
    "absent_kp": [
      "proftpd"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "recommended way to organize and build multiple services for app engine flexible environment with custom runtime. when deploying to flexible environment using the default runtime (e.g. python), we just need to specify different app.yaml files with different entrypoint: values.but what would the 'best practice' for custom runtimes be for this case? multiple dockerfile's with different cmd values?but that would mean that each service needs to be built individually. maybe have a 'base' dockerfile and image to speed up the build?",
    "present_kp": [
      "python",
      "docker"
    ],
    "absent_kp": [
      "google app engine"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "creating a list containing the rank of the elements in the original list. i don't really know how to explain what i'm looking for in a way that makes sense, but here goes:say i have a list $$l=(4,7,9,10,6,11,3)$$what i want to produce is a corresponding list$$ k = (1,3,4,5,2,6,0)$$where element k[i] has the value of the 'rank' of the element for the corresponding location in l. so the higher the number the higher the rank, starting from rank 0 for the smallest number in lthe code i have written for this is:x = [4,7,9,10,6,11,3]index = [0]*len(x)for i in range(len(x)): index[x.index(min(x))] = i x[x.index(min(x))] = max(x)+1and it works, but i just feel it looks horrible, and was wondering if there might exist a more aesthetic way.",
    "present_kp": [],
    "absent_kp": [
      "python",
      "sorting"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why predicted proababilities from this binary classifier does not sum up to 1?. i have a c5.0 model that is trained to predict binary class (c1/c2) on a dataset with 20 features. the model is configured to perform boosting (10 trials) and it has a miss-classification cost function (100:1 where 100 is the cost for miss-classifying a negative sample as positive and 1 is the cost for miss-classifying negative as positive).looking at predicted probabilities generated by the model, i can see that it ranges from 0 to 1 for each class. i.e, i have instances where the predicted class (c1) has a probability lower than 0.5 ( for example: predicted class=c1 and predicted probability=0.1 ) - this is where the question arises: if p(c1) < 50% why is it classified as c1 (there are only two classes, c1 and c2)based on my understanding of decision trees, the predicted probabilities are often generated based on the percentage of test cases on the leaf node that were labeled in each class divided by the total number of instances hitting that leaf node. this method will dictate the probability for two classes must sum up to 1.my question is why does the model classify an instance in class c1 if it has only got 0.1 confidence in it. does a predicted probability of 0.1 on class c1 mean that there is a 0.9 confidence in it belonging to class c2? if so, why does it classify an instance in class c1 is it has less than 0.5 confidence in it?my own theory is that this might be due to boosting and miss-classification cost and the way they are influencing the predicted class.",
    "present_kp": [
      "classification",
      "probability",
      "binary"
    ],
    "absent_kp": [
      "predictive modeling"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how do i send a link to a shared trello board?. possible duplicate:bookmarking in trello is it possible to give or share a direct link to a particular trello board? there's no direct link that i can see to either the list of boards, or to one board that i share with others. where might this simple functionality be hiding?",
    "present_kp": [
      "trello"
    ],
    "absent_kp": [
      "sharing",
      "links"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to reduce the boot time of a beaglebone using systemd. i'm currently trying to reduce the time it takes to boot my beaglebone green and start a python script. my simple python script just turns on a relay on a cape.i'm using the time it takes for the relay to turn as the time it takes to boot and do something useful.previously the relay would take 18 secs and i've gotten it down to 14 secs messing around with systemd.i've created a simple unit file that will start my python program[unit]description=relaycheck run on startupdefaultdependencies=noafter=systemd-system.slice[service]workingdirectory=/home/execstart=/home/relaycheck2.pystandardoutput=null[install]alias=relaycheck2.serviceand here's a cropped picture of the current boot chart:bootchartto my understanding, my service is being run quite early and the time consuming services like networking, don't impact my relay service. is this correct?is there anything else i can do reduce the time to boot using systemd?",
    "present_kp": [
      "boot",
      "systemd"
    ],
    "absent_kp": [
      "linux",
      "debian",
      "beagleboneblack"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "member functions vs. non-member functions for math operators. i'm writing a linear algebra library (long story short, it's a school assignment) that involves matrices, vectors, etc. in the process of creating this library, i'm going to be creating functions that perform mathematical operations on objects. for example, transpose matrix, invert matrix, normalize vector, etc.i was curious as to what is the best practice for this sort of function... that is, should i make the function a member function, or non-member? (for clarity/library use sake)example://member function way:b = a.transpose();c = a.inverse();//non-member function way:b = linalg::transpose(a); //non-member transpose function in linear algebra namespacec = linalg::inverse(a);is there some standard regarding these sorts of operations? or, at least, is there a common way people do this? i'm leaning towards the first option, but i'd like to know if this is recommended.",
    "present_kp": [],
    "absent_kp": [
      "c++",
      "libraries",
      "methods"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "router for mvc framework. the class routes urls based on the domain.com/class/method/param/.. format. it also checks the request type (get or post) and calls the method name get or post from the identified class.class router { private $_routes, $_path, $_method, $_found; public function __construct(array $routes, $path, $method) { krsort($routes); $this->_routes = $routes; $this->_path = $path; $this->_method = strtoupper($method); } public function load() { foreach($this->_routes as $regex => $class) { $regex = str_replace('/', '\\/', $regex); $regex = '^' . $regex . '\\/?$'; if(preg_match('/' . $regex . '/i', $this->_path, $params)) { $this->_found = true; $this->_classinstantiate($class, $params); break; } } if(!$this->_found) { throw new exception('url location not found: ' . $this->_path); } } private function _classinstantiate($class, $params) { if(class_exists($class)) { $obj = new $class($params); if(method_exists($obj, $this->_method)) { $method = $this->_method; $obj->$method(); } else { throw new badmethodcallexception('method not found: ' . $this->_method); } } else { throw new exception('class not found: ' . $class); } }}i load it up in my entry file like so://url to class router$routes = (array) $config->routes;$path = $_server['request_uri'];$method = $_server['request_method'];$router = new router($routes, $path, $method);$router->load();am i utilizing dependency injection correctly?are there any parts of my class that i should decouple?is there anything i could improve in my code when it comes to oo php (and mvc)?",
    "present_kp": [
      "php",
      "mvc"
    ],
    "absent_kp": [
      "object oriented",
      "url routing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "wrong root shell in /etc/passwd. i wanted to change the standard shell for my root user to bash but i got the path wrong. i typed /bin/bash instead of /opt/bin/bash. now my /etc/passwd looks like this:root:x:0:0:root:/root:/bin/bashthe problem is that i can no longer login via ssh. additionally i can't su to root from an other user because of: >su rootsu: must be suid to work properly any suggestions?",
    "present_kp": [
      "shell",
      "ssh"
    ],
    "absent_kp": [
      "linux"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "disable root password caching. how can i disable root password caching? i looked at this question, but i didn't really understand how to do it, is not the same as my question but it is indirectly related.i tried adding this to /etc/sudoers:timestamp_timeout=0but it gave me:>>> /etc/sudoers: syntax error near line 46 <<<so i just pressed x. 46 is the line where i added it, so there isn't a problem with anything else.",
    "present_kp": [
      "sudo",
      "password",
      "root"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "proper action when a java program fails. i have a program that has to initialize a few big things (connect to a few databases, parse some xml) and without the initialization being successful the program would not be able to continue.right now i have my main method throwing just a general exceptionpublic static void main(string[] args) throws exception{ //throws numerous types of exceptions whateverobject we = whateverobject.getinstance(); we.dosomething();}my question is, is there a better way to handle this? should i catch the exception and then print out that it failed an exit? something else maybe? note, there's no hope of program recovery at this point.",
    "present_kp": [
      "java"
    ],
    "absent_kp": [
      "exception handling"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to deal with item belonging to more than one category. so i'm training a classification task which takes as input some description of the state of a 2x2x2 rubix cube and outputs the optimal move to take. and a potential problem i noticed is that in many states more than one move is optimal. in particular, only about 46% of states have only one optimal move, and 24% have 2, 12% have three, etc. so i have a few choices.the options i thought of werehave each data point choose an optimal move at randomdo cross-entropy minimization with the data point containing the same probability for each optimal move. i.e (0.5,0.5,0,0,0,0,0,0,0) if the first two are optimaldiscard states with more than one optimal move (this seems really bad)what is the standard practice? also, is there a difference between 1 and 2?if necessary you may assume that i'm using a neural network model",
    "present_kp": [
      "classification",
      "neural network"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "multiple sequence progressive alignment example using five sequences. i tried to find sources that explain multiple sequence alignment with an example using progressive alignment, but i couldn't find one.i actually know till creating a guide tree, after creating a guide tree we use guide tree to arrange sequences. this is the critical part. can some please help me in explaining how to align sequences that are already aligned with a new sequence.for example there are 5 sequences s1, s2, s3,s4 and s5. after aligning s1 and s3, suppose the aligned sequence is a1. now the problem is i don't know how to align a1 with s2 or s4 or s5.please help me by explaining that with an example. your help is much appreciated.thanks, ravi",
    "present_kp": [],
    "absent_kp": [
      "algorithms",
      "bioinformatics"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "adding complexity by generalising: how far should you go?. reference question: <url> above question asked to solve a problem for an nxn matrix. while there was an easy solution, i gave a more general solution to solve the more general problem for an nxm matrix. a handful of people commented that this generalisation was bad because it made the solution more complex. one such comment is voted +8.putting aside the hard-to-explain voting effects on so, there are two types of complexity to be considered here:runtime complexity, i.e. how fast does the code runcode complexity, i.e. how difficult is the code to read and understandthe question of runtime complexity is something that requires a better understanding of the input data today and what it might look like in the future, taking the various growth factors into account where necessary.the question of code complexity is the one i'm interested in here. by generalising the solution, we avoid having to rewrite it in the event that the constraints change. however, at the same time it can often result in complicating the code. in the reference question, the code for nxn is easy to understand for any competent programmer, but the nxm case (unless documented well) could easily confuse someone coming across the code for the first time.so, my question is this: where should you draw the line between generalising and keeping the code easy to understand?",
    "present_kp": [
      "complexity"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "which should be done first: use cases or user stories?. i've heard both about use cases (i'm talking about the description, not the diagram) and user stories being used to gather requirements and organize them better.i work alone, so i'm just trying to find the best way to organize requirements, to understand what has to be done in the development. i don't have, nor need, any formal methodologies with huge documents and so forth.user stories i've seem being used to build the product backlog, which contains everything that needs to be done in the development.use cases, on the other hand, provide a description of how things are done in the system, the flow of interaction between external actors and the system.it seems to me that for one use case there are several user stories.this leads me to the following question: when discovering requirements, what should i do first? find and write user stories or find and write use cases? or they should be done at the same time somehow?i'm actually quite confused. regarding use cases and user stories, for a developer who works alone, what is a good workflow, to use these methodologies correctly in order to have a better development?",
    "present_kp": [
      "requirements",
      "use case"
    ],
    "absent_kp": [
      "agile",
      "user story"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "issues installing ruby without sudo. i am migrating a website to a server that is running ubuntu 14.04.2 lts, the website is managed through git and built using jekyll. jekyll is installed as ruby gem and unfortunately the server does not have ruby installed.i attempted to install ruby to my user folder (i am not a sudoer)./configure --prefix=$home/bin/ruby && make && make installand it appears to install fine, with the one warning:skip installing bundle gems because of lacking zlibwhen i execute the ruby executable, i get the following errors: $./ruby system --upgrade ./ruby: no such file or directory -- system (loaderror)$./tmp/ruby-2.2.3/bin/gem install jekyll/usr/bin/env: ruby: permission denied$ ./bin/ruby/bin/gem install jekyllerror: loading command: install (loaderror) cannot load such file -- zliberror: while executing gem ... (nomethoderror) undefined method 'invoke_with_build_args' for nil:nilclassi looked into the zlib, but could not find how to install it locally and set ruby to point to it.when trying rvm, i found i was missing the following packages:libreadline6-dev zlib1g-dev libssl-dev libyaml-dev libsqlite3-dev sqlite3 autoconf libgdbm-dev libncurses5-dev automake libtool bison pkg-config libffi-devanyone have some build experience that may help?",
    "present_kp": [
      "ubuntu",
      "ruby"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "asynchronous encryption with wordlists. as i understand it encrypted emails have a high profile (detectable through pgp header / attachment). would it be possible to match the encrypted data with a wordlist to hide the fact that the email contains a encrypted message? i know that the other person would need the same list of words for decrypting and the message would be only a bunch of random words, but the fact that its encrypted world be not as obvious for automated scanning.edit: as pointed out i am referring to steganography to hide from traffic analysis.",
    "present_kp": [
      "encryption"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to delete, without rm/unlink. related to another question i asked here [<url>]i'm wondering if it's possible to bypass the normal rm/rmdir/unlink binaries and delete a file using another method. maybe say by finding out exactly where on the disk a file or folder resides, and writing different data to that location, wiping out the folder or file. how would one go about doing that, and would it cause problems for the os or filesystem?or, is there a .. i hesitate to call it a file but an area of the disk that can be viewed and edited with a hex editor, say, to find and unlink the exact reference to this folder/file? and then, is there a different way of doing that specific to hfs+ and/or os x?edit1: i'm interested not in deleting the contents of a file here, i'm interested in removing a file and/or folder using methods outside of rm/unlink. i'm not trying to get rid of data in a forensic sense, i'm trying to remove a folder that refuses to be removed (see question i linked to above).",
    "present_kp": [],
    "absent_kp": [
      "filesystems",
      "osx"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "rails create create default database records. in my application i have an accounts model:class account < activerecord::base belongs_to :user has_many :invoicesendthe account model belongs to the user model. after the user is created, some default accounts must be created:class user < activerecord::base has_many :accounts after_create do account.create(name: repairs,user: self) account.create(name: supplies, user: self) endendinvoices can then be assigned to one of these initial accounts:class invoice < activerecord::base belongs_to :accountsendis this the proper way to to create the default accounts?",
    "present_kp": [
      "database"
    ],
    "absent_kp": [
      "ruby",
      "ruby on rails"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "removing elements of an 'at least one' association. let's assume - regardless from technology and programming languages - you have a type and the type has an association to another type. this association has the complexity of 'at least one' (1..n). how would you specify the behavior, when removing elements from this association based on a predicate? say, the predicate fits to all elements, thus the removal would violate the 1..n constraint, would you remove all elements but one or would you rather not remove any element in this case (and have a transactional-like behavior)?",
    "present_kp": [],
    "absent_kp": [
      "language agnostic"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "list directories, then files - with single command?. i really don't like how linux ls -al mixes files and directories. is it possible to list directories, then files, with single command? diradirbdircfileafilebfilec",
    "present_kp": [
      "linux",
      "ls"
    ],
    "absent_kp": [
      "shell script"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how does a search functionality fit in ddd with cqrs?. in vaughn vernon's book implementing domain driven design and the accompanying sample application i found that he implemented a cqrs approach to the iddd_collaboration bounded context.he presents the following classes in the application service layer:calendarapplicationservice.java calendarentryapplicationservice.javacalendarentryqueryservice.javacalendarqueryservice.javai'm interested to know if an application will have a search page that feature numerous drop downs and check boxes with a smart text box to match different search patterns; how will you structure all that search logic?in a command service or a query service?taking a look at the calendarqueryservice.java i can see that it has 2 methods for a huge query, but no logic at all to mix and match any search filters for example.i've heard that the application layer shouldn't have any business logic, so where will i construct my dynamic query? or maybe just clutter everything in the query service?",
    "present_kp": [
      "design",
      "domain driven design",
      "cqrs"
    ],
    "absent_kp": [
      "object oriented",
      "architecture"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "return true if the elements of an array do not contain one or the other. i am gradually completing the codingbat exercises for java. here is the one i just did:given an array of ints, return true if it contains no 1's or it contains no 4's. here is my code:public boolean no14(int[] nums) { int onecount = 0; int fourcount = 0; for (int i = 0; i < nums.length; i++) { if (nums[i] == 1) { onecount++; } if (nums[i] == 4) { fourcount++; } } if (onecount > 0 && fourcount > 0) { return false; } return true;}now, i am pretty unfamiliar with arrays, so i would just like to know if there is a simpler/shorter way of finding such information without setting a count for each number? perhaps using a regular expression?",
    "present_kp": [
      "java",
      "array"
    ],
    "absent_kp": [
      "beginner"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "knapsack 01 solution. solution to bounded knapsack 01 problem. once again comprehensive description is difficult in this space, refer here. looking for code review. optimizations and best practices.final class item { private final int value; private final int weight; public item (int value, int weight) { if (value == 0) { throw new illegalargumentexception(the value + value + should be positive.); } if (weight == 0) { throw new illegalargumentexception(the weight + weight + should be positive.); } this.value = value; this.weight = weight; } public int getvalue() { return value; } public int getweight() { return weight; }}public final class knapsack01 { private knapsack01() { } /** * returns the maximum value, given set of items and maxweight * * @param items the set of items. * @param maxweight the max weight * @return the max value obeying the constraints */ public static int getmaxvalue (item[] items, int maxweight) { if (maxweight <= 0) { throw new illegalargumentexception(the maxweight: + maxweight + should be positive); } int[][] knapsack = new int[items.length + 1][maxweight + 1]; for (int item = 0; item <=items.length; item++) { for (int weight = 0; weight <= maxweight; weight++) { if (item == 0 || weight == 0) { continue; } int itemweight = items[item - 1].getweight(); if (weight >= itemweight) { int remainingweight = weight - itemweight; // check the max value upto remainingwieght, the we might have computed before. int valueuptoremainingweight = knapsack[item - 1][remainingweight]; // how about trying currentitem + remainingitem ? int tentativevalue = items[item - 1].getvalue() + valueuptoremainingweight; // compare this against previously calculated max-value int previousmaxvalue = knapsack[item - 1][weight]; knapsack[item][weight] = math.max(tentativevalue, previousmaxvalue); } else { knapsack[item][weight] = knapsack[item - 1][weight]; } } } return knapsack[items.length][maxweight]; } public static void main(string[] args) { item i1 = new item( 60, 10); item i2 = new item(100, 20); item i3 = new item(120, 30); item[] items = new item[3]; items[0] = i1; items[1] = i2; items[2] = i3; assertequals(180, getmaxvalue(items, 40)); assertequals(220, getmaxvalue(items, 50)); assertequals(280, getmaxvalue(items, 60)); assertequals(280, getmaxvalue(items, 60)); }}",
    "present_kp": [],
    "absent_kp": [
      "java",
      "algorithm",
      "dynamic programming",
      "knapsack problem"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to capitalize maximally on location-independence my personal #1-incentive for working as a developer. to me the ultimate beauty in working as a developer is the fact that given a nice cv, your are going to find a new job, everywhere at any time.so i would like to ask if somebody here as experience in working while travelling for example. or job-hopping from metropolis to metropolis every, say, six months.for example i have been investigating for how to get to brazil. but it seems like that working as an employee in brazil would be no option, b/c it takes a lot of time/money/effort to get the proper visas and permissions. so the only practical solution would be to freelance and the just travel, while getting your job done wherever you are.i bet my ass that there are loads of it-guys out there and on here who know exactly what i am talking about.i'm looking forward to interesting ideas and stories.edit for the bounty:i am not so much intersted in general wisdoms but rather in concrete accounts of personal experiences addressing the subject from people who can relate to my question and do have actual personal experiences to share. i am not asking for opinions and accounts of second-degree nature.edit for everybody (concrete questions):where do you work while travelling? (office pooling? libraries? cafs?)where do you sleep? i guess hotels are somewhat to expensive. (hostels? couchsurfing?)editi accepted andy's reply as the answer mainly b/c of its romantic and positive undertone. though of course there is not the answer to that question. i was hoping for an intriguing discussion and given 11 vote ups and 5 bookmarks i seem to be not the only one who is interested in some input.so i hope some more people chime in and share their experiences.",
    "present_kp": [],
    "absent_kp": [
      "freelancing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "user generated articles, how to do meta description?. if users submit a lot of good quality articles on the site, what is the best way to approach the meta description tag?i see two options:have a description box and rely on them to fill it sensibly and in a good quality wayjust exclude the meta descriptionmethod 1 is bad initially, but i'm willing to put time in going through and editing/checking all of them on a permanent basis.method 2 is employed by the stack exchange site, and lets the search bots extract the best part of the page in the serp.thoughts? ideas? i'm thinking a badly formed description tag is more damaging than not having one at all at the end of the day.i don't expect content to ever become unwieldy and too much to manage.",
    "present_kp": [
      "meta description",
      "articles"
    ],
    "absent_kp": [
      "user engagement",
      "user generated content"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "get to command line from arch cinnamon. i have a new arch install with gnome window manager and cinnamon. i created ~/.xinitrc with the command start cinnamon, and restarted the computer. now the system boots into the login screen for gnome and cinnamon, but there is no command line shell available in the guis, and i'm unable to boot the computer into command line mode either.",
    "present_kp": [
      "command line",
      "gnome",
      "cinnamon"
    ],
    "absent_kp": [
      "arch linux"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is a microframework?. why are some frameworks (e.g. flask for python, sinatra for ruby) called microframeworks?what differentiates them from full-fledged frameworks, like django or rails?",
    "present_kp": [
      "frameworks"
    ],
    "absent_kp": [
      "terminology"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why is the folder name bin used in some frameworks and languages?. i have been learning java. and still after a prolonged time i don't know why the name of the folder is bin where one find all the tools for java?is there is any logical reason behind that?i have also noticed the same in .net framework also.",
    "present_kp": [
      "java"
    ],
    "absent_kp": [
      "terminology"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is the [pid] of kernel panic the killer of kernel panic?. the following kernel panic occurred on my embedded board.i am using 3.10 kernel.i am analyzing the cause of the kernel panic.the kernel panic message shows the pid(735).feb 22 19:40:28 test kenel: cpu: 0 'pid: 735' comm: cat not tainted 3.10.73 #2feb 22 19:40:28 test kernel: process cat '(pid: 735', stack limit = 0xee46c238)does the pid mean the killer of the kernel panic?below is my entire kernel panic message.kernel panic seems to be a really hard friend.... :(feb 22 19:40:28 test kernel: unable to handle kernel null pointer dereference at virtual address 0000000cfeb 22 19:40:28 test kernel: pgd = c0004000feb 22 19:40:28 test kernel: [0000000c] *pgd=00000000feb 22 19:40:28 test kernel: internal error: oops: 17 [#1] preempt smp armfeb 22 19:40:28 test kernel: modules linked in: m2i_trn g_m2i libcomposite iptable_filter ip_tables smsc95xx usbnet mousedev rh_special_switches pn5xx_i2c mram tsc2007 spicc buzzer cover backlight device_info [last unloaded: spi_fieldbus]feb 22 19:40:28 test kernel: cpu: 0 pid: 735 comm: cat not tainted 3.10.73 #2feb 22 19:40:28 test kernel: task: ee404000 ti: ee46c000 task.ti: ee46c000feb 22 19:40:28 test kernel: pc is at set_page_dirty+0x20/0x68feb 22 19:40:28 test kernel: lr is at set_page_dirty+0xc/0x68feb 22 19:40:28 test kernel: pc : [<c00a3c50>] lr : [<c00a3c3c>] psr: a0000013feb 22 19:40:28 test kernel: sp : ee46de48 ip : 0002f9f9 fp : b6ffa000feb 22 19:40:28 test kernel: r10: ee46dee0 r9 : ee46de84 r8 : b6ffc000feb 22 19:40:28 test kernel: r7 : c0fd3860 r6 : <phone> r5 : c0fd3860 r4 : ee3d87ecfeb 22 19:40:28 test kernel: r3 : <phone> r2 : 40080068 r1 : c0fd3860 r0 : 00000012feb 22 19:40:28 test kernel: flags: nzcv irqs on fiqs on mode svc_32 isa arm segment userfeb 22 19:40:28 test kernel: control: 10c5387d table: 2eb3c04a dac: 00000015feb 22 19:40:28 test kernel: feb 22 19:40:28 test kernel: pc: 0xc00a3bd0:feb 22 19:40:28 test kernel: 3bd0 e2505000 1a000001 e1a00005 e8bd8038 e375001c e2841048 03a0001a 13a00019feb 22 19:40:28 test kernel: 3bf0 eb0456e6 eafffff7 e92d4010 e1a04000 eb002f48 e590304c e5933010 e3130c02feb 22 19:40:28 test kernel: 3c10 08bd8010 e5943000 e3130a02 08bd8010 e1a00004 e3a0100d e8bd4010 eaffdee6feb 22 19:40:28 test kernel: 3c30 e92d4038 e1a05000 eb002f3a e3500000 0a00000a e5903044 e1a01005 e3a00012feb 22 19:40:28 test kernel: 3c50 e593400c eb044d0a e59f3034 e1a00005 e3540000 01a04003 e12fff34 e8bd8038feb 22 19:40:28 test kernel: 3c70 e5953000 e3130010 18bd8038 e1a01005 e3a00004 eb045711 e2700001 33a00000feb 22 19:40:28 test kernel: 3c90 e8bd8038 c00fd924 e92d4030 e1a04000 e24dd00c eb002f1f e5943000 e1a05000feb 22 19:40:28 test kernel: 3cb0 e3130001 0a00002b e3500000 0a000003 e590304c e5933010 e3130001 0a000004feb 22 19:40:28 test kernel: feb 22 19:40:28 test kernel: lr: 0xc00a3bbc:feb 22 19:40:28 test kernel: 3bbc e92d4038 e1a04002 e5923044 e5933000 e12fff33 e2505000 1a000001 e1a00005feb 22 19:40:28 test kernel: 3bdc e8bd8038 e375001c e2841048 03a0001a 13a00019 eb0456e6 eafffff7 e92d4010feb 22 19:40:28 test kernel: 3bfc e1a04000 eb002f48 e590304c e5933010 e3130c02 08bd8010 e5943000 e3130a02feb 22 19:40:28 test kernel: 3c1c 08bd8010 e1a00004 e3a0100d e8bd4010 eaffdee6 e92d4038 e1a05000 eb002f3afeb 22 19:40:28 test kernel: 3c3c e3500000 0a00000a e5903044 e1a01005 e3a00012 e593400c eb044d0a e59f3034feb 22 19:40:28 test kernel: 3c5c e1a00005 e3540000 01a04003 e12fff34 e8bd8038 e5953000 e3130010 18bd8038feb 22 19:40:28 test kernel: 3c7c e1a01005 e3a00004 eb045711 e2700001 33a00000 e8bd8038 c00fd924 e92d4030feb 22 19:40:28 test kernel: 3c9c e1a04000 e24dd00c eb002f1f e5943000 e1a05000 e3130001 0a00002b e3500000feb 22 19:40:28 test kernel: feb 22 19:40:28 test kernel: sp: 0xee46ddc8:feb 22 19:40:28 test kernel: ddc8 000000a8 0000006d b6f6a000 c105ace0 <phone> ffffffff 0000006d c06c8f40feb 22 19:40:28 test kernel: dde8 c00a3c50 a0000013 ffffffff ee46de34 b6ffc000 c000dd98 <phone> c0fd3860feb 22 19:40:28 test kernel: de08 400<phone> ee3d87ec c0fd<phone> c0fd3860 b6ffc000 ee46de84feb 22 19:40:28 test kernel: de28 ee46dee0 b6ffa000 0002f9f9 ee46de48 c00a3c3c c00a3c50 a0000013 fffffffffeb 22 19:40:28 test kernel: de48 326c375f ee3d87ec ee3d87e8 c00b8db0 ee46de58 ee785c00 c0f51b1c edef6f60feb 22 19:40:28 test kernel: de68 ee93edb8 326c375f ee46c000 ee93edb8 b6ffc000 b6ffbfff 600<phone>feb 22 19:40:28 test kernel: de88 fffffffe <phone> ee46c000 edef6f60 ffffffff ee46dee0 <phone> 00000000feb 22 19:40:28 test kernel: dea8 ee46c000 ee785c40 <phone> c00b98c0 <phone> ee46c000 edef6c60 ee46df08feb 22 19:40:28 test kernel: feb 22 19:40:28 test kernel: r1: 0xc0fd37e0:feb 22 19:40:28 test kernel: 37e0 40080068 ee9<phone>ea <phone> <phone> c0fd37d4 c0fd3a34 00000000feb 22 19:40:28 test kernel: 3800 40080068 ee9<phone>e2 <phone> <phone> c0f94cf4 c0fd<phone>feb 22 19:40:28 test kernel: 3820 40080068 ee9<phone>da <phone> <phone> c0fd3694 c0fd<phone>feb 22 19:40:28 test kernel: 3840 4002002c ef074d64 <phone> ffffffff <phone> c0f92af4 c0f8ed94 00000000feb 22 19:40:28 test kernel: 3860 40080068 ee97a400 000b6ffa <phone> <phone> c0fbbeb4 c0f<phone>feb 22 19:40:28 test kernel: 3880 4002002c ef074d64 <phone> ffffffff <phone> c0fa6034 c0f<phone>feb 22 19:40:28 test kernel: 38a0 400<phone> <phone> ffffffff <phone> c0fc8a14 c0f<phone>feb 22 19:40:28 test kernel: 38c0 40080068 ee9<phone> <phone> <phone> c0fd8d54 c0f92d34 00000000feb 22 19:40:28 test kernel: feb 22 19:40:28 test kernel: r4: 0xee3d876c:feb 22 19:40:28 test kernel: 876c <phone> <phone> <phone> <phone> <phone> <phone> <phone> 00000000feb 22 19:40:28 test kernel: 878c <phone> <phone> <phone> <phone> <phone> <phone> <phone> 00000000feb 22 19:40:28 test kernel: 87ac <phone> <phone> <phone> <phone> <phone> <phone> <phone> 00000000feb 22 19:40:28 test kernel: 87cc <phone> <phone> <phone> <phone> <phone> <phone> <phone> 00000000feb 22 19:40:28 test kernel: 87ec 305b375f <phone> 31af57df 305d375f <phone> <phone> <phone> 00000000feb 22 19:40:28 test kernel: 880c <phone> <phone> <phone> <phone> <phone> <phone> <phone> 00000000feb 22 19:40:28 test kernel: 882c <phone> <phone> <phone> <phone> <phone> <phone> <phone> 00000000feb 22 19:40:28 test kernel: 884c <phone> <phone> <phone> <phone> <phone> <phone> <phone> 00000000feb 22 19:40:28 test kernel: feb 22 19:40:28 test kernel: r5: 0xc0fd37e0:feb 22 19:40:28 test kernel: 37e0 40080068 ee9<phone>ea <phone> <phone> c0fd37d4 c0fd3a34 00000000feb 22 19:40:28 test kernel: 3800 40080068 ee9<phone>e2 <phone> <phone> c0f94cf4 c0fd<phone>feb 22 19:40:28 test kernel: 3820 40080068 ee9<phone>da <phone> <phone> c0fd3694 c0fd<phone>feb 22 19:40:28 test kernel: 3840 4002002c ef074d64 <phone> ffffffff <phone> c0f92af4 c0f8ed94 00000000feb 22 19:40:28 test kernel: 3860 40080068 ee97a400 000b6ffa <phone> <phone> c0fbbeb4 c0f<phone>feb 22 19:40:28 test kernel: 3880 4002002c ef074d64 <phone> ffffffff <phone> c0fa6034 c0f<phone>feb 22 19:40:28 test kernel: 38a0 400<phone> <phone> ffffffff <phone> c0fc8a14 c0f<phone>feb 22 19:40:28 test kernel: 38c0 40080068 ee9<phone> <phone> <phone> c0fd8d54 c0f92d34 00000000feb 22 19:40:28 test kernel: feb 22 19:40:28 test kernel: r7: 0xc0fd37e0:feb 22 19:40:28 test kernel: 37e0 40080068 ee9<phone>ea <phone> <phone> c0fd37d4 c0fd3a34 00000000feb 22 19:40:28 test kernel: 3800 40080068 ee9<phone>e2 <phone> <phone> c0f94cf4 c0fd<phone>feb 22 19:40:28 test kernel: 3820 40080068 ee9<phone>da <phone> <phone> c0fd3694 c0fd<phone>feb 22 19:40:28 test kernel: 3840 4002002c ef074d64 <phone> ffffffff <phone> c0f92af4 c0f8ed94 00000000feb 22 19:40:28 test kernel: 3860 40080068 ee97a400 000b6ffa <phone> <phone> c0fbbeb4 c0f<phone>feb 22 19:40:28 test kernel: 3880 4002002c ef074d64 <phone> ffffffff <phone> c0fa6034 c0f<phone>feb 22 19:40:28 test kernel: 38a0 400<phone> <phone> ffffffff <phone> c0fc8a14 c0f<phone>feb 22 19:40:28 test kernel: 38c0 40080068 ee9<phone> <phone> <phone> c0fd8d54 c0f92d34 00000000feb 22 19:40:28 test kernel: feb 22 19:40:28 test kernel: r9: 0xee46de04:feb 22 19:40:28 test kernel: de04 c0fd<phone> <phone> ee3d87ec c0fd<phone> c0fd3860 b6ffc000feb 22 19:40:28 test kernel: de24 ee46de84 ee46dee0 b6ffa000 0002f9f9 ee46de48 c00a3c3c c00a3c50 a0000013feb 22 19:40:28 test kernel: de44 ffffffff 326c375f ee3d87ec ee3d87e8 c00b8db0 ee46de58 ee785c00 c0f51b1cfeb 22 19:40:28 test kernel: de64 edef6f60 ee93edb8 326c375f ee46c000 ee93edb8 b6ffc000 b6ffbfff 60000013feb 22 19:40:28 test kernel: de84 <phone> fffffffe <phone> ee46c000 edef6f60 ffffffff ee46dee0 00000000feb 22 19:40:28 test kernel: dea4 <phone> ee46c000 ee785c40 <phone> c00b98c0 <phone> ee46c000 edef6c60feb 22 19:40:28 test kernel: dec4 ee46df08 ee785c00 ee785c00 ee46c000 c00bf4ec c0f696e0 0025ceb0 ee785c00feb 22 19:40:28 test kernel: dee4 <phone> <phone> <phone> ffffffff c045ec74 <phone> <phone> 00000400feb 22 19:40:28 test kernel: feb 22 19:40:28 test kernel: r10: 0xee46de60:feb 22 19:40:28 test kernel: de60 c0f51b1c edef6f60 ee93edb8 326c375f ee46c000 ee93edb8 b6ffc000 b6ffbffffeb 22 19:40:28 test kernel: de80 600<phone> fffffffe <phone> ee46c000 edef6f60 ffffffff ee46dee0feb 22 19:40:28 test kernel: dea0 <phone> <phone> ee46c000 ee785c40 <phone> c00b98c0 <phone> ee46c000feb 22 19:40:28 test kernel: dec0 edef6c60 ee46df08 ee785c00 ee785c00 ee46c000 c00bf4ec c0f696e0 0025ceb0feb 22 19:40:28 test kernel: dee0 ee785c00 <phone> <phone> <phone> ffffffff c045ec74 <phone> 00000072feb 22 19:40:28 test kernel: df00 <phone> ee9<phone> c045ec74 <phone> c045e9dc <phone> c045e7f8feb 22 19:40:28 test kernel: df20 <phone> c045ec74 ee785c30 ee785c00 <phone> ee404000 ee785c00 c0029360feb 22 19:40:28 test kernel: df40 ee4042fc c00302a0 <phone> <phone> c0680eb4 <phone> <phone> c004fa88feb 22 19:40:28 test kernel: process cat (pid: 735, stack limit = 0xee46c238)feb 22 19:40:28 test kernel: stack: (0xee46de48 to 0xee46e000)feb 22 19:40:28 test kernel: de40: 326c375f ee3d87ec ee3d87e8 c00b8db0 ee46de58 ee785c00feb 22 19:40:28 test kernel: de60: c0f51b1c edef6f60 ee93edb8 326c375f ee46c000 ee93edb8 b6ffc000 b6ffbffffeb 22 19:40:28 test kernel: de80: 600<phone> fffffffe <phone> ee46c000 edef6f60 ffffffff ee46dee0feb 22 19:40:28 test kernel: dea0: <phone> <phone> ee46c000 ee785c40 <phone> c00b98c0 <phone> ee46c000feb 22 19:40:28 test kernel: dec0: edef6c60 ee46df08 ee785c00 ee785c00 ee46c000 c00bf4ec c0f696e0 0025ceb0feb 22 19:40:28 test kernel: dee0: ee785c00 <phone> <phone> <phone> ffffffff c045ec74 <phone> 00000072feb 22 19:40:28 test kernel: df00: <phone> ee9<phone> c045ec74 <phone> c045e9dc <phone> c045e7f8feb 22 19:40:28 test kernel: df20: <phone> c045ec74 ee785c30 ee785c00 <phone> ee404000 ee785c00 c0029360feb 22 19:40:28 test kernel: df40: ee4042fc c00302a0 <phone> <phone> c0680eb4 <phone> <phone> c004fa88feb 22 19:40:28 test kernel: df60: c0680eb4 ef391d40 <phone> ee46c000 000000f8 c000e3e8 ee46c000 00000000feb 22 19:40:28 test kernel: df80: <phone> c0030a20 <phone> 000700de b6fd9760 b6fd<phone>f8 c0030aa4feb 22 19:40:28 test kernel: dfa0: <phone> c000e240 000700de b6fd<phone> 000700ca b6ff84c0 00000000feb 22 19:40:28 test kernel: dfc0: 000700de b6fd9760 b6fd<phone>f8 <phone> <phone> <phone> 00000000feb 22 19:40:28 test kernel: dfe0: 000000f8 be8b5474 b6f6afc3 b6f<phone> <phone> <phone> 00000000feb 22 19:40:28 test kernel: [<c00a3c50>] (set_page_dirty+0x20/0x68) from [<c00b8db0>] (unmap_single_vma+0x4d0/0x63c)feb 22 19:40:28 test kernel: [<c00b8db0>] (unmap_single_vma+0x4d0/0x63c) from [<c00b98c0>] (unmap_vmas+0x54/0x68)feb 22 19:40:28 test kernel: [<c00b98c0>] (unmap_vmas+0x54/0x68) from [<c00bf4ec>] (exit_mmap+0xd8/0x1f8)feb 22 19:40:28 test kernel: [<c00bf4ec>] (exit_mmap+0xd8/0x1f8) from [<c0029360>] (mmput+0x48/0xf4)feb 22 19:40:28 test kernel: [<c0029360>] (mmput+0x48/0xf4) from [<c00302a0>] (do_exit+0x244/0x878)feb 22 19:40:28 test kernel: [<c00302a0>] (do_exit+0x244/0x878) from [<c0030a20>] (do_group_exit+0x3c/0xb0)feb 22 19:40:28 test kernel: [<c0030a20>] (do_group_exit+0x3c/0xb0) from [<c0030aa4>] (__wake_up_parent+0x0/0x18)feb 22 19:40:28 test kernel: code: 0a00000a e5903044 e1a01005 e3a00012 (e593400c) feb 22 19:40:28 test kernel: ---[ end trace 28aa0afec355b4f9 ]---feb 22 19:40:28 test kernel: fixing recursive fault but reboot is needed!",
    "present_kp": [
      "kernel",
      "kernel panic"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "assertions vs exceptions - is my understanding of the differences between the two correct?. design by contract uses preconditions and postconditions of the public methods in a class together to form a contract between the class and its clients.a) in code we implement preconditions and postconditions either as assertions or as exceptions? b) we implement preconditions and postconditions in code as exceptions if not fulfilling preconditions or postconditions doesn't indicate logically impossible situations or programming errors?c) we implement them in code as assertions when not fulfilling preconditions or postconditions does indicate logically impossible situations or programming errors?d) should preconditions and postconditions only be defined on public methods?edit:aren't the following checks considered to be part of a normal operation ( and as i already mentioned, i've seen plenty of articles on dbc using similar examples for preconditions, where checks were made against arguments supplied by the user ), since if the user enters bad data, then without checks operation won't be rejected and as such system will stop working according to specs:link:public user getuserwithidof(int id, userrepository userrepository) { // pre-conditions if (userrepository == null) throw new argumentnullexception( userrepository); if (id <= 0) throw new argumentoutofrangeexception( id must be > 0); user founduser = userrepository.getbyid(id); // post-conditions if (founduser == null) throw new keynotfoundexception(no user with + an id of + id.tostring() + could be located.); return founduser;}",
    "present_kp": [
      "exceptions",
      "assertions"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "website access...dns, isp, issue?. this isn't so much a code issue as it might be an issue with my isp. for some reason when i visit a site very often, like one i manage or write stories on, it will just stop pulling data down after a while. it's very random when it happens, but probably happens once a week. if effects everyone who is accessing the site from this connection, and i can access other sites no problem. also, if i go outside the office back home, which is right down the street, and access the site it is fine. i'm using comcast in both locations.it's almost as if i have a limit on requests to each site and have hit my limit so it blocks the site for a while.anybody have any clue what this might be?",
    "present_kp": [
      "dns",
      "isp"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "networkmanager tries to connect to previous network after suspend, even if the network isn't there. at home i am connected to network 'a' and can see wlans 'b' and 'c' from my neighbours. i suspend my computer and when i get to work the next day, the system still thinks it's connected to the same network 'a' and that it still can see the 'b' and 'c' networks, even though they aren't there. it will stay there forever trying to connect to the 'a' network, until it's stopped and i select the correct network.it is not a major problem (i can just open the list of networks and connect to the correct one), but is quite annoying.it is like (but is different) this bug: <url> (in the bug, the system connects to a different wlan even though the previous one is still available; with my case it's the opposite).i think it's a networkmanager problem, because i never saw it when i used wicd. what can it be?i'm using x86-64 arch linux, nm 0.9.6.4, intel wireless 1030 card (iwlagn module).",
    "present_kp": [
      "networkmanager"
    ],
    "absent_kp": [
      "wifi"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "web development: no local server workflow. i'm considering the reengineering of my web development ecosystem. we use git very successfully to deploy new changes to our production, staging, and development servers. traditionally, i've always had a copy of apache on my local machine - in the interest of keeping my local machine as lean as possible, i'm considering running apache in a non-local vm and syncing changes somehow.my initial thought was to run something like rsync and cron (mac development system) to monitor the directory for file changes and then sync them up. i have been tempted to use a separate branch in git and writing a hook to have the apache server pull in the changes, but versioning all of my changes may be unproductive, particularly on very experimental additions.my question is, is there something i'm not considering in this workflow? this particular project requires coldfusion, so am i going to realize significant benefits over not having apache/cfide locally?",
    "present_kp": [
      "web development"
    ],
    "absent_kp": [
      "workflows"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "bringing images into alignment. i observe a scene with two cameras, c1 and c2, that produce two images i1 and i2, respectively. what i now want is to bring i1 and i2 into alignment, that is i want to know where pixel (x,y) in i1 is in i2. this alignment-matrix should be valid in general, that is not only for i1 and i2, but also for further images of the cameras c1 and c2 as long as they haven't been moved. i believe this problem is called correspondence problem. i have now read quite a bit about camera calibration, image rectification, essential and fundamental matrix. but there are still open questions and i would not know how to achieve my task.so, the question basically is:the essential matrix e and the fundamental matrix f both only give constraints for the correspondence problem (point p in i1 must lie on line l in i2). how do i actually solve the problem?thanks a lot for your time and answers!",
    "present_kp": [],
    "absent_kp": [
      "computational geometry",
      "image processing",
      "computer vision"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "s.gpg-agent.browser not found in debian strech while adding ppa. i do not know about s.gpg-agent.browser error while adding ppa repository.can i get more information for solve this?add-apt-repository ppa:webupd8team/java (or another repository)gpg: keybox '/tmp/tmph1zsbhtw/pubring.gpg' createdgpg: /tmp/tmph1zsbhtw/trustdb.gpg: trustdb createdgpg: key c2518248eea14886: public key launchpad vlc importedgpg: no ultimately trusted keys foundgpg: total number processed: 1gpg: imported: 1gpg: no valid openpgp data found.exception in thread thread-1:traceback (most recent call last): file /usr/lib/python3.5/threading.py, line 914, in _bootstrap_inner self.run() file /usr/lib/python3.5/threading.py, line 862, in run self._target(*self._args, **self._kwargs) file /usr/lib/python3/dist-packages/softwareproperties/softwareproperties.py, line 688, in addkey_func func(**kwargs) file /usr/lib/python3/dist-packages/softwareproperties/ppa.py, line 386, in add_key return apsk.add_ppa_signing_key() file /usr/lib/python3/dist-packages/softwareproperties/ppa.py, line 273, in add_ppa_signing_key cleanup(tmp_keyring_dir) file /usr/lib/python3/dist-packages/softwareproperties/ppa.py, line 234, in cleanup shutil.rmtree(tmp_keyring_dir) file /usr/lib/python3.5/shutil.py, line 480, in rmtree _rmtree_safe_fd(fd, path, onerror) file /usr/lib/python3.5/shutil.py, line 438, in _rmtree_safe_fd onerror(os.unlink, fullname, sys.exc_info()) file /usr/lib/python3.5/shutil.py, line 436, in _rmtree_safe_fd os.unlink(name, dir_fd=topfd)filenotfounderror: [errno 2] no such file or directory: 's.gpg-agent.browser'",
    "present_kp": [
      "debian",
      "ppa"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "what factors influence you to try out a new framework or tool?. i'm in the process of putting the final touches on an open-source framework that i hope to release in the next few months. it's something that i'd like to develop a community for and so i'm curious about what factors influence your decision to use a new framework or tool and why. some of the specific things i'd like to know more about (feel free to add to this):types of documentation/tutorials/instructioncommunity support (comments/forum)updates (blog/social media/feeds)look and feel of the project website designwhite papers/testimonialsa big feature listcommunity sizetoolsability to contributeproject test coverage (stability/security)level of buzz (recommended by friends or around the web)convincing marketing copyideally, i'd like to have all of the above, but what specific features/qualities will carry greater weight in getting programmers to adopt something new? what says, 'this is a professional-grade project,' and what are red flags that keep you from trying it out?",
    "present_kp": [],
    "absent_kp": [
      "open source",
      "frameworks"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "fedora 21 wired internet problem. booted up fedora on my desktop (previously was running w8.1, with also some network problems on restart/wake but usually a reset of the adapter solved them.) no internet on the fedora. here are the outputs of ping 192.168.1.1 (i.e. the router), ifconfig, ping <url>, ping a google ip and netstatnotice the extremely high packet loss on the pings. what could be wrong, what to try?",
    "present_kp": [
      "fedora"
    ],
    "absent_kp": [
      "networking"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to have a discussion in trello similar to the ones in teambox. one cool thing about teambox is that you have a dedicated discussion page. whats the best way to have a discussion in trello?",
    "present_kp": [
      "trello"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "prize collecting steiner tree. i'm reading about the prize collecting steiner tree problem and an approximation algorithm that uses randomization to set a lower bound on the optimal solution (see chapter 5.7 in the design of approximation algorithms by williamson and shmoys). i don't understand the second line in the proof for lemma 5.16: .it seems to me that $v-v(t)$ is a much larger set than $u$. so, how can the total penalty for this set be upper bounded by the total penalty of a set that is much smaller?",
    "present_kp": [
      "approximation"
    ],
    "absent_kp": [
      "algorithm analysis",
      "optimization",
      "trees"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "can i test actions on a decision tree (or other classification model)?. disclaimer: i created this question on cross-validated where it is well on the way to earning me the tumbleweed badge, but then i found this site and thought it might be more at home here.i'm using azure ml, and i've produced a basic poc model of student success using a two-class boosted decision tree. one of the features i've included is whether the student has received a particular kind of support. what i'd like to do is test whether providing that support to a particular student will result in an improved success rate.in order to test this, i thought i would run through the details of students who haven't received that support to get the baseline score from the model, then run through the same information but flipping the feature indicating that they've received support, and record the new score.assuming that the support feature is reasonably independent (i.e. it's causing the label, not caused by it - one that i tried to test initially was a contact type that only happened when the students had been absent, so it wasn't appropriate to apply it to other students), is this a valid tactic for evaluating whether we should provide that support to other students in the future? are there any pit-falls i can run in to when performing these kinds of tests?",
    "present_kp": [
      "azure ml"
    ],
    "absent_kp": [
      "predictive modeling",
      "decision trees"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "rsync hardlink attempt copies. i've tried setting up a script to hardlink my files to my box.com account (as it's a backup of my music library). as i want to run it automatically to sync my music across several devices, i wanted to use rsync as i'm on mac os x 10.7.4 (if anyone cares).the script i came up with however only copies the files instead of hardlinking them (the available disc space lowers when i start the script). what i'm trying to achieve is the box.com app syncing something outside its actual folder.i already rtfmed and googled but i can't come up with a proper solution so i hope you guys could help me.this is the script i use:rsync -azluphmt --progress --link-dest=./itunes ./itunes/users/admin/box.com/itunes --delete-during --exclude=*album artwork*",
    "present_kp": [
      "rsync"
    ],
    "absent_kp": [
      "synchronization",
      "hard link"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "changing active directory password for service account in linux. i am really new to but i am trying to use kerberos to authenticate my active directory service account from my own centos 7 box, after which i then proceed to run queries in a remote database using sqlcmd.to give more context, a snippet of my working python code is below:import subprocesskinit = ['kinit', '<email>', '-k', '-t', '/usr/local/var/krb5kdc/serviceaccount.keytab']kinit_cl = subprocess.popen(kinit, stdout=subprocess.pipe)kinit_output = kinit_cl.stdout.read()klist = ['klist','-l']klist_cl = subprocess.popen(klist, stdout=subprocess.pipe)klist_output = klist_cl.stdout.read()print klist_outputif 'keyring:persistent' in klist_output: print service account authenticated #proceed to run sqlcmd queriesmy question relates more to the management of the keytab where my credential is stored, which is great because my password is not stored in the clear. the password expires every 30 days though, and i've noticed that if i use kpasswd to change my password from the command line, it will ask for my old password. this would mean i would have to store it in the clear somewhere and pass it, whereas i am happy not knowing my password and letting a script manage this for me.to generate a new strong password in python wont be an issue, my question is what command can i use to change a password without being prompted for the old password. a command from kadmin.local says the password is changed, but the password accepted is still the old password. kadmin gives me the error below even though i added the service account in as an admin to the local file:[root@osboxes krb5kdc]# kadmin.local -q addprinc serviceaccount/adminauthenticating as principal serviceaccount/<email> with password.warning: no policy specified for serviceaccount/<email> defaulting to no policyenter password for principal serviceaccount/<email> re-enter password for principal serviceaccount/<email> principal serviceaccount/<email> created.[root@osboxes krb5kdc]# kinit serviceaccount/adminkinit: client 'serviceaccount/<email>' not found in kerberos database while getting initial credentials[root@osboxes krb5kdc]# kadmin serviceaccountkadmin: client 'serviceaccount/<email>' not found in kerberos database while initializing kadmin interfacedo i need to do this only through kadmin? or else how would i script this?",
    "present_kp": [
      "centos",
      "python",
      "password",
      "active directory",
      "kerberos"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "is there a name for this relation on cfgs?. i'm looking for the name (or a name if there isn't one already) of this relation between $g_1=\\left<\\sigma_1,\\mathcal{n}_1,\\mathcal{r}_1,s_1 ight>$ and $g_2=\\left<\\sigma_2,\\mathcal{n}_2,\\mathcal{r}_2,s_2 ight>$:$$\\exists f_\\sigma\\in\\sigma_1 imes\\sigma_2,f_\\mathcal{n}\\in\\mathcal{n_1} imes\\mathcal{n}_2 ext{ surjective functions s.t. }\\ \\mathcal{r}_2=\\left\\{f_\\mathcal{n}(n) ightarrow f(lpha)\\mid n ightarrowlpha\\in r_1 ight\\} ext{ and } s_2=f_\\mathcal{n}(s_1) $$where $f$ is the extension of $f_\\sigma$ and $f_\\mathcal{n}$ to words ($f\\in(\\sigma_1\\cup\\mathcal{n}_1)^* imes(\\sigma_2\\cup\\mathcal{n}_2)^*$).in other words, you can map $g_1$ to $g_2$ (but not necessarily $g_2$ to $g_1$).if the mapping function were bijective, this would be an isomorphism, but they're not.what about if only $f_\\mathcal{n}$ is surjective, i.e. $f_\\sigma$ is bijective, or even the identity function?",
    "present_kp": [],
    "absent_kp": [
      "terminology",
      "formal grammars",
      "context free"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to make svn save credentials when --non-interactive. i'm trying to get svn to save my https username+password to ~/.subversion from within an automated script. i can pass creds on the command-line but i do not want to be prompted about whether to save the password unencrypted. unfortunately this does not create ~/.subversion/:svn --non-interactive --trust-server-cert --username myusername --password secret co <url> i'm trying to do this for a dockerfile that invokes a bower install with a bower.json that references a password-protected svn repo. unfortunately there's no way to pass the svn credentials to bower via command-line or environment. i am currently working around it by running svn interactively and letting it create ~/.subversion, then zipping up that entire directory and adding it in the dockerfile. i guess i could look at the file formats in ~/.subversion and create it with a script, but would rather let svn do it.",
    "present_kp": [
      "subversion"
    ],
    "absent_kp": [
      "shell script"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "are free will and motivation related?. i'm aware of numerous experiments, like stanford prison experiment, milgram experiment which indicate that humans can act in ways opposite to their best intentions. for the purposes of this question, let's define free will as ability to act and strive without external coercion. ability to accomplish my goals and ambitions. in this context free will suddenly seems a lot like motivation. found these quotes:likewise, compatibilists define free will as freedom to act according to one's determined motives without hindrance from other individualsthis makes me ask - is free will related or is just another word for a more modern concept of motivation?",
    "present_kp": [
      "motivation",
      "free will"
    ],
    "absent_kp": [
      "cognitive psychology",
      "cognitive modeling",
      "behavior"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "secure custom password hashing. my team and i have ended up creating this class, which is called directly from asp.net identity as a custom password hasher. i'd like to know whether this would be overkill/use a lot of cpu, specially because the site is going to be hosted in azure.to take into account: hashpassword is called when a new user is being created by identityverifyhashedpassword is called when a user logs inprivate const int saltbytelength = 16;private const int derivedkeylength = 20;private const int miniterationcount = 44000;private const int maxiterationcount = 50000;public string hashpassword(string password){ return createsecurepasswordhash(password);}public passwordverificationresult verifyhashedpassword(string hashedpassword, string providedpassword){ providedpassword = convert.tobase64string(computehash(providedpassword)); bool result = comparepasswordhashes(providedpassword, hashedpassword); return result ? passwordverificationresult.success : passwordverificationresult.failed;}private static byte[] computehash(string password){ using (memorystream ms = new memorystream()) using (streamwriter sw = new streamwriter(ms)) { sw.write(password); sw.flush(); ms.position = 0; using (sha512cryptoserviceprovider provider = new sha512cryptoserviceprovider()) return provider.computehash(ms); }}private static string createsecurepasswordhash(string password){ byte[] hashedpassword = computehash(password); byte[] salt = generatesecuresalt(); random rand = new random(); int iterationcount = rand.next(miniterationcount, maxiterationcount); byte[] hashvalue = generatesecurehashvalue(hashedpassword, salt, iterationcount); byte[] iterationcountbtyearr = bitconverter.getbytes(iterationcount); byte[] valuetosave = new byte[saltbytelength + derivedkeylength + iterationcountbtyearr.length]; buffer.blockcopy(salt, 0, valuetosave, 0, saltbytelength); buffer.blockcopy(hashvalue, 0, valuetosave, saltbytelength, derivedkeylength); buffer.blockcopy(iterationcountbtyearr, 0, valuetosave, salt.length + hashvalue.length, iterationcountbtyearr.length); return convert.tobase64string(valuetosave);}private static byte[] generatesecuresalt(){ using (rngcryptoserviceprovider rngcsp = new rngcryptoserviceprovider()) { byte[] salt = new byte[saltbytelength]; rngcsp.getbytes(salt); return salt; }}private static byte[] generatesecurehashvalue(byte[] password, byte[] salt, int iterationcount){ using (var pbkdf2 = new rfc2898derivebytes(password, salt, iterationcount)) { return pbkdf2.getbytes(derivedkeylength); }}private static bool comparepasswordhashes(string guess, string saved){ if (string.isnullorempty(guess) || string.isnullorempty(saved)) return false; byte[] passwordguess = convert.frombase64string(guess); byte[] savedpassword = convert.frombase64string(saved); byte[] salt = new byte[saltbytelength]; byte[] actualpasswordbytearr = new byte[derivedkeylength]; int iterationcount = savedpassword.length - (salt.length + actualpasswordbytearr.length); byte[] iterationcountbytearr = new byte[iterationcount]; buffer.blockcopy(savedpassword, 0, salt, 0, saltbytelength); buffer.blockcopy(savedpassword, saltbytelength, actualpasswordbytearr, 0, actualpasswordbytearr.length); buffer.blockcopy(savedpassword, (salt.length + actualpasswordbytearr.length), iterationcountbytearr, 0, iterationcount); byte[] passwordguessbytearr = generatesecurehashvalue(passwordguess, salt, bitconverter.toint32(iterationcountbytearr, 0)); return constanttimecomparison(passwordguessbytearr, actualpasswordbytearr);}private static bool constanttimecomparison(byte[] passwordguesshash, byte[] savedhash){ uint difference = (uint)passwordguesshash.length ^ (uint)savedhash.length; for (var i = 0; i < passwordguesshash.length && i < savedhash.length; i++) { difference |= (uint)(passwordguesshash[i] ^ savedhash[i]); } return difference == 0;}all connections to my website are done strictly through tls 1.2. also, in the application where this code is used, absolutely no information about the user is stored, other than username, password and email. all other information is temporary and is deleted upon the user logging out.",
    "present_kp": [],
    "absent_kp": [
      "c#",
      "authentication"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "ctrl+c on login. when i ssh using mobaxterm to a rhel7 server (with uname and pwd saved) i just get a blinking cursor that will take input but not execute anything. i can press ctrl+c and then appears bash-4.2$ and all is well however this is annoying and adding ctrl+c to mobaxterm execute command on log in does not solve the issue here.using other ssh/telnet tools such as putty,securefx yields same results (ctrl+c to get interactive shell)",
    "present_kp": [
      "bash",
      "login"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "at what point can i call gpl code my code?. i need some legal advice if possible.so i'm going to be implementing some code under oss license. i realize that even if i rewrote the implementation, i need to include the license.however a lot of sources that i use to learn are under gpl licenses. so even if i take what i learned and implemented it in my own way and heavily modified the code, with the only similarities being logic flow, i still can't claim the work as my own?what about algorithms that are extremely proficient? (like binary heaps)do i need to include the inventor of the binary heap into my code?also, as i understand it from gpl, if i use the code, even modified, does this mean i need to release the all of my code that even associates with it? so if i make a whole application in a closed system and i plan to sell it, i still have to release all my code via oss license?let me know if i need to provide more information about my question, thanks.",
    "present_kp": [
      "legal"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "quarantine implementation. i have an assignment to implemented a quarantine project where i was left with the some unit tests and a skeleton of the implementation. i provided the solution below and get rejected from the interview. the feedbacks i get (i feel lucky that they provided, usually, they have legal issues) are, no indicator of design (ddd, or design patterns) the code is not scalable (extendable)overall, this is c code written in java (bit harsh and made me sad). is there any better way to do that? i mainly looking suggestions from the very experienced engineers. public class quarantinetest { private quarantine quarantine; @before public void setup() { // the responsibility of the quarantine object is to simulate diseases on a group of patients. // it is initialized with a list of patients' health status, separated by a comma. // each health status is described by one or more characters // (in the test below, we will always have only one disease / patient) // the characters mean: // h : healthy // f : fever // d : diabetes // t : tuberculosis quarantine = new quarantine(f,h,d,d,d,h,t); // quarantine provides medicines to the patients, but can not target a specific group of patient. // the same medicines are always given to all the patients. // then quarantine can provide a report with this format: // f:1 h:2 d:0 t:1 x:3 // report give the number of patients that have the given disease. // x means dead } @test public void beforetreatment() throws exception { assertequals(f:1 h:2 d:3 t:1 x:0, quarantine.report()); } // people died in the diabetes @test public void notreatment() throws exception { quarantine.wait40days(); // diabetics die without insulin assertequals(f:1 h:2 d:0 t:1 x:3, quarantine.report()); } // feaver is cured // people died in the diabetes @test public void aspirin() throws exception { quarantine.aspirin(); quarantine.wait40days(); // aspirin cure fever assertequals(f:0 h:3 d:0 t:1 x:3, quarantine.report()); } @test public void antibiotic() throws exception { quarantine.antibiotic(); quarantine.wait40days(); // antibiotic cure tuberculosis // but healthy people catch fever if mixed with insulin. assertequals(f:1 h:3 d:0 t:0 x:3, quarantine.report()); } @test public void insulin() throws exception { quarantine.insulin(); quarantine.wait40days(); // insulin prevent diabetic subject from dying, does not cure diabetes, assertequals(f:1 h:2 d:3 t:1 x:0, quarantine.report()); } @test public void antibioticplusinsulin() throws exception { quarantine.antibiotic(); quarantine.insulin(); quarantine.wait40days(); // if insulin is mixed with antibiotic, healthy people catch fever assertequals(f:3 h:1 d:3 t:0 x:0, quarantine.report()); } @test public void paracetamol() throws exception { quarantine.paracetamol(); quarantine.wait40days(); // paracetamol heals fever assertequals(f:0 h:3 d:0 t:1 x:3, quarantine.report()); } @test public void paracetamolandaspirin() throws exception { quarantine.paracetamol(); quarantine.aspirin(); quarantine.wait40days(); // paracetamol kills subject if mixed with aspirin assertequals(f:0 h:0 d:0 t:0 x:7, quarantine.report()); }}import java.util.linkedhashmap;import java.util.map;import java.util.regex.pattern;import java.util.stream.collectors;public class quarantine { private map<character, integer> map; boolean insuline; boolean wait40days; boolean antibiotic; boolean aspirin; boolean paracetamol; public quarantine(string subjects) { try { map = pattern.compile(,) .splitasstream(subjects) .collect(collectors.groupingby( s -> s.charat(0), linkedhashmap::new, collectors.collectingandthen(collectors.counting(), long::intvalue) )); map.put('x', 0); } catch (exception e) { e.printstacktrace(); } } // aspirin cures fever public void aspirin() { aspirin = true; } public void antibiotic() { antibiotic = true; } public void insulin() { insuline = true; } public void paracetamol() { paracetamol = true; } public void wait40days() { if (antibiotic) { // antobiotic is mixed with the insuline if (insuline) { map.put('f', map.get('f') + map.get('h')); map.put('h', map.get('t')); map.put('t', 0); return; } // only the antibiotic else { map.put('h', map.get('h') + map.get('t')); map.put('t', 0); wait40days = true; } } else if (paracetamol) { // paracetamol mixed with the aspirin kills everyone if (aspirin) { map.put('x', map.get('x') + map.get('f') + map.get('h') + map.get('d') + map.get('t')); map.put('f', 0); map.put('h', 0); map.put('d', 0); map.put('t', 0); return; } else { // only provides the paracetamol as medication map.put('h', map.get('h') + map.get('f')); map.put('f', 0); wait40days = true; } } else if(aspirin) { // only provides aspirin as medication map.put('h', map.get('h') + map.get('f')); map.put('f', 0); wait40days = true; } else if (insuline) { // only provision of insuline prevents death from the diabetes return; } else { // no medicine was provided, just waited for the 40 days wait40days = true; } /* check if we will needs to wait for 40 days after the medication to see the affect * */ if (wait40days) { map.put('x', map.get('d')); map.put('d', 0); wait40days = false; } } // get the quarantine report public string report() { try { final string[] result = {}; map.foreach((k, v) -> result[0] += k.tostring() + : + v.tostring() + ); return result[0].trim(); } catch (exception e) { e.printstacktrace(); } return null; }}",
    "present_kp": [
      "java",
      "design patterns"
    ],
    "absent_kp": [
      "object oriented",
      "junit"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "resilient wrapper for robocopy in c#. i was being troubled a lot by robocopy (or maybe anti-virus or maybe network hardware). i copy files from the dynamic view of clearcase to the local machine for a fresh build. the copy would frequently fail due to:2012/12/06 15:35:07 error 64 (0x00000040)... the specified network name is no longer available.the server and all network hardware resides in another geographical location, so there is no way to ascertain the hardware issues. the anti-virus can never be disabled as per some policy. the snapshot view cannot be used as per another weird policy or prejudice.i am left with just one option: to make robocopy resilient. the following is what i have come up with, which is a wrapper for robocopy. please review this.summary: call robocopy with parametersgive it some time and certain number of tries for proper executionfirst try is allowed 10 minutes. the subsequent tries will have an increment of 5 minutes. maximum 5 tries and 30 minutes are allowed.catch all robocopy error codes(0, 1 and 2 are success codes) and re-try.namespace resilientrobocopy { class program { static int main(string[] args) { string commandline = string.empty; int count = 0; foreach (string str in args) { string temp = string.empty; if ((count == 0) || (count == 1)) { temp = \\ + str + \\ + ; } else { temp = str + ; } commandline = commandline + temp; count += 1; } console.writeline(--------------------------------------------------------------------); console.writeline(robocopy command line: + commandline); int returncode = -1; int tries = 0; while (((returncode == -1) || (returncode == -2)) && (tries != 5)) { console.writeline(calling robocopy); returncode = startcopy(commandline, tries); tries += 1; } console.writeline(--------------------------------------------------------------------); return returncode; } static int startcopy(string commandline, int tries) { process robocopy = new process(); try { robocopy = process.start(c:\\windows\\syswow64\\robocopy.exe, commandline); int timelimit = 120 + (60 * tries); for (int i = 0; i <= timelimit; i++) { thread.sleep(1000 * 5); if (!robocopy.hasexited) { continue; } else { if (robocopy.exitcode > 2) { console.writeline(robocopy exited with code: + robocopy.exitcode.tostring()); console.writeline(retrying...); return -1; } else { console.writeline(robocopy exited with code: + robocopy.exitcode.tostring()); console.writeline(robocopy done!); return robocopy.exitcode; } } } if (!robocopy.hasexited) { console.writeline(killing robocopy. took too much time. try try again till you succeed...); robocopy.kill(); return -1; } else { if (robocopy.exitcode > 2) { console.writeline(robocopy exited with code: + robocopy.exitcode.tostring()); console.writeline(retrying...); return -1; } else { console.writeline(robocopy exited with code: + robocopy.exitcode.tostring()); console.writeline(robocopy done!); return robocopy.exitcode; } } } catch(exception ex) { console.writeline(exception: + ex.tostring()); return -2; } finally { robocopy.close(); } } } }",
    "present_kp": [
      "c#"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "calculating disk capacity and max data transfer rate of a hard drive?. i am learning about hard drive concepts and i came across this question:suppose we have a 10000 rpm disk has 8 heads and 480 cylinders. it is divided into 120- cylinder zones with the cylinders in different zones containing 200, 240, 280, and 320 sectors. as- sume each sector contains 4096 bytes and a seek time between adjacent cylinders of 2 msec.what is the disk capacity?what is the maximum transfer rate?for the first question, i started off with these stepsadd the sectors together for a 120 cylinder zone: (200+240+280+320) = 1040 sectorssince there are 4 cylinder zones, there are a total of 4160 sectors. multiply the sectors by 4096: 4160 * 4096 bytes = 17.03 mbthat seems incredibly small. is this correct?for problem 2, i am not sure where to start.",
    "present_kp": [],
    "absent_kp": [
      "storage"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "divide and conquer approach in search for max element, take 2. adopted some suggestions from the answer of my previous post. here is the code:trait maxelem<t> { fn max_elem<'a>(&'a self) -> &'a t;}impl<t> maxelem<t> for [t] where t: ord { fn max_elem<'a>(&'a self) -> &'a t { max_elem_helper(self, 0, self.len()) }}fn max_elem_helper<'a, t>(array: &'a [t], left: usize, right: usize) -> &'a t where t: ord{ if right - left == 1 { return &array[left]; } let mid = (left + right) / 2; let max1 = max_elem_helper(array, left, mid); let max2 = max_elem_helper(array, mid, right); if max1 > max2 { max1 } else { max2 }}#[cfg(test)]mod test { use max_elem::maxelem; #[test] fn test_max_elem() { let array = [11, 2, 9, 1, 3, 88]; let m = array.max_elem(); assert_eq!(*m, 88); }}all suggestions are welcome.",
    "present_kp": [],
    "absent_kp": [
      "rust"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "truncate a file on a certain pattern. how would i go about truncating a binary file when a certain pattern is found?for instance, i want to truncate the file at the first occurrence of the pattern 0xffffffff.i think something like awk could do the trick... but i'm not exactly sure how.thanks",
    "present_kp": [
      "awk"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "using snippets of open source code in my application. i'm working on an internal company application(not built on wordpress) that needs to have shortcode functionality similar to wordpress (<url>). wordpress has a bunch of functions relating to handling short codes but the ones i was looking at were just the ones that handled the regex portion of the process.is there something wrong with just using the few functions i need from wordpress in my application? if it is ok, how do i go about giving credit in those sections.i did a quick check and wordpress is licensed under gplv2. i'm using the kohana framework in my application which is licensed under bsd. i'm not sure if that changes anything but i thought i'd include it just in case.",
    "present_kp": [
      "open source"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "capacitated multiple vehicle routing problem with handovers. i'm looking for literature about a variant of the capacitated vehicle/fleet routing problem (a.k.a. vrp, cvrp, etc.) that takes into account the possibility of handovers between multiple vehicles, i.e. the ability to drop off an item from a vehicle and let another one pick it up. put otherwise, we could say we are allowing an item to be transported (sequentially) by more than one vehicle, e.g.:at time $t^u$ vehicle $v_1$ takes the item from the pick-up location $p^u$ at time $t^d_1 \\geq t^u$ vehicle $v_1$ drops it off at an intermediate point $p_1$at time $t^u_1 \\geq t^d_1$ vehicle $v_2$ picks it up from the intermediate point $p_1$ at time $t^d_2 \\geq t^u_1$ vehicle $v_2$ drops it off at the intermediate point $p_2$ ... at time $t^u_{n-1} \\geq t^d_{n-1}$ vehicle $v_n$ picks it up from intermediate point $p_{n-1}$ at time $t^d \\geq t^u_{n-1}$ vehicle $v_n$ delivers it to the final destination $p^d$.in the vrp overviews i found this variant is not mentioned so i was wondering if anybody knew if it has been investigated at all. note: the term handover is something i came up with to describe the problem: it may not be the one commonly used to denote this kind of variant. the intended meaning, w.r.t. the example above is that e.g. when vehicle $v_1$ drops off the object $o_1$ at $p_1$ and $v_2$ picks it up we could say that $v_1$ hands over object $o_1$ to $v_2$ in $p_1$. update: reworded to clarify that i'm talking about multiple vehicles (it was just kind-of implicit in the original wording).",
    "present_kp": [],
    "absent_kp": [
      "ds.algorithms",
      "reference request",
      "graph theory",
      "graph algorithms",
      "optimization"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "dual boot opensuse13.2 and windows8.1 boot loader. i installed opensuse13.2 using dual boot on my system that originally had windows 8.1. if i was using uefi boot option then my computer automatically logged onto the windows os. so i was using legacy boot option which automatically logged into opensuse and when i wanted to log into windows i changed it from the boot menu at the start up. the grub boot loader was not showing windows8.1 at the startup as an option. in an attempt to correct this i followed a tutorial and used the following command. bcdedit /set {bootmgr} path \\efi\\opensuse\\shim.efiafter doing this now i can not log into my windows installation. it says that no boot option can be found. i am logged into my linux installation and have access to my windows os files. is there some way that i can manually edit some file to correct this?",
    "present_kp": [
      "dual boot",
      "boot loader",
      "uefi"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "plz help meh improva meh lolcat codez, whuch calculuts fawctorials. sunce meh been wruten codez in muny diffrnt programming languages, meh thoughtz that me try to write codez in an esoteric language. meh codez calculuts a fawctorial of uh usr inputted numbah in lolcode, lol.some of meh qustions:iz meh codezs a idmitic lolcat progum?iz there a gooder way of wruting meh factorial funcshuns?translation:since i've been writing and trying programs in many different languages, i thought i'd give esoteric language a crack. so i used lolcode to write a program that calculates the factorial of a user given number:questions for consideration:is my lolcode code idiomatic to the lolcode language?is there a easier way to write my factorial function?lolcat.lolbtw calculates the factorial for a user given integerbtw author: pythonicbtw version: 1.0hai 1.2how duz i factorial yr n both saem n an 0, o rly? ya rly found yr 1 no wai found yr produkt of n an factorial diff of n an 1 oicif u say sohow duz i main i has a user_input visible enter an numbah visible to findz itz fawctorial:: gimmeh user_input visible de fawctorial of user_input iz:: factorial user_input visible kthxbye!if u say somainkthxbyefor those of you who are not fortunate enough to have a lolcode compiler, here is a link to a repl.it of the code.note: all answer must be given in proper lolcode english, with an optional translation in regular english. failure to do so will result in me releasing my lolcats upon you....",
    "present_kp": [
      "lolcode"
    ],
    "absent_kp": [
      "recursion",
      "functional programming"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "does it make sense to break fluid interface if a bad argument is passed?. if i chain some setters together and one of them does not return $this, then i will get a fatal error. but maybe that is a good thing.$object = new object();$object->set('name','foo')->set('number',12)->set('color'=>'brown');class object { protected $name; protected $number; protected $color; protected $allowed_to_set = array('name','color'); public function set($property,$value) { if(!in_array((string)$property,$this->allowed_to_set)) { return false; } else { $this->$property = $value; return $this; } }}",
    "present_kp": [],
    "absent_kp": [
      "php",
      "interfaces"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "listening for values changed by the memory. i'm fetching some values from the memory. i would like to create some custom events that i can listen to for each value. since there are no existing events regarding memory changes, i've decided to go with a polling solution. i'm restarting the timer below manually just to make certain the work is finished.would this be an okay solution (considering the circumstances) to my problem? do you see any issues with creating multiple timers? as far as i know, each instance will run on its own thread.this was the best i could come up with right off the bat. i would like to hear your input regarding possible improvements:var vc = new valuechange(1000, blah.getmemoryvalue());vc.propertychanged += onpropertychanged;void onpropertychanged(object sender, propertychangedeventargs e){ console.writeline(prop change);}public class valuechange : inotifypropertychanged{ private int _val; private readonly system.timers.timer _timer; private int value { set { _val = value; raisepropertychanged(val); } get { return _val; } } public valuechange(double polinginterval, int val) { _val = val; _timer = new system.timers.timer { autoreset = false, interval = polinginterval }; _timer.elapsed += timerelapsed; _timer.start(); } private void timerelapsed(object sender, elapsedeventargs e) { var newvalue = blah.getmemoryvalue(); if (value != newvalue) { value = newvalue; } _timer.start(); } public event propertychangedeventhandler propertychanged; private void raisepropertychanged(string caller) { if (propertychanged != null) { propertychanged(this, new propertychangedeventargs(caller)); } }}",
    "present_kp": [
      "timer"
    ],
    "absent_kp": [
      "c#",
      "multithreading"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how can i list the patches of an installed package in suse?. let's say that i am using a program called hello which i have downloaded using the zypper. the question is how can i see if a specific .patch is included into this hello package?basically, what i think it that i need the rpm source file (but how can i find it using zypper ?) and then do unrpm and check if the *.patch file is included. is it correct or there's another way?",
    "present_kp": [
      "rpm",
      "suse",
      "patch",
      "zypper"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "confusion over w3c powder standard (pics alternative). now that pics is dead, i still need an easy way to tell users that my website is for mature audiences only. i intend to use pics tags, but powder is said to be the new standard. is it really used? and how do i set it up? are there simple example xml files which i can copy for my own use?",
    "present_kp": [],
    "absent_kp": [
      "meta tags",
      "adult content"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to remove non english chars from a string. i have a bash script i wrote that removes meta data off mp3 (etc) then i chop them up to reform them if needed resample the mp3 and reasign the data to make a directory to corraspond to artist / album ... i got a hold of some mp3's that have non english chars in the middle of the song title. i need to know the best way to remove that middle part leaving both ends of the sting put back together to the title of the song. using exiftool i strip the meta data off giving me this output placed into the var-name artist1='exiftool -artist $filename -p '$artist'' title1='exiftool -title $filename -p '$title'' album1='exiftool -album $filename -p '$album''first strip... artist is -> the stranglers and friendsfirst strip... album is -> live infirst strip... title is -> the raven with basil gabbisong title in meta datathe raven with basil gabbihow would i strip what is between the meta data song name to get this instead?the raven with basil gabbiwhere the syntext would look something like this newsongname=$( what ever code goes here to strip out that non english sting part )so i can write the new string back into the file replacing the old meta data with the new string. thanks",
    "present_kp": [
      "string"
    ],
    "absent_kp": [
      "shell script"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is it a good programming practice to have a class with several .h files?. i suppose the class have several different interfaces. some it shows to some class, some it shows to other classes.are there any good reason for that?one thing i can think of is with one .h per class, interface would either be public or private.what about if i want some interface to be available to some friends' class and some interface to be truly public?sample:@interface listnewcontroller:badgerstandardviewviewcontroller <uitableviewdelegate,uitableviewdatasource,uitextfielddelegate,nsfetchedresultscontrollerdelegate,uiscrollviewdelegate,uigesturerecognizerdelegate> {}@property (nonatomic) iboutlet nsfetchedresultscontroller *fetchcontroller;@property (nonatomic) iboutlet uitextfield *searchbar1;@property (nonatomic) iboutlet uitableview *tableviewa;+ (listnewcontroller *) singleton; //for easier access-(void)collapseall;-(void)titleviewclicked:(titleview *) thetitleview;-(nsuinteger) countofeachsection:(nsinteger)section;@endmany of those public properties and function are only ever called by just one other classes. i wonder why i need to make them available to many classes.it's in objective-c by the waywhat do you mean? interface is interface. in c++ you can declare some classes as friends. in objective-c you cannot i think. basically i want to emulate that. my friends can access some members that's not available too publicly,.",
    "present_kp": [
      "interfaces"
    ],
    "absent_kp": [
      "objective c",
      "inheritance"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to verify if firefox is actually sending zlib data over spdy?. i recently posted a stackoverflow bounty related to figuring out whether it is wireshark, firefox or just the data sample that is wrong, judging by the fact that latest wireshark cannot dissect it. this is a continuation of the question.assuming that the bounty linked above is correct, here is the full syn_stream spdy packet that wireshark cannot dissect:800300010100015500000001000000004000783fe3c6a7c2003b01c4fe00000009000000073a6d6574686f6400000003474554000000053a70617468000000012f000000083a76657273696f6e00000008485454502f312e31000000053a686f73740000000d3139322e3136382e302e313734000000073a736368656d650000000568747470730000000a757365722d6167656e74000000414d6f7a696c6c612f352e30202857696e646f7773204e5420352e313b2072763a33302e3029204765636b6f2f32303130303130312046697265666f782f33302e30000000066163636570740000003f746578742f68746d6c2c6170706c69636174696f6e2f7868746d6c2b786d6c2c6170706c69636174696f6e2f786d6c3b713d302e392c2a2f2a3b713d302e380000000f6163636570742d6c616e67756167650000000e656e2d55532c656e3b713d302e3500000003646e740000000131000000ffffaccording to wireshark, the compressed header should start at 783f. unfortunately, i cannot decompress it:$ pythonpython 2.7.8 (default, nov 10 2014, 08:19:18) [gcc 4.9.2 20141101 (red hat 4.9.2-1)] on linux2type help, copyright, credits or license for more information.>>> import zlib>>> header = 783fe3c6a7c2003b01c4fe00000009000000073a6d6574686f6400000003474554000000053a70617468000000012f000000083a76657273696f6e00000008485454502f312e31000000053a686f73740000000d3139322e3136382e302e313734000000073a736368656d650000000568747470730000000a757365722d6167656e74000000414d6f7a696c6c612f352e30202857696e646f7773204e5420352e313b2072763a33302e3029204765636b6f2f32303130303130312046697265666f782f33302e30000000066163636570740000003f746578742f68746d6c2c6170706c69636174696f6e2f7868746d6c2b786d6c2c6170706c69636174696f6e2f786d6c3b713d302e392c2a2f2a3b713d302e380000000f6163636570742d6c616e67756167650000000e656e2d55532c656e3b713d302e3500000003646e740000000131000000ffff.decode('hex')>>> zlib.decompress(header)traceback (most recent call last): file <stdin>, line 1, in <module>zlib.error: error 2 while decompressing datais it actually zlib? how can i prove it?",
    "present_kp": [
      "decompress"
    ],
    "absent_kp": [
      "https protocol"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "redhat5 stop copy on select. at work do we use rhel5, with gnome 2.28.2, we are also required to do development work in rhel5.rhel5 have this strange feature that it will replace the clipboard with anything i highlight. this is very unproductive for me and really slows me down when working.how can i disable this feature in rhel5, i am not able to update to a newer version of redhat or gnome.",
    "present_kp": [
      "clipboard"
    ],
    "absent_kp": [
      "x11"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how do cloud platforms-as-a-service measure cpu time?. i'm currently assessing google app engine, and one of the answers from hidden limitations of google app engine? stated:performance will surprise you. gae is optimized for many tiny queries and you get warned if a query takes any cpu time at all. you get 6.5 (at last check) free hours per day, but it's a mystical number and you should test.you'll find that time as you measure it doesn't relate to the cpu or datastore cpu time, because (for example) under the covers there might be multiple machines updating indexes during deletes/updates. some users have found huge cpu usage when uploading bulk data - many hours of usage for e.g. 20 min of real time.your java instance might need to be powered-up if it hasn't been hit in (i think) 20 minutes. the benefit is that they can pass their smart management on to you as cheaper costs, but it does mean you'll experience a short delay, and see a high cpu warning on the first request in a while.for many cases, python datastore access is faster than java jdo. you'll likely find that using the low-level api for java faster.some developers seem to have experienced more datastore errors thank you would expect (around 0.4-1% maybe?). i haven't yet.i'm wondering what factors go into determining a paas's cpu time (like, in this case, the 6.5 hours free that you get with gae). for example, does it include the total time it takes for the database to transfer data over to the client?do the costs under this model increase dramatically as you get more users?",
    "present_kp": [
      "google app engine",
      "paas"
    ],
    "absent_kp": [
      "hosting",
      "google cloud datastore"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "storing values in sql whose types are determined in runtime. i have come across a need of storing runtime determined values in a sql database.for example, there is a gui where a user can add new editable fields. so the user adds a field name, chooses the value type string and fills in the value arthur. then the user adds a new field money, chooses the value type decimal and fills in the value 10.05.as seen above, the user is able to add any fields with any value types. my idea is to have a sql table schema as follow:fieldname (varchar) - eg. 'money' or 'isalive'fieldtype (varchar) - eg 'decimal' or 'boolean'fieldvalue (varchar) - eg. '10.5' or '1'this would mean that i can store any pre-defined data type in the table whether it's a string, boolean or long. the problem is that i am not sure it's the best approach to cast those values to string and then cast again when reading from the table.another option would be to have a column of each type but that sounds even worse.does my approach to cast everything to string sounds sensible or should i try something else?",
    "present_kp": [
      "sql"
    ],
    "absent_kp": [
      "type casting"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "openvpn server doesn't nat the incoming traffic. it's couple of days that i'm having hard time figuring out what's wrong with openvpn. few days ago, i installed and configured openvpn on a debian server and also on my ubuntu client. i am facing a weird behavior: openvpn server nats the incoming traffic whenever it wants, but not always! it looks that the iptables rules on the debian server get deleted at some specified time every day (i'm not sure though), because the next day if i re-enter the iptable rules (for nat & dns queries forwarding) like following,iptables -t nat -a postrouting -s 10.8.0.0/24 -o eth0 -j masqueradeiptables -t nat -a prerouting -i tun+ -p udp --dport 53 -j dnat --to-destination 8.8.8.8and force-reload openvpn daemon and restart openvpn (at both ends), the traffic gets routed as it should. i've saved iptables rules in the file /etc/iptables.conf and i reload it at boot (/etc/rc.local), but nevertheless the problem still persists. here is the content of my /etc/iptables.conf:# generated by iptables-save v1.4.14 on tue apr 15 13:50:46 2014*security:input accept [166249:<phone>]:forward accept [3659:<phone>]:output accept [165776:<phone>]commit# completed on tue apr 15 13:50:46 2014# generated by iptables-save v1.4.14 on tue apr 15 13:50:46 2014*raw:prerouting accept [169911:<phone>]:output accept [165776:<phone>]commit# completed on tue apr 15 13:50:46 2014# generated by iptables-save v1.4.14 on tue apr 15 13:50:46 2014*nat:prerouting accept [42:2524]:input accept [1:40]:output accept [24:1504]:postrouting accept [24:1504]-a prerouting -i tun+ -p udp -m udp --dport 53 -j dnat --to-destination 8.8.8.8-a postrouting -s 10.8.0.0/24 -o eth0 -j masqueradecommit# completed on tue apr 15 13:50:46 2014# generated by iptables-save v1.4.14 on tue apr 15 13:50:46 2014*mangle:prerouting accept [169911:<phone>]:input accept [166249:<phone>]:forward accept [3659:<phone>]:output accept [165776:<phone>]:postrouting accept [169435:<phone>]commit# completed on tue apr 15 13:50:46 2014# generated by iptables-save v1.4.14 on tue apr 15 13:50:46 2014*filter:input accept [166249:<phone>]:forward accept [3659:<phone>]:output accept [165776:<phone>]commit# completed on tue apr 15 13:50:46 2014here i've posted my config files. the differences in my working client.conf & server.conf with those in that post are as follows:# client.confuser nobodygroup nogroup redirect-gateway def1 #bypass-dns bypass-dhcpserver:# server.conf; duplicate-cn #not needed, commented out user nobodygroup nogroup i'll be deeply grateful if you save me from this chaos!",
    "present_kp": [
      "openvpn"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "samba/cifs connection error. i have been getting errors connecting to samba/cifs shares from like 3 weeks ago, i have 4 machines, one with windows, 2 with fedora 24 and one with fedora 22. i have shares on the two fedora 24 machines and if i try to connect, for example, from one f24 machine to the other via thunar i get a no route to host. error. i also had configured a one-liner script on my fedora 22 box that connects to the f24 machine using the following command:# mount -t cifs -o username=****,password=**** '\\192.168.1.1\\share' remote/and it worked like a charm for a very long time, but recently it fails with this error:[ 91.981816] cifs vfs: error connecting to socket. aborting operation.[ 91.981960] cifs vfs: cifs_mount failed w/return code = -113unable to find suitable address.i also tried to connect from one of my f24 machines to the other, and i get (on both machines, trying to connect to each other):unable to find suitable address.but, if i try to connect using the same command from within the same target machine, it works without any issues, and it gets mounted correctly.the windows machine simply fails to connect. why does this happen? how can i fix this? this is new to me since everything was working correctly until 2-3 weeks ago. also i can ssh into any of them without any problem, the issue is completely on samba/cifs.",
    "present_kp": [
      "samba",
      "cifs"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "collision attacks, message digests and a possible solution. i've been doing some preliminary research in the area of message digests. specifically collision attacks of cryptographic hash functions such as md5 and sha-1, such as the postscript example and x.509 certificate duplicate.from what i can tell in the case of the postscript attack, specific data was generated and embedded within the header of the postscript (which is ignored during rendering) which brought about the internal state of the md5 to a state such that the modified wording of the document would lead to a final md equivalent to the original. the x.509 took a similar approach where by data was injected within the comment/whitespace of the certificate.ok so here is my question, and i can't seem to find anyone asking this question:why isn't the length of only the data being consumed added as a final block to the md calculation?in the case of x.509 - why is the whitespace and comments being taken into account as part of the md?wouldn't a simple processes such as one of the following be enough to resolve the proposed collision attacks:$md(m + |m|) = xyz$$md(m + |m| + |m| * magicseed_0 +...+ |m| * magicseed_n) = xyz$where :m : is the message|m| : size of the message in bitsmd : is the message digest function (eg: md5, sha, whirlpool etc)xyz : is the pairing of the acutal message digest value for the message m and |m|. <m,|m|>$magicseed_{i}$: is a set of random values generated with seed based on the internal-state prior to the size being added.'+' : is a concatentation operation a+b+c = abcthis technqiue should work, as to date all such collision attacks rely on adding more data to the original message.in short, the level of difficulty involved in generating a collision message such that:it not only generates the same mdbut is also comprehensible/parsible/compliantand is also the same size as the original message,is immensely difficult if not near impossible. has this approach ever been discussed? any links to papers etc would be nice.further question: what is the lower bound for collisions of messages of common length for a hash function h chosen randomly from u, where u is the set of universal hash functions ?is it $1/n$ (where n is $2^{|m|}$) or is it greater? if it is greater, that implies there is more than 1 message of length |m| that will map to the same md value for a given h.if that is the case, how practical is it to find these other messages? bruteforce would be of $o(2^n)$, is there a method of time complexity less than bruteforce?links:postscript: <url> <url> this question is not as easy as it seems. please read it very carefully and make sure you have understood exactly what is being asked. the answer will require understanding of martingales and distribution ensembles. if you are unsure what these are or how they apply to this particular problem then please do not post answers. prior knowledge of this class of attack is required, please access the links above and the following: csse question. please do not focus on answers that relate to generating messages of length different from the original message as any such solution is invalid.",
    "present_kp": [
      "hash function"
    ],
    "absent_kp": [
      "cr.crypto security"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "core dumps for out of memory in linux. how to create memory dumps for out of memory.if system is out of memory then oom-killer kills the process which occupies highest memory by some calculations.how can we get core dump of the process killed",
    "present_kp": [
      "linux",
      "core dump"
    ],
    "absent_kp": [
      "ubuntu",
      "crash"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "kde how to remove kickoff application launcher popup. every time i hover the cursor over k button, i get a popup: kickoff application launcher. it irritates me a lot. i managed to remove task manager popups in it's properties. but for kickoff there is no property to remove it.kde version: 4.8.5.how to remove this popup?",
    "present_kp": [
      "kde"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "i would like to improve in writing goal-oriented user stories/features. i would like to improve my user stories writing abilities so i need your experience and guidance. i think that each variable part of a user story can be improved by following certain guidelines.1. roledefining possible roles may be the easiest part. the only problem is that user stories usually just describe functional features that need development.do you write non-functional stories? and i'm not thinking about technical stories that are related to other aspects of product planning like administrative things. i'm more thinking of additional supporting stories that support the same high-level goal. like advertising related stories that are related to features?2. featuredefining features seems to be easy but it turns out it's not. because i can see stories that are heavily related so certain roles can't be developed until others are finished. example: user registration is a pillar user story that needs to be done first in order to develop other user stories that are related to registered user roles (logged in users and/or admins etc.). this one is very obvious but sometimes relationships are much more subtle.how do you avoid relationships between user stories? is there a technique to break related stories apart so we can have better development parallelism? breaking relationships also helps better sprint cycles because one story can't stall the whole sprint.3. benefitthis one is the hardest part that i think i have least knowledge how to write good user stories with clear benefits.do you actually write benefits per story or do you rather provide high level goals instead? providing these instead of benefits makes it easy to see why certain user story is more important than the other? so it helps prioritisation.is it wise to have shared benefits/goals between set of user stories?do you write user stories without benefits part? is that wise and what do you do instead?do you write benefits in a way so they can be measurable? can we do that and is it beneficial?4. user stories as a wholehave you ever defined all user stories as epics? this would help filter out/prioritise those that give more to the common goals. then split them into digestible chunks or per single feature user-stories.what additional metadata do you add to user stories? also talking about user story grouping per higher feature or goal...",
    "present_kp": [
      "user story"
    ],
    "absent_kp": [
      "agile",
      "product features"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to extend bash aliases. how to create an alias that actually extends another alias of the same name inbash?why:i used to have grep_options set on .bashrc to something like this:grep_options=-i --exclude=\\*~i also had a script (let us say, setup-java.sh) which i would call beforeworking on some java projects. it would contain the line:grep_options=$grep_options --exclude-dir=classesif i also use sass, then i would call setup-sass.sh which contains the line:grep_options=$grep_options --exclude-dir=\\*/.sass-cachebut grep_options wasdeprecated andapparently the standard solution is either create an alias or some script...",
    "present_kp": [
      "bash",
      "grep",
      "alias"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "what uses does smalltalks become: have?. the become: message in smalltalk causes one object to change into another, affecting all references to it.what uses does this language feature have? does it get used in real code? is it just a curiosity? is it considered a good/bad practice to use it?",
    "present_kp": [
      "smalltalk"
    ],
    "absent_kp": [
      "object oriented",
      "memory",
      "language features"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "create news ticker animation. i have this function in this fiddlejavascript:var newsticker = function (ele) { var eles = ele.find('ul li'), indexele = 0, dataele = ele.find('.ticker-post'), rotatechars = function (title) { dataele.text(''); indexcut = 0; rotatecharstimer = setinterval(function () { if (dataele.text().length == title.length) { clearinterval(rotatecharstimer); } else if (dataele.text().length < title.length) { text = dataele.text().concat(title.substr(indexcut, 1)); dataele.text(text); }; if (title.length - 1 > indexcut) indexcut++; else indexcut = 0; }, 90); }, loopls = function () { loopele = eles.get(indexele); elehref = $(loopele).data('href'); eletitle = $(loopele).data('title'); dataele.attr('href', elehref); if (typeof rotatecharstimer != 'undefined') { dataele.fadeout(); clearinterval(rotatecharstimer); } dataele.fadein(); rotatechars(eletitle); if (eles.length - 1 > indexele) indexele++; else indexele = 0; } loopls(); setinterval(function (){loopls()},9400);}html:<div id=news-ticker> <div class=ticker-title>breaking news:</div> <ul> <li data-href=#1 data-title=no 10 armed police arrested over hardcore pornography></li> <li data-href=#2 data-title=edf extends life of uk nuclear plants></li> <li data-href=#3 data-title=superwoman stephanie flanders on how to do it all></li> <li data-href=#4 data-title=will liverpool deliver for surez? can arsenal last?></li> </ul> <a class=ticker-post></a></div>is this the best way to do this?",
    "present_kp": [
      "javascript"
    ],
    "absent_kp": [
      "jquery"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how do i combine the -v and -b switches in grep?. i have a list of currently installed kernels, and i'm trying to grep out both the currently installed kernel and the previously installed kernel. for this example, linux-image-3.2.0-60-generic is the currently running kernel, and these are the installed kernels:linux-image-3.2.0-49-genericlinux-image-3.2.0-51-genericlinux-image-3.2.0-52-genericlinux-image-3.2.0-53-genericlinux-image-3.2.0-54-genericlinux-image-3.2.0-55-genericlinux-image-3.2.0-56-genericlinux-image-3.2.0-57-genericlinux-image-3.2.0-58-genericlinux-image-3.2.0-59-genericlinux-image-3.2.0-60-genericlinux-image-3.2.0-61-genericlinux-image-3.2.0-63-genericlinux-image-3.2.0-64-generici can use grep -v $(uname -r) to remove the currently running kernel from this list. however, i can't seem to use grep -v -b 1 $(uname -r) to remove current and previous kernel from the list. is there a way to combine the -b and -v? or am i approaching this entirely from the wrong direction?",
    "present_kp": [
      "grep"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how can one intuitively understand generative v/s discriminative models, specifically with respect to when each is useful?. i'm trying to gain some intuition beyond definitions, in any possible dimension. i'd appreciate references to read.",
    "present_kp": [
      "models"
    ],
    "absent_kp": [
      "machine learning"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how discoverable is a phone-number-only messenger user by full facebook users (and vice versa)?. facebook's new messenger app now lets you sign up with only a phone number, sidestepping the need for a full facebook account. the docs are a bit sparse for those who want to be informed before signing up, so...if i sign up for a messenger-only account with a phone number:how am i discoverable by users with full facebook accounts? (i.e. will they need my phone number to find me, or can they look me up by name in the global graph search and see my picture?)and how can i discover other contacts on the messenger app to start chatting with them? (i.e. will the contact search bar search only contacts whose phone numbers i know, or will it search the global facebook graph too - including facebook users without phone numbers?)this seems an obvious enough thing to be in the messenger.com docs, but astonishingly this does not yet seem to be there",
    "present_kp": [
      "facebook"
    ],
    "absent_kp": [
      "facebook chat"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "rspec vs test::unit in rails. i've never been really convinced of the advantages that you get by switching over to rspec from test::unit in ruby on rails (despite reading from time to time about rspec).what is it about rspec that most rails project seems to be using it?(some code examples clearly indicating advantages of one over the other would be much appreciated)",
    "present_kp": [
      "ruby on rails"
    ],
    "absent_kp": [
      "testing",
      "unit testing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "linuxbrew can not find 'erl'(erlang) command when i try to install 'elixir' on opensuse tumbleweed. i don't know ruby language, but it seems there's no error on elixir.rb formulae script.firstly depends_on erlang18requirement causes this error even erlang installed.and it can go to make phase if i delete that depends_on ... line with brew install elixir but also spit an error /bin/sh: erl: command not found.it looks like...==> downloading <url> already downloaded: /home/algorist/.cache/homebrew/elixir-1.4.5.tar.gz ==> make last 15 lines from /home/algorist/.cache/homebrew/logs/elixir/01.make: 2017-07-04 16:57:44 +0900 make/bin/sh: erl: command not found at least erlang 18.0 is required to build elixir make: *** [makefile:62: lib/elixir/src/elixir.app.src] error 1but erl command runs fine on terminal with any shells and irb(ruby interactive shell?) i don't know why erl command doesn't work with linuxbrew.please help!",
    "present_kp": [
      "homebrew",
      "brew"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "is it ok to redirect to a new address instead of showing 404 error page?. here is my question, recently i moved my site from blog.mysite.com to <url> i added 301 permanent redirection using this code in .htaccess file.rewriteengine onrewritecond %{http_host} ^blog\\.mysite\\.com$ [or]rewritecond %{http_host} ^www\\.blog\\.mysite\\.com$rewriterule ^/?$ http\\:\\/\\/www\\.mysite\\.com\\/ [r=301,l]everything was ok, and when i go to my site by typing blog.mysite.com it automatically redirected to <url>, i found that when i click a google search result of my old site doesn't redirect to the new site. guess google result url is <url> it doesn't redirect to <url> only goes to the old url which is <url> shows my hosting provider's 404 error page. so, then what i did was creating a custom 404 error page and added a php code to get the google's search result url and replace the word 'blog' with 'www' then redirecting to <url> now everything's working perfectly and no any error pages. but i want to know if what i've done is ok with seo logics. i've heard that 301 permanent redirection tells the search engines that the site has moved. and where the new site is.i want to know if there's no damage to 301 redirection after i've created custom 404 page and added a tricky redirection...thanks... hope you understand. :-)",
    "present_kp": [
      "seo"
    ],
    "absent_kp": [
      "redirects"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "creating an enumerator based on an atl collection. i am designing a com wrapper for a c++ library using atl. currently i am a little bit confused about enumerators that are based on atl collections. i already wrote several enumerators based on stl collections, but since there is no icollectiononatl-interface (such as the icollectiononstl-interface), i am a little bit confused if my implementation is correct.here is how i am creating the enumerator at the moment:stdmethodimp ccollectiononatl::get_newenum(ienumitem** enumerator){ // iitem implements iunknown. typedef cinterfacearray<iitem> container; typedef ccomenum<ienumitem, &iid_ienumitem, iitem*, _copyinterface<iitem>> ccomenumitem; // copy all elements to an array that can be enumerated. container items; auto elements = m_coll.getcount(); //items.setcount(elements); // see below for the definition of m_coll. position pos = m_coll.getheadposition(); for (size_t element = 0; element < elements; element++) { auto current = m_coll.getnext(pos); // automatically calls addref! items.add(current->m_value); } // create an enumerator over the array. // using 'new' is valid here (since enumerators do not implement finalconstruct). ccomobject<ccomenumitem>* penum = new ccomobject<ccomenumitem>(); penum->init(&items[0], &items[elements - 1], nullptr, atl::atlflagtakeownership); // return the enumerator. return penum->queryinterface(enumerator);}the member m_coll is defined as crbmap:crbmap<ulong, iitem*> m_coll;there are some points with this implementation, where i am not really sure if i understood them correctly:iterating the crbmap should give me an ordered list, where the lowest key is the first and the highest key is the last element. however, i am not sure if there's a better (in terms of simplicity) way to perform the iteration.usage of cinterfacearray to initialize the enumerator:penum->init(&items[0], &items[elements - 1], nullptr, atl::atlflagtakeownership);is the initialization correct? especially i am wondering about the third parameter, that references an iunknown-instance to keep alive for as long as the enumerator lives. currently i am passing nullptr, but i think the right value should be the items-array, which however does not implement iunknown.regarding the enumerator initialization: was my choose of atl::atlflagtakeownership correct in here? i am not completely sure, because the actual ownership of the instances is taken by m_coll.since the size of the final array is known, i could initialize it (and therefor prevent relocations/copy-steps) using items.setcount(elements);. however, the msdn only states that cinterfacearray::add automatically calls addref. there's no information about setat or operator[]. could i simply change the way i add items to the array to items[element] = current->m_value; without any change in the behaviour of reference counting?also if you have any suggestions on how i can further improve this code, please let me know!",
    "present_kp": [
      "c++",
      "atl"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "is it possible to run process that use only 2 cpu from 16 cpu in my linux machine. is it possible to run process that use only 2 cpu from 16 cpu in my linux machinewe have red-hat machine version 6and we have 16 cpubut because license cost money and if we limit the script that will run only on 2 cpu then we can save the money",
    "present_kp": [
      "linux",
      "process",
      "cpu"
    ],
    "absent_kp": [
      "rhel"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "keychain can't add gpg key. i'm setting up keychain to manage gpg-agent, with keychain --eval --agents gpg mygpgkeyi get the following:* keychain 2.8.1 ~ <url> * starting gpg-agent... * adding 1 gpg key(s): df1a7077 * error: problem adding (is pinentry installed?); giving upi've checked that pinentry is indeed installed (link to pinentry-gnome3).digging through /usr/bin/keychain, the line causing trouble is:gpg --no-options --use-agent --no-tty --sign --local-user localuser -o >/dev/null 2>&1i can call this command, when gpg-agent is running; pinentry-gnome3 prompts for my gpg passphrase, but after i close it, the gpg command doesn't finish.what is the right way to get gpg working with keychain ?",
    "present_kp": [],
    "absent_kp": [
      "gpg agent"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "completeness spanning trees. a spanning tree of a graph is called a completeness tree if the set of its leaves induces a complete subgraph in the host graph. given a graph $g$ and an integer $k$, what is the complexity of deciding if $g$ contains a completeness tree with at most $k$ leaves?a reason for asking this question is that the corresponding problem for independency trees is np-complete, here an independency tree is a spanning tree such that the set of its leaves is an independent set in the host graph. another reason is this question (and the corresponding answers). it turns out that every spanning tree of $g$ is a completeness tree if and only if $g$ is a complete graph or a cycle.",
    "present_kp": [],
    "absent_kp": [
      "cc.complexity theory",
      "graph theory",
      "graph algorithms"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "should a business create social media accounts for founders in addiion to a company social media account?. is it a good idea to have a company profile on social media as well as individual accounts of people that are involved in the company, for example the founders, ceo, etc.?i am thinking of a business that is in a specific niche, let us say for example bicycles (and everything else related to bicycles). it has a website (e-commerce and blog)and social media profiles on all the major social media channels like facebook, twitter, google+, instagram, etc. this just for the business side of things.the blog posts are written by 2 people - they both are founders of this bicycle company. at the end of each post (as is the norm) is a photo of the author and a link to his social media profiles (for example to google+).so this brings me back to my question. do you need social media profiles for the company and the founders?i just can't get my head around things. something tells me that you need to separate your business profile for the profiles of the founders, each 1 has their own presence.i also read in a book that you need to be intimate as possible - when reaching out to people (for example in forums) that you need respond as a person, and not a business. this is why i am thinking of the 2 separate profiles.so if this is the case how do you differentiate between the 2 profiles seeing that you have to generate the content on the social media profiles? what would you post on the one and not the other?",
    "present_kp": [
      "blog",
      "social media"
    ],
    "absent_kp": [
      "social networks",
      "social networking"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how can i get a transcript of a youtube video?. possible duplicate:downloading youtube transcriptionshow do i download subtitles from a youtube video?i see options for closed captions for some videos. is there anyway to get the full transcript of a video?",
    "present_kp": [
      "youtube"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "performance and other issues with using floating point types in c++. being interested in c++ performance programming there is one aspect i really have no clue about- and that is the implications of using floating point calculations vs doubles vs normal integer mathematics? what performance issues with these types of calculations should i consider? what other issues need to be considered when deciding which kind of operators to use? all i currently know is that doubles are probably the most expensive because they are larger, floating point is next and then integer calculations are usually the fastest on a cpu because they are much simpler.",
    "present_kp": [
      "c++",
      "performance",
      "floating point"
    ],
    "absent_kp": [
      "optimization",
      "arithmetic"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "streamlined for-loop for comparing two lists. i'm new to python and i'm wondering is there a way to streamline this clunky code i've written. maybe a built-in function i've never come across before?i run through two lists of binary numbers and if the same number appears at the same index in list one and two, do x.so in the example below the number 1 appears at index 2 in both lists.list1 = [0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]list2 = [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]for position, binary in enumerate(list1): for placement, number in enumerate(list2): if position == placement and binary == 1 and binary == number: do x",
    "present_kp": [
      "python"
    ],
    "absent_kp": [
      "beginner",
      "python 2.7"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "automatic kerberos ticket initialization on login. i'm using ksshaskpass to add my password protected keys into ssh-agent upon logging into kde, is there something similar for kerberos?",
    "present_kp": [
      "kde",
      "login",
      "kerberos"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "do not open a new instance of an application, if it's already open in another workspace in mint. i am using linux mint and i enabled multiple workspaces. suppose i have an instance of an application (i.e., google chrome) open on workspace 1. when i switch to workspace 2 and i open google chrome, i would like to be redirected to the instance already open on workspace 1, while linux makes me open another (new) window on workspace 2. this is a bit annoying.is there a way to prevent linux mint to open a new instance of an application, if there is another one already open in another workspace?",
    "present_kp": [
      "linux mint",
      "window",
      "workspaces"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "list of links to images in website with wget. i have a question, as much as i think i cannot think of the way to put all links of pictures in a list, i asked this question before but it was marked as duplicate to post where op asked to download all pictures.i tried wget -r -p /save/location -a jpeg,jpg <url> but it's only downloading to computer. how can i extract a list of all images to a file?",
    "present_kp": [
      "wget",
      "web"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to describe a position of a point w.r.t the position and orientation of 3 other points. is it possible to describe a position of a point $a$ (in 3d) w.r.t. the position and orientation of 3 other points? if so, how?fyi, the 3 other points lie on a plane, whereas the point $a$ is not on the plane.just to elaborate, initially i have the info for all four points. my goal is to find where point $a$ is when the other three points changing position and orientation.",
    "present_kp": [],
    "absent_kp": [
      "computational geometry"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "if one is flexing a muscle, for any given motoneuron involved, how many action potentials occur per second to keep the muscle flexed?. also, will the axon terminals in a given motoneuron ever run out of neurotransmitters to release if they are constantly undergoing action potentials (say in the situation outlined above where a muscle is being kept flexed)? or is there always enough reuptake to ensure that this could never happen?",
    "present_kp": [],
    "absent_kp": [
      "neurobiology"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to spawn a shell using netcat on the client side?. i know i can spawn the shell on server side using: nc -l 1111 -e /bin/bashbut i want to spawn the shell on the client side.i tried doing: nc 127.0.0.1 1111 | /bin/bashit works but i can't see the output of the executed commands.so the question is, is there any way to spawn the shell on the client side using netcat?",
    "present_kp": [
      "shell",
      "netcat"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "class modelling for a shogi notation reader. i have made gpl software in github whose purpose is reading shogi notations (shogi is japanese chess). i have been told that my software modelling is underdeveloped in this question and advised to post a question about the classes i made to manage the board, so here i go:the main concern is about the file managers.py, which contains the classes that i call managers. two of those managers are for handling the board's coords for pieces.one of them is the class coords_manager - managers.py line 27:class coords_manager: def __init__(self): # pieces arrays self.lista_pn = none self.lista_spn = none self.cnt_pn = none self.rpn = none self.lista_pb = none self.lista_spb = none self.cnt_pb = none self.rpb = none self.lista_ln = none self.lista_sln = none self.cnt_ln = none self.rln = none self.lista_lb = none self.lista_slb = none self.cnt_lb = none self.rlb = none self.lista_nn = none self.lista_snn = none self.cnt_nn = none self.rnn = none self.lista_nb = none self.lista_snb = none self.cnt_nb = none self.rnb = none self.lista_sn = none self.lista_ssn = none self.cnt_sn = none self.rsn = none self.lista_sb = none self.lista_ssb = none self.cnt_sb = none self.rsb = none self.lista_gn = none self.cnt_gn = none self.rgn = none self.lista_gb = none self.cnt_gb = none self.rgb = none self.lista_tn = none self.lista_stn = none self.cnt_tn = none self.rtn = none self.lista_tb = none self.lista_stb = none self.cnt_tb = none self.rtb = none self.lista_bn = none self.lista_sbn = none self.cnt_bn = none self.rbn = none self.lista_bb = none self.lista_sbb = none self.cnt_bb = none self.rbb = none self.rey_n = none self.rey_b = none self.reverted = 1 #1 or -1 self.coords_ax = { '1': 837, '2': 766, '3': 695, '4': 624, '5': 553, '6': 482, '7': 411, '8': 340, '9': 269 } self.coords_ay = { 'i': 589, 'h': 518, 'g': 447, 'f': 376, 'e': 305, 'd': 234, 'c': 163, 'b': 92, 'a': 21 } self.coords_bx = { '1': 269, '2': 340, '3': 411, '4': 482, '5': 553, '6': 624, '7': 695, '8': 766, '9': 837 } self.coords_by = { 'a': 589, 'b': 518, 'c': 447, 'd': 376, 'e': 305, 'f': 234, 'g': 163, 'h': 92, 'i': 21 } self.coords_x = none self.coords_y = none self.update() self.begin() def begin(self): self.lista_pn = {1:[self.coords_x['1'],self.coords_y['g']],2:[self.coords_x['2'],self.coords_y['g']],3:[self.coords_x['3'],self.coords_y['g']],4:[self.coords_x['4'],self.coords_y['g']],5:[self.coords_x['5'],self.coords_y['g']],6:[self.coords_x['6'],self.coords_y['g']],7:[self.coords_x['7'],self.coords_y['g']],8:[self.coords_x['8'],self.coords_y['g']],9:[self.coords_x['9'],self.coords_y['g']]} self.lista_spn = {} self.cnt_pn = 10 self.rpn = 0 self.lista_pb = {1:[self.coords_x['1'],self.coords_y['c']],2:[self.coords_x['2'],self.coords_y['c']],3:[self.coords_x['3'],self.coords_y['c']],4:[self.coords_x['4'],self.coords_y['c']],5:[self.coords_x['5'],self.coords_y['c']],6:[self.coords_x['6'],self.coords_y['c']],7:[self.coords_x['7'],self.coords_y['c']],8:[self.coords_x['8'],self.coords_y['c']],9:[self.coords_x['9'],self.coords_y['c']]} self.lista_spb = {} self.cnt_pb = 10 self.rpb = 0 self.lista_ln = {1:[self.coords_x['1'],self.coords_y['i']],2:[self.coords_x['9'],self.coords_y['i']]} self.lista_sln = {} self.cnt_ln = 3 self.rln = 0 self.lista_lb = {1:[self.coords_x['1'],self.coords_y['a']],2:[self.coords_x['9'],self.coords_y['a']]} self.lista_slb = {} self.cnt_lb = 3 self.rlb = 0 self.lista_nn = {1:[self.coords_x['2'],self.coords_y['i']],2:[self.coords_x['8'],self.coords_y['i']]} self.lista_snn = {} self.cnt_nn = 3 self.rnn = 0 self.lista_nb = {1:[self.coords_x['2'],self.coords_y['a']],2:[self.coords_x['8'],self.coords_y['a']]} self.lista_snb = {} self.cnt_nb = 3 self.rnb = 0 self.lista_sn = {1:[self.coords_x['3'],self.coords_y['i']],2:[self.coords_x['7'],self.coords_y['i']]} self.lista_ssn = {} self.cnt_sn = 3 self.rsn = 0 self.lista_sb = {1:[self.coords_x['3'],self.coords_y['a']],2:[self.coords_x['7'],self.coords_y['a']]} self.lista_ssb = {} self.cnt_sb = 3 self.rsb = 0 self.lista_gn = {1:[self.coords_x['4'],self.coords_y['i']],2:[self.coords_x['6'],self.coords_y['i']]} self.cnt_gn = 3 self.rgn = 0 self.lista_gb = {1:[self.coords_x['4'],self.coords_y['a']],2:[self.coords_x['6'],self.coords_y['a']]} self.cnt_gb = 3 self.rgb = 0 self.lista_tn = {1:[self.coords_x['2'],self.coords_y['h']]} self.lista_stn = {} self.cnt_tn = 2 self.rtn = 0 self.lista_tb = {1:[self.coords_x['8'],self.coords_y['b']]} self.lista_stb = {} self.cnt_tb = 2 self.rtb = 0 self.lista_bn = {1:[self.coords_x['8'],self.coords_y['h']]} self.lista_sbn = {} self.cnt_bn = 2 self.rbn = 0 self.lista_bb = {1:[self.coords_x['2'],self.coords_y['b']]} self.lista_sbb = {} self.cnt_bb = 2 self.rbb = 0 self.rey_n = [self.coords_x['5'],self.coords_y['i']] self.rey_b = [self.coords_x['5'],self.coords_y['a']] def update(self): if self.reverted == 1: self.coords_x = self.coords_ax self.coords_y = self.coords_ay else: self.coords_x = self.coords_bx self.coords_y = self.coords_by def revert(self): self.reverted *= -1 revert_x = { 269: 837, 340: 766, 411: 695, 482: 624, 553: 553, 624: 482, 695: 411, 766: 340, 837: 269 } revert_y = { 21: 589, 92: 518, 163: 447, 234: 376, 305: 305, 376: 234, 447: 163, 518: 92, 589: 21 } for k, e in self.lista_pn.items(): self.lista_pn[k] = [revert_x[e[0]],revert_y[e[1]]] for k, e in self.lista_spn.items(): self.lista_spn[k] = [revert_x[e[0]],revert_y[e[1]]] for k, e in self.lista_pb.items(): self.lista_pb[k] = [revert_x[e[0]],revert_y[e[1]]] for k, e in self.lista_spb.items(): self.lista_spb[k] = [revert_x[e[0]],revert_y[e[1]]] for k, e in self.lista_ln.items(): self.lista_ln[k] = [revert_x[e[0]],revert_y[e[1]]] for k, e in self.lista_sln.items(): self.lista_sln[k] = [revert_x[e[0]],revert_y[e[1]]] for k, e in self.lista_lb.items(): self.lista_lb[k] = [revert_x[e[0]],revert_y[e[1]]] for k, e in self.lista_slb.items(): self.lista_slb[k] = [revert_x[e[0]],revert_y[e[1]]] for k, e in self.lista_nn.items(): self.lista_nn[k] = [revert_x[e[0]],revert_y[e[1]]] for k, e in self.lista_snn.items(): self.lista_snn[k] = [revert_x[e[0]],revert_y[e[1]]] for k, e in self.lista_nb.items(): self.lista_nb[k] = [revert_x[e[0]],revert_y[e[1]]] for k, e in self.lista_snb.items(): self.lista_snb[k] = [revert_x[e[0]],revert_y[e[1]]] for k, e in self.lista_sn.items(): self.lista_sn[k] = [revert_x[e[0]],revert_y[e[1]]] for k, e in self.lista_ssn.items(): self.lista_ssn[k] = [revert_x[e[0]],revert_y[e[1]]] for k, e in self.lista_sb.items(): self.lista_sb[k] = [revert_x[e[0]],revert_y[e[1]]] for k, e in self.lista_ssb.items(): self.lista_ssb[k] = [revert_x[e[0]],revert_y[e[1]]] for k, e in self.lista_gn.items(): self.lista_gn[k] = [revert_x[e[0]],revert_y[e[1]]] for k, e in self.lista_gb.items(): self.lista_gb[k] = [revert_x[e[0]],revert_y[e[1]]] for k, e in self.lista_tn.items(): self.lista_tn[k] = [revert_x[e[0]],revert_y[e[1]]] for k, e in self.lista_stn.items(): self.lista_stn[k] = [revert_x[e[0]],revert_y[e[1]]] for k, e in self.lista_tb.items(): self.lista_tb[k] = [revert_x[e[0]],revert_y[e[1]]] for k, e in self.lista_stb.items(): self.lista_stb[k] = [revert_x[e[0]],revert_y[e[1]]] for k, e in self.lista_bn.items(): self.lista_bn[k] = [revert_x[e[0]],revert_y[e[1]]] for k, e in self.lista_sbn.items(): self.lista_sbn[k] = [revert_x[e[0]],revert_y[e[1]]] for k, e in self.lista_bb.items(): self.lista_bb[k] = [revert_x[e[0]],revert_y[e[1]]] for k, e in self.lista_sbb.items(): self.lista_sbb[k] = [revert_x[e[0]],revert_y[e[1]]] self.rey_n = [revert_x[self.rey_n[0]],revert_y[self.rey_n[1]]] self.rey_b = [revert_x[self.rey_b[0]],revert_y[self.rey_b[1]]] self.update()the other one is matrix_manager - managers.py line 436:class matrix_manager: def __init__(self): self.adaptx = {'9':0,'8':1,'7':2,'6':3,'5':4,'4':5,'3':6,'2':7,'1':8} self.adapty = {'a':0,'b':1,'c':2,'d':3,'e':4,'f':5,'g':6,'h':7,'i':8} self.coords_hx = { 837:'1', 766:'2', 695:'3', 624:'4', 553:'5', 482:'6', 411:'7', 340:'8', 269:'9' } self.coords_hy = { 589:'i', 518:'h', 447:'g', 376:'f', 305:'e', 234:'d', 163:'c', 92:'b', 21:'a' } # x es el 1er ndice self.matrix = [ [1,1,1,1,1,1,1,1,1], [0,1,0,0,0,0,0,1,0], [1,1,1,1,1,1,1,1,1], [0,0,0,0,0,0,0,0,0], [0,0,0,0,0,0,0,0,0], [0,0,0,0,0,0,0,0,0], [1,1,1,1,1,1,1,1,1], [0,1,0,0,0,0,0,1,0], [1,1,1,1,1,1,1,1,1] ] def empty(self, coords_h): self.matrix[self.adapty[coords_h[1]]][self.adaptx[coords_h[0]]] = false def fill(self, coords_h): self.matrix[self.adapty[coords_h[1]]][self.adaptx[coords_h[0]]] = true def get_hcoords(self, coords): return str(self.coords_hx[coords[0]])+str(self.coords_hy[coords[1]]) def check_ln(self, h_begin, h_destiny): cursorx = self.adaptx[h_begin[0]] cursory = self.adapty[h_begin[1]] while cursory != self.adapty[h_destiny[1]]+1: cursory -= 1 if self.matrix[cursory][cursorx] == true: return false return true def check_lb(self, h_begin, h_destiny): cursorx = self.adaptx[h_begin[0]] cursory = self.adapty[h_begin[1]] while cursory != self.adapty[h_destiny[1]]-1: cursory += 1 if self.matrix[cursory][cursorx] == true: return false return true def check_t(self, h_begin, h_destiny): cursorx = self.adaptx[h_begin[0]] cursory = self.adapty[h_begin[1]] destx = self.adaptx[h_destiny[0]] desty = self.adapty[h_destiny[1]] if cursorx == destx: mod = 1 if cursory < desty else -1 while cursory != desty - mod: cursory += mod if self.matrix[cursory][cursorx] == true: return false return true else: mod = 1 if cursorx < destx else -1 while cursorx != destx - mod: cursorx += mod if self.matrix[cursory][cursorx] == true: return false return true def check_b(self, h_begin, h_destiny): cursorx = self.adaptx[h_begin[0]] cursory = self.adapty[h_begin[1]] destx = self.adaptx[h_destiny[0]] desty = self.adapty[h_destiny[1]] modx = 1 if cursorx < destx else -1 mody = 1 if cursory < desty else -1 while (cursorx != destx - modx) or (cursory != desty - mody): cursorx += modx cursory += mody if self.matrix[cursory][cursorx] == true: return false return truecoords_manager handles dictionaries of coords of pieces that are present on the board. it also handles revertion of board. there is a list for each kind of piece (promoted pieces count as kind of piece for this) for each player. doing it like this (instead of a global list for all pieces with a parameter to determine the kind of piece) allows me to make loops to iterate only over certain kind of piece. another secondary function of coords_manager is to provide dictionaries to translate human-readable coords in numeric coords to blit the sprites. every list of pieces gets the coords in numeric version.kinds of pieces are abbreviated this way:s -> if present, it would mean promoted piecep -> english abbreviature for the kind of piece - pawn in this casen -> player's color (n if 'black' or 'sente', b if 'white' or 'gote')when i noticed that i needed a way to prevent pieces jumping over other pieces, i created matrix_manager to resolve the problem. it basically handles a matrix of boolean values to keep in account which squares are filled by a piece.the point i'm trying to make is that i want to make a move class to clean up the code to prevent abuse of exec statements and regexp matching, as pointed in my previous question. but i'm convinced that, before doing such a thing, i should improve the managers first, as @200_success suggested me in the previous question.which ways would you try to improve the board managers? should i join those classes into a single one for a better modelling? which deficiencies or redundancies am i not noticing in these managers?i started this project at first just because i wanted a way to read my shogi notations. when the basic functionality worked, i decided to make the code public under gpl license for sharing, and with the hope that some entusiasts would join me to suggest improvements or even to work on the code if they felt willing to. so, you will find extensive general information in the readme section in github. any little contribution is highly appreciated.",
    "present_kp": [
      "chess"
    ],
    "absent_kp": [
      "python",
      "object oriented",
      "python 2.7",
      "pygame"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "problems understanding the union functionality of the union-find algorithm. i am currently doing a course based on algorithms (coursera). i've come across an algorithm called quick find. the course does have reference to big o notation. despite the fact that i do not have much knowledge of big o, i have been doing some basic reading.implementation (java):initializes the array:public quickfinduf(int n){ id = new int[n]; for (int i = 0; i < n; i++) id[i] = i;}checks if two input values are connected:public boolean connected(int p, int q){ return id[p] == id[q];}union implementation:public void union(int p, int q){ int pid = id[p]; int qid = id[q]; for (int i = 0; i < id.length; i++) { if (id[i] == pid) { id[i] = qid; } }}the id variable is a private instance variable.my problem:1) in the study material it says:it takes n2 array accesses to process a sequence of n union commands on n objectshow can it take n2 if the union method only has a single for loop?2) it also says that the union method takes at most 2n + 2 array accesseswhat does this mean? my speculation is that 2n could mean that the array is accessed twice (line 7 and line 9 of the union method). i have no idea what + 2 refers to though.i apologize if i have incorrectly stated something.any help would be highly appreciated. thank you.",
    "present_kp": [
      "algorithms"
    ],
    "absent_kp": [
      "asymptotics",
      "union find"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how reliable is the number of hits provided by google site: command?. a partner of my company (not sure whether i can tell their name here - ill use example.com instead) asked me yesterday, why there is such a big difference when they enter the site command at google at first with www and then without www, to check the number of indexed pages.first of all, the google webmaster tools show about 9.000.000 indexed pages.site:example.com provides me about 10.000.000 hits.site:<url> are only about 4.000.000 hits.i first thought that they have some subdomains registered, such as blog.example.com, or test.example.com ...but entering site:example.com -site:<url> to detect pages from subdomains other than www, brings up exactly 2 hits.where are the other 6.000.000 ?the next thing is, providing google a little bit more in the query such as site:<url> in (or other linking words) brings me more hits than the blank site command with www (site:<url>). sometimes about 2.000.000 more than the blank www query...is the number of hits really that inaccurate?",
    "present_kp": [
      "google"
    ],
    "absent_kp": [
      "google search console",
      "google search"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "asp.net mvc create a draft without persisting to db. i have a requirement to allow users to suggest updates to their profiles on a site i'm working. they would be able to view their profile details, then click on and edit button, which would basically make a copy of all their data contained in a handful of one-to-many related db tables. they can then add or remove items and update various text fields before saving the draft. upon saving, an administrator would have to approve this change before the new data is actually published onto the live site.i currently have an identical set of tables for live data and drafts (and history, but that's not important). my difficulty is that the client doesn't want an actual draft entry created in the drafts db structure until the user has hit save. a user would be able to perform crud operations against all the data, including uploading new images, but i am not allowed to touch the db unless they're ready to commit these updates.my great problem is that i have no idea how to begin developing what seems to the client to be a very simple thing, even though it's technically quite challenging. i also can't seem to make the client understand that hitting the cancel button next to the save button would remove the entry plus all its related data from the db, making the problem moot in my humble opinion.so, the question is such: is there a way to phrase my objection in such a way that the client can understand what i'm getting at. or failing that, what would be a good way of developing such a session-stored object?",
    "present_kp": [
      "asp.net mvc"
    ],
    "absent_kp": [
      "c#"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "watching facebook videos without flash. i do not have flash installed on my computer. is there any way to watch videos on facebook without using flash?",
    "present_kp": [
      "facebook",
      "video"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "while(true) and loop-breaking - anti-pattern?. consider the following code:public void dosomething(int input){ while(true) { transforminsomeway(input); if(processingcomplete(input)) break; dosomethingelseto(input); }}assume that this process involves a finite but input-dependent number of steps; the loop is designed to terminate on its own as a result of the algorithm, and is not designed to run indefinitely (until cancelled by an outside event). because the test to see if the loop should end is in the middle of a logical set of steps, the while loop itself currently doesn't check anything meaningful; the check is instead performed at the proper place within the conceptual algorithm.i was told that this is bad code, because it is more bug-prone due to the ending condition not being checked by the loop structure. it's more difficult to figure out how you'd exit the loop, and could invite bugs as the breaking condition might be bypassed or omitted accidentally given future changes.now, the code could be structured as follows:public void dosomething(int input){ transforminsomeway(input); while(!processingcomplete(input)) { dosomethingelseto(input); transforminsomeway(input); }}however, this duplicates a call to a method in code, violating dry; if transforminsomeway were later replaced with some other method, both calls would have to be found and changed (and the fact that there are two may be less obvious in a more complex piece of code).you could also write it like:public void dosomething(int input){ var complete = false; while(!complete) { transforminsomeway(input); complete = processingcomplete(input); if(!complete) { dosomethingelseto(input); } }}... but you now have a variable whose only purpose is to shift the condition-checking to the loop structure, and also has to be checked multiple times to provide the same behavior as the original logic.for my part, i say that given the algorithm this code implements in the real world, the original code is the most readable. if you were going through it yourself, this is the way you'd think about it, and so it would be intuitive to people familiar with the algorithm.so, which is better? is it better to give the responsibility of condition checking to the while loop by structuring the logic around the loop? or is it better to structure the logic in a natural way as indicated by requirements or a conceptual description of the algorithm, even though that may mean bypassing the loop's built-in capabilities?",
    "present_kp": [],
    "absent_kp": [
      "programming practices",
      "language agnostic",
      "readability"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "find or create kernel configuration of kernel binary. i've downloaded a kernel binary which i am using now. in order to use the watchdog on my system i must recompile the kernel with watchdog support. is it possible to obtain the current kernel configuration of the binary?the binary is obtained from this page. i've used version r5.",
    "present_kp": [
      "kernel",
      "configuration"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "parsing a complex number using regular expressions. requirements were to create a constructor for a complex number, that receives a string. for example: \\$3+5i\\$. the regex extracts positive or negative numbers from the string with optional decimal.my professor told me that my regex was beyond wrong:public complex(string str) { arraylist<double> list = new arraylist<double>(); pattern p = pattern.compile([-+]?[0-9]*\\.?[0-9]); // tostring() handles the letter i // find positive and negative doubles in string and add to list matcher m = p.matcher(str); while (m.find()) { double num = double.parsedouble(m.group()); list.add(num); } this.re = list.get(0); this.im = list.get(1);}here is a link to the class if you need a better understanding of the code.he then posted his chosen correct solution, which i found to be repetitive. he may argue that i am not checking for i, but i see no point of storing i if we are not using it later.in addition, if he argues that i am not checking for and storing i, that would mean that if i did store them, i would explicitly have to go exclude the i from the string when i perform mathematical operations on the numbers. it seems counter-productive.here is the solution he preferred:public complex(string c) { string numbernowhitespace = c.replaceall(\\s,); // matches complex number with both real and imaginary parts. // ex: -3-2.0i pattern patterna = pattern.compile(([-]?[0-9]+\\.?[0-9]?)([-|+]+[0-9]+\\.?[0-9]?)[i$]+); // matches only real number. // ex: 3.145 pattern patternb = pattern.compile(([-]?[0-9]+\\.?[0-9]?)$); // matches only imaginary number. // ex: -10i pattern patternc = pattern.compile(([-]?[0-9]+\\.?[0-9]?)[i$]); matcher matchera = patterna.matcher(numbernowhitespace); matcher matcherb = patternb.matcher(numbernowhitespace); matcher matcherc = patternc.matcher(numbernowhitespace); if (matchera.find()) { real = double.parsedouble(matchera.group(1)); imaginary = double.parsedouble(matchera.group(2)); } else if (matcherb.find()) { real = double.parsedouble(matcherb.group(1)); imaginary = 0; } else if (matcherc.find()) { real = 0; imaginary = double.parsedouble(matcherc.group(1)); } }",
    "present_kp": [
      "parsing",
      "regex",
      "constructor"
    ],
    "absent_kp": [
      "java",
      "comparative review"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "scraping pubmed query results. how can i make the following php code better, more efficient, shorter, elegant, etc? while it already works, i am still learning php and want to improve upon my current code:<?php$query = 'psoriasis';$esearchqueryparameters = array( 'db' => 'pubmed', 'term' => $query, 'retmode' => 'xml', 'retstart' => '0', 'retmax' => '500', 'usehistory' => 'y',);$esearchqueryresults = simplexml_load_file('<url> . http_build_query($esearchqueryparameters));$efetchqueryparameters = array( 'db' => 'pubmed', 'retmax' => '500', 'query_key' => (string) $esearchqueryresults->querykey, 'webenv' => (string) $esearchqueryresults->webenv,);$efetchurl = '<url> . http_build_query($efetchqueryparameters);$matches = array();preg_match_all('/[a-za-z0-9._%+-]+@[a-za-z0-9.-]+\\.[a-za-z]{2,4}/', file_get_contents($efetchurl), $matches);foreach ($matches[0] as $key => $value) { echo $value . <br />;}?>",
    "present_kp": [
      "php",
      "url"
    ],
    "absent_kp": [
      "web scraping"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "apache 2.4 basic auth don't work. i have a problem with basic auth in apache 2.4. i have these lines:<virtualhost *:80> serveradmin <email> documentroot /var/www/html/foo servername my.domain.com<directory /var/www/html/foo/> options followsymlinks require all granted directoryindex index.php authtype basic authname authentication required authuserfile /etc/httpd/.htpasswd require valid-user</directory></virtualhost>authentication is bypassed and shows the site without a password request.",
    "present_kp": [
      "authentication"
    ],
    "absent_kp": [
      "apache httpd"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "can sitemaps for multiple sites be added to google webmaster tools through a single universal account?. i had a lot of client websites and recently i had to do seo for about 3-4 websites simultaneously. it was all simple on page seo, but i have a question is about xml map submission. i realize that creating the xml sitemap and adding it to the root directory is critical. after that, you submit the xml sitemap to google. for each website sitemap, do i need a new webmaster account? or can i have a universal account from which i can submit the sitemap for each of my client websites?",
    "present_kp": [
      "xml sitemap"
    ],
    "absent_kp": [
      "google search console"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to prove that algorithm returns the value which appears more than $n/2$ times in the array?. given the following algorithm (pseudocode):alg1(a[1,..,n]) i <- 1 candidate <- null count <- 0 while(i <= n) if(count = 0) candidate <- a[i] count <- count + 1 else if(candidate = a[i]) count <- count + 1 else count <- count - 1 i <- i + 1 return candidatei need to prove that if a value appears more than $n \\over 2$ times (let the value be named $m$) in the array then it will be returned. $\\mathbf{edit:}$ here's my 2 attempt. first we assume that the array $a$ contains at least 3 elements (if less then the proof is trivial). the element which appears more than $n/2$ times is $m$ else it's $m'$. let a two-element combination of $mm'$ or $m'm$ be called $c$. every time we traverse a $c$ the count decreases by one. case 1: if the pattern is strictly alternating combinations of type $c$ then the first and the last element must be $m$ (otherwise $m$ will not appear more than $n/2$ times) and this means that after the last iteration the candidate is $m$.case 2: we can see that after any number of adjacent $c$'s the counter will be $0$ and the candidate will be either $m$ or $m'$. it is given that more than $n/2$ elements of $m$ do exist in the array, so the number of $c$ combinations must be supplemented by at least $1$ occurrence of $k$ adjacent elements of $m$ either before $c$'s or after $c$'s. if it's after, then the next candidate is $m$ and it will continue to be so as long as we keep traversing $k$ elements of adjacent $m$'s. if it's before than the count will be at least $1$ before it proceeds to iterate through $c$'s and essentially it'll behave just like case 1.i'm still aware that the proof is not perfect but i feel like it's an improvement. suggestions/advice is very welcome.",
    "present_kp": [],
    "absent_kp": [
      "correctness proof"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "send server side file as attachment in a web application. in our web application development project, we need a feature that lets the users to email themselves an attachment (~1mb) which resides on the server.i was considering two alternatives. upon clicking on the email buttondownload the attachment to local folder and attach it to a new email window in outlook. the flaw with this approach is that we are assuming user has outlook installed on their machine. i am also doubtful if browsers others than ie support this feature of being able to open outlook message and attach a file.send the attachment from middleware code to the user using a programmatic api for sending email. the downside of this approach is that it doesn't give the flexibility to the user to use the out of the box outlook features such as being able to add more recipients / cc / bcc, edit the message etc, though it is possible to let the user do the same by providing a ui which lets them do all that. what would be the recommended approach to this requirement?",
    "present_kp": [
      "email"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "why does cp not set source attributes and fail with -p?. i have these files:-rw-rw-r-- 1 root adm 0 jun 22 11:25 a-rw-rw-r-- 1 wilmes wilmes 0 jun 22 11:23 bwhen i cp b over to a as user wilmes why does it keep the original owner of a (root)?i use this command (no aliases): cp b awhy does it fail when i use -p in command cp -p b a ?cp: preserving times for 'a': operation not permittedi noticed this on ubuntu 17.04 with ext4. user wilmes is member of group adm and the containing directory looks like this:drwxrwxr-x 2 wilmes wilmes 4,0k jun 22 12:09 ../user/and most important: where is this documented?",
    "present_kp": [
      "cp"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to automatically launch a lxc container on ssh connection?. i'm attempting to setup a system that automatically creates a new sandbox on a ssh login to use as a temporary jump box into my server. so to do this i was wonder how to setup lxc to spin up a new shell in the container once there is a ssh connection then destroy that container after the session is closed.what would be the best way to go about this?edit:thanks to everyone that has placed their input. i have been able to devise the method to do this same setup as follows:/etc/ssh/sshd_config:match group ssh forcecommand sudo docker run --rm -t -i busybox /bin/shwhat this does is forces the user session to instantly go into a busybox container then delete the changes on exit. one could change or create their own image and specify this in place of busybox. though this is included here since the only thing the default busybox container offers is wget and telnet which is good enough for most oob testing/jump-boxes and this was the use case goal with this design.",
    "present_kp": [
      "shell",
      "ssh",
      "lxc",
      "docker"
    ],
    "absent_kp": [
      "deployment"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "set initramfs to prompt for luks passowrd on startup on mint 18?. after updating to mint 18.1, i can't get initramfs to prompt for a password to unlock the volume with the root file system on it. i have to wait until initramfs times out to a prompt then run cryptsetup luksopen manually.i've tried running update-initramfs while the system is mounted and running (as well as from the live cd in chroot) and i have an entry in /etc/cryptab.this was working for me before the mint 18 upgrade, but for some reason i'm still not getting a password prompt now no matter what i try.what should i check?",
    "present_kp": [
      "initramfs"
    ],
    "absent_kp": [
      "linux mint",
      "boot",
      "grub"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "finding 10-unique-digit number with i-first-digits divisible by i. i was posed this question by a friend:i am a natural number written with 10 different digits such that for all \\$i {1, . . ., 10}\\$ the number formed by my \\$i\\$ first digits (from left to right) is divisible by \\$i\\$. who am i?i came up with a brute-force solution over the next hour or so (a few attempts failed) and came eventually finished with this:# !/usr/bin/python# -*- coding: utf-8 -*-# build all possible permutationsimport itertoolsbase_number = ['1','2','3','4','5','6','7','8','9','0']perms = itertools.permutations(base_number)# join the permutation lists into single numbers by joining each into a stringall_nums = [''.join(item[0:10]) for item in perms]''' check whether each item is divisible by i (from 10-->2) for subset of item, item[0:i]. for example; i = 10, number = <phone> number[0:10] = <phone> is this divisible by i(10)? yes. i = 9, number = <phone> number[0:9] = 123456789 is this divisible by i(9)? yes. i = 8, number = <phone> number[0:8] = <phone> is this divisible by i(8)? no.we do not check whether the number is divisible by 1 as all integers are divisible by 1.''' for i in range(10,1,-1): legits = [x for x in all_nums if int(x[0:i])%i==0] all_nums = legits print %-10d # of result can be divided by %d % (len(legits),i)print legitsi considered that a recursive algorithm could perform this task much more efficiently but have not considered how to actually code the pseudo-code'ish solution i was shown;f(1) = {1, 2, 3, 4, 5, 6, 7, 8, 9}f(2) = for n in f(1): find solutions to i for $$n * 10 + i = 0 (mod 2)$$ where i isn't a digit of n.i welcome all feedback, but i'm specifically hoping to hear about my coding-style/practices and the algorithm i developed.",
    "present_kp": [
      "python",
      "algorithm"
    ],
    "absent_kp": [
      "python 2.7"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "compare two input and merge the difference. how can i compare two input strings and merge differences and get the result string like below with tools like diff?e.g. when i have two stringsunix&linux stackoverflow unix&linuxand unixandlinux stackexchangei'd like to get a string uunix&andllinux stackoverflowexchange unix&linux",
    "present_kp": [
      "diff"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "printing all binary strings of length n. i have completed my homework with directions as follows:declare and implement a class named binary. this class will have a method named printb(int n) that prints all binary strings of length n. for n = 3, it will print000001010011100101110111in this order.here is my code:import java.util.scanner;class binary{ string b; int temp; void printb(int n) { for(int i = 0; i < math.pow(2,n); i++) { b = ; int temp = i; for (int j = 0; j < n; j++) { if (temp%2 == 1) b = '1'+b; else b = '0'+b; temp = temp/2; } system.out.println(b); } } }class runner{ public static void main(string [] args) { scanner in = new scanner(system.in); system.out.print(enter n:); int n = in.nextint(); binary myb = new binary(); myb.printb(n); }}my question is...is there anyway to make this shorter or more efficient?",
    "present_kp": [
      "java",
      "homework"
    ],
    "absent_kp": [
      "combinatorics",
      "formatting",
      "number systems"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "command-line incremental backup tools. which command line backup tool(s) has the best compression ratio? i want to backup my entire system, including media files, text files etc. i found this list on the arch wiki site but i don't know how these tools compare to each other and which one would offer the best compression ratio overall. i realize that different tools might give better results for specific file types, but which tool and using which settings would result in the smallest archive given a mix of various input files?",
    "present_kp": [
      "command line",
      "backup",
      "compression"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "solving the recurrence relation $t(n) = 2t(\\lfloor n/2 floor) + n$. solving the recurrence relation $t(n) = 2t(\\lfloor n/2 floor) + n$.the book from which this example is, falsely claims that $t(n) = o(n)$ by guessing $t(n) \\leq cn$ and then arguing $\\qquad egin{align*} t(n) & \\leq 2(c \\lfloor n/2 floor ) + n \\ &\\leq cn +n \\ &=o(n) \\quad \\quad \\quad \\longleftarrow ext{ wrong!!} \\end{align*}$ since $c$ is constant.the error is that we have not proved the exact form of the inductive hypothesis.above i have exactly quoted what the book says. now my question is why cannot we write $cn+n=dn$ where $d=c+1$ and now we have $t(n) \\leq dn$ and hence $t(n) = o(n)$?note: the correct answer is $t(n) =o(n \\log n).$ the book i am referring here is introduction to algorithms by cormen et al., page 86, 3rd edition.",
    "present_kp": [
      "recurrence relation"
    ],
    "absent_kp": [
      "proof techniques",
      "asymptotics",
      "landau notation",
      "induction"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "java 8 stream to produce all permutations of an array using recursion. i want to write a class that returns a stream of permutations of an int[].public class permutations { public static stream<int[]> of(int[] array) { return permute(array, array.length); } public static stream<int[]> permute(int[] array, int n) { if (n == 1) { return stream.of(arrays.copyof(array, array.length)); } else { stream tmp = stream.empty(); for (int i = 0; i < n; i++) { swap(array, i, n - 1); tmp = stream.concat(permute(array, n - 1), tmp); swap(array, i, n - 1); } return tmp; } } private static void swap(int[] a, int i, int j) { if (i != j) { int tmp = a[i]; a[i] = a[j]; a[j] = tmp; } }}problem with my solution is that it is nesting a lot of streams with concat, many of them being just empty ones. another issue is that it creates all permutations before returning the stream, so it not really streaming... :- )i saw minborg's solution, but i wanted to make something simpler, here based on sedgewick and wayne's algorithm.can above code be improved?",
    "present_kp": [
      "java",
      "recursion",
      "stream"
    ],
    "absent_kp": [
      "combinatorics"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "creating javafx bindable property readed from joystick. i'm lately messing up with handling joystick in java.so here is the thing. i wrote a simple class that handle input from joystick, but i'm no certain if i'm doing it well. here is what i want to achieve. so i have joypad, i'm using jinput from lwjgl 2, and want to bind axis and buttons state to javafx application, and than send it to my arduino rc car. it's working, but i'm just wondering is there more efficient way to read joystick input? now i have that thread which never ends and read joystick value. i was looking for some event handler but non of them works. if anyone have some suggesions i would appreciate it. if none of above is clear so here is todo list:i have joypad which i'm reading data.i'm wonder what is most efficient way to read data from joystick?i'm using lwjgl 2 jinput.is there any way to get rid of that thread?controller.classpackage wcc;import java.util.arraylist;import org.lwjgl.input.controller;import javafx.beans.property.simplebooleanproperty;import javafx.beans.property.simplefloatproperty;public class fxcontroller implements runnable{ private boolean threadalive = true; private controller controller; private arraylist<simplefloatproperty> axis = new arraylist<>(); private arraylist<simplebooleanproperty> buttons = new arraylist<>(); public fxcontroller(controller controller){ this.controller = controller; for(int i = 0; i < this.controller.getaxiscount(); i++){ this.axis.add(new simplefloatproperty(0)); } for(int i = 0; i < this.controller.getbuttoncount(); i++){ this.buttons.add(new simplebooleanproperty(false)); } } public arraylist<simplefloatproperty> getaxisproperty(){ return this.axis; } public arraylist<simplebooleanproperty> getbuttonproperty(){ return this.buttons; } public controller getcontroller(){ return this.controller; } public void stopthread(){ this.threadalive = false; } public boolean isthreadalive(){ return this.threadalive; } @override public void run() { while(threadalive){ for(int i = 0; i < this.controller.getaxiscount(); i++){ this.axis.get(i).set(this.controller.getaxisvalue(i)+1); } for(int i = 0; i < this.controller.getbuttoncount(); i++){ this.buttons.get(i).set(this.controller.isbuttonpressed(i)); } this.controller.poll(); } }}@fxml controllerpackage wcc;import java.net.url;import java.util.resourcebundle;import org.lwjgl.input.controllers;import javafx.fxml.fxml;import javafx.fxml.initializable;import javafx.geometry.insets;import javafx.scene.control.combobox;import javafx.scene.control.progressbar;import javafx.scene.control.titledpane;import javafx.scene.control.togglebutton;import javafx.scene.image.image;import javafx.scene.image.imageview;import javafx.scene.layout.gridpane;import javafx.scene.layout.tilepane;import javafx.scene.layout.vbox;public class wcccontroller implements initializable{ @fxml gridpane _root; @fxml combobox<string> _controllers; @fxml vbox _axisbox; @fxml tilepane _buttonsgrid; @fxml imageview _cameraview; fxcontroller selected = null; @override public void initialize(url arg0, resourcebundle arg1) { try{ controllers.create(); for(int i = 0; i < controllers.getcontrollercount(); i++){ this._controllers.getitems().addall(controllers.getcontroller(i).getname()); } new thread(new runnable(){ @override public void run() { while(true){ _cameraview.setimage(new image(<url>)); } } },camerathread).start();; } catch(exception ex){ ex.printstacktrace(); } } @fxml private void oncontrollerselectionchanged(){ this._axisbox.getchildren().clear(); if(this.selected != null){ this.selected.stopthread(); } this.selected = new fxcontroller(controllers.getcontroller(this._controllers.getselectionmodel().getselectedindex())); new thread(this.selected, controller thread.).start(); for(int i = 0; i < this.selected.getcontroller().getaxiscount(); i++){ progressbar axisvalue = new progressbar(); axisvalue.setpadding(new insets(8)); axisvalue.progressproperty().bind(this.selected.getaxisproperty().get(i).divide(2)); titledpane axisname = new titledpane(); axisname.settext(axis: [ + this.selected.getcontroller().getaxisname(i) + ]); axisname.setcollapsible(false); axisname.setcontent(axisvalue); this._axisbox.getchildren().add(axisname); } this._buttonsgrid.getchildren().clear(); this._buttonsgrid.setprefcolumns(4); for(int i = 0; i < this.selected.getcontroller().getbuttoncount(); i++){ togglebutton buttonstate = new togglebutton([o]); buttonstate.setpadding(new insets(8)); buttonstate.setdisable(true); buttonstate.selectedproperty().bind(this.selected.getbuttonproperty().get(i)); titledpane buttonname = new titledpane(); buttonname.settext([ + this.selected.getcontroller().getbuttonname(i) + ]); buttonname.setcollapsible(false); buttonname.setcontent(buttonstate); this._buttonsgrid.getchildren().add(buttonname); } }}",
    "present_kp": [
      "java",
      "controller",
      "javafx"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how can i access gmx calendar outside of web ui?. gmx provides:a mail service accessible through imap, a file service accessible through webdav,a calendar service.but i've never found if there is any way of accessing that calendar service through a standard protocol (caldav or ical). is there such an access?",
    "present_kp": [
      "calendar",
      "gmx"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "data structures for ordering noisy data. in a certain robotics application, i encountered a problem in which we need to determine the order of positions of several robots on $\\mathbb{r}$. each measurement that we take of robot positions is subject to random errors beyond our control. i've formalised the algorithmic part of the problem as follows. this part has been reformulated as per d.w.'s comments.the positions of $n$ robots are given to us in a $3 imes n$ array, each row being of the form $[i, a, \\epsilon]$.this row states that the position of the robot with id $i$ is uniformly distributed on the interval $a \\pm \\epsilon$.problem : output the most probable ordering (mpo) of the robot positions,or one such if this is non-unique.example input: $egin{bmatrix} \\mathrm{id} & a & \\epsilon \\ \\hline 1 & 1& 0.5 \\ 2& 1.5 & 2 \\end{bmatrix}$. associate the two robot positions with random variables $x_1$ and $x_2$. note that the pair $(x_1,x_2)$ is uniform on the rectangle $[0.5,1.5] imes [-0.5,3.5]$. the probability $p_1$ (resp. $p_2$) that $x_1\\leq x_2$ (resp. $x_1 \\geq x_2$) is the area of this rectangle below (resp. above) the line $x_1\\leq x_2$. if $p_1 \\leq p_2$, then the mpo is $[1,2]$ , else $[2,1]$. what data structure efficiently computes the mpo? i'm most interested in the range $n\\in [10,1000]$.simple idea that doesn't work (thanks to d.w.): when $n\\geq 3$, the mpo isn't equal to the sorted order of the means $a_i$. for example, the input $(1, 0.4\\pm0), (2,0.5\\pm 0.5), (3,0.7 \\pm 0)$ has the mpo $[2,1,3]$ and not $[1,2,3]$ as the $a$'s would have us think. one possible approach uses the sweep line algorithm. think of a robot position as a rectangle corresponding to its uniform pdf. compute all intersecting rectangles using the priority queue of the sweep line, and reorder them as per the probability calculation above. (it may be possible to reduce the $o(n^2)$ pairwise comparison of probabilities to $o(n \\log n)$; i haven't checked this though.) non-overlapping rectangles leave their ordering intact.intuitively, large error bounds lead to long rectangles and potentially more overlaps, consequently increasing the running time. likewise, small error bounds cause few overlaps, speeding up computations. this fact will be reflected in the sweep line.",
    "present_kp": [
      "data structures"
    ],
    "absent_kp": [
      "randomized algorithms",
      "permutations"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is the language $l = \\{a^nb^m : n = 2^m\\}$ context-free?. is the language $l = \\{a^nb^m : n = 2^m\\}$ context-free?assume l is a context-free language. then $\\ \\exists p\\in \\mathbb{z}^{+}: orall s\\in l\\left | s ight |\\geq p. s = uvxyz,\\left | vy ight |\\geq 1,\\left | vxy ight |\\leq p. s_i = uv^{i}xy^{i}z\\in l orall i\\geq 0\\ $.let s = $\\ a^{2^p}b^{p}\\ $pumping i times will give a string of length $\\ 2^{p} + (i - 1)*j\\ $ a's and $\\ p + (i - 1)*k\\ $ b's where $\\ 1 \\leq j + k \\leq p\\ $case 1: $\\ j eq 0\\ $ $\\ k eq 0\\ $??case 2: $\\ j = 0\\ $ $\\ k eq 0\\ $??case 3: $\\ j eq 0\\ $ $\\ k = 0\\ $??it can be concluded from this that l is not a context-free language.",
    "present_kp": [],
    "absent_kp": [
      "pumping lemma"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "wifi usb stick not recognised by beaglebone black. i'm trying to set up a tp-link tl-wl725n v2 wifi usb stick on my beaglebone black.however, it doesn't even appear to be recognised, i.e. it doesn't show up in the output of lsusb:$ lsusb bus 001 device 001: id 1d6b:0002 linux foundation 2.0 root hub bus 002 device 001: id 1d6b:0002 linux foundation 2.0 root hubneither dmesg nor the syslog contain any new messages when the stick is plugged in or unplugged.oddly enough, the stick even seems to break the usb port for the rest of the session: after plugging and unplugging the wifi stick, a memory stick that previously worked fine was no longer recognised until i rebooted the beaglebone.the same wifi stick works flawlessly on a laptop running ubuntu 12.04.3 with a 3.11.0 kernel and a raspberry pi running raspbian with a 3.10.24 kernel.relevant lsusb line: bus 001 device 006: id 0bda:8179 realtek semiconductor corp.however, the raspberry reboots whenever i plug in the stick, so it might be drawing too much current.i'm not quite sure how to proceed:does this indicate a hardware failure (presumably of the wifi stick)? or is there anything else i could try to at least get the stick recognised on the beaglebone?(i'm aware that i will have to compile the necessary kernel module myself, just like on the raspberry pi, but that's no use if the stick isn't even recognised...)editi've checked the supply voltage on the usb port and it is actually switched off the moment i connect the wifi stick. i tried forcing it back on by pulling the !en pin on the tps2051 switch ic low, but that didn't help (i.e. the voltage came back on, but the system didn't seem to poll the bus anymore).i've googled for the usb1_drvvbus output on the processor that drives the input of the switch ic and it seems that the supply for the usb host port is automatically disabled when an over-current condition is detected.that explains why the stick crahes the raspberry pi and fails to work on the beaglebone, but leaves me to wonder if it might be broken, since several raspberry users have reported that it worked for them without a powered hub.second editi perused the schematic some more and bypassed the switch ic and current sensing resistor, but even then the the usb host section is disabled as soon as the wifi stick is connected.",
    "present_kp": [
      "usb",
      "wifi"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "expression problem looking for a similar standard problem. the expression problem, populated by philip wadler, is a often used to standard problem to evaluate programming languages.i think it is a very clear and popular example and i wonder if there are any similar standard problems that are possibly also as widely used and as clear.so, are there any similar standard problems?(in the case of feature oriented programming (link link) i found some standard problems, like:- implementation of a stack with different features- implementation of linked lists- implementation of a calculator- the graph product line- stock broker and bank account examples- hierarchical display)",
    "present_kp": [
      "programming languages"
    ],
    "absent_kp": [
      "reference request",
      "software engineering"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "asking a specific question from a general one. settingi have data about a board game which has been played multiple times with different players per game (though players reoccur in all kinds of different constellations). during the game one can score points in one of seven categories. the general question is: what is the best strategy? on what category should players concentrate?details about the data and performed transformationsthe original data has the variables:game - the number of the gamename - the name of the playermilitary pointsmoney pointsworld wonder points... pointssum of pointswhich i transferred intogamenamesum of points (a little repetitive, since its the same for a game and name)place in game (a little repetitive, since its the same for a game and name)type of points (one of the seven categories)number of pointspercentage this type of points contributed to the total amount of points of this player for this gamemy questionasking the right question is been said to be the most important thing in data science and in most situations i managed to find a good concrete (programmable) question, but this one seems tough to me.so: how do i ask a specific question to answer the general question?of course i have already asked/answered simple questions regarding the winning players of each game. for example: what is the type of points that contributes most to the total points for winning players (whose success proved them right)?now i'd like to make analyses over the entire field of players.an even more advanced question would consider that player reoccur across games.i have no idea what tag to use here.",
    "present_kp": [],
    "absent_kp": [
      "simulation"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "which http/https www/no-www versions should be registered in search console when using cloudflare flexible ssl. i have a question about the use of cloudflare flexible ssl and the settings in google search console. say i use flexible ssl on my website example.com. i have wrote a redirect from http to https in my .htaccess so every request on <url> redirects to <url> do i need to register on search console? right now i have registered all four possible urls, meaning <url>, <url>, <url> and <url>.",
    "present_kp": [
      "google",
      "https",
      "search",
      "cloudflare"
    ],
    "absent_kp": [
      "seo"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what do you believe is a better of method of learning languages: using books or jumping straight into a project?. do you find that it is better to learn through reading books or to just jump straight into a project and pick up what you need to know using the web, or some combination of both?",
    "present_kp": [
      "learning",
      "books"
    ],
    "absent_kp": [
      "programming languages"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "framework logs on client application. we are currently building a framework (closed source static library) that will communicate with some smart home devices via wi-fi. this framework will be used by 3rd party developer to build their own applications (mainly mobile application) in order to communicate with those devices.currently, we have split opinions on if the framework should generate any visible logs (say a log file, or event logger) on a release version (we will supply a debug and a release version to 3rd party developers).reasons to have logs:logs are always helpful if we need to find out the root cause of unexpected error.any form of logs is always goodgetting the logs information from mobile phone should be easy. (by app user or support desk technician)we can prove it is application developers' fault if they blame it to us.some issues may only happen in production that cannot be reproduce on test environment.log files are not big anyway. a small log file that use some of the device storage shouldn't be an issue.all servers application/api always have logs.reverse engineer is always possible via decompile, so it shouldn't really matters.reasons to not have logs:framework doesn't own the application. most users have no idea how to get the log files out of application's storage (hence less likely to be able to get it), so don't add something that will not be usedapplication developer should be able to pinpoint the problem by their own logs/debug method before coming to usrisk of exposing too much information to end users.no other frameworks seems to do it (e.g. facebook sdk/google sdk)taking up device storage. every byte count.it is the responsible of the framework user (developer) to have their own logs/crash reporting if they want to.debug version with console/debugger logs should be enough for developers.so basically we are not able to get into an agreement. just wondering what does the wider community think about this if you are the developer that use a closed source static library on a client application.",
    "present_kp": [],
    "absent_kp": [
      "android",
      "ios",
      "logging",
      "third party libraries"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "regex for matching a us phone number. i have written my first regex:static void main(string[] args){ string phonenumber= console.readline(); regex pattern = new regex((1-)?\\p{n}{3}-\\p{n}{3}-\\p{n}{4}\\b); matchcollection collection = pattern.matches(phonenumber); console.writeline(phonenumber + is + (collection.count == 1 && collection[0].tostring() == phonenumber ? : not ) + a valid us number.);}it needs to match the entire input string, and it can work for any us phone number with or without country code 1-. it can most likely be improved; any suggestions?",
    "present_kp": [
      "regex"
    ],
    "absent_kp": [
      "c#",
      ".net"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "record daily internet bandwidth usage. i am using internet connection with data cap. i want to record my daily internet usage in a file, is there any tool for this or perhaps you can suggest a script that would run as daemon?(i am not pro in bash scripting or with linux administrating software so a simple script will be recommended)",
    "present_kp": [
      "internet",
      "bandwidth"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "unable to login as a newly created user. i have a dual boot kali linux along with windows 7. now when i installed kali it logs me in as root. i do not have a login in my name. so i created a user with the following steps. however when i log off and login, there is an authentication failure with the new login. where am i going wrong? laa@aa-lu:~$ sudo useradd testinguser alaa@aa-lu:~$ sudo passwd testinguser enter new unix password: retype new unix password: passwd: password updated successfully alaa@aa-lu:~$ sudo ls -l /home total 20 drwxr-xr-x 55 alaa alaa 4096 aug 22 22:00 alaa drwx------ 2 root root 16384 jun 5 09:46 lost+found alaa@aa-lu:~$ sudo mkdir /home/testinguser alaa@aa-lu:~$ sudo chown testinguser:testinguser /home/testinguser alaa@aa-lu:~$ ls -l /home total 24 drwxr-xr-x 55 alaa alaa 4096 aug 22 22:00 alaa drwx------ 2 root root 16384 jun 5 09:46 lost+found drwxr-xr-x 2 testinguser testinguser 4096 aug 23 10:03 testinguser alaa@aa-lu:~$ ls -l /home/testinguser/ total 0 alaa@aa-lu:~$",
    "present_kp": [
      "kali linux"
    ],
    "absent_kp": [
      "users"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "np-hardness proof, what is wrong with it?. my question is the following: if we have a problem divided into two versions, weighted and unweighted. can we prove that the unweighted problem is np-hard from the fact that the weighted problem is np-hard ?for example let say i have the knapsack problem (the weighted version):$$\\max \\sum_{i}^n w_i x_i$$$$ ext{s.t.} \\sum_{i=1}^n p_i x_i\\leqslant w,\\quad(wp)$$$$x_i\\in\\{0,1\\}.$$this problem is know to be np-hard problem. however, the unweighted version of this problem is given by:$$\\max \\sum_{i}^nx_i$$$$ ext{s.t.} \\sum_{i=1}^n p_i x_i\\leqslant w,\\quad(p)$$$$x_i\\in\\{0,1\\}.$$is not np-hard since the optimal solution is to sort the items in the ascending order with respect to $p_i$ and start filling the bag until the capacity $w$ is reached.the problem is the following: i used the unweighted version of this problem as a special case of the weighted version and i showed that unweighted is an np-hard problem. what is my mistake? here is my steps: i will reduce $(wp)$ to $(p)$ in polynomial time. so, an instance of $(p)$ is constructed easily by taking $w_i=1$. and i am done and hence $(p)$ is np-hard which is obviously wrong.thank you to correct me and give me if possible some good references that deal with such weighted and unweighted problems?",
    "present_kp": [],
    "absent_kp": [
      "complexity theory",
      "optimization",
      "np hard"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "kiosk mode on non chrome os. my team is working on a progressive web app. we've been searching for a solution to so that viewers of our app cannot exit the app (it's an in shop app, which we use at our stores that shows content related to the store). i've found several solutions involving putting physical covers over the devices back/home buttons, as well as installing third party software that disables usage of all other apps/locks the screen to one specific app. i've also found the solution of adding kiosked_enabled:true to the manifest.json file. however, upon viewing this link (<url>) i see that this seems to be a solution specific to chrome os/chrome devices.so my question is: do progressive web apps support kiosk mode on non-chrome devices.?if so, how can this be implemented? if not, what is a solution that my team can implement with our code base to lock the device to only use our application that doesn't use a physical cover over the hardware or a third party app (would love to know how these third party applications do this)?",
    "present_kp": [
      "chrome"
    ],
    "absent_kp": [
      "web applications"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "create the same png with imagemagick 2 times: binaries differ. i create two images that should be identical, but their binaries are different:$ convert -size 1x1 xc:white out1/w.png$ sleep 1$ convert -size 1x1 xc:white out2/w.png$ diff out1/w.png out2/w.png binary files out1/w.png and out2/w.png differprobably because of a timestamp in embedded metadata.question: how to make imagemagick create a binary that will always be the same?contexti have a big imagemagick script that creates many images that are then saved to git (because most developers don't have the environment necessary to run the script).i often edit the script (ex:define a new image) and then run it to regenerate all images. but i don't want to have git differences for images that have not changed.apparently some compression algorithms produce slightly different results on different architectures. not a big problem since i always generate on the same machine. but even on the same machine, the files always all differ.imagemagick 6.8.9-9 q16 x86_64 2015-08-06, on ubuntu 2015.10",
    "present_kp": [
      "binary",
      "imagemagick",
      "png"
    ],
    "absent_kp": [
      "version control"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "kill an existing tcp connection quickly. i'm working on a pentesting project, and i have the following setup: i have an ssh daemon that is trying to log all of the activity happening on ssh connections out to another server. i'm trying to disable this logging, and in particular, do so in a way that avoids being detected. thus, i have the following requirements:i need to be able to kill an existing tcp connection, and ideally with as little buffered data sent as possible. that is, if there's outbound data buffered in the kernel, it would be ideal if this data were dropped without being sent.in order to achieve this, it's best if the command that i need to type in is as short as possible so that the entire command can be buffered either in the logging application or in the kernel tcp buffer. this way, if the first bullet point is achieved, the logging server will never receive information about the fact that logging was disabled; it will just appear as a network outage.i've tried iptables -a output -d <ip of logging server> -j drop, but that doesn't do the trick - the text of the command itself manages to get sent before the rule is enforced. i've also tried tcpkill, similarly to no avail.",
    "present_kp": [
      "ssh",
      "kernel",
      "tcp"
    ],
    "absent_kp": [
      "logs",
      "socket"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how mllib in spark select variables in logistic regression. i have a question about mllib in spark.(with scala)i'm trying to understand how logisticregressionwithlbfgs and logisticregressionwithsgd work. i usually use sas or r to do logistic regressions but i now have to do it on spark to be able to analyze big data.how is the variable selection done? is there any try of different variable combinations in logisticregressionwithlbfgs or logisticregressionwithsgd? something like a test of significance of variable one by one? or a correlation calculation with the variable of interest? is there any calculation of bic, aic to choose the best model?because the model only returns weights and intercept...how can i understand those spark functions and compare to what i'm used to with sas or r ?",
    "present_kp": [
      "logistic regression",
      "scala"
    ],
    "absent_kp": [
      "machine learning",
      "bigdata",
      "apache spark"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "uva #10258 - contest scoreboard. the problem statement is here.problem descriptionthe task is to rank the contestants in the contest based on following criteriacontestants are ranked first by the number of problems solved (the more the better)then by decreasing amounts of penalty timeif two or more contestants are tied in both problems solved and penalty time, they are displayed in order of increasing team numbers.how penalty time is calculatedpenalty time is computed as the number of minutes it took for the first correct submission for a problem to be received plus 20 minutes for each incorrect submission received prior to the correct solution. the penalty time will be considered only if the problem is solved. there will not be any penalty if the problem is not solved even if the several incorrect submissions were made. a problem is solved if any of the submissions for that problem is judged correct. inputthe input begins with a single positive integer on a line by itself indicating the number of the cases following, each of them as described below. this line is followed by a blank line, and there is also ablank line between two consecutive inputs.input consists of a snapshot of the judging queue, containing entries from some or all of contestants 1 through 100 solving problems 1 through 9. each line of input will consist of three numbers and a letter in the format contestant problem time lwhere l can be c, i, r, u or e. these stand for correct, incorrect, clarification request, unjudged and erroneous submission. the last three cases do not affect scoring.lines of input are in the order in which submissions were received.outputfor each test case, the output must follow the description below. the outputs of two consecutive cases will be separated by a blank line.output will consist of a scoreboard sorted as previously described. each line of output will contain a contestant number, the number of problems solved by the contestant and the time penalty accumulated by the contestant. since not all of contestants 1-100 are actually participating, display only the contestants that have made a submission.algorithmfor every test_case for every submission if a new contestant submitted a solution to a problem consider the user for the rest of the contest judge the solution to the problem else if the problem has not already been solved judge the solution to the problem calculate the penalty time for solved problems for every contestant print itdata structurestruct boardcontestant: id of the contestantnproblem: no of problem solved by the contestantproblem[1..9]: 1 if ith problem is solved by the contestant else 0penalty[1..9]: total penalty time for the problem to be solved by contestanttime: total penalty time for whole contest awarded to the contestantany suggestions for improving efficiency or readability?#include <iostream>#include <algorithm>#include <sstream>#include <vector>using namespace std;struct board{ int contestant; int nproblem; int problem[10]; int penalty[10]; int time; board(int c): contestant(c), nproblem(0) { time = 0; for ( int i = 0; i < 10; ++i ) { problem[i] = 0; penalty[i] = 0; } }};inline void init(int index[]){ for ( int i = 0; i < 101; ++i ) index[i] = -1;}void judge(board &b, const int &problem, const int &time, const char &l){ if ( l == 'c' ) { ++b.nproblem; b.problem[problem] = 1; b.penalty[problem] += time; } else if ( l == 'i' ) { b.penalty[problem] += 20; }}bool operator<(const board &b1, const board &b2){ if ( b1.nproblem > b2.nproblem ) return true; if ( b1.nproblem == b2.nproblem && b1.time < b2.time ) return true; if ( b1.nproblem == b2.nproblem && b1.time == b2.time && b1.contestant < b2.contestant ) return true; return false;}void calc_time(vector<board> &v){ for ( vector<board>::iterator it = v.begin(); it != v.end(); ++it ) { for ( int i = 1; i < 10; ++i ) { if ( it->problem[i] == 1 ) it->time += it->penalty[i]; } }}int main(){ int t; string s; int contestant, problem, time; char l; cin >> t; getline(cin, s); getline(cin, s); for ( int t = 1; t <= t; ++t ) { int index[101]; init(index); vector<board> v; while( getline(cin, s) ) { if ( s == ) break; istringstream iss(s); iss >> contestant >> problem >> time >> l; if ( index[contestant] == -1 ) { v.push_back(*new board(contestant)); index[contestant] = (int)v.size() - 1; judge(v[index[contestant]], problem, time, l); } else { if ( v[index[contestant]].problem[problem] == 0 ) { judge(v[index[contestant]], problem, time, l); } } } calc_time(v); sort(v.begin(), v.end()); for ( vector<board>::iterator it = v.begin(); it != v.end(); ++it ) cout << it->contestant << ' ' << it->nproblem << ' ' << it->time << endl; if ( t < t ) cout << endl; } return 0;}refactored codethe changes in the code is suggested by vnp#include <iostream>#include <algorithm>#include <sstream>#include <vector>struct board{ int contestant; int nproblem; int problem[10]; int penalty[10]; int time; board(int c): contestant(c), nproblem(0) { time = 0; for ( int i = 0; i < 10; ++i ) { problem[i] = 0; penalty[i] = 0; } } void calc_time() { for ( int i = 1; i < 10; ++i ) { if ( problem[i] == 1 ) time += penalty[i]; } }};inline void init(int index[]){ for ( int i = 0; i < 101; ++i ) index[i] = -1;}void judge(board &b, const int &problem, const int &time, const char &l){ if (b.problem[problem] == 1) { return; } if ( l == 'c' ) { ++b.nproblem; b.problem[problem] = 1; b.penalty[problem] += time; } else if ( l == 'i' ) { b.penalty[problem] += 20; }}bool operator<(const board &b1, const board &b2){ if ( b1.nproblem > b2.nproblem ) return true; if ( b1.nproblem == b2.nproblem && b1.time < b2.time ) return true; if ( b1.nproblem == b2.nproblem && b1.time == b2.time && b1.contestant < b2.contestant ) return true; return false;}int main(){ int t; std::string s; int contestant, problem, time; char l; std::cin >> t; getline(std::cin, s); getline(std::cin, s); for ( int t = 1; t <= t; ++t ) { int index[101]; std::fill(index, index + 101, -1); std::vector<board> v; while( getline(std::cin, s) ) { if ( s == ) break; std::istringstream iss(s); iss >> contestant >> problem >> time >> l; if ( index[contestant] == -1 ) { v.push_back(*new board(contestant)); index[contestant] = (int)v.size() - 1; } judge(v[index[contestant]], problem, time, l); } for ( std::vector<board>::iterator it = v.begin(); it != v.end(); ++it ) it->calc_time(); sort(v.begin(), v.end()); for ( std::vector<board>::iterator it = v.begin(); it != v.end(); ++it ) std::cout << it->contestant << ' ' << it->nproblem << ' ' << it->time << std::endl; if ( t < t ) std::cout << std::endl; } return 0;}",
    "present_kp": [
      "algorithm"
    ],
    "absent_kp": [
      "c++",
      "programming challenge",
      "sorting"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what are some best papers on gradient descent for nn implementation?. i'm trying to implement gd for standard task of nn training :) the best papers for practioneer i've founded so far are:1) efficient backprop by yann lecun et al.2) stochastic gradient descent tricks by leon bottouare there some other must read papers on this topic?thank you!",
    "present_kp": [
      "gradient descent"
    ],
    "absent_kp": [
      "neural network"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "ntpd fails to sync. i'm transferring from one server to another. identical configuration, newer hardware, same data center. the new machine can speak ntp:$ sudo ntpdate -d ntp.jentfoo.compassword:24 aug 14:50:59 ntpdate[10935]: ntpdate <email>-o sun aug 24 17:40:33 utc 2014 (1)looking for host ntp.jentfoo.com and service ntphost found : stripes.jentfoo.comtransmit(173.203.211.73)receive(173.203.211.73)transmit(173.203.211.73)receive(173.203.211.73)transmit(173.203.211.73)receive(173.203.211.73)transmit(173.203.211.73)receive(173.203.211.73)server 173.203.211.73, port 123stratum 2, precision -20, leap 00, trust 000refid [173.203.211.73], delay 0.04141, dispersion 0.00014transmitted 4, in filter 4reference time: d7a4aec6.c2942e94 sun, aug 24 2014 14:34:46.760originate timestamp: d7a4b299.ef951806 sun, aug 24 2014 14:51:05.935transmit timestamp: d7a4b29a.0400cdfd sun, aug 24 2014 14:51:06.015filter delay: 0.04141 0.04160 0.04147 0.04152 0.00000 0.00000 0.00000 0.00000filter offset: -0.08754 -0.08769 -0.08770 -0.08774 0.000000 0.000000 0.000000 0.000000delay 0.04141, dispersion 0.00014offset -0.<phone> aug 14:51:06 ntpdate[10935]: adjust time server 173.203.211.73 offset -0.087543 secso there's no firewall issue. but no matter how long i leave it running, the ntpd server never synchronizes with its peers:$ ntpq 127.0.0.1ntpq> pe remote refid st t when poll reach delay offset jitter============================================================================== andromeda.cs.pu .init. 16 - - 1024 0 0.000 0.000 0.000 ns.nts.umn.edu .init. 16 - - 1024 0 0.000 0.000 0.000 stan.greyware.c .init. 16 - - 1024 0 0.000 0.000 0.000 stripes.jentfoo .init. 16 - - 1024 0 0.000 0.000 0.000ntpq> asind assid status conf reach auth condition last_event cnt=========================================================== 1 18223 8011 yes no none reject mobilize 1 2 18224 8011 yes no none reject mobilize 1 3 18225 8011 yes no none reject mobilize 1 4 18226 8011 yes no none reject mobilize 1ntpq> rvassocid=0 status=c012 leap_alarm, sync_unspec, 1 event, freq_set,version=ntpd <email>-o sun aug 24 17:40:33 utc 2014 (1),processor=x86_64, system=linux/3.15.5-hardened-r2, leap=11,stratum=16, precision=-23, rootdelay=0.000, rootdisp=355.605, refid=init,reftime=<phone>.<phone> sun, dec 31 1899 19:00:00.000,clock=d7a510c6.0be8b4ee sun, aug 24 2014 21:32:54.046, peer=0, tc=3,mintc=4, offset=0.000, frequency=0.000, sys_jitter=0.000,clk_jitter=0.000, clk_wander=0.000ntpq> rv 18226associd=18226 status=8011 conf, sel_reject, 1 event, mobilize,srcadr=stripes.jentfoo.com, srcport=123, dstadr=0.0.0.0, dstport=0,leap=11, stratum=16, precision=-23, rootdelay=0.000, rootdisp=0.000,refid=init, reftime=<phone>.<phone> sun, dec 31 1899 19:00:00.000,rec=<phone>.<phone> sun, dec 31 1899 19:00:00.000, reach=000,unreach=34, hmode=3, pmode=0, hpoll=10, ppoll=10, headway=0,flash=1600 peer_stratum, peer_dist, peer_unreach, keyid=0, offset=0.000,delay=0.000, dispersion=15937.500, jitter=0.000,filtdelay= 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00,filtoffset= 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00,filtdisp= 16000.0 16000.0 16000.0 16000.0 16000.0 16000.0 16000.0 16000.0and i can't figure out why. the old server is using the exact same configuration, and it works fine. i don't know what to do.update: ntpd has been running overnight and:2014-08-24 14:57:46.244875500 starting ntpd ...2014-08-24 14:57:46.246304500 finished parsing!!... normal startup status ...2014-08-25 01:24:53.871674500 receive: at 975 0.0.0.0<-212.1.209.194 mode 3 len 482014-08-25 01:24:53.871699500 transmit: at 975 0.0.0.0->212.1.209.194 mode 4 len 482014-08-25 01:24:53.871700500 receive: at 1117 0.0.0.0<-212.1.209.194 mode 3 len 482014-08-25 01:24:53.871700500 transmit: at 1117 0.0.0.0->212.1.209.194 mode 4 len 482014-08-25 01:24:53.871700500 receive: at 1300 0.0.0.0<-198.23.230.118 mode 3 len 482014-08-25 01:24:53.871701500 transmit: at 1300 0.0.0.0->198.23.230.118 mode 4 len 482014-08-25 01:24:53.871701500 receive: at 2115 0.0.0.0<-66.172.12.144 mode 3 len 482014-08-25 01:24:53.871725500 transmit: at 2115 0.0.0.0->66.172.12.144 mode 4 len 482014-08-25 01:24:53.871725500 auth_agekeys: at 4096 keys 1 expired 02014-08-25 01:24:53.871725500 auth_agekeys: at 8192 keys 1 expired 02014-08-25 01:24:53.871726500 auth_agekeys: at 12288 keys 1 expired 0... more of the same ...it does appear to be transmitting/recieving. all the ntpq output is identical except rv 18226 answers unreach=69 now.",
    "present_kp": [
      "ntpd"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how can i update this sed oneliner?. input: $ echo -e 'aaaart5530408xxxx aaaaze6530408xxxx aaaart12345678xxxx'aaaart5530408xxxxaaaaze6530408xxxxaaaart12345678xxxx$output: $ echo -e 'aaaart5530408xxxx aaaaze6530408xxxx aaaart12345678xxxx' | sed -e 's/\\(aaaa[a-z]\\{2\\}[0-9]\\{7\\}\\)xxxx/ /g'aaaart5530408aaaaze6530408aaaart12345678xxxx$ how can i extend the: sed -e 's/\\(aaaa[a-z]\\{2\\}[0-9]\\{7\\}\\)xxxx/ /g'sed oneliner, so that it would optionally accept sed -e 's/\\(aaaa[a-z]\\{2\\}[0-9]\\{8\\}\\)xxxx/ /g'8 numbers too? (not just 7) is it possible with only 1 sed?",
    "present_kp": [
      "sed"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "defining function for arithmetic expressions. i've got these arithmetic expressions a ::= n | x | a1 + a2 | a1 ? a2 | a1 a2 which are a part of language.an undefined element is allowed which extends the set of intergers to z{}.problem:i need to define the function a: aexp (state z{})by defining equations for each form of the arithmetic expression a.now i've tried to come up with an solution but i am pretty sure i am wrong and i gotta admit i am pretty stuck.some help understanding this problem would be great.",
    "present_kp": [],
    "absent_kp": [
      "semantics",
      "denotational semantics"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "defining classes in javascript that exist in your back-end. doesn't it seem relatively duplicative to define your models in your backend code and on your front end for a rich internet application?i'm porting a gui application i had written to have a web interface, which is all grand and nice any all, but things like spine, sproutcore, javascriptmvc would have you define your models and views and implement specific controllers.being that i've got a well defined mvc pattern on my backend code (which is making this super easy to port; the views in my app took python dicts and returned python dicts to the controllers which could easy interface with the models; i can just convert these to json back and forth to speak to the web front end), why would i want to recreate the entire pattern again on the front end?what are good ways to work around this?should i just say screw this and use something like <url> i write a bunch of code to export my models into json and then write some javascript code to build the models on the front-end automatically so i'm still only defining them once?what would be the best approach to this?",
    "present_kp": [
      "javascript",
      "mvc",
      "front end"
    ],
    "absent_kp": [
      "methodology"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "system hanging when it runs out of memory. i've got an eeepc 900a: it has a 8gb flash as disk and only 1gb of ram. the linux distribution installed on it is archlinux.when the system runs out of memory it becomes extremely unresponsive: it takes several seconds/minutes to do things like switching to tty1 or even moving the mouse pointer. sometimes it looks like the system just freezes: three ours ago i let it alone and nothing at all is changed so far.i'd rather avoid creating a swap partition/file on this eeepc since the disk is already that small, and also because the many writes on the swap space would shorten a lot the flash card life.moreover i think that a swap file/partition would just move the problem, rather than definitely fixing it.isn't the kernel supposed to kill some random applications when it runs out of memory? why does it fail (or takes ages) at doing that?a few months/years ago i already tried to look further into this, but couldn't find anything that would actually work...",
    "present_kp": [
      "linux",
      "memory",
      "freeze"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "divide and conquer division algorithm explained (as used in gmp bignum). i am trying to understand the divide and conquer division algorithm that is used in the gmp bignum arithmetic library.the code is very optimised and that makes it somewhat hard to understand. the doc does not really offer an explanation of how it is done, (and the references do not point to an actual algorithm, not one that i could see anyway).i understand the concept of divide and conquer for sorting, but i am not sure i understand how it can be used in a division.how can $x \\over y$ be calculated using divide and conquer?can someone give a small example of how it is done?",
    "present_kp": [],
    "absent_kp": [
      "optimization",
      "algorithms",
      "computer arithmetic"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is it ok to avoid testing base classes?. i have a base class with a fair amount of meta programming to give it the flexibility/abstraction it needs to be rather generic.i do have a lot of subclasses using the common methods in the base class, and i have behavior oriented unit tests covering all of the cases in each subclass.is it ok to skip testing the base class?",
    "present_kp": [
      "testing"
    ],
    "absent_kp": [
      "unit testing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "mapping ctrl with equal sign. i am trying to map my ctrl and plus sign together. this is what i am trying in my vimrc:nnoremap <c-=> : echo hello <cr> however it seems like the mapping is not being triggered.any suggestions on what i might be doing wrong? i am using macvim.",
    "present_kp": [
      "macvim"
    ],
    "absent_kp": [
      "key bindings"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "falling to git checkout on non-found bash commands. when i type (in bash) a command which linux does not find, it usually does something like this:$ xx: command not foundbut when i type a command which is not found but which is similar in some sense to other possible commands, the reply may look like this:$ pypno command 'pyp' found, did you mean: command 'pcp' from package 'pcp' (universe) command 'pype' from package 'pype' (universe) command 'pgp' from package 'pgpgpg' (universe) command 'pp' from package 'libpar-packer-perl' (universe) command 'php' from package 'php5-cli' (main) command 'gyp' from package 'gyp' (universe) command 'pip' from package 'python-pip' (universe) command 'pap' from package 'netatalk' (universe)pyp: command not foundthat means that some hook checked pyp before it let bash reply with pyp: command not found.i'd like to write a hook like that, that will check if the command is a name of a branch in a current repository (if we're indeed in a repository), and will try to checkout into it before replying with command not found, but only if the command was really not found.to this end, i need to understand something about the flow; when i type the command (and press enter), where does it go to? what is the program which replies with did you mean, and how does it get the command string from bash?",
    "present_kp": [
      "bash",
      "hook"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to allocate a process specific amount of cpu power?. i would like to allocate certain number of cpu cores to a specific process for performance improvement.how can i do this?",
    "present_kp": [
      "cpu"
    ],
    "absent_kp": [
      "process management"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "elixir - relay between telegram and discord - small project. i built a relay bot (well, they're two bots acting as one, you get it) between two specifics channels in discord and telegram, i want this to be reviewed because i have some concerns about code-style, probably i could have more elegant code but who knows.this is the project and the two things that i don't know if should be done that way are multi-line parameters on the dispatcher module on lib/kass/gram/dispatcher.ex:def dispatch(updates) do enum.each(updates, &task.supervisor.start_child( kass.tasksupervisor, kass.gram.bot, :handle_message, [&1.message])) endand the handle_message function on lib/kass/gram/bot.ex:def handle_message(%message{chat: %chat{type: group}, from: %user{first_name: first_name, last_name: last_name, id: id}, text: text}) do ...endmaybe this could be done differently? i just didn't want to exceed the 80 legth limit from the style guide. anyway, besides that, feel free to check it and criticize as much as possible, thanks ^-^",
    "present_kp": [
      "elixir"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "bash: #: command not found. i just moved to arch linux with kde a week ago. everything worked well until i installed ibus-unikey and ibus-qt, after that, whenever i open konsole, this error appears:bash: #: command not foundi doubt that # is a usual command because when i type it in konsole, nothing happens. but when i run pacman -ss #, a ton of things appear.my question: what is the # command and what should i do to fix this error? here are my .bashrc, .bash_profile and path$ cat .bashrc # # ~/.bashrc # # if not running interactively, don't do anything [[ $- != *i* ]] && return$ cat .bash_profile # # ~/.bash_profile # [[ -f ~/.bashrc ]] && . ~/.bashrc alias ls='ls --color=auto' ps1='[\\u@\\h \\w]\\$ '$ echo $path /usr/local/texlive/2015/bin/x86_64-linux:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perladdition 1: as @terdon suggest, i run$ grep -fh '\\#' ~/.bashrc ~/.profile ~/.bash_profile ~/bash.login ~/.bash_aliases /etc/bash.bashrc /etc/profile /etc/profile.d/* /etc/environment 2>/dev/nulland receive nothing. but the later command give me$ grep -p '(^|\\s+)(\\.|source) .' ~/.bashrc ~/.profile ~/.bash_profile ~/bash.login ~/.bash_aliases /etc/bash.bashrc /etc/profile /etc/profile.d/* /etc/environment 2>/dev/null /home/thuyenarc/.bash_profile:[[ -f ~/.bashrc ]] && . ~/.bashrc /etc/bash.bashrc:[ -r /usr/share/bash-completion/bash_completion ] && . /usr/share/bash-completion/bash_completion /etc/profile: test -r $profile && . $profile /etc/profile: . /etc/bash.bashrc /etc/profile.d/locale.sh: . $xdg_config_home/locale.conf /etc/profile.d/locale.sh: . $home/.config/locale.conf /etc/profile.d/locale.sh: . /etc/locale.confaddition 2: i tried renaming the .bashrc file, then opened konsole, the error had gone. i suspect that the errors must come from .bashrc. any ideas?",
    "present_kp": [
      "bash",
      "bashrc"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "handling null-references in c# logic. let's say i have an api method with can be used to calculate the sum of all orders made by a specific customer:amount calculateordersum(int customerid){ // perform authentication to make sure caller has access to customerid // retrieve customer with id customerid // retrieve all orders related to the customer // retrieve details for different orders (not always, depending on state)}at any point in this function, some system administrator may purge old items from the system. this means that one of the below can happen while the method above is running:the authentication will fail, because the customer no longer existsthe customer can't be loaded, since it was deletedthe customer can be loaded, but a millisecond later the orders are deleted and cannot be retrieved.retrieving order details works for some orders, but fails for others since they have been deleted mid-processing.i want the application to return a friendly error message when any of this happens, rather than returning a nullreferenceexception or similar.as i see it, there's some different approaches to add error-handling for this logic:i could introduce a lot of null-checks throughout the code for example: if (customer != null) throw orderretrievalfailedexception(customer is a goner.). since all the data in the database can be purged at any time, this would lead to quite a lot of if's spread throughout the code (which seems to get messy)i could change the purging functionality to mark customers or orders as deleted (rather than actually removing the database rows). this way the function could still do its work because the data will still be there. the issue here is that we actually want to purge old data for different reasons (less attack surface and performance considerations for example).i could change all methods to throw if an object can't be loaded (so getorders(customerid) could throw customernotfoundexception if the customer cannot be loaded) which would be catched in the calculateordersum function and an error given to the user. so basically the code would have to be littered with if (something == null) throw new someexception.i could introduce some global locking mechanism, so that a customer can't be deleted while any one is reading any of its data. the issue here is that our system is distributed so we would need to implement a central locking mechanism. also, i have a bad experience with locking of database rows in use-cases like this in high-traffic database.all of these approaches feels quite convoluted and tricky to get right to me, and it means that the main success flow of the code will be littered with handling of exception scenarios. i'm leaning towards alternative 3, but i would to hear if there's some other standard and robust way of handling this.(i'm using c#, but i assume that the same issue would apply to users of for example java or c++)",
    "present_kp": [
      "c#",
      "null"
    ],
    "absent_kp": [
      "error handling"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "deprecation considered harmful?. i've just been compiling some of my own code with the -std=c++0x flag in gcc, as i want to vaguely keep up with what all the young folks are doing (provided they stay of my lawn), and i ended up with a load of warnings about auto_ptr being deprecated. of course, i knew that auto_ptr was deprecated in c++0x, but...isn't deprecation a waste of time and effort? reasons for not deprecating (with auto_ptr as an example):there is an ocean of code out there that still needs to be supported, producing millions of warnings will only tempt people to turn warnings off.auto_ptr is a bit naff, but it does actually do what it says on the tin.if we really want to deprecate things, i nominate printf(). but just imagine the squeals that would ensue. auto_ptr doesn't have too many friends, but in at least my c++ code it is used more than printf, which isn't used at all.the committee have a bad record of getting this right - they deprecated static at namespace scope, and now it seems to have been undeprecated - i wouldn't be surprised if auto_ptr made a similar come-backlastly, whatever the committee say, the compiler implementers ignore them - they simply can't risk breaking their customers code, all they can do is issue irritating warnings.so my question - do you consider deprecation (of anything, not just auto_ptrs, and not just in c++) a good idea, and if so, why?",
    "present_kp": [
      "c++"
    ],
    "absent_kp": [
      "c++11",
      "standardization"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "cannot add new mode in xrandr for external monitor. today i was trying to connect my second monitor to my notebook. i have:nvidia graphic card304.43 drivers [support xrandr 1.2]archlinux [up-to-date]awesome wmxrandr 1.3my problem is with bigger resolution than 640x480 for my external monitor (vga).xrandr -q:screen 0: minimum 8 x 8, current 1920 x 800, maximum 8192 x 8192vga-0 connected 640x480+1280+0 (normal left inverted right x axis y axis) 0mm 640x480 59.9*+ 320x240 120.1 lvds-0 connected 1280x800+0+0 (normal left inverted right x axis y axis) 331m 1280x800 59.9*+hdmi-0 disconnected (normal left inverted right x axis y axis)as we can see the is no higher resolution fo vga, so i add new mode:xrandr --newmode $(gtf <phone> 70.4 | grep modeline | sed s/modeline\\ // | tr -d '')i checked avaible resolution and refresh rate under windows: one of them was 1024x768x70 (osd of my monitor said that it is 70.4hz). after create new mode, i wanted to add it:xrandr --addmode vga-0 1280x1024_70.40and... it failed:x error of failed request: badmatch (invalid parameter attributes) major opcode of failed request: 153 (randr) minor opcode of failed request: 18 (rraddoutputmode) serial number of failed request: 29 current serial number in output stream: 30from google i learned that in older xrandr / nvidia drivers was problem with list of avaible modes, but now with support of xrandr 1.2 by nvidia drivers it should be better. i also try with lower resolution and refresh rate (eg. 1024x768x50), but i've got the same error.i'm out of ideas what to do with this problem...",
    "present_kp": [
      "nvidia",
      "xrandr",
      "awesome"
    ],
    "absent_kp": [
      "xorg",
      "arch linux"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "breakout-clone built in sfml. after reading the sfml game development book, i decided to put what was learned to the test by creating a simple breakout clone. i have posted the source code to github: <url> wanted to see if i could get any sort of feedback on the general code structure that i used or any programming techniques that i could improve on before i proceed to my next project.application.h/// <summary>/// initializing class for the game app/// </summary>class application : sf::noncopyable{public: application(); void run();private: void update(sf::time dt); void processevents(); void render(); void registerstates(); void loadresources();private: fontholder mfonts; textureholder mtextures; sf::renderwindow mwindow; statestack mstatestack; static const sf::time mtimeperframe; soundplayer msoundplayer;}; application.cpp const sf::time application::mtimeperframe = sf::seconds(1.f / 60.f);/// <summary>/// initializes a new instance of the <see cref=application/> class./// </summary>application::application(): mwindow(sf::videomode(865, 956), drewski's block breaker, sf::style::close), mstatestack(mwindow), mfonts(), mtextures(), msoundplayer(){ registerstates(); loadresources(); // start statestack at title screen mstatestack.pushstate(states::title);}/// <summary>/// starts the main game loop./// </summary>void application::run(){ sf::clock clock; sf::time elapsedtime = sf::time::zero; while (mwindow.isopen()) { elapsedtime += clock.restart(); while (elapsedtime >= mtimeperframe) { update(mtimeperframe); processevents(); elapsedtime -= mtimeperframe; } render(); }}/// <summary>/// updates all game objects./// </summary>/// <param name=dt>the delta time.</param>void application::update(sf::time dt){ mstatestack.update(dt); msoundplayer.removeinactivesounds();}/// <summary>/// processes the events./// </summary>void application::processevents(){ sf::event event; while (mwindow.pollevent(event)) { mstatestack.handleevent(event); switch (event.type) { case sf::event::closed: mwindow.close(); } } if (mstatestack.isempty()) mwindow.close();}/// <summary>/// renders this all game objects./// </summary>void application::render(){ mwindow.clear(); mstatestack.draw(); mwindow.display();}/// <summary>/// registers the game states./// </summary>void application::registerstates(){ mstatestack.registerstate<gamestate>(states::game); mstatestack.registerstate<titlestate>(states::title); mstatestack.registerstate<pausestate>(states::pause);}/// <summary>/// loads fonts and textures used in the game./// </summary>void application::loadresources(){ // set servicelocator window servicelocator::setwindow(mwindow); // load all game fonts and set servicelocator font holder mfonts.load(fonts::main, media/fonts/sansation.ttf); servicelocator::setfontholder(mfonts); // load all game textures and set servicelocator texture holder mtextures.load(textures::title, media/textures/titlescreen.png); mtextures.load(textures::paddle, media/textures/paddles.png); mtextures.load(textures::ball, media/textures/ball.png); mtextures.load(textures::bricks, media/textures/bricktextures.png); mtextures.load(textures::particle, media/textures/particle.png); servicelocator::settextureholder(mtextures); // set servicelocator soundplayer servicelocator::setsoundplayer(msoundplayer);}statestack.h/// <summary>/// used to manage various states in the game/// </summary>class statestack : public sf::noncopyable{public: // actions for the statestack enum action { pop, push, clear, }; explicit statestack(sf::renderwindow& window); void update(sf::time dt); void handleevent(const sf::event& event); void draw(); template <typename t> void registerstate(states::id id); // called by outside code for changing the current statestack void popstate(); void pushstate(states::id id); void clearstates(); bool isempty() const;private: void applypendingchanges(); // called from applypendingchanges when a state is pushed state::ptr createstate(states::id);private: // structure for applying actions to the statestack at the correct time during game update struct pendingaction { pendingaction(action a, states::id i); action action; states::id id; };private: sf::renderwindow& mwindow; std::vector<state::ptr> mstack; std::vector<pendingaction> mpendinglist; std::map<states::id, std::function<state::ptr()>> mfactories;};/// <summary>/// registers the game state./// </summary>/// <param name=id>the state identifier.</param>template <typename t>void statestack::registerstate(states::id id){ // map functors for states that are registered mfactories[id] = [this]() { return state::ptr(new t(*this, mwindow)); };}statestack.cppstatestack::pendingaction::pendingaction(action a, states::id i = states::none): action(a), id(i){}statestack::statestack(sf::renderwindow& window): mwindow(window), mpendinglist(), mstack(), mfactories(){}void statestack::popstate(){ mpendinglist.push_back(pendingaction(pop));}void statestack::pushstate(states::id id){ mpendinglist.push_back(pendingaction(push, id));}void statestack::clearstates(){ mpendinglist.push_back(pendingaction(clear));}/// <summary>/// applies the pending changes./// </summary>void statestack::applypendingchanges(){ // we only update the statestack at optimal times, // so all requests go through a pending list and are then executed for (auto change : mpendinglist) { switch (change.action) { case push: mstack.push_back(createstate(change.id)); break; case pop: mstack.pop_back(); break; case clear: mstack.clear(); break; } } mpendinglist.clear();}/// <summary>/// find the functor for the state and return the instantiated state/// </summary>/// <param name=id>the state identifier.</param>/// <returns>pointer to the new state</returns>state::ptr statestack::createstate(states::id id){ auto found = mfactories.find(id); assert(found != mfactories.end()); return found->second();}void statestack::update(sf::time dt){ // update each state until one returns false in it's update function for (auto state = mstack.rbegin(); state != mstack.rend(); state++) { if (!(*state)->update(dt)) break; } applypendingchanges();}void statestack::handleevent(const sf::event& event){ for (auto state = mstack.rbegin(); state != mstack.rend(); state++) { if (!(*state)->processevent(event)) break; } applypendingchanges();}void statestack::draw(){ for (auto state = mstack.begin(); state != mstack.end(); state++) (*state)->draw();}bool statestack::isempty() const{ return mstack.size() <= 0;}gamestate.h/// <summary>/// represents main gameplay state/// </summary>class gamestate : public state{public: gamestate(statestack& stack, sf::renderwindow& window); virtual bool update(sf::time dt); virtual bool processevent(const sf::event& event); virtual void draw();private: void displaycompletionstatus() const;private: world mworld; sf::text mcompletionstatustext; sf::time mcompletiondisplaytimer;};gamestate.cpp// used for displaying success/failure text at end of gameconst sf::time completiondisplayinterval = sf::seconds(3.f);gamestate::gamestate(statestack& stack, sf::renderwindow& window): state(stack, window), mworld(window), mcompletionstatustext(), mcompletiondisplaytimer(sf::time::zero){ mcompletionstatustext.setfont(servicelocator::getfontholder().get(fonts::main)); mcompletionstatustext.setposition(window.getsize().x / 2.f, window.getsize().y / 2.f); mcompletionstatustext.setcharactersize(30);}bool gamestate::update(sf::time dt){ if (!mworld.isworldcomplete()) mworld.update(dt); else { // if world is complete, show result text mcompletiondisplaytimer += dt; if (mworld.getmissionstatus() == world::success) mcompletionstatustext.setstring(you won!!!); else mcompletionstatustext.setstring(sorry, you failed.); } if (mcompletiondisplaytimer > completiondisplayinterval) { requeststackclear(); requeststackpush(states::title); } return true;}bool gamestate::processevent(const sf::event& event){ mworld.handleinput(event); if (event.type == sf::event::keypressed && event.key.code == sf::keyboard::escape) requeststackpush(states::pause); return true;}void gamestate::draw(){ mworld.draw(); if (mworld.isworldcomplete()) { sf::renderwindow& window = getwindow(); centerorigin(mcompletionstatustext); window.draw(mcompletionstatustext); }}world.h/// <summary>/// represents the game world/// </summary>class world{public: enum missionstatus { running, success, failed }; world(sf::renderwindow& window); void handleinput(const sf::event& event); void update(sf::time dt); void draw(); bool isworldcomplete() const; world::missionstatus getmissionstatus() const;private: void handlecollisions(); void loadnextlevel(); void loadballs(levels::id level); void removedestroyedballs(); void updateremainingballs();private: sf::renderwindow& mwindow; player mplayer; std::vector<std::unique_ptr<ball>> mballs; std::vector<std::unique_ptr<brick>> mbricks; unsigned int mcurrentlevel; bool mischanginglevel; sf::time mleveltransitiontimer; bool mspawnednewbricks; sf::text mlevelchangetext; int mremainingballs; sf::text mballdisplaytext; bool misworldcomplete; world::missionstatus mmissionstatus; particlesystem mballtrails; particlesystem mbackgroundbounceparticles; particleemitter mbounceparticleemitter;};world.cppconst sf::time leveltransitioninterval = sf::seconds(4.f);namespace{ // sets up table indexed by level::id, provides: max balls and lives per ball const std::vector<levelinfo> leveltable = initializeleveltable();}world::world(sf::renderwindow& window): mwindow(window), mplayer(), mballs(), mbricks(), mcurrentlevel(0), mischanginglevel(false), mleveltransitiontimer(sf::time::zero), mspawnednewbricks(false), mlevelchangetext(), mballdisplaytext(), mremainingballs(0), misworldcomplete(false), mmissionstatus(world::running), mballtrails(particle::trail), mbackgroundbounceparticles(particle::backgroudbouncers), mbounceparticleemitter(mbackgroundbounceparticles){ auto windowbounds = mwindow.getsize(); mplayer.setposition(windowbounds.x / 2.f, windowbounds.y - 50.f); // set position for background particles mbounceparticleemitter.setposition(windowbounds.x / 2.f, windowbounds.y / 2.f); // text to display on level changes mlevelchangetext.setfont(servicelocator::getfontholder().get(fonts::main)); mlevelchangetext.setposition(window.getsize().x / 2.f, window.getsize().y / 2.f); mlevelchangetext.setcharactersize(30); // balls remaining display mballdisplaytext.setfont(servicelocator::getfontholder().get(fonts::main)); mballdisplaytext.setposition(window.getsize().x - 200, window.getsize().y - 30); mballdisplaytext.setcharactersize(20); loadnextlevel();}/// <summary>/// handles events and input for the game world./// </summary>/// <param name=event>game event.</param>void world::handleinput(const sf::event& event){ mplayer.handleinput();}/// <summary>/// updates the game world./// </summary>/// <param name=dt>the delta time.</param>void world::update(sf::time dt){ updateremainingballs(); handlecollisions(); removedestroyedballs(); // if there are no more ball, but still bricks left, you lose if (mremainingballs <= 0 && !mbricks.empty()) { mmissionstatus = world::failed; misworldcomplete = true; } // if bricks are cleared and world is not complete, load next level if ((mbricks.empty() || mischanginglevel) && !misworldcomplete) { mischanginglevel = true; mleveltransitiontimer += dt; loadnextlevel(); } mplayer.update(dt); for (auto& ball : mballs) { ball->update(dt); } mballtrails.update(dt); mbounceparticleemitter.update(dt); mbackgroundbounceparticles.update(dt); for (auto& brick : mbricks) brick->update(dt);}/// <summary>/// draws the game world./// </summary>void world::draw(){ mwindow.draw(mbackgroundbounceparticles); for (auto& ball : mballs) { mwindow.draw(*ball); } mwindow.draw(mplayer); mwindow.draw(mballdisplaytext); mwindow.draw(mballtrails); for (auto& brick : mbricks) mwindow.draw(*brick); if (mischanginglevel) { centerorigin(mlevelchangetext); mwindow.draw(mlevelchangetext); }}/// <summary>/// determines whether the world is complete./// </summary>/// <returns></returns>bool world::isworldcomplete() const{ return misworldcomplete;}world::missionstatus world::getmissionstatus() const{ return mmissionstatus;}/// <summary>/// handles collisions with all game objects./// </summary>void world::handlecollisions(){ for (auto& ball : mballs) { auto ballrect = ball->getboundingrect(); float ballwidth = ballrect.width; float ballheight = ballrect.height; float ballleft = ballrect.left; float balltop = ballrect.top; // create four contact points for the ball sf::vector2f ballleftpoint(ballleft, balltop + (ballheight / 2.f)); // middle of left side sf::vector2f ballrightpoint(ballleft + ballwidth, balltop + (ballheight / 2.f)); // middle of right side sf::vector2f balltoppoint(ballleft + (ballwidth / 2.f), balltop); // middle of top sf::vector2f ballbottompoint(ballleft + (ballwidth / 2.f), balltop + ballheight); // middle of bottom // ball/paddle collisions auto playerrect = mplayer.getboundingrect(); if (ballrect.intersects(playerrect)) { // how the paddle velocity will affect the ball bounce float anglechangefactor = 0; if (mplayer.getvelocity().x > 0) anglechangefactor = -10.f; else if (mplayer.getvelocity().x < 0) anglechangefactor = 10.f; // adjust ball angle for bounce if (playerrect.contains(ballbottompoint)) { ball->setangle(360.f - ball->getangle() + anglechangefactor); ball->setposition(ball->getposition().x, mplayer.getposition().y - (mplayer.getboundingrect().height / 2.f) - (ballheight / 2.f)); } else if (playerrect.contains(ballleftpoint) || playerrect.contains(ballrightpoint)) ball->setangle(180.f - ball->getangle() + anglechangefactor); } // ball/brick collisions for (auto itr = mbricks.begin(); itr != mbricks.end();) { auto brickrect = (*itr)->getboundingrect(); float ballangle(ball->getangle()); if (ballrect.intersects(brickrect)) { // if contact is on the left or right of the ball, adjust angle properly if (brickrect.contains(ballleftpoint) || brickrect.contains(ballrightpoint)) ball->setangle(180.f - ball->getangle()); // if contact is on the top or top of the ball, adjust angle properly else if (brickrect.contains(balltoppoint) || brickrect.contains(ballbottompoint)) ball->setangle(360.f - ball->getangle()); } // if the angle of the ball changed, then we know a brick was hit. we damage that brick and if it is destroyed, // if so, we remove the brick from our brick container. if (ballangle != ball->getangle()) { // play a random bounce sound servicelocator::getsoundplayer().play(static_cast<sounds::id>(randomint(sounds::ballcount))); (*itr)->damage(ball->getpower()); if ((*itr)->isdestroyed()) mbricks.erase(itr); break; } else itr++; } }}/// <summary>/// loads the next level./// </summary>void world::loadnextlevel(){ // show level completed text at beginning of level transition if (mcurrentlevel > 0 && mleveltransitiontimer < leveltransitioninterval / 2.f ) mlevelchangetext.setstring(level complete!); // spawn the bricks in at half the level transition interval if (mleveltransitiontimer >= leveltransitioninterval / 2.f && !mspawnednewbricks) { mspawnednewbricks = true; mballs.clear(); mcurrentlevel++; // set loading level text mlevelchangetext.setstring(loading level + tostring(mcurrentlevel)); // load new bricks and spawn balls switch (mcurrentlevel) { case 1: mbricks = spawnlevelone(); loadballs(levels::one); break; case 2: mbricks = spawnleveltwo(); loadballs(levels::two); break; case 3: mbricks = spawnlevelthree(); loadballs(levels::three); break; default: mlevelchangetext.setstring(); } } // close out level transition when the interval is closed if (mleveltransitiontimer >= leveltransitioninterval) { mischanginglevel = false; mspawnednewbricks = false; mleveltransitiontimer = sf::time::zero; // if we passed to last level, we win if (mcurrentlevel > levels::count) { misworldcomplete = true; mmissionstatus = world::success; } }}/// <summary>/// load balls./// </summary>/// <param name=number>the number of balls to load.</param>void world::loadballs(levels::id level){ for (int i = 0; i < leveltable[level].maxballs; i++) { std::unique_ptr<ball> ball(new ball(leveltable[level].livesperball, mballtrails)); mballs.push_back(std::move(ball)); }}/// <summary>/// removes the destroyed balls./// </summary>void world::removedestroyedballs(){ for (auto itr = mballs.begin(); itr != mballs.end();) { if ((*itr)->isdestroyed()) itr = mballs.erase(itr); else itr++; }}/// <summary>/// updates the remaining balls and updates ball display./// </summary>void world::updateremainingballs(){ int sum = 0; for (auto& ball : mballs) { sum += ball->getlives(); } mremainingballs = sum; mballdisplaytext.setstring(balls remaining: + tostring(mremainingballs));}ball.h/// <summary>/// represents the ball object/// </summary>class ball : public entity{public: ball(int lives, particlesystem& particlsystem); float getangle() const; void setangle(float angle); void update(sf::time dt); void resetposition(); sf::floatrect getboundingrect() const; unsigned int getpower() const; unsigned int getlives() const; bool isdestroyed() const; void markfordestroyed(); void reacttobounce();private: void handlewallcollision(); void checkifballislive(); void draw(sf::rendertarget& target, sf::renderstates states) const; float linearvelocityx() const; float linearvelocityy() const;private: float mangle; sf::vector2f mspawnpoint; sf::sprite msprite; unsigned int mpower; float mdefaultvelocity; sf::time mmovementtimer; int mlives; bool misdestroyed; particleemitter mballtrailemitter;};ball.cppconst sf::time movementdelayinterval = sf::seconds(.1f);ball::ball(int lives, particlesystem& particlsystem): entity(), msprite(servicelocator::gettextureholder().get(textures::ball)), mspawnpoint(servicelocator::getwindow().getsize().x / 2.f, servicelocator::getwindow().getsize().y / 2.f), mdefaultvelocity(500.f), mpower(1), mlives(lives), mmovementtimer(sf::time::zero), misdestroyed(false), mangle(), mballtrailemitter(particlsystem){ centerorigin(msprite); resetposition();}void ball::draw(sf::rendertarget& target, sf::renderstates states) const{ states.transform *= gettransform(); target.draw(msprite, states);}void ball::resetposition(){ // reset the movement timer for a delay after the reset mmovementtimer = sf::time::zero; setposition(mspawnpoint); mangle = randomint(361);}unsigned int ball::getlives() const{ return mlives;}void ball::update(sf::time dt){ mmovementtimer += dt; // provide updates to ball trail particles mballtrailemitter.setposition(getposition()); mballtrailemitter.update(dt); if (mmovementtimer >= movementdelayinterval) { checkifballislive(); handlewallcollision(); sf::vector2f velocity; velocity.x = linearvelocityx() * mdefaultvelocity; velocity.y = linearvelocityy() * mdefaultvelocity; setvelocity(velocity); entity::update(dt); }}/// <summary>/// handles the ball collisions with the wall./// </summary>void ball::handlewallcollision(){ sf::renderwindow& window = servicelocator::getwindow(); // if the ball hits the sides of the window, rebound its angle by 180 if (getposition().x + (msprite.getlocalbounds().width / 2.f) > window.getsize().x || getposition().x - (msprite.getlocalbounds().width / 2.f) < 0.f) { setangle(180.f - getangle()); // play collision sound servicelocator::getsoundplayer().play(sounds::wall1); // make sure ball is inside the screen if (getposition().x + (msprite.getlocalbounds().width / 2.f) > window.getsize().x) setposition(window.getsize().x - msprite.getlocalbounds().width / 2.f, getposition().y); else setposition(msprite.getlocalbounds().width / 2.f, getposition().y); // if angle is too flat, make it wider if ((getangle() < 190.f && getangle() > 170.f)) setangle(getangle() - 20.f); else if (getangle() < 10.f || getangle() > 350.f) setangle(getangle() + 20.f); } // if the ball hits the top of the window, rebound angle by 360 else if (getposition().y - (msprite.getlocalbounds().height / 2.f) < 0.f) { setangle(360.f - getangle()); servicelocator::getsoundplayer().play(sounds::wall2); if (getposition().y + (msprite.getlocalbounds().height / 2.f) < 0) setposition(getposition().x, 0); } // keep angle from 0-360 if (getangle() > 360) setangle( getangle() - 360); else if (getangle() < 0) setangle(getangle() + 360);}/// <summary>/// checks if ball is inbounds./// </summary>void ball::checkifballislive(){ sf::renderwindow& window = servicelocator::getwindow(); if (getposition().y - (msprite.getlocalbounds().height / 2.f) > window.getsize().y) { mlives--; if (mlives > 0) resetposition(); else markfordestroyed(); }}sf::floatrect ball::getboundingrect() const{ return gettransform().transformrect(msprite.getglobalbounds());}brick.h/// <summary>/// represents a brick object./// </summary>class brick : public entity{public: enum type { none, green, blue, purple, red, count }; brick(brick::type type, float x, float y); unsigned int gettype() const; sf::floatrect getboundingrect() const; unsigned int gethitpoints() const; void sethitpoints(unsigned int hitpoints); void damage(unsigned int damage); bool isdestroyed() const; void update(sf::time dt);private: void draw(sf::rendertarget& target, sf::renderstates states) const;private: brick::type mtype; sf::sprite msprite; unsigned int mhitpoints;};brick.cppnamespace{ // setup brick table indexed by brick::type that provides: hitpoints and texture rect. const std::vector<brickinfo> table = initializebricktable();}brick::brick(brick::type type, float positionx, float positiony): entity(), msprite(servicelocator::gettextureholder().get(textures::bricks), table[type].texturerect), mtype(type), mhitpoints(table[type].hitpoints){ setposition(positionx, positiony); centerorigin(msprite); //updatetexture();}bool brick::isdestroyed() const{ return mhitpoints <= 0;}void brick::draw(sf::rendertarget& target, sf::renderstates states) const{ states.transform *= gettransform(); target.draw(msprite, states);}void brick::update(sf::time dt){ entity::update(dt);}sf::floatrect brick::getboundingrect() const{ return gettransform().transformrect(msprite.getglobalbounds());}",
    "present_kp": [
      "game",
      "sfml"
    ],
    "absent_kp": [
      "c++"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "client has set up a mirror on dreamhost, but original site is hosted elsewhere. sorry. newbie here. picked up a client that needs has a blog through lexblog. they want to recreate it on dreamhost and move all of the information over.she puchased her hosting through dreamhost and added the current domain. then set up a mirror site. i have never used dreamhost or a mirror. question is, will the mirror even work since the original site is hosted elsewhere? or should a basic subdomain be set up to build the new site on before we make the switch over.",
    "present_kp": [
      "subdomain",
      "dreamhost",
      "mirror"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to get tty from current pts. the system is centos 7 x64 running at level 3 mode which is no gui. i am connecting remotely to the machine. i use tmate and teamviewer. i run who and i can view who is login on ttys.usera tty1 2017-02-24 11:47userb tty2 2017-02-24 12:00userc tty6 2017-02-24 02:00the result is same in tmate session and teamviewer session. but when i type tty. tmate session returns /dev/pts/1 while teamviewer return /dev/tty6. it seems teamviewer returns tty number which is same with terminal number opened by ctrl-shift-fn, and here is ctrl-alt-f6.i have read several article which describes tty and pty. what-is-the-exact-difference-between-a-terminal-a-shell-a-tty-and-a-con, difference-between-pts-and-ttywhat i would like to do is that how to get current tty number from pty session? in my specific case, i want to get tty number in tmate session. so which tty does pts running on? in the case above tmate is started on teamviewer session. so i type tty get /dev/pts/1 and then i exit tmate session by typing exit and back to teamviewer session and type tty again get /dev/tty6. it seems means that /dev/pts/1 working on /dev/tty6. and i want to get tty6 in tmate session without exit to the session. it should nothing related to tmate and teamviewer. it should same when ssh to a physic machine. so there should a solution to get tty number back in pty.manay solutions how-to-get-the-tty-in-which-bash-is-running, returns the filename of the terminal connected to standard input, get-current-terminal-name only get pty number. but i would like to get tty number.how to do that? thanks.",
    "present_kp": [
      "centos",
      "tty",
      "pty"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how can i install mint without a dvd?. i am running debian, but lack of driver support for stuff i want has made me decide to switch to mint. unfortunately, i don't have a dvd and the mint community seems to have decided i really, really need a dvd. i have a usb, but i'm not sure how to burn the .iso image, rather than the file, to said usb in order to then boot from it in order to install the os. i'm also not convinced this is necessarily the best/simplest way to go about installing mint. i just want to wipe everything and get a clean install of mint. what do i need to do to do that without a dvd?",
    "present_kp": [
      "debian",
      "usb"
    ],
    "absent_kp": [
      "linux mint",
      "system installation"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is there an image classification dataset which has objects in varying distances?. i would like to analyze / estimate the influence of the architecture on scale invariance in photos.to clarify: it is possible that a network can classify objects well if they are close, but totally fails when they are far away. although the main object of the photo is still the same. is there a dataset which is suitable for this task?",
    "present_kp": [
      "dataset",
      "image classification"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "who should read an external resource. i am often in the following situation: i have a config file, specifying the path to a resource filethis config file gets parsed and the values (e.g., file path) are stored in a configstore objecta business logic object needs to content of the resource file. now who should actually read the resource file? configstorethe business logic objecta special parser?i don't like any of the 3 options: both 1 and 2 violate the single responsibility principle, on the other hand, since file io is only a few lines, introducing a special parser class for this also seems like an overkill. so who should read the file?",
    "present_kp": [],
    "absent_kp": [
      "design",
      "configuration",
      "resources"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "does https affect adsense?. i am starting a new guest blog where i write posts and ask others to write as well. i will be using adsense with my website very soon and i want to know if the ssl i'm using will affect the performance of the ads. also if it is going to affect it, is it better to https with login and submission screens only or it wont make a difference?",
    "present_kp": [
      "https"
    ],
    "absent_kp": [
      "google",
      "google adsense"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "should an ecommerce website be translated when moving into a new country, or a new site built from scratch?. i have a friend whose ecommerce is working very well in one language.he would like to have me translating the website into italian and to manage all the italian customers.from a seo point-of-view, since the website right now is not ranked for italian language at all, wouldn't it be better if i used a new new site with a new geographic domain .it containing some relevant keyword, rather than translating the original one? the original one has no seo strategy in place anyway.",
    "present_kp": [
      "seo"
    ],
    "absent_kp": [
      "multilingual",
      "internationalization",
      "translation"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "vs code not working on elementary os. i have downloaded vs code on elementary os, and i followed the setup instruction of setting up visual studio code on linux.the executable file code seems to be unknown, it is not opening, any ideas?",
    "present_kp": [
      "linux",
      "executable",
      "elementary os"
    ],
    "absent_kp": [
      "ubuntu"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "designing an asynchronous polling service in java. i am currently writing a java bridge to a eyetracking library written in c. the whole thing works very well, but building an actual application with it is hard, since eyetracking is done by polling for new data.polling becomes a problem as soon as you have a graphical user interface, as a poll-loop blocks the main event loop, freezing the gui. this also makes it impossible to stop the polling loop through user input.my goal is to build a polling service that is generic enough to be used for a lot of different applications. it has to be asynchronous and has to have some kind of callback mechanism to use any relevant data.this is my idea (pseudocode):pollservice { // the filterfunction decides if a eyetracking event is relevant or can be ignored setfilterfunction(filterfunction); // the callbackfunction is executed if a relevant eyetracking event is polled setcallbackfunction(callbackfunction); start(); stop();}the pollservice itself only manages the asynchronous polling process. it is supplied a couple of functions to perform the two basic steps:filter if a eyetracking event is relevant, skip event if notperform some kind of operation on any event that is relevantpossible callback operations might be:perform a long running computation with the eyetracking event dataprint something to the screenstore something to a databasecommunicate the event to another thread (using a messaging queue for example)update a gui (using platform.runlater() for example)the filterfunction and callbackfunction would be implemented as functional interfaces, so java 8 users can easily supply these functions using lambdas or method references.what do you think about this design? are there any obvious flaws i am not seeing?",
    "present_kp": [
      "java",
      "design"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "from df device name to physical drive name (vendor / type). seeing the device name by df, is it somehow possible to resolve it to the physical drive name such as vendor / type./dev/sda3 915.4g 34.9g 880.0g 4% /share/hda_data/dev/sdd3 1.8t 668.1g 1.1t 36% /share/hdd_datai have learnt that i find some info in sys/block, but i do not find the vendor's type name in there?-- added --my system is a linux based qnap nas, so things might be a little different there. for me as a non linux expert hard to tell, what is standard, and what not.-- added as of steven's answer --[~] # hdparm -i /dev/sdb3/dev/sdb3: hdio_drive_cmd(identify) failed: invalid argument",
    "present_kp": [
      "linux",
      "df"
    ],
    "absent_kp": [
      "hard disk",
      "block device"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "strategies to manage unruly system environments. in our it dept we develop multiple systems in parallel, the systems are deployed to different environments, and they talk to each other using soap and rest. it's all asp.net but we're not in the cloud. i'm trying to find a way of streamlining environments to reduce complexity.simplified view, let's say there are 3 systems in production: a,b,c. they communicate like this:prod: a 1.0 => b 1.0 => c 1.0we mirror that in a test environment like this:test: a 1.0 => b 1.0 => c 1.0if there's an urgent production bug in b, we would make a fix to b and deploy it to the test environment. test: a 1.0 => b 1.1 => c 1.0and if integration tests pass we can release it to production:prod: a 1.0 => b 1.1 => c 1.0now let's say we develop a new feature which means a and b both need to change. both a and b are being deployed to test.test: a 2.0 => b 2.0 => c 1.0during this development period, if an urgent production bug is found in b, we would have to make a fix to b and deploy it to test environment:test: a 2.0 => b 1.2 => c 1.0obviously this is shaky ground now because the test environment does not resemble production. and what about when b 2.0 has a breaking change - in this case the a 2.0 will fail when pointing to b 1.2.so what we've done is create another environment for emergency fixes, called patch. so we can have:test: a 2.0 => b 2.0 => c 1.0patch: a 1.0 => b 1.2 => c 1.0now a, b and c each have different delivery cycles, so what tends to happen is that at different stages of the project, the patch environment is abused in order to test different versions of each component for the current release or for the next release. we end up doing crazy stuff like pointing test a to patch b, or pointing patch b to test c. and this obviously generates huge confusion. not to mention that this is a massively simplified version, in reality we have at least 8 systems involved here, with 6 environments. the teamcity summary page is like spaghetti junction. also some environments talk to each other (in my example above it would be like a => b => c => a).please help me understand how this type of scenario can work.",
    "present_kp": [
      "environment"
    ],
    "absent_kp": [
      "deployment",
      "configuration",
      "release management"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why is my bind mount visible outside its mount namespace?. so i'm trying to get a handle on how linux's mount namespace works. so, i did a little experiment and opened up two terminals and ran the following:terminal 1root@goliath:~# mkdir a broot@goliath:~# touch a/foo.txtroot@goliath:~# unshare --mount -- /bin/bashroot@goliath:~# mount --bind a broot@goliath:~# ls bfoo.txtterminal 2root@goliath:~# ls bfoo.txthow come the mount is visible in terminal 2? since it is not part of the mount namespace i expected the directory to appear empty here. i also tried passing -o shared=no and using --make-private options with mount, but i got the same result.what am i missing and how can i make it actually private?",
    "present_kp": [
      "linux",
      "namespace",
      "bind mount"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "cleaned wikipedia content used on google maps - dump available?. on google, there are pages such as the following which provide a summary of the place in question taken from wikipedia: <url> am not a lawyer, but i assume that as google has obviously gone to the trouble of extracting/cleaning the data from the dump files provided by wikipedia, this constitutes a derived work, which i think means that google must provide the cleaned data for download.is providing the content online (at the link above) enough to meet that requirement, or is there a dump file of the cleaned data available somewhere?i only need the first paragraph or two for certain wikipedia pages, and downloading a cleaned dump from google would save me a lot of time and trouble :)",
    "present_kp": [
      "google maps",
      "wikipedia"
    ],
    "absent_kp": [
      "copyright"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "dependency ordering algorithm of a compiler. let's say, hypothetically, i'm writing a java compiler. and we assume that in my case a class can't be fully compiled until all signatures of dependencies (imports and other used classes) are known. because i don't want to keep the source code and ast of all classes in memory at the same time, i'll need an algorithm to manage those dependencies and process them all in the right order.what would be a good algorithm for ordering all dependencies?that:is not recursivedoes not keep all source code and/or ast nodes in memoryis linear in both space and timecan handle cyclical dependencies or maybe more general, how is this normally done?my approach looks like the following:abstract class compiler { typesystem ts; type compile(string classname) { if (ts.containstype(classname)) { return ts.gettype(classname); } // create skeleton type for this class: type type = ts.createtype(classname); // parse the class file: node ast = parse(classname); // create signature: for (node attribute : ast.findall(attributedeclaration)) { // get the text value of the name of the attribute: string attributename = attribute.find(identifier).text(); // get the text value of the type of the attribute: string attributetypename = attribute.find(type).text(); // call compile recursive! type attributetype = compile(attributetypename); type.createattribute(attributename, attributetype); } return type; } abstract node parse(string classname); // this method can find the file by class name.} for sake of simplicity, does this code only process attributes and simple structs.note that this algorithm is recursive!",
    "present_kp": [
      "java",
      "compiler",
      "dependencies"
    ],
    "absent_kp": [
      "algorithms",
      "graph"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to choose classifer. is the best way to create the most accurate classifier to train a bunch of classifying algorithms like ann, svm, knn, etc, and test different parameters to get optimal parameters for each classifier, and see which classifier has the least testing error?or is it better to use ensemble method and choose the majority decision of different kinds of trained classifiers?",
    "present_kp": [],
    "absent_kp": [
      "classification",
      "ensemble modeling"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "dealing with coworkers when developing, need advice. i developed our current project architecture and started developing it on my own (reaching something like, revision 40).we're developing a simple subway routing framework and my design seemed to be done extremely well - several main models, corresponding views, main logic and data structures were modeled as they should be and fully separated from rendering, algorithmic part was also implemented apart from the main models and had a minor number of intersection points.i would call that design scalable, customizable, easy-to-implement, interacting mostly based on the black box interaction and, well, very nice.now, what was done:i started some implementations of the corresponding interfaces, ported some convenient libraries and wrote implementation stubs for some application parts.i had the document describing coding style and examples of that coding style usage (my own written code).i forced the usage of more or less modern c++ development techniques, including no-delete code (wrapped via smart pointers) and etc.i documented the purpose of concrete interface implementations and how they should be used.unit tests (mostly, integration tests, because there wasn't a lot of actual code) and a set of mocks for all the core abstractions. i was absent for 12 days.what do we have now (the project was developed by 4 other members of the team):3 different coding styles all over the project (i guess, two of them agreed to use the same style :), same applies to the naming of our abstractions (e.g commonpathdata.h, subwayschemestructures.h), which are basically headers declaring some data structures.absolute lack of documentation for the recently implemented parts.what i could recently call a single-purpose-abstraction now handles at least 2 different types of events, has tight coupling with other parts and so on.half of the used interfaces now contain member variables (sic!).raw pointer usage almost everywhere.unit tests disabled, because (rev.57) they are unnecessary for this project.... (that's probably not everything).commit history shows that my design was interpreted as an overkill and people started combining it with personal bicycles and reimplemented wheels and then had problems integrating code chunks.now - the project still does only a small amount of what it has to do, we have severe integration problems, i assume some memory leaks.is there anything possible to do in this case?i do realize that all my efforts didn't have any benefit, but the deadline is pretty soon and we have to do something. did someone have a similar situation?basically i thought that a good (well, i did everything that i could) start for the project would probably lead to something nice, however, i understand that i'm wrong.",
    "present_kp": [
      "c++"
    ],
    "absent_kp": [
      "project management",
      "refactoring",
      "teamwork"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "check image dimensions, calculate aspect ratio, set image dimensions, resize. i have created a set of functions to calculate the aspect ratio of an image, set a height/width on document.ready and on window.resize resize the image. now, i have got these functions working, however i feel like they could be more efficient/clean. so far i have to use cases. first, an image gallery i have setup and now, iframes.the below scriptfile includes the issue above, and some functionality for mobile, and an overlay.note: all these functions work (i do have an error imageresize('.com-background > img', ratio1); ratio1 is not defined within the window.resize function.scriptfile.jsfunction mobile() { var ua = navigator.useragent; var event = (ua.match(/ipad/i)) ? 'touchstart' : 'click'; jquery('.mobile-contain > a').bind(event, function() { jquery('#menu-main').slidetoggle(500); });}; function overlay() { jquery('.com-block').mouseenter(function() { jquery(this).find('.com-title').css('display', 'none'); jquery(this).find('.com-desc').css('display', 'table-cell'); }); jquery('.com-block').mouseleave(function() { jquery(this).find('.com-title').css('display', 'table-cell'); jquery(this).find('.com-desc').css('display', 'none'); });}function imagecalc(selector) { var obj=$(selector); var $imgwidth = obj.width(); var $imgheight = obj.height(); var $imgaspectratio = $imgheight / $imgwidth; // $(selector).css('margin-left', function( calcmargin ) { return parseint($('.main-content').css('padding')) * -1 + px; }); fix for ie obj.css('margin-left', '-10px' ); return $imgaspectratio;}function setimagedims(selector, content_area, $imgaspectratio) { var container = $(content_area); $(selector).css('height', function() { return $imgaspectratio * container.width(); }); $(selector).css('width', function() { return container.width() + 20; }); }function imageresize(selector, $imgaspectratio) { $(selector).css('width', function() { return $(body).width() }); $(selector).css('height', function() { return $imgaspectratio * $(selector).width(); });}function cycleimages(){ var $active = $('.com-background .active'); var $next = ($active.next('img').length > 0) ? $active.next('img') : $('.com-background > img:first'); $next.css('z-index',2); if ($('.com-background > img').length > 1) { $active.fadeout(1500,function(){ $active.css('z-index',1).show().removeclass('active'); $next.css('z-index',3).addclass('active'); }); };}function setcontainheight() { var biggestheight = 0; $(.com-background *).each(function(){ if ($(this).height() > biggestheight ) { biggestheight = $(this).height(); } }); $(.com-background).height(biggestheight);}function setimgmaxheight() { var smallestheight = 0; $(.com-background *).each(function(){ if ($(this).height() > smallestheight ) { smallestheight = $(this).height(); } }); // set the container height $(.com-background img).css('max-height', smallestheight);}$( document ).ready(function() { $('.com-wrap').hide(); ratio1 = imagecalc('.com-background > img'); setimagedims('.com-background > img', '#main-content', ratio1); ratio2 = imagecalc('.blog-entry-content iframe'); setimagedims('.blog-entry-content iframe', '#content', ratio2); // imagecalc('.com-background > img'); // setimagedims('.com-background > img', '#main-content'); // imagecalc('.blog-entry-content iframe'); // setimagedims('.blog-entry-content iframe'); setimgmaxheight();});$(window).bind(load, function() { setcontainheight();$('.com-more a').click(function(){ $('.com-wrap').toggle(); $('html, body').animate({ scrolltop: $( $(this).attr('href') ).offset().top }, 500); return false;}); setinterval(function(){cycleimages()}, 7000); mobile(); overlay();});$(window).resize(function() { setcontainheight(); if ($(body).width() < 980 ) { imageresize('.com-background > img', ratio1); // var ratio1 = imagecalc('.com-background > img'); // setimagedims('.com-background > img', '#main-content', ratio1); } else { setimagedims('.com-background > img', '#main-content'); } if ($(body).width() < 770 ) { jquery('.menu-main-container').css('display', 'none'); } if ($(body).width() > 770 ) { jquery('.menu-main-container').css('display', 'block'); }}).resize(); i have also created a rudimentary fiddle to illustrate the imagecalc(), setimagedims(), and imageresize() functionality here.",
    "present_kp": [
      "jquery",
      "image"
    ],
    "absent_kp": [
      "javascript",
      "animation"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to select topology for neural network?. i was given a target function to design neural network and train: (y = (x1 x2) (x3 x4))the number of input and number of output seems obvious (4 and 1). and the training data can use truth table.however, in order to train as a multilayer artificial neural network, i need to choose number of hidden units. may i know where can i find some general guideline for this?thank you!",
    "present_kp": [
      "neural network"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "natural language processing (nlp) open source algorithms. i am looking for a way to give an algorithm (service) a sentence, and it will tell me if its positive context or negative or even neutral.does such a service exist?",
    "present_kp": [
      "algorithms"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "postfix alias for root not working in suse. first tryi've set up multiple servers running postfix.to get a general grasp of the state of the machine i usually redirect e-mail for root to my normal adress.i recently installed suse and tried to do the same. opened /etc/aliases, added a line like this root: <email> and ran newaliases. but messages are still delivered locally.i've tried doing the same configuration using yast2. it seems to do the same thing, but it does not deliver the message to my other machine.while troubleshooting something else broke, so now messages are no longer delivered locally, but rather the server refuses to create them:# telnet localhost 25trying ::1...connected to localhost.escape character is '^]'.220 localhost esmtphelo localhost250 localhostmail from: <email> 2.1.0 okrcpt to: root451 4.3.0 <<email> temporary lookup failurequit221 2.0.0 byeconnection closed by foreign host.second tryi've tried nuking the installation:# zypper rm postfixloading repository data...reading installed packages...resolving package dependencies...the following package is going to be removed: postfix1 package to remove.after the operation, 4.8 mib will be freed.continue? [y/n/? shows all options] (y):(1/1) removing postfix-2.11.4-2.1 ........................................[done]additional rpm output\u26a0\ufe0f /etc/postfix/virtual saved as /etc/postfix/virtual.rpmsavewarning: /etc/postfix/sender_canonical saved as /etc/postfix/sender_canonical.rpmsavewarning: /etc/postfix/master.cf saved as /etc/postfix/master.cf.rpmsavewarning: /etc/postfix/main.cf saved as /etc/postfix/main.cf.rpmsavethere are some running programs that might use files deleted by recent upgrade. you may wish to check and restart some of them. run 'zypper ps' to list these programs.# rm -rf /etc/postfix# rm -rf /etc/aliasesand starting over, using only yast2:# zypper in postfixloading repository data...reading installed packages...resolving package dependencies...the following new package is going to be installed: postfix1 new package to install.overall download size: 1007.1 kib. already cached: 0 b after the operation, additional 4.8 mib will be used.continue? [y/n/? shows all options] (y):retrieving package postfix-2.11.4-2.1.x86_64 (1/1), 1007.1 kib ( 4.8 mib unpacked)retrieving: postfix-2.11.4-2.1.x86_64.rpm .......................................................................................................................[done (799.0 kib/s)]checking for file conflicts: ..................................................................................................................................................[done](1/1) installing: postfix-2.11.4-2.1 ..........................................................................................................................................[done]# yast2 mailbut mail to root does not get sent to my other machine:# telnet localhost 25trying ::1...connected to localhost.escape character is '^]'.220 suse-server.domain.com esmtphelo localhost250 suse-server.domain.commail from: <email> 2.1.0 okrcpt to: root451 4.3.0 <<email> temporary lookup failurequit221 2.0.0 byeconnection closed by foreign host.the same connection looks like this in the mail.logmar 23 21:12:56 suse-server postfix/smtpd[22842]: connect from unknown[::1]mar 23 21:13:22 suse-server postfix/trivial-rewrite[22844]: error: open database /etc/postfix/virtual.db: no such file or directorymar 23 21:13:22 suse-server postfix/trivial-rewrite[22844]: error: open database /etc/postfix/relay.db: no such file or directorymar 23 21:13:22 suse-server postfix/trivial-rewrite[22844]: error: open database /etc/postfix/relocated.db: no such file or directorymar 23 21:13:22 suse-server postfix/trivial-rewrite[22844]: error: open database /etc/postfix/transport.db: no such file or directorymar 23 21:13:22 suse-server postfix/trivial-rewrite[22844]: warning: hash:/etc/postfix/transport is unavailable. open database /etc/postfix/transport.db: no such file or directory mar 23 21:13:22 suse-server postfix/trivial-rewrite[22844]: warning: hash:/etc/postfix/transport lookup error for * mar 23 21:13:22 suse-server postfix/trivial-rewrite[22844]: warning: hash:/etc/postfix/transport is unavailable. open database /etc/postfix/transport.db: no such file or directory mar 23 21:13:22 suse-server postfix/trivial-rewrite[22844]: warning: hash:/etc/postfix/transport lookup error for *mar 23 21:13:22 suse-server postfix/trivial-rewrite[22844]: warning: hash:/etc/postfix/virtual is unavailable. open database /etc/postfix/virtual.db: no such file or directorymar 23 21:13:22 suse-server postfix/trivial-rewrite[22844]: warning: hash:/etc/postfix/virtual: table lookup problemmar 23 21:13:22 suse-server postfix/trivial-rewrite[22844]: warning: hash:/etc/postfix/transport is unavailable. open database /etc/postfix/transport.db: no such file or directory mar 23 21:13:22 suse-server postfix/trivial-rewrite[22844]: warning: hash:/etc/postfix/transport lookup error for <email> 23 21:13:22 suse-server postfix/trivial-rewrite[22844]: warning: transport_maps lookup failuremar 23 21:13:26 suse-server postfix/trivial-rewrite[22844]: warning: hash:/etc/postfix/virtual is unavailable. open database /etc/postfix/virtual.db: no such file or directorymar 23 21:13:26 suse-server postfix/trivial-rewrite[22844]: warning: hash:/etc/postfix/virtual: table lookup problemmar 23 21:13:26 suse-server postfix/trivial-rewrite[22844]: warning: hash:/etc/postfix/transport is unavailable. open database /etc/postfix/transport.db: no such file or directory mar 23 21:13:26 suse-server postfix/trivial-rewrite[22844]: warning: hash:/etc/postfix/transport lookup error for <email> 23 21:13:26 suse-server postfix/trivial-rewrite[22844]: warning: transport_maps lookup failuremar 23 21:13:26 suse-server postfix/smtpd[22842]: noqueue: reject: rcpt from unknown[::1]: 451 4.3.0 <<email> temporary lookup failure; from=<<email> to=<root> proto=smtp helo=<localhost>mar 23 21:13:29 suse-server postfix/smtpd[22842]: disconnect from unknown[::1]please help.keep troubleshootingabout my system# cat /etc/os-releasename=opensuseversion=20150319 (tumbleweed)version_id=20150319pretty_name=opensuse 20150319 (tumbleweed) (x86_64)id=opensuseansi_color=0;32cpe_name=cpe:/o:opensuse:opensuse:20150319bug_report_url=<url>_url=<url> errorleft the machine in a broken state over night, log is filled with lines like the following:mar 24 06:48:28 suse-server postfix/pickup[25006]: 6b0de6fd: uid=0 from=<root>mar 24 06:48:28 suse-server postfix/cleanup[25051]: warning: hash:/etc/postfix/sender_canonical is unavailable. open database /etc/postfix/sender_canonical.db: no such file or directorymar 24 06:48:28 suse-server postfix/cleanup[25051]: warning: hash:/etc/postfix/sender_canonical lookup error for <email> 24 06:48:28 suse-server postfix/cleanup[25051]: warning: 6b0de6fd: sender_canonical_maps map lookup problem for <email> -- message not accepted, try again latermar 24 06:48:28 suse-server postfix/cleanup[25051]: warning: hash:/etc/postfix/canonical is unavailable. open database /etc/postfix/canonical.db: no such file or directorymar 24 06:48:28 suse-server postfix/cleanup[25051]: warning: hash:/etc/postfix/canonical lookup error for <email> 24 06:48:28 suse-server postfix/cleanup[25051]: warning: 6b0de6fd: canonical_maps map lookup problem for <email> -- message not accepted, try again latermar 24 06:48:28 suse-server postfix/pickup[25006]: warning: maildrop/80e795fc: error writing 6b0de6fd: queue file write errorcleaning up the logsthe numerous lookup errors can be silenced with postman (why yast2 mail does not run postmap is beyond me to understand):suse-server:~ # postmap /etc/postfix/transportsuse-server:~ # postmap /etc/postfix/sender_canonicalsuse-server:~ # postmap /etc/postfix/canonicalsuse-server:~ # postmap /etc/postfix/virtualsuse-server:~ # postmap /etc/postfix/relaysuse-server:~ # postmap /etc/postfix/relocatedsuse-server:~ # systemctl restart postfix.servicenew errorspostmapping is not the solution to the problem however:mar 24 15:18:02 suse-server echo[27096]: starting mail service (postfix)mar 24 15:18:02 suse-server postfix/postfix-script[27178]: starting the postfix mail systemmar 24 15:18:02 suse-server postfix/master[27180]: daemon started -- version 2.11.4, configuration /etc/postfixmar 24 15:18:02 suse-server postfix/pickup[27182]: c9084931: uid=0 from=<root>mar 24 15:18:02 suse-server postfix/cleanup[27187]: c9084931: message-id=<<email> 24 15:18:02 suse-server postfix/qmgr[27183]: c9084931: from=<<email>, size=785, nrcpt=1 (queue active)mar 24 15:18:02 suse-server postfix/cleanup[27187]: d6a16932: message-id=<<email> 24 15:18:02 suse-server postfix/qmgr[27183]: d6a16932: from=<<email>, size=910, nrcpt=1 (queue active)mar 24 15:18:02 suse-server postfix/local[27189]: c9084931: to=<<email>, orig_to=<root>, relay=local, delay=45656, delays=45656/0.01/0/0.03, dsn=2.0.0, status=sent (forwarded as d6a16932)mar 24 15:18:02 suse-server postfix/qmgr[27183]: c9084931: removedmar 24 15:18:02 suse-server postfix/smtp[27190]: fatal: valid hostname or network address required in server description: []mar 24 15:18:03 suse-server postfix/qmgr[27183]: warning: private/smtp socket: malformed responsemar 24 15:18:03 suse-server postfix/qmgr[27183]: warning: transport smtp failure -- see a previous warning/fatal/panic logfile record for the problem descriptionmar 24 15:18:03 suse-server postfix/master[27180]: warning: process /usr/lib/postfix/smtp pid 27190 exit status 1mar 24 15:18:03 suse-server postfix/master[27180]: warning: /usr/lib/postfix/smtp: bad command startup -- throttlingmar 24 15:18:03 suse-server postfix/error[27191]: d6a16932: to=<<email>, orig_to=<root>, relay=none, delay=1.1, delays=0.03/1/0/0.04, dsn=4.3.0, status=deferred (unknown mail transport error)mar 24 15:23:02 suse-server postfix/qmgr[27183]: d6a16932: from=<<email>, size=910, nrcpt=1 (queue active)mar 24 15:23:02 suse-server postfix/smtp[27196]: fatal: valid hostname or network address required in server description: []mar 24 15:23:03 suse-server postfix/qmgr[27183]: warning: private/smtp socket: malformed responsemar 24 15:23:03 suse-server postfix/qmgr[27183]: warning: transport smtp failure -- see a previous warning/fatal/panic logfile record for the problem descriptionmar 24 15:23:03 suse-server postfix/master[27180]: warning: process /usr/lib/postfix/smtp pid 27196 exit status 1mar 24 15:23:03 suse-server postfix/master[27180]: warning: /usr/lib/postfix/smtp: bad command startup -- throttlingmar 24 15:23:03 suse-server postfix/error[27198]: d6a16932: to=<<email>, orig_to=<root>, relay=none, delay=300, delays=299/1/0/0.03, dsn=4.3.0, status=deferred (unknown mail transport error)mar 24 15:23:48 suse-server postfix/smtpd[27202]: error: open database /etc/postfix/access.db: no such file or directorymar 24 15:23:48 suse-server postfix/smtpd[27202]: connect from unknown[::1]mar 24 15:24:11 suse-server postfix/smtpd[27202]: warning: hash:/etc/postfix/access is unavailable. open database /etc/postfix/access.db: no such file or directorymar 24 15:24:11 suse-server postfix/smtpd[27202]: warning: hash:/etc/postfix/access: table lookup problemmar 24 15:24:11 suse-server postfix/smtpd[27202]: noqueue: reject: rcpt from unknown[::1]: 451 4.3.5 <user>: sender address rejected: server configuration error; from=<user> to=<root> proto=smtp helo=<localhost>mar 24 15:24:18 suse-server postfix/smtpd[27202]: disconnect from unknown[::1]i do start to believe that postfix on suse is blessed with rather insane defaults, with the pile of logged errors not getting any smaller even though a lot of the errors has been removed.mail accepted after another postmapfound another file to postmap, after which postfix accepts the message.suse-server:~ # postmap /etc/postfix/accesssuse-server:~ # systemctl restart postfix.servicesuse-server:~ # telnet localhost 25trying ::1...connected to localhost.escape character is '^]'.220 suse-server.domain.com esmtphelo localhost250 suse-server.domain.commail from: user250 2.1.0 okrcpt to: root250 2.1.5 okdata354 end data with <cr><lf>.<cr><lf>subject: testasd.250 2.0.0 ok: queued as 4797894fquit221 2.0.0 byeconnection closed by foreign host.unfortunately it still tosses the message in the bin:suse-server:~ # journalctl | tail -18mar 24 15:35:18 suse-server echo[27287]: starting mail service (postfix)mar 24 15:35:19 suse-server postfix/postfix-script[27369]: starting the postfix mail systemmar 24 15:35:19 suse-server postfix/master[27371]: daemon started -- version 2.11.4, configuration /etc/postfixmar 24 15:35:45 suse-server postfix/smtpd[27380]: connect from unknown[::1]mar 24 15:35:59 suse-server postfix/smtpd[27380]: 4797894f: client=unknown[::1]mar 24 15:36:12 suse-server postfix/cleanup[27383]: 4797894f: message-id=<<email> 24 15:36:12 suse-server postfix/qmgr[27374]: 4797894f: from=<<email>, size=299, nrcpt=1 (queue active)mar 24 15:36:12 suse-server postfix/cleanup[27383]: e8c94950: message-id=<<email> 24 15:36:12 suse-server postfix/qmgr[27374]: e8c94950: from=<<email>, size=424, nrcpt=1 (queue active)mar 24 15:36:12 suse-server postfix/local[27384]: 4797894f: to=<<email>, orig_to=<root>, relay=local, delay=18, delays=18/0.01/0/0.03, dsn=2.0.0, status=sent (forwarded as e8c94950)mar 24 15:36:12 suse-server postfix/qmgr[27374]: 4797894f: removedmar 24 15:36:12 suse-server postfix/smtp[27385]: fatal: valid hostname or network address required in server description: []mar 24 15:36:14 suse-server postfix/qmgr[27374]: warning: private/smtp socket: malformed responsemar 24 15:36:14 suse-server postfix/qmgr[27374]: warning: transport smtp failure -- see a previous warning/fatal/panic logfile record for the problem descriptionmar 24 15:36:14 suse-server postfix/master[27371]: warning: process /usr/lib/postfix/smtp pid 27385 exit status 1mar 24 15:36:14 suse-server postfix/master[27371]: warning: /usr/lib/postfix/smtp: bad command startup -- throttlingmar 24 15:36:14 suse-server postfix/error[27386]: e8c94950: to=<<email>, orig_to=<root>, relay=none, delay=1.1, delays=0.03/1/0/0.03, dsn=4.3.0, status=deferred (unknown mail transport error)mar 24 15:36:14 suse-server postfix/smtpd[27380]: disconnect from unknown[::1]valid hostnameyast2 sets relayhost = [] in /etc/postfix/main.cf, removing that line removes the error:fatal: valid hostname or network address required in server description: []fatal: unknown service: smtp/tcpthat gives way to the error:postfix/smtp[6108]: fatal: unknown service: smtp/tcpwhich can be solved by disabling chroot on smtp by changing master.cf from:# grep ^smtp master.cf smtp inet n - y - - smtpdsmtp unix - - y - - smtpto# grep ^smtp master.cf smtp inet n - n - - smtpdsmtp unix - - n - - smtpafter disabling the chroot i do get root's e-mail to my expected mailbox.so the question now is probably why postfix is installed without a working chroot for smtp?",
    "present_kp": [
      "email",
      "opensuse",
      "postfix",
      "suse"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "file limit on shared hosting - does the issue still apply?. i've heard that shared hosting often imposes a limit on files per directory, though obviously this would be different per host.generally speaking, is number of files still an issue on modern hosts? i am creating a static-page generator meant to be distributed -- yet if someone has a 500k pages (files, even if small) and this passes some arbitrary shared-hosting file limit, then a dynamic solution would be preferred.at our current time, do many shared hosts impose file limits?",
    "present_kp": [
      "files"
    ],
    "absent_kp": [
      "web hosting"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "apache: how to access files using the requesting user's credentials?. i have a domain-joined (via samba/sssd) centos 7 server, acting as a sftp server, which has a mountpoint mapped to a windows' share:sftp client -> linux sftp server -> windows file servermy goal here is to eliminate linux-sided permissions and configs as much as possible.the mount is configured with multiuser and sec=krb5 options. users logging into the sftp server are authenticated against ad database, and everything works great: for each user logging in and accessing the mounted/shared folder, the multiuser+krb5 settings kick in and folder permissions are fully controlled by the windows file server holding the files, since each user mounts this share using their own kerberos ticket obtained upon logon to the sftp server.great. now onto the sad part of the story.i have to provide https access to the same files mentioned above. the problem is that i couldn't find a way to make apache/httpd access files using the authenticated user's credentials, which would cause the share to be mounted with the right (requesting user's) credentials, instead of apache's user.i could work around this by setting mod ldap's group authorization, but i really wanted to control everything using windows' permissions, just as i do with sftp access. any ideas aside from using iis (which does exactly what i want by default)?i can already use single sign-on through apache, and even cache the kerberos tickets. so what's stopping me (aside from my lack of programming skills) from using that same ticket (and username info) to call automount with it?",
    "present_kp": [
      "mount",
      "samba",
      "kerberos"
    ],
    "absent_kp": [
      "apache httpd",
      "automounting"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "using google drive (google docs) in offline mode. i want to have a copy of my microsoft excel and word documents in offline mode. is there way to to copy or backup those files with scheduling tools everyday or another automatic sync software that do it for me?",
    "present_kp": [
      "google drive"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "paid software includes mit licensed library, does that put my app under mit too?. if i include mit licensed source code in my program, am i obliged to provide my whole software under mit?the above copyright notice and this permission notice shall be included in all copies or substantial portions of the software.how is substantial determined? the wording is not very specific.",
    "present_kp": [
      "mit license"
    ],
    "absent_kp": [
      "licensing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how do i find out who has already reserved a resource i.e. conference room in google calendar?. scenario:a small group of employees has reserved the conference room via a google calendar event through gmail. at a later date a meeting with a bunch of executives is scheduled only to find the conference room is already reserved. how can i find out who originally reserved the room so that we can negotiate a swap?if i'm not mistaken a calendar is created behind the scenes when you add an asset that you can check out with calendar events. is there an easy way to hook directly into that calendar?the image below shows an asset added to an event, which makes it a guest. if i hover over that guest i can see there appears to be a calendar url. unfortunately i don't see any way to copy that or otherwise get to the calendar without writing it down by hand and then manually typing it into google calendar to search.",
    "present_kp": [
      "google calendar"
    ],
    "absent_kp": [
      "google apps"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to see what processes were running?. is there a chance to get the processes that ran before my system crash?editwhat i really want is to see the past processes.my system crashed & i want to know if a specific process was the main reason.i search into all /var/log logs, but nothing, the only suspect in this were some apache logs, where i found some kind of scans... so now i want to check out for all processes running at that time.",
    "present_kp": [
      "crash"
    ],
    "absent_kp": [
      "monitoring",
      "ps"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to show all archived messages in gmail?. how do we search for all our archived messages in gmail?non-archived messages should not be displayed,only the archived messages should be shown.",
    "present_kp": [
      "gmail"
    ],
    "absent_kp": [
      "gmail archive"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "are if statements unnecessary if a program is represented as an explicit state machine?. this question occurred to me some time ago when i was thinking about whether or not if statements are fundamental in computation.consider a program that manages a single bank account (for the sake of simplicity). the bank account could be defined as something likeaccount { int balance; // current amount of money boolean withdraw(int n) { if (balance >= n) { balance = balance -n; return true; } else return false; } void deposit(int n) { amount = amount + n; }}since the program has no way to known in which state it currently is unless it performs validations using if statements, as in the withdraw operation, if statements are unavoidable.however, over the course of time, the program will pass through a finite set of states that can be known beforehand. in this particular case, a state is defined solely by the value of the balance variable, hence we would have states: {balance = 0 , balance = 1, balance = 2...}.if we assign each state a number, say state {0,1,2,....} with a 1-1 correspondence to the above set of states, and assign to each operation a number identifier as well (say deposit = 0 and withdraw = 1), we could model the program as an explicit transition between states and therefore remove every if statement from the code.consider that state = 0 is the state where balance = 0 and we want to perform a deposit of 50 dollars, if we coded every single possible execution of the deposit function, we could just define the deposit function asvoid deposit (int n){ deposit[state][n]; // ndex deposit instance for state = 0, amount = n;}deposit[][] would be a matrix of pointers for a set of functions that represent each possible execution of deposit, likedeposit[0][0] -> balance = balance + 0; state = 0;deposit[0][1] -> balance = balance + 1; state = 1;....in the case of withdrawal, it would be like:boolean withdraw (int n){ // ndex withdraw instance for the current state and amount=n return withdraw[state][n]; }withdraw[][] would be a matrix of pointers for a set of functions that represent each possible execution of withdraw, like:deposit[0][100] -> return false; state = state;...deposit[200][100] -> balance = balance - 100; return true; state = 100;in this situation, the program the managers the single bank account can be written without using a single if statement!as a consequence however, we have to use a lot more memory, which may make the solution unreasonable. also one may put the question of how did we fill the deposit[][] and withdraw[][] matrices? by hand? it somehow implies that a previous computation that used ifs as necessary to determine and possible states and transitions.all in all, are ifs fundamental or does my example prove that they aren't? if they are, can you provide me an example where this does not work? if they are not, why dont we use solutions like these more often?is there some law of computation which states that if statements are unavoidable?",
    "present_kp": [],
    "absent_kp": [
      "programming languages",
      "computation models"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "keep metadata or edit metadata with tar. i have a tarball. this tarball containsof coursevarious files and directories. when i extract the tar, some of these files already exist, while others don't.what i would like to know is this: how can i extract a tarball to keep the metadata of already existing files intact (ownership, group, read, write, execute etc.), regardless who executes the command and what was the original directory layout upon tar creation?so far what i have found are these:--mode='555' --owner=owner --group=groupwhen i use these flags when creating the tarball, it actually does change the metadata of all the files in the tar accordingly. however i don't know how could i change it for a small set of files which should have other permissions.--no-overwrite-diras i understand this flag is used when extracting the tarball and should keep the metadata of already existing files, but i don't get the expected results.",
    "present_kp": [
      "permissions",
      "tar"
    ],
    "absent_kp": [
      "compression",
      "file metadata"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "simple c++ class for storing and manipulations with money. problemi want to create a simple class for storing money - i will be storing dollars/cents as just ints for this model.idea is to implement various operations on money class - to be able to add/subtract/multiply/divide money by constant (division is integer for our model - so we don't support money less then a penny - for simplification).please, comment on the correctness of implementations in terms of - operators overloading, copy/move constructors/numbers manipulations and general code style and quality.i have a few tests passing for my class here - so this is fully functional (except overflow checks - marked as todo - would be great if someone could recommend industry standard for overflow/underflow checks in cpp which is used everywhere, safe and fast).money.h#ifndef change_money_h_#define change_money_h_#include <exception>#include <iostream>#include <string>namespace change { class money { private: int32_t whole; int8_t fraction; void swap(money other); public: money(const money& other); money(const money&& other); money(int32_t _whole, int8_t _fraction); ~money(); int32_t getwhole() const { return whole; } int8_t getfraction() const { return fraction; } money& operator=(const money& other) { if (&other != this) { whole = other.whole; fraction = other.fraction; } return *this; } bool operator==(const money& other) const { return whole == other.whole && fraction == other.fraction; } bool operator!=(const money& other) const { return !(*this == other); } bool operator>(const money& other) const { if (whole > other.whole) { return true; } else if (whole == other.whole) { return fraction > other.fraction; } else{ return false; } } bool operator>=(const money& other) const { return (*this == other || *this > other); } bool operator<(const money& other) const { return !(*this == other || *this > other); } bool operator<=(const money& other) const { return (*this == other || *this < other); } money operator+(money& other) const { money result(0, 0); int16_t sum = fraction + other.fraction; int16_t newfrac = static_cast<int16_t>(sum) % 100; int64_t carry = sum > 100 ? 1 : 0; // todo: overflow check result.whole = whole + other.whole + carry; result.fraction = static_cast<int8_t>(newfrac); return result; } money operator+=(money other) { *this = *this + other; return *this; } money operator-(const money& other) const { money result(0, 0); int8_t newfrac = (fraction - other.fraction) % 100; int32_t carry = newfrac < 0 ? -1 : 0; newfrac = newfrac < 0 ? 100 + newfrac : newfrac; // todo: overflow result.whole = whole - other.whole + carry; result.fraction = newfrac; return result; } money operator-=(money other) { *this = *this - other; return *this; } template <class t> money operator*(t number) const { // todo: overflow check int64_t fracmult = static_cast<int64_t>(fraction) * number; int8_t newfrac = static_cast<int8_t>(fracmult % 100); // todo: overflow check int32_t newwhole = whole * number + (fracmult / 100); money tmp(newwhole, newfrac); return tmp; } template <class t> money operator/(t number) const { if (number == 0) { throw std::invalid_argument(division by zero); } // todo: overflow check int64_t total = (100 * static_cast<int64_t>(whole)) + fraction; int64_t result = total / number; int8_t newfrac = static_cast<int8_t>(result % 100); int32_t newwhole = static_cast<int32_t>(result / 100); money tmp(newwhole, newfrac); return tmp; } friend std::ostream& operator<<(std::ostream& os, const money& money); };}#endif //change_money_h_money.cpp#include money.h#include <iomanip>namespace change { money::money(const money& other) { whole = other.whole; fraction = other.fraction; } money::money(const money&& other) { *this = std::move(other); } money::money(int32_t _whole, int8_t _fraction) { whole = _whole; fraction = _fraction; } money::~money() { // nothing for now } void money::swap(money other) { std::swap(whole, other.whole); std::swap(fraction, other.fraction); } // this is just for test purposes - print in dollars always std::ostream& operator<<(std::ostream& os, const money& money) { return os << '$' << std::to_string(money.whole) << '.' << std::setw(2) << std::setfill('0') << static_cast<int16_t>(money.fraction); }}",
    "present_kp": [
      "c++"
    ],
    "absent_kp": [
      "c++11"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "implementing an algorithm that walks the dom without recursion. here's a simple algorithm that walks the dom given a node:function walkdom(n) { do { console.log(n); if (n.haschildnodes()) { walkdom(n.firstchild) } } while (n = n.nextsibling)}i wanted to implement it iteratively as an exercise, and came up with this:function walkdom2(n) { var recstack = []; // first get the parent of the given node, so that // you can get the siblings of the given node too // (starting from the last sibling), // rather than just start with the children of the // given node. // (this is to make this behave the // same way as the recursive one.) recstack.push(n.parentnode); while (recstack.length > 0) { var current = recstack.pop(); // log only if the current node is // the given node or a node below it. // (this is to make this behave the // same way as the recursive one.) if (current != n.parentnode) console.log(current); if (!current.haschildnodes()) continue; current = current.lastchild; do { recstack.push(current); // skip the sibling nodes // before the given node. // (this is to make this behave the // same way as the recursive one.) if (current === n) break; } while (current = current && current.previoussibling); }}i have used a couple tricks to make it behave the same way as the first recursive version. is there a more concise way of writing this without recursion?",
    "present_kp": [
      "recursion",
      "dom"
    ],
    "absent_kp": [
      "javascript",
      "iteration"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to rotate video?. i have many videos taken from nokia n73 that are getting played on my laptop (after transferring from phone to laptop) horizontally. how to make them play vertically.i have read this link. but the issue is this link talks of mp4 file & mine is avi. regarding my mp4 video file i ran:ffmpeg -i infile.mp4 -vf transpose=1 -an -vcodec mpeg4 outfile.mp4it worked for mp4 but i lost my sound from the video. also after getting rotated, the area on screen on which the video is running has shrinked (one can call it frame size or so). now it has become more narrow vertically. is this how it will be?",
    "present_kp": [
      "video"
    ],
    "absent_kp": [
      "video editing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "tunctl complains that device or resource is busy if used in /etc/qemu-ifup script. i have a following /etc/qemu-ifup script which is executed when i start /usr/bin/qemu-system-i386 binary:#!/bin/shset -xswitch=br0if [ -n $1 ];then /usr/sbin/tunctl -u 'whoami' -t $1 /sbin/ip link set dev $1 up sleep 0.5s /sbin/brctl addif $switch $1 exit 0else echo error: no interface specified exit 1fithe problem is, that tunctl complains that device or resource is busy:root@vm-host:~# qemu -hda /root/1.raw -device e1000,netdev=net0,mac=de:ad:be:ef:69:01 -netdev tap,id=net0 -display vnc=:1+ switch=br0+ [ -n tap0 ]+ whoami+ /usr/sbin/tunctl -u root -t tap0tunsetiff: device or resource busy+ /sbin/ip link set dev tap0 up sleep 0.5serror: either dev is duplicate, or sleep is a garbage.+ /sbin/brctl addif br0 tap0+ exit 0when i execute simply the /usr/sbin/tunctl -u root -t tap0, then tap0 is created without any issues:root@vm-host:~# /usr/sbin/tunctl -u root -t tap0set 'tap0' persistent and owned by uid 0root@vm-host:~# any ideas what causes such behavior?",
    "present_kp": [
      "qemu"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "eliminating need to create near-duplicate content. ever since i included the tutorial feature on my site, my adsense income consistently went to the bottom every day since the feature was added. now i feel the only way i can make money again is to re-create every single url with extra characters.for example: i had these urls:<url> i feel i need to invent these urls which mimic the output resulting from accessing the above urls: <url> it's approximate that:<url> = <url> = <url> = <url> = <url> i have currently done in an attempt to save a headache is to only use these urls:<url> i use a cookie that determines whether the tutorial is activated or not. users can easily access tutorial mode with a click of a button.somehow i get the impression that the googlebot is so confused it believes i'm cloaking content but i'm not sure. my question then (in the most generic form) is:is there another approach i could use to display different content on the same url that google (and adsense) would gladly accept? i tried using cookie values to define the actual content that appears on screen on a url, but i don't think adsense likes it?",
    "present_kp": [
      "google",
      "duplicate content",
      "content",
      "cloaking"
    ],
    "absent_kp": [
      "google adsense"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "centos7 ssh can't connect localhost?. i'm configuring hadoop environment.i have use $ ssh-keygen -t rsa -p to generate id_rsa.pub and id_rsa.and use cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys to set password-free login.now, i enter ssh localhost command and get this error: the authenticity of host 'localhost (::1)' can't be established.how can i solve this problem?",
    "present_kp": [
      "centos",
      "ssh",
      "hadoop"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "extracting a maximum from zipped lists. i have code that zips 3 lists, finds a maximum and extracts part of the max tuple. isn't there a shorter way to do this in f#?let triplets = list.zip3 a b clet t1 (x, _, _) = xlet t2 (_, x, _) = xlet t3 (_, _, x) = xlet best = list.maxby t3 tripletst1 best,t2 best // return to c# code",
    "present_kp": [
      "f#"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "optimisation xml handling within vb.net application. i need to optimize this snippet to be faster:dim lst = (from t in docelet.childnodes select id = t.item(id).outerxml).distinct().tolist() parallel.for(0, lst.count, sub(i) dim p as xmlelement = getelement(lst(i)) dim ls = (from t in docelet.childnodes where t.item(id).innertext = p.innertext select t) parallel.foreach(ls, sub(d) dim verif_date as string = d.item(dad).innertext sej.id = d.item(id).innertext end sub) end sub)this is the xml structure:<?xml version=1.0 encoding=utf-8?><patientdata><sejour><id></id><dad></dad></sejour> </patientdata>i'm asking how i can optimize my code because it takes a lot of time (50 sec) in the case where the list contains 20000 elements.",
    "present_kp": [
      ".net",
      "xml",
      "vb.net"
    ],
    "absent_kp": [
      "linq"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "recommendation for laptop for photo editing. i want to purchase a laptop for my job. i need it to do photo editing and short film editing.main programs i use are adobe productsi need an fhd 1920x1080 screen, processor needs to be an i7my budget is around $600",
    "present_kp": [
      "laptop"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "graph has several mst what does it mean combinatorically?. this question is not theoretical, it's about combinatorial meaning.in graph theory there is a notion of complexity of a graph, which is equal to the number of spanning trees in a graph, which combinatorically simply means how much the graph is 'connected' (it is the way i understand it, is there any other combinatorial meaning? (there are different meanings of 'connectivity', including algebraic connectivity, which is second eigenvalue of laplacian, but this one is more intuitive, as i think)). there is a fast algorithms to compute complexity of a given graph (look here for the discussion).there is also a fast algorithm to compute the number of minimal spanning trees(mst) of a given weighted graph (for more information read this paper).the question is: what does having several mst's mean in terms of graph and distances? p.s. it certainly means that not all distances are different, but, maybe there is some property, which provides more interesting consequences?thank you!",
    "present_kp": [
      "graph theory"
    ],
    "absent_kp": [
      "co.combinatorics",
      "spectral graph theory"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "kill process running on port 80. this is the process i want to kill:sooorajjj@treako ~/desktop/merkmod $ sudo netstat -tunap | grep :80tcp6 0 0 :::80 :::* listen 20570/httpd",
    "present_kp": [],
    "absent_kp": [
      "linux",
      "debian"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is there any way to verify the integrity of a xz archive without decompressing the entire archive?. as the title says it all can i check the integrity of the archive without decompressing it as the archive is 64gb. it is a backup of my old laptop and before i format my old laptop i wanted to verify the backup but it will take too long to decompress and verify. so is there a faster way to do so?",
    "present_kp": [
      "archive",
      "integrity",
      "xz"
    ],
    "absent_kp": [
      "tar",
      "compression"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "api access to cognito forms. is there any api access to cognito forms or are there plans to make that available?i'm working on a particular form and finding the email notification is significantly delayed and having api access would be a much simpler solution.",
    "present_kp": [
      "cognito forms"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "install python package in linux opensuse. i am using this weird suse machine. i need to install a python package in my python 2.7. i downloaded it from the site and then untar it and cd to the directory.. tried:cd /python-package_pyxnat/python2.7 setup.py installi am getting following output:/usr/lib/python2.7/distutils/dist.py:267: userwarning: unknown distribution option: 'install_requires' warnings.warn(msg)/usr/lib/python2.7/distutils/dist.py:267: userwarning: unknown distribution option: 'summary' warnings.warn(msg)running installrunning buildrunning build_pyrunning install_librunning install_egg_inforemoving /usr/local/lib/python2.7/site-packages/pyxnat-1.0.0.0-py2.7.egg-infowriting /usr/local/lib/python2.7/site-packages/pyxnat-1.0.0.0-py2.7.egg-infoerror after importing pyxnat package:>>> import pyxnattraceback (most recent call last): file <stdin>, line 1, in <module> file pyxnat/__init__.py, line 13, in <module> from .core import interface file pyxnat/core/__init__.py, line 4, in <module> from .interfaces import interface file pyxnat/core/interfaces.py, line 8, in <module> import requestsimporterror: no module named requestswhy am i not able to import python package even after installing it?",
    "present_kp": [
      "python",
      "opensuse"
    ],
    "absent_kp": [
      "package management"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "sorting in a probabilistic order. given a list of real numbers $p_1, \\dots, p_n$, i am looking for a most efficient algorithm to sort this list in a probabilistic ascending order, meaning that $p_i < p_j$ implies that it is likely for $i$ to be placed before $j$, but not certain. in principle, every permutation is a possible output, but the less sorted the permutation is, the less likely it is to occur.the best solution i could come up with is to modify selection sort. instead of selecting the minimal element in every step, you select a random element with probability proportional to $ rac{1}{p_i}$. this has quadratic complexity of course, so i was wondering if there are better alternatives.",
    "present_kp": [
      "sorting"
    ],
    "absent_kp": [
      "probability theory"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "cuda kernel - neural net. i'm building a spiking neural net (recurrent, integrate and fire), and i'm curious about how to reduce the warp divergence (and other problems) i may have.here's an example with a few hand-placed neurons and synapses for a better apprehension. i upload the whole code on a git repo for faster access, make tui then ./cudasnn to try.the very basic workflow is to execute the 4 kernels (explanation in the comments) one after another.after 5000 cycles, here's the time in ms for each kernel in order:1 - 0.196ms2 - 3.558ms3 - 0.038ms4 - 4.416ms to 6.278msmy code is split into three files, whose name are pretty explicit.nn.hpp (which contains my structures)#ifndef nn_hpp# define nn_hpp# include <iostream>/* window setting -- for sfml, no need here */# define width 1280# define height 720# define xoffset -0# define yoffset -95/* network settings */# define stimulus 1# define inhibition -1# define stimulus_ratio 80# define spike_buffer 4# define input 0# define output 1# define hidden 2typedef struct s_neuron_info{ float x, y, z; char gid; unsigned char group; // hidden, input, output} t_neuron_info;typedef struct s_neuron{ short in_time; float in_value; int next_time; float action_potential; float threshold; // fire when threshold reached float weight; char type; // stimulus, inhibition char carry;} t_neuron;typedef struct s_synapse{ int id_in; int id_out; int axonal_delay; // in timestep} t_synapse;typedef struct s_spike{ int syn_id; int id_out; int start_t, end_t; // in timestep float value; bool active;} t_spike;typedef struct s_network{ t_neuron *neurons; t_neuron_info *neurons_info; t_synapse *synapses; t_spike *spikes; int neur_count; int syn_count; int timestep; int group_size;} t_network;/* generation */t_network *generate_network(void);/* gpu simulation */float simulate_network(t_network *nw);#endifsimulate_network.cu (which contains the kernels and a small temporary main)#include <cuda.h>#include nn.hpp#define cudaerrorabort(msg) \\ { \\ cudaerror_t __err = cudagetlasterror(); \\ if (__err != cudasuccess) { \\ fprintf(stderr, [cuda error] %s : %s (%s:%d) , \\ msg, cudageterrorstring(__err), \\ __file__, __line__); \\ exit(1); \\ } \\ }/* first kernel * ============ * neuronal parallelism on the input group, * check if an input neuron should fire or not. */__global__ void check_input(t_neuron *n, int timestep){ int idx = blockidx.x * blockdim.x + threadidx.x; if (timestep >= n[idx].next_time) { n[idx].action_potential += n[idx].in_value; n[idx].next_time += n[idx].in_time; }}/* second kernel * ============= * synaptic parallelism, check pre-synaptic neurons * and create a spike if ap >= threshold */__global__ void check_synapses(t_neuron *n, t_synapse *s, t_spike *sp, int timestep, int scount){ int idx = blockidx.x * blockdim.x + threadidx.x; int j; if (n[s[idx].id_in].action_potential < n[s[idx].id_in].threshold) return ; n[s[idx].id_in].carry = 1; for (int i = idx * spike_buffer; i < idx * spike_buffer + spike_buffer; ++i) { if (sp[i].active) continue; j = i; break ; } sp[j].syn_id = idx; sp[j].id_out = s[idx].id_out; sp[j].start_t = timestep; sp[j].end_t = timestep + s[idx].axonal_delay; sp[j].value = n[s[idx].id_in].action_potential * 0.2f * n[s[idx].id_in].type; sp[j].active = true;}__global__ void carry_reset(t_neuron *n, int ncount){ int idx = blockidx.x * blockdim.x + threadidx.x; /* might be faster depending of the architecture */ // n[idx].action_potential -= (n[idx].carry * n[idx].action_potential); // n[idx].carry = 0; if (n[idx].carry) { n[idx].action_potential = 0.0f; n[idx].carry = 0; }}/* third kernel * ============ * process the multi-circular buffer by batch of spike_buffer, and update * the ap of the post-synaptic neuron (+ kill the spike) if needed */__global__ void check_spikes(t_neuron *n, t_spike *sp, int timestep, int scount){ int idx = blockidx.x * blockdim.x + threadidx.x; for (int i = idx * spike_buffer; i < idx * spike_buffer + spike_buffer; ++i) { if (timestep >= sp[i].end_t) { sp[i].active = false; sp[i].end_t = int_max; // to avoid the 'continue' branching n[sp[i].id_out].action_potential += (sp[i].value * n[sp[i].id_out].weight); if (n[sp[i].id_out].action_potential < 0.0f) n[sp[i].id_out].action_potential = 0.0f; } }}float simulate_network(t_network *nw){ static t_neuron *neur_cptr = null; static t_synapse *syn_cptr = null; static t_spike *spike_cptr = null; static size_t cp_size = sizeof(t_neuron) * ((nw->neur_count + 511) & ~511); static size_t cp_syn_size = sizeof(t_synapse) * ((nw->syn_count + 511) & ~511); static size_t cp_spike_size = sizeof(t_spike) * ((nw->syn_count + 511) & ~511) * spike_buffer; /* add some more space to avoid useless conditions in kernel */ if (!neur_cptr) { /* malloc/cpy for neurons/synapses/spikes */ cudamalloc((void**) &neur_cptr, cp_size); cudamemcpy(neur_cptr, nw->neurons, cp_size, cudamemcpyhosttodevice); cudamalloc((void**) &syn_cptr, cp_syn_size); cudamemcpy(syn_cptr, nw->synapses, cp_syn_size, cudamemcpyhosttodevice); cudamalloc((void**) &spike_cptr, cp_spike_size); cudamemcpy(spike_cptr, nw->spikes, cp_spike_size, cudamemcpyhosttodevice); cudaerrorabort(cuda malloc/memcpy error.); } /* block size/n for kernel 1, 2 and 3 */ int syn_block_size = 512 > nw->syn_count ? nw->syn_count : 512; int syn_block_count = nw->syn_count / syn_block_size + (!(nw->syn_count % syn_block_size) ? 0 : 1); int carry_block_size = 512 > nw->neur_count ? nw->neur_count : 512; int carry_block_count = nw->neur_count / carry_block_size + (!(nw->neur_count % carry_block_size) ? 0 : 1); cudaevent_t start, stop; float elapsed_time; cudaeventcreate(&start); cudaeventrecord(start, 0); /* kernel call */ check_input <<< 1, nw->group_size >>> (neur_cptr, nw->timestep); cudadevicesynchronize(); cudaerrorabort(cuda kernel error 1.); check_synapses <<< syn_block_count, syn_block_size >>> (neur_cptr, syn_cptr, spike_cptr, nw->timestep, nw->syn_count); cudadevicesynchronize(); cudaerrorabort(cuda kernel error 2.); carry_reset <<< carry_block_count, carry_block_size >>> (neur_cptr, nw->neur_count); cudadevicesynchronize(); cudaerrorabort(cuda kernel error 3.); check_spikes <<< syn_block_count, syn_block_size >>> (neur_cptr, spike_cptr, nw->timestep, nw->syn_count); cudadevicesynchronize(); cudaerrorabort(cuda kernel error 4.); /* timer */ cudaeventcreate(&stop); cudaeventrecord(stop, 0); cudaeventsynchronize(stop); cudaeventelapsedtime(&elapsed_time, start, stop); printf([%04d] elapsed time: %f ms , nw->timestep, elapsed_time); cudaeventcreate(&start); cudaeventrecord(start, 0); /* cpy back */ cudamemcpy(nw->neurons, neur_cptr, cp_size, cudamemcpydevicetohost); cudamemcpy(nw->synapses, syn_cptr, cp_syn_size, cudamemcpydevicetohost); cudamemcpy(nw->spikes, spike_cptr, cp_spike_size, cudamemcpydevicetohost); cudaerrorabort(cuda memcpy error.); return (elapsed_time);}int main(void){ t_network *nw; srand(time(null)); if (!(nw = generate_network())) return (1); while (43) { simulate_network(nw); ++nw->timestep; } // return (0);}generate_network.cpp (which initialize my structures)#include <stdlib.h>#include <time.h>#include nn.hppint randab(int a, int b){ return (rand() % (b - a) + a);}double frandab(double a, double b){ return ((rand() / (double)rand_max) * (b - a) + a);}void init_neuron(t_neuron_info *nfo, t_neuron *ret, int id, int x, int y, char type){ nfo->x = x; nfo->y = y; nfo->z = 0; nfo->gid = id / 100; ret->in_value = frandab(0.8f, 3.0f); ret->in_time = randab(50, 100); ret->next_time = ret->in_time * frandab(0.2f, 1.0f); ret->action_potential = 0.0f; ret->threshold = frandab(6.5f, 10.5f); ret->weight = frandab(0.3f, 1.2f); ret->type = type; nfo->group = (nfo->gid == 0 ? input : (nfo->gid == 1 ? output : hidden)); ret->carry = 0;}void init_synapse(t_synapse *ret, int in, int out){ ret->id_in = in; ret->id_out = out; ret->axonal_delay = randab(5, 30);}t_network *generate_network(void){ t_network *ret; if (!(ret = (t_network*)malloc(sizeof(t_network)))) return (null); ret->timestep = 0; ret->group_size = 100; /* alloc */ ret->neur_count = 20000; if (!(ret->neurons = (t_neuron*)malloc(sizeof(t_neuron) * ((ret->neur_count + 511) & ~511))) || !(ret->neurons_info = (t_neuron_info*)malloc(sizeof(t_neuron_info) * ((ret->neur_count + 511) & ~511)))) return (null); ret->syn_count = 400000; if (!(ret->synapses = (t_synapse*)malloc(sizeof(t_synapse) * ((ret->syn_count + 511) & ~511)))) return (null); if (!(ret->spikes = (t_spike*)malloc(sizeof(t_spike) * ((ret->syn_count + 511) & ~511) * spike_buffer))) return (null); /* assign */ for (int i = 0; i < ret->neur_count; ++i) init_neuron(&(ret->neurons_info[i]), &(ret->neurons[i]), i, (i / 12) * 11, (i % 60) * 18, randab(0, 100) >= stimulus_ratio ? inhibition : stimulus); for (int i = 0; i < ret->syn_count; ++i) { int in = randab(0, ret->neur_count); int out = (randab(0, 2) || ret->neurons_info[in].gid == input ? randab(0, ret->neur_count) : randab((int)(in / ret->group_size) * ret->group_size, ((int)(in / ret->group_size) + 1) * ret->group_size)); while (in == out) out = randab(0, ret->neur_count); init_synapse(&(ret->synapses[i]), in, out); } for (int i = 0; i < ret->syn_count * 4; ++i) ret->spikes[i].active = false; return (ret);}everything should run nicely using:nvcc generate_network.cpp simulate_network.cu",
    "present_kp": [
      "cuda"
    ],
    "absent_kp": [
      "c++",
      "neural network"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "encrypt php script to a domain (rsa). i have a php script i want to share with someone but only limit them to be able to use it on one domain.i have found <url> - i think i can do this with rsa encryption but need help.how would i go about encrypting it to one domain?",
    "present_kp": [
      "php",
      "encryption"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how do i delete my ohloh profile?. how do i go about deleting my ohloh profile?i have two ohloh profiles. i want to delete one of them so that the profile that remains may use the email and username of the deleted profile.",
    "present_kp": [
      "profile"
    ],
    "absent_kp": [
      "account management"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "converting a bounded knapsack problem to 0/1 knapsack problem. i originally posted this question at the programmers section of stackexchange (because that section is supposed to deal with data structures and algorithms), but they suggested posting in the math section. read the faq there didn't mention anything about algorithms, but did suggest this section: cs theory. if this should really live in the math section, i'll repost there and delete from here.link to the the original question: <url> ran across a problem where goal was to use dynamic programming (instead of other approaches). there is a distance to be spanned, and a set of cables of different lengths. what is the minimum number of cables needed to span the distance exactly?to me this looked like a knapsack problem, but since there could be multiples of a particular length, it was a bounded knapsack problem, rather than a 0/1 knapsack problem. (treat the value of each item to be its weight.) taking the naive approach (and not caring about the expansion of the search space), the method i used to convert the bounded knapsack problem into a 0/1 knapsack problem, was simply break up the multiples into singles and apply the well-known dynamic programming algorithm. unfortunately, this leads to sub-optimal results.for example, given cables: 1 x 10ft, 1 x 7ft, 1 x 6ft, 5 x 3ft, 6 x 2ft, 7 x 1ftif the target span is 13ft, the dp algorithm picks 7+6 to span the distance. a greedy algorithm would have picked 10+3, but it's a tie for minimum number of cables. the problem arises, when trying to span 15ft. the dp algorithm ended up picking 6+3+3+3 to get 4 cables, while the greedy algorithm correctly picks 10+3+2 for only 3 cables. [i suspect that i just need to tweak my value function, but does the dp algorithm handle value functions that vary depending on what else has been selected? my instincts say no.]anyway, doing some light scanning of converting bounded to 0/1, it seems like the well-known approach to convert multiple items to { p, 2p, 4p ... }. my question is how does this conversion work if p+2p+4p does not add up to the number of multiple items. for example: i have 5 3ft cables. i can't very well add { 3, 2x3, 4x3 } because 3+2x3+4x3 > 5x3. should i add { 3, 4x3 } instead?[i'm currently trying to grok the oregon trail knapsack problem paper, but it currently looks like the approach used there is not dynamic programming.]",
    "present_kp": [
      "dynamic programming"
    ],
    "absent_kp": [
      "ds.algorithms"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "google and random redirects. i have a site where homepage example.com redirects to random joke(example.com/random-joke). first i redirected with php, but then google indexed that joke page(rather to be empty). i used 302 found status code. now i'll echo links to latest jokes and hide it with javascript and redirect with javascript.but this doesn't seem to help..it seems that google still indexes joke page instead a list of jokes provided to homepageit is slower for the visitor to redirect with javascriptso does google detects javascript redirects and is there any good solution for random redirects?",
    "present_kp": [
      "google",
      "php",
      "javascript",
      "redirects"
    ],
    "absent_kp": [
      "seo"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "2 domains, same website in plesk. i have a domain with a ssl at <url>.i want that <url> shows the same website as domain1 (same website folder).i've tried with domain alias in plesk but it's redirecting to <url> ideas please?",
    "present_kp": [
      "plesk"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "why does adsense require sufficient content?. i get that to apply for an adsense account, a website needs to already have sufficient content. as you can guess my website, being brand-new, did not. i am not disputing that fact, so i do not think this is a duplicate with other questions that do.i do however wonder why this is a policy?i guess their reasoning might be because google wants to uphold a sense of quality in the content they support. a practically blank website is not quality content (yet).but isn't this a catch 22? creating quality content takes time (and thus money). without ad-revenue to back this up, it's not easy to justify the expense (to my wife mostly, but that is beside my point :) )?compare with youtube where you can apply for a partner account having uploaded 0 videos.another reason might be that they won't know what ads to show on an empty website, but seems to me they could just as well pick something random, but safe themselves...if anyone has an authoritative answer (based on information from google or personal experience) i'd be happy to hear it.",
    "present_kp": [
      "content"
    ],
    "absent_kp": [
      "google adsense",
      "advertising"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why is the outer div border not dependent on the element's rendered content?. i have a problem i can't figure out. when i nest a div inside another div and i want to give the outer div a border, the border doesn't wrap around the rendered content. even though the content of the inside divs might be several lines high the outside border only adds whatever padding there might be.the css:.wrapper { width:550px; border: 1px solid #ccc; clear:both; padding: 5px; }.content { float:left; width:250px; background-color:#e1e1e1;}the html:<div class=wrapper><div class=content> text<br> text<br> text<br></div><div class=content> pic</div>",
    "present_kp": [
      "css"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "unduplicated counts over various dimensions. i'm working with some data on college students, which has a unique key of ssn, campus, credential (type of degree sought, like bachelor's), progcat (broad subject area, like history), program (specific subject area, like european history), and schoolyear. i want to calculate a lot of statistics (for the sake of this example, we'll just stick with basic counts of number of students) and i want those statistics to be calculated for various combinations of aggregation of campus, program, and program category. in other words, i want to calculate statistics that get as specific as number of students getting a bachelor's degree in european history at big oak university, as general as number of students getting a bachelor's degree in anything at any university and every level in between.now, usually, i'd just use cube in my group by statement, which would give me exactly what i want. however, there's an added complication... i need unduplicated counts of students, and it's possible that one student is enrolled in two different subjects, or at two different universities. so, if jim bob is enrolled both at big oak university and sunrise college, i'd want him to show up once in the statistics for big oak, once in the statistics for sunrise, and once and only once in the statistics for all colleges grouped together. as far as i know, there's no way to accomplish this with cube.i've therefore been pursuing some solutions to deal with this. what i've come up with is a dynamic sql solution. basically, i create a table to hold my output, then i repeatedly cycle through calling the same sql code with different parameters and adding the output to the output table. the parameters control what level of aggregation i'm using. i use a simple subquery with row_number() to pull only one record per student at any level of aggregation, making sure i get unduplicated counts. it looks something like this...if object_id('tempdb..##countout') is not null drop table ##countout;create table ##countout(agency char(3), credential varchar(2), campus varchar(10), progcat varchar(2), program varchar(6), schoolyear smallint, students int, graduates int);go;create procedure ##calcgroup @usecampus bit, @useprogcat bit, @useprogram bitas declare @alllist varchar(max); declare @shortlist varchar(max); declare @sql varchar(max); select @alllist=''; select @shortlist=''; if @usecampus=1 begin select @alllist=@alllist+'campus, '; select @shortlist=@shortlist+'campus, '; end else select @alllist=@alllist+'''all'' as campus, '; if @useprogcat=1 begin select @alllist=@alllist+'progcat, '; select @shortlist=@shortlist+'progcat, '; end else select @alllist=@alllist+'''al'' as progcat, '; if @useprogram=1 begin select @alllist=@alllist+'program, '; select @shortlist=@shortlist+'program, '; end else select @alllist=@alllist+'''all'' as program, '; select @sql=' insert into ##countout(agency, credential, campus, progcat, program, schoolyear, students, graduates) select agency, credential, '+@alllist+'schoolyear, count(*) as students, count(exitquarter) as graduates from ( select agency, credential, '+@shortlist+'schoolyear, exitquarter, row_number() over (partition by agency, credential, '+@shortlist+'schoolyear, ssn order by exitquarter desc) as rn from cfs_esc.dbo.webapp_data ) as inside where rn=1 group by agency, credential, '+@shortlist+'schoolyear; '; exec(@sql);goexecute ##calcgroup @usecampus=1, @useprogcat=1, @useprogram=1;execute ##calcgroup @usecampus=1, @useprogcat=1, @useprogram=0;/* etc. */is there anything you see to improve the clarity or the performance of this code?i'm also wondering if this is the best way to accomplish this task? the only other solution that i have thought of is to create multiple records for every student enrollment. in other words, each student would have a record with their campus, program category, and program specified, a second record with their campus and program category specified and their program set to all, a third record with... etc. that would make the initial data i'm querying over about 6x larger, but i'd only have to run one query to get my results, and i wouldn't have to worry about dynamic sql. do you think that might be a better option?any other solutions out there?i'd appreciate any and all feedback. i have absolutely no formal training in sql, though i've taught myself plenty to accomplish most tasks thrown at me. i'm now just hoping to learn some best practices.",
    "present_kp": [
      "sql"
    ],
    "absent_kp": [
      "sql server",
      "t sql"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why is there considerable difference after applying standardization on the input data?. i am running a analysis on a soccer data set. i applied regression directly on the input data and this is the output i obtained. i then applied the standardization technique on the input features. this the result: i got my required parameter correct in both the cases but i was surprised to see the huge difference in both. some parameters are having negative weights now which had positive ones before and were positive large enough. why is there this statistical difference? and which one depicts the analysis better?",
    "present_kp": [
      "regression"
    ],
    "absent_kp": [
      "machine learning",
      "data mining",
      "feature scaling"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "which process/program create/write the file which i/o is redirected to?. when learning selinux, i executed semanage fcontext -l > fcontext.txt under /root directory to dump massive fcontext information to a file. the funny result is: fcontext.txt was created, but the size of it is 0.i asked in #fedora-selinux irc channel, a fedora qa told me try change the type of selinux context of fcontext.txt to semanage_tmp_t. it works finally.but what still confusing me is:which process/program create/write the standard output to the redirected file ? (so that selinux targetd policy should applied to that process/program)the bash shell ? /bin/bashor semanage (it's a python script) ? /usr/sbin/semanageor the interpreter - python ? /usr/bin/pythoni thought it should be the bash shell, because i/o redirection is controlled by shell (right?), not the program itself. so semanage/python didn't create the file directly.",
    "present_kp": [
      "shell",
      "fedora",
      "selinux"
    ],
    "absent_kp": [
      "io redirection"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "fedora auto suspend. i am using fedora 13 and for shutting down and rebooting automatically, we have the following commands: # shutdown -h/-r now similarly, if i want to make my system go into suspend mode after sometime, what command should i use?",
    "present_kp": [
      "fedora",
      "suspend"
    ],
    "absent_kp": [
      "command line"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "counting pairs of numbers that differ by a given amount. i am trying to do these online competitions where you get the given exercise and are supposed to upload the source file.the site is not in english so i will try to summarize the exercise.the first line of input states p, the number of problems ( 1 <= p <= 100 ). subsequently, there are two lines for each problem. the first contains numbers x and y ( 1 <= x, y <= 100000) x being number of elements and y being the difference i am looking for. on the next line there are x numbers (0 <= n <= 100000) divided by a space. for every problem, i am supposed to print the number of pairs that differ by y.as an example input :25 21 5 3 4 25 46 3 6 2 2example output:34i wrote a simple programme that did the thing but when i uploaded the source file i got time limit exceeded error. after that i discovered that my program is unable to handle big input. i tried to move on to the next exercise but i discovered almost all of them have big inputs. i tried to improve it myself, then i tried to google it, but sadly i still cannot get past the error. this is the code i ended up with :#include <stdio.h>#include <stdlib.h>#include <string.h>#define size 80000int compare(const void *a, const void *b);int main(int argc, char const *argv[]) { char buffer[size]; unsigned int numberofgames, numberofstudents, wanteddifference; unsigned int numberofdifferentpairs; unsigned int counter; size_t i, j; scanf(%u, &numberofgames); while (numberofgames--) { scanf(%u %u, &numberofstudents, &wanteddifference); scanf( ); unsigned long long int *studentsheights = (unsigned long long int *)malloc(sizeof(unsigned long long int) * numberofstudents); counter = 0; numberofdifferentpairs = 0; studentsheights[counter] = 0; while (counter < numberofstudents) { fgets(buffer, size, stdin); size_t stringsize = strlen(buffer); for (i = 0; i < stringsize; i++) { if (buffer[i] == ' ' || buffer[i] == eof || buffer[i] == ' ') { counter++; if (counter < numberofstudents) studentsheights[counter] = 0; } else { studentsheights[counter] = studentsheights[counter] * 10 + (buffer[i] - '0'); } } } qsort(studentsheights, numberofstudents, sizeof(unsigned int), compare); for (i = 0; i < numberofstudents - 1; i++) { unsigned int lastaddition; if (studentsheights[i] == studentsheights[i - 1]) { numberofdifferentpairs += lastaddition; continue; } else { lastaddition = 0; } for (j = i + 1; j < numberofstudents; j++) { if ((studentsheights[j] - studentsheights[i]) == wanteddifference) { numberofdifferentpairs++; lastaddition++; } } } printf(%u , numberofdifferentpairs); } return 0;}int compare(const void *a, const void *b) { if (*(unsigned int *)a < *(unsigned int *)b) return -1; else if (*(unsigned int *)a == *(unsigned int *)b) return 0; else return 1;}i know i am probably doing some trivial mistake but if you could point it out for me i would be really grateful.there are the variables i used:p = numberofgamesx = numberofstudentsy = wanteddifferencen = studentsheights",
    "present_kp": [
      "c",
      "time limit exceeded"
    ],
    "absent_kp": [
      "programming challenge"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is the benefit of git's two-stage commit process (staging)?. i'm learning git and i've noticed that it has a two-step commit process:git add <files>git committhe first step places revisions into what's called a staging area or index.what i'm interested in is why this design decision is made, and what its benefits are?also, as a git user do you do this or just use git commit -a?i ask this as i come from bzr (bazaar) which does not have this feature.",
    "present_kp": [
      "git"
    ],
    "absent_kp": [
      "version control",
      "workflows"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "semidecidability/decidability for strings seperated by a non alphabet symbol. i am trying to prove the following:let $\\sigma $ be an alphabet not containing the symbol $;$, and suppose that$l \\subseteq \\sigma^*$; $\\sigma^*$ is recursively enumerable. if this is the case language $l' = \\{x \\in \\sigma^*: x; y \\in l \\ for \\ some \\ y \\in \\sigma^*\\}$ is recursively enumerable.which left me a little bit confused. since $;$ is not in the alphabet $\\sigma$ should i seperate $x$ and $y$ and show that check for $x \\in \\sigma^*$ and $y \\in \\sigma^*$ is recursively enumerable?also, will this statement be the case if we checked for being recursive instead, that is if $l$ was recursive, is $l'$ necessarily recursive as well? and is the proof of this the same with the recursively enumerable case?any help will be appreciated.",
    "present_kp": [],
    "absent_kp": [
      "turing machines",
      "undecidability",
      "semi decidability"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "comparison of assessments of cognitive skills in children. what test is better in terms of its usability and informativeness - wisc-iv (wechsler intelligence scale for children) or bas (british ability scales)?",
    "present_kp": [
      "test"
    ],
    "absent_kp": [
      "cognitive psychology",
      "measurement",
      "developmental psychology",
      "educational psychology"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "optimal way to output a dictionary. finishedsomeone helped me see that i was framenting the output by writing to each file inside the for loop instead i needed to have one big for loop for each file so that the harddrivedidnt have to move its head everytimehttps://stackoverflow.com/questions/<phone>/outputing-dictionary-optimallyi have 4 dictionarys that contain 800k strings with 200 to 6000 characters.when i load it into memory it takes up about 11 gigs of memory.it is taking me 2 minutes to parse the data and 2 minutes to output the data.is there anyway to output the data faster than what i am using below?i am only getting 20-31 mb per second disk io and i know the hard drive can do 800ishvar hash1 = new dictionary<int, dictionary<string, string>>(f.count + 2);var hash2 = new dictionary<int, dictionary<string, string>>(f.count + 2);var hash3 = new dictionary<int, dictionary<string, string>>(f.count + 2);var hash4 = new dictionary<int, dictionary<string, string>>(f.count + 2);...foreach (var me in mswithfilenames){ filename = me.key.tostring(); string filenamef = filename + index1; string filenameq = filename + index2; string filenamefq = filename + index3; string filenameqq = filename + index4; streamwriter sw = file.appendtext(filenamef); streamwriter sw2 = file.appendtext(filenameq); streamwriter swq = file.appendtext(filenamefq); streamwriter sw2q = file.appendtext(filenameqq); for (i = 0; i <= totalinhash; i++) { if (hashs1[i].containskey(filenamef)) { sw.write(hashs1[i][filenamef]); } if (hashs2[i].containskey(filenameq)) { sw2.write(hashs2[i][filenameq]); } if (hashs3[i].containskey(filenamefastaq)) { swq.write(hash4[i][filenamefastaq]); } if (hash4[i].containskey(filenameqq)) { sw2q.write(hash4[i][filenameqq]); } } sw.close(); sw2.close(); sw3.close(); sw4.close(); swq.close(); sw2q.close();}",
    "present_kp": [],
    "absent_kp": [
      "c#",
      "performance"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "does a file move operation over samba preserve the inode number?. if i have a directory exposed over a samba share and a user, from either a linux or windows system, moves a file from one place to another on that share, does samba know to ask the operating system for a move operation and keep the inode number?similarly does renaming a file on a samba share keep the inode number consistent?bonus question that might need to be its own question: i use rsync to create differencing backups within the server (centos 7) from the device to a different backup device. can the differencing backup track inode numbers on the device and the backup, to itself correctly track the file move/rename?",
    "present_kp": [
      "rsync",
      "backup",
      "samba",
      "inode"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "which wikis have markdown support?. possible duplicate:which content management system (cms) should i use? there exist a markdown extension for mediawiki, but it is very buggy.does anyone know of wikis that support markdown?",
    "present_kp": [
      "wiki",
      "markdown"
    ],
    "absent_kp": [
      "looking for a script"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is it possible to backdate a facebook post without the little clock icon indicating that you've done so?. i have one set of people saying that a page did not pre-date a facebook post, and another set of people saying that they did.as best as i understand it, it is not possible to pre-date a facebook post without a little clock icon showing up. when you hover your mouse over the clock, it shows you the original (actual) date that the post was created.changing the date is only possible on the desktop (both on personal and fan pages). however, on a mobile device you can still see this little clock. although you cannot click or hover over it, it at least gives the user an indication that the post was moved.here is a post that does not have a clock icon. (it's on a page, not a group, meaning it's impossible to be anything but public).is there anyway to hack facebook, such that you can change the date of (or simply create) a post back in time but suppressing the clock icon? if this is possible, then it essentially gives people a way to create fraudulent posts. and in this particular case, determining if this is possible turns out to be very important.",
    "present_kp": [
      "facebook"
    ],
    "absent_kp": [
      "facebook pages",
      "facebook groups",
      "facebook timeline"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "reads data from file. calculates on percentage and shows a table. so this code is not yet completed. the end result has to calculate the average grade by each question. a=1 b=2 etc... and checks if the question is passed or not. this is what i got this far. #include <stdio.h>#define row 6#define colomn 5int input[row][colomn] , row, column;int main(void){float passedtests , returnedtests , percentage = (passedtest/returnedtest*100);char filepath[100]= ;//----- openen van extern bestand door middel van input van user.file *inputfile;printf( );printf(enter path of file. );scanf(%s, &filepathpath);inputfile=fopen(filepath, r ); if (inputfile == null){ printf(file not found.); }fscanf(inputfile, %f, &passedtests);fscanf(inputfile, %f, &returnedtests); for (row=0; row<row; row++) { for (colomn=0; column<column ; colomn++) { fscanf(inputfile, %d, &input[row][colomn]); } }printf(passed tests: %0.f , passedtests);printf(returned tests: %0.f , returnedtests);printf(response is: %.1f %% , percentage);printf( a b c d e grade comment );for (row=0; row<row; row++) {if (colomn==(colomn-1)) {printf(question %d ,row+1);} if (colomn==colomn) {printf( );} for (colomn=0; colomn<colomn ; colomn++) { printf(%d , input[row][colomn]); } }}the output has to be like something like this: number of passed tests: 20number of returned tests: 19response is 95%. a b c d e grade passedquestion 1 2 2 2 2 2 2.4 yesquestion 2 2 2 2 2 2 2.4 yesquestion 3 2 2 2 2 2 2.4 yesquestion 4 2 2 2 2 2 2.4 yesquestion 5 2 2 2 2 2 2.4 yesquestion 6 2 2 2 2 2 2.4 yesso how far am i ? i have made everything until calculating the average. anyone got a good idea on how to do this ? also i think the response variable is not good. please check it out thank you !btw the guys @ stackoverflow said to post code review here. the input file is like this: 201912457018738542022411023356004123",
    "present_kp": [
      "c"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "is there a difference in resource allocation & efficiency between a web app and a compiled one?. i'm a web developer. i feel like i develop less resource-intensive functionality often times, because i have this feeling that if i ask too much of the web app (animation, calculation, connections, etc), it will get lagged for users with slower computers. i've been curious for a while now about just how much difference there is in the capability (resource allocation) of a web (interpreted) app vs a compiled one. also, i've heard that interpreted (js, for example) programs are very inefficient and resource-hungry in comparison to compiled ones (c++, for example). google's v8 js engine is said to make js faster, but i still see people talk about js being much less efficient than c++.the reason that i ask this question is to separate fact from biased opinion. is there a difference in available cpu & ram between a web app and a compiled one? is there a large efficiency difference between a program written in (an interpreted language) js versus one written in (a compiled language) c++?edit: i realize that we are comparing apples and oranges here. there are some really well-defined aspects of choosing between c++ (anything compiled) and javascript (web) when developing a client application, and this is one side aspect of that decision that i would not want to be misinformed about if it were to come up at any point in my career.",
    "present_kp": [
      "efficiency"
    ],
    "absent_kp": [
      "web development",
      "web applications",
      "applications",
      "browser"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "path work with relative directories. i'm trying to get a program to work that isn't in my distro's package manager. it doesn't provide a makefile or sources, just binaries. if i copy the whole folder to /usr/local and then add that to my path in .bashrc, i can execute the program but there are problems. the program requires some local files from its directory, and they are stored as a relative path. the program looks for the files in the directory i'm currently in, not the directory that the program itself is in. is there any way around this? thanks.",
    "present_kp": [
      "directory",
      "path"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "saving and restoring radixtree object. i have a large word-file which is fixed with over 240 000 words. i need to check if a specific word exists in this list. i thought that it would be a good idea to create a radixtree for this problem. the text file consists of 240 000 lines, each of which contains one word, which is read into a string. each string is then parsed into a char[] to access the individual characters, each char is then placed into the tree separately. android puts a limit of 16-26mb of ram usage per application if i am correct. when my application runs it consistently hits this mark during the initialization of the large vocabulary object. it takes around 47 seconds to initialize the vocabulary object and about 1/10 of a ms to see if a word exists in this vocabulary. right now i am afraid that i made a mistake by implementing the radixtree data-structure since it costs too much ram and takes way too long with 47 seconds...would this every be possible to use on a real device? e.g. that the speed would be under 0.5 seconds instead of 47.my idea was to load the object to later restore it. however this is even slower, loading an object took about 5x longer. oh dear. vocabulary classpackage nl.mprog.ghost.datastructure;import android.app.activity;import android.content.context;import android.util.log;import java.io.serializable;import nl.mprog.ghost.enumeration.language;public class vocabulary implements serializable { radixtree mradixtree; language mlanguage; public vocabulary(context context, language language) { this.mlanguage = language; mradixtree = new radixtree(context, language); } public boolean isword(string word) { return mradixtree.isword(word); }radixtree classpackage nl.mprog.ghost.datastructure;import android.content.context;import android.util.log;import java.io.serializable;import java.util.arraylist;import java.util.scanner;import nl.mprog.ghost.r;import nl.mprog.ghost.enumeration.language;public class radixtree implements serializable { private radixtreenode mroot = new radixtreenode(); private arraylist<radixtreenode> mactivenodes; public radixtree(context context, language language) { readinwords(getfilescanner(context, language)); } public void readinwords(scanner scanner) { string word; while(scanner.hasnextline()) { word = scanner.nextline(); mroot.insertword(word, mroot); } scanner.close(); } public scanner getfilescanner(context context, language language) { scanner scanner; switch (language) { case dutch: scanner = new scanner(context.getresources().openrawresource(r.raw.dutch)); break; case english: scanner = new scanner(context.getresources().openrawresource(r.raw.english)); break; default: scanner = new scanner(context.getresources().openrawresource(r.raw.english)); break; } return scanner; } public boolean isword(string word) { radixtreenode activenode = mroot; char[] chars = word.tochararray(); for(int i = 0; i < chars.length; i++) { activenode = activenode.findnode(activenode, chars[i]); if(activenode == null) return false; } if(activenode.miswordend) { return true; } else { return false; } }}radixtreenode classimport android.util.log;import java.io.serializable;public class radixtreenode implements serializable { radixtreenode[] mchildren = new radixtreenode[0]; char mcharacter = '0'; boolean miswordend; public radixtreenode() { } public radixtreenode(char character) { this.mcharacter = character; } public radixtreenode(char character, boolean iswordend) { this.mcharacter = character; this.miswordend = iswordend; } public void insert(char character) { for(radixtreenode node : mchildren) { if(node.mcharacter == character) { return; } } addchild(new radixtreenode(character)); } public void insert(char character, boolean iswordend) { for(radixtreenode node : mchildren) { if(node.mcharacter == character) { return; } } addchild(new radixtreenode(character, iswordend)); } private void addchild(radixtreenode node) { if(mchildren.length == 0) { mchildren = new radixtreenode[1]; mchildren[0] = node; } else { radixtreenode[] temp = new radixtreenode[mchildren.length + 1]; system.arraycopy(mchildren, 0, temp, 0, mchildren.length); temp[mchildren.length] = node; mchildren = temp; } } public radixtreenode[] getchildren() { return mchildren; } public void insertword(string word, radixtreenode root) { radixtreenode activenode = root; char[] array = word.tochararray(); for(int i = 0; i < (array.length - 1); i++) { activenode.insert(array[i]); activenode = findnode(activenode, array[i]); } activenode.insert(array[array.length - 1], true); } public radixtreenode findnode(radixtreenode startposition, char character) { radixtreenode[] children = startposition.getchildren(); if(children.length == 0) { return null; } else { for(radixtreenode rtn : children) { if(rtn.mcharacter == character) return rtn; } } return null; }}",
    "present_kp": [
      "java",
      "android"
    ],
    "absent_kp": [
      "performance",
      "memory management",
      "serialization"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is it harder to get a post to rank for a specific keyword if you already have a post which ranks highly for it?. say i've written two articles: one on interesting plot twists and another on how to write a plot twist. if the interesting plot twist post already ranks quite highly for the keyword 'how to write a plot twist', does it make it less likely that the other post will get a high ranking for that keyword too. does google prefer a diversity of domains in its serps, meaning that your own posts can block your more relevant posts from rising to the top?",
    "present_kp": [],
    "absent_kp": [
      "seo"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "resource for understanding the kernel and drivers. possible duplicate:linux kernel: good beginners' tutorial what resources are available if i wanted to develop, change and understand device drivers.",
    "present_kp": [
      "linux",
      "drivers"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how long before legacy 302s are recognised as 301s?. we have some legacy 302 redirects in place on our site that should have always been 301s. we've now changed the redirects to 301s but various seo tools (and presumably google) still see the 302 redirect rather than the 301. my question is, how long do 302s normally get cached for (if there is a 'normally' in this situation)? and is there anything we can do to speed up the process of recognising the change to 301s?",
    "present_kp": [
      "seo",
      "cache",
      "302 redirect"
    ],
    "absent_kp": [
      "301 redirect"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "pulseaudio: clear preferences saved by plugin module-stream-restore. i am trying to find a way for pulseaudio to clear all the saved associations between input/output streams and sources/sinks, which get saved by the plugin module-stream-restore.i want to do this to only happen at login.as far as i could find, the easiest way would be to load the plugin module-stream-restore with restore_device=false, and it does work. however, it creates a major inconvenient if one wants to change a stream's sink to one different than the default, especially if that stream gets interrupted and recreated often (such as with browser videos, for example), because it causes the stream to lose the setting and fallback to the default sink, which is annoying. thus, i am looking for a way, if possible, to manually clear these saved associations (at my convenience).",
    "present_kp": [
      "pulseaudio"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "graphical logviewer that auto reload when file changes?. i'm wondering if there's a graphical viewer, also it needs to support auto-reloading when log file has changed.ksystemlog doesn't reload automatically.",
    "present_kp": [],
    "absent_kp": [
      "logs",
      "gui",
      "tail"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is the motivation behind typescript?. we had javascript, then we had flash, then we had silverlight and then html5 ownd them all. so what is the motivation behind typescript? what problems are going to be tackled and what improvements do we get with typescript?<url>",
    "present_kp": [
      "javascript",
      "typescript"
    ],
    "absent_kp": [
      "web development"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "tridiagonal solver in python. i have a code i'm working on that involves solving a 1d schrodinger equation using a crank-nicolson time step. the code is written in numpy/scipy, and i was doing a bit of profiling and discovered that the bulk of the simulation time is taken up with calls to the linear solver (spsolve in this case). i'm using a simple discretization such that my matrices are symmetric and tridiagonal, and i was wondering if anyone had tried doing things like implementing the thomas algorithm directly and if this was able to beat spsolve.",
    "present_kp": [
      "python",
      "linear solver",
      "scipy"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "multi-dimensional knapsack with added one-per-group constraint. i have some sets (a, b, ...) each containing some vectors (e.g. [1, 0, -1]). how can i pick exactly one vector per set such that they sum to a specified target vector?for example:a: { [1, 0, -1], [3, 0, 0], ... }b: { [0, -1, -1], [2, -1, 0], ... }target: [3, -1, -1]solutions:1) a=[1, 0, -1], b=[2, -1, 0]2) a=[3, 0, 0], b=[0, -1, -1]i think this is a variation of the multi-dimensional knapsack problem with the added 'one-per set' constraint. has this been written about anywhere?for my domain, there are 30 sets each containing 50 vectors with 30 dimensions. these vectors are sparse with ~80% zeroes. there is at most one negative number per vector.any pointers to research papers or proposed algorithms would be hugely appreciated.thanks.",
    "present_kp": [
      "algorithms"
    ],
    "absent_kp": [
      "complexity theory",
      "knapsack problems"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "meal plan for the week. so i made this program, where you can add a recipe which is saved as a json and you can get a plan for your meals and a grocerylist back.program that help you to decide what to eat for the week you should start adding some recipies and ingredience you need for the recipie. if you have at least 7 of them they will be used to tell you what you could cook and give you a grocery list keywords are add_recipe and get_mealsimport sysimport getoptimport jsonimport globimport randomclass usage(exception): def __init__(self, msg): self.msg = msgdef main(argv=none): takes the keywords add_recipe and get_meals and calles the underlying functions. if argv is none: argv = sys.argv try: try: opts, args = getopt.getopt(argv[1:], hamg, [help, add get]) except getopt.error as msg: raise usage(msg) for o, args in opts: print(args) if o in (-h, --help): print('''valid options ase -a to add a recipe and -m to get meals and -g to get meals and grocerielist ''') return 1 elif o in (-a): save_recipe(add_recipe()) elif o in (-m): recipes = get_recipes() weekly_meals = return_shuffled_max_seven(recipes) print_meals(weekly_meals) elif o in (-g): recipes = get_recipes() weekly_meals = return_shuffled_max_seven(recipes) print_meals(weekly_meals) print_grocerylist(weekly_meals) else: raise usage(no valid option) except usage as err: print(sys.stderr, err.msg) print(sys.stderr, for help use --help) return 2def add_recipe(): asks the user interactivly to input a recipe name and the ingredience returns ------- a dictionary with the recipe name and the list of ingedience example ------- {'applepie':['apple','pie']} print(name of the recipe ) name = input(> ) recipe = [] while true: print(add an ingredience or finalize the recipe with 'q') ingredience = input(> ) if ingredience in 'q': break else: recipe.append(ingredience) return {name: recipe}def save_recipe(recipe): saves the recipe to a json file #get the first key from the dictionary: name of the recipe name = next(iter(recipe.keys())) with open(name+'.recipe', 'w') as f: json.dump(recipe, f, indent=1)def get_recipes(): gets all recipes and returns them as one list grand_list_of_recipes = [] for recipe in glob.glob(*.recipe): with open(recipe, 'r') as f: grand_list_of_recipes.append(json.load(f)) return grand_list_of_recipesdef return_shuffled_max_seven(a_list): returns 7 items from a list random.shuffle(a_list) return a_list[:7]def print_meals(weekly_meals): prints a weekplan for meals week=[] for meal in weekly_meals: name = next(iter(meal.keys())) week.append(name) if len(week) < 7: print(you only have +str(len(week))+ recipes) for i in range(7-len(week)): week.append('') print('mondays:') print(' '+str(week[0])+' ') print('tuesdays:') print(' '+str(week[1])+' ') print('wednesday:') print(' '+str(week[2])+' ') print('thursday:') print(' '+str(week[3])+' ') print('friday:') print(' '+str(week[4])+' ') print('saturday:') print(' '+str(week[5])+' ') print('sunday:') print(' '+str(week[6])+' ')def print_grocerylist(weekly_meals): prints the grocersylist meals=[] for meal in weekly_meals: name = next(iter(meal.keys())) meals.append(name) recipe_list=[] for idx, meal in enumerate(meals): recipe_list.append(weekly_meals[idx][meal]) grocery_list=[] for sublists in recipe_list: grocery_list += sublists print(grocery list: ) grocery_list = list(set(grocery_list)) for item in grocery_list: print( * +item+ )if __name__ == '__main__': sys.exit(main())",
    "present_kp": [],
    "absent_kp": [
      "python",
      "beginner"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "support vector regression trained with data sets. i am now searching for a long time on the internet and on papers for an answers of simple questions. am i able to train a support vector regression algorithm with different data sets? if yes, how is the approach called?i have 10 times the same battery with different usage, temperature and capacity.usage and temperature are features (x_i,i) and capacity is the output (y_i,i).battery_1 till timepoint n: [x_1,1 y_1,1; ... ;x_1,n y_1,n]...battery_10 till timepoint n: [x_10,1 y_10,1; ... ;x_10,n y_10,n]now i want to train my svr with these sets, where the samples within a set belong together. i want give the algorithm a set of usage and temperature where i don't know the capacity and my svr should predict it as such : x_d --> y_d.thank you very much for your help and input.",
    "present_kp": [],
    "absent_kp": [
      "machine learning",
      "predictive modeling",
      "svm"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "determining big oh time complexity. i am practicing questions in which i need to determine the big oh time complexity for a given piece of code. i have attempted the first 3 questions. could anyone please confirm if the answers are correct?also, i am not sure how to calculat the big oh time complexity of the code in the 4th question. could someone please explain how to calculate it?thank you1:sum, i = 0, 0 while i**2 < n: sum = sum + i i += 2o(n^(1/2))2.i, j, sum =1,1,0 while i < n**3: while j < n: sum=sum+ i j += 1 i=i+no(n^3)3.i, sum = 0, 0 while i *2 < n: sum = sum + i i=i+1o(log(n))4.i, sum =0,0 while i < 3 * n: if i %2 == 0: j = 1 while j < n: sum = sum + j j=j*2 else : j = 1 while j < n: sum = sum * j j += (n // 5) i=i+5",
    "present_kp": [],
    "absent_kp": [
      "asymptotics"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "filling the gap between conception and unit test. i am stuck trying to do tdd. i'd rather seek the counsel of others who have gone before rather than waste time with trial and error. question:what diagrams / exercises / development processes can i insert between foggy notion of what system is supposed to do (aka conception) and write test?anything else included in this question is simply additional information to help communicate / clarify my question; not to change the question in any way. i do not care what diagrams / exercises / development processes i use, as long as they get me unstuck.when i say conception i just mean a foggy informal mental-only notion of what a system should / can / will / might do.form of answer:i'm trying to navigate from conception to unit test in my development process. i'm looking for a way, a path, a road map, a bridge. an answer might look something like this:do a xyxyx type diagram to get the aaaas, then take all the aaaas and do a wywywyw diagram, then all the ccccs on the wywywyw diagram will be the classes you need to unit test and the lllllls will be the scenarios of the tests.problem background:i found a tdd flowchart and converted it to a google drawing. i'm using nunit. i have studied tdd, know what it is and practiced it somewhat. this flow chart is for tdd only; no steps before unit testing are included.the flowchart starts out with write test. i had my concept, nothing i had read said i needed anything else first, so i tried following the flow chart. it was too much of a leap to go from foggy notion to unit test. this caused me designer's block. then i read someone say just do it! i tried that, but the resulting tests and ensuing code wandered somewhat aimlessly since there was nothing to guide what tests to write.attempts at solution:some ideas i have explored already that seem helpful (but the question is not about these things):bdd (i used specflow)use case diagramsspecflow starts with a verbal description of a feature and creates tests that nunit can run; they don't look like the unit tests i created manually but maybe i'll figure out how they relate to one another.use cases helped me get convert the initial foggy notion into a formal description, which was a bit of sunshine. i used visual studio 2013 for that. i don't see any facility (in visual studio at least) to convert my use case into unit tests, although i found some evidence that this is one purpose of a use case. quoting from the uml user guide, 2nd ed., p.246:a use case diagram can be forward engineered to form tests for the element to which it applies.and it looks like visual studio maybe wants to convert it because it can store template data on the use case and has code generation settingshere is a picture of what i have tried so far",
    "present_kp": [
      "design",
      "unit testing",
      "tdd"
    ],
    "absent_kp": [
      "c#"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "determining efi text modes supported by notebook. i am using a lenovo ideapad in freebsd 12.0i need to setup a different efi text mode, to change the screen resolution upon boot. as in this question using native 1366 x 768 resolution in a lenovo ideapadhow do i get a listing of the different modes supported by my particular notebook?",
    "present_kp": [
      "freebsd"
    ],
    "absent_kp": [
      "uefi"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why is amazon written with strange characters in this url?. when i do a search on amazon france, one of the url parameters is m, as in the url below:<url> idea why they have this strange parameter there?",
    "present_kp": [
      "amazon",
      "url"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "testing the property of being a union of three disjoint cliques. design an $\\epsilon$-test for the following property in the dense graph model: $g(v,e)$ is a union of three disjoint cliques.i've been sitting for a few hours and i don't have any idea of how to solve this one ... anyone has any ideas / leads?i have tried to pick the vertices randomly and then conclude from the graph they make g' what it says about g , i've divided into cases by connectivity : if the connectivity is 4 or more it is easy to conclude that the graph do not has the property . i have struggled with the math in the other cases...sorry for not posting my progress (i am new)",
    "present_kp": [],
    "absent_kp": [
      "graph theory"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "unmet dependencies using apt-get on ubuntu. i am working on a ubuntu 12.04.2 lts (gnu/linux 3.5.0-43-generic x86_64) machine. it is a server running in production environment with other projects so i have to be quite careful.i have to install a certain list of programs, so first i run the usual apt-get update and then i proceeded to run the following command:apt-get install python-dev libxslt1-dev libpq-dev git python-pip nginx supervisor wget libxml2-dev mongodb build-essential libxslt1-dev libxrender1 libxext6'this is what i get:root@serverlinux:~# apt-get install python-dev libxslt1-dev libpq-dev git python-pip nginx supervisor wget libxml2-dev mongodb build-essential libxslt1-dev libxrender1 libxext6reading package lists... donebuilding dependency tree reading state information... donegit is already the newest version.python-pip is already the newest version.build-essential is already the newest version.libxext6 is already the newest version.libxrender1 is already the newest version.libxrender1 set to manually installed.python-dev is already the newest version.you might want to run 'apt-get -f install' to correct these:the following packages have unmet dependencies: libpq-dev : depends: libpq5 (= 9.1.14-0ubuntu0.12.04) but 9.1.9-0ubuntu12.04 is to be installed depends: libkrb5-dev but it is not going to be installed depends: comerr-dev but it is not going to be installed libxml2-dev : depends: libxml2 (= 2.7.8.dfsg-5.1ubuntu4.9) but 2.7.8.dfsg-5.1ubuntu4.6 is to be installed linux-image-generic-lts-quantal : depends: linux-image-3.5.0-44-generic but it is not going to be installed mongodb : depends: mongodb-server but it is not going to be installed depends: mongodb-dev but it is not going to be installed nginx : depends: nginx-full but it is not going to be installed or nginx-light but it is not going to be installed supervisor : depends: python-medusa (>= 0.5.4) but it is not going to be installed depends: python-meld3 but it is not going to be installed depends: python-support (>= 0.90.0) but it is not going to be installede: unmet dependencies. try 'apt-get -f install' with no packages (or specify a solution).what should i do to manage to install the packages with unmet dependencies? how can i meet those dependencies? i am afraid to use the -f option as suggested because it might just mess everything up.please bear with me, i just want to be extra cautious before taking any risk in this server.",
    "present_kp": [
      "ubuntu",
      "apt"
    ],
    "absent_kp": [
      "software installation"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "c++ file handling, inserts character while copying contents. [please don't comment on using turbo c++. i know that it's obsolete but we are taught this way only.]#include<fstream.h>#include<conio.h>void main(){ clrscr(); char ch; ifstream read; read.open(employee.txt); ofstream write; write.open(another.txt); while(!read.eof()) { read.get(ch); //also when i use, write<<read.get(ch) is it writes some kind of write<<ch; //address in the file. please tell me about that too why it happens. } read.close(); write.close(); getch();}the problem i'm facing is that it appends character at the end of 'another' file.",
    "present_kp": [
      "c++",
      "file"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "proving np-complete help. i am trying to prove that the problem of having a person at the minimum x number of intersections to be able to see each street is np-complete. i think that the street problem is very similar to the vertex set problem, and to prove it is np-complete i could some form of reduction like 3-sat to vertex set but with my problem in place, to verify the requirement that it is np-hard (i can prove it is np). i'm having an extreme amount of trouble getting started with this however. i've looked many examples of reduction but cannot think how to start my own. how would i show the reduction from 3-sat to the street problem/vertex set?",
    "present_kp": [],
    "absent_kp": [
      "complexity theory",
      "reductions",
      "np hard",
      "3 sat"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "are clean coding rules less relevant for large open source projects?. i've been reading robert martin's book clean code. one of his core tenants is to remove unnecessary comments and strive to create meaningful variable/method names that are self documenting.some of my colleagues disagree with this approach arguing that it is impractical for large software projects. they cite the example of jquery's codebase which is littered with comments, unclear variable names like fn and sometimes even commented out code, e.g. jquery/deferred.jstheir conclusion is that open source code can't really follow clean coding principles because people want line by line explanations for why a particular approach was used and so liberal comments are a necessity. they also argue that the long descriptive method names advocated by uncle bob are harder to read than a short name with a descriptive function comment.do you think this conclusion is true? if not, do you have examples of vendor codebases that follow clean code principles faithfully?codebases i've examinedjquery:see aboveangular js:example: angular.js- lots of comments that sometimes seem disconnected with context- very long methods not following the extract method patternreact jsexample: reactcomponent.js- not much vertical spacing to separate code blocks- relatively long methods not following extract method pattern+ comments relatively rare and generally used to explain obtuse cases that would not be obvious",
    "present_kp": [
      "jquery",
      "clean code"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "can googlebot handle robots.txt with a 302 redirect?. in google webmaster tools, i get an error: google couldn't crawl your site because we were unable to access your site's robots.txt filethe associated help says: if your robots.txt file exists but is unreachable (in other words, if it doesnt return a 200 or 404 http status code), well postpone our crawl rather than risk crawling disallowed urlsmy site 302 redirects all http traffic to https - so access to http://blah/robots.txt is 302 redirected to https://blah/robots.txt, so literally it doesn't return 200 or 404 as requested above.my question - does googlebot object to the 302 redirect when it attempts to access the robots.txt file?note: a lot of the server configuration is out of my control, and is configured this way due to corporate it. i'm just the poor guy who needs to get this working in spite of the constraints.",
    "present_kp": [
      "robots.txt",
      "googlebot"
    ],
    "absent_kp": [
      "google search console"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "distribute output through fifo. i'm trying to distribute very long make output using following methodmkfifo myfifo make 2>&1 | tee myfifo | grep -e errors|warnings myfifo > errors.log | cat myfifoidea is that subset of output should be copied to log files, while whole output would be redirected to stdout of console.it seem to work at first, but suddenly stops and stays frozen until i press ctrl-z. a use of process substitutionmake 2>&1 | tee >(grep -e errors|warnings > errors.log)is not possible, because shell compiled with default strict posix compliance.platform - ubuntu-flavored distro. shell bash 4.2 (and only one available)",
    "present_kp": [
      "bash",
      "grep",
      "fifo"
    ],
    "absent_kp": [
      "pipe"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to write bash script without using sudo in it?. how can i write a gem install or apt-get install in bash without letting it use sudo?#!/usr/bin/env bashapt-get update -yapt-get upgrade -ygpg --keyserver hkp://keys.gnupg.net --recv-keys 409b6b1796c275462a1703113804bb82d39dc0e3\\curl -ssl <url> | bash -s stable --rubysource /usr/local/rvm/scripts/rvm((euid)) || exitgem install jekyll",
    "present_kp": [
      "bash"
    ],
    "absent_kp": [
      "shell script"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "aggregation of a collection of object's nested array-properties. use caseso our lead programmer loves to follow the nih anti-pattern and consequently i'm not allowed to use underscore.js and can only use vanilla js... and we have to support ie8.the goal is to produce an aggregation of unique values from nested collections buried within various collections of objects.desired interfacethe idea is to call a function to create an aggregation function give the list of the nested properties.examplethe following would flatten all options collection that are buried under each object within the properties property.var alluniqueoptions = aggregatebyproperty('properties', 'options');var alluniquedoodadsoptions = aggregatebyproperty('doodad', 'properties', 'options');alluniqueoptions(mycollection);alluniquedoodadsoptions(mydoodadcollection);current codethe code below works, but i'd like any advice on improving it for:performancereadabilityany further generalizationi have a jsfiddle created./* returns the index of an object within a collection */function arrayobjectindexof(arr, obj) { var search = json.stringify(obj); for ( var i = 0, k = arr.length; i < k; i++ ){ if (json.stringify(arr[i]) == search) { return i; } }; return -1;};/* concatentates objects that don't exist in the source collection */function concatifnotexists(source, additions) { var result = source; for ( var i = 0, k = additions.length; i < k; i++ ) { var addition = additions[i]; if (arrayobjectindexof(result, addition) === -1) { result.push(addition); } }; return result;};/* returns a nested collection buried within an object */function getinnercollection(obj, props) { var innercollection = obj; for (var j = 0, l = props.length; j < l; j++) { if (innercollection.hasownproperty(props[j])) { innercollection = innercollection[props[j]]; } else { throw new typeerror(property doesn't exist!); } } if (!innercollection instanceof array) { throw new typeerror(inner property is not a collection!); } return innercollection;};/* returns an aggregation of objects based on collection properties */function aggregatebyproperty() { var props = array.prototype.splice.call(arguments, 0); return function(objectcollection) { var result = []; for (var i = 0, k = objectcollection.length; i < k; i++) { var innercollection = getinnercollection(objectcollection[i], props); result = concatifnotexists(result, innercollection); } return result; }};",
    "present_kp": [],
    "absent_kp": [
      "javascript",
      "functional programming"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "calculate intervals. how to explicit the intervals between numerical values in a file? (better if an awk one-liner)input:a 1-3b 5-7outputa 1,2,3b 5,6,7code i've tried so far: sed -i 's/-/ /g' input #splits the digits to separated columnsawk '{print$1 $3-$2+1}' input > output #prints key and interval widthbut, thid only allows computing the number of values within the interval. for example, the output will look like:a 3b 3",
    "present_kp": [
      "awk"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "determine hash method - busybox linux. i'm trying to figure out how an arm-based device running busybox linux is storing hashes for users in a configuration file. basically, the system allows you to backup the entire configuration to a file. within this file i can see the hashes for one of the default accounts - admin.if i configure the admin password to be test, i get this hash in the output file:3785a9c61b587f2ff0dd7132db844f14being a 32 character hash, i suspect it's md5. i know from testing that if i configure one of the other built-in accounts to have the same password, the hash is different. so it seems that whatever the hashing algorithm, it probably does something like concatenate the username and password together before hashing. i've tried a bunch of combinations, and am thinking i might just have to try brute force. seeing as i know the username and password, i suppose i could do a custom dictionary attack or something.any suggestions ?",
    "present_kp": [
      "linux"
    ],
    "absent_kp": [
      "hash functions"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "sed with shell script giving error. i am trying to create a script to add an indent to a file. so i put this in my file called ind--sed 's/^./ &/' $*when i run it my command is ind lines and it is supposed to open file lines and print it with an indent. instead it gives an error no command ind found did i mean and then it gives 15 suggestions and then last line reads ind command not found. i am using putty and i just run it ind lines. i created script in via. can someone point out what is wrong and why it won't indent please??",
    "present_kp": [],
    "absent_kp": [
      "linux"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how can i add network interface to lxc container. i have a question about network interfaces in lxc container:in my container,i have by default these interfaces:ubuntu@u5:~$ ifconfigeth0 link encap:ethernet hwaddr 00:16:3e:b7:de:91 inet addr:10.0.3.138 bcast:10.0.3.255 mask:255.255.255.0 inet6 addr: fe80::216:3eff:feb7:de91/64 scope:link up broadcast running multicast mtu:1500 metric:1 rx packets:56 errors:0 dropped:0 overruns:0 frame:0 tx packets:40 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 rx bytes:7230 (7.2 kb) tx bytes:3500 (3.5 kb) lo link encap:local loopback inet addr:127.0.0.1 mask:255.0.0.0 inet6 addr: ::1/128 scope:host up loopback running mtu:65536 metric:1 rx packets:0 errors:0 dropped:0 overruns:0 frame:0 tx packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 rx bytes:0 (0.0 b) tx bytes:0 (0.0 b)i'd like to add this new interface: auto eth1 iface eth1 inet static address 192.168.1.3 netmask 255.255.255.0 network 192.168.1.1 broadcast 192.168.1.255so,i have modified this file: /etc/network/interfaces # this file describes the network interfaces available on your system # and how to activate them. for more information, see interfaces(5). # the loopback network interface auto lo iface lo inet loopback auto eth0 iface eth0 inet dhcp auto eth1 iface eth1 inet static address 192.168.1.3 netmask 255.255.255.0 network 192.168.1.1 broadcast 192.168.1.255i have done reboot but it didn't work ! when i use ifconfig, i cant' find the new interface: ubuntu@u5:/etc/network$ ifconfig eth0 link encap:ethernet hwaddr 00:16:3e:b7:de:91 inet addr:10.0.3.138 bcast:10.0.3.255 mask:255.255.255.0 inet6 addr: fe80::216:3eff:feb7:de91/64 scope:link up broadcast running multicast mtu:1500 metric:1 rx packets:57 errors:0 dropped:0 overruns:0 frame:0 tx packets:40 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 rx bytes:7337 (7.3 kb) tx bytes:3500 (3.5 kb) lo link encap:local loopback inet addr:127.0.0.1 mask:255.0.0.0 inet6 addr: ::1/128 scope:host up loopback running mtu:65536 metric:1 rx packets:0 errors:0 dropped:0 overruns:0 frame:0 tx packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 rx bytes:0 (0.0 b) tx bytes:0 (0.0 b)have you an idea please ?",
    "present_kp": [
      "lxc"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "ubuntu 14.04 mouse pointer does not move. i'm using ubuntu 14.04, after updating the unity doesn't load. the sidebar and topbar disappear. on the main screen, there are only the background and the pointer. i've solved this problem by entering these commandssudo apt-get updatesudo apt-get install --reinstall ubuntu-desktopsudo apt-get install unitysudo apt-get remove --purge nvidia*sudo shutdown -r nowafter rebooting, the side bar does appear, however now the mouse pointer disappears. i make the pointer visible by entering this command:gsettings set org.gnome.settings-daemon.plugins.cursor active falsethe mouse pointer now appears but it does not move (it freezes). can someone guide me how to solve this problem ?",
    "present_kp": [
      "ubuntu"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "is writing code chronologically the best way for readability?. so i've been writing a lot of javascript lately. despite what many people say i think javascript can be a simple and beautiful language. what i've pondered of late is how to structure the code so it is best readable to the human eye.sometimes there are cases like this:mainfunction = function(){ var init = function(){ // do some initialization dosomething(); doanotherthing(); window.addeventlistener(scroll, somecallback); } var somecallback = function() {} var dosomething = function() { // code } var doanotherthing = function() { // code } init();}you first have to define all functions before you can initialize some thing and finally do the stuff you wanted to do.now when i'm reading code, i always try to read it in a chronological order first, that's just how my brain works. but with javascript, the longer the code gets, the more complex i find to see where the start point is.so i guess my questions are:are most people reading code chronologically? is chronological the best way for readability?how to achieve this in javascript? (what are some languages that do it better)what are some common javascript code writing philosophies out there?",
    "present_kp": [
      "javascript"
    ],
    "absent_kp": [
      "coding style",
      "coding standards",
      "clean code"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "direct execution of python scripts. i have noticed that sometimes python scripts are not being started directly, ie /foo/bar.py, but rather from a shell script, which does nothing else than /usr/bin/python -o /foo/bar.py $@one such example is wicd network manager. /usr/bin/wicd-gtk is a shell script, which starts the wicd-client.py:$ cat /usr/bin/wicd-gtkexec /usr/bin/python -o /usr/share/wicd/gtk/wicd-client.py $@what is the purpose of this extra step?what would be the difference if i started /usr/share/wicd/gtk/wicd-client.py directly (provided it is executable) ?",
    "present_kp": [
      "shell script",
      "python",
      "executable",
      "exec"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "the interrupt timeline for a single process doing output. i'm studying the book 'operating system concepts' 9th edition.in the first chapter, part 1.2.1 computer system operation, i can't understand the figure 1.3:can any one make a quick interpretation on this for me? especially about the peaks of this graph?",
    "present_kp": [
      "io",
      "interrupt"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "ansi escapes in pash prompt are displayed as garbage in gnome terminal. i would like to use pash, a powershell implementation for mono on my centos box.after compilation i started it and got$ mono source/pashconsole/bin/debug/pash.exe[%?%p1%{8}%<%t37%e%p1%{16}%<%t9%p1%{8}%-%d%e38;5;7%;m[%?%p1%{8}%<%t40%e%p1%{16}%<%t10%p1%{8}%-%d%e48;5;0%;mpash - copyright (c) pash contributors. license: gpl/bsd. see <url>}%<%t37%e%p1%{16}%<%t9%p1%{8}%-%d%e38;5;7%;m[%?%p1%{8}%<%t40%e%p1%{16}%<%t10%p1%{8}%-%d%e48;5;0%;m[%?%p1%{8}%<%t37%e%p1%{16}%<%t9%p1%{8}%-%d%e38;5;7%;m[%?%p1%{8}%<%t40%e%p1%{16}%<%t10%p1%{8}%-%d%e48;5;0%;mpash /home/gbuday/projects/pash> [%?%p1%{8}%<%t37%e%p1%{16}%<%t9%p1%{8}%-%d%e38;5;7%;m[%?%p1%{8}%<%t40%e%p1%{16}%<%t10%p1%{8}%-%d%e48;5;0%;m i was told that my terminal does not understand ansi sequences.i use gnome terminal. is it capable of this with some configuration trick?or, i need to use some other terminal emulator software?",
    "present_kp": [
      "terminal",
      "prompt"
    ],
    "absent_kp": [
      "escape characters"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "remove (text based) linux logo that comes after booting. i ported petalinux on zynq board by xilinx. i am having a log message as shown below: .....[wed feb 12 14:55:44.<phone>] hash algo: sha1[wed feb 12 14:55:44.<phone>] hash value: 266e39ed71a93229a26f0cf7e9f5317b64c2e407[wed feb 12 14:55:44.<phone>] verifying hash integrity ... crc32+ sha1+ ok[wed feb 12 14:55:44.<phone>] booting using the fdt blob at 0x14bafcc[wed feb 12 14:55:44.<phone>] uncompressing kernel image ... ok[wed feb 12 14:55:44.<phone>] loading device tree to 07ffa000, end 07fffb98 ... ok[wed feb 12 14:55:44.<phone>] [wed feb 12 14:55:44.<phone>] starting kernel ...[wed feb 12 14:55:44.<phone>] [wed feb 12 14:55:45.<phone>] init: version 2.88 booting[wed feb 12 14:55:45.<phone>] starting bootlog daemon: bootlogd.[wed feb 12 14:55:45.<phone>] creating /dev/flash/* device nodes[wed feb 12 14:55:45.<phone>] configuring network interfaces... ifconfig: socket: address family not supported by protocol[wed feb 12 14:55:46.<phone>] ifconfig: socket: address family not supported by protocol[wed feb 12 14:55:46.<phone>] starting busybox inet daemon: inetd... done.[wed feb 12 14:55:46.<phone>] starting uweb server:[wed feb 12 14:55:46.<phone>] petalinux <error> (httpd): failed to create server socket: address family not supported by protocol[wed feb 12 14:55:46.<phone>] init: entering runlevel: 5[wed feb 12 14:55:46.<phone>] stopping bootlog daemon: bootlogd.[wed feb 12 14:55:47.<phone>] [wed feb 12 14:55:47.<phone>] _____ _ _ _[wed feb 12 14:55:47.<phone>] | ___ \\ | | | | (_)[wed feb 12 14:55:47.<phone>] | |_/ / ___ | |_ __ _ | | _ _ __ _ _ __ __[wed feb 12 14:55:47.<phone>] | __/ / _ \\| __| / _' || | | || '_ \\ | | | |\\ \\/ /[wed feb 12 14:55:47.<phone>] | | | __/| |_ | (_| || |____| || | | || |_| | > <[wed feb 12 14:55:47.<phone>] \\_| \\___| \\__| \\__,_|\\_____/|_||_| |_| \\__,_|/_/\\_\\[wed feb 12 14:55:47.<phone>] [wed feb 12 14:55:47.<phone>] petalinux v2013.10 (yocto 1.4) xilinx-zc702-14_7 ttyps0[wed feb 12 14:55:47.<phone>] [wed feb 12 14:55:47.<phone>] xilinx-zc702-14_7 login: rootat time stamp [wed feb 12 14:55:47.<phone>], there is a big size logo of petalinux. i have seen such logos on many flavors including angstrom linux. how do we remove such logos, which file is to be edited?",
    "present_kp": [
      "linux",
      "boot"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "cant delete 2 partitions (fat32 and ext3) on toshiba usb flash disk 16g. i had tried fdisk command to delete it and dd command to overwrite that 2 partitions but that 2 partitions still exist. any suggestion to delete that 2 partitions on my toshiba usb flash disk 16 gb? it's hardware problem or not.because it took time to mount it",
    "present_kp": [],
    "absent_kp": [
      "linux"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "finding all (weighted) cycles through a given vertex. for a connected undirected graph $g$, given a particular vertex $v$, is there a known (efficient) algorithm to find all simple cycles in $g$ that contain $v$?in my case, i have weights for every edge. ideally i'd like to be able to compute, for every $v \\in v(g)$, the product of the edge weights over all cycles that go through $v$. doing this to some level of approximation would still be very helpful.is this a known result? any pointers would be very helpful.",
    "present_kp": [],
    "absent_kp": [
      "algorithms",
      "graph theory"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is there a lightweight method to describe web services, similar to robots.txt or sitemap.xml?. i'd like to provide a very lightweight method to describe my web services. no wsdl, wadl or similar complex stuff. just a mapping from the web service's base url to a simple description which can be detected and parsed automatically. basicallythe name of the servicea link to some human readable documentation (optional)possibly a contact addressusage conditions, url patterns etc. are less needed to keep it simple.the best i could find is opensearch description documents, but first it could be simpler with plain text config files, and second there is no standard how to connect these service description documents to arbitrary web services (only autodiscovery in atom/rss and html).i thought about creating my own format and provide it similar to robots.txt:$ curl <url> <url> foobar apiabout: <url> <url> example org's unapi# additional fields, to be discussedstandard: <url> ?id={id}&format={format}so if you know the base url of a service, such as <url> you can look up at least the name in the service description file. instead of defining my own standard, however, i'd like to know whether there is already something similar to reuse.",
    "present_kp": [
      "documentation",
      "web services"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "dual booting on hp laptop seems impossible due to hd partitioning. i'd like to run debian on my laptop in dual boot with win7.i started shrinking my c drive, freeing 150gb to give to debian, but when installing and arriving at the point in which i have to choose where to put the os, the installer list me the partition as unusuable.i read that's because we are only allowed to have 4 partitions on our hd and hp has already claimed all of them:any suggestions?if there are no solution i'll stick with this, i already run debian and ubuntu on my desktop pc so it's not a big deal, mostly a challenging problem that i'm trying to solve in those winter holidays.",
    "present_kp": [
      "partition",
      "dual boot"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "is there a way to automatically exclude folders when adding a google doc to a trello card?. recently, when attaching a google doc to a trello card, i get the list of all folders and files in my google docs. and it lists the folders at the top. there are so many folders, in fact (maybe 40 or 50) that i don't see any filespreviously it would show me a list of files, sorted with most recent first.that meant that 95% of the time, the file i wanted to attach was in the first few documents.if i manually type -folder everytime, then it shows no folders.i'm wondering if there is a setting to automated this either in gdrive or in trello",
    "present_kp": [
      "trello"
    ],
    "absent_kp": [
      "google drive"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how does google prevent ga accounts from being spammed. since the google analytics snippet is available in the source code of the page (e.g. below is stackexchange's) <script type=text/javascript>var _gaq=_gaq||[];_gaq.push(['_setaccount','ua-<phone>']); _gaq.push(['_setdomainname','.stackexchange.com']); _gaq.push(['_setcustomvar', 2, 'accountid', '148108',2]); _gaq.push(['_trackpageview']); var _qevents = _qevents || []; (function(){ var s=document.getelementsbytagname('script')[0]; var ga=document.createelement('script'); ga.type='text/javascript'; ga.async=true; ga.src='<url> s.parentnode.insertbefore(ga,s); var sc=document.createelement('script'); sc.type='text/javascript'; sc.async=true; sc.src='<url> s.parentnode.insertbefore(sc,s); })(); </script>i was wondering how google were preventing people's ga accounts from being spammed. for instance i could modify my /etc/hosts file to point webmasters.stackexchange.com to a server of mine, then on that server i create few dummy webpages with the above ga code, and i could use an automated browser like selenium to continuously browse these pages and execute the ga code which would generate plenty of requests from my browser to ga.how does google prevent people's ga account statistics from being spammed?",
    "present_kp": [
      "google analytics",
      "spam"
    ],
    "absent_kp": [
      "spam prevention",
      "hacking",
      "google analytics spam"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "data flow in dependency injection. what we havea complex tree model stored across multiple xml filesjava model classes which represent the xml modelall classes as pojosall classes annotated with jaxbjaxb to read the xml filesa tree of handler classeseach handler is responsible for handling a layer inside the model tree or specific elementsthe handler classes are created using guice as di frameworkproblematic bitsome handlers need data that is processed in layers that were parsed earlier.example:roothandler\\- childhandler \\- grandchildhandlerin this example the grandchildhandler needs the data which the roothandler computed. at the moment the code looks similar to this:# roothandler:data rootdata = compute();childhandler.handlelayer(rootdata);# childhandler:data childdata = compute();grandchildhandler.handlelayer(rootdata, childdata);questionhow should we handle this trickle down of data in the handler tree?is it acceptable that the methods call 'explode' the further we go down in the tree?i was also thinking about storing the child data in singletons, but i fear that it will be unintuitive if this data changes depending on each computation from a childhandler.the eclipse4 dependency framework supports context sensitive injection. do you think it would be good to create a context object which contains all the passed down data and every handler just grabs what he needs from there?another idea we had was to separate localized data and global data and then pass the global through different methods. my fear here is that the outside has to guarantee that the most recent global data is set.is there some standard solution for this? all dependency injections tutorials just ignore bigger projects :(",
    "present_kp": [
      "dependency injection"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "test 2 time ranges to see if they overlap. i am doing a project for school and had to create a function to check if 2 time ranges overlap. i searched the net a bit but failed to find a simple function to do that. i played around a bit with my code and i think i created a working one.note: it works for 24 hour times only & don't forget the 0 in front of am time e.g 08:15if anyone has a better function to do this or finds some bugs in the code below - please post below.here is the code you can test directly on.$start_time1 = 10:15;$end_time1 = 12:20;$start_time2 = 13:15;$end_time2 = 14:25;function testrange($start_time1,$end_time1,$start_time2,$end_time2){ $timecheck; if(($end_time1 < $start_time2)) { $timecheck = true; return $timecheck; } else if(($start_time1 > $start_time2) && ($start_time1 > $end_time2)) { $timecheck = true; return $timecheck; } else { $timecheck = false; return $timecheck; }}if(testrange($start_time1,$end_time1,$start_time2,$end_time2)){ echo not in range;}else{ echo in range;}",
    "present_kp": [],
    "absent_kp": [
      "php",
      "interval"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "anti spam temporary e-mail service. please list some web apps that allows you either tocreate temporarily forwarding addresses (like mailexspire)use anonymous inboxes (like mailinator)as always with such lists: one service per answer.",
    "present_kp": [],
    "absent_kp": [
      "webapp rec",
      "email",
      "spam prevention"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why should viewmodel route actions to controller when using the mvcvm pattern?. when reading examples across the internet (including the msdn reference) i have found that code examples are all doing the following type of thing:public class fooviewmodel : baseviewmodel { public fooviewmodel(foocontroller controller) { controller = controller; } protected foocontroller controller { get; private set; } public void performsuperaction() { // this just routes action to controller... controller.superaction(); } ...}and then for the view:public class fooview : baseview { ... private void onsuperbuttonclicked() { viewmodel.performsuperaction(); }}why do we not just do the following?public class fooview : baseview { ... private void onsuperbuttonclicked() { viewmodel.controller.superaction(); // or, even just use a shortcut property: controller.superaction(); }}",
    "present_kp": [
      "mvc"
    ],
    "absent_kp": [
      "design",
      "design patterns",
      "object oriented design",
      "mvvm"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "brackets in distributive law?. the (second) distributive law in boolean algebra is defined as$a + (b c) = (a + b) (a + c)$but wouldn't it be correct to define it that way:$(a + (b c)\\, ) = (a + b) (a + c)$because if you transform$(\\sim d + \\sim c) (\\sim d + b + a) (c + \\sim b + \\sim a)$to$\\sim d + (\\sim c (b + a) \\, ) (c + \\sim b + \\sim a)$it would be incorrect since * has higher precedence than +, so $(\\sim d + (\\sim c (b + a)\\, )\\, ) (c + \\sim b + \\sim a)$would be correct, or am i totally mistaken?",
    "present_kp": [
      "boolean algebra"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "devise an algorithm of complexity o(n) finding no of 1's from a 2 dimensional matrix[n][n] containing 1's and 0's. assume a 2d [n][n] matrix of 1's and 0's. all the 1's in any row should come before 0's. the number of 1's in any row i should be at least the number of 1's row (i+1). find a method and write a c program to count the number of 1's in a 2d matrix. the complexity of the algorithm should be order n.the question is from cormen's algorithm book. kindly point out the mistakes in my algorithm and hopefully suggest a better way. #include<stdio.h> #include<stdlib.h> int **map; int getmatrix(); main() { int n,i,j,t; j=0; n=getmatrix(); i=n-1; int sum[n]; for(t=0;t<n;t++) sum[t]=0;int count=0; while ( (i>=0) && (j<n) ) { if ( map[i][j] == 1 ) { j++; count=count+1; } else { if (i==(n-1)) { sum[i]==count; count=0; } else { sum[i]=sum[i+1]+count; count=0; i--; } } } for (t=0;t<n;t++) { if ((t==(n-1)) && (sum[t]==0)) sum[t]=0; else if ((sum[t]==0) && (sum[t+1]>0)) sum[t]=sum[t+1]; } int s=0; for (t=0;t<n;t++) s=s+sum[t]; printf( the no of 1's in the given matrix is %d ,s); } int getmatrix() { file *input=fopen(matrix.txt,r); char c; int nver=0,i,j; while((c=getc(input))!=' ') if(c>='0' && c<='9') nver++; map=malloc(nver*sizeof(int*)); rewind(input); for(i=0;i<nver;i++) { map[i]=malloc(nver*sizeof(int)); for(j=0;j<nver;j++) { do { c=getc(input); }while(!(c>='0' && c<='9')); map[i][j]=c-'0';} } fclose(input); return nver; }",
    "present_kp": [
      "algorithm",
      "c",
      "matrix"
    ],
    "absent_kp": [
      "programming challenge"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "unification functions. i need to apply the unification function to unify the following expression:foo(x,x,y) and foo(z,p(z),w)so far, i've determined that i must substitute 'w's for occurances of 'y', makingfoo(x,x,w) and foo(z,p(z),w)then, i can saw x/z (replaces 'z' with 'x'), making:foo(x,x,w) and foo(x,p(x),w)now all that's left is making p(x) equivalent to xmy question here is, is it allowed to replace this p(x) with x using the substitution x/p(x)? or am i going about the question in the wrong pattern?thank you!",
    "present_kp": [
      "unification"
    ],
    "absent_kp": [
      "artificial intelligence"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is it possible to start python thread from inside vim under hp-ux?. i've compiled python 2.7.10 and vim 7.4 with python support (hp-ux 11.31 ia64).everything seems to be working well except for threads. here's simple example.from threading import threadfrom time import sleepdef threaded_function(arg): for i in range(arg): print i sleep(1)def start_thread(): thread = thread(target = threaded_function, args=(5, )) thread.start()if __name__ == '__main__': start_thread()python interpreter executes it perfectly well.% python thread.py01234but vim produces an error when i'm trying to source it (enclosed in python << eof / eof brackets):traceback (most recent call last): file <string>, line 14, in <module> file <string>, line 11, in start_thread file /home/users/zemtsoe/local/lib/python2.7/threading.py, line 745, in start _start_new_thread(self.__bootstrap, ())thread.error: can't start new threadafaik python threads are available (despite vim thread-unsafety).also i'm unable to use ycm plugin because of this error.what can i do?",
    "present_kp": [],
    "absent_kp": [
      "vimscript python"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "ubuntu - booting black screen with purple border. after the installation of ubuntu, when i try to boot a black screen with a purple box appears without let me see the grub menu. then the screen becomes black and ubuntu starts booting. ubuntu is the only os installed, but it's quite annoying as i can't see the bootloader options. what could be wrong?",
    "present_kp": [
      "ubuntu",
      "boot"
    ],
    "absent_kp": [
      "linux",
      "grub2"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "overwriting a c system after exec?. so i used wrap to override the malloc call in my program, i didn't think that if i exec'd another it would work, and it didn't. i figure this is because it wasn't linked with my program. how could i overwrite a c call in another program?sample:void* __wrap_malloc(size_t size) { void* mal = __real_malloc(size); printf(malloced %i @ %i , size, mal); return mal;}int main(int argc, char *argv[]) { malloc(1024); execl(/bin/bash, /bin/bash, (char *) null); return 0;}",
    "present_kp": [
      "c"
    ],
    "absent_kp": [
      "linux"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "read input with startup script finnix 110. i am in the process of setting up a liveusb that will run some scripts on a machine when it is booted. the main os on the usb is finnix 110, and i am setting up startup scripts in the /indep/rc directory. so far, this has worked for the most part, but anytime that i want to get user input, the script just skips over the section where it is supposed to wait for input:#!/bin/bashecho -n [select] /dev/:read sdaclearecho /dev/$sda selectedon startup, the read command doesn't wait for user input, and just continues with sda as a blank variable, rather than giving the user a typing prompt.when i run ./cdrom/finnix/arch/indep/rc/test.sh the script runs fine and waits for the user's input. i'm not completely sure why it just skips this section when run as a startup script.is there any way that i can make this script run after startup is completed, like i do manually with the ./path/to/test.sh command? or even, is there an alternate way that i can collect user input when it is run in the /rc/ directory at startup? i've been working on this issue for a couple days using various articles, but none of which have been able to get this working properly.",
    "present_kp": [
      "startup",
      "input"
    ],
    "absent_kp": [
      "scripting"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "preconditioning symmetric schur complement. consider a $2 imes 2$ block matrix and a linear system of equations associated to it:egin{equation} egin{pmatrix} - a & b \\ b^t & c \\end{pmatrix} egin{pmatrix} x \\ y \\end{pmatrix} = egin{pmatrix} \\phi \\ \\psi \\end{pmatrix}\\end{equation}assume that $a$ and $c$ are symmetric, and symmetric positive semi-definite, and that $a$ is even invertible. one can construct the schur complement system$(c + b^t a^{-1} b ) y = \\psi - b^t a^{-1} \\phi$where $s = c + b^t a^{-1} b$ is symmetric positive-definite, by assumption.how do you precondition such a system in general? i have noted that preconditioners for $s$ are derived from block matrix preconditioners for the original block matrix. is there a general consensus how such a block matrix preconditioner looks like?",
    "present_kp": [
      "preconditioning"
    ],
    "absent_kp": [
      "block decomposition"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "running a software (python script with gui) as soon as possible after boot. i'm working on an embedded computer, which will have no keyboard / no mouse, but only a touchscreen (a kiosk).i would like to start, as soon as possible on boot, without any message / splash logo, and without a login prompt, the python script with its gui in fullscreen. (my python script uses wxpython for gui).how should i configure my debian 8 or ubuntu server for this?should i use remove the existing desktop manager (probably unity for ubuntu?) should i then install nodm, lightdm or something else? then i thought about creating a systemd service that would start my python myscript.py. is that the correct way to go?",
    "present_kp": [
      "debian",
      "desktop",
      "kiosk"
    ],
    "absent_kp": [
      "x11",
      "desktop environment"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "which are the priority online mediums for local businesses? is google adwords appropriate for local business marketing?. we received a promotion offer from google adwords to tempt us to try the adwords service (spend at least 25 in the next few months and get 75 credit). but this is a small local business that is only interested in drawing traffic from the surrounding area so only wants to market to people within driving range.it has been suggested (by someone with more experience of online marketing than i have) that google adwords is not really a suitable marketing tool for small local businesses looking to draw in traffic only from their immediate area. they indicated adwords is a tool that should only be used by those looking to market nationally or statewide, and they suggested that for small local business, forget adwords and put the same marketing effort/spend into improving/driving google my business instead.are they correct? is google adwords pointless for local marketing?",
    "present_kp": [
      "google adwords",
      "marketing"
    ],
    "absent_kp": [
      "local seo",
      "google local search",
      "google places"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to join two csv with same number of rows. i am new to linux and i have two csv files which are sorted and having the same number of rows. i wanted to join these two csv files.1.csv1,2,34,5,67,8,92.csv10,11,1213,14,1516,17,18result.csv1,2,3,10,11,124,5,6,13,14,157,8,9,16,17,18i tried with cat 1.csv 2.csv >result.csv but it appends instead of joining.",
    "present_kp": [
      "linux",
      "csv"
    ],
    "absent_kp": [
      "awk",
      "sed"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "really slow wordpress website admin panel with a total query time 1664ms for the home page. i have a problem with my wordpress website being too slow, i installed a debug bar to check queries in the home page, it reports 202 queries, and a total query time equal to 1665ms.the cpu and the ram are in critical use.here is what i tried to do:optimize plugindb to defrag the db.themecheck plugin to analyse and clean my template.w3c total cache plugin with object cachr enabled.necessary plugins are:wordpress seo by yoastjetpack service by wordpress.comcontact form 7thank you",
    "present_kp": [
      "wordpress",
      "cpu"
    ],
    "absent_kp": [
      "mysql"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how do i check if a given parameter is in an given array?. i have a shell script script.sh like this:names=( jack john jerry)and i want the user to type any of these three names as its first parameter, just like:./script.sh jackif the user typed a wrong one, for example./script.sh kateit will trigger a exit and ask the user to only type one from those 3.what should i do?",
    "present_kp": [
      "shell"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "simple image gallery. i have a simple gallery that hides and shows images. it works fine, however, i am not satisfied with my approach, and my code seems redundant. can you check my code and give a better idea on how i can improve it?html<div class=big_img_wrapper> <img src=<?= imagepath ?>front_page/phinfo/house_rentals/westgate/big_img_1.jpg id=big_img_1 class=big_img> <img src=<?= imagepath ?>front_page/phinfo/house_rentals/westgate/big_img_2.jpg id=big_img_2 class=big_img> <img src=<?= imagepath ?>front_page/phinfo/house_rentals/westgate/big_img_3.jpg id=big_img_3 class=big_img> <img src=<?= imagepath ?>front_page/phinfo/house_rentals/westgate/big_img_4.jpg id=big_img_4 class=big_img> <img src=<?= imagepath ?>front_page/phinfo/house_rentals/westgate/big_img_5.jpg id=big_img_5 class=big_img> <img src=<?= imagepath ?>front_page/phinfo/house_rentals/westgate/big_img_6.jpg id=big_img_6 class=big_img> </div> <div class=thumbs_img_wrapper> <img src=<?= imagepath ?>front_page/phinfo/house_rentals/westgate/thumbnails/thumbs_img_1.jpg id=thumbs_img_1 calss=thumbs_img> <img src=<?= imagepath ?>front_page/phinfo/house_rentals/westgate/thumbnails/thumbs_img_2.jpg id=thumbs_img_2 calss=thumbs_img> <img src=<?= imagepath ?>front_page/phinfo/house_rentals/westgate/thumbnails/thumbs_img_3.jpg id=thumbs_img_3 calss=thumbs_img> <img src=<?= imagepath ?>front_page/phinfo/house_rentals/westgate/thumbnails/thumbs_img_4.jpg id=thumbs_img_4 calss=thumbs_img> <img src=<?= imagepath ?>front_page/phinfo/house_rentals/westgate/thumbnails/thumbs_img_5.jpg id=thumbs_img_5 calss=thumbs_img> <img src=<?= imagepath ?>front_page/phinfo/house_rentals/westgate/thumbnails/thumbs_img_6.jpg id=thumbs_img_6 calss=thumbs_img> </div>css.big_img_wrapper, .big_img_wrapper img{ width: 370px; height: 246px; /*display: none;*/ } .thumbs_img_wrapper{ padding:0; } .thumbs_img_wrapper img{ width: 111px; height: 70px; margin: 14px 0 0 14px; } #thumbs_img_1, #thumbs_img_4{ margin: 14px 0 0 0; }jquery$(document).ready(function(){ $('img.big_img').hide(); // hides all big images $('img#big_img_1').fadein('slow'); // serve as default image $('img#thumbs_img_1').click(function(){ $('img.big_img').hide(); // hides all big images $('img#big_img_1').fadein('slow'); //slowly fades in selected image }); $('img#thumbs_img_2').click(function(){ $('img.big_img').hide(); // hides all big images $('img#big_img_2').fadein('slow'); //slowly fades in selected image }); $('img#thumbs_img_3').click(function(){ $('img.big_img').hide(); // hides all big images $('img#big_img_3').fadein('slow'); //slowly fades in selected image }); $('img#thumbs_img_4').click(function(){ $('img.big_img').hide(); // hides all big images $('img#big_img_4').fadein('slow'); //slowly fades in selected image }); $('img#thumbs_img_5').click(function(){ $('img.big_img').hide(); // hides all big images $('img#big_img_5').fadein('slow'); //slowly fades in selected image }); $('img#thumbs_img_6').click(function(){ $('img.big_img').hide(); // hides all big images $('img#big_img_6').fadein('slow'); //slowly fades in selected image });});",
    "present_kp": [
      "jquery",
      "html",
      "css",
      "image"
    ],
    "absent_kp": [
      "javascript"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "uml diagram for ssis process. i'm trying to do some technical documentation and would like to include some diagrams to better explain the processes in the system.the process i'm currently trying to describe is the following:user uses ftp to put a file on some folderssis packages sees if there is files to be processedfor each file it validates the format and creates a record in dbfor each line, does some validations and contacts some services to get additional infoafter getting all information, it creates line records in dbmoves original file to ok or err foldersprocess finishespretty standard process used in many situations. my only doubt is what kind of diagram is best for this case? doesn't have to be very formal but it has to describe the process in a understandable way. also, i would prefer to have every step in a single diagram. any suggestions? thanks in advanceps: i'm not very experienced in formal uml diagrams. probably that is the issue here :) but if you can point out some nice examples, that would be excellent!",
    "present_kp": [
      "documentation",
      "uml",
      "diagrams",
      "ssis"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "controlling drone with ios speech recognition. i got a parrot mambo which comes with an sdk for creating your own app for controlling it.i decided to use the ios speech recognition library so i could use basic voice commands to control it. the majority of the ios sdk that comes with the drone is written in pure c, so it has to be used in objective-c (something i have little previous experience in using). any advice on how to clean this up, or if i'm using best practices are appreciated....@property (nonatomic, strong) iboutlet uibutton *startlisteningbutton;@property (nonatomic, strong) iboutlet uibutton *stoplisteningbutton;@property (nonatomic, strong) iboutlet uitextview *transcribedtextview;@property (nonatomic, strong) sfspeechrecognizer *speechrecogniser;@property (nonatomic, strong) sfspeechaudiobufferrecognitionrequest *speechrecognitionrequest;@property (nonatomic, strong) sfspeechrecognitiontask *speechrecognitiontask;@property (nonatomic, strong) avaudioengine *audioengine;...- (ibaction)startlistening:(id)sender { [_startlisteningbutton setenabled:yes]; [_stoplisteningbutton setenabled:no]; [self startsession];}- (ibaction)stoplistening:(id)sender { if ([_audioengine isrunning]) { [_audioengine stop]; [_speechrecognitionrequest endaudio]; [_startlisteningbutton setenabled:yes]; [_stoplisteningbutton setenabled:no]; }}- (void) startsession{ if (_speechrecognitiontask != nil) { [_speechrecognitiontask cancel]; _speechrecognitiontask = nil; } avaudiosession *audiosession = [avaudiosession sharedinstance]; [audiosession setcategory:avaudiosessioncategoryrecord error: nil]; _speechrecognitionrequest = [[sfspeechaudiobufferrecognitionrequest alloc] init]; _speechrecognitionrequest.shouldreportpartialresults = yes; avaudioinputnode *inputnode = [_audioengine inputnode]; _speechrecognitiontask = [_speechrecogniser recognitiontaskwithrequest:_speechrecognitionrequest resulthandler:^(sfspeechrecognitionresult * _nullable result, nserror * _nullable error) { bool finished = no; if (result) { self.transcribedtextview.text = result.besttranscription.formattedstring; finished = result.isfinal; nsstring *flycommand = @fly; nsstring *landcommand = @land; nsstring *formattedstring = [[result besttranscription] formattedstring]; if ([formattedstring rangeofstring:flycommand options:nscaseinsensitivesearch].location != nsnotfound) { [_minidrone takeoff]; } if ([formattedstring rangeofstring:landcommand options:nscaseinsensitivesearch].location != nsnotfound) { [_minidrone land]; } } if (error != nil || finished) { [[self audioengine] stop]; [inputnode removetaponbus:0]; self.speechrecognitionrequest = nil; self.speechrecognitiontask = nil; _transcribedtextview.text = @; [[self startlisteningbutton] setenabled:yes]; } }]; avaudioformat *recordingformat = [inputnode outputformatforbus:0]; [inputnode installtaponbus:0 buffersize:1024 format:recordingformat block:^(avaudiopcmbuffer * _nonnull buffer, avaudiotime * _nonnull when) { [[self speechrecognitionrequest] appendaudiopcmbuffer:buffer]; }]; [_audioengine prepare]; [_audioengine startandreturnerror:nil];}- (void)authorisesr{ [sfspeechrecognizer requestauthorization:^(sfspeechrecognizerauthorizationstatus status) { [[nsoperationqueue mainqueue] addoperationwithblock:^{ switch (status) { case sfspeechrecognizerauthorizationstatusauthorized: [[self startlisteningbutton] setenabled:yes]; break; case sfspeechrecognizerauthorizationstatusdenied: [[self startlisteningbutton] setenabled:no]; [[self startlisteningbutton] settitle:@speech recognition access denied by user forstate:uicontrolstatedisabled]; break; case sfspeechrecognizerauthorizationstatusrestricted: [[self startlisteningbutton] setenabled:no]; [[self startlisteningbutton] settitle:@speech restricted forstate:uicontrolstatedisabled]; break; case sfspeechrecognizerauthorizationstatusnotdetermined: [[self startlisteningbutton] setenabled:no]; break; } }]; }];}...right now it works and the drone responds when i speak to it, but the app itself seems really slow in registering my voice, maybe 3-5 seconds to register just the one word commands. i'm not sure if this is something in my code running slow of its the standard time for the framework to respond.i removed other references to the sdk to avoid confusion just leaving in the basic drone commands i need, i.e. [_minidrone takeoff]; and [_minidrone land];",
    "present_kp": [
      "ios"
    ],
    "absent_kp": [
      "objective c"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "bash reuse process substitution file. i have a big script which takes a file as input and does various stuff with it. here is a test version:echo cat: $1cat $1echo grep: $1grep hello $1echo sed: $1sed 's/hello/world/g' $1i want my script to work with process substitution, but only the first command (cat) works, while the rest don't. i think this is because it is a pipe.$ myscript.sh <(echo hello)should print\ud83d\udc31 /dev/fd/63hellogrep: /dev/fd/63hellosed: /dev/fd/63worldis this possible?",
    "present_kp": [
      "bash",
      "process substitution"
    ],
    "absent_kp": [
      "io redirection"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "count number of leaves in binary tree. description:given a binary tree find the number of leaves. although the problem is simple and has been solved many times i am more interested in find object oriented ways to solve the algorithmic problems.code:interface node { public int count(); } class leafnode implements node { leafnode() {} public int count() { return 1; } } class nonleafnode implements node { private node left; private node right; nonleafnode(node left, node right) { this.left = left; this.right = right; } public int count() { return this.left.count() + this.right.count(); } } class nullnode implements node { public int count() { return 0; } } public class main { public static void main(string[] args) { node tree = new leafnode(); system.out.println(tree.count()); // 1 tree = new nonleafnode(new leafnode(), new leafnode()); system.out.println(tree.count()); // 2 tree = new nonleafnode( new nonleafnode(new leafnode(), new nullnode()), new nullnode()); system.out.println(tree.count()); // 2 } }",
    "present_kp": [
      "object oriented",
      "tree"
    ],
    "absent_kp": [
      "java",
      "programming challenge"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "mate login without marco. i've managed to corrupt my user-wide mate configuration so that marco (the window manager) does not start up when i log in. thankfully the taskbar still does, so as a stopgap, i open up a terminal and run marco and functionality is back to normal. the last thing i tinkered with was removing ~/.config/menus/applications-merged but i doubt that did it and i've been logged in a long time so i'm not sure what did it. it's definitely a local problem since i created a new user and their login worked fine. i am on debian 8 if that makes a difference.first i checkedorg.mate.desktop.session.require-components windowmanager but that looks fine and shows 'marco.'then i tried:removing the various ~/.gnome directories and re-logging inremoving my ~/.config directory and re-logging ini am stumped. i appreciate any help!",
    "present_kp": [
      "login",
      "mate"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "setting monitor active with hdmi splitter. i've recently bought a hdmi splitter the splitter is inactive, which means only one screen can be active at a time.it's starting to get frustrating that my tv seems to have a higher priority than the monitor its shared with, and having to switch it off and on again to display. i'd prefer the monitor to have a higher priority and when i want to use the tv i'd simply turn off the monitor or run a script that switches it off. i've tried setting it active again with xrandrbut can't seem to get it to work.the following is the connected screensxrandr --queryscreen 0: minimum 8 x 8, current 5760 x 1080, maximum 16384 x 16384dvi-i-0 disconnected (normal left inverted right x axis y axis)dvi-i-1 connected 1920x1080+0+0 (normal left inverted right x axis y axis) 531mm x 299mm 1920x1080 60.00*+ 59.94 50.00 60.00 50.04 1680x1050 59.95 1440x900 59.89 1280x1024 75.02 60.02 1280x960 60.00 1280x720 60.00 59.94 50.00 1024x768 75.03 70.07 60.00 800x600 75.00 72.19 60.32 56.25 720x576 50.00 720x480 59.94 640x480 75.00 72.81 59.94 59.93 hdmi-0 connected 1920x1080+3840+0 (normal left inverted right x axis y axis) 530mm x 290mm 1280x720 60.00 + 59.94 1920x1080 60.00* 60.05 1680x1050 59.95 1600x900 60.00 1280x1024 60.02 1280x960 60.00 1152x720 60.00 1024x768 60.00 800x600 60.32 720x576 50.00 720x480 59.94 640x480 59.94 59.93 dp-0 disconnected (normal left inverted right x axis y axis)dvi-d-0 connected primary 1920x1080+1920+0 (normal left inverted right x axis y axis) 598mm x 336mm 1920x1080 60.00*+ 1680x1050 59.95 1440x900 74.98 59.89 1280x1024 75.02 60.02 1024x768 75.03 60.00 800x600 75.00 60.32 640x480 75.00 72.81 59.94 dp-1 disconnected (normal left inverted right x axis y axis)hdmi-0 is the connected tv and i believe that dvi-i-0 is the disconnected monitor i'm trying to activate.i've tried switching it on by running xrandr --output dvi-i-0 --autowithout success, all i seem to be able to do is make the screens flicker when calling calling xrandr --output hdmi-0 --auto with the tv as output.",
    "present_kp": [
      "xrandr",
      "hdmi"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "extract common code to enable/disable, show/hide controls on some condition. i have few controls on my page which i want to show/hide and/or enable/disable on some condition.images to help illustrate the requirements:following is the code to show/hide, enable/disable the controls. please review it and tell me how i can extract common code.any other suggestion are also more than welcome.// change controls status (enable/disable, show/hide) on the basis of selected itemprotected void ddllocationtype_selectedindexchanged(object sender, eventargs e){ int locationtype = 0; int.tryparse(ddllocationtype.selecteditem.value, out locationtype); enablerelavantlocaitoncontrols(locationtype);}private void enablerelavantlocaitoncontrols(int locationtype){ switch (locationtype) { case 0: defaultcontrolsposition(); break; case 1: enablecontrolsforprovince(); break; case 2: enablecontrolsfordistrict(); break; case 3: enablecontrolsfortehsil(); break; case 4: enablecontrolsforuc(); break; case 6: enablecontrolsforvillage(); break; default: defaultcontrolsposition(); break; }}// show all drop downs.// hide all textboxes and disable all controls.private void defaultcontrolsposition(){ ddlprovince.enabled = false; ddlprovince.visible = true; txtprovince.enabled = false; txtprovince.visible = false; ddldistrict.enabled = false; ddldistrict.visible = true; ddldistrict.enabled = false; txtdistrict.visible = false; ddltehsil.enabled = false; ddltehsil.visible = true; txttehsil.enabled = false; txttehsil.visible = false; ddluc.enabled = false; ddluc.visible = true; txtuc.enabled = false; txtuc.visible = false; ddlvillage.enabled = false; ddlvillage.visible = true; txtvillage.enabled = false; txtvillage.visible = false;}// show all drop downs except province drop down and disable all.// show and enable textbox for province instead of drop down so user can // enter name of province.private void enablecontrolsforprovince(){ ddlprovince.enabled = false; ddlprovince.visible = false; txtprovince.enabled = true; txtprovince.visible = true; ddldistrict.enabled = false; ddldistrict.visible = true; ddldistrict.enabled = false; txtdistrict.visible = false; ddltehsil.enabled = false; ddltehsil.visible = true; txttehsil.enabled = false; txttehsil.visible = false; ddluc.enabled = false; ddluc.visible = true; txtuc.enabled = false; txtuc.visible = false; ddlvillage.enabled = false; ddlvillage.visible = true; txtvillage.enabled = false; txtvillage.visible = false;}// show and enable 'province' drop down.// hide and disable 'district' drop down.// show and enable 'district' text box.// show and disable all other drop downs beneath district.private void enablecontrolsfordistrict(){ ddlprovince.enabled = true; ddlprovince.visible = true; txtprovince.enabled = false; txtprovince.visible = false; ddldistrict.enabled = false; ddldistrict.visible = false; txtdistrict.enabled = true; txtdistrict.visible = true; ddltehsil.enabled = false; ddltehsil.visible = true; txttehsil.enabled = false; txttehsil.visible = false; ddluc.enabled = false; ddluc.visible = true; txtuc.enabled = false; txtuc.visible = false; ddlvillage.enabled = false; ddlvillage.visible = true; txtvillage.enabled = false; txtvillage.visible = false;}private void enablecontrolsfortehsil(){ ddlprovince.enabled = true; ddlprovince.visible = true; txtprovince.enabled = false; txtprovince.visible = false; ddldistrict.enabled = true; ddldistrict.visible = true; txtdistrict.enabled = false; txtdistrict.visible = false; ddltehsil.enabled = false; ddltehsil.visible = false; txttehsil.enabled = true; txttehsil.visible = true; ddluc.enabled = false; ddluc.visible = true; txtuc.enabled = false; txtuc.visible = false; ddlvillage.enabled = false; ddlvillage.visible = true; txtvillage.enabled = false; txtvillage.visible = false;}private void enablecontrolsforuc(){ ddlprovince.enabled = true; ddlprovince.visible = true; txtprovince.enabled = false; txtprovince.visible = false; ddldistrict.enabled = true; ddldistrict.visible = true; txtdistrict.enabled = false; txtdistrict.visible = false; ddltehsil.enabled = true; ddltehsil.visible = true; txttehsil.enabled = false; txttehsil.visible = false; ddluc.enabled = false; ddluc.visible = false; txtuc.enabled = true; txtuc.visible = true; ddlvillage.enabled = false; ddlvillage.visible = true; txtvillage.enabled = false; txtvillage.visible = false;}private void enablecontrolsforvillage(){ ddlprovince.enabled = true; ddlprovince.visible = true; txtprovince.enabled = false; txtprovince.visible = false; ddldistrict.enabled = true; ddldistrict.visible = true; txtdistrict.enabled = false; txtdistrict.visible = false; ddltehsil.enabled = true; ddltehsil.visible = true; txttehsil.enabled = false; txttehsil.visible = false; ddluc.enabled = true; ddluc.visible = true; txtuc.enabled = false; txtuc.visible = false; ddlvillage.enabled = false; ddlvillage.visible = false; txtvillage.enabled = true; txtvillage.visible = true;}",
    "present_kp": [],
    "absent_kp": [
      "c#"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "batch edit all events in a week in google calendar. i have 13+ events that i have in a certain week. i need to move all of them back a week, same times and weekdates, just back a week.i know that you can drag and drop dates within the weekly view, but:1) you can't do that with multiple at a time.2) it's restricted to a week.this seems like something that should be a basic feature. any way to get this done without editing each and every event i have?",
    "present_kp": [
      "google calendar"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "external user id to internal user id. there is an internal api, rest client over db, all of which methods require user id as parameter. user id is an integer value - numeric pk in user table.also there is an public api, that is like a auth+proxy gateway: it checks the jwt token, extract user id and then redirect to internal api.so right now user id is passed as part of jwt token, so everyone can extract/know it value - the problem that i want to solve.the thirst what i thought was to change public api to work with an external user id (guid for example): - add additional mapping table to have mapping between external and internal id;- external api takes external id from jwt, makes query to retrieve corresponding internal user id- call internal api using internal user idwhat i do not like in this approach is an additional query to db with each request. using caching can reduce number of queries, butanother option, that i see, is to use jwe token instead of jwt. at least one disadvantage of this approach is the encryption/decryption expenses.",
    "present_kp": [
      "api"
    ],
    "absent_kp": [
      "security",
      "microservices"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "finding where to breakpoint?. i have a program written in vb6 with a label that is updated every time i push a button:when i push the hourglass button, the 72 label goes up by 10. how can i find the part of the program that handles this so i can set a breakpoint and examine the assembly?also, if the answer involves searching for strings, how would i do it without (in case i have a future program that encrypts strings or something)?",
    "present_kp": [],
    "absent_kp": [
      "binary analysis"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "dumping flash memory using bus pirate. i'm planning on dumping and reading the flash memory of a winbond w25q128fv chip. i've done some research and plan on buying the following tools to achieve this:bus pirate 3.6aprobe cablesoic8 test clipi already have a soldering kit. are these the right tools and are they sufficient to read the flash memory to my computer?also, i already have an arduino uno and a raspberry pi. can either of those be used in place of a bus pirate?",
    "present_kp": [
      "dumping",
      "flash"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how do i remove permission from an app that i gave google oauth access to?. possible duplicate:unauthenticate google oauth connections i gave google oauth access to this app that wanted to read google analytics api info.http://www.panguintool.comno problem, but now i want to take away that app's permission to read my data. where does google hide these authorized apps so i can remove it?",
    "present_kp": [
      "google analytics",
      "oauth"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "javascript function to create 2 soccer teams as even as possible, from a list of players. ? given a list of players with their respective scores (how well does a player play soccer, from 1 to 5 points), return two teams that are as even as possible.i've created this code, which is a first version, but i really would like to hear about ideas of how can i improve it.exports.teamcreator = (function () { this.players = []; function setplayers(players){ this.players = object.assign([], players); } function createteams() { var input = this.players; var teams = [], i = 0, scores = [], max = input.length; scores[a]=0; scores[b]=0; teams[a]=[]; teams[b]=[]; input.sort(function(a, b){ return b.score - a.score;}); var nextplayer; var teamfornextplayer; for (i = 0; i < max; i++){ next(); teams[teamfornextplayer].push(nextplayer); scores[teamfornextplayer] += nextplayer.score; } function getbest() { nextplayer = input.splice(0, 1)[0]; //best in list } function getworst() { nextplayer = input.splice(input.length - 1, 1)[0]; //worst in list } function next(){ var countera = teams[a], counterb = teams[b], scorea = scores[a], scoreb = scores[b]; if(countera > counterb) { //b has less players than a teamfornextplayer = b; if(scoreb >= scorea){ getworst(); }else{ getbest(); } } else if(countera < counterb) { //a has less players than b teamfornextplayer = a; if(scorea >= scoreb){ getworst(); }else{ getbest(); } } else //same amount of players { getbest(); teamfornextplayer = (scorea >= scoreb) ? b : a; } } return { teama: teams[a], teamb: teams[b], diff: math.abs(scores[a] - scores[b]), scorea: scores[a], scoreb: scores[b] }; } return { setplayers : setplayers, createteams: createteams }}());if it's easier, you can pull the code from here.",
    "present_kp": [
      "javascript"
    ],
    "absent_kp": [
      "algorithm"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "using url without query parameter for sitelinks searchbox?. all the examples i see to attempt to gain a search box in sitelinks in the serps use a target url that has a url parameter like so:<url>}can any url be used or does this only work with query parameters? for example:<url>}",
    "present_kp": [
      "url",
      "search"
    ],
    "absent_kp": [
      "schema.org",
      "rich snippets"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "proftpd does not change default group for new files. i am trying to configure proftpd to change group for newly created files/directories.in my config i have this:<directory /home/*> groupowner www</directory>which does not seem to work.all users are added to www group.debug shows nothing regarding to a group change.i'm using freebsd 9.0-release.edit: i'm willing to try any other ftp server that makes this easier.",
    "present_kp": [
      "freebsd",
      "ftp"
    ],
    "absent_kp": [
      "permissions"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "data science / machine learning books for mathematicians. i have found other requests for references here. in particular in:where to start, which booksandbooks about the science in data science?i have given a glance to:artificial intelligence: a modern approach (russel & norvig)machine learning: the art and science of algorithms that make sense of data (flach)learning from data (abu-mostafa et al.)introduction to statistical learning (james et al.)elements of statistical learning (hastie et al.)pattern recognition and machine learning (bishop)now it is difficult to evaluate if they would fit my needs because only a few pages are generally available online. however my first impression is that they do not. in the appendices of artificial intelligence: a modern approach i can read:mathematicians define a vector as a member of a vector space, but we will use a more concrete definition: a vector is an ordered sequence of values.this is exactly the kind of approach i am not looking for.i'm looking for a book which assumes the reader has a good understanding in set theory, abstract algebra, measure and probability theory, statistics, topology, graph theory, complexity theory, etc and a preference for formal and axiomatic explanations rather than lenghty and so-called intuitive approaches based on basic mathematical objects and examples. furthermore i don't want something that looks like a recipe book from the very beginning. i want a book that formalizes the abstract and common shape of all data science methods as well as their common aim first. only after that it can start to explain the different categories by explicitely stating which further hypotheses each category is assuming and which cases/problems/domains they are known to handle efficiently or not.at last, to be clear, i have no problem with being shown concrete examples and their treatment via a specific programming language for example. i just want this to come second as an illustration for the conceptual explanation, not as a substitute.",
    "present_kp": [
      "books"
    ],
    "absent_kp": [
      "reference request"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "pay per impression adsense provider. i am searching for a good adsense provider who pays per impression. please suggest",
    "present_kp": [],
    "absent_kp": [
      "google adsense"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "gulpfile to build and serve a book off a manuscript. i have a gulpfile.js that compiles the manuscript (resume project) and serves it as a book on the browser using browser-sync.how to optimize this gulpfile.js both for performance and writing style? it'd be awesome if an expert can review this for me. there are many fixes and writing patterns that i can go after. for example, i can see myself not using -sync methods or cutting out the fs module completely because fsp module is better -- using promises and all and things like that. but what is the right approach?+!~-(((r, undefined) => { // *************************************// // *********** dependencies ************// // *************************************// const gulp = r('gulp') const fsp = r('fs-promise') const fs = r('fs') const path = r('path') const browsersync = r('browser-sync') const handlebars = r('gulp-compile-handlebars') const concat = r('gulp-concat') const gulpif = r('gulp-if') const book = r('book-length') const del = r('del') const delayed = r('delayed') // preprocessors / transpilers const haml = r('gulp-haml') const markdown = r('gulp-markdown') const sass = r('gulp-sass') const less = r('gulp-less') const stylus = r('gulp-stylus') const postcss = r('gulp-postcss') // *************************************// // ************ default task ***********// // *************************************// gulp.task('default', ['renderbook', 'indexpage', 'watchbook']) // *************************************// // ************ build build ************// // *************************************// gulp.task('renderbook', ['pages', 'templates'], () => { const folders = getfolders(path.join('.', 'build', 'manuscript')) folders.map(folder => { renderpage(folder) }) }) function getfolders (dir) { return fs.readdirsync(dir) .filter(file => fs.statsync(path.join(dir, file)).isdirectory()) } gulp.task('pages', () => gulp.src(path.join('manuscript', '*', '*')) .pipe(gulpif(/[.]haml$/, haml())) .pipe(gulpif(/[.]md$/, markdown())) .pipe(gulpif(/[.]markdown$/, markdown())) .pipe(gulpif(/[.]scss|sass$/, sass())) .pipe(gulpif(/[.]less$/, less())) .pipe(gulpif(/[.]styl$/, stylus())) .pipe(gulp.dest(path.join('build', 'manuscript'))) ) gulp.task('templates', () => gulp.src(path.join('templates', '*.*')) .pipe(gulpif(/[.]haml$/, haml())) .pipe(gulpif(/[.]md$/, markdown())) .pipe(gulpif(/[.]markdown$/, markdown())) .pipe(gulpif(/[.]scss|sass$/, sass())) .pipe(gulpif(/[.]less$/, less())) .pipe(gulpif(/[.]styl$/, stylus())) .pipe(gulp.dest(path.join('build', 'templates'))) ) // *************************************// // ************ book indexer ***********// // *************************************// gulp.task('indexpage', () => { const booklength = book.length() let contentstring = '' for (let index = 1; index <= booklength; index++) { contentstring += '<div class='page'><iframe src='build/renders/page-${index}.html'></iframe></div>' } fsp.readjson(path.join('.', '.bookrc')).then((json) => { return templatedata = { content: contentstring, bookname: json.name } }).then((templatedata) => { gulp.src(path.join('.', 'crust', 'index-template.html')) .pipe(handlebars(templatedata, {})) .pipe(concat('index.html')) .pipe(gulp.dest('.')) .pipe(browsersync.stream()) }).catch((err) => { console.log('something went wrong', err) }) }) // *************************************// // ************ page renderer **********// // *************************************// function renderpage (page) { const bodypath = path.join('.', 'build', 'manuscript', page, 'body.html') const headpath = path.join('.', 'build', 'manuscript', page, 'head.html') const scriptpath = path.join('.', 'build', 'manuscript', page, 'script.js') const stylepath = path.join('.', 'build', 'manuscript', page, 'style.css') const templatestylepath = path.join('.', 'build', 'templates', 'style.css') const templateheadpath = path.join('.', 'build', 'templates', 'head.html') let bodycontent = '' let stylecontent = '' let templatestylecontent = '' let scriptcontent = '' let headcontent = '' let templateheadcontent = '' // todo: use promises here? if (fs.existssync(bodypath)) { bodycontent = fs.readfilesync(bodypath, 'utf-8').tostring() } if (fs.existssync(stylepath)) { stylecontent = fs.readfilesync(stylepath, 'utf-8').tostring() } if (fs.existssync(templatestylepath)) { templatestylecontent = fs.readfilesync(templatestylepath, 'utf-8').tostring() } if (fs.existssync(headpath)) { headcontent = fs.readfilesync(headpath, 'utf-8').tostring() } if (fs.existssync(templateheadpath)) { templateheadcontent = fs.readfilesync(templateheadpath, 'utf-8').tostring() } if (fs.existssync(scriptpath)) { scriptcontent = fs.readfilesync(scriptpath, 'utf-8').tostring() } const pagetemplatedata = { bodycontent, templatestylecontent, stylecontent, headcontent, templateheadcontent, scriptcontent} gulp.src(path.join('.', 'crust', 'page-template.html')) .pipe(handlebars(pagetemplatedata, {})) .pipe(concat('${page}.html')) .pipe(gulp.dest(path.join('.', 'build', 'renders'))) .pipe(browsersync.stream()) } // glob pattern matching const glob = [path.join('manuscript', '*'), path.join('manuscript', '*', '*.markdown'), path.join('manuscript', '*', '*.scss'), path.join('manuscript', '*', '*.js') ] // *************************************// // ************ page renderer **********// // *************************************// gulp.task('watchbook', () => { browsersync.init({ server: './', port: 4567, notify: false, loglevel: 'debug' }) // watch deleted pages gulp.watch(path.join('trash', '*'), obj => { const pagepath = obj.path const paths = pagepath.split(path.sep) let page = paths[paths.length - 1] === '' ? paths[paths.length - 2] : paths[paths.length - 1] page = '${page.split('-')[0]}-${page.split('-')[1]}' if (obj.type === 'added') { del(path.join('build', 'manuscript', page)) del(path.join('build', 'renders', '${page}.html')) gulp.start('indexpage') } }) // watch everything else gulp.watch(glob, obj => { let page let pagepath = obj.path const paths = pagepath.split(path.sep) if (paths[paths.length - 1] === '') { page = paths[paths.length - 2] } else if (paths[paths.length - 1].split('-')[0] === 'page') { page = paths[paths.length - 1] } else { page = paths[paths.length - 2] pagepath = path.dirname(obj.path) } del(path.join('build', 'manuscript', page)) if (obj.type === 'added') { gulp.start('indexpage') } let delay = 1000 delayed.delay(() => { const stats = fs.statsync(pagepath) if (stats.isdirectory()) { gulp.src(path.join(pagepath, '*')) .pipe(gulpif(/[.]haml$/, haml())) .pipe(gulpif(/[.]md$/, markdown())) .pipe(gulpif(/[.]markdown$/, markdown())) .pipe(gulpif(/[.]scss|sass$/, sass())) .pipe(gulpif(/[.]less$/, less())) .pipe(gulpif(/[.]styl$/, stylus())) .pipe(gulp.dest(path.join('build', 'manuscript', page))) .on('end', () => { renderpage(page) }) } }, delay) }) gulp.watch(path.join('templates', '**.*'), obj => { gulp.start('renderbook') }) })}))(require)",
    "present_kp": [],
    "absent_kp": [
      "javascript",
      "gulp.js"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is the proper infrastructure and release cycle for developing and testing an application?. i am developing a web application for my coworkers, and from a little management app it is becoming a more and intense large project that is involving more people as i go. i am pretty sure my boss will ask me to add even more as i will go on.so i became unsure if i am doing things right, because i never worked on large projects (actually this is my first job).the webapp i am working on is used by my company so i cannot do edits and changes while they are working on it; i will obviously expose them to some bugs and ux issues that could be deadly, like adding wrong data in databases etc. etc. etc.so in the last months i divided my web application in two directories; one is the main index.php where they work, the other is the development directory, in the same server, which is only used by me.when i finish all the edits and changes, i move everything in the development directory to the main directory used by my coworkers.but i ask myself: is this a proper way to develop and test web apps? lately i had been caught in terms like continuous test driven development, unit-testing, sandbox development, etc. and moreover, making little changes in my code and uploading it every time to see what happens doesn't sound very professional to me anymore.so how can i understand if this is a proper way to develop web applications? i am not sure even if i need all this testing, since i am the only webdev here.i know that this can be too broad but i don't know any other ways to ask something like that as i know just a few things about web development in a larger scale. this is something that could be answered by a software architect for example, or a senior web dev, but i am the only web dev here.",
    "present_kp": [
      "web development",
      "testing",
      "web applications"
    ],
    "absent_kp": [
      "development process"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is there something more behind mvc in web develop frameworks. i just started my career in the web development about two months ago. during the two months, i wrote one web service in php symfony2, get involed investigation of one project written in python flask and also help debugging a web service using java spring mvc.ok, here is the summary.php symfony2python flaskjava spring mvcafter getting a touch with these three frameworks, i've got to know that they are following the same pattern, mvc.controller one front controller to recieve requests from clients and transform those requests to calls to functions within controller objects which are wriitten by developers.viewafter processing the requests, the framework uses the processing results to generate the pages that will be sent back the clients or just return some data to clients to response to ajax calls.modeli have not quite got the idea of model. perhaps it is the way of handling database?questions:is this workflow that i wrote above a standard way of writing web services? if so, could you give me some useful links that contain the offcial documments?or is it just a custom followed by developers? or just an industrial standard.i think there are still some other factors behind the mvc pattern, like filters. (i haven't got the chance to use filters in my project, but i think there is more than filters behind the scene.)i was told that these frameworks get ideas from ruby on rails which introduces this workflow. is that true? where can i get the full introduction or documents about this kind of web services workflow?thanks very much.",
    "present_kp": [
      "web development",
      "web services"
    ],
    "absent_kp": [
      "web framework"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "set the servername directive globally to suppress this message. i am setting up php for the first time and i am getting this error from the apache server:set the servername directive globally to suppress this message.this is the content of my httpd.conf file:# this is the main apache http server configuration file. it contains the# configuration directives that give the server its instructions.# see <url:<url> for detailed information.# in particular, see # <url:<url> for a discussion of each configuration directive.## do not simply read the instructions in here without understanding# what they do. they're here only as hints or reminders. if you are unsure# consult the online docs. you have been warned. ## configuration and logfile names: if the filenames you specify for many# of the server's control files begin with / (or drive:/ for win32), the# server will use that explicit path. if the filenames do *not* begin# with /, the value of serverroot is prepended -- so logs/access_log# with serverroot set to /usr/local/apache2 will be interpreted by the# server as /usr/local/apache2/logs/access_log, whereas /logs/access_log # will be interpreted as '/logs/access_log'.## note: where filenames are specified, you must use forward slashes# instead of backslashes (e.g., c:/apache instead of c:pache).# if a drive letter is omitted, the drive on which httpd.exe is located# will be used by default. it is recommended that you always supply# an explicit drive letter in absolute paths to avoid confusion.## serverroot: the top of the directory tree under which the server's# configuration, error, and log files are kept.## do not add a slash at the end of the directory path. if you point# serverroot at a non-local disk, be sure to specify a local disk on the# mutex directive, if file-based mutexes are used. if you wish to share the# same serverroot for multiple httpd daemons, you will need to change at# least pidfile.#serverroot c:/users/lotusms/desktop/apache/apache24## mutex: allows you to set the mutex mechanism and mutex file directory# for individual mutexes, or change the global defaults## uncomment and change the directory if mutexes are file-based and the default# mutex file directory is not on a local disk or is not appropriate for some# other reason.## mutex default:logs## listen: allows you to bind apache to specific ip addresses and/or# ports, instead of the default. see also the <virtualhost># directive.## change this to listen on specific ip addresses as shown below to # prevent apache from glomming onto all bound ip addresses.##listen 12.34.56.78:80listen 8080## dynamic shared object (dso) support## to be able to use the functionality of a module which was built as a dso you# have to place corresponding 'loadmodule' lines at this location so the# directives contained in it are actually available _before_ they are used.# statically compiled modules (those listed by 'httpd -l') do not need# to be loaded here.## example:# loadmodule foo_module modules/mod_foo.so#loadmodule access_compat_module modules/mod_access_compat.soloadmodule actions_module modules/mod_actions.soloadmodule alias_module modules/mod_alias.soloadmodule allowmethods_module modules/mod_allowmethods.soloadmodule asis_module modules/mod_asis.soloadmodule auth_basic_module modules/mod_auth_basic.so#loadmodule auth_digest_module modules/mod_auth_digest.so#loadmodule authn_anon_module modules/mod_authn_anon.soloadmodule authn_core_module modules/mod_authn_core.so#loadmodule authn_dbd_module modules/mod_authn_dbd.so#loadmodule authn_dbm_module modules/mod_authn_dbm.soloadmodule authn_file_module modules/mod_authn_file.so#loadmodule authn_socache_module modules/mod_authn_socache.so#loadmodule authnz_ldap_module modules/mod_authnz_ldap.soloadmodule authz_core_module modules/mod_authz_core.so#loadmodule authz_dbd_module modules/mod_authz_dbd.so#loadmodule authz_dbm_module modules/mod_authz_dbm.soloadmodule authz_groupfile_module modules/mod_authz_groupfile.soloadmodule authz_host_module modules/mod_authz_host.so#loadmodule authz_owner_module modules/mod_authz_owner.soloadmodule authz_user_module modules/mod_authz_user.soloadmodule autoindex_module modules/mod_autoindex.so#loadmodule buffer_module modules/mod_buffer.so#loadmodule cache_module modules/mod_cache.so#loadmodule cache_disk_module modules/mod_cache_disk.so#loadmodule cern_meta_module modules/mod_cern_meta.soloadmodule cgi_module modules/mod_cgi.so#loadmodule charset_lite_module modules/mod_charset_lite.so#loadmodule data_module modules/mod_data.so#loadmodule dav_module modules/mod_dav.so#loadmodule dav_fs_module modules/mod_dav_fs.so#loadmodule dav_lock_module modules/mod_dav_lock.so#loadmodule dbd_module modules/mod_dbd.so#loadmodule deflate_module modules/mod_deflate.soloadmodule dir_module modules/mod_dir.so#loadmodule dumpio_module modules/mod_dumpio.soloadmodule env_module modules/mod_env.so#loadmodule expires_module modules/mod_expires.so#loadmodule ext_filter_module modules/mod_ext_filter.so#loadmodule file_cache_module modules/mod_file_cache.so#loadmodule filter_module modules/mod_filter.so#loadmodule headers_module modules/mod_headers.so#loadmodule heartbeat_module modules/mod_heartbeat.so#loadmodule heartmonitor_module modules/mod_heartmonitor.so#loadmodule ident_module modules/mod_ident.so#loadmodule imagemap_module modules/mod_imagemap.soloadmodule include_module modules/mod_include.so#loadmodule info_module modules/mod_info.soloadmodule isapi_module modules/mod_isapi.so#loadmodule lbmethod_bybusyness_module modules/mod_lbmethod_bybusyness.so#loadmodule lbmethod_byrequests_module modules/mod_lbmethod_byrequests.so#loadmodule lbmethod_bytraffic_module modules/mod_lbmethod_bytraffic.so#loadmodule lbmethod_heartbeat_module modules/mod_lbmethod_heartbeat.so#loadmodule ldap_module modules/mod_ldap.so#loadmodule logio_module modules/mod_logio.soloadmodule log_config_module modules/mod_log_config.so#loadmodule log_debug_module modules/mod_log_debug.so#loadmodule log_forensic_module modules/mod_log_forensic.so#loadmodule lua_module modules/mod_lua.soloadmodule mime_module modules/mod_mime.so#loadmodule mime_magic_module modules/mod_mime_magic.soloadmodule negotiation_module modules/mod_negotiation.so#loadmodule proxy_module modules/mod_proxy.so#loadmodule proxy_ajp_module modules/mod_proxy_ajp.so#loadmodule proxy_balancer_module modules/mod_proxy_balancer.so#loadmodule proxy_connect_module modules/mod_proxy_connect.so#loadmodule proxy_express_module modules/mod_proxy_express.so#loadmodule proxy_fcgi_module modules/mod_proxy_fcgi.so#loadmodule proxy_ftp_module modules/mod_proxy_ftp.so#loadmodule proxy_html_module modules/mod_proxy_html.so#loadmodule proxy_http_module modules/mod_proxy_http.so#loadmodule proxy_scgi_module modules/mod_proxy_scgi.so#loadmodule ratelimit_module modules/mod_ratelimit.so#loadmodule reflector_module modules/mod_reflector.so#loadmodule remoteip_module modules/mod_remoteip.so#loadmodule request_module modules/mod_request.so#loadmodule reqtimeout_module modules/mod_reqtimeout.so#loadmodule rewrite_module modules/mod_rewrite.so#loadmodule sed_module modules/mod_sed.so#loadmodule session_module modules/mod_session.so#loadmodule session_cookie_module modules/mod_session_cookie.so#loadmodule session_crypto_module modules/mod_session_crypto.so#loadmodule session_dbd_module modules/mod_session_dbd.soloadmodule setenvif_module modules/mod_setenvif.so#loadmodule slotmem_plain_module modules/mod_slotmem_plain.so#loadmodule slotmem_shm_module modules/mod_slotmem_shm.so#loadmodule socache_dbm_module modules/mod_socache_dbm.so#loadmodule socache_memcache_module modules/mod_socache_memcache.so#loadmodule socache_shmcb_module modules/mod_socache_shmcb.so#loadmodule speling_module modules/mod_speling.so#loadmodule ssl_module modules/mod_ssl.so#loadmodule status_module modules/mod_status.so#loadmodule substitute_module modules/mod_substitute.so#loadmodule unique_id_module modules/mod_unique_id.so#loadmodule userdir_module modules/mod_userdir.so#loadmodule usertrack_module modules/mod_usertrack.so#loadmodule version_module modules/mod_version.so#loadmodule vhost_alias_module modules/mod_vhost_alias.so#loadmodule watchdog_module modules/mod_watchdog.so#loadmodule xml2enc_module modules/mod_xml2enc.so<ifmodule unixd_module>## if you wish httpd to run as a different user or group, you must run# httpd as root initially and it will switch. ## user/group: the name (or #number) of the user/group to run httpd as.# it is usually good practice to create a dedicated user and group for# running httpd, as with most system services.#user daemongroup daemon</ifmodule># 'main' server configuration## the directives in this section set up the values used by the 'main'# server, which responds to any requests that aren't handled by a# <virtualhost> definition. these values also provide defaults for# any <virtualhost> containers you may define later in the file.## all of these directives may appear inside <virtualhost> containers,# in which case these default settings will be overridden for the# virtual host being defined.### serveradmin: your address, where problems with the server should be# e-mailed. this address appears on some server-generated pages, such# as error documents. e.g. <email>#serveradmin <email>## servername gives the name and port that the server uses to identify itself.# this can often be determined automatically, but we recommend you specify# it explicitly to prevent problems during startup.## if your host doesn't have a registered dns name, enter its ip address here.##servername <url>## deny access to the entirety of your server's filesystem. you must# explicitly permit access to web content directories in other # <directory> blocks below.#<directory />allowoverride nonerequire all denied</directory>## note that from this point forward you must specifically allow# particular features to be enabled - so if something's not working as# you might expect, make sure that you have specifically enabled it# below.### documentroot: the directory out of which you will serve your# documents. by default, all requests are taken from this directory, but# symbolic links and aliases may be used to point to other locations.#documentroot c:/users/lotusms/desktop/apache/apache24/htdocs<directory c:/users/lotusms/desktop/apache/apache24/htdocs>## possible values for the options directive are none, all,# or any combination of:# indexes includes followsymlinks symlinksifownermatch execcgi multiviews## note that multiviews must be named *explicitly* --- options all# doesn't give it to you.## the options directive is both complicated and important. please see# <url> for more information.#options indexes followsymlinks## allowoverride controls what directives may be placed in .htaccess files.# it can be all, none, or any combination of the keywords:# allowoverride fileinfo authconfig limit#allowoverride none## controls who can get stuff from this server.#require all granted</directory>## directoryindex: sets the file that apache will serve if a directory# is requested.#<ifmodule dir_module>directoryindex index.html</ifmodule>## the following lines prevent .htaccess and .htpasswd files from being # viewed by web clients. #<files .ht*>require all denied</files>## errorlog: the location of the error log file.# if you do not specify an errorlog directive within a <virtualhost># container, error messages relating to that virtual host will be# logged here. if you *do* define an error logfile for a <virtualhost># container, that host's errors will be logged there and not here.#errorlog logs/error.log## loglevel: control the number of messages logged to the error_log.# possible values include: debug, info, notice, warn, error, crit,# alert, emerg.#loglevel warn<ifmodule log_config_module>## the following directives define some format nicknames for use with# a customlog directive (see below).#logformat %h %l %u %t \\%r\\ %>s %b \\%{referer}i\\ \\%{user-agent}i\\ combinedlogformat %h %l %u %t \\%r\\ %>s %b common<ifmodule logio_module> # you need to enable mod_logio.c to use %i and %o logformat %h %l %u %t \\%r\\ %>s %b \\%{referer}i\\ \\%{user-agent}i\\ %i %o combinedio</ifmodule>## the location and format of the access logfile (common logfile format).# if you do not define any access logfiles within a <virtualhost># container, they will be logged here. contrariwise, if you *do*# define per-<virtualhost> access logfiles, transactions will be# logged therein and *not* in this file.#customlog logs/access.log common## if you prefer a logfile with access, agent, and referer information# (combined logfile format) you can use the following directive.##customlog logs/access.log combined</ifmodule><ifmodule alias_module>## redirect: allows you to tell clients about documents that used to # exist in your server's namespace, but do not anymore. the client # will make a new request for the document at its new location.# example:# redirect permanent /foo <url> alias: maps web paths into filesystem paths and is used to# access content that does not live under the documentroot.# example:# alias /webpath /full/filesystem/path## if you include a trailing / on /webpath then the server will# require it to be present in the url. you will also likely# need to provide a <directory> section to allow access to# the filesystem path.## scriptalias: this controls which directories contain server scripts. # scriptaliases are essentially the same as aliases, except that# documents in the target directory are treated as applications and# run by the server when requested rather than as documents sent to the# client. the same rules about trailing / apply to scriptalias# directives as to alias.#scriptalias /cgi-bin/ c:/apache24/cgi-bin/</ifmodule><ifmodule cgid_module>## scriptsock: on threaded servers, designate the path to the unix# socket used to communicate with the cgi daemon of mod_cgid.##scriptsock cgisock</ifmodule>## c:/users/lotusms/desktop/apache/apache24/cgi-bin should be changed to whatever your scriptaliased# cgi directory exists, if you have that configured.#<directory c:/users/lotusms/desktop/apache/apache24/cgi-bin>allowoverride noneoptions nonerequire all granted</directory><ifmodule mime_module>## typesconfig points to the file containing the list of mappings from# filename extension to mime-type.#typesconfig conf/mime.types## addtype allows you to add to or override the mime configuration# file specified in typesconfig for specific file types.##addtype application/x-gzip .tgz## addencoding allows you to have certain browsers uncompress# information on the fly. note: not all browsers support this.##addencoding x-compress .z#addencoding x-gzip .gz .tgz## if the addencoding directives above are commented-out, then you# probably should define those extensions to indicate media types:#addtype application/x-compress .zaddtype application/x-gzip .gz .tgz## addhandler allows you to map certain file extensions to handlers:# actions unrelated to filetype. these can be either built into the server# or added with the action directive (see below)## to use cgi scripts outside of scriptaliased directories:# (you will also need to add execcgi to the options directive.)##addhandler cgi-script .cgi# for type maps (negotiated resources):#addhandler type-map var## filters allow you to process content before it is sent to the client.## to parse .shtml files for server-side includes (ssi):# (you will also need to add includes to the options directive.)##addtype text/html .shtml#addoutputfilter includes .shtml</ifmodule>## the mod_mime_magic module allows the server to use various hints from the# contents of the file itself to determine its type. the mimemagicfile# directive tells the module where the hint definitions are located.##mimemagicfile conf/magic## customizable error responses come in three flavors:# 1) plain text 2) local redirects 3) external redirects## some examples:#errordocument 500 the server made a boo boo.#errordocument 404 /missing.html#errordocument 404 /cgi-bin/missing_handler.pl#errordocument 402 <url> maxranges: maximum number of ranges in a request before# returning the entire resource, or one of the special# values 'default', 'none' or 'unlimited'.# default setting is to accept 200 ranges.#maxranges unlimited## enablemmap and enablesendfile: on systems that support it, # memory-mapping or the sendfile syscall may be used to deliver# files. this usually improves server performance, but must# be turned off when serving from networked-mounted # filesystems or if support for these functions is otherwise# broken on your system.# defaults: enablemmap on, enablesendfile off##enablemmap off#enablesendfile on# supplemental configuration## the configuration files in the conf/extra/ directory can be # included to add extra features or to modify the default configuration of # the server, or you may simply copy their contents here and change as # necessary.# server-pool management (mpm specific)#include conf/extra/httpd-mpm.conf# multi-language error messages#include conf/extra/httpd-multilang-errordoc.conf# fancy directory listings#include conf/extra/httpd-autoindex.conf# language settings#include conf/extra/httpd-languages.conf# user home directories#include conf/extra/httpd-userdir.conf# real-time info on requests and configuration#include conf/extra/httpd-info.conf# virtual hosts#include conf/extra/httpd-vhosts.conf# local access to the apache http server manual#include conf/extra/httpd-manual.conf# distributed authoring and versioning (webdav)#include conf/extra/httpd-dav.conf# various default settings#include conf/extra/httpd-default.conf# configure mod_proxy_html to understand html4/xhtml1<ifmodule proxy_html_module>include conf/extra/proxy-html.conf</ifmodule># secure (ssl/tls) connections#include conf/extra/httpd-ssl.conf## note: the following must must be present to support# starting without ssl on platforms with no /dev/random equivalent# but a statically compiled-in mod_ssl.#<ifmodule ssl_module>sslrandomseed startup builtinsslrandomseed connect builtin</ifmodule>## uncomment out the below to deal with user agents that deliberately# violate open standards by misusing dnt (dnt *must* be a specific# end-user choice)##<ifmodule setenvif_module>#browsermatch msie 10.0; bad_dnt#</ifmodule>#<ifmodule headers_module>#requestheader unset dnt env=bad_dnt#</ifmodule>all right so that is a lot. i'm sorry but i didn't want to leave anything out.so do you see anything there i can't see?",
    "present_kp": [
      "php",
      "apache",
      "httpd.conf"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "should a web service return an error message if it receives unknown parameters?. i have a web service which allows to retrieve users: <url> it returns a list of users.that service accepts a number of parameters (age, gender) to select which users to retrieve. examples:<url> that service return an error message if the client is passing an unknown parameter? example:>>> get <url> <<< 400 bad request<<< { 'message' : invalid parameter 'cylinders' }(to me it's clear that a web service should validate the parameters it receives, but the case of unknown parameters is not clear to me)",
    "present_kp": [],
    "absent_kp": [
      "code quality",
      "web services",
      "error handling"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "complexity of weighted cycle in a hamiltonian graph. assume a weighted graph g and a positive value k are given. what is the complexity of finding a cycle with total weight k when g is hamiltonian or hamiltonian-connected? pointing to papers and books is also welcome!i wish it wouldnt look as a homework!",
    "present_kp": [],
    "absent_kp": [
      "cc.complexity theory",
      "graph theory"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "silverlight 5 in or out of browser?. i'm deciding which i need for my new lob application. i will list pro's of each and if i'm wrong - correct me please as this is how i see it.in browserusers can navigate using url's. i see it as a big thing. users can share links like app.com/orders/12345with elevated permissions i can do all the same stuff. use com, file system, etc.i can open other browser windows with other functionality if multi-window needed. the only difference - i won't have such control over those windows as with out of browser.out of browserruns out of browser. not sure why i care?more problematic to handle updatesyes, there is windows i can open but not modal. so, in-browser separate ie window almost identicaldo i miss anything? i tend to keep it in browser but i want to see where i'm limiting myself.",
    "present_kp": [
      "silverlight"
    ],
    "absent_kp": [
      "design"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to make sure your company doesn't go underwater if your programmers win the lottery. i have a few programmers under me, they are all doing very great and very smart obviously. thank you very much.but the problem is that each and every one of them is responsible for one core area, which no one else on the team have foggiest idea on what it is. this means that if anyone of them is taken out, my company as a business is dead because they aren't replaceable.i'm thinking about bringing in new programmers to cover them, just in case they are hit by a bus, or resign or whatever. but i afraid thatthe old programmers might actively resist the idea of knowledge transfer, fearing that a backup might reduce their value.i don't have a system to facilitate technology transfer between different developers, so even if i ask them to do it, i've no assurance that they will do it properly.my question is, how to put it to the old programmers in such they would agreewhat are systems that you use, in order to facilitate this kind of backup? i can understand that you can do code review, but is there a simple way to conduct this? i think we are not ready for a full blown, check-in by check-in code review.",
    "present_kp": [
      "knowledge transfer"
    ],
    "absent_kp": [
      "teamwork"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to capture the xtrace output (only) in a file?. i know that i can redirect the xtrace output to some_file with something like this:exec 2 >> some_fileset +x...but this sends to some_file not only the xtrace output, but also any other content originally sent to fd 2, which includes most error messages and warnings, all unrelated to xtrace.is there a way to capture only the xtrace output in some_file?i should add that i'm looking for a way to do this that would distort as little as possible the xtrace output itself, and the timing information gathered through a ps4 setting like, e.g.zmodload zsh/datetimeexport ps4='${(j::)epochtime} %n:%i> '",
    "present_kp": [
      "zsh"
    ],
    "absent_kp": [
      "performance",
      "audit",
      "tracing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "neural network input preprocessing. it's clear that the effectiveness of a neural network depends strongly on the format you give it to work with. you want to pre-process it into the most convenient form you can algorithmically get to, so that the neural network doesn't have to account for that itself.i'm working on a little project that is going to be using neural networks. my future goal is to eventually use neat, which i'm really excited about. anyway, one of my ideas involves moving entities in continuous 2d space, from a top-down perspective (this would be a really cool game ai). of course, unless these guys are blind, they're going to be able to see the world around them.there's a lot of different ways this information could be fed into the network. one interesting but expensive way is to simply render a top-down view of things, with the entities as dots on the picture, and feed that in. i was hoping for something much simpler to use (at least at first), such as a list of the x (maybe 7 or so) nearest entities and their position in relative polar coordinates, orientation, health, etc., but i'm trying to think of the best way to do it.my first instinct was to order them by distance, which would inherently also train the neural network to consider those more important. however, i was thinking- what if there's two entities that are nearly the same distance away? they could easily alternate indexes in that list, confusing the network.my question is, is there a better way of representing this? essentially, the issue is the network needs a good way of keeping track of who's who, while knowing (by being input) relevant information about the list of entities it can see.thanks!",
    "present_kp": [
      "neural networks"
    ],
    "absent_kp": [
      "artificial intelligence"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "resolve dependencies using before-after constraints. what i would like to do is the following.(not sure if this question should go to stackoverflow, or here)consider a datastructure like this:interface iaction { iaction[] afteractions() iaction[] beforeactions() void execute()}what i would like to do is execute() a iaction collection, in the manner, that all beforeactions() are executed before the supplier of the beforeactions() calls result.all afteractions() are executed after the supplier of the afteractions() calls result.be able to detect cyclic dependencies, so if one should execute before and after another, or after itself, or after another action, that on any path should execute before, etc.in other worlds i would like to sort these actions in an executable order.i would like to get only an explanation on how i should (could) do this, i don't expect anyone to implement anything for me (notice, that i intentionally didn't specify a language).if you know about any generic algorithm for this purpose, that you think is suitable for the problem, please feel free to share with a minimal explanation, on how you would translate it to this problemeditafter thinking a lot about the problem, i figured out, that dependency-wise the afteractions() is absolutely useless (an action shouldn't know about what executes after it, it should only describe what it needs to be completed before, to successfully execute). knowing this i ended up doing something like what @rafael eyng suggested:i created an iterator, which wrapped the original collection, and had a next(), which always returned the next (resolved) action. the iterator was not stateless, it had a reference to the collection with all the action, and had a (growing) set of actions, which are resolved. to achieve this, every time next() was called i did the following:iterate on all the actionsif one has no dependencies, or all of them are in the already resolved list, then it can be returned as next.if it has dependencies, then recursively do this for all the prerequisites.i was also collecting the already visited actions, from the root of the recursive call, so cycles could be detected",
    "present_kp": [],
    "absent_kp": [
      "algorithms",
      "dependency management"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is the actual time complexity of gaussian elimination?. in an answer to an earlier question, i mentioned the common but false belief that gaussian elimination runs in $o(n^3)$ time. while it is obvious that the algorithm uses $o(n^3)$ arithmetic operations, careless implementation can create numbers with exponentially many bits. as a simple example, suppose we want to diagonalize the following matrix:$$egin{bmatrix}2 & 0 & 0 & \\cdots & 0 \\1 & 2 & 0 & \\cdots & 0 \\1 & 1 & 2 & \\cdots & 0 \\dots & dots & dots & \\ddots & dots\\1 & 1 & 1 & \\cdots & 2 \\\\end{bmatrix}$$if we use a version of the elimination algorithm without division, which only adds integer multiples of one row to another, and we always pivot on a diagonal entry of the matrix, the output matrix has the vector $(2, 4, 16, 256, \\dots, 2^{2^{n-1}})$ along the diagonal.but what is the actual time complexity of gaussian elimination? most combinatorial optimization authors seem to be happy with strongly polynomial, but i'm curious what the polynomial actually is.a 1967 paper of jack edmonds describes a version of gaussian elimination (possibly due to gauss) that runs in strongly polynomial time. edmonds' key insight is that every entry in every intermediate matrix is the determinant of a minor of the original input matrix. for an $n imes n$ matrix with $m$-bit integer entries, edmonds proves that his algorithm requires integers with at most $o(n(m+\\log n))$ bits. under the reasonable assumption that $m=o(\\log n)$, edmonds' algorithm runs in $o(n^5)$ time if we use textbook integer arithmetic, or in $ ilde{o}(n^4)$ time if we use fft-based multiplication, on a standard integer ram, which can perform $o(\\log n)$-bit arithmetic in constant time. (edmonds didn't do this time analysis; he only claimed that his algorithm is good.)is this still the best analysis known? is there a standard reference that gives a better explicit time bound, or at least a better bound on the required precision?more generally: what is the running time (on the integer ram) of the fastest algorithm known for solving arbitrary systems of linear equations?",
    "present_kp": [],
    "absent_kp": [
      "ds.algorithms",
      "reference request",
      "linear algebra"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "switching to ramdisk root. my application is to do a secure erase on my system disk (an ssd) using hdparmfrom a shell script. so my thoughts were:create a ramdiskcopy the linux os to the ramdiskunmount the original system drivedo a secure erasei created the ramdisk and mounted it, but when i do a pivot_root, it can't find any of the applications. i copied all of /bin and /sbin to the ramdisk.",
    "present_kp": [
      "ramdisk"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "haskell - adjacency matrix. this program is used to solve challenge #140 [intermediate] on /r/dailyprogrammer. the challenge is, given a input set of directed edges, print the corresponding adjacency matrix.here is a sample input:5 50 3 -> 11 -> 22 -> 43 -> 40 -> 0 3and the corresponding solution:1101000100000010100100000note that there can be more than one edge per line - the set of edges generated is then the cartesian product of those nodes. for instance, 0 3 -> 1 would generate the edges [(0, 1), (3, 1)].here is my solution:import data.functionimport data.iximport data.listimport qualified data.set as settype edge = (integer, integer)gengrid :: integer -> set.set edge -> [[bool]]gengrid n edges = (map . map) ('set.member' edges) coordgrid where coordgrid = groupby ((==) 'on' fst) coords coords = range ((0, 0), (n - 1, n - 1))parseedge :: string -> [edge]parseedge s = [(from, to) | from <- outgoing, to <- incoming] where outgoing = map read (takewhile (/= ->) split) incoming = map read (tail $ dropwhile (/= ->) split) split = words sdochallenge :: [string] -> [string]dochallenge s = (map . map) pprint (gengrid n edges) where n = read (head $ words $ head s) edges = set.fromlist $ concatmap parseedge (drop 1 s) pprint true = '1' pprint _ = '0'input :: [string]input = [5 5, 0 3 -> 1, 1 -> 2, 2 -> 4, 3 -> 4, 0 -> 0 3]output :: [string]output = [11010, 00100, 00001, 01001, 00000]main :: io ()main = print $ output == dochallenge inputmy main points of focusreadability of the codehow idiomatic the program isassume that the input is well-formed - it's okay if the io stuff crashes on a bad input.",
    "present_kp": [
      "haskell",
      "matrix"
    ],
    "absent_kp": [
      "programming challenge",
      "graph"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "vi or vim: mac/iterm 2 highlight-paste. when editing with vim on macbook pro with iterm2 interface i can copy text byhighlighting text with left-button double click on mouse,paste text by pressing center wheel on mouse.instead of (2) i would much prefer pasting the text by right-clicking the mouse button.a related problem is that when highlighting the text in one window and pasting into another, the active window does not switch from window 1 to window 2; the active window remains window 1 but i would much prefer a switch to window 2.is there a way to fix these two problems?",
    "present_kp": [
      "iterm2"
    ],
    "absent_kp": [
      "cut copy paste",
      "macos"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is finite precision? why is finite precision a problem in machine learning?. can you explain what is finite precision? why is finite precision a problem in machine learning?",
    "present_kp": [
      "machine learning"
    ],
    "absent_kp": [
      "predictive modeling"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what does the term closed expression mean?. in the field of logic systems there is a property for reasoning algorithms called incompleteness or incompletion. in this context the phrase any closed expression that is not derivable inside the same system appeared. my question is what means closed expression that is not derivable.",
    "present_kp": [
      "algorithm",
      "logic",
      "reasoning"
    ],
    "absent_kp": [
      "terminology"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "can i find out who filed a dmca request with adsense against my genuine website?. so basically adsense reported today that two of my pages have been dmca and ads for those two pages are disabled.now i think my pages are genuine, i wrote the text my self without plagiarism: it's a software review where i provide the link to the original software house.is there any way to know who filed the dmca request? if it's a my competitor i would fill the counter dmca request.",
    "present_kp": [
      "dmca"
    ],
    "absent_kp": [
      "google adsense",
      "content",
      "google adsense policies"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "including extra headers that i do not really need. i always wondered what are the effects of including not needed headers to the final executable once compiled. in my code some times i may included many different headers that i do not need (or i used to need but now i don't and forgot to remove them). now before compiling the final executable i always clean up my header files. but i am wondering, is there a problem if i don't? i am aware that in c# having unused using in my code only reduces (slightly) the speed of intellisense and the compiler only includes stuff that is really needed. is the same thing true for c and c++? what about java and other languages?",
    "present_kp": [
      "java",
      "c++",
      "c",
      "speed"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "software for managing ads on site. could you recommend software to manage ads on a website? i'm talking about self managed banners, not adwords or anything like that. what are some of the commonly used ones?thanks",
    "present_kp": [],
    "absent_kp": [
      "advertising"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "running script on gps fix. i am working on an embedded device which is designed to capture images and i want to make it time and location aware. the device is currently a raspberry pi 2 running debian jessie.i have gps set up through the hardware uart, and that works well, also updating the ntp for when the device is offline.i would like to minimize the user input though, and with that in mind i found a python library which uses shapefiles to take a given latitude and longitude and return the approximate timezone that location would fall into.since the device is turned off between locations, i would like to try and run a script when the device is started up and a gps fix is established.with ethernet there is the ifup system, is there such a thing with gpsd that can be set to run a python script when a gps fix is first established?many thanks",
    "present_kp": [
      "python",
      "gpsd"
    ],
    "absent_kp": [
      "linux"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "searching for someone who may have deactivated their account. when you search a name on facebook, and that person's name appears on their friends' posts (and you click on the name of the person you are searching for on those posts), it takes you to their (former) page with no information on it other than their profile picture. does that mean that they only allow friends to search them and see their postings or does that mean that the account is deactivated?",
    "present_kp": [
      "facebook"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "cannot connect to ssh from crontab. i can create ssh connection from the command line with this command(without password):ssh -r 9900:localhost:22 user@hosti need to run this command whenever the system reboots, so i put it in a bash script and added this line to crontab:@reboot /home/me/script.sh > outputbut in output i get this error: ssh: could not resolve hostname myhost: name or service not knowni think it's because this script starts before internet connection is established. i put a sleep(180) before the ssh command, but it didn't help. any suggestion?",
    "present_kp": [
      "ssh"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to get a list of applications associated with a file using command line. file associations usually are made in the desktop environment, but how to get a list of applications associated with a file using command line?something like:$ getassoc foo.pdf <cr>$ acroread, okulardoes not need to be a command, can a cat + grep on any gnome filei'm using gnome 2.28.2. a response with other environments would help but not solve.",
    "present_kp": [
      "command line"
    ],
    "absent_kp": [
      "files"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "batch insert a group of elements into a sorted list and get their indices. i'm working on an update process which inserts items into a sorted list, and processes those items' indices in the sorted list. to help me with this, i created a sorted list with an insert method which returns the index of the new element in the list. for example, given a list of [1, 3, 5], then insert(4) will change the list to [1, 3, 4, 5] and return 2 (0-indexed). i then process the result with process(2).currently, when i need to do multiple updates, i do something likefor el in elements index = list.insert(el) beginupdates() process(index) endupdates()in want to optimise update time by processing multiple indices in a single update. of course, the following would be incorrect:indices = []for el in elements indices.append(list.insert(el))beginupdates()for index in indices process(index)endupdates()because each insert could invalidate a previously-calculated index. for example:list = [1, 3, 5]index1 = list.insert(4) // index of 4: 2index2 = list.insert(2) // index of 4: 3now, if i process(index1), it will process the wrong index.so i think that, in the same way that i can batch-process indices, i should have a method to batch-insert elements and get their correct indices. something that will allow me to doindices = list.batchinsert(elements)beginupdates()for index in indices process(index)endupdates()is there an algorithm that can elegantly do this?",
    "present_kp": [],
    "absent_kp": [
      "algorithms",
      "optimization",
      "sorting"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "clojure tictactoe (logic, no gui). i whipped this up last night and was wondering if anyone had any thoughts/comments on it; for example, on:code organisationcoding styleother improvementssemantic errors(ns tictactoe.core)(comment author: liam goodacre date: 03/06/12 this module allows the creation and progression of tictactoe games. to create a game, use the make-game function: (make-game) a game is a map with the following data: :play-state :playing | :player1 | :player2 | :draw :level-state [:player1|:player2|nil] :scores {:player1 int, :player2 int, :draw int} :turn :player1 | :player2 | :none if the game isn't over, play-state is :playing. if play-state is :player1 or :player2, this indicates that that player won. if play-state is :draw, then the game completed with no winner. level-state is a vector or nine elements. an empty cell has the value of nil. if a player uses a cell, then the cell has that player's id. e.g., if a player1 uses an empty cell, that cell now has the value :player1 scores is a map from ids to integers. it includes score values for draws and for the two players. when the game is over, turn is :none. otherwise, it describes which player's turn it is. when a new game is created, turn is randomised between the two players. to play a turn, use the play-turn command. for example, assuming 'g' is a predefined game: (play-turn g 2) will produce a new game state with play progressed by one move, in which the current player used cell 2. calling new-game clears the level data of a game and randomises the initial player turn. scores are the only preserved datum. the following are game analysis functions: get-play-state - what state of play are we in? get-turn - whos turn is it? get-scores - get the scores get-level-state - get the state of the level playing? - are we still playing? game-over? - is the game over? valid-move? - is this a valid move?)(declare ;; public functions ; game transforming make-game new-game play-turn ; game analysing get-play-state get-turn get-scores get-level-state playing? game-over? valid-move? ; extra get-game-grid)(declare ;; private functions ^:private ; transforming apply-move next-turn recalculate-state ; analysis calculate-winner try-combination);; utility functions(defmacro ^:private def- a private version of def. [& info] '(def ^:private ~@info))(defn- within-range determines if a value lies within an inclusive range. [n x] (fn [v] (and (>= v n) (<= v x))));; initial data for making new games(def- players [:player1, :player2])(def- initial-play-state :playing)(def- initial-level-state (vec (repeat 9 nil)))(def- initial-scores {:player1 0, :player2 0, :draw 0})(defn- random-turn [] (players (int (rand 2))))(def- winning-combinations [[1 2 3][4 5 6][7 8 9][1 5 9][3 5 7][1 4 7][2 5 8][3 6 9]]);; public transforming functions(defn make-game [] creates a new game object. { :play-state initial-play-state :level-state initial-level-state :scores initial-scores :turn (random-turn) })(defn new-game [g] sets up a game object for the next play. (assoc (make-game) :scores (get-scores g)))(defn play-turn [game move] progresses game-play by one move if possible. (if (and (playing? game) (valid-move? game move)) (-> game (apply-move move) (recalculate-state) (next-turn)) game));; private transforming functions(defn- apply-move [game move] progresses game-play by one move. (assoc game :level-state (assoc (get-level-state game) move (get-turn game))))(defn- next-turn [game] updates which player should play next. if no player is currently active, then do not switch. (assoc game :turn (case (get-turn game) :none :none :player1 :player2 :player2 :player1)))(defn- recalculate-state [game] calculate if there is a winner. if so update the play-state, turn, and scores; to reflect the result. (let [winner (calculate-winner game) scores (get-scores game)] (if (= winner :none) game (-> game (assoc :play-state winner, :turn :none) (update-in [:scores winner] inc)))));; private analysis functions(defn- calculate-winner [game] calculates if there is a winner. (let [state (get-level-state game) matching (partial try-combination state)] (if (some matching winning-combinations) (get-turn game) (if (some nil? state) :none :draw))))(defn- try-combination [state [one two three]] calculates if a winning combination has occurred. (let [lookup (comp (partial nth state) dec) player (lookup one)] (and (not= player nil) (= player (lookup two) (lookup three)))));; public analysis functions(def get-play-state :play-state)(def get-turn :turn)(def get-scores :scores)(def get-level-state :level-state)(defn playing? [game] (= :playing (get-play-state game)))(def game-over? (comp not playing?))(defn valid-move? [game move] (and ((within-range 0 8) move) (nil? (nth (get-level-state game) move))));; public extra functions(defn get-game-grid [g] builds a simple string representation of the current level state. (clojure.string/join (map vec (partition 3 (get-level-state g)))))",
    "present_kp": [
      "clojure"
    ],
    "absent_kp": [
      "functional programming",
      "tic tac toe",
      "lisp"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why are the laws of an applicative functor defined the way they are?. let's recall the definition of an applicative functor. throughout this question, i write $x: t$ to denote that the value $x$ has type $t$.definition: an applicative functor consists of a type constructor $f: * o *$ together with two operations:a function $\\mathsf{pure}: a o f(a)$ that transforms a base value $x: a$ into a wrapped value $\\mathsf{pure}(x): f(a)$.a binary operation $\\circledast: f(a o b) o f(a) o f(b)$ that applies a wrapped function $f: f(a o b)$ to a wrapped value $x: f(a)$ and produces a wrapped result $f \\circledast x: f(b)$.we require these operations to satisfy the following conditions:wrapped identities apply as identities. $$ \\mathsf{pure}(\\operatorname{id}) \\circledast x = x $$wrapped composition applies as composition. $$ \\mathsf{pure}(\\circ) \\circledast f \\circledast g \\circledast x = f \\circledast g \\circledast x $$ here, $\\circ$ denotes the function composition operator.wrapping distributes across function application. $$ \\mathsf{pure}(f(x)) = \\mathsf{pure}(f) \\circledast \\mathsf{pure}(x) $$wrapping exchanges across application. $$ f \\circledast \\mathsf{pure}(x) = \\mathsf{pure}(g \\mapsto g(x)) \\circledast f $$question: why are these the correct laws for an applicative functor? to be more precise,are these laws known to be logically independent, in the sense that none of these four laws can be derived from the other three?assuming the answer to question 1 is yes, what essential characteristic of applicative functors would be lost if we dropped each law, in turn, from the definition? in other words, for each law $l$, there should exist structures $(f, \\mathsf{pure}, \\circledast)$ satisfying all laws except $l$; why do we not want to call these applicative functors?",
    "present_kp": [],
    "absent_kp": [
      "terminology",
      "functional programming",
      "category theory"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how do i loop over the lines in stdin and run a shell command?. i'd like to run a shell command on each line taken from stdin.in this case, i'd like to run xargs mv. for example, given two lines:mfoo foombar bari'd like to run:xargs mv mfoo fooxargs mv mbar bari've tried the following strategies with ruby, awk, and xargs. however, i'm doing it wrong:just xargs:$ echo mbar bar mbaz baz | xargs mvusage: mv [-f | -i | -n] [-v] source target mv [-f | -i | -n] [-v] source ... directorythrough awk:$ echo mbar bar mbaz baz | awk '{ system(xargs $0) }'through ruby:$ echo mbar bar mbaz baz | ruby -ne ''xargs mv''$ lscat foo mbar mbazi have some questions:how do i do what i'm trying to do?what is wrong with each of my attempts?is there a better way to think about what i'm trying to do?i'm especially confused that my xargs attempt isn't working because the following works:$ echo foo bar | xargs touch$ lsbar foo",
    "present_kp": [
      "awk",
      "xargs"
    ],
    "absent_kp": [
      "bash",
      "command line"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "better way to create samples. i've done this piece of code for creating all possible samples without repetitions, but i think that it is a so heavy algorithm because every time i call this recursive function i create a new vector that is passed to it. what means create any samples without repetitions? it means creating every combination given a population. is there a way for making this algorithm faster?here is the algorithm and the description:as already said, this function creates samples without repetitions recursively. if n, that is the number of elements that should be found, is equals 0 i print the sample.else to the sample that is passed i add the elements of the population that there aren't yet in the combination in a way that there should't be samples that are equals between every samples(i have to create a vector for everyone of this new samples), and i call the function with this new sample and n(the number of element to be found) decremented and incrementing the element from wich i'll start to add.private static void distribuzioneinblocco(int n, int firstelementconsidered, vector<float> population, vector<float> sample){ // exit condition, when n equals 0 the fuction doesn't self-calls if(n==0){ joptionpane.showmessagedialog(null, sample); return; } // for every element of the population the function adds this element // at the previous combination for(int x = firstelementconsidered; x<population.size();x++){ vector<float> aggiunta = new vector<float>(sample); aggiunta.add(population.elementat(x)); distribuzioneinblocco(n-1, x+1, population, aggiunta); }}for example if i give as population the vector 0 2 4 6 and n = 3 the answer will be0 2 40 2 60 4 62 4 6if i give the same population but n = 2 the result will be:0 20 40 62 42 64 6if n = 40 2 4 6moreover is there a way for doing it without recursion?",
    "present_kp": [
      "recursion"
    ],
    "absent_kp": [
      "java",
      "combinatorics"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to approach design to minimize conflicts when distributing individual locations to merchants. one of my potential customers is operating an open market in their town.from what i'm told, they make a decent profit by renting out the individual locations on that market to small merchants who come there every day to sell produce and other goods you normally find in such places.each merchant needs to pay some fixed amount up front to receive a seasonal contract that grants them the right to sell their goods on the market during given season.the contract does not, however, specify at which micro-location the merchant will be stationed, or indeed, on which days they will be present. the only thing the contract grants them, is the right to sell certain types of goods and during which season.for each day that they are actually present on the market, a cashier comes to their spot and collects the daily rent for that spot.now, certain micro-locations have more traffic than others, so there is bound to be some competition. which is good, normally. established merchants wish to retain their spot so their loyal customers can find them.the problem is that the existing mechanism of reservation is basically first-come, first-(re)served. in reality, this means at around 02:00 am, merchants start appearing and placing all their stuff on their sweet spots in order to prevent others from doing the same. at times, competition can be quite literal. things escalated into a physical altercation a couple of times in the past, so an overseer is now required to be present during the night before busiest days of the season.the customer has now asked me to come up with a software-based solution for this problem. an idea for a web and/or mobile app has been tossed around, but whatever the form, it would have to support the following features:merchant authentication,selection of a preferred micro-location for the next day,advance payment of the daily rent via some online method.while i was reading through the requirements, one thing immediately jumped out at me: i cannot possibly do this in an app form! here's what i think it will happen, if i do anyway:in the first days and weeks, merchants that know about and use the app, will have an unfair advantage over merchants that don't. it would be easy to assume this problem will self-correct quickly enough, but it would be a wrong assumption, imho. many of the small merchants are in reality older, computer-illiterate people.imagine a kind, old lady selling flowers, for example. she will never use the app, because it's space technology to her. but she will continue to show up at 02:00 am and happily place her flowers on the same spot as the day before. only to learn 4 hours later that someone else had already reserved and payed for that spot yesterday, from their home, using a computer!how is she to compete with that?but things don't end here... let's assume that, somehow, she does learn to use the app (or gets someone else do do it for her, which is more likely). now, everyone is using the app to reserve their spot. fast forward a couple of weeks, and this is what we get:joe s. merchant logs into the app at 05:10 pm, 10 minutes after the reservations for the next day have opened. to his surprise, almost all the spots have already been taken, including his usual one. now he can either hope someone will not show up so he can take their spot at the last minute, or he can accept he's been too late and he lost a whole business day because of this confounded computer thing!naturally, i've explained my views to the customer... and they agree it's a problem. they wouldn't want to make a bad situation worse just to have a more convenient way of collecting daily rent.we both believe the present system could stand improvement. the basis of present system is competition and that is probably here to stay. the problem is that currently the competition is directly based on physical effort (i.e. how early a person is willing to wake up and travel to the market place to get the spot). this effort is in turn a natural limiting factor on the (un)fairness of the whole competition.by introducing any software substitute, this key factor is effectively gone! it reduces the problem to who is faster in the first minutes when reservations start. there's no real effort involved anymore which automatically appears unfair!so we feel like the rules have to be adjusted somehow. we've tossed around some ideas that could make the online competition more fair. ideas like:1. building a reputation based systemthis one brings in a lot of extra complexity, not to mention responsibility of tracking and assessing the various factors that contribute to reputation score. most of these cannot be automated which means they would incur extra cost to the customer.2. randomizing the start of each day's reservation windowthis one has the potential of lessening the competition pressure in the first minutes of the reservation window, giving it an appearance of being more fair. yet, i fear it would only annoy the merchants.what both me and my customer agree about, is that we absolutely cannot implement a reservation mechanism based on bidding. that would be in violation of financial laws the customer's company must abide by which call for a fixed rent in this situation.i refuse to believe that the best solution is the one that presently exists, even if merchants are used to coming in at 02:00 am and waiting 4 hours for the market to actually open.i turn to this community to ask: how can i earnestly better the present system with a software component? what options am i missing?",
    "present_kp": [],
    "absent_kp": [
      "web applications",
      "concepts"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "openbsd - x window system - no choice on reinstall. i have an amd64 desktop that i currently run a centos client on and today i got inspired to convert it into a bsd machine with a gnome desktop environment. i have been running an open bsd server on this machine prior and as it was an server i choose no on the question do you expect to run the x window system and i have since had several different linux flavours running on the machine and that whats making me a bit confused because now when i ran the open bsd install it skips this question and goes direct to ask me if i want the x window system to be started by xdm. is the information from my initial openbsd install stored somewhere despite the fact that i have reinstalled the machine several times between?i have googled and found answers on how to start the x windows system but nothing on why the install skips the question. as to test this more i downloaded openbsd 5.9 from an official ftp and pushed it to an usb and used that to install openbsd to one of my machines. on the install i choose no on the question to do you expect to use x window on this system. after verifying the install i went ahead and did a clean centos install on the machine. after verifying the centos install i then started an openbsd install on the same machine using the same usb stick as on the first openbsd install but on the second install it skipped the question on expecting to run x window system and went directly to the question on if i want the x window system to be started by xdm. so how come that the installation behaves differently on what should be a clean install ? and on this machine i have previously been running openbsd with the xfce desktop.",
    "present_kp": [
      "desktop",
      "openbsd",
      "bsd"
    ],
    "absent_kp": [
      "system installation",
      "gui"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "ssh does not decrypt private rsa key. i've noticed problems i've been having pushing code to a friend's gitlab. if the rsa key specified for use by ssh is encrypted with a password, ssh fails to ask me to provide the password and subsequently fails to authenticate with the private key.i've tried generating rsa keys by default with a password and and with the optionsssh-keygen -t rsa -b 4096 -o -a 100as well, with no change.i've tried hosting sshd on my own system, setting up the key for my own user (copied the .pub to authorized_keys), same results strangely. permissions for the key doesn't appear to be an issue.i'm running archlinux 64 bit with an unmodified linux kernel.here's the log of a sample ssh -i test -p 9988 -vt 127.0.0.1 on my own system with sshd running with mostly default settings (disabled passwords, only publickey authentication allowed, port is set to 9988). the key is generated with default settings and an encryption password is set on it.openssh_6.8p1, openssl 1.0.2c 12 jun 2015debug1: reading configuration data /home/stephen/.ssh/configdebug1: reading configuration data /etc/ssh/ssh_configdebug1: connecting to 127.0.0.1 [127.0.0.1] port 9988.debug1: connection established.debug1: identity file test type 1debug1: key_load_public: no such file or directorydebug1: identity file test-cert type -1debug1: enabling compatibility mode for protocol 2.0debug1: local version string ssh-2.0-openssh_6.8debug1: remote protocol version 2.0, remote software version openssh_6.8debug1: match: openssh_6.8 pat openssh* compat 0x04000000debug1: ssh2_msg_kexinit sentdebug1: ssh2_msg_kexinit receiveddebug1: kex: server->client aes128-ctr <email> nonedebug1: kex: client->server aes128-ctr <email> nonedebug1: expecting ssh2_msg_kex_ecdh_replydebug1: server host key: ecdsa-sha2-nistp256 sha256:jnykvy5o5p79huka+bwoqlomqb0oo90fuput18erxmadebug1: host '[127.0.0.1]:9988' is known and matches the ecdsa host key.debug1: found key in /home/stephen/.ssh/known_hosts:2debug1: ssh2_msg_newkeys sentdebug1: expecting ssh2_msg_newkeysdebug1: ssh2_msg_newkeys receiveddebug1: roaming not allowed by serverdebug1: ssh2_msg_service_request sentdebug1: ssh2_msg_service_accept receiveddebug1: authentications that can continue: publickeydebug1: next authentication method: publickeydebug1: offering rsa public key: testdebug1: server accepts key: pkalg ssh-rsa blen 279debug1: offering rsa public key: stephen@archdynamodebug1: authentications that can continue: publickeydebug1: no more authentication methods to try.permission denied (publickey).edit: i recently pushed to my friend's gitlab and realized it did so without prompting me for the password of the key. does this mean my ssh-keygen is broken somehow?",
    "present_kp": [
      "ssh"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "i'm not getting syntax coloring in vim (using cygwin). i've been so far unable to figure out why vim's syntax coloring is not working for most files, and it's driving me batty. in case this is relevant: i'm working in cygwin (more specifically/accurately the bash shell that comes with git for windows) because it's the only thing i've been able to smuggle into this windows shop so far. (i sometimes bring in my macbook air but it is somewhat frowned upon. also, as with most windows shops they're paranoid about installing software, so a full cygwin installation would need justification. also, i'm not using gvim because i prefer to work with vim in a terminal.)i can get syntax coloring to work if i tell vim that the file is c++:set syntax=cppor if it is a shell script, but not if i'm working in ruby or php.i've ensured that syntax is turned on with both :syntax on and :syntax enable.i've tried installing vim syntax files in ~/.vim/syntax/<syntax-type>.vim but this has not made a difference.i've ensured files had the standard file extension, so if working with a vagrantfile i tried adding '.rb', and i've tried avoiding my .vimrc file (as well as tried other .vimrc files):vim -u /dev/null vagrantfile.rbi've also ensured ruby files had the normal 'shebang' line.nothing has worked so far.what could prevent vim from properly coloring certain files?",
    "present_kp": [
      "vim",
      "cygwin"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to calculate kl-divergence between matrices. given there are two matrices of dimensionality 100x2 with absolute values ranging from -50 to +50. is it possible to determine the kl-divergence by applying the entropy algorithm from scipy.stats to the two flattened vectors of size 200?",
    "present_kp": [],
    "absent_kp": [
      "scikit learn",
      "tsne"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "are there advanced keyboard shortcuts for google calendar?. i like to do most of my stuff using the keyboard. on gmail, this works quite nicely, but on google calendar, some keyboard shortcuts are missing.for example, i'd like to browse days by using the or keys, to actually navigate in the calendar itself:or, i'd like to be able to create an event on the selected day by pressing another key. the way it currently is, i always have to switch to the mouse to select the day first.is it possible to do this probably with a userscript? i haven't found anything related.",
    "present_kp": [
      "google calendar",
      "keyboard shortcuts"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "creating controls dynamically in the code-behind or viewmodel?. right now i'm working migrating an app i made entirely using code behind to mvvm and had a question on where i'm supposed to be creating controls dynamically.basically i have a web service that returns {#} of items. for each item a button will be created and the item will be assigned to its data context.now so far i know that i should set the command to the viewmodel as well as the command property. also i know i should call those items in my web service inside of the view model (or model, right now its irrelevant to the question). the part that is questionable is where to create the buttons.i didn't really like the idea of creating buttons in my viewmodel since it was, well, related to the view. is this correct or should i be creating them inside the viewmodel and then some how pass them back to the view via messaging?",
    "present_kp": [
      "mvvm"
    ],
    "absent_kp": [
      "silverlight"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to set up a distributed version control that can commit to subversion. we have a trac server setup that works with the svn commits, so we can do things like fixed #183 in the commit messages, and reference the changes involved. right now i have eclipse with subclipse + trac plugins, and it works pretty well.but i don't have internet all the time and it becomes very difficult to commit to an inaccessible svn server.i would like to use some type of local repository for commits, and then push commits all at once, but individually to server. changing svn / trac isn't an option at this time. other developers can touch the svn server directly.is there a way to cache the commits locally, and then send them when i have internet again? take in mind that i can't just do one big commit of all my changes because it makes it a nightmare since i can't selectively merge.basically is there a way to do this with git/bzr/mercurialand still use svn/trac as intended?i'd be willing to do some scripting, but i don't know where to start.",
    "present_kp": [
      "version control",
      "eclipse"
    ],
    "absent_kp": [
      "distributed development"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "descriptive message services. i have debian 8 jessie installed on my laptop, before when i had a debian 7 wheezy, the system startup and shutdown showing descriptive messages such as these:now on debian 8 jessie, when i start the system only see this:sometimes it displays messages as to wheezy. but in 95% displays as above.when i restarting the services does not show me the messages successfully:as i can make my debian 8 to be more descriptive when my system startup|shutdown and when start|stop|restart the services.is this related to systemd?",
    "present_kp": [
      "debian",
      "systemd"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "understanding the sipser-gacs-lautemann theorem. the class $bpp$ contains all the languages decided by a probabilistic turing machine in polynomial time with probability of success more that 2/3 for every input.the class $\\sigma^p_2$ contains all the languages for which there is a polinomial time turing machine $m$ and a plynomial function $q : \\mathbb{n} ightarrow \\mathbb{n}$ such that:$$ x \\in l \\iff \\exists u \\in \\{0,1\\}^{q(|x|)} orall v \\in \\{ 0,1 \\}^{q(|x|)} m(u,x,v)=1$$define $\\pi^p_i=\\{ar{l} : l \\in \\sigma^p_2 \\}$the theorem states that the class $bpp$ is contained by the intersection of $\\sigma^p_2$ and $\\pi^p_2$.to prove the theorem it is proved that for every set $s \\subseteq \\{0,1\\}^m$ with $|s| \\leq 2^{m-n}$ and every k vectors $u_1, \\ldots, u_k$$$igcup_{i=1}^k(s+u_i) eq \\{0,1\\}^m$$where $s+u = \\{ x+u : x \\in s \\}$ and + denotes addition modulo 2 i.e. bitwise xor.it is also proved that for every set $s \\subseteq \\{0,1\\}^m$ with $|s| \\geq (1-2^{-n})2^m$ and every k vectors $u_1, \\ldots, u_k$$$igcup_{i=1}^k(s+u_i) = \\{0,1\\}^m$$i don't get why this claims imply that if a language is in $bpp$, then$$ \\exists u_1, \\ldots,u_k \\in \\{ 0,1 \\}^m orall r \\in \\{ 0,1 \\}^m igvee_{i=1}^k m(x,r \\oplus u_i) = 1 $$how does the claims about sets of binary strings imply the computation above?what i don't understand is how the translations preserve the original random strings and how can a small set of translates cover all possible random strings.",
    "present_kp": [
      "polynomial time"
    ],
    "absent_kp": [
      "complexity theory",
      "complexity classes",
      "probabilistic algorithms"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "google webmaster central channel. since there are so many google webmaster central videos on youtube which one do you think every webmaster should see?<url>",
    "present_kp": [
      "google"
    ],
    "absent_kp": [
      "seo"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why do large it projects tend to fail or have big cost/schedule overruns?. i always read about large scale transformation or integration project that are total or almost total disaster. even if they somehow manage to succeed the cost and schedule blow out is enormous. what is the real reason behind large projects being more prone to failure. can agile be used in these sort of projects or traditional approach is still the best. one example from australia is the queensland payroll project where they changed test success criteria to deliver the project.see some more failed projects in this so question (on wayback machine)have you got any personal experience to share?",
    "present_kp": [
      "project"
    ],
    "absent_kp": [
      "project management"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to killall the process created by upstart service which is killed (not stopped). upstart service is responsible for creating a gearman workers which run in parallel as number of cpus with the help of gnu-parallel. to understand the problem you can read my stackoverflow post which describes how to run workers in parallel.https://stackoverflow.com/q/451<phone>upstart service: workon.conf# workondescription worker loadstart on runlevel [2345]stop on runlevel [!2345]respawnscript exec seq <phone> | parallel -n0 --joblog out.log ./workerend scriptoright. so above service is started $ sudo service workon startworkon start/running, process 46204620 is the process id of service workon.4 workers will be spawned as per cpu cores. for example.___________________name | pidworker 1011worker 1012worker 1013worker 1014perl 1000perl is the process which is running gnu-parallel.and, gnu-parallel is responsible for running parallel worker processes.now, the problem is.if i kill the workon service.$ sudo kill 4620the service has instruction to re-spawn if killed so it restarts. but, the processes created by the service are not killed. which means it creates a new set of processes. now we have 2 perl and 8 workers.name | pidworker 1011worker 1012worker 1013worker 1014worker 2011worker 2012worker 2013worker 2014perl 1000perl 2000if you ask me, the old process which abandon by service, are they zombies?well, the answer is no. they are alive cuz i tested them. everytime the service dies it creates a new set.well, this is one problem. another problem is with the gnu-parallel.lets say i started the service as fresh. service is running good.i ran this command to kill the gnu-parallel, i.e. perl $ sudo kill 1000this doesn't kill the workers,and they again left without any parent. but, the workon service intercept the death of perl and respawn a new set of workers. this time we have 1 perl and 8 workers. all 8 workers are alive. 4 of them with parent and 4 are orphan.now, how do i solve this problem? i want kill all processes created by the service whenever it crashes.",
    "present_kp": [
      "upstart"
    ],
    "absent_kp": [
      "services",
      "parallelism",
      "gnu parallel"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is this documented behavior for bsd awk, or a bug?. apparently, a bracket expression in bsd awk which contains a character class will ignore any further characters after the character class:macos $ cat file.txt_-.ab8:;@~,macos $ awk '/[@~.[:alnum:]:;-]/' file.txt .ab8@~macos $ awk '/[-;:@~.[:alnum:]]/' file.txt -.ab8:;@~macos $ awk '/[^@~.[:alnum:]:;-]/' file.txt _-:;,macos $ awk '/[^-;:@~.[:alnum:]]/' file.txt _,macos $ on gnu awk (shown on ubuntu 16.04), the behavior is different; other characters in the bracket expression are handled the same regardless of whether they come before or after the character class:linux $ cat file.txt_-.ab8:;@~,linux $ awk '/[@~.[:alnum:]:;-]/' file.txt -.ab8:;@~linux $ awk '/[-;:@~.[:alnum:]]/' file.txt -.ab8:;@~linux $ awk '/[^@~.[:alnum:]:;-]/' file.txt _,linux $ awk '/[^-;:@~.[:alnum:]]/' file.txt _,linux $ is this documented anywhere? or, if it is a bug, is it a known bug? (and if it is a known bug, is it fixed in later versions of awk?)what should i do with this discovery? is there somewhere i should open a bug report?",
    "present_kp": [
      "awk",
      "bsd"
    ],
    "absent_kp": [
      "osx",
      "bugs"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is the linq version faster than the foreach one?. since i don't know how linq works under the hood, i can't decide what version is best to use in term of rapidity of execution. i've done some testing with my testing data (point cloud) but i can't see a clear difference between the 2. the only thing i know is that the real life data will be a larger point cloud so my guess is that the linq would be faster but this is only if linq doesn't do a for each under the hood. if it's the case, the 2 functions would be the same. what is your advice?by the way, cylindre is a 3d cylinder and i want to know which point are inside.version 1 without linqfor (int i = 0; i < fpc.vertices.length; i++){ if (cylindre.ispointinside(fpc.vertices[i])) listpoint.add(fpc.vertices[i]);}version 2, with linqvar insidepoint = from pt1 in fpc.vertices where cylindre.ispointinside(pt1) select pt1; foreach (point3d pt2 in insidepoint) { listpoint.add(pt2); }",
    "present_kp": [
      "linq"
    ],
    "absent_kp": [
      "c#",
      "performance",
      "comparative review"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "tightest upper bound on binary search tree insertion?. the upper bound on the runtime of binary search tree insertion algorithm is o(n) which is if it is not balancedwhat will be the tighter upper bound on this,will it become o(logn)i have read that tighter upper and lower bounds are often equivalent to the theta notation.",
    "present_kp": [],
    "absent_kp": [
      "complexity theory",
      "algorithm analysis",
      "time complexity",
      "runtime analysis",
      "binary trees"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "implement rpc via sip. look at the following use case.i have a client (java) application, which wants to get/set the state of another, remote application (c). the communication between them is done via sip, which is run in another thread.the sip interface can do the following:sendmessageonrequesti have two ideas for the architecture:rpc (json-rpc)define a class which does the marshalling/unmarshalling for jsonrpcrequests and jsonrpcresponse (<url>) define a invoker class, which has something like a call(server, name, arguments) method.in the invoker class, the name and arguments are put into a jsonrpcrequest and sent via the sip layer sendmessagenow comes my problem. how do i actually get the right back to the caller? the control flow is now:the onrequest method is called, but i do now know whether it is the answer to my previous call. what i do is putting all responses reaching my server into a map, and just poll that list in the invoker.a rough sketch might be;invoker (provides api to client)class invoker { private channel channel; public invoker(channel channel) { this.channel = channel; } public object call(string server, string name, object .. args) { jsonrpcrequest req = ...; channel.sendmessage(server, req.tostring()); while( ! channel.hasresponse(req.id()) { thread.sleep(42); } return channel.getresponse(req.id()).result(); }}channel (interface to messenger):class channel { private map<object, jsonrpcresponse> responses = new //; private sip sip = new sip() { public void onrequest(string msg) { jsonrpcresponse response = jsonrpcresponse.parse(msg); responses.put(msg.id(), response); } }; public void sendmessage(string server, string message) { sip.sendmessage(); } public boolean hasresponse onrequest(object id) { responses.haskey(id); } public jsonrpcresponse getresponse(object id) { responses.get(id); responses.delete(id); }}sip (messenger itself):abstract class sip { public void sendmessage(string msg) { // sip magic } public abstract void onrequest(string msg); }is there a better way to do that? my biggest problems/code smells are:the blocking in invoker the protocol is in invoker, maybe i want to switch marshalling to something elsethe map as mean to get the correct response for a requestthe sip abstract method looks strangeno error handlingno timeoutmessage passingis there an easy way to get rid of rpc, and implement something like rpc with just message passing? any hints for pattern are welcome. i do not need the code itself, i am totally fine with just architecture. i tried to google for message passing implementations, and how they actually change state with it, but i did not find anything useful. how to implement timeout/ error handling?any good books/literature on that topic is also welcome, as i never programmed such distributed stuff.any other ideas on which protocol to use inside sip to change state is welcome, too, as rpc was my initial thought, and i did not find anything other useful.the code will not compile, i guess, it was just to visualize my idea.",
    "present_kp": [
      "java",
      "rpc",
      "message passing"
    ],
    "absent_kp": [
      "design",
      "design patterns"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why does decreasing the gap size in shell sort never undo previous sorts?. i came across this sentence in the wikipedia article on shell short, which states that if the file is then k-sorted for some smaller integer k, then the file remains h-sorted.i've seen this claim in other texts like lafore as well, but i can't wrap my head around why it's true. of course i can't find a counterexample, but why must it be so?",
    "present_kp": [],
    "absent_kp": [
      "algorithms",
      "sorting"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "socket on a webserver. so what i know about socket is that a socket is an end point of a connection for a process, hence 1 socket on a host binds to an ip and a unique port number for each connection.but a webserver (by default use port 80) to listen for connections coming in from multiple clients. my question is:does that mean a single socket on the server is listening to multiple clients simultaneously? this would conflict with my understanding of socketcould someone please shed some light on this topic?",
    "present_kp": [
      "server"
    ],
    "absent_kp": [
      "sockets",
      "server side",
      "websockets",
      "web servers"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "populating a property grid in mfc. we have some code to populate a cmfcpropertygridctrl grid with controls that is based on copied samples. we are currently using visual studio 2012 but intend to upgrade to vs2015 soon. a typical excerpt is:void cpropgridprops::addpropertiesjoint(const bool bisdataeditable, const cjointdefault& ent){ cmfcpropertygridproperty* pprop; { // entity std::auto_ptr<cmfcpropertygridproperty> apent(new cmfcpropertygridproperty(_t(joint))); { // master pprop = new cmfcpropertygridproperty(_t(master node), (_variant_t) (long) ent.master, _t(joint master node), joint_master); pprop->allowedit(bisdataeditable); pprop->enable(bisdataeditable); apent->addsubitem(pprop); } { // direction cstring csdir[6] = { lx,ly,lz,lxx,lyy,lzz }; for (int j=0; j<6; ++j) { cmfcpropertygridproperty* pboolprop = new cmfcpropertygridproperty(_t(direction + csdir[j]), colevariant(), _t(joint direction + csdir[j]), joint_direction_x + j); pboolprop->allowedit(bisdataeditable); pboolprop->enable(bisdataeditable); apent->addsubitem(pboolprop); } } m_ppropgrid->addproperty(apent.release(), false, false); }}the routines addproperty and addsubitem, which both take ownership of the pointers, take either auto_ptr or simple pointers.the smart pointers are auto_ptr where unique_ptr may be better.there is an awkward double cast (_variant_t) (long).the code makes no checks for addproperty and addsubitem failing, not even a call of verify().",
    "present_kp": [],
    "absent_kp": [
      "c++",
      "windows"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "403 when query string parameter contains an encoded url. i'm using shared hosting with minimal access to any configuration settings.i've created a more or less empty html file, emptypage.html, with the following contents:<!doctype html><html><body></body></html>when i pass an encoded url as a query string parameter, i get a 403 error from the server.example url:<url> if the encoded url is for the same domain, the page displays normally.example url:<url> if the 'h' from the beginning of the url is removed, the page displays normally.example url:<url> this be the result of some web server setting that i may be able to override in my local .htaccess file? my web host uses apache (not sure which version).note: i asked a similar question earlier but it was not well understood and was put on hold. i've since done more research to figure out what exact conditions are yielding this issue.",
    "present_kp": [
      "htaccess",
      "apache"
    ],
    "absent_kp": [
      "403 forbidden"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "interpretation of goals relative to visits and pageviews in google analytics. i am stumped on some of the data i am seeing. the image below summarizes the data that i am pulling from the api into tableau.in ga, we have the url goal setup for the page shown. i am confused on how the following situation exists:how can 121 visits generate 483 pageviews and 420 goals (unique pageviews)?the way that i interpret the results:the url shown was seen over 121 visits but viewed 420 unique times. i simply don't get how that is possible.",
    "present_kp": [
      "google analytics"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "what is the percentage of users that block cookies in their web browser?. what is the percentage of web users that use either cookie blocking software for their web browsers or software that blocks google analytics tracking?",
    "present_kp": [
      "google analytics",
      "cookie"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "n-dimensional array generic methods. i asked a similar question yesterday and got a brilliant answer, but unsure how i could adapt these methods.the methods perform a bfs on a state and then store the values in a multi-dimensional array.you can see that the code is repeating itself, just with different array dimensions.method 1: public void bfs2(int[][][] three, pdb15tile.state s, int[] storednums, string filename) throws ioexception { queue<pdb15tile.state> q = new linkedlist<>(); int[] positions = new int[storednums.length]; for (int i = 0; i < storednums.length; i++) { positions[i] = getpatternposition(storednums[i], s.getstate()); } three[positions[0]][positions[1]][positions[2]] = 0; q.add(s); pdb15tile.state current; while (!q.isempty()) { current = q.poll(); // system.out.println(arrays.tostring(current.getstate())); for (pdb15tile.state neighbour : current.findneighbours2()) { // system.out.println(arrays.tostring(neighbour.getstate())); for (int i = 0; i < storednums.length; i++) { positions[i] = getpatternposition(storednums[i], neighbour.getstate()); } if (three[positions[0]][positions[1]][positions[2]] == 0) { three[positions[0]][positions[1]][positions[2]] = neighbour.geth(); q.add(neighbour); } } } serializearraytofile(three, filename); }method 2: public void bfs2(int[][][][][][] six, pdb15tile.state s, int[] storednums, string filename) throws ioexception { queue<pdb15tile.state> q = new linkedlist<>(); int[] positions = new int[storednums.length]; for (int i = 0; i < storednums.length; i++) { positions[i] = getpatternposition(storednums[i], s.getstate()); } six[positions[0]][positions[1]][positions[2]][positions[3]][positions[4]][positions[5]] = 0; q.add(s); pdb15tile.state current; while (!q.isempty()) { current = q.poll(); // system.out.println(arrays.tostring(current.getstate())); for (pdb15tile.state neighbour : current.findneighbours2()) { // system.out.println(arrays.tostring(neighbour.getstate())); for (int i = 0; i < storednums.length; i++) { positions[i] = getpatternposition(storednums[i], neighbour.getstate()); } //system.out.println(six[positions[0]][positions[1]][positions[2]][positions[3]][positions[4]][positions[5]]); if (six[positions[0]][positions[1]][positions[2]][positions[3]][positions[4]][positions[5]] == 0) { six[positions[0]][positions[1]][positions[2]][positions[3]][positions[4]][positions[5]] = neighbour.geth(); q.add(neighbour); } } } serializearraytofile(six, filename); }how do i determine the number of dimensions of the object that is passed in and then manipulate the array accordingly?",
    "present_kp": [],
    "absent_kp": [
      "java"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "expected behavior of fft. i am debugging an fft. my fft uses complex numbers in polar form, $(r, heta)$. i am trying to establish that it works, with a unit test. my real values are all correct, after a polynomial multiplication... however, i pad out my polynomials to a power of two with complex zeros, and when the multiplication is complete, i have random angle values in the polar form of the complex numbers. for instance, i multiply the following polynomials, listed in standard form by their coefficients: $$a = \\lbrace1,2,3 brace\\b = \\lbrace4,5,6 brace\\$$and i am expecting the answer to be: $$c = \\lbrace (4,0),(13,0),(28,0),(27,0),(18,0),(0,0),(0,0),(0,0) brace$$however, my fft returns this exactly (ignoring floating point errors), except the last three terms are: $$c = \\lbrace \\ldots,(0,4.518),(0,0.4769),(0,2.2877) brace$$my question is whether these angle values in the extra terms are expected, or whether this is a sign of something going wrong in my fft code?(this is tricky for me, because it seems like everything is calculating correctly; and shouldn't it not work at all in a d&c if something is wrong?)",
    "present_kp": [],
    "absent_kp": [
      "fourier transform"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "awk: set desired number of columns?. i'm parsing a simple conf file of the following format:key = valueto do this, i use awk likekey=$(awk -f' *= *' '/^key/{print $2}' file.conf)however, some values might containt = symbol, so what i want is basically to tell awk that i need only two columns, i.e. force it to accept everything it encounters after the first separator as one column. i didn't find such option. how can i do that?",
    "present_kp": [
      "awk"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "removing grub from windows 10 + zorin dual boot. i'm having some trouble removing grub from my dual boot laptop!i can't boot from the usb windows media to access the terminal to type in the boot rec /fixmbr etc command.it seems to be that grub is some how overriding my uefi bios settings, because my boot order is usb first, but it still won't boot from usb.when installing zorin on my computer i didn't turn secure boot off, so on the grub menu screen, the option for windows is windows boot manager and not windows 10. this could be causing some problems.i also held shift down and clicked start to launch the recovery settings on windows. i ran the command on the terminal in there, but it didn't work.",
    "present_kp": [
      "windows",
      "dual boot",
      "grub",
      "zorin"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "would the milgram experiment results be replicated in eastern cultures?. when the milgram experiment was performed in the 1960s and replicated multiple times up until the 1980s, it was performed in western cultures. when it was replicated in 2006, it was again performed in the usa. one of the interesting observations that was made, was that the subjects in most cases, agreed to continue the shock treatments if they were reassured that they cannot be held responsible for anything that happens and cannot be sued*. this to me, sounds to be an individualistic reaction to the experimenter's command as opposed to a collectivistic response where subjects would not only try to protect their self interests but would also act to protect the confederate being shocked from ill treatment**. is this an accurate assertion?as we know, western cultures are known to emphasize on individualism whereas eastern cultures are known for inculcating collectivism. so, would the experimental results differ if the experiment were replicated today in an eastern culture (japan, for example)?have any such studies been conducted in the east? *mentioned in burger, jerry m. (2008). replicating milgram: would people still obey today? and also a video clip was featured on abc newss january3, 2007, broadcast of primetime.**personal argument/reasoning",
    "present_kp": [],
    "absent_kp": [
      "social psychology",
      "cross cultural psychology"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why does while [ 0 ] go into infinite loop?. i see the same behaviour for below loop as the loop with while [ 1 ]. why is that so?while [ 0 ]; do echo hellodone",
    "present_kp": [],
    "absent_kp": [
      "bash",
      "shell",
      "test"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "are cognitive maps still widely studied within psychology? why or why not?. cognitive maps refer to mental representations of physical spaces. it seems like the work on this concept dropped off significantly in the 1990s but im not in the field, so i cant say for sure. are they still relevant for use in studies in psychology?",
    "present_kp": [],
    "absent_kp": [
      "cognitive psychology",
      "cognitive modeling"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "converting embedded json into flat table. i am new python user, who decided to use python to create simple application that allows for converting json files into flat table and saving the output in cvs format. i was wondering if you could give me some advice how i could improve my code to make it work in more efficient way. i am asking since if i convert relatively small files everything works just fine but when i try to convert ~200 mb file it starts to take a while. i am afraid that when i begin to work with bigger files it might take quite some time to convert my datasets.here is my code, which i created with help of this great blog post about flattening json objects :import sys, os, json, tkfiledialog, tkmessageboxfrom tkinter import *from pandas.io.json import json_normalizedef openfile(): currdir = os.getcwd() filename = tkfiledialog.askopenfilename( initialdir = currdir, title='please select a file', filetypes=[('json file','.json')]) return filenamedef loading_file(path): #file path file_path = path #loading json file json_data = open(file_path) data = json.load(json_data) return data#function that recursively extracts values out of the object into a flattened dictionarydef flatten_json(data): flat = [] #list of flat dictionaries def flatten(y): out = {} def flatten2(x, name=''): if type(x) is dict: for a in x: if a == name: flatten2(x[value], name + x[a] + '_') else: flatten2(x[a], name + a + '_') elif type(x) is list: for a in x: flatten2(a, name + '_') else: out[name[:-1]] = x flatten2(y) return out#loop needed to flatten multiple objects for i in range(len(data)): flat.append(flatten(data[i]).copy()) return json_normalize(flat)#outputing normalized data into csvdef csv_out(data, path): #creating csv file name name = '~/desktop/' + os.path.basename(os.path.splitext(path)[0]) + '.csv' #converting to the csv data.to_csv(name, encoding='utf-8') #'~/desktop/out.csv'def done(): tkmessagebox.showinfo('json2csv',done!)def main(): filepath = openfile() data_file = loading_file(filepath) table = flatten_json(data_file) csv_out(table, filepath) done()### application interface ###tk = tk()#creating window:tk.geometry('250x150+600+300')tk.title('json2csv')#creating convert buttonconvertbutton = button(tk, text = 'convert to .csv', command = main)convertbutton.place(x = 25, y = 50)tk.mainloop()here you will find short example of the json structure i work with:[{ _id: { id: 123 }, device: { browser: safari, category: d, os: mac }, exid: { $oid: 123 }, extreme: false, geo: { city: london, country: united kingdom, countrycode: uk, ip: 00.000.000.0 }, viewed: { $date: 2011-02-12 }, attributes: [{ name: gender, numeric: 0, value: 0 }, { name: email, value: false }], change: [{ id: { $id: 1231 }, seen: [{ $date: 2011-02-12 }] }]}, { _id: { id: 456 }, device: { browser: chrome 47, category: d, os: windows }, exid: { $oid: 345 }, extreme: false, geo: { city: berlin, country: germany, countrycode: de, ip: 00.000.000.0 }, viewed: { $date: 2011-05-12 }, attributes: [{ name: gender, numeric: 1, value: 1 }, { name: email, value: true }], change: [{ id: { $id: 1231 }, seen: [{ $date: 2011-02-12 }] }]}]",
    "present_kp": [
      "python",
      "json",
      "csv"
    ],
    "absent_kp": [
      "beginner",
      "python 2.7"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "should i keep parenthesis information while source modeling?. i'm developing an ast model for a source code reverse engineering platform. my question is, should i keep parenthesis data in my ast? because i have everything in a tree, i already know which operation will be executed first.",
    "present_kp": [
      "ast"
    ],
    "absent_kp": [
      "static analysis",
      "code modeling"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is it smart to use an api on the same server as your web application. im new to restful api's and would like to find some info out. currently we run a daily digest of ehealth material for the public to read and get up to date content regarding that topic. which is powered by laravel 5.2 and a mysql database. we have a linux based server which is hosted with an isp currently runs 8gb ram, core i7 processor.my boss has suggested we start using api's as the middleman between our apps as we have moved over to mobile and our readership is growing as we now support users which have access to a wide variety of functions.my question is: is it smart to run your web application and api on the same server. i suggested renting another server, moving our database across to it and then building the api to sit there and serve content. however my boss has advised that another server is out of question due to cost implications. my thoughts as a developer is why build an api on the same server if the web app is already connecting via the same server to the db and querying the content. it just seems like additional work. any thoughts would be great.",
    "present_kp": [
      "api"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "does getting top marks and ranks in school or college make one intelligent?. please do not discuses about iq tests that have problems like: 2,4,8,16,... what is the next number? having the answer as 32, because one can relate any number of random numbers mathematically! whenever i ask this question to someone they would just say: yes! getting top marks and ranks in school or college does make one intelligent because they could just score a top rank in iq tests!this paradox about such iq tests not being correct can obviously be reasoned, but are top rank holders in proper subjects at school or college more intelligent than the rest of the students?i agree upon the fact that there can be diverse definitions for the word intelligent. the definition i use for the term intelligent is: a person who has the ability to learn any kind of subject that involves reasoning and extend his abilities to be able to develop that subject**; time frame is not a problem in my definition. to be elegant, i believe that one who has the ability to learn and research on any subject is intelligent. i do not consider people who could do stuff any computer program can do, as the existence of an algorithm to solve that task makes that task less imaginative but could involve high calculation ability (a weight lifter is different from a martial artist) . scientists haven't achieved human level intelligence in machines and that makes me set machines as my reference point for intelligence. please provide references enough to convince me and others on your answer.*lets imagine relating a series 2,4,8,16,111; to do this we would first have to define a 5th degree polynomial with unknown coefficient values. equate: $f(1) = 2, f(2) = 4,...f(5) =111$. its obviously possible to solve and get 4 values for each coefficients and the rest 2 coefficients end up being linearly related to each other (substitute any value other than zero for one and get the other). so hence you have a polynomial relating 2,4,8,16,111! **when i say develop that subject i do not mean writing books and teaching, but i refer to the method of using one's creativity to enlighten themselves on the deepest regions of that subject. p.s. also please suggest the best possible ways to measure one's intelligence. update:could you also please explain why we rely on tests that do not have defined answers to its questions for measuring one's iq? whenever i see a question that first gives a series like 2, 4, 6, 8. ? and ask for the missing number, i would be tempted to write 15356346546434 instead of 10 and form a polynomial relation that expresses the term in the series given its term number (like $f(n)$ would express the $n^{th}$ term). i have also seen iq question papers with questions that has a star drawn on with random numbers displayed within each blank loop we find within the star and a question mark in one of the blank loops which asks for the answer. even they have arbitrary answers.",
    "present_kp": [],
    "absent_kp": [
      "social psychology",
      "learning",
      "measurement",
      "reference request"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "tracking orders efficiently using a python class. i've written a ledger class for a daemon process, which is supposed to keep track of an exchange's orderbook. my main concern is the way i keep track of the orders - more precisely, the containers i use to store them: a combination of set() and dict() appeared like the most feasible solution.however, i'm wondering if haven't missed a sweet feature or nifty recipe that works better. but first, my code - the lines in question are within the constructor:import loggingfrom heapq import nlargest, nsmallestlog = logging.getlogger(__name__)class orderremovalerror(exception): passclass ledger: def __init__(self): self._ask_orders = {} # keeps orders by price {price: (amount, count)} self._ask_ladder = set() # keeps ask prices self._bid_orders = {} # keeps orders by price {price: (amount, count)} self._bid_ladder = set() # keeps bid prices def _add(self, order, ledger_orders, ledger_oders_ladder): price, count, amount = order price_key = str(price) ledger_orders[price_key] = [amount, count] ledger_oders_ladder.add(price) return true def _remove(self, order, ledger_orders, ledger_order_ladder): price, amount, count = order price_key = str(price) err = [] c = 0 try: ledger_orders.pop(price_key) except keyerror: c += 1 except exception as e: err.append(e) try: ledger_order_ladder.remove(price) except valueerror: c += 1 except exception as e: err.append(e) if c == 2: pass # order didn't exist. return false elif c == 1 or len(err) > 1: raise orderremovalerror(something went wrong during deletion of order %s! error: %s % ((*order,), err)) elif c == 0: log.debug(order deleted successfully! %s % (order,)) return true else: log.error(ledger._remove() error dump: {order: %s, ledger_orders: %s, ledger_order_ladder: %s, ((*order,), ledger_orders, ledger_order_ladder)) raise valueerror(passed order caused unforeseen result during removal! inputs have been logged.) def bids(self, n=none): returns n bids, returning best first; default returns all. if n is none: bids = [] keys = [str(k) for k in sorted(self._bid_ladder, reverse=true)] for key in keys: bids.append((key, *self._bid_orders[key])) return bids elif n == 1: # return best bid return self._bid_orders[str(max(self._bid_ladder))] elif n > 1: #return nlargest return self._bid_orders[str(nlargest(n, self._bid_ladder))] def asks(self, n=none): returns n asks, returning best first; default returns all. if n is none: asks = [] keys = [str(k) for k in sorted(self._ask_ladder)] for key in keys: asks.append((key, *self._ask_orders[key])) return asks elif n == 1: # return best bid return self._ask_orders[str(min(self._ask_ladder))] elif n > 1: #return nsmallest return self._ask_orders[nsmallest(n, self._ask_ladder)] def remove_bid(self, order): removes bid order if it exists in our ledger :param order: :return: true if it was removed, false if it didn't exist try: return self._remove(order, self._bid_orders, self._bid_ladder) except orderremovalerror as e: log.error(e) def remove_ask(self, order): removes ask order if it exists in our ledger :param order: :return: true if it was removed, false if it didn't exist try: return self._remove(order, self._ask_orders, self._ask_ladder) except orderremovalerror as e: log.error(e) def add_bid(self, order): return self._add(order, self._bid_orders, self._bid_ladder) def add_ask(self, order): return self._add(order, self._ask_orders, self._ask_ladder)if __name__ == '__main__': l = ledger() l.add_ask((500, 5.5, 5)) l.add_ask((501, 1.0, 4)) l.add_ask((502, 5.4, 70)) l.add_bid((500, 5.5, 5)) l.add_bid((501, 1.0, 4)) l.add_bid((502, 5.4, 70)) print(l.asks()) print(l.bids()) # test updating print(testing update..) l.add_bid((502, 5.4, 10)) l.add_ask((502, 5.4, 10)) print(l.asks()) print(l.bids()) # test removing print(testing removing..) l.remove_bid((501, 6, 0)) l.remove_ask((501, 6, 0)) print(l.asks()) print(l.bids()) print(testing update..)the main concern is performance - i will be making thousands of calls to this class, so i'd like to make the adding, removing and querying as fast as possible. some implementations i've already considered:using the itemgetter from the operator module in combination with dict()s only, eliminating the need for set()s.i didn't like this, because i had to check the entire dict by calling the index of each key (if i stored orders like so: {'price': [price, amount, count]}). keeping a set of dict keys seemed simpler and faster. using dict().keys() as substitute for set()i did consider this, but wasn't sure on how well this would perform, given the frequent calls it would receive. could someone elaborate on how much better this would perform ? the most obvious advantage would be that it only calls on a single object (dict()) instead of two (dict() and set()). the disadvantage appears to be to have to call dict.keys() every time i want to return values from it (since i need to sort them first) - instead of just sorting the set().using namedtuple instead of dict()i considered this at first, but since they're immutable, creating a new namedtuple instance on every update seemed redundant and wasteful. those were my considerations. please do feel free to correct any of these assumptions - i've been coding for just under a year professionally, so my experience is limited. also, pointers where i can improve my code style are welcome as well.",
    "present_kp": [
      "python",
      "performance"
    ],
    "absent_kp": [
      "object oriented",
      "python 3.x"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how much bandwidth do i need? please help?. possible duplicate:how to find web hosting that meets my requirements? i'm an inexperienced webmaster trying to estimate hosting requirements for a site. i need to work out a dollar amount for a grant.the tricky part is that traffic fluctuates dramatically throughought the year, because we're an international day of celebration. that means most of the year, we get ~50 visitors a day. then in the months ramping up to the day, we get between 200-600 visitors a day. in a three day span at the peak, we can get 5,000-200,000 a day. last year the site crashed because of this and i want to avoid it this year.i'm writing a grant to get better hosting, but i have no idea what the best solution for this would be. i need to have a solid dollar amount it will probably have if things go the way they did last year. any help is greatly appreciated.edit: we're in the usa. right now we have our site hosted on godaddy but i don't have the details. the site handled the traffic okay right up until june 8th, when it crashed. we don't have an accurate measure of how many people visited, but our sysadmin thought it was a ddos attack at first. he said it was around 200,000 at once. the site is <url> we also use cloudflare to cache and serve our pages, not sure how that may affect it",
    "present_kp": [
      "web hosting",
      "bandwidth"
    ],
    "absent_kp": [
      "high traffic",
      "web traffic"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why failing to get cran key for r backports?. i do and get# <url> gpg --keyserver subkeys.pgp.net --recv-key 06f90de5381ba480gpg: directory '/root/.gnupg' createdgpg: new configuration file '/root/.gnupg/gpg.conf' createdgpg: warning: options in '/root/.gnupg/gpg.conf' are not yet active during this rungpg: keyring '/root/.gnupg/secring.gpg' createdgpg: keyring '/root/.gnupg/pubring.gpg' createdgpg: requesting key 381ba480 from hkp server subkeys.pgp.net?: subkeys.pgp.net: network is unreachablegpgkeys: http fetch error 7: couldn't connect: network is unreachablegpg: no valid openpgp data found.gpg: total number processed: 0i am trying to change to r backports because i need r 3.3.2, so i did already the addition of the following line in /etc/apt/sources.listdeb <url> jessie-cran3/iterating rcs' proposalroot@masi:/home/masi/documents# gpg --keyserver keys.gnupg.net --recv-key 06f90de5381ba480gpg: requesting key 381ba480 from hkp server keys.gnupg.netgpg: /root/.gnupg/trustdb.gpg: trustdb createdgpg: key 381ba480: public key johannes ranke (cran debian archive) <<email> importedgpg: no ultimately trusted keys foundgpg: total number processed: 1gpg: imported: 1root@masi:/home/masi/documents# apt-get update...fetched 116 kb in 3s (32.1 kb/s) reading package lists... donew: gpg error: <url> jessie-cran3/ release: the following signatures couldn't be verified because the public key is not available: no_pubkey 06f90de5381ba480os: debian 8.5",
    "present_kp": [
      "debian",
      "r"
    ],
    "absent_kp": [
      "software installation",
      "key authentication"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "density-based clustering of image keypoints. i have implemented the dbscan algorithm for clustering image keypoints. i have been following the pseudocode on the wiki page pretty strictly, and it's working, but i get the feeling its a very naive implementation and could be improved in terms of performance. i was hoping you could offer me some feedback on how to improve it./* dbscan - density-based spatial clustering of applications with noise */vector<vector<keypoint>> dbscan_keypoints(vector<keypoint> *keypoints, float eps, int minpts){vector<vector<keypoint>> clusters;vector<bool> clustered;vector<int> noise;vector<bool> visited;vector<int> neighborpts;vector<int> neighborpts_;int c;int nokeys = keypoints->size();//init clustered and visitedfor(int k = 0; k < nokeys; k++){ clustered.push_back(false); visited.push_back(false);}//c =0;c = 0;clusters.push_back(vector<keypoint>()); //will stay empty?//for each unvisted point p in dataset keypointsfor(int i = 0; i < nokeys; i++){ if(!visited[i]) { //mark p as visited visited[i] = true; neighborpts = regionquery(keypoints,&keypoints->at(i),eps); if(neighborpts.size() < minpts) //mark p as noise noise.push_back(i); else { clusters.push_back(vector<keypoint>()); c++; //expand cluster // add p to cluster c clusters[c].push_back(keypoints->at(i)); //for each point p' in neighborpts for(int j = 0; j < neighborpts.size(); j++) { //if p' is not visited if(!visited[neighborpts[j]]) { //mark p' as visited visited[neighborpts[j]] = true; neighborpts_ = regionquery(keypoints,&keypoints->at(neighborpts[j]),eps); if(neighborpts_.size() >= minpts) { neighborpts.insert(neighborpts.end(),neighborpts_.begin(),neighborpts_.end()); } } // if p' is not yet a member of any cluster // add p' to cluster c if(!clustered[neighborpts[j]]) clusters[c].push_back(keypoints->at(neighborpts[j])); } } }}return clusters;}vector<int> regionquery(vector<keypoint> *keypoints, keypoint *keypoint, float eps){float dist;vector<int> retkeys;for(int i = 0; i< keypoints->size(); i++){ dist = sqrt(pow((keypoint->pt.x - keypoints->at(i).pt.x),2)+pow((keypoint->pt.y - keypoints->at(i).pt.y),2)); if(dist <= eps && dist != 0.0f) { retkeys.push_back(i); }}return retkeys;}",
    "present_kp": [
      "c++",
      "performance",
      "algorithm",
      "clustering"
    ],
    "absent_kp": [
      "opencv"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "two isp and redirecting traffic. i have a gateway with two isp connected to eth0 and eth2. there is also a eth1 with local network.i'm trying to group some services in one interface and some in the second:incoming traffic1 eth0: 22 sshd, 80 http, 8080 http2 eth2: 22 sshdoutgoing traffic 3 eth2: 22 ssh, 25 smtp, 80 http, 110 pop3, 443 https, 587 smtp4 eth0: the rest of the portsi managed to reroute the traffic in points 1,3,4 using iptable, iproute2 with fwmark.here is interface setup:auto loiface lo inet loopbackauto eth0iface eth0 inet static address aaa.aaa.aaa.90 netmask 255.255.255.248 gateway aaa.aaa.aaa.89auto eth2iface eth2 inet static address bbb.bbb.bbb.137 netmask 255.255.255.192 pre-up /usr/local/bin/firewall.shauto br0iface br0 inet static address 192.168.1.1 netmask 255.255.0.0 bridge-ports eth1 post-up ifconfig eth1 0.0.0.0 promisc uphere is ip route and ip rules:ip route show table mainaaa.aaa.aaa.88/29 dev eth0 proto kernel scope link src aaa.aaa.aaa.90bbb.bbb.bbb.128/26 dev eth2 proto kernel scope link src bbb.bbb.bbb.137192.168.0.0/16 dev br0 proto kernel scope link src 192.168.1.1default via aaa.aaa.aaa.89 dev eth0ip route show table 4aaa.aaa.aaa.88/29 dev eth0 proto kernel scope link src aaa.aaa.aaa.90bbb.bbb.bbb.128/26 dev eth2 proto kernel scope link src bbb.bbb.bbb.137192.168.0.0/16 dev br0 proto kernel scope link src 192.168.1.1default via bbb.bbb.bbb.129 dev eth20: from all lookup 25532765: from all fwmark 0x4 lookup 432766: from all lookup main32767: from all lookup defaultand iptables: iptables -f iptables -t nat -f iptables -t mangle -f iptables -p input drop iptables -a input -i lo -j accept iptables -a input -p udp --sport 68 --dport 67 -m physdev --physdev-in tap1 -j drop iptables -a input -i eth1 -j accept iptables -a input -i eth2 -j accept iptables -a input -i br0 -j accept iptables -a input -p icmp -j accept iptables -a input -m state --state established,related -j accept iptables -a input -p tcp --dport 22 -s 0/0 -j accept iptables -t mangle -a prerouting -p tcp --dport 22 -d bbb.bbb.bbb.137 -j mark --set-mark 4 iptables -t mangle -a prerouting -p tcp --dport 25 -s 192.168.0.0/16 -j mark --set-mark 4 iptables -t mangle -a prerouting -p tcp --dport 80 -s 192.168.0.0/16 -j mark --set-mark 4 iptables -t mangle -a prerouting -p tcp --dport 8080 -s 192.168.0.0/16 -j mark --set-mark 4 iptables -t mangle -a prerouting -p tcp --dport 110 -s 192.168.0.0/16 -j mark --set-mark 4 iptables -t mangle -a prerouting -p tcp --dport 443 -s 192.168.0.0/16 -j mark --set-mark 4 iptables -t mangle -a prerouting -p tcp --dport 587 -s 192.168.0.0/16 -j mark --set-mark 4 iptables -t nat -a prerouting -p tcp -d aaa.aaa.aaa.90 --dport 80 -j dnat --to-destination 192.168.1.252:80 iptables -t nat -a postrouting -o eth0 -j snat --to-source aaa.aaa.aaa.90 iptables -t nat -a postrouting -o eth2 -j snat --to-source bbb.bbb.bbb.137 iptables -t nat -a postrouting -s 192.168.1.0/16 -j masqueradei cannot access eth2 bbb.bbb.bbb.137:22 from outside even when i listen to all interfaces:tcp 0 0 0.0.0.0:22 0.0.0.0:* listen 3111/sshddoes anyone issued any instability in performance using fwmark iproute2? eth2 provider is much faster then eth0 but using eth2 i experienced some stalls - like there were some isp problems but there is no problem with internet service provider - checked with second router at the same time?could some one please point me in right direction.thx",
    "present_kp": [
      "iptables",
      "sshd"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "url to see all google reviews for a local business. on google search and google maps it's possible to see reviews for local business. i would like to bookmark the all reviews listing. how do i find the url to bookmark?",
    "present_kp": [
      "google search",
      "google maps"
    ],
    "absent_kp": [
      "google my business"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "stripping whitespace in a csv file. i am interested in removing leading/trailing whitespace in a .csv file, and i was wondering if there's a better way to execute this:with open(csv_file.csv, rb) as infile: r = csv.dictreader(infile) fieldnames = r.fieldnames #creates list of fieldnames for row in r: for f in fieldnames: row[f] = row[f].strip()i am fine with using this method, but i was wondering if there's a way to circumvent using a nested for loop.",
    "present_kp": [
      "csv"
    ],
    "absent_kp": [
      "python"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "possible to get sms/text message notification when process ends or is killed?. there are scripts that will send an e-mail when a server process is finished.however, i do not want to check my email every so often just to see whether a job has finished. therefore i'd like to get an sms message.my question is similar to this one, just exchange sms with all occurrences of e-mail: is there a program that can send me a notification e-mail when a process finishes?can you think of any workaround / app / script / whatever that would enable an sms to be sent when a job is finished (or prematurely ended?)",
    "present_kp": [
      "process",
      "email",
      "sms"
    ],
    "absent_kp": [
      "shell",
      "monitoring"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "return object or dynamic from a method. i have created string to linq query library for internal company needs. we will use this library for querying over rest mainly, but it could be used in a lot of other different situations.the gist of it is this, a user can create a query and put it in a url like this:<url>)&&#gt(id,12345)]$orderby[name,-id]but they could also write something like thiswww.test.com/api/v1/users?query=$where[#eq(name,test)&&#gt(id,12345)]$last[]$last[] returns a single object, while$orderby[name,-id] returns a collectionin the library i have a method that will accept that string and accordingly will produce a collection or a single object, i am just not sure if that method should return a type of object or dynamic.which is more suitable, which one's going to be less confusing for other devs? also, i'm thinking of releasing this lib as open source, but i don't know if there is a need for something like this to get released.",
    "present_kp": [],
    "absent_kp": [
      "c#"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how can longest isoforms (per gene) be extracted from a fasta file?. is there a convenient way to extract the longest isoforms from a transcriptome fasta file? i had found some scripts on biostars but none are functional and i'm having difficulty getting them to work.i'm aware that the longest isoforms aren't necessarily 'the best' but it will suit my purposes.the fasta was generated via augustus. here is what the fasta file looks like currently (sequence shortened to save space)>doug_noindex_l005_r1_001_contig_2.g7.t1atggggcataacatagagactggtgaacgtgctgaaattctacttcaaagtctacctgattcgtatgatcaactcatcattaatataaccaaaaacctagaaattctagccttcgatgatgttgcagctgcggttcttgaagaagaaagtcggcgcaagaacaaagaagatagaccg>doug_noindex_l005_r1_001_contig_2.g7.t2atggggcataacatagagactggtgaacgtgctgaaattctacttcaaagtctacctgattcgtatgatcaactcatcathe format is as such:gene 1 isoform 1 gene 1 isoform 2 gene 2 isoform 1 gene 2 isoform 2 and so forth. there are several genes that have more than one pair of isoforms (up to 3 or 4). there are roughly 80,000 total transcripts, probably 25,000 genes. i would like to extract the single longest isoform for each gene.",
    "present_kp": [
      "fasta",
      "isoform"
    ],
    "absent_kp": [
      "filtering"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is it possible to design a language that doesn't have syntax errors?. in other words, a language where every possible string is valid syntax?edit: this is a theoretical question.i have no interest in using such a language; i'm just asking whether it's possible.further editi went ahead a designed such a language. see errorfree",
    "present_kp": [
      "syntax"
    ],
    "absent_kp": [
      "language design"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "modeling timing characterists of an architecture. i am building a system and i have a couple of architectures in mind. i want to have an idea of which architecture is likely to be most performant (quickest).i can make different decisions like 1) do everything in one thread2) separate this part into one thread, that part into another thread since it can run in parallel and so oninstead of actually building the system in two different ways and seeing how it performs, is there a way to quickly model/prototype the system where i can express the above concepts and the times it takes for inter thread i/o etc (of which i have a very good estimate) and use the model/prototype to estimate which architecture is better?",
    "present_kp": [],
    "absent_kp": [
      "efficiency",
      "software engineering"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "pruned fft runtime. pruned fast fourier transforms compute only a specified subset of the result indices in faster time, although sometimes with a slower implementation constant (because fft is generally so optimized). they can also exploit sparsity in the input vector to further accelerate computation. on <url> the runtime is reported to be $\\in o(n\\log k)$, where $n$ is the length of the input vector and $k$ is the number of desired indices to compute in the result (of course, the standard fft could always be used to compute the full result in $o(n\\log n)$ steps); however, i cannot figure out where the benefit of sparsity in the input vector is measured in this bound.does anyone know the runtime / how to get the runtime of a pruned fft on $n$ inputs, $m$ of which are nonzero, and where $k$ outputs are computed? i have an algorithm that depends on a step very similar to a pruned fft calculation, and i have a lot of sparsity.thanks a lot for any advice you can lend.",
    "present_kp": [
      "fourier transform"
    ],
    "absent_kp": [
      "algorithm analysis",
      "runtime analysis"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "a specific problem with architecturing a part of our application. firstly, i'll try to describe the real world model of what we are trying to express in c# code.we have a device called m100. it's purpose is to read\\write data from\\into cards (plastic or ultralight cards such as mifare). it can be with one up to 3 antennas (the device read\\write data through antenna). one of antennas is internal (so card can be put on the device surface and the device will be able to perform read\\write operations), the other are external. let's consider the usual situation. we have a point of service terminal. m100 reside inside the terminal and it has two external antennas wich are placed over a tract of cards dispenser. cards dispenser is another device which is responsible for accepting and dispensing cards for performing read\\write operations by m100. so, card dispenser takes the card from the user, move it to the read\\write position and after that m100 can perform read\\write operations. m100 can be equipped with only one (internal) antenna. so we have to main cases: with and without card dispensers.we have one application that interacts with dispensers and the other uses m100 without card dispensers.we developed m100provider with it's interface im100provider. this is the low-level class which interacts with m100 directly, passes commands through com-port.we have a higher level-operations such as authorizeoperator, writeticket, readcard (all this operations require m100) so we developed m100communicator class (maybe the name does not reflect it's intention at the time, but we will think about the name a bit later). m100communicator is a singleton. because we need to manipulate card dispensers to accept and dispense users cards m100communicator interacts with carddispensersmanager class. we haven't decide yet wil it be composed into m100communicator or will it be a singleton (it seems like it's a singleton).all these operations are have to perform in a kind of a unified way:block the current object.check if there are some dispensers configuredif they are - pass to them a command to open the tract for card acception.pass to m100 a command to search a card through specified antennas.when the card was found, perform read\\write operations.dispense card back to the user.here are the problems we are faced:considering that m100communicator sometimes doesn't require carddispensersmanager we are forced to check it's state to null reference everywhere (and it will be quite strange to have a special case carddispensersmanager which will return meaningfull responses while there are no dispensers!).m100communicator and carddispensersmanager have to be initialized from the start of the system so it's unclear how to update the configuration of carddispensersmanager and m100communicator and how to communicate with carddispensersmanager directly when we want to close ports and stop communication with dispensers explicitly, because it will be very strange to expose dispensers api through m100communicator. (yes we have the requirement to be able to access dispensers api from everywhere, so we think that cardsdispensermanager has to be a singleton).",
    "present_kp": [
      "c#"
    ],
    "absent_kp": [
      "architecture",
      ".net",
      "domain driven design",
      "enterprise architecture"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to recover a picasa account username. dont ask me how, but i forgot my picasa username and password. i dont have the details on my gmail either. i have searched high and low to check if there is any way of recovering my username of picasa.any one here know of a way, any way, to recover a picasa account?",
    "present_kp": [],
    "absent_kp": [
      "google account",
      "password recovery",
      "picasa web albums"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why is black box called functional testing when it tests also non functional?. this has been bothering me for a while. security, performance tests etc. are all done typically using the black box approach. but these are nonfunctional,while black box is called functional testing.is it because it judges the function and it is just a naming or there is an inconsistency?references:software engineering by sallehsoftware engineering and testing by gupta, tayalsoftware engineering by a.a.puntambekarsoftware testing: a practical approach by sandeep desai, abhishek srivastava",
    "present_kp": [
      "functional testing"
    ],
    "absent_kp": [
      "terminology",
      "standards",
      "theory"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "microsoft application registration portal for university mail account. based on this project in python i am trying to make a web application to access my mail inbox to then filter a set of messagges (with a regex) and extract the addresses of the receivers. i am stuck on the following step (from the readme.md):generate a client id and secrethead over to apps.dev.microsoft.com to quickly get a client id and secret. using the sign in buttons, sign in with either your microsoft account (outlook.com), or your work or school account (office 365).this works for my own personal outlook account, but it fails for my university mail account (which i access through outlook web app and that i actually want to use it on). my personal account is a regular <email> account, whereas my university one is <email>(*). when i enter <email> as user, it automatically goes to a personalised university themed log in page where i am prompted to log in with a shibboleth system. however, when i enter the correct credentials, it loops back to the log in page, saying i have not entered the right username and/or password. with the very same credentials however, i can log in to our outlook web app mail client. does this mean my university has blocked the possibility to get client id's based on mail addresses for their domain via the application registration portal?i am completely new to this, so sorry if this is also the wrong q&a community for the question - i was not sure were to post this :)(*) acutally .be since i'm from belgium, but whatever.",
    "present_kp": [
      "email",
      "outlook",
      "microsoft"
    ],
    "absent_kp": [
      "outlook on the web",
      "azure"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "provide web images with high resolution and let the browser scale it. a bad idea?. back in the old days when i learned to create web pages, the rule of thumb was: don't resize your images with html/css, provide the image in its native resolution. otherwise, it will be upscaled or downscaled by the browser and the image will look blurry or ugly.those were the days when we didn't have high-resolution tablets, huge screens and browsers with built-in zoom functionality using smart algorithms.my gut feeling tells me that a lot of people don't have their browser at a zoom factor of 100%, so browser-side scaling is going to happen anyway -- providing a high-resolution source should lead to a strictly better result than the native-for-100%-version in that case. have we reached the time yet where it makes sense to provide images with higher resolution than required when designing new web sites? ...within reasonable limits considering the users' bandwidth, of course.",
    "present_kp": [
      "image"
    ],
    "absent_kp": [
      "web development"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to group values based on a connectedness metric?. there is a file with three columns.columns 1 and 2 contain numbers drawn from the same set.the file will typically have one line for each pair of members of the set;so, if there are n members,there should be n(n1)/2 lines.column 3 shows the amount of connectedness between numbers in columns 1 and 2.i want to partition my source set into groups of consecutive values(i.e., ranges) for which connectedness is greater than or equal to 0.2.for example, in this small data set:input:1 2 0.2221 3 0.2131 4 0.0141 5 0.0011 6 0.5551 7 0.5092 3 0.2132 4 0.0142 5 0.0012 6 0.5552 7 0.7092 8 0.5093 4 0.9953 5 0.3233 6 0.5553 7 0.2253 8 0.0014 5 0.0954 6 0.0584 7 0.3354 8 0.0055 6 0.9955 7 0.6585 8 0.6506 7 0.4316 8 0.3337 8 0.754the output should be like:output: g1: 1 2 3 g2: 4 g3 :5 6 7 8connectedness between 1 with 2 and 3, is greater than 0.2, so 1, 2 and 3 should be placed in first group. in fact, any pair of numbers within a group must have enough connectedness together. despite high relation between 1/2/3 and 6 (0.555 > 0.2), 6 should not be placed in the first group, since previous numbers (4 and 5) had low connectedness with 1. so we must not jump over 4 and 5 and connect numbers in the first group with 6.number 4 does not have the high connectedness with 5, so number 4 should be in the second group individually. no matter that 4 has a high connectedness with 7 since the previous numbers (5 and 6) were in low connectedness with 4 and we must not jump over numbers in between and connect 4 with 7.5 has a high connectedness with 6, 7 and 8. also, any pair of numbers (like 6/7, 6/8, 7/8) have high connectedness together.therefore they should be placed together in the third group.that is why all of these numbers can be placed in one group.note that the real data does not begin from number 1 and there are over 100,000 lines. so it is huge.here is a part of my real data:input: 49997 49998 0.082 49997 49999 0.953 49997 50000 0.060 49998 49999 0.288 49998 50000 0.288 49999 50000 0.265output should be: g1:49997 g3: 49998 49999 50000",
    "present_kp": [],
    "absent_kp": [
      "linux",
      "shell script",
      "perl"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "zsh: echoing every statement?. i have an odd problem with zsh when i switch between shells in a particular way:option 1 (works well):i start a zsh shell. i switch to tcsh with /bin/tcsh, and i switch back to zsh with /bin/zshif i then run:> lsi get:./ ../ file1 file1 file3option 2 (problematic):i start a zsh shell. i switch to tcsh with:exec env -i home=$home term=$term display=$display /bin/tcsh. and i then switch back to zsh with /bin/zsh. if i then enter any commands, the zsh shell echoes the command and then the result. using the same example as before:> ls2;ls --color=yes -af1;./ ../ file1 file2 file3in other words, zsh shows 2;command 1; and then the output, which is of course very different from what i was getting with option 1.what's even more strange is that this only happens within ansi-term or multi-term terminals in emacs, and not under gnome-terminal.what else can i do to diagnose the problem? any thoughts on what may be causing this?update:my .cshrc prompt isset prompt = >",
    "present_kp": [
      "shell",
      "terminal",
      "zsh",
      "emacs"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "which gpu should i buy for deep learning project. i have less than $1k as budget for my project.my project is predicting drug target interaction. i am going to use gpu on vector multiplication of visible layer and neurons of hidden layer. i have 1007 drugs and 881 substructure of that drug and 2000 neuons on no. of hidden layer. what i have to do is take one drug and provide its 881 substructures as input and multiply each values to 2000 neorons of one hidden layer again and again. when i did this from normal laptop just by using 200 drugs it took me 7 days. so i am thinking of using gpu and reducing time. so please suggest what type of gpu do i need. thank you.",
    "present_kp": [],
    "absent_kp": [
      "graphics cards",
      "motherboard",
      "memory"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "sleepsort in go. i implemented sleepsort in go. i am new to channels, and i'd like to get some advice on them, especially on how to close channels after n messages have been sent on them. currently i use a counter and an if statement (see the second for loop in sleepsort), but maybe this can be done in a nicer way?package mainimport ( fmt time)func sleep(n int, channel chan<- int) { time.sleep(time.duration(n) * time.second) channel <- n}func sleepsort(xs []int) []int { channel := make(chan int, len(xs)) for _, x := range xs { go sleep(x, channel) } result := make([]int, len(xs)) i := 0 for x := range channel { result[i] = x i++ if i == len(xs) { close(channel) } } return result}func main() { xs := []int{2, 5, 2, 1, 4, 3, 5} fmt.printf(%v , sleepsort(xs))}other advice is also very appreciated.",
    "present_kp": [
      "go"
    ],
    "absent_kp": [
      "concurrency"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "learning scala, feeling discouraged. i'm coming from a java background and trying to learn scala. at the moment i'm feeling pretty overwhelmed, there seems to be so much more to learn with scala, so many different ways of doing the same thing.i was wondering if anyone has any advice on where to start, and how long it took them to feel reasonably competent in the language?even little things like for loop comprehensions seem really powerful but it's just another piece of syntax you need to remember!",
    "present_kp": [
      "java",
      "scala"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "simple javascript module loader. i have written a simple js module loader (loader is in quotes because it doesn't actually load the files) that is designed to resolve simple dependencies between modules. it is heavily inspired by module.js and takes some of it's design from it.here is the code itself:(function() { function create() { var definitions = {}, instances = {}; // returns whether or not a module with the provided id is defined. var defined = function(id) { return definitions.hasownproperty(id); }; // define a module with the provided id and definition. var define = function(id, definition) { if(defined(id)) { throw new error('module already defined: ' + id); } definitions[id] = definition; }; // undefine a module with the provided id. var undefine = function(id) { if (!defined(id)) { throw new error('module not defined: ' + id); } delete definitions[id]; delete instances[id]; }; // require a module with the provided id. var require = function(id) { var stack = []; var internalrequire = function(id) { if(!defined(id)) { throw new error('module not defined: ' + id); } // if we have already seen this id on the require stack, we've got // some form of cyclic dependency. if (stack.indexof(id) !== -1 && stack.push(id)) { throw new error('cyclic dependency: ' + stack.join(' -> ')); } else { stack.push(id); } // if we already have an instance for this module, return it. if (instances.hasownproperty(id)) { return instances[id]; } else if (typeof(definitions[id]) === 'function') { // otherwise if our definition is a function, call it and pass the // require method to it. return instances[id] = definitions[id].call(null, internalrequire); } else { // otherwise just return the definition itself (useful for objects // e.g. constants). return instances[id] = definitions[id]; } }; return internalrequire(id); }; return { define: define, undefine: undefine, require: require }; } window.module = create();})();and some example usage:module.define('person', function() { var person = function(name) { object.defineproperty(this, 'name', { value: name }); }; person.prototype.speak = function(text) { return this.name + ': ' + text; }; return person;});module.define('main', function(require) { var person = require('person'); var jack = new person('jack'), rocky = new person('rocky'); console.log(jack.speak('hi rocky!')); console.log(rocky.speak('hi jack!'));});i don't really have any burning questions about my code; i'd just like some feedback on what people think of it and whether anyone can see any improvements.",
    "present_kp": [
      "javascript",
      "modules"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "is this padded version of the halting problem in np?. i'm using the following definition of $np$:$$a \\in np \\longleftrightarrow a(x) = \\exists w: b(x,w) $$where $b \\in p$ and $|w| = poly(|x|)$. now instead of the problem whether the program $\\pi$ halts on input $x$, i'll use the close cousin that asks whether $\\pi$ halts on $0$. then i can write:$$haltsonzero(\\pi) = \\exists t: haltsonzerointime(\\pi, t) $$where $haltsonzerointime$ is the decidable problem of checking whether $\\pi$ halts on input $0$ after $t$ steps. comparing this with the definition of $np$ $haltsonzerointime$ is already in $p$ as long as $t$ is encoded in unary. after all running $\\pi$ for $t$ steps takes only about $t$ steps, so this is a linear algorithm.the only issue is that $t$ isn't in $poly(|\\pi|)$ because programs of size $|\\pi|$ when given $0$ as input run for $busybeaver(|\\pi|)$ time, which isn't even computable, much less polynomial. but now suppose we insist that $\\pi$ is padded until it reaches length $busybeaver(|\\pi|)$ and call that new input $\\pi'$. then $t$ would be polynomial (in fact linear) in the size of $\\pi'$ and $haltsonzero(\\pi')$ would be in $np$.at this point there's of course a voice inside me shouting that this is all nonsense because $busybeaver$ isn't computable. well i certainly haven't reduced the classical formulation of the halting problem to a problem in $np$ because i had an uncomputable step in the reduction. but it seems to me like i've still defined a problem in $np$ with the strange property that just writing down a problem instance requires uncomputable superpowers.is this really a bona fide $np$-problem or am i making a subtle mistake somewhere?",
    "present_kp": [
      "np",
      "halting problem"
    ],
    "absent_kp": [
      "busy beaver"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how can i track (funnel) if someone from a particular banner campaign watched our demo video?. i'm setting up a banner on a website with the caption view our demo video. the banner has the utm campaign tracking code setup and working.on our website i've got an embedded iframe youtube video that triggers two events: one if it is played, and one if it reaches the end.what i'm trying to figure out now is if there's a way to create a funnel goal that sees if someone came in from the banner, did they proceed to watch the video?this is a snippet of the code i used to capture the youtube eventif (event.data == yt.playerstate.playing) { ga('send', 'event', 'youtube', 'started', 'promo from main page'); }",
    "present_kp": [
      "youtube"
    ],
    "absent_kp": [
      "goal tracking",
      "analytics api",
      "event tracking"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why my neural network accuracy is 100%?. i wanna ask about neural network.i have a research and programming about neural network using backpropagation algorithm for prediction. my input data using binary form. but, i'm still confuse, because i get some input data from expert. i have separate some data for training and the remaining data i have use to testing. i get accuracy 100% for training and testing. i think 100% for training is possible but 100% for testing it's something weird.is it possible if the data i get from expert including linear separability?if the data is linear separability, is my neural network fail or my neural network works because the pattern too easy?thanks..",
    "present_kp": [],
    "absent_kp": [
      "neural networks"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "preventing brute force attack on samba server. i have a lan with many users(clients) in it. each of them has a private samba share folder. how can i prevent somebody (within the same network) brute forces another account (so he can access somebody else data)?googling brings only: samba shares should not be on the public internet. but i am in my local network.i think many admins have exact this problem (for example companies or university admins) so is there any way to allow only 3 login attempts per minute or blacklisting the ip of an client after 3 attempt? or any other solution for this problem? i tried fail2ban, but there is no host ip address in the log file.more info:clients mostly windows 7 or abovesamba current version on ubuntu 16.04",
    "present_kp": [
      "samba"
    ],
    "absent_kp": [
      "security"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "an eigenvalue algorithm to solve constrained quadratic form minimization. i have a quadratic form $\\mathbf{x}^t a \\mathbf{x}$ (where $a\\in \\mathbb{r}^{n imes n}$ is symmetric matrix and $\\mathbf{x}\\in \\mathbb{r}^n$) that i want to minimize given the normalization constraint $\\mathbf{x}^t\\mathbf{x}=1$.because $\\mathbf{a}$ is a adjacency matrix of an undirected graph then i know that it is symmetric and real and also sparse.what is an appropriate memory conservative algorithm to solve this kind of problem?is it good to solve the eigensystem $\\mathbf{a}\\mathbf{x}=\\lambda \\mathbf{x}$ and then taking as solution the first smallest nonzero eigenvalue and its related eigenvector?if this is the way to proceed how is it possible to get the smallest eigenvalue with subspace iteration?",
    "present_kp": [
      "eigensystem"
    ],
    "absent_kp": [
      "optimization"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is it normal that a restart is required to mount usb after a kernel upgrade?. sometimes i see the linux kernel being mentioned in the list of upgrades, when running pacman -syu (updating my packages in arch linux). whenever this happens, after installation of the packages, i can not mount usb drives anymore until i restart. i would just like to know if this is something that is common and expected (and if so, why, do i wonder), or if this is something not expected that i should investigate.",
    "present_kp": [
      "kernel",
      "usb",
      "upgrade"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "clarification for argument in proof of search in n^1/3 queries with hidden variables/non-collapsing measurements. let $n=2^n$. in aaronson's quantum computing and hidden variables (1) and the recent follow up by aaronson, bouland, fitzsimons, and lee the space just above bqp (2), we consider models of computation involving hidden variables or non-collapsing measurements, and prove that this model of computation is able to simulate grover's algorithm in time $o(n^{1/3})$.the proof theorem 10 of (1) or theorem 4.1 of (2); both go the same way for my question starts with applying $n^{1/3}$ grover iterations.after this we should have the state:$$lpha \\lvert x angle+eta \\!\\!\\!\\sum_{y \\in \\{0,1\\}^n} \\!\\!\\lvert y angle$$where $lpha = \\dfrac{1}{\\sqrt{2^{n/3} + 2^{-n/3 + 1} + 1}}$ and $eta = 2^{-n/3}lpha$.my questions:from where do we follow this? the original paper from grover is referenced, but he provides only the recurrence (in later versions he added a reference to the explicit sin/cos solution). in my opinion, the state should be something like $lpha \\lvert x angle+eta \\sum_{y \\in \\{0,1\\}^n \\:\\! \\setminus\\{x\\}} \\lvert y angle$, otherwise we would have for $n=3$ a superposition of $1 + 2^3 = 9$ states. i guess this is only a lazy notation for convenience?is this state normalized? if i try e.g. $n=3$, then i will have:$$egin{aligned}[b] lpha &= rac{1}{\\sqrt{2^{1} + 2^{-1 + 1} + 1}} = rac{1}{2} \\[1ex] eta &= 2^{-n/3}lpha = 2^{-1} \\cdot rac{1}{2} = rac{1}{4} \\end{aligned}$$if we assume a factor $k$ from the sum, we can try to check the normalization with:$$egin{aligned}[b] 1 = lpha^2 + (keta)^2= igl( rac{1}{2}igr)^2 + igl( rac{k}{4}igr)^2={}& rac{4 + k^2}{16}\\[1ex]\\implies{}& k^2 = 12\\end{aligned}$$which is not solvable in integer. (if i take the $n$ general, substitute by $2^{n/3} = s$, and solve for $lpha^2 + (keta)^2 = 1$, i get a polynomial in 3. degree. the general case does not give any insight for me)the statement from (1) that one can check that this state is normalized does not provide a good feeling for my question. scott aaronson published a list of some errata on his blog for this paper, but the list did not include this. what am i missing?",
    "present_kp": [
      "quantum computing"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "making a progressbar with dialog from rsync output. i'm looking for a way to filter/redirect rsync output in a manner where it can be fed to the dialog --gauge command, so i can get a nice looking progressbar during file sync.currently i have only tested it directly at the prompt, but i'm planning to do this in a (bash) shell script.i have looked around the internet and found bits and pieces, but i'm still missing something to make it work(disclaimer: this might be a totally wrong approach, and is a redirect/piping monstrosity)what i have currently put together:rsync -avz --progress -e ssh user@server:/home/user/data/ /home/user/data | awk -f /home/user/rsync.awk | sed 's/\\([0-9]*\\).*//' | dialog --title my gauge --gauge hi, this is a gauge widget 20 70first i have the actual rsync command with the --progress optionoutput from rsync is piped into awk and uses the followng awk filter:{ if (index($0, to-check=) > 0) { split($0, pieces, to-check=) split(pieces[2], term, )); split(term[1], division, /); print (1-(division[1]/division[2]))*100}# else# {# print #$0;# } fflush();}this filters out rsync output and provides the percentage in the following format:53.7<phone>.4<phone>.111162.963so to get rid of the decimal numbers, i feed the output to sed:sed 's/\\([0-9]*\\).*//'which gives the following output:6466687072747577those numbers are piped into dialog like this:dialog --title my gauge --gauge hi, this is a gauge widget 20 70 as far as i know, dialog --gauge etc. should accept this, but it just displays progress to be 0% until it suddenly reaches 100%can someone point me in the right direction here?am i far away from a working progressbar?is there a better way to achieve this?regards,christeredit: after taking @lynxlynxlynx' answer into account, the correct command line is: rsync -avz --progress -e ssh user@server:/home/user/data/ /home/user/data \\ | awk -f /home/user/rsync.awk \\ | sed --unbuffered 's/([0-9]*).*//' \\ | dialog --title my gauge --gauge hi, this is a gauge widget 20 70",
    "present_kp": [
      "bash",
      "awk",
      "rsync"
    ],
    "absent_kp": [
      "linux",
      "scripting"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to fairly share bandwidth between ips with tc?. i have a linux router (d-link 320-nru) with tc installed.i have some users' computers to fairly share bandwidth between them.also there is such complication as inconstant uplink bandwidth.how can i configure tc to meet all these requirements?thanks in advance.",
    "present_kp": [
      "linux",
      "ip",
      "bandwidth",
      "tc"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "direct link to facebook page's post by others (now renamed to posts to page). before the last redesign of facebook pages i could use a link like <url> to go directly to a view where i saw only the posts on the page by people liking the page. that is, it excluded posts by administrators (the third option was to view highlights iirc, that is, a mix of posts by users and administrators).after the last redesign, facebook changed all this. now i have to go to the page and then scroll down a bit and click post to page. the link i click (#) is not possible to use directly. anyone know if it is possible to link directly to a view showing the likers posts?the reason i want this is that i admin a page where my admin posts are uninteresting for the users. people like this page to see other non-admins posts. it is a mess to give the users instructions for how to find what they are looking for. instead i want to give them the direct link.",
    "present_kp": [
      "facebook",
      "facebook pages"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "mercurial repository structure with heavyweight corporate comms, configuration management & test requirements. i am yet another subversion user struggling to re-educate myself in the tao of distributed version control.when using subversion, i was a big fan of the project-minor approach, and, with most of my former employers, we would structure our repository branches; tags & trunk as follows:branches-+ +-personal-+ | +-alice-+ | | +-shinynewfeature | | +-automated-+ | | +-shinynewfeature | +-bob-+ | +-automated-+ | +-bespokecustomerproject +-project-+ +-shinynewfeature +-fixstinkybugtags-+ +-m20110401_releasecandidate_0_1 +-m20110505_release_0_1 +-m20110602_milestonetrunkwithin the actual source tree itself, we would use (something like) the following structure: (src)-+ +-developmentautomation-+ | +-testautomation | +-deploymentautomation | +-docgeneration | +-staticanalysis | +-systemtest | +-performancemeasurement | +-configurationmanagement | +-utilities +-libraries-+ | +-log-+ | | +-build | | +-doc | | +-test | +-statistics-+ | | +-build | | +-doc | | +-test | +-charting-+ | | +-build | | +-doc | | +-test | +-distributedcomputing-+ | | +-build | | +-doc | | +-test | +-widgets-+ | +-build | +-doc | +-test +-productlines-+ | +-flagshipproduct-+ | | +-coolfeature | | +-anothercoolfeature | | +-build | | +-doc | | +-test | +-coolnewproduct +-project-+ +-bigimportantcustomer-+ | +-bespokeprojectone | +-bespokeprojecttwo +-anotherimportantcustomer-+ +-anotherbespokeprojectthe idea was (and still is) to use the structure of the repository to help structure communication between the engineering team; the customer-facing part of the business and various other stakeholders & domain experts.to wit: source documents that sit in one of the project directories get used (and earn money) only once. documents that sit in one of the productlines directories earn money as many times as a product from that particular line gets sold. documents that sit in one of the libraries directories earn money as many times as any of the products that use them get sold.it makes the notion of amortization of costs explicit, and helps build support for source document reuse across the business.it also means that there is a common structure over which our build automation tools can operate. (our build scripts walk the source tree looking for build folders within which they find configuration files specifying how each component is to be built; a similar process happens for documentation generation and testing).significantly, the products on which i work typically take a long time to run performance measurement & characterization tests; from 20 to 200 hours; generating somewhere between several gb to several tb of processed test results/intermediate data (that must be stored and tied to a particular system configuration so performance improvement over time can be measured). this issue makes configuration management an important consideration, and also imposes some requirement for centralisation, as typically the computational resources needed to run the performance measurement and characterization tests are limited; (a small cluster of 64-128 cores).as one final note; the continuous integration system knows that it needs to trigger a build; static analysis; smoke test & unit test run each time trunk is modified, each time any tag branch is modified, and each time any automated branch branch is modified. this way, individual developers can use the ci system with their personal branches, an important capability, imho.now, here is my question: how can i replicate all of the above (and improve upon it, if possible), with mercurial.--edit:my current line of thinking is to use a central subversion repository, to define the overall structure, but to allow the use of hg as a client so developers can have repos available locally.",
    "present_kp": [
      "mercurial",
      "continuous integration",
      "configuration management"
    ],
    "absent_kp": [
      "organization",
      "svn"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "create permissions for group to write and files created by owner are only writeable by owner. i have created a folder, /hello: it has the following permissions:owner = rwxgroup = rwxothers = rand the folder is owned by john and belongs to the group sales. i have created another user named david and added him to the group sales.currently, the files created by john are:owner = rwgroup = rwothers = rand files created by david are:owner = rwgroup = rothers = ri do not want files created by john to have write permissions granted by default to the sales group.i have tried changing the permissions and it resulted in david not being able to write into the directory.",
    "present_kp": [
      "permissions"
    ],
    "absent_kp": [
      "filesystems"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "c-style va_args replacement. i inherited a lot of c code with many ellipsis (variadic) functions.i have a lots of api with the following signature:void getxy(int foo, ...) // many parametersand this is used in this way as usual:getxy(1, sizex, 12, sizey, 24, 0);now i started to think about how i can replace it with a typesafe c++ api, and i came up with the following:#include <string>#include <iostream>#include <cstdarg>#include <vector>class test{ struct getparam { std::string name; int id; }; struct attr { attr(test& test) : test(test) { std::cout << attr() << std::endl; test.get_params.clear(); } ~attr() { std::cout << ~attr() << std::endl; test.end_get(); } attr& add(const std::string& name, int id) { getparam param = {name, id}; test.get_params.push_back(param); return *this; } test& test; }; void end_get() { for (auto get_param : get_params) std::cout << name: << get_param.name << , id: << get_param.id << std::endl; } std::vector<getparam> get_params;public: // old code void get1(int foo, ...) { va_list args; va_start(args, foo); const char* name = va_arg(args, const char *); for (; name != null; name = va_arg(args, const char *)) { int id = va_arg(args, int); std::cout << name: << name << , id: << id << std::endl; } va_end(args); } // new code plan attr get2(int foo) { attr attr(*this); return attr; }private:};int main(){ test test; test.get1(1, sizex, 12, sizey, 24, 0); // old style test.get2(1).add(sizex, 12).add(sizey, 24); // new style return 0;}what do you think? is there a simpler solution? how can i improve this?",
    "present_kp": [
      "c++",
      "variadic"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "nautilus opens but not displayed (debian jessie). in sometimes when i open a folder nautilus suddenly is stopped. (freezes the folders and/or graphic interfaces)then i must restart the system for solve problem because nautilus not open.i try:killall -9 nautilussudo apt-get remove nautilus && sudo apt-get install nautilusi've tried delete cache, but don't work.restart gdm with service gdm restart and service gdm3 restartclose user sessions edit: also i've tried clicked in new window, appears loading but the nautilus don't shownbut when i opens the nautilus don't shown, only can see this:how could i solve it?. thank you very much!pd: nautilus version is: gnome nautilus 3.14.1",
    "present_kp": [
      "debian",
      "gnome",
      "nautilus"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "try/catch or test parameters. possible duplicate:arguments for or against using try/catch as logical operatorsefficient try / catch block usage? i was recently on a job interview and i was given a task to write simple method in c# to calculate when the trains meet. the code was simple mathematical equation.what i did was that i checked all the parameters on the beginning of the method to make sure, that the code will not fail.my question is: is it better to check the parameters, or use try/catch?here are my thoughts:try/catch is shorter try/catch will work always even if you forget about some conditioncatch is slow in .net testing parameters is probably cleaner code (exceptions should be exceptional)testing parameters gives you more control over return valuesi would prefer testing parameters in methods longer than +/- 10 lines, but what do you think about using try/catch in simple methods just like this i.e. return (a*b)/(c+d);there are many similar questions on stackexchnage, but i am interested in this particular scenario.",
    "present_kp": [],
    "absent_kp": [
      "clean code"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is the best email address for a personal website with my name as the .com domain name?. i convinced one of my creative friends to finally purchase his own name as a domain name and start a portfolio. it has been years coming, but mission finally accomplished. now i am helping him build his website.for my own personal website, i registered pflanz.me and my personal website is travis.pflanz.me. my email address is travis at pflanz.me. i really like this idea for a personal website. i also have travispflanz.com which redirects to travis.pflanz.me, as does pflanz.me (pflanz.com was not available).while i really like this idea, he did not, and only wanted the .com, so his domain is firstnamelastname.com.one of the main reasons i went the route i did is because i couldn't come up with a suitable @travispflanz.com email address, travis at travispflanz.com just seems odd, as does me at travispflanz.com.my question, what are the best personal email addresses to use for personal full-name .com domain names?thanks!",
    "present_kp": [
      "email",
      "personal website",
      "email address"
    ],
    "absent_kp": [
      "domains"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "job queueing on a single machine. i have a shiny new server for running simulations on, with a pair of tesla gpus and 32 cores, running centos 7.2. i'd like for multiple users to be able to submit jobs to the server that get queued up and run when the previous finishes, preferably with some sort of prioritisation system and time limit, like pbs/torque but for a single machine rather than a cluster. i know i can install and configure torque for a single machine, but it seems like overkill - theoretically, the scheduler should only have to run when jobs finish or run overtime. i can probably homebrew a set of scripts, but i was wondering if a solution already exists?",
    "present_kp": [
      "centos"
    ],
    "absent_kp": [
      "scheduling",
      "process management"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "calculating the factorial of any number. i have created what was a rather simple piece of code to calculate the factorial of a number, although i wanted to ensure that my program could calculate the factorial of any given number. to do this, the only data type i thought was relevant was biginteger. the problem i had with this was that my code quickly became quite messy, nevertheless, it works! i have chosen to take the approach of using recursion to calculate the answer, as i felt it is the best way to keep the code short and efficient.i have looked at others code on this site, and read the reviews given, and it seems most users have chosen to use int, which isn't a problem, although as i say, i don't want an integeroverflow to occur when the user gives their input.is there a way to clean up this code; or make it more efficient?import java.math.biginteger;public class factorial { public static void main (string [] args) { java.util.scanner sc = new java.util.scanner(system.in); system.out.print(enter an integer to calculate the factorial of: ); biginteger temp = sc.nextbiginteger(); sc.close(); system.out.println(temp + ! = + calculatefactorial(temp)); } private static biginteger calculatefactorial (biginteger n) { if (n.compareto((new biginteger(1))) == -1) { return new biginteger(1); } else { return (n.multiply(calculatefactorial(n.subtract(new biginteger(1))))); } }}",
    "present_kp": [
      "java",
      "recursion"
    ],
    "absent_kp": [
      "performance"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to recover gmail draft. i was working on an important email in gmail over several days, on both my ipad and computer.when i look for it now, it's not there. i did not discard the draft.it's not anywhere in gmail, not in trash, not in drafts.how can i recover this email?",
    "present_kp": [
      "gmail"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "invoking a web service in a web api projectin which layer to invoke?. i am using microsoft asp.net web api 2 and one of my end points has to internally invoke a legacy non-microsoft web service (not asmx or svc) .which layer should i invoke this in?i currently have :repository layer: where all the crud calls to db are done now.domain manager: where respective manager classes invoke the repository layer methods.and my web api controller methods invoke the respective domain manager methods.should i just have another method in my repository layer which invokes the web service? and follow the usual pattern above?",
    "present_kp": [
      ".net",
      "web api"
    ],
    "absent_kp": [
      "c#",
      "web services",
      "asp.net mvc web api"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "selecting random results within doctrine. i'm new to doctrine and had a hard time deciding how to get random rows within it without creating custom dql functions or using a native query. the solution i have come up with is below, this is within a custom repository for an entity. i'm hoping for some constructive criticism in regards to it, however it currently is working. class myrepository extends entityrepository{/** * @param int $numberofresults * @return array */public function getrandomresults($numberofresults = 1){ $maxid = $this->getmax(); $count = 0; $randomnumbercounter = 0; $randomnumbers = array(); while ($randomnumbercounter < $numberofresults) { $randomnumbercounter++; $randomnumbers[] = $this->getrandom(0, $maxid); } $qb = $this->createquerybuilder('r'); while ($count < $numberofresults) { $qb ->andwhere( $qb->expr()->orx( // the greater than is to account for holes within the primary key $qb->expr()->gte(r.id, ?.$count), /* the less than is in case we have a beginning database with a large disparity between starting and ending id */ $qb->expr()->lte(r.id, ?.$count) ) ); $count++; } $result = $qb ->setparameters($randomnumbers) ->setmaxresults($numberofresults) // ensure we have no duplicates ->groupby('r.id') ->getquery(); return $result->getresult();}/** * get the maximum id that the table has * @return mixed * todo: create a cron job to set this every 6 hours */public function getmax(){ $maxid = $this->createquerybuilder('r') ->select('(max(r.id))') ->getquery() ->getsinglescalarresult(); return $maxid;}/** * @param $min * @param $max * @return int */public function getrandom($min, $max){ return mt_rand($min, $max);}}",
    "present_kp": [
      "doctrine"
    ],
    "absent_kp": [
      "php"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what do the first 16 bytes on the stack represent?. whenever i attach a process in ollydbg v1.10 on my windows 7 64-bit machine, i notice that the first saved ebp on the stack doesn't point to the very base of the stack. instead it points 16 bytes before it.to illustrate what i mean, see the following screenshot:the ebp (highlighted in gray), which is right above the return to ntdll.76fc9f45, is pointing to 1b05ffec. note that this address ends with ec, not fc.question 1: why isn't the ebp pointing to 1b05fffc?question 2: what do the first 16 bytes on the stack represent?question 3: is the number of bytes (16), which are between stackbase and the address to where the first ebp points to, fixed for windows oss?",
    "present_kp": [
      "windows",
      "ollydbg"
    ],
    "absent_kp": [
      "debuggers",
      "x86",
      "callstack"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "place the aliased version of an existing command in /usr/bin/. i use vim a lot, and i know how i can start vim in insert mode. so i have an alias named vii in my .bash_aliases file.on other hand i use git a lot too, and i have this line in my .gitconfig:[core] editor = vito write a commit message the vi editor is opened every time and i have to go in insert mode. so i thought of replacing vi with vii, and did.but the problem is when i do git commit, instead of opening the vim in insert mode, it gives this error:error: cannot run vii: no such file or directoryerror: there was a problem with the editor 'vii'.please supply the message using either -m or -f option.this makes clear that git does not looks to .bash_aliases file, even it isn't related to bash in any way. it does directly looks if there is /usr/bin/vii or not. and executes it if it is.the questioncan i place the aliased version of vi as vii in /usr/bin/?(and please don't suggest me to use git commit -m <commit message>. there are other situation where i need vim in insert mode.)",
    "present_kp": [
      "bash",
      "vim",
      "alias",
      "git"
    ],
    "absent_kp": [
      "executable"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "fizz having an argument with buzz. for my current project i need to validate responses. the requests will be send from multiple different shell scripts and should be controlled from another script. i'd never targeted a .sh with a .sh before, so let's try some fizzbuzz first.fizz.sh#!/bin/bashif [ $# -eq 0 ]then echo this fizzbuzz is interactive. please provide the upper limit. echo usage : $0 limit exit 1fifor i in 'seq $1'do echo 'source ./buzz.sh $i'donebuzz.sh#!/bin/bash([ $(($1%15)) -eq 0 ] && echo 'fizzbuzz') ||([ $(($1%5)) -eq 0 ] && echo 'buzz') ||([ $(($1%3)) -eq 0 ] && echo 'fizz') ||echo $1;is it idiomatic?is this the best way to hand all output from buzz to fizz?the goal is to learn decent bash. the fizzbuzz has no need for optimization, it's mostly about style, how data should be passed from script to script and whether i followed bash-practices or not.",
    "present_kp": [
      "bash",
      "fizzbuzz",
      "sh"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "difference between /bin/bash and .. env: ubuntu 14.04i have a file called bc on my home folder. the whole content of the file isfunc() { local a b=() echo $0}funcnote there's no #!/bin/bash at the beginning.then, if i do /bin/bash ~/bc i get/home/dev/bcas expected.however, if i source it instead . ~/bc i get the following error:bash: /home/dev/bc: line 2: syntax error near unexpected token '('bash: /home/dev/bc: line 2: ' local a b=()'i am assuming a different shell is executed for sourcing. if that is the case, then how can i change it?if i do a chsh i get dev@c1:~$ sudo chsh[sudo] password for dev: changing the login shell for rootenter the new value, or press enter for the default login shell [/bin/bash]: ------ from comments --------echo $shell returns /bin/bash",
    "present_kp": [
      "bash",
      "shell"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": ".htaccess language redirects with seo-friendly urls. how do i setup my .htaccess file to detect several languages, and redirect them to specific seo-friendly urls?basically every url needs to go to index.php?lang=(...).so, for english language detection <url> has to go to <url> (index.php?lang=en).my .htaccess as of now (not working):rewriteengine onrewritecond %{http:host} <url> %{http:accept-language} ^en [nc]rewriterule ^$ <url> [l,r=301]rewritecond %{http:accept-language} ^de [nc]rewriterule ^$ <url> [l,r=301]rewritecond %{http:accept-language} ^nl [nc]rewriterule ^$ <url> [l,r=301]rewritecond %{http:accept-language} ^fr [nc]rewriterule ^$ <url> [l,r=301]rewritecond %{http:accept-language} ^es [nc]rewriterule ^$ <url> [l,r=301]rewritecond %{request_filename} !-drewritecond %{request_filename} !-frewritecond %{request_filename} !-lrewriterule ^(en|de|nl|fr|es)$ index.php?lang=$1 [l,qsa]",
    "present_kp": [
      "htaccess",
      "redirects"
    ],
    "absent_kp": [
      "mod rewrite"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "untouchable numbers. i wrote this program in response to a codegolf challenge that required generating this sequence of untouchable numbers. this sequence can be found on oeis as a005114. my initial implementation was prohibitively slow when asked for anything other than the first few terms so, i wrote this implementation with the goal of trading off memory usage for speed by maintaining an internal state representing already calculated values.import control.applicativeimport control.monadimport control.monad.state.lazyimport data.listimport qualified data.map as mimport system.environmentmain = do n:_ <- getargs print $ evalstate (take (read n) <$> untouchables) m.empty--a state monad containing a map from integers to the sum of their proper divisorstype calcualtedsumsst = state (m.map int int) --generates an infinite list of untouchable numbersuntouchables :: calcualtedsumsst [int]untouchables = filterm untouchable [1..]--tests weather a number is untouchableuntouchable :: int -> calcualtedsumsst booluntouchable 1 = return falseuntouchable n = all (/=n) <$> mapm properdivisorsum [1..(n-1)*(n-1)]--calculated the sum of the proper divisors of a number and adds it to the state--or returns the pre-calculated if one is presentproperdivisorsum :: int -> calcualtedsumsst intproperdivisorsum n = do precalulated <- get if m.member n precalulated then return $ precalulated m.! n else do let dsum = sum $ properdivisors n put $ m.insert n dsum precalulated return dsum--generate a list of all the proper divisors of a numberproperdivisors :: int -> [int]properdivisors n = 1:(nub $ do --i think this is better than an equivalent list comprehension because --it lets me utilize divmod instead of using div and mod separately x <- [2..floor.sqrt.fromintegral $ n] let (q,r) = divmod n x if r==0 then [x,q] else [])i'm primarily looking for review of my use of the state monad as this is an area of haskell i have very little experience in. this code is still slower than i would like it, struggling to compute the first 50 terms of the sequence.",
    "present_kp": [
      "haskell",
      "state"
    ],
    "absent_kp": [
      "performance",
      "mathematics"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why vb local variables can be 'static', not 'shared'?. in visual basic .net, i can see different keyword for the same(?) concept: methods shared properties shared class-level variables sharedbut local variables staticwhy there is static and not shared in case of local variables? does different keyword indicate a different concept? or is it only due to historical reasons?to be clear - i understand that both static and shared variables are allocated on heap instead of stack and retain their value independently of instances. i would expect the same keyword then.",
    "present_kp": [
      "visual basic",
      "static"
    ],
    "absent_kp": [
      "keywords"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "developer-tool: creates large arrays with random-strings for testing-purposes. i've made this developer-tool for my own use. perhaps for colleagues too. currently i'm thinking about enhancing it. so that different data-types could be chosen.but before that i like it to be reviewed. therefore: any hints and suggestions concerning algorithm, design, user-interface, whatever welcomed. var textarea = document.queryselector('textarea');var exec = document.queryselector('#exec');var countofstrings = document.queryselector('#count-strings');var linebreak = document.queryselector('#linebreak');var arrayname = document.queryselector('#array-name');var maxselect = document.queryselector('#max');var separator = document.queryselector('#separator');// set of arbitrary words.var words = [ 'red', 'blue', 'green', 'orange', 'yellow', 'white', 'black', 'pink', 'cyan', 'crimson', 'teal', 'lime', 'alpha', 'beta', 'gamma', 'delta', 'north', 'east', 'south', 'west' ];// create the options of the select-input.(function() { var options = ''; var currentnumber; var i; const max = 5; for (i = 0; i <= max; i++) { currentnumber = math.pow(10, i); options += '<option value=' + currentnumber + ''; if (i === 3) { options += 'selected=selected '; } options += '>' + currentnumber + '</option> '; } countofstrings.innerhtml = options;})();/* creates a stringified array. this array contains * strings as elements. each string is made out * of one or multiple random-words. * * @param { array } words - an array with strings. * @param { number } countofstrings - count of strings * within the array. * @min { number } [min = 1] - minimal count of random-words. * @max { number } [max = 1] - maximal count of random-words. * @param { string } [separator = '-'] - separator between the * random-words. * @param { boolean } [linebreak = false] - add a linebreak * after each element. * @param { string } [arrayname = 'testdata'] - the name to the * returned array. * * @returns { string } - stringified array in case of success. * empty string in case of failure. */function getarraywithteststrings( words, countofstrings, min, max, separator, linebreak, arrayname ) { var ret = []; var i; // -- helper-functions ------- var checkifnumber = function(num) { return typeof num === 'number' && !isnan(num); } var getteststrings = function() { var size = math.floor(min + (math.random() * (max + 1 - min))); var ret = ''; var i; for (i = 0; i < size; i++) { ret += words[math.floor(math.random() * words.length)] + separator; } return ret.slice(0, ret.length - 1); } // -- param-checks ------------ if ( !words || !array.isarray(words) || !words.length ) { return ''; } if (!countofstrings || !checkifnumber(countofstrings)) { return ''; } if (!min) { min = 1; } else if (!checkifnumber(min)) { return ''; } if (!max) { max = 1; } else if (!checkifnumber(max)) { return ''; } arrayname = arrayname || 'testdata'; min = min || 1; max = max || 1; separator = separator || '-'; // -- assemble the array ------- ret = 'var ' + arrayname + ' = ['; for (i = 0; i < countofstrings; i++) { ret += '' + getteststrings() + ','; (linebreak && i < countofstrings - 1) ? ret += ' ' : ret += ' '; } ret = ret.slice(0, ret.length - 2); ret += '];'; return ret;}exec.addeventlistener('click', function() { textarea.textcontent = getarraywithteststrings( words, parseint(countofstrings.value), 1, parseint(maxselect.value), separator.value, linebreak.checked, arrayname.value ); textarea.select();});// -- from here on everything is just an // usage-example! ---------------------function transferstring ( somestring ) { var ret = somestring.replace(/-/g, '_'); return ret.touppercase();}// real testing is done with much larger arrays.var firstparam = [pink-cyan-pink-crimson,north-orange-crimson-black-beta-yellow,delta-beta-alpha-blue-red-alpha,beta-delta-north-beta-cyan-yellow,white-beta-black,teal-gamma-north-pink,blue,beta-delta-lime-beta-orange,orange-orange-cyan-cyan,south-white-south];var startdate = date.now();firstparam.foreach(function(item, i) { item = transferstring(item); if (/-|[a-z]/.test(item)) { console.error('test %s failed.', i); console.error(item); } else { console.log('test %s succeeded.', i); console.log(item); }});console.log( 'time needed to process the array: %s ms', date.now() - startdate );body { background-color: #f0f0f0; }.wrap { max-width: 1000px; margin: 2% auto; }.input-group label { display: inline-block; }.input-group { margin-bottom: 2%; background-color: #fafafa; border-radius: 10px; padding: 10px 10px; display: flex; justify-content: space-around; }.input-group-part { width: 45%; }textarea { width: 100%; min-height: 80px; }.input-group button { font-weight: bold; border-radius: 2px; padding: 5px 5px; }@media screen and (max-width: 800px) { .input-group { display: block; } .input-group-part { width: 100%; } .input-group-part:first-of-type { margin-bottom: 4%; } .input-group label { width: 45%; } #exec { margin-left: 47%; } }<div class=wrap> <div class=input-group> <textarea></textarea> </div> <div class=input-group> <div class=input-group-part> <label for=array-name>name of array: </label> <input type=text id=array-name value=testdata /> </div> <div class=input-group-part> <label for=separator>word-separator: </label> <input type=text id=separator size=5 value=- /> </div> </div> <div class=input-group> <div class=input-group-part> <label for=count-strings>count of strings: </label> <select id=count-strings ></select> </div> <div class=input-group-part> <label for=max>max. parts within string: </label> <select id=max> <option>1</option> <option>2</option> <option>3</option> <option>4</option> <option selected=selected>5</option> <option>6</option> </select> </div> </div> <div class=input-group> <div class=input-group-part> <label for=linebreak>add a linebreak: </label> <input type=checkbox id=linebreak /> </div> <div class=input-group-part> <button id=exec >execute</button> </div> </div></div>any help or advice is appreciated, thank you in advance.",
    "present_kp": [],
    "absent_kp": [
      "javascript",
      "css"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "download a file from a remote machine while ssh'd into it?. it often happens that i'm ssh'd into another machine and i'd like to download a file onto my local machine without having to open a new terminal and scp. for example, something like this:local $ ssh remoteremote $ lsremote_fileremote $ download remote_fileremote $ ^dlocal $ lsremote_fileis there any way even a terrible hack to let me download a file while i'm ssh'd into the remote host, without using scp?i'd considered playing some tricks with, ex, $ssh_client, but that won't work when my local machine is behind a nat (which is basically all the time).",
    "present_kp": [
      "ssh"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "discrepancy of hadamard type matrix. let $h$ be $\\{-1,+1\\}$ hadamard matrix of size $2$ and $j$ be the same size all $1$ matrix. let $w$ be $ rac{h+j}{2}$. is the discrepancy of $w^{\\otimes k}$ atmost $\\sqrt{k^{-1}}$?",
    "present_kp": [],
    "absent_kp": [
      "co.combinatorics"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how can i change the url of my google+ account?. possible duplicate:how can i get custom vanity google plus urls? i just want to add my own name to that url instead of its(google+'s) default boring alphanumerical codes......",
    "present_kp": [
      "google plus",
      "url"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "are some methods of teaching reading bad?. i have a young daughter who i am teaching to read, and i was given a your baby can read dvd set by a friend. when discussing it with friends, several of my teacher friends frowned upon the use of memorization to teach reading, and implied it would have negative long term effects. is there data to support this claim? since i expect there is a lot of research in this area, are some methods of teaching reading considered bad?for reference, the your baby can read series proposes that babies even can read even before they can talk by watching videos and looking at books which show words and then show a picture or short video that explains the word. this is intended to make the connection between the word and what it represents, and ideally guide the child in a reaction to indicate they know what it means. there is no mention whatsoever of spelling, or phonetic sounds - it is all word/picture or word/video associations one after another.",
    "present_kp": [
      "reading"
    ],
    "absent_kp": [
      "learning",
      "developmental psychology"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "opensuse tumbleweed cannot install latest version of opencv. available opencv version for tumbleweed on the website <url> is listed 3.2, but when i try installing it on my laptop version 3.1 is installed. can you please help?",
    "present_kp": [
      "opensuse",
      "opencv"
    ],
    "absent_kp": [
      "zypper"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "sort events popular in your network by date. in facebook, i like the events popular in your network page, but i want to see it in date order.possible?",
    "present_kp": [
      "facebook"
    ],
    "absent_kp": [
      "facebook events"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "grep from the end of a file to the beginning. i have a file with about 30.000.000 lines (radius accounting) and i need to find the last match of a given pattern.the command:tac accounting.log | grep $patterngives what i need, but it's too slow because the os has to first read the whole file and then send to the pipe.so, i need something fast that can read the file from the last line to the first.",
    "present_kp": [
      "grep"
    ],
    "absent_kp": [
      "text processing",
      "files",
      "tail"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why does a turing machine recognise exactly one language?. i am trying to understand the existence of non-recognisable languages. to get this, i need to know why a turing machine recognises only one language, not multiple. why is this?",
    "present_kp": [],
    "absent_kp": [
      "turing machines",
      "computation models"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "external soundcard under $100 (2.0 speakers). i'm looking for external sound card up to $100 because i'm using build on msi z87i gaming ac (or can i use internal sound card on this motherboard ?) i will be using it to gaming and music with 2.0 speakers for about $100 but i don't know which i will buy, yet (separate question).i was thinking about:asus xonar u5 - $70prodigy cube black edition - $90which product should i choose or maybe you have other more intresting propositions ?",
    "present_kp": [],
    "absent_kp": [
      "audio",
      "sound cards"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how do you limit whitespace differences for developers using different ides/environments?. i've used ultraedit for years and never had a problem with sharing code with fellow developers whether they worked on windows, mac, linux, eclipse, visual studio -- what have you...but now i am in an office where some developers are using emacs and they complain profusely about the white space differences my code introduces. i have ultraedit set to write to files as unix-style (no windows returns), but it seems the whitespace renders differently for the emacs developers.since i don't use emacs (or vi), its a pain for me to try to figure out why this is impacting them, but i more so just want them to shuttup! lolthis is even causing problems in svn/diff!any thoughts on how to fix this?forget about everyone using emacs because thats not gonna happen lol.",
    "present_kp": [
      "ide",
      "whitespace"
    ],
    "absent_kp": [
      "development environment"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "setting up apache2 port forwarding on raspbian. i am able to acess my wordpress apache 2 server on my local network using either my local ip adress or my external rounter ip address. when attempting to access my site using my external rounter ip adress from my phone that is disconneted to my wifi, it times out. the webpage imediantly changes the site url from my external router ip adress to my webserver's local static ip and then times out.i'm not sure what the problem or even what to look into. i have configure ufw to allow port 80 from anywhere. what am i missing?",
    "present_kp": [
      "raspbian",
      "port forwarding"
    ],
    "absent_kp": [
      "apache httpd"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "database backup script generating file of different sizes?. i am using the following script to create a database backup and sending it to a storage location using scp.what i noticed is that the resulting file very frequently delivers a seemingly correct file. for the most of the time, the file size is around the 3gib mark, but sometimes the resulting file is some 900b, sometimes 2.2mib.the script is:#!/bin/bash# database credentialsdatabasehost=<host>databaseuser=<user>databasepassword=<password>databaseschema=<schema>databaseenv=<env># local directory of mysqldump filelocaldir=<localdir># temporary directory for compressed filetempdir=<tempdir># remote directory for backups.remotedir=<remote-dir># username to login asbackupuser=<backupuser># backup host to login tobackuphost=<backuphost># mysqldump filemysqldumpfile=$(date +%y%m%d)_bkp_$databaseschema.sql# compressed filecompressedfile=$(date +%y%m%d)_$databaseenv_$databaseschema.tar.gz#--- end configecho $(date +%h:%m)echo creating the mysql dump mysqldump --host=$databasehost --user=$databaseuser --password=$databasepassword --single-transaction $databaseschema > $localdir/$mysqldumpfile#echo generating md5summd5sum $localdir/* > $localdir/checklist.chk#echo compressing the dump and checklisttar -cvzf $tempdir/$(date +%y%m%d)_$databaseenv_$databaseschema.tar.gz $localdir/*#echo sending the compressed file to storage locationscp $tempdir/$compressedfile $backuphost:$remotedirecho removing generated filesrm $localdir/checklist.chk > /dev/null 2>&1rm $localdir/$mysqldumpfile > /dev/null 2>&1rm $tempdir/$compressedfile > /dev/null 2>&1echo $(date +%h:%m)since most of the times the resulting file is ok, i'm thinking the problem may rely on the scp part.how can i ensure the storage location generated file is the same as the original and, if they're different, try to scp again?",
    "present_kp": [
      "backup",
      "scp"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "dynamic time zone in a running service. a unique service instance needs to change the timezone according to the data being processed. so, it can change the tz environment variable for each data before its processing, and so on.what is the best approach to change the tz value to set time zone and dst (daylight saving time) dynamically ? should it use olson format or posix format ? is possible to use olson format and turn off dst ?consider the characteristics below:time zone info's origins:the way it detects the time zone is from each information coming to be processed, which are:the absolute offset (from utc)a flag to indicate if dst must be applied or not. note that this flag doesn't say if the dst is in course, but just if the current dst settings are applicable.restrictions:the dst flag assumes that there is just one dst configuration for all possible time zones expected. but although that service is supposed to run with a group of time zones with just one type of dst, some time zones have dst and others don't.the service doesn't know the dst settings to set tz variable according to the posix format: mm.w.d/time...possible solution:the service can expects a tz already configured with a default time zone and its dst (unique for all others time zones expected), in posix format, like as tz=xxxst3xxxdt,m11.1.1/0,m3.1.1/0 for each time the service needs to change the time zone, it can change de original tz variable just adjusting the offset and clearing or keeping the original dst settings as required by dst flag. examples considering absolute offsets to this default time zone tz=xxxst3xxxdt,m11.1.1/0,m3.1.1/0:tzoffset=5, dst=false tz=xxxst5tzoffset=5, dst=true: tz=xxxst5xxxdt,m11.1.1/0,m3.1.1/0tzoffset=2, dst=true: tz=xxxst2xxxdt,m11.1.1/0,m3.1.1/0drawbacks of this solution:the service needs to run in a exclusive (separated) user with its own tz variable - or any other way of launch process with its exclusive variables.this service cannot use the olson time zone database: if the dst settings change every year, this system environment cannot use an integrated olson database with a centralized update, being necessary to do an specific configuration (by hand or a customized update) for the service's tz variable, every year in each server.the service will need a new configuration to know how to change the tz offset, if it's absolute (from utc) or if it's relative (from the time zone name)",
    "present_kp": [
      "timezone"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "is a superserver 6017r-72rftp debian-compatible?. can i install debian on supermicro superserver 6017r-72rftp?i was investigating supermicro webpage and at some point it got confusing because i was not sure do i and for i have all necessary drivers, e.g. ubuntu 12.04 server ...",
    "present_kp": [
      "debian"
    ],
    "absent_kp": [
      "linux",
      "hardware compatibility"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "bash: different output between ctrl-c vs ordered cleanup. i really tried to find a better title for this question. i am open for suggestions.i've written a bash script that traps exit and calls a function if that signal is received. it calls the same function when a file called stop exists. here it is:#!/bin/bashtail_pid=0cat_pid=0device=/dev/ttyacm0wdir=plasmalogfile=$wdir/$(date +%y%m%d_%h%m%s.log)cmdfile=$wdir/todevicefunction kill_tail{ if [ $tail_pid -ne 0 ] then kill $tail_pid tail_pid=0 echo killed tail fi}function kill_cat{ if [ $cat_pid -ne 0 ] then kill $cat_pid cat_pid=0 echo killed cat fi}function on_die{ echo 't 0' >> $device kill_tail kill_cat echo stopped logging}trap on_die exit# mount plasma oven directory if it is not already mountedmountpoint -q $wdir || sshfs user@server:plasma $wdir # see if device is available/wait for devicewhile [ ! -c $device ]do sleep 1doneecho found controller# stop output, remove start and stop filesecho 't 0' >> $devicerm $wdir/start $wdir/stop# outer loopwhile [ 1 ]do while [ ! -f $wdir/start ] do sleep 1 done rm $wdir/start # stop output echo 't 0' >> $device # pass commands to device # but clear existing commands first > $cmdfile tail -f $cmdfile > $device & tail_pid=$! echo tail pid = $tail_pid # start logging cat $device >> $logfile & cat_pid=$! echo cat pid = $cat_pid # start output echo 't 1000' >> $device echo started logging to $logfile while [ ! -f $wdir/stop ] do sleep 1 done rm $wdir/stop on_diedone # end of outer loopwhen i run this script, after touch start, it gives me different output depending on whether i ctrl-c or touch stop. this is the output after ctrl-cing:killed tailkilled catstopped loggingthis is the output after touch stop:killed tailkilled catstopped logging./mountplasma: line 93: 21200 terminated tail -f $cmdfile > $device./mountplasma: line 93: 21201 terminated cat $device >> $logfilewhy? the same function is called, and i would expect the same output from both calls to on_die. the output indicates that the two extra messages are emitted on line 93, which is done # end of outer loop (the line number does not exactly match the code above, as i had to remove a few lines for this post).as i am very unexperienced with bash, i don't know if there are any side-effects to this. both tailand cat are killed as expected.",
    "present_kp": [
      "bash",
      "kill"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "lenovo 3 and setting up wi-fi offline fedora version 22. can't seem to get the wi-fi to work.i'm rather new to linux.$ lspci | grep -i broadcom02:00.0 network controller: broadcom corporation bcm4352 802.11ac wireless network adapter (rev 03)$ uname -r4.0.4-301.fc22.x86_64i also do not have access to ethernet on the laptop since there is no wired ethernet port. i do have access to a desktop with internet, though.how can i install packages / set up wi-fi? i've been un-able to figure it out. most use 'yum' commands.",
    "present_kp": [
      "fedora"
    ],
    "absent_kp": [
      "networking"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is objective-c++ being phased out?. is apple phasing out support for objective-c++? note the following:in the objective-c language manual, there used to be a section on mixing objective-c and c++. that section is now missing.links to articles on objective-c++ on the apple developer website seem to be broken, and are now redirected, e.g. this one, which i found on this stackoverflow question.searching for c++ on the apple dev website brings very little in the way of current information.should i be concerned about using c++ for ios development?",
    "present_kp": [
      "c++",
      "apple"
    ],
    "absent_kp": [
      "objective c"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "difference between 'strace -r' and 'strace -t' options?. what's the difference between the two? in the man pages it says: -r print a relative timestamp upon entry to each system call. this records the time difference between the beginning of successive system calls. -t show the time spent in system calls. this records the time difference between the beginning and the end of each system call.my only interpretation is that -t shows the time taken just for each call, whereas -r also takes into account any time spent waiting after each syscall.this seems to be supported by the fact that when looking at corresponding snippets of an strace on the uptime command, the -r command is showing longer times per call than -t below. see below:0.000039 uname({sysname=linux, nodename=ip-172-31-55-20, ...}) = 00.000043 open(/sys/devices/system/cpu/online, o_rdonly|o_cloexec) = 30.000045 read(3, 0 , 8192) = 20.000035 close(3) = 0&uname({sysname=linux, nodename=ip-172-31-55-20, ...}) = 0 <0.000010>open(/sys/devices/system/cpu/online, o_rdonly|o_cloexec) = 3 <0.000018>read(3, 0 , 8192) = 2 <0.000014>close(3) = 0 <0.000023>is this right?",
    "present_kp": [
      "linux",
      "system calls",
      "strace"
    ],
    "absent_kp": [
      "statistics"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what encryption/compression is this, and how to encode/decode from it?. i've been trying to reverse engineer an android app's communication protocol using a combination of xposed and packet sniffing. most of the packets the app sends have this header set: content-type: application/octet-streamwhich leads me to beleive it is some sort of byte array. the sniffed packet body is encoded in something i do not recognize, but thanks to xposed, i can get the decoded version. this is what i receive from the server: <url> (raw data is at <url> ). i've tried various ways of decoding it to no avail. after hooking some methods in xposed which receive the final, decoded data, i found that the decoded version of the data i was sniffing is this:{miis:[{owner_player_id:12bea2c3d7654b66,position:12bea2c3d7654b66}],sakasho_current_asset_revision:,sakasho_current_master_revision:b324a24d2cf7cd0effb2941e9f5e515456e9e82d,sakasho_current_date_time:<phone>}i'm not sure how it got decoded, but i have some clues.first, the encoded packet has a size of 183 bytes, while the decoded version has a size of 231 bytes. this leads me to believe it's using some sort of compression. the client also sends an accept-encoding: gzip to the server which supports this, but after decoding it using gzip, i still get unreadable data, which i think means there is some sort of encryption going on. looking at the app's code, translated from smali into a rough java equivalent, this is the method that supposedly takes in a byte array and spits out a stringpublic static string a(byte[] barr) { if (barr == null) { return null; } try { return new string(barr, utf-8); } catch (throwable e) { throw new runtimeexception(can't happen!, e); } }however, while trying to replicate this in my own java program, i always get the original strange character representation of the data.what i'm wondering is, how does the original packet data get to the final, uncompressed json structure, and if it's a general encoding method, or one the app developers made themselves.thanks in advance.",
    "present_kp": [
      "encryption",
      "sniffing"
    ],
    "absent_kp": [
      "decompress",
      "function hooking"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "script injection on different hosting servers = ftp hacking?. i've been facing the exact same injected script on several of my websites for the last few weeks. today i realize these websites don't share the same hosting servers, yet the attacks (1) started at the same time (2) are identical.does this prove that my ftp passwords have been stolen (they're all stored in filezilla xml file on my hard drive) ? what else could be the security hole?",
    "present_kp": [
      "hacking"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "why can't i buy a zune pass in the uk?. as a long time us zune pass user now living in the uk, i want to buy a zune pass for here... but i can't seem to get it to work. going to the follow address brings up a page unavailable error...https://live.zune.net/en-gb/account/buysubscriptiondoes anyone know why?thanks!",
    "present_kp": [],
    "absent_kp": [
      "windows"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "access a file located with find. in many cases after i find a file using the find command i then want to open the file or cat it or maybe print it. how can i operate on the result from find? for example, : find . -name myfile.txt./docs/myfile.txt: find . -name myfile.txt | lessdoes not work because it feeds the string ./docs/myfile.txt to less, not the contents of the file at the specified path.",
    "present_kp": [
      "find"
    ],
    "absent_kp": [
      "shell",
      "command line",
      "output"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "open_basedir restriction causing huge error_log in wordpress. i am running a wordpress blog with cpanel and i get an huge error_log file (40gb) in my public_html folder.the content:warning: is_readable(): open_basedir restriction in effect. file(/var/www/html/usefulvid/wp-content/uploads/backwpup-cf4b54-logs/) is not within the allowed path(s): (/home/:/usr/lib/php:/usr/local/lib/php:/tmp) in /home/kd29314/public_html/wp-content/plugins/backwpup/inc/class-cron.php on line 76[17-apr-2017 01:02:33 utc] php warning: is_dir(): open_basedir restriction in effect. file(/var/www/html/usefulvid/wp-content/uploads/backwpup-cf4b54-logs) is not within the allowed path(s): (/home/:/usr/lib/php:/usr/local/lib/php:/tmp) in /home/kd29314/public_html/wp-content/plugins/backwpup/inc/class-file.php on line 163[17-apr-2017 01:02:33 utc] php warning: file_exists(): open_basedir restriction in effect. file(/var/www/html/usefulvid/wp-content/uploads/backwpup-cf4b54-logs) is not within the allowed path(s): (/home/:/usr/lib/php:/usr/local/lib/php:/tmp) in /home/kd29314/public_html/wp-includes/functions.php on line 1608[17-apr-2017 01:02:33 utc] php warning: is_dir(): open_basedir restriction in effect. file(/var/www/html/usefulvid/wp-content/uploads) is not within the allowed path(s): (/home/:/usr/lib/php:/usr/local/lib/php:/tmp) in /home/kd29314/public_html/wp-includes/functions.php on line 1613[17-apr-2017 01:02:33 utc] php warning: is_dir(): open_basedir restriction in effect. file(/var/www/html/usefulvid/wp-content) is not within the allowed path(s): (/home/:/usr/lib/php:/usr/local/lib/php:/tmp) in /home/kd29314/public_html/wp-includes/functions.php on line 1613[17-apr-2017 01:02:33 utc] php warning: is_dir(): open_basedir restriction in effect. file(/var/www/html/usefulvid) is not within the allowed path(s): (/home/:/usr/lib/php:/usr/local/lib/php:/tmp) in /home/kd29314/public_html/wp-includes/functions.php on line 1613[17-apr-2017 01:02:33 utc] php warning: is_dir(): open_basedir restriction in effect. file(/var/www/html) is not within the allowed path(s): (/home/:/usr/lib/php:/usr/local/lib/php:/tmp) in /home/kd29314/public_html/wp-includes/functions.php on line 1613[17-apr-2017 01:02:33 utc] php warning: is_dir(): open_basedir restriction in effect. file(/var/www) is not within the allowed path(s): (/home/:/usr/lib/php:/usr/local/lib/php:/tmp) in /home/kd29314/public_html/wp-includes/functions.php on line 1613[17-apr-2017 01:02:33 utc] php warning: is_dir(): open_basedir restriction in effect. file(/var) is not within the allowed path(s): (/home/:/usr/lib/php:/usr/local/lib/php:/tmp) in /home/kd29314/public_html/wp-includes/functions.php on line 1613[17-apr-2017 01:02:33 utc] php warning: is_dir(): open_basedir restriction in effect. file(/) is not within the allowed path(s): (/home/:/usr/lib/php:/usr/local/lib/php:/tmp) in /home/kd29314/public_html/wp-includes/functions.php on line 1613[17-apr-2017 01:02:33 utc] php warning: is_dir(): open_basedir restriction in effect. file(/) is not within the allowed path(s): (/home/:/usr/lib/php:/usr/local/lib/php:/tmp) in /home/kd29314/public_html/wp-includes/functions.php on line 1613[17-apr-2017 01:02:33 utc] php warning: is_dir(): open_basedir restriction in effect. file(/) is not within the allowed path(s): (/home/:/usr/lib/php:/usr/local/lib/php:/tmp) in /home/kd29314/public_html/wp-includes/functions.php on line 1613context of line 1613 in functions.php// we need to find the permissions of the parent folder that exists and inherit that. $target_parent = dirname( $target ); while ( '.' != $target_parent && ! is_dir( $target_parent ) ) { $target_parent = dirname( $target_parent ); }i already read that open_basedir must be changed but i have only access to cpanel and no root access. it seems that i have no access to the php.ini?.htaccess in public_html folder:# begin wordpress<ifmodule mod_rewrite.c>rewriteengine onrewritebase /rewriterule ^index\\.php$ - [l]rewritecond %{request_filename} !-frewritecond %{request_filename} !-drewriterule . /index.php [l]</ifmodule># end wordpressoptions +indexesindexoptions -fancyindexing enter code here",
    "present_kp": [
      "wordpress",
      "php",
      "cpanel"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to analyse the complexity of a problem with two or more size measures. consider this example: a problem of dimension $n$ and $m$ ($m,n$: any given integers).has a search space of size $o(n^n * m^n)$. it is clear that this problem is exponential in $n$,whatsoever $m$ may be.my question: is this same problem polynomial in $m$? what are the assumptions if we can say that?is this way of complexity analysis correct?",
    "present_kp": [],
    "absent_kp": [
      "complexity theory",
      "time complexity",
      "asymptotics"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "mathematically correct a* heuristic / distance estimator for a latitude / longitude graph. i have a graph in which each node is a geographical point on the surface of the earth, defined by it's latitude / longitude coordinates.correct ways to calculate the distance between two such points could be the haversine formula for spherical earth models, or vincenty's inverse problem for spheroidal earth models.but these are very costly in terms of computational resources, and in a* basically you don't need the absolute values of those results, you only need them for comparison purposes.in my a* algorithm the heuristic function is the shortest distance between 2 points (defined as the length of the smaller great circle arc between the 2 points in a spherical model), and the actual path between two nodes is a linestring, whose length is calculated basically in the same way, just that you sum distances between consecutive vertices.so, if d(a, b) is the actual geographical distance between a and b (as latitude / longitude points), the problem basically is to find the most computationally efficient distance estimator d*(a, b) that satisfies conditions needed for a* to work properly, such as:if d(a, b) < d(c, d) then d*(a, b) < d*(c, d). if d(a, b) + d(e, f) < d(c, d) then d*(a, b) + d*(e, f) < d*(c, d)i even saw in some places that people recommend euclidean distance for such a case, even though latitude / longitude are angles. it may be the case, but i'm interested if it's mathematically correct to assume that it satisfies conditions such as above.",
    "present_kp": [
      "math",
      "graph"
    ],
    "absent_kp": [
      "language agnostic",
      "graph traversal",
      "heuristics"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is there a light http proxy i can run without root privileges?. i am on a network where outgoing port 8080 is blocked on most machines (i.e. i cannot access services on port 8080).some of the machines on the network however do not have this restriction.i would like to run a proxy on an unrestricted machine and forward outgoing requests to 8080 via that proxy.i do not have root access on any of these machines.i have looked at squid, which compiles fine on the machine i want to run the proxy on, but as it is a service i do not think i can run it effectively without root access.i have also experimented with ssh port forwarding, but as there is no service running on the unrestricted machine i don't think ssh will solve my problem.if there is some other way to achieve this objective i am open to that too.",
    "present_kp": [
      "http proxy"
    ],
    "absent_kp": [
      "not root user",
      "web"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "which jquery slider do you suggest?. i need a slider like this one . as you can see when it's on the last part, and is needed to show the first again, it scrolls fastly to the left. i want a slider that doesn't have this behavoir. what do you suggest? all i found do the same thing. if you can find a solution for the above slider i have an open question at stackoverflow.com.thanks in advance",
    "present_kp": [
      "jquery"
    ],
    "absent_kp": [
      "javascript",
      "suggestions"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "limitation of participants google docs. i would like to create a registration form for a serie of lectures participants could sign up for. however, depending on the lectures, there is a different limitation of numbers of participants that can sign up for it. do you know if there is any possibility to specify a limited number of participants for each lecture in google docs? so far, i found the course registration template: <url> it does not allow to use it with the checkboxes but only with choose from the list question type. in the registration form i try to create, they should be able to choose more than one option (therefore use the question type checkboxes instead of choose from a list)thanks in advance for your help!can anyone help ?",
    "present_kp": [],
    "absent_kp": [
      "google spreadsheets",
      "google forms"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "live usb with kali not booting. i created a live usb according to this instructioins <url> run the dd command on a ubuntu 15.04 machine to create the live usb.the operatioon seemed to have finished successful but when restarting and selecting the usb from the boot menu, the boot menu disappears for a second and then comes back again!?i have a usb cd entry and a usb hdd entry, but both, when selecting, the menu disappears and appears shortly after again...on the machine i am trying to install kali, i have also installed w7 and ubuntu 15.04 which i also performed with a live usb!the content after running the dd command of the upper folder is total 584-r--r--r-- 1 dan dan 25 mar 12 18:51 autorun.infdr-xr-xr-x 1 dan dan 2048 mar 12 18:52 bootlr-xr-xr-x 1 dan dan 1 mar 12 18:35 debian -> .dr-xr-xr-x 1 dan dan 2048 mar 12 18:35 distsdr-xr-xr-x 1 dan dan 8192 mar 12 18:34 firmware-r--r--r-- 1 dan dan 159564 mar 12 18:51 g2ldr-r--r--r-- 1 dan dan 8192 mar 12 18:51 g2ldr.mbrdr-xr-xr-x 1 dan dan 2048 mar 12 18:51 installdr-xr-xr-x 1 dan dan 2048 mar 12 18:51 isolinuxdr-xr-xr-x 1 dan dan 2048 mar 12 18:50 live-r--r--r-- 1 dan dan 39734 mar 12 18:52 md5sum.txtdr-xr-xr-x 1 dan dan 2048 mar 12 18:34 pool-r--r--r-- 1 dan dan 366293 mar 12 18:51 setup.exedr-xr-xr-x 1 dan dan 2048 mar 12 18:51 tools-r--r--r-- 1 dan dan 228 mar 12 18:51 win32-loader.iniand the content of the boot folder efi.imglet me know if you need any further information",
    "present_kp": [
      "usb"
    ],
    "absent_kp": [
      "kali linux"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "becoming a better bug-fixer. i love being a programmer. there, i said it. however, with that said, i've realized lately that i really can't stand bug-fixing. at all. in fact, while i'm developing something, my productivity is extremely high. even when writing unit-tests and doing self testing of my development, i'm generally really productive. i can focus well, and i can get tasks done. however, when qa time comes around and i'm working on fixing bugs, my inspiration takes a massive nosedive. i have to force myself with pretty extreme measures (you know, high bpm music, excessive amounts of caffeine, etc.) to get anything done. my job is usually involved with stepping into an existing massive project and adding new features or fixing bugs, so i can't exactly tell my employer that i need a couple weeks to write unit tests for all of their code :) in addition, the server technology that we often use is very prohibitive to both unit and integration testing, as it has quite a few java classloader issues. i'm not completely against bug-fixing, sometimes it can be fun, but it's not fun at all when you have to make minor changes and wait 30 seconds to 3 minutes to be able to see if they worked or not (due to the way the system works). how can i improve my productivity and motivation when bugfixing? is this something that most programmers deal with?",
    "present_kp": [],
    "absent_kp": [
      "debugging",
      "maintenance"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is [2-party d3-rolling with maximum probability 1/2] known to imply one-way functions?. most things in complexity-based cryptography (for examples, see page 4) are known to implythe existence of one-way functions, especially after this paper proved that implication for weak coin-flipping with any constant bias. however, this paper shows a relativized world with non-trivial 2-round argument systems but no one-way functions, which makes we wonder about thepossibility of complexity-based cryptography, beyond ideal snarks, without one-way functions.with that in mind, i initially thought about multi-party versions of honest-majority coin-flippingwith negligible bias, but discovered that there were too many possible specificationsfor me to decide which of them to ask about. (for example, are there secret channels,is broadcast available, what are the requirements in case of abort?)accordingly, i'm instead asking about 2 parties choosing an element from {0,1,2}.consider the 2-party functionality: if the parties are both corrupt or both honest then choose y uniformly from {0,1,2}. otherwise, receive an element x of {0,1,2} from the adversary and choose y uniformly from {0,1,2} - {x} . in either case, output y to both parties. is it known that if there is a secure 2-party implementation of that functionality then one-way functions exist?(obviously, one doesn't need to care about what happens when both parties are corrupt.slightly less obviously, when both parties are honest, it suffices to make sure they get the same value, since the protocol could be modified to start with one party choosing an element of {0,1,2} uniformly at random and then at the end, adding that to the inner protocol's output mod 3.)",
    "present_kp": [],
    "absent_kp": [
      "cr.crypto security",
      "relativization"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why is it risky to give sudo vim access to ordinary users?. i'd like to create a new user and give him sudo access. to be specific, i want him to use sudo vim and edit httpd.conf. i wrote this in sudoers:user all=(all) /usr/bin/vim /etc/httpd/confs/httpd.confi, however, heard that this could be risky. why is this problematic? how serious is the problem?",
    "present_kp": [
      "vim",
      "sudo"
    ],
    "absent_kp": [
      "security"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "automate a gratuitous arp every x seconds. there may be another issue here, so any other questions or comments are welcome.i have a centos 6.6 vm running under virtualbox. it is connected to a gns3 topology using a udptunnel. everything works, but eventually the arp entry for the default gateway goes stale and doesn't refresh. if i manually do an arping, it starts working again, but only temporarily.other than the obvious cron job or continuous ping, is there a way to automate gratuitous arp replies in centos?",
    "present_kp": [
      "centos",
      "virtualbox",
      "arp"
    ],
    "absent_kp": [
      "linux",
      "networking"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "merge (combine) directories using rsync. i need to create a deploy script to combine the following directory structure: lib_common file1.php file2.php file3.php file4.php lib_cz file2.php file3.php file5.php file6.php...which result should look like: lib_result file1.php ...with content from lib_common file2.php ...from lib_cz file3.php ...from lib_cz file4.php ...from lib_common file5.php ...from lib_cz file6.php ...from lib_czone way to do it is: rsync lib_common/ lib_result/ --delete-afterrsync lib_cz/ lib_result/...but this will always transfer many files. other way could be:cp lib_common/ tmp/ cp lib_cz/ tmp/rsync tmp/ lib_result/ --delete-afterso, does anyone know an elegant way to achieve this?",
    "present_kp": [
      "rsync",
      "directory"
    ],
    "absent_kp": [
      "bash",
      "deployment"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "compare oracle table with sql server table and update/insert. below is the current code which is used to update/insert records from oracle view to sql server table using dapper. there is not a field to check last record updated date in the oracle view so i have added a method to get hashcode by using property values. since oracle table has more than 15k records and each record has more more than 60 columns, this approach take more than 5 minutes. any ideas/suggestions to improve below code?using system;using system.configuration;using system.data.oracleclient;using system.data.sqlclient;using system.linq;using system.reflection;using dapper;namespace syncsqlsvrwithhrdb{ internal class program { public static propertyinfo[] propertynames = typeof(employee).getproperties(); private static void main(string[] args) { var oracleconstr = configurationmanager.connectionstrings[oraclecon].connectionstring; var sqlsvrconstr = configurationmanager.connectionstrings[sqlsvrcon].connectionstring; using (oracleconnection oracon = new oracleconnection(oracleconstr)) { var res = oracon.query<employee>(constants.selectsql).tolist(); res = res.groupby(x => x.empnumber.toupper()).select(x => x.lastordefault()).tolist(); using (sqlconnection sqlcon = new sqlconnection(sqlsvrconstr)) { sqlcon.open(); for (int i = 0; i < res.count; i++) { var item = sqlcon.query<employee>(constants.selectempsql, new { empnumber= res[i].empnumber}).firstordefault(); if (item == null) // new record found { sqlcon.execute(constants.insertsql, res[i]); } else if (gethashcode(res[i]) != gethashcode(item)) // record updated { sqlcon.execute(constants.updatesql, res[i]); } } } } } public static int gethashcode(employee o) { int ret = 0; foreach (var prop in propertynames) { object propvalue = o.gettype().getproperty(prop.name).getvalue(o, null); if (propvalue != null) { ret += propvalue.gethashcode(); } } return ret; } } public class employee { public string empnumber{ get; set; } // ... other properties (70) }}",
    "present_kp": [
      "sql server",
      "oracle",
      "dapper"
    ],
    "absent_kp": [
      "c#"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how is /etc/fstab accessed before root is mounted?. i was making some changes to /etc/fstab, when this chicken and egg question occurred to me - if /etc/fstab contains the instructions for mounting the file systems, including the root partition, then how does the os read that file in the first place?",
    "present_kp": [
      "mount",
      "fstab"
    ],
    "absent_kp": [
      "boot",
      "startup",
      "root filesystem"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "associative arrays in shell scripts. i saw a trick for implementing associative arrays in a shell script. for example print array[apples] could be scripted as echo \\$array$key where key=apples.however, there was no mention of how to generate the keys to iterate over the array.the only way i could think of was to store the keys in a variable delimited by spaces so i could use a for-loop to iterate over the array.so, is there some other way to store the keys for later use?",
    "present_kp": [
      "shell script",
      "associative array"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "search for special characters like ! in gmail subject. does anyone know if in gmail you can search all messages that have an exclamation mark in the subject?i would like to create a filter that adds a red ! label to the messages that have an exclamation mark in the subject. but searching for subject:! results in all messages. i've tried things like /!/, /\\!/, (/\\!/), !, but all these searches just give me all messages, not just those with ! in the subject.",
    "present_kp": [
      "gmail"
    ],
    "absent_kp": [
      "gmail filters",
      "gmail search"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "clean, efficient and extensible code for querying collections. i recently wrote code that would query a collection of objects and rank them according to a given criteria and then filter based on an object property. i attach the code below. i would like to know if my approach can be improved upon such as made more efficient and extensible. is there a better approach or just another approach?i know that lambdas in java 8 would make the code much cleaner. what other improvements are available?here's the scenario:rank journals numerically by score, those with a shared rank ordered lexicographically and filter out journals that are review journals.here's the data:given the following journals have scores:journal a = 2.2 journal b = 6.2journal c = 6.2journal d = 1.2and journal d is a review journal.the result should be:rank journal name score 1 journal b 6.2 1 journal c 6.2 3 journal a 2.2note: journal d is filtered out of the list.here's my code:this code produces the actual sort and filter:list<journal> journals = // add journals to collection.collate<journal> collatejournals = new collate<>();collate<journal> collatedjournals = collatejournals.from(journals).filter(new outreview()).sort(new byscorethenname(direction.asc)).rank(new numerical());this code controls the collation of sorted and filtered data.public class collate<t> { list<t> collection = new arraylist<>(); public collate<t> sort(comparator<t> sortcomparator) { collections.sort(collection, sortcomparator); return this; } public t get(int i) { return this.collection.get(i); } public boolean contains(t element){ return collection.contains(element); } public collate<t> from(list<t> collection) { this.collection = collection; return this; } public collate<t> filter(predicate<t> predicate) { list<t> result = new arraylist<t>(); for (t element : (collection<t>) collection) { if (predicate.apply(element)) { result.add(element); } } this.collection = result; return this; } public collate<t> rank(rank<t> rankengine) { rankengine.dorank(collection); return this; } }this code implements a comparator interface and is used to sort the collection:note: there is also an enum called direction that i have omitted.public class byscorethenname implements comparator<journal> { direction direction; public byscorethenname(){ this.direction = direction.asc; } public byscorethenname(direction direction){ this.direction = direction; } @override public int compare(journal j1, journal j2) { int comparatorvalue = 0; if (j1.getscore() == j2.getscore()) { comparatorvalue = j2.getname().comparetoignorecase(j1.getname()); } else { if ((j1.getscore() < j2.getscore())) { comparatorvalue = -1; } else if (j1.getscore() > j2.getscore()) { comparatorvalue = 1; } } return direction.coefficent() * comparatorvalue; }}this code ranks numerically the journals based on their score, setting the journals rank in the journal object:public class numerical implements rank<journal> { @override public void dorank(list<journal> journals) { for(journal journal : journals){ int index = journals.lastindexof(journal); if(index != 0 && journals.get(index-1).getscore() == journal.getscore()){ journal.setrank(journals.get(index-1).getrank()); } else { journal.setrank(++index); } } } }this filters out the review journals. it implements a predicate interface:public class outreview implements predicate<journal>{ @override public boolean apply(journal j) { return !j.isreview(); }}and finally the journal object:public class journal implements serializable{ private static final long serialversionuid = 8964756258469682390l; private string name; private float score; private boolean review; private int rank; public string getname() { return name; } public void setname(string name) { this.name = name; } public float getscore() { return score; } public void setscore(float score) { this.score = score; } public boolean isreview() { return review; } public void setreview(boolean review) { this.review = review; } public int getrank() { return rank; } public void setrank(int rank) { this.rank = rank; } public int hashcode() { // code omitted for brevity } public boolean equals(object obj) { // code omitted for brevity } @override public string tostring() { // code omitted for brevity } }i would appreciate any kind of feedback big or small.edit:i have followed the advise given below and implemented the from() method as a static factory method as follows:public static <t> collation<t> from(list<t> items) { return new collation<t>(new arraylist<t>(items));}i have changed the names of the class and variables following the given advice:the direction enum class is now imported as a static import. this cleans the code a bit.and the array list field is now private.",
    "present_kp": [
      "java",
      "collections"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "curve fitting problem matlab. i have a system of differential equations with some unknown parameters and i need to find the optimal parameter set that fits best with the data i have.for each parameter set choice (using for loops), my code solves the system of de numerically, then extracts from the time vector a time subvector that agrees with data time values. the next step requires to extract a new output subvector for each of the variables that match with the time subvector (in order to compute the error).here is how my code is looking:oo=[];y2=(y(:,2))';t1=t';ff=[];y1=(y(:,1))';global parameters and other global variablesfor parameter_1 for parameter_2 [t,y]=ode45('system',[time interval], [initial conditions]); time; % time is a code that extracts the time subvector for i=1:length(time) for j=1:length(t1) if time(i)==t1(j); ff(end+1) = y1(j); end end end for k=1:length(time) for m=1:length(t1) if time(k)==t1(m); oo(end+1) = y2(m); end end end endendat this point the code is not over, but i would be expecting it to return the oo vector and the ff vector for the last parameter set choice of the loop. however when the code runs (without any errors) it produces vectors oo and ff that are longer than vector time: length(time) < {length(oo),length(ff)}.",
    "present_kp": [
      "matlab",
      "curve fitting"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "boundary conditions for the given pde. i'm working on the black-scholes equation, but i'm pretty new to financial modeling. right now, i am trying to understand the black-scholes pde. i understand that the black-scholes equation is given byegin{equation*} rac{\\partial c}{\\partial t} + rac{1}{2}\\sigma^2 s^2 rac{\\partial^2 c}{\\partial s^2} + rs rac{\\partial c}{\\partial s} - rc = 0\\end{equation*}with initial conditionegin{equation*}c(s,t) = \\max (s-k, 0)\\end{equation*}and boundary conditionsegin{equation*}c(0,t) = 0 \\hspace{35pt} c(s,t) ightarrow s ext{ as } s ightarrow \\infty\\end{equation*}and $c(s,t)$ is defined over $0 < s < \\infty$, $0 \\leq t \\leq t$.this can be further transformed and simplified into a heat diffusion equation as described here. if we make the following change of variableegin{equation*}u = e^{-r au}c \\hspace{20pt} ext{ or } \\hspace{20pt} c = ue^{r au}\\end{equation*}andegin{equation*}s = e^x \\hspace{25pt} ext{ and } \\hspace{25pt} t = t- au \\hspace{20pt} \\end{equation*}we get the transformed heat equationegin{equation*} rac{\\partial u}{\\partial au} = rac{\\partial^2 u}{\\partial x^2} + (k-1) rac{\\partial u}{\\partial x} - ku\\end{equation*}where $k = rac{2r}{\\sigma^2}$. the following matlab code implements this. my question is, what exactly is the form of the boundary conditions for the the transformed equation? i can't seem to understand the parameters (related to the boundary conditions) given in the matlab code. any related literature would be highly appreciated.",
    "present_kp": [
      "pde",
      "boundary conditions"
    ],
    "absent_kp": [
      "parabolic pde"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "scheduling identical jobs with cmax/ci. i have $x$ uniform machines that are identical, except that each runs at a different speed; machine $j$ runs at speed $s_j$. i have $n$ identical jobs. each machine can handle one job at a time. the time to complete a job on machine $j$ will be $1/s_j$ seconds.i want to prove that the optimal schedule that minimizes $c_ ext{max}$ is in fact also an optimal schedule for minimizing $\\sum_i c_i$. here, $c_i$ is the time when job $i$ completes (under that schedule); $\\sum_i c_i$ is the sum of completion times of all jobs; and $c_ ext{max} = \\max_i c_i$ is the time when the last job completes. in other words, i want to prove that, when minimizing $c_{max}$ with identical jobs, the optimal schedule is also the optimal schedule for minimizing $\\sum c_i$.how can i prove this?or, to put it another way, using the standard notation for scheduling problems, i want to prove that$$q|p_i=1|c_{max} \\quad \\equiv \\quad q|p_i=1|\\sum c_i.$$",
    "present_kp": [],
    "absent_kp": [
      "process scheduling"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "securely allowing users to create accounts. 1) i'm struggling to find a secure way to allow new users to create accounts without granting them admin rights to do so. if you already have an account on my system and can work unix command line, then you can use programs such as putty to access it. i'm currently making a program that lets you visually transfer files back and forth, with a layout similar to dropbox. however, i'd like to add a create new user feature and i'm not sure how to securely do it.sure, i can have the program execute something like sudo adduser jim and supply the sudo password, but this would require me to include an administrative password within the jar file. i know there are obfuscation programs out there, but it doesn't necessarily make it more secure. i'd prefer to create users with ssh, but i'm open to whatever works and is secure.2) i just thought of something. what if i created an administrative user with a their sudo ability limited to only creating new users? does this sound like a reasonably secure solution? if so, how should i go about doing this? if not, what are the cons of this method?edit: this is my solution so far: assuming i have sanitized inputs i'll make a script that will take the first argument as a username, and the second as the password, then set it's permissions (setuid bit) to be non-writable, and when non-root accounts run it, they are allowed to run this and only this with root privileges. here is the most basic code so far:adduser --disabled-password --gecos $1echo $1:$2 | chpasswdwhat do you think? how do you think i can improve it?",
    "present_kp": [
      "ssh",
      "sudo"
    ],
    "absent_kp": [
      "security",
      "not root user",
      "useradd"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "find gene at position from gff or gbk file. i have a vcf file with snps from a bacterial genome and want to find if the snps are located inside genes, is there some cli-tool where you can pass a vcf file and a gff or gbk file and it returns the name of the genes?",
    "present_kp": [
      "snp"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "one time key for authentication. in a recent project i was to create a web service that would be consumed by another application. one of the requirements was to have a key in the service that authenticated the request as valid. but this key must never be the same and the key should be invalid soon after its use.this is the code i wrote to that purpose.public static bool iskeyvalid(long testkey){ var k = getkey(); var allowedvariance = <phone>; var variance = k * allowedvariance; return (k + variance >= testkey && k - variance <= testkey);}public static long getkey(){ var i = (long)datetime.utcnow.ticks * 5; var d = (long)math.sqrt(math.ceiling(400 - (datetime.utcnow.day * math.pi))); var k = (long)((math.round((math.sqrt(i) * math.pow(math.pi + d, 3)) * math.e - d)) * d) + datetime.utcnow.minute; return k;}the point being that the caller generates a key and the receiver generates its own and then compares these two and says everything is ok if the difference is within a given variance.note that nothing in the message is really secret and this is just a simple check so that the request is probably from a valid source.i have two questions for this. how could i have done this better in terms of existing encryptionlibraries (in .net) or using other things such as certificates. this must work cross platform (windows, ios, and android devices)how can the given code be modified or improved to make it better forthe intended purpose?",
    "present_kp": [],
    "absent_kp": [
      "c#",
      "validation",
      "api",
      "cryptography"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "nth number in a infinite sequence of numbers. this was interview question.when the input is a infinite sequence of numbers starting from 1, what is the nth digit?e.g.) 123456789101112131415161718192021.....here 28th digit is 1.",
    "present_kp": [],
    "absent_kp": [
      "algorithms"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to match responses from a server with their corresponding requests?. there is a server that responds to requests on a socket. the client has functions to emit requests and functions to handle responses from the server.the problem is that the request sending function and the response handling function are two unrelated functions. given a server response x, how can i know whether it's a response to request x or some other request y?i would like to make a construct that would ensure that response x is definitely the answer to request x and also to make a function requestx() that returns response x and not some other response y.this question is mostly about the general programming approach and not about any specific language construct. preferably, though, the answer would involve ruby, tcp sockets, and php.my code so far:require 'socket'class theconnection def initialize(config) @config = config end def send(s) toconsole(--> #{s}) @conn.send #{s} , 0 end def connect() # connect to the server begin @conn = tcpsocket.open(@config['server'], @config['port']) rescue interrupt rescue exception => detail toconsole('exception: ' + detail.message()) print detail.backtrace.join(' ') retry end end def getspecificanswer(input) send get #{input} end def handle_server_input(s) case s.strip when /^hello. (.*)$/i toconsole [ server says hello ] send hello to you too! #{$1} else toconsole(s) end end def main_loop() while true ready = select([@conn, $stdin], nil, nil, nil) next if !ready for s in ready[0] if s == $stdin then return if $stdin.eof s = $stdin.gets send s elsif s == @conn then return if @conn.eof s = @conn.gets handle_server_input(s) end end end end def toconsole(msg) t = time.new puts t.strftime([%h:%m:%s]) + ' ' + msg endend@config = hash[ 'server'=>'test.server.com', 'port'=>'2020']$conn = theconnection.new(@config)$conn.connect()$conn.getspecificanswer('itemsx')begin $conn.main_loop()rescue interruptrescue exception => detail $conn.toconsole('exception: ' + detail.message()) print detail.backtrace.join(' ') retryend",
    "present_kp": [
      "ruby",
      "sockets"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "are direct solvers affect by the condition number of a matrix?. if i were to solve a relatively small problem, that is, a problem that can be handled by a direct method like lu, then does the condition number of the linear operator affect the accuracy of the solution? one of the research problems i am working on focuses on the development of optimization techniques to solve linear systems of equation, and the issues i am running into are that the condition numbers of the matrices can be very high. this would be an important factor to consider if i were to use an iterative method and preconditioner, but right now i am solving small problems (less than 1m degrees of freedom), so a direct solver is appropriate for now.",
    "present_kp": [
      "condition number"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "do self-loops in dfa cause infinite languages?. a true/false question: if a dfa $m$ contains a self-loop on some state $q$, then $m$ must accept an infinite language.the answer is false. i've read this question, but i'm still wondering why $m$ does not necessarily accept an infinite language. isn't the language $b^*$ infinite? don't all self-loops look like $b^*$?",
    "present_kp": [],
    "absent_kp": [
      "regular languages",
      "automata",
      "finite automata"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "bash scripts ran from from gnome/nautilus don't have enviroment variables. in my ~/.bashrc and ~/.profile i have some variables set to some filepaths:export my_var1=/path/to/somewherein my ~/bin/ (which is added to my path) i have a few scripts that rely on those variables being set.if i run those commands from the terminal, they work as expected.however, if i try to double click on a script in nautilus and run it, it runs as if those variables were not set.same thing happens if i add one of the scripts to gnome's startup applications using the gui utility or if i add them to the gnome menu using the main menu gui utility.i would've thought that since i included these variables in my ~/.profile, which is run at login that gnome and nautilus and processes spawned by them would have access to those variables.both gnome and nautilus are run under my login user. all my scripts in ~/bin have the bash shebang at the top.i am new to bash scripting, sorry if i am missing something obvious. i am using debian wheezy and gnome3.",
    "present_kp": [
      "bash",
      "debian",
      "gnome3",
      "bashrc"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "user permissions problem linux. every time i create a new file from eclipse, i need to run these two lines of code to get the r/w permission:chmod -r 775 /var/www/folder/filename.extensionand/orsudo chown -r www-data:www-data /var/wwwand, if i copy some files over that server folder, and access it from browser url like :localhost/folder/filename.extensioni get a permission error by apache server.isn't there any way so that i can get rid of this problem?*note: the main error occurs when my move_uploaded_file() function tries to move the file to /var/www/myproject/ directory.",
    "present_kp": [
      "files",
      "permissions"
    ],
    "absent_kp": [
      "group"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "handling errors in a client/server application. i am developing a web application and struggling to follow a clear semantic while returning response to the client. taking an example of authenticating an user there can be following scenarios:1. request is successfulthe user is authenticated and i send a json response{success: true} // and an optional message field?2. an application error occurreduser is not authenticated.{error: true, message: 'invalid email or password'} // again i am not sure if i should let client decide to show the error message3. system errorbad sql query, db crashed etc. typically server should return 500 status code but should i again send the json response with it? if yes, how much should i tell the client? because the client or user may not be interested in knowing that db crashed or sql was not well formatted.any other suggestion/modification is most welcome.",
    "present_kp": [],
    "absent_kp": [
      "web development",
      "web services",
      "error handling"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "c# smtp notifier client. i've come from a php background and i'm really trying to unlearn the bad habits i acquired from not doing things properly with php through my own lack of understanding..i'm looking to advance my understanding and implementation of oop principles. below i have a simple email notifier class. i have a few ideas of how i could do this better. i would like your guys advice as well.first i believe i would take the notifier constructor and pass in both the port & host information. when i instantiate the class i would pass these values in.both the generaterequestnotification & generateusernotification methods could be broken down into smaller pieces i believe as well...then maybe i could create a static method that would generate 'body' of the email. other than that though i'm kind of at a loss of ways to improve this code, though i'm quite certain it's in need of refactoring. currently the code is operating correctly but i think there could be improvement.public class notifier { private smtpclient _client = new smtpclient(); private list<usermanager> _tomanagers; private user _fromuser; private personnelmanagemententities personnel = new personnelmanagemententities(); public notifier() { _client.port = 25; _client.deliverymethod = smtpdeliverymethod.network; _client.usedefaultcredentials = false; _client.host = mail.xxxxxx.com; } public void sendnotificationtomanager(timerequest trfromuser) { generaterequestnotification(trfromuser); } public void sendnotificationtouser(timerequest trfrommanager) { generateusernotification(trfrommanager); } //fetch all managers associated with user private list<usermanager> findmanagerids(int x) { var managerlist = personnel.usermanagers.where(u => u.userid == x).tolist(); return managerlist; } //fetch user object associated with timerequest private user finduser(int x) { return personnel.users.where(u => u.userid == x).firstordefault(); } //create the notification private void generaterequestnotification(timerequest tr) { _tomanagers = findmanagerids(tr.employeeuserid); _fromuser = finduser(tr.employeeuserid); foreach (usermanager manager in _tomanagers) { string body = string.format(@user: {0} has submitted a time off request in timetracks. start date: {1} end date: {2} comment: {3} please login to review the request. thank you. <url>, _fromuser.adusername, tr.sdatetime, tr.edatetime, tr.comment); var tomanager = personnel.users.where(x => x.userid == manager.managerid).firstordefault(); mailmessage email_message = new mailmessage(_fromuser.email ?? _fromuser.adusername + @xxxxxxxx.com, tomanager.email); email_message.subject = timetracks notification: + _fromuser.fullname + has submitted a request; email_message.body = body; _client.send(email_message); } } private void generateusernotification(timerequest tr) { string managerresult = ((tr.managerapproval == true) ? approved : denied); _fromuser = finduser(tr.employeeuserid); string body = string.format(@your time request for {0} to {1} has been {2}. please login to review the request. thank you. <url>, tr.sdatetime, tr.edatetime, managerresult); var tomanager = personnel.users.where(x => x.userid == tr.managerid).firstordefault(); mailmessage email_message = new mailmessage(tomanager.email, _fromuser.email); email_message.subject = timetracks notification: + tomanager.fullname + has submitted a response to your request. timerequest status: + tr.managerapproval ; email_message.body = body; _client.send(email_message); }}",
    "present_kp": [
      "c#",
      "email"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "reference problem in using sed. i'm trying to use sed command to find/replace a pattern in file.it gives:sed: -e expression #1, char 27: invalid reference on 's' command's rhsmy command is:sed 's/([a-z]+),/,\\l/g' file.txt what is the problem?how can i fix it?",
    "present_kp": [
      "sed"
    ],
    "absent_kp": [
      "regular expression"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "finding the distance between two points in c++. i made a program in c++ where it calculates the distance between two coordinates. is there anything to improve? add? make it more user-friendly? code: #include <iostream> #include <cmath>using namespace std;int ch;double x;double y;double a;double b;double answer;double distancebetweentwopoints(double x, double y, double a, double b);int main(){ cout << enter the points for the coordinates; cout << endl; cout << point x for first coordinates: ; cin >> x; cout << endl; cout << endl; cout << point y for first coordinate: ; cin >> y; cout << endl; cout << endl; cout << point x for the second coordinate: ; cin >> a; cout << endl; cout << endl; cout << point y for the second coordinate: ; cin >> b; cout << endl; cout << endl; answer = distancebetweentwopoints(x, y, a, b); cout << the answer is << answer;}double distancebetweentwopoints(double x, double y, double a, double b){return sqrt(pow(x - a, 2) + pow(y - b, 2));}",
    "present_kp": [
      "c++"
    ],
    "absent_kp": [
      "beginner"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "minimize function with convex constraints. i need to solve the following problem: for a given p=(x0,y0,z0,w0) and arbitrary t. for example , let p=(0.8,0.1,0.06,0.04) and t=-1.2. i need to find a vector q=(x,y,z,w) with the minimum distance from p, under the given constraints. i need a numerical solution, i tried using matlab but i had a lot of problems with that. i will be glad for some help / ideas. thanks!",
    "present_kp": [],
    "absent_kp": [
      "convex optimization"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "assigning defaults for smarty using object-oriented style. i have a custom class for smarty that was partially borrowed. this is how the only example reflects the basic idea of using it across my current project:class template { function template() { global $smarty; if (!isset($smarty)) { $smarty = new smarty; } } public static function display($filename) { global $smarty; if (!isset($smarty)) { template::create(); } $smarty->display($filename); }then in the php, i use the following to display templates based on the above example:template::display('head.tpl');template::display('category.tpl');template::display('footer.tpl');i made the following example of code (see below) work across universally, so i wouldn't repeat the above lines (see 3 previous lines) all the time in each php file.i would just like to set, e.g.:template::defauls();that would load:template::display('head.tpl');template::display('template_name_that_would_correspond_with_php_file_name.tpl');template::display('footer.tpl');as you can see template::display('category.tpl'); will always be changing based on the php file, which name is corresponded with the template name, meaning, if for example, php file is named stackoverflow.php then the template for it would be stackoverflow.tpl.i've tried my solution that have worked fine but i don't like it the way it looks (the way it's structured).what i did was:assigned in config a var and called it $current_page_name (that derives the current php page name, like this: basename($_server['php_self'], .php); ), which returned, for e.g.: category.in my php file i used template::defaults($current_page_name);.in my custom smarty class i added the following:public static function defaults($template) { global $smarty; global $msg; global $note; global $attention; global $err; if (!isset($smarty)) { templates::create(); } templates::assign('msg', $msg); templates::assign('note', $note); templates::assign('attention', $attention); templates::assign('err', $err); templates::display('head.tpl'); templates::display($template . '.tpl'); templates::display('footer.tpl');}is there a way to make it more concise and well structured?",
    "present_kp": [
      "php",
      "smarty"
    ],
    "absent_kp": [
      "beginner"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is the difference between output and forward chains in iptables?. centos 6.0i'm studying iptables and am getting confused on the difference between forward and output chains. in my training documentation, it states: if you're appending to (-a) or deleting from (-d) a chain, you'll want to apply it to network data traveling in one of three directions: input - all incoming packets are checked against the rules in this chain. output - all outgoing packets are checked against the rules in this chain. forward - all packets being sent to another computer are checked against the rules in this chain.this confuses me because, in my mind, packets leaving for a host would be outgoing. so are there scenarios where a packet would be going to another computer but not be outgoing? how would iptables distinguish between the two?",
    "present_kp": [
      "iptables"
    ],
    "absent_kp": [
      "linux"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "boot problem with fiber nic, centos 7. strange problem here. i'm upgrading to a new desktop (hp z440) with centos 7. the install was done on a cat5 network with the built in nic. everything appeared to work fine.the final environment is on a fiber network, so a pci fiber card is installed. lspci description (on the old system):06:00.0 ethernet controller: advanced micro devices, inc. [amd] 79c970 [pcnet32 lance] (rev 54)subsystem: allied telesis at-2701fxcontrol: i/o+ mem+ busmaster+ speccycle- memwinv- vgasnoop- parerr- stepping- serr+ fastb2b- disintx-status: cap+ 66mhz- udf- fastb2b+ parerr- devsel=medium >tabort- <tabort- <mabort- >serr- <perr- intx-latency: 32 (6000ns min, 6000ns max)interrupt: pin a routed to irq 20region 0: i/o ports at d000 [size=32]region 1: memory at fa010000 (32-bit, non-prefetchable) [size=4k]expansion rom at f0000000 [disabled] [size=64k]capabilities: <access denied>kernel driver in use: pcnet32kernel modules: pcnet32code herethat appeared to be fine as well - as long as no cable is connected.once the network cable is connected to the fiber nic, it won't boot. it gets as far as the kernel select screen, then the screen goes blank for a bit. on rare occasion, i get the message:uncompression errorsystem haltedand then it just hangs. most of the time it simply kicks it back to bios and an apparently never ending loop of boot failures.the card works fine on the old system (hp 8200 elite, centos 6.2). and two different cards (same manufacturer, different model) have been tried with the same behavior.if the network cable is plugged in once the system is booted, nothing happens. haven't actually tried configuring it there yet. for some reason the network management tool (system-config-network) isn't installed.any ideas?",
    "present_kp": [
      "centos",
      "boot"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "is this a good measure of php script usage of the system?. i've been trying to see a way to get a measure of how much a php script costs of memory to the server. well, i've found some solutions out there that requires some software to make tests and even require to install something on the server. those solutions are not what i'm looking for, as i just want a simple measure of the consumption.my try was to do the following:<?php $initialmem = memory_get_usage(); /* script comes here */ $finalmem = memory_get_usage(); echo ($finalmem - $initialmem)/1024 . kbytes;?>where i divided by 1024 to convert from bytes to kilobytes. the idea was that the function memory_get_usage() gets the amount of memory allocated for the execution of the script and so i thought that taking the difference would be a good measure of the usage.is this correct? is the difference between those values a good measure of the usage of memory by the script? if not, how can i get a good measure of this usage without having to install anything on the server?",
    "present_kp": [
      "php"
    ],
    "absent_kp": [
      "performance"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "redirect input from file, but show in stdout as well?. one can redirect input from a file with the < operator. so if i have a python script, like:name = input(enter your name: )print(hello, name)then i can put the input in a file like this:boband then run it with this:$ python program.py < input.txtwhen doing this, the output looks like this:what is your name? hello bobis there any way to have the input text appear on the screen as well, so that it looks the same as if you ran the program normally? for the example above, it would look like this:what is your name? bobhello bobthe reason that i want this is because i am working on a book which has code examples, and i want to automate running the programs and having the output appear in the book. i don't know if this is possible, but it would make the whole process much easier since i can just write the programs, and setup the input files, and then have the rest happen automatically!thanks for reading!",
    "present_kp": [],
    "absent_kp": [
      "shell",
      "io redirection"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to check for no longer supported installed packages on arch linux?. recently linux-grsec became unsupported and was removed from the arch linux repositories, however i was unaware of this at the time and went for a little bit with an unsupported kernel on my system which is obviously a security risk to have such a core package not be supported any more.but now i am concerned that there may be more packages like this that have been removed from the repositories and their support on my system ended which i am simply unaware of. so as pacman does not appear to notify me about such packages installed on my system. i was wondering if there is a way to check if all currently installed packages are still available in the repositories and thus are still supported?obviously other than checking everything manually which would take ages.",
    "present_kp": [
      "arch linux"
    ],
    "absent_kp": [
      "package management",
      "repository"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "enclosing curly braces around objects, reasoning behind it?. i've seen some snippets of code do the following:calculatesum mysum = new calculatesum();{ mysum.add(50);};while other developers do it this way (without the enclosing braces:calculatesum mysum = new calculatesum();mysum.add(50);i understand that the first method ensures that the object is disposed of as soon as it falls out of scope but i thought that .net garbage collector works well enough that you do not need the above.am i wrong? is it still best practice?",
    "present_kp": [],
    "absent_kp": [
      "c#"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "split text file into lines with fixed number of words. related, but no satisfactory answers: how can i split a large text file into chunks of 500 words or so?i'm trying to take a text file (<url>) with > 10^7 words all in one line, and split it into lines with n words each. my current approach works, but is fairly slow and ugly (using shell script):i=0for word in $(sed -e 's/\\s\\+/ /g' input.txt)do echo -n ${word} > output.txt let i=i+1 if [ $i -eq 1000 ] then echo > output.txt let i=0 fidoneany tips on how i can make this faster or more compact?",
    "present_kp": [
      "sed",
      "split"
    ],
    "absent_kp": [
      "text processing",
      "awk"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "thunar keyboard shortcut to change tab. currently in thunar, to shift to the next tab, ctrl+alt+pagedown is to be used. i want to change it to either ctrl+tab or ctrl+pagedown.now, ~/.config/thunar/accels.scm is where all the shortcuts are defined in this format :(gtk_accel_path <actions>/thunarwindow/new-tab <primary>t)my question is:what is the keyword for changing tab that should replace new-tab in the above example?ctrl -> <primary>. so how about tab -> ? and pagedown -> ?",
    "present_kp": [
      "gtk",
      "thunar"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "where do spatial dimensions enter in single compartment neuronal models?. i am trying to understand how the length and diameter of a compartment are specified. for example, in the hodgkinhuxley model, we only have conductances specified in $ m ms/cm^2$. how do you specify that a compartment is say $100 \\mu m m$ long?",
    "present_kp": [],
    "absent_kp": [
      "measurement",
      "theoretical neuroscience"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "rotate a cylinder from xy plane to given points. i am given geometry of a cylinder which lies on the xy plane with 1 unit radius. and given coordinates of 2 points in 3d i need to move the cylinder so that it connects the two points.what i tried so far after searching online:calculated the vector between the two points p = p1 - p2;took the normal vector to the plane(0,0,1) vaxis of rotation = v x p (cross)angle of rotation = acos(|v|.|p|) (unit vectors)i applied this rotation.translated one of the end point of cylinder to one of the points.but it moves the object to a wrong location. any help what i'm doing wrong?edit: my apologies for not being able to respond.the main issue was with this line.angle of rotation = acos(|v|.|p|) (unit vectors)i normalized the vectors then used this wrong formula. which will always return 90 degrees.i am still not getting exactly the correct output. here is the screenshot.",
    "present_kp": [],
    "absent_kp": [
      "opengl",
      "transformations"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to draw (intuitively) the first and second principal component in pca methods?. assume i have some data in 2d. how to draw the first and second principal component in pca method?by referring to the image: the plot on the right is confusing, how i can detect the position of first pca?",
    "present_kp": [],
    "absent_kp": [
      "machine learning"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "setting periodicity in the week in google calendar. i am trying to write a generic javascript subroutine to set the periodicity in the week:var items = {0: none, 1:daily, 2: every weekday (monday to friday), 3: weekly};i would like to improve this to make it more generic and readable if possible.var callback = selectday(idvalue);createselect(items, idvalue, callback);var createselect = function (items, idvalue) { var selelem = document.createelement(select); $.each(items, function (key, value) { var ov = document.createelement(option); ov.value = key; ov.appendchild(document.createtextnode(value)); selelem.appendchild(ov); }); $(idvalue).prepend(selelem);};var selectday = function (idvalue, element) { var element = $(idvalue); var childelements = idvalue + '_'; element.find('select').change(function () { if($(this).val() === '0') { element.find('label').hide(); $(this).closest('div').find('input').attr(checked, false); } if($(this).val() === '1') { element.find('label').hide(); $(this).closest('div').find('input').attr(checked, true); } if($(this).val() === '2') { element.find('label').show(); for (var i = 0; i < 5; i += 1) { $(childelements + i).attr(checked, true); } for (var i = 5; i < 7; i += 1) { $(childelements + i).attr(checked, false); } } if($(this).val() === '3') { element.find('label').show(); $(this).closest('div').find('input').attr(checked, false); $(childelements + (new date()).getday()).attr(checked, true); } });};var items = {0: none, 1:daily, 2: every weekday (monday to friday), 3: weekly};var idvalue = '#contest_data_updateperiodicity_days';createselect(items, idvalue);selectday(idvalue);my questions are:how can i ameliorate this piece of code? how can make a callback in order to make something like this?jsfiddle",
    "present_kp": [
      "javascript"
    ],
    "absent_kp": [
      "jquery",
      "datetime"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "soda crate program. i would love to have some suggestion for my program, i would prefer no more suggestion to use linq and other advanced concepts as i would like to keep it as simple as possible for now. i love suggestion on the structure of the program and methods and so on. if needed to condense the program into the basics, only the things in the menu are needed. maybe i should use an constructor in the beveragedata class instead of property? the menu class is in the bottom of the code block. i've tried to split it up for readability. the code is working and without bugs as far as i know. public enum beveragetype { soda, juice, water, alcohol } public class beveragedata // skapar { public string name { set; get; } public beveragetype type { set; get; } public decimal price { set; get; } public int size { get; set; } public override string tostring() { return ${name} ({type}) {size}{cl} = {price:c}; } }the beverage class, everything related to the list of beverages here. public class beverages { private static readonly random rnd = new random(); public static beveragedata getbeveragefromuser() { return new beveragedata { name = getname(enter name: ), price = getcurrency(enter cost: ), type = getbeveragetype(enter type: ), size = getsize(enter size: ) }; } public static beveragedata getrandombeverage() { var names = new list<beveragedata> { new beveragedata {name = coke, price = .75m, size = 12, type = beveragetype.soda}, new beveragedata {name = pepsi, price = .75m, size = 12, type = beveragetype.soda}, new beveragedata {name = sprite, price = .75m, size = 12, type = beveragetype.soda}, new beveragedata {name = rootbeer, price = .75m, size = 12, type = beveragetype.soda}, new beveragedata {name = orange juice, price = .5m, size = 10, type = beveragetype.juice}, new beveragedata {name = apple juice, price = .5m, size = 10, type = beveragetype.juice}, new beveragedata {name = grape juice, price = .5m, size = 10, type = beveragetype.juice}, new beveragedata {name = water, price = .25m, size = 20, type = beveragetype.water}, new beveragedata {name = beer, price = 2.75m, size = 16, type = beveragetype.alcohol}, new beveragedata {name = wine, price = 3.5m, size = 9, type = beveragetype.alcohol}, }; return names[rnd.next(names.count)]; } private static beveragetype getbeveragetype(string message) { beveragetype beveragetype; console.write(message); while (!enum.tryparse(console.readline(), true, out beveragetype)) { console.foregroundcolor = consolecolor.red; console.writeline(invalid beverage type); console.resetcolor(); console.write(valid beverage types are: ); console.foregroundcolor = consolecolor.yellow; console.writeline(string.join(, , enum.getnames(typeof(beveragetype)))); console.resetcolor(); console.write(message); } return beveragetype; } private static string getname(string message) { console.write(message); return console.readline(); } private static decimal getcurrency(string message) { decimal result; console.write(message); while (!decimal.tryparse(console.readline(), numberstyles.currency, cultureinfo.currentculture, out result)) { console.foregroundcolor = consolecolor.red; console.writeline(invalid number); console.resetcolor(); console.write(message); } return result; } private static int getsize(string message) { int result; console.write(message); while (!int.tryparse(console.readline(), numberstyles.currency, cultureinfo.currentculture, out result)) { console.foregroundcolor = consolecolor.red; console.writeline(invalid size); console.resetcolor(); console.write(message); } return result; } }here is my beverage crate class, where there crate is represented and sort of the base class of the program. class crate : ienumerable<beveragedata> { private beveragedata[] crate = new beveragedata[24]; private int numberofbottles = 0; private const int maxitems = 24; public void add(beveragedata beverage) { if (numberofbottles >= maxitems) { console.writeline(the crate is full. please remove an item first!); } else { crate[numberofbottles] = beverage; numberofbottles++; } } public ienumerator<beveragedata> getenumerator() { return crate.asenumerable().getenumerator(); } ienumerator ienumerable.getenumerator() { return getenumerator(); } public void remove(string name) { try { remove(crate.firstordefault(i => i.name.equals(name, stringcomparison.ordinalignorecase))); } catch (exception) { console.writeline(please only enter name); } } public void remove(beveragedata beverage) { int index = array.indexof(crate, beverage, 0, numberofbottles); if (index < 0) return; this.removeat(index); } /// <summary> /// removes the element at the specified index of the beverage array. /// </summary> /// <param name=index>the zero-based index of the element to remove.</param> public void removeat(int index) { if (index < numberofbottles) { numberofbottles--; array.copy(crate, index + 1, crate, index, numberofbottles - index); crate[numberofbottles] = default(beveragedata); } } public void printcrate() { if (numberofbottles == 0) { console.writeline(there are no items in the crate.); } else { foreach (var beverage in this) console.writeline(beverage); } } public void findbeverageincrate() { string cratebeveragename = console.readline(); for (int i = 0; i < crate.length; i++) { var bottle = crate[i]; if (bottle == null) { continue; } if (crate[i].name == cratebeveragename) { console.writeline(found your beverage {0}, cratebeveragename); } } } public void sortcrate() { int max = crate.length; //yttre loop fr att f med hela for (int i = 1; i < max; i++) { //inre loop fr att g rad per rad int nrleft = max - i; for (int j = 0; j < (max - i); j++) { var bottle1 = crate[j]; var bottle2 = crate[j + 1]; if ((bottle1 == null) || (bottle2 == null)) // kontrollerar att ingen av dem r tom innan den byter plats { continue; } if (bottle1.name.compareto(bottle2.name) == 1) // byter plats { var temp = crate[j]; crate[j] = crate[j + 1]; crate[j + 1] = temp; } } } } public decimal calculatetotalincrate() { decimal summa = 0; foreach (var bottles in crate) { if (crate != null && bottles != null) summa += bottles.price; } console.writeline(the total value of the crate is: , summa); return summa; } public static int getint(string message) { int result; console.write(message); while (!int.tryparse(console.readline(), out result)) { console.foregroundcolor = consolecolor.red; console.writeline(invalid number); console.resetcolor(); console.write(message); } return result; } }and finally the menu class, the starting point of the program.* class menu : crate { static void main(string[] args) { showmenu(); console.writeline( done! press any key to exit...); // vntar efter input en sista gng efter anvndaren har tryckt 0 console.readkey(); } public static void showmenu() { bool exit = false; var beverages = new crate(); // skapar en instans av klassen crate som beverage do { console.writeline([1] add custom beverage to array); console.writeline([2] remove bottle from bottle crate); console.writeline([3] show bottle crate); console.writeline([4] autofill crate); console.writeline([5] sort bottles in bottle crate according to name); console.writeline([6] calculate the total cost in crate); console.writeline([7] search in crate after beverage); console.writeline([9] empty the crate); console.writeline([0] exit the program); consolekeyinfo info = console.readkey(); int selection; int.tryparse(info.keychar.tostring(), out selection); switch (selection) // anvnder switch fr att kunna se vad { case 1: var numbevs = math.max(getint(how many beverages would you like to enter: ), 0); for (int i = 0; i < numbevs; i++) { console.writeline($ enter beverage #{i + 1} info); console.writeline(-----------------------); beverages.add(beverages.getbeveragefromuser()); // calls the add function before the getbeveragefromuser method console.writeline(----------------------- ); } console.clear(); break; case 2: console.clear(); console.foregroundcolor = consolecolor.black; console.writeline(please enter the name of the beverage you would like removed: ); beverages.remove(console.readline()); console.writeline(your beverage has been removed); break; case 3: console.clear(); console.writeline( here are the contents of the crate: ); console.foregroundcolor = consolecolor.green; beverages.printcrate(); console.resetcolor(); break; case 5: numbevs = math.max(getint(how many would you like to be auto-added: ), 0); for (int i = 0; i < numbevs; i++) { beverages.add(beverages.getrandombeverage()); } console.writeline(----------------------- ); console.clear(); break; case 6: console.clear(); beverages.sortcrate(); console.writeline(the crate has been sorted); break; case 7: console.foregroundcolor = consolecolor.red; beverages.calculatetotalincrate(); console.resetcolor(); break; case 8: beverages.findbeverageincrate(); console.writeline(this is your beverage?); break; case 9: showmenu(); console.clear(); console.writeline(your crate has been emptied); break; case 0: exit = true; break; default: console.clear(); console.writeline(an error has occured, try again); break; } } while (!exit); } }",
    "present_kp": [],
    "absent_kp": [
      "c#",
      "beginner"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how do i contact a domain owner who doesnt use email? (private registration). so there is this .com usa based domain set up wrong and is dumping traffic onto our dns. this domain is privatized with namecheap whois masking, and since it's wrong routing, i can't send emails through the methods they provide. the owner is an elderly gentleman and doesn't even reply to mails sent to his gmail (which i had to sleuth). his name is too broad to find a phone or address.how do i get ahold of this guy? namecheap is being totally amateur claiming its not an abuse issue, they can't help, this and that, keep telling me to email even though it's broken. i have gone as far as sending a mail right to the ceo -- no go. they claim they don't offer phone numbers as a method to contact an owner. it's like the twilight zone run by a bunch of kids over there at namecheap. can i go above them somehow to get this point of contact such as phone number?",
    "present_kp": [
      "dns",
      "whois"
    ],
    "absent_kp": [
      "ownership"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "run fish script in background?. i'd like to always run a fish script in the background even if the user doesn't specify that.in bash, this can be done by surrounding the script with ( at the start and ) & at the end.is there anyway for a fish script to run itself in the background?",
    "present_kp": [
      "fish"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "friendly terminal color names in shell scripts?. i'm aware of libraries in languages such as ruby and javascript to make colorizing your terminal scripts easier by using color names like red.but is there something like this for shell scripts in bash, or ksh, or whatever?",
    "present_kp": [
      "bash",
      "shell script",
      "terminal"
    ],
    "absent_kp": [
      "colors"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "modifying a file pointed to by a lot of snapshots. i'm using btrfs and snapper. so, i have many periodic btrfs snapshots, which are marked read-only (can of course be changed by btrfs to r/w and changed back.)let's say during my initial install i set a vimrc. and, let's say now i want to change the vimrc, so that if i go back to any of the snapshots my vimrc is the new one. yeah, this totally re-writes history, and goes against the idea of having historical backups. but let's say that i change active snapshots often for testing and don't want to keep having to re-change certain configuration files like this. and i'm ok with blowing away my backups of the file.right now, there should be only one copy of the vimrc file contents that are being used/pointed to, since it's never been modified since the very first snapshot.is there a way i can modify the vimrc file, so that all of them are updated?can i bypass copy on write during the edit, doing so just for the one file, so any system files in he background still use cow? i see i can't use chattr's nocow because the file isn't new or empty. is there another way?or, do i have to edit the live file, turn all the snapshots to r/w, perform a cp -ax --reflink=always to all the snapshot files, and turn all the snapshots to r/o? there's a lot of snapshots to do this to. (granted a script would make this less painful.)",
    "present_kp": [
      "btrfs"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "reset counter when change occurs while reading. i have a file that contains something like the following: red dog red cat red bird red horse blue hamster blue monkey blue lion pink pony pink whale pink pig pink dolphin i need to increment a counter for every color, and then for every animal. so red would be 1, blue 2, pink 3. next, dog, cat, bird, and horse would be 1, 2, 3, and 4. i need hamster to begin at 1 again because we are starting a new color. if i do a while read color animal of said file, what can i do to compare when color is no longer equal to the previous color? i am looking for something like this:1.1 1.2 1.3 1.4 2.1 2.2 2.3 3.1 3.2 3.3 3.4 any suggestions would be greatly appreciated :)",
    "present_kp": [
      "read"
    ],
    "absent_kp": [
      "text processing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to do a sed substitution (s///g) based on a list? i need to swap multiple words, with other corresponding words. i don't think this question has been asked before, so i don't know if sed is capable of this.suppose i have a bunch of numbers in a sentence that i need to expand into words, a practical example being to swap the numbered citations in a typical essay into mla format:essay.txt:sentence 1 [1]. sentence two [1][2]. sentence three[1][3].key.txt (this is a tab delimited file):1 source-one2 source-two3 source-three...etcexpected result.txt:sentence 1 [source-one]. sentence two [source-one][source-two]. sentence three[source-one][source-three]here's my pseudocode attempt, but i don't understand enough about sed or tr to do it right: cat essay.txt | sed s/$(awk {print $1} key.txt)/$(awk {print $2} key.txt)/gps: if there's a trick in notepad++ for mass find-and-replace using multiple terms, that'd be great. as it is, it seems like find-and-replace only works for one term at a time, but i need a way to do it en masse for many terms at once.",
    "present_kp": [
      "sed",
      "tr"
    ],
    "absent_kp": [
      "text processing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is this technique a design pattern? if so, what's it called?. i'll use c# as an example, but it should apply globally. say i have a string value that should be one of a few constants, but i also want the client to set which string value to use so:private int foo;private string bar;public int foo { get { return foo; } set { foo = value; bar = getstringvaluefromdatabase(value); }}public string bar { get { return bar; } }i use this technique quite a lot and want to know if it's considered as any formal concept.",
    "present_kp": [],
    "absent_kp": [
      "design patterns"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why is conway's game of life used for code retreats?. code retreat is an all-day training event that focuses on the fundamentals of software development. there's a global code retreat day coming up, and i'm looking forward to it. that said, i've been to one before and have to say there was a huge amount of chaos... which is fine.one thing that i still don't get is why the game of life is a good problem for tdd, and what good and bad tdd for it feels like.realize this is a pretty open ended question, so feel free to comment.",
    "present_kp": [
      "tdd"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how can i tar.gz a huge directory on a shared host? the process keeps getting killed. i'm working on moving a site to a new host for someone and i'm having trouble with their media. they have a directory with thousands and thousands of images and videos. i attempted to zip this directory using the following:tar -zcvf media.tar.gz path/to/directorybut it gets about 3/4 of the way through and then just stops with a message that just sayskilledhow can i compress this directory? should i just compress it in pieces? this site is hosted on hostgator.",
    "present_kp": [],
    "absent_kp": [
      "web hosting",
      "migration",
      "gzip"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to loop a command through a file column values?. i have a simple command like this grep 'x' results.dat | awk '{print $nf}' > y.dati want to loop this command taking the xs from column 1 and the corresponding ys from column 2 of the same file eg. names names file has the formatc11-c12 p01c13-c14-c17 p02etc ..so the first two steps in the loop should be like thisgrep 'c11-c12' results.dat | awk '{print $nf}' > p01.datgrep 'c13-c14-c17' results.dat | awk '{print $nf}' > p02.dat",
    "present_kp": [],
    "absent_kp": [
      "bash",
      "shell script",
      "text processing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is there a stable heap?. is there a priority queue data structure that supports the following operations?insert(x, p): add a new record x with priority pstableextractmin(): return and delete the record with minimum priority, breaking ties by insertion order.thus, after insert(a, 1), insert(b, 2), insert(c, 1), insert(d,2), a sequence of stableextractmin's would return a, then c, then b, then d.obviously one could use any priority queue data structure by storing the pair $(p, time)$ as the actual priority, but i'm interested in data structures that do not explicitly store the insertion times (or insertion order), by analogy to stable sorting.equivalently(?): is there a stable version of heapsort that does not require $\\omega(n)$ extra space?",
    "present_kp": [],
    "absent_kp": [
      "ds.data structures"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "convert one column of multicolumn output regexreplace from string to number. in an expression of the form (one column in, two out):=arrayformula(regexreplace(a1:a100,...,{$1,$2}))how can i convert just one of the output columns from text to a number (like value)?the documentation says:if a number is desired as the output, try using the value function in conjunction with this function. however, if i do this:=arrayformula(regexreplace(a1:a100,...,{value($1),$2}))i receive the error (1 1 in this example):function regexreplace parameter 3 expects text values. but '1' is a number and cannot be coerced to a text.how can i convert just one of the output columns to a number?i have a complicated expression that produces output like this, and i need to sort() on one of the numeric output columns, but i need to convert that column to a number or find some other way to properly sort it, because it sorts incorrectly as text (e.g. 1, 10, 2), so if that's not possible, how can i sort in numerical order on a text column in an array?",
    "present_kp": [],
    "absent_kp": [
      "google spreadsheets"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "facebook getting search history from gmail, linkedin and twitter. recently, in my facebook search bar i've noticed it shows recent searches that i didn't search on facebook. for instance, i looked at paul graham and scott adams' twitter feeds. this showed up in my facebook search history:<url> found searches there fromgmail (an email draft i made on my phone)linkedin (pages i visited, not searches)twitter (also direct page visits)are these sites selling facebook data? i've got a decently restricted setup:ios, no facebook app or messenger. (however, facebook login is enabled, with access to my calendar.)i browse on firefox, with noscript, umatrix and ublock origin. i have most things disabled on umatrix.how are they doing it, and is there anything further i can do? or is it data sales through data gathered by first party scripts?i'm grateful facebook is showing this search history from elsewhere so i can see their data sources.",
    "present_kp": [
      "gmail",
      "facebook",
      "twitter",
      "linkedin"
    ],
    "absent_kp": [
      "privacy"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to remove a bot integration from slack?. i want to completely remove a bot i created on slack. i can disable it but not completely remove it. any ideas?",
    "present_kp": [
      "slack"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "what's bootstrap.css and why and how is it used. i've seen so many templates having this bootstrap.min.css file which also was like 125kbs. what is it used for?",
    "present_kp": [
      "css"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "why is the size of embeded presentations in google sites restricted?. when i embed a google presentation into my google site, the maximum size offered is large (700px). i would like to use the entire width of the page. why is this limited and/or is there a way to overcome this restriction?",
    "present_kp": [
      "google sites"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how do people manage to write and maintain extremely complex and hard to read code?. reading sqlite source code is imo mission impossible. yet it is a usable piece of quite complex software (it's a full-blown embedded database after all) that can be downloaded, compiled and used from others code and it is constantly updated.how do people manage to write and maintain such extremely complex and hard to read code?",
    "present_kp": [
      "source code"
    ],
    "absent_kp": [
      "maintenance",
      "readability"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "behavior of grep -o on osx. on linux, when i do echo foo | grep -o '^.' to get the first character, it prints f as expected. (i know, | cut -c1 would probably be faster.)but on osx, doing the same results in 3 matches (3 lines):foowtf? even without the -o option, the color of the output (in my environment) tells me that the whole line matches, instead of just the first character.can anybody please enlighten me?",
    "present_kp": [
      "grep",
      "osx"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "name of an exact cover by 3 sets variant. exact cover by 3-sets is $\\sf{np}$-complete:instance: given a finite set $x = \\{ x_1,x_2,...,x_{3n}\\}$ of $3n$ elements and a collection $c = \\{ ( x_{i_1}, x_{i_2}, x_{i_3}) \\} $ of $m$ 3-elements subsets of $x$;question: find a subcollection $c'$ of $c$ such that every element in $x$ is contained in exactly one member of $c'$.the problem remains npc even if we add the following condition:every element of $x$ appears exactly in three subsets of $c$has this variant an official name?",
    "present_kp": [],
    "absent_kp": [
      "terminology"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is there an operating system named linux?. i found that the operating system used in my college labs to learn linux or unix is ubuntu. is there an operating system named linux?",
    "present_kp": [
      "linux"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "slow mysql query - 1200 rows in 5secs. can anyone help me speed up this query? at present, it returns 1200 rows in 5 secs. i've notice that it is looking at 240000 response records. i think this is where the issue may be.i've created the following indexes:create index idx_eventid on 'action' (eventid);create index idx_actionid on 'response' (actionid);create index idx_date on 'response' ('date');create index idx_stockid on 'eventstocklink' (stockid);create index idx_eventid on 'eventstocklink' (eventid);create index idx_cusid on 'event' (cusid);select statement:select response2.actionid, response2.typeid, response2.notes, response2.eventid, response2.actiondate, response2.userid, response2.eventtype, response2.firstname, response2.surname, response2.postcode, response2.eventtypeid, response2.dealtrue, response2.dealid, response2.eventpic, response2.registrationnumber, response2.deptlinkid, response2.customtype, response2.enquiryid, response2.eventstocklinkid, response2.cusid, response2.stockid, response2.custitle, response2.actiontypeid, response2.deptbut, response2.cushomtel, response2.cusworktel, response2.cusmobtel, response2.cusadd1, response2.cusadd2, response2.cusadd3, response2.cuscounty, response2.cushomemail, response2.cusworkemail, response2.responsetype, response2.date, response2.done, response2.responsebut, response2.reasonid, response2.responseid, response2.depttype, response2.responsetypeid, response2.username, response2.actionusername, diarytime.diarytime, response2.prospectmake, response2.prospectmod, response2.prospectnu, response2.statedesc, response2.site from diarytime left join (select action.actionid, action.typeid, response.notes, action.eventid, action.actiondate, response.userid, eventtype.event as eventtype, cus.firstname, cus.surname, cus.postcode, event.typeid as eventtypeid, if(isnull(deal.dealid), 0, 1) as dealtrue, if(isnull(deal.dealid), 0, deal.dealid) as dealid, eventtype.eventpic, if( isnull(stock.registrationnumber), 0, stock.registrationnumber ) as registrationnumber, event.deptlinkid, action.customtype, prospect.enquiryid as enquiryid, action.eventstocklinkid, event.cusid, eventstocklink.stockid, cus.custitle, action.actiontypeid, dept.deptbut, cus.cushomtel, cus.cusworktel, cus.cusmobtel, cus.cusadd1, cus.cusadd2, cus.cusadd3, cus.cuscounty, cus.cushomemail, cus.cusworkemail, responsetype.responsetype, response.date, response.done, responsetype.responsebut, response.reasonid, response.responseid, dept.depttype, response.typeid as responsetypeid, response.username, response.username as actionusername, prospect.stockmake as prospectmake, prospect.stockmod as prospectmod, prospect.otdbtype as prospectnu, stockstate.statedesc, site.site from response inner join users_eden.users as users on users.userid = response.userid inner join action on response.actionid = action.actionid left join responsetype on responsetype.responsetypeid = response.typeid left join event on event.eventid = action.eventid left join eventtype on eventtype.eventid = event.typeid left join cus on cus.cusid = event.cusid left join deal on deal.dealid = action.dealid left join enquiries as prospect on prospect.actionid = action.actionid left join deptlink on deptlink.deptlinkid = event.deptlinkid left join dept on dept.deptid = deptlink.deptid left join site on site.siteid = deptlink.siteid left join eventstocklink on eventstocklink.eventstocklinkid = action.eventstocklinkid left join stock on stock.stockid = eventstocklink.stockid left join stockstate on stockstate.stateid = eventstocklink.statusid where ucase(response.reasonid) <> 'first' and ucase(response.reasonid) <> 'cancelled' and ucase(response.reasonid) <> 'website' and date(response.date) = '20130228' order by date(response.date) asc, time(response.date) asc) as response2 on hour(response2.date) = hour(diarytime.diarytime)results of explain:id select_ty table type poss_keys key key_len ref rows extra1 primary diarytime index idx_diarytime 4 24 using index 1 primary <derived2> all 1119 2 derived response all idx_actionid 240542 using filesort 2 derived action eq_ref primary primary 4 response.actionid 1 2 derived users eq_ref primary primary 4 response.userid 1 using index 2 derived responsetype eq_ref primary primary 4 response.typeid 1 2 derived event eq_ref primary primary 4 action.eventid 1 2 derived eventtype eq_ref primary primary 4 event.typeid 1 2 derived cus eq_ref primary primary 8 event.cusid 1 2 derived deal eq_ref primary primary 4 action.dealid 1 using index2 derived prospect ref idx_actionididx_actionid 5 action.actionid 1 2 derived deptlink eq_ref primary primary 4 event.deptlinkid 1 2 derived dept eq_ref primary primary 4 deptlink.deptid 1 2 derived site eq_ref primary primary 4 deptlink.siteid 1 2 derived eventstocklink eq_ref primary primary 4 action.eventstocklinkid 1 2 derived stock eq_ref primary primary 8 eventstocklink.stockid 1 2 derived stockstate eq_ref primary primary 4 eventstocklink.statusid 1",
    "present_kp": [
      "mysql",
      "sql"
    ],
    "absent_kp": [
      "optimization",
      "performance"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "ais' self-evaluation threshold. is it possible that at some time in the future, ais will be able to initiatively develop themselves, rather than passively being developed by humanity?",
    "present_kp": [],
    "absent_kp": [
      "neural networks",
      "machine learning",
      "unsupervised learning",
      "self learning"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "regular expression representing the following language. i am having trouble understanding how to write a regular expression for the set of words that contain at least two b's and at least two a's, where the alphabet is {a,b}.i understand that set of words that contain at least two b's is: [1] (a+b)*b(a+b)*b(a+b)* and the set of words that contain at least two a's is: [2] (a+b)*a(a+b)*a(a+b)*. in addition, i understand that the set of words that contain at least one a and at least one b is: [3] (a+b)*(ab+ba)(a+b)* (or at least i think that is correct).i know the easy solution would be to use an intersection between [1] and [2]; however, i would like to understand how to accomplish this without using an intersection.i know i would need to have (a+b)* at the beginning and at the end to say that any string made up of a's and b's can be at the beginning and at the end. but i am having trouble understanding the logic between those two points.just looking for some direction as i do not know where to go from there.",
    "present_kp": [],
    "absent_kp": [
      "finite automata",
      "regular expressions"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "google spreadsheet scatter chart with multiple data series. i want to create a scatter chart with google spreadsheet which turns out to be very difficult. based on a similar question here. i came up with this approach:what i want to achieve is sketched here:any idea how i can get google spreadsheet to do that?",
    "present_kp": [],
    "absent_kp": [
      "google spreadsheets",
      "google spreadsheet charts"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "disable location or language in google search?. is it possible to turn off for example everything in a certain language (hl parameter) or disable everything from a certain geographic location in google search?the reason is that i'm searching for very specific information and try to narrow it.",
    "present_kp": [
      "google search"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "problem understanding 'hash' command in a .sh file. so i wanted to install etherpad lite on a linux machine.if i try to run it, i will get the error please install node.js ( <url> )the command 'which node' gives me the correct path to node js.so i went into the .sh file of etherpad lite and found this: #is node installed? hash node > /dev/null 2>&1 || { echo please install node.js ( <url> ) >&2 exit 1 }i guess it means: check for node --> if not available print line and exit. but what exactly does this code do? what does hash do? what's with all these & and > ? anybody who can explain to me this 3 lines would be really appreciated.",
    "present_kp": [],
    "absent_kp": [
      "bash",
      "shell"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to keep velocities in check in molecular dynamics simulation?. i am trying to make a very simple molecular dynamics simulator with reflective boundary conditions. i am assigning the initial positions in a cube randomly while making sure they are not too close to each other and keeping the initial velocities zero. i am using the lennard-jones potential and velocity-verlet algorithm. but as soon as i start the simulation, velocities of some of the particles reach very high value and total energy also goes to very high value(not consistent with the initial total energy). can anyone point out what are the possible reasons for such a behaviour?here is my code (in fortran).program mol_dyn_ref implicit none double precision,allocatable,dimension(:) :: posx,posy,posz,velx,vely,velz,ax,ay,az,tempx,tempy,tempz integer,allocatable,dimension(:) :: seed double precision :: x,y,z,m,rad,eps,kin,pot,k,p,avgvx,avgvy,avgvz double precision :: xinit,yinit,zinit,xlen,ylen,zlen,mindist integer :: i,j,st,seedsize,n,iter,t double precision :: tot_t,dt,dist,r,fx,fy,fz interface subroutine acc(n,posx,posy,posz,m,eps,tempx,tempy,tempz) implicit none integer :: i,j,n double precision,dimension(n) :: posx,posy,posz,ax,ay,az,tempx,tempy,tempz double precision :: r,dist,fx,fy,fz,eps,m end subroutine acc subroutine pos_upd(n,posx,posy,posz,velx,vely,velz,ax,ay,az,dt,xlen,ylen,zlen,rad) implicit none double precision,dimension(n) :: posx,posy,posz,velx,vely,velz,ax,ay,az integer :: n double precision :: tot_t,dt,xlen,ylen,zlen,rad end subroutine pos_upd subroutine vel_upd(n,velx,vely,velz,ax,ay,az,dt,tempx,tempy,tempz) implicit none integer n double precision,dimension(n) :: velx,vely,velz,ax,ay,az,tempx,tempy,tempz double precision :: dt end subroutine vel_upd end interface n=50 !number of particles eps=1.0d0 m=1.0d0 xlen=20.0d0 ylen=20.0d0 zlen=20.0d0 xinit=0.0d0 yinit=0.0d0 zinit=0.0d0 mindist=1.5d0 rad=mindist/2.0d0 tot_t=10.0d0 dt=0.0001d0 iter=int(tot_t/dt) allocate(posx(n),posy(n),posz(n),velx(n),vely(n),velz(n),ax(n),ay(n),az(n),tempx(n),tempy(n),tempz(n)) open(100,file=pos.dat,status=replace) open(200,file=vel.dat,status=replace) open(300,file=acc.dat,status=replace) open(400,file=energy.dat,status=replace) call random_seed(size=seedsize) allocate(seed(seedsize)) do i=1,seedsize call system_clock(st) seed(i)=st enddo call random_seed(put=seed) !assigning initial position to first particle10 call random_number(x) posx(1)=xinit+x*xlen call random_number(y) posy(1)=yinit+y*ylen call random_number(z) posz(1)=zinit+z*zlen if(posx(1)<rad .or. posx(1)>xlen-rad) goto 10 if(posy(1)<rad .or. posy(1)>ylen-rad) goto 10 if(posz(1)<rad .or. posz(1)>zlen-rad) goto 10 !assigning initial position do i=2,n20 call random_number(x) posx(i)=xinit+x*xlen call random_number(y) posy(i)=yinit+y*ylen call random_number(z) posz(i)=zinit+z*zlen if(posx(i)<rad .or. posx(i)>xlen-rad) goto 20 if(posy(i)<rad .or. posy(i)>ylen-rad) goto 20 if(posz(i)<rad .or. posz(i)>zlen-rad) goto 20 do j=1,i-1 if (dist(posx(i),posy(i),posz(i),posx(j),posy(j),posz(j))<mindist) goto 20 enddo enddo print*, position initialisation finished !assigning initial velocities do i=1,n velx(i)=0.d0 vely(i)=0.d0 velz(i)=0.d0 enddo print*, velocity initialisation finished !calculating initial acceleration do i=1,n ax(i)=0 ay(i)=0 az(i)=0 do j=1,n if (j==i) cycle r=dist(posx(i),posy(i),posz(i),posx(j),posy(j),posz(j)) ax(i)=ax(i)+ fx(r,posx(i)-posx(j),posy(i)-posy(j),posz(i)-posz(j),eps)/m ay(i)=ay(i)+ fy(r,posx(i)-posx(j),posy(i)-posy(j),posz(i)-posz(j),eps)/m az(i)=az(i)+ fz(r,posx(i)-posx(j),posy(i)-posy(j),posz(i)-posz(j),eps)/m enddo enddo print*, acceleration initialisation finished. !molecular dynamics simulation do t=1,iter print*, t do i=1,n write(100,*) posx(i),posy(i),posz(i) write(200,*) velx(i),vely(i),velz(i) write(300,*) ax(i),ay(i),az(i) enddo k=kin(velx,vely,velz,m,n) p=pot(posx,posy,posz,eps,n) write(400,*) t,k,p,k+p call pos_upd(n,posx,posy,posz,velx,vely,velz,ax,ay,az,dt,xlen,ylen,zlen,rad) call acc(n,posx,posy,posz,m,eps,tempx,tempy,tempz) call vel_upd(n,velx,vely,velz,ax,ay,az,dt,tempx,tempy,tempz) enddo call system(gnuplot --persist plot.gp)endprogram mol_dyn_ref!updating accelerationsubroutine acc(n,posx,posy,posz,m,eps,tempx,tempy,tempz) implicit none integer :: i,j,n double precision,dimension(n) :: posx,posy,posz,ax,ay,az,tempx,tempy,tempz double precision :: r,dist,fx,fy,fz,eps,m do i=1,n tempx(i)=ax(i) tempy(i)=ay(i) tempz(i)=az(i) ax(i)=0.d0 ay(i)=0.d0 az(i)=0.d0 do j=1,n if (j==i) cycle r=dist(posx(i),posy(i),posz(i),posx(j),posy(j),posz(j)) ax(i)=ax(i)+ fx(r,posx(i)-posx(j),posy(i)-posy(j),posz(i)-posz(j),eps)/m ay(i)=ay(i)+ fy(r,posx(i)-posx(j),posy(i)-posy(j),posz(i)-posz(j),eps)/m az(i)=az(i)+ fz(r,posx(i)-posx(j),posy(i)-posy(j),posz(i)-posz(j),eps)/m enddo enddoend subroutine acc!updating positionsubroutine pos_upd(n,posx,posy,posz,velx,vely,velz,ax,ay,az,dt,xlen,ylen,zlen,rad) implicit none integer n,i double precision,dimension(n) :: posx,posy,posz,velx,vely,velz,ax,ay,az double precision :: dt,xlen,ylen,zlen,rad do i=1,n posx(i)=posx(i) + velx(i)*dt + (ax(i)*(dt**2))/2 posy(i)=posy(i) + vely(i)*dt + (ay(i)*(dt**2))/2 posz(i)=posz(i) + velz(i)*dt + (az(i)*(dt**2))/2 if(posx(i)<rad .or. posx(i)>xlen-rad) velx(i)=-velx(i) if(posy(i)<rad .or. posy(i)>ylen-rad) vely(i)=-vely(i) if(posz(i)<rad .or. posz(i)>zlen-rad) velz(i)=-velz(i) enddoend subroutine pos_upd!updating velocitysubroutine vel_upd(n,velx,vely,velz,ax,ay,az,dt,tempx,tempy,tempz) implicit none integer n,i double precision,dimension(n) :: velx,vely,velz,ax,ay,az,tempx,tempy,tempz double precision :: dt do i=1,n velx(i)=velx(i) + 0.5d0*(ax(i)+tempx(i))*dt vely(i)=vely(i) + 0.5d0*(ay(i)+tempy(i))*dt velz(i)=velz(i) + 0.5d0*(az(i)+tempz(i))*dt enddoend subroutine vel_updfunction pot(posx,posy,posz,eps,n) implicit none double precision pot,r,dist,eps integer i,j,n double precision, dimension(n) :: posx,posy,posz pot=0.d0 do i=1,n do j=1,n if(i==j) cycle r=dist(posx(i),posy(i),posz(i),posx(j),posy(j),posz(j)) pot= pot + (4.d0*eps*((1.d0/r)**12 - (1.d0/r)**6)) !r is relative distance. x,y,z are components of r. enddo enddoend function potfunction kin(velx,vely,velz,m,n) implicit none double precision :: kin,m integer :: i,n double precision, dimension(n) :: velx,vely,velz kin=0.d0 do i=1,n kin = kin + ((velx(i)**2.d0 + vely(i)**2.d0 + velz(i)**2.d0)/(2.d0*m)) enddoend function kinfunction dist(x1,y1,z1,x2,y2,z2) implicit none double precision :: dist,x1,y1,z1,x2,y2,z2 dist = sqrt((x1-x2)**2.d0 + (y1-y2)**2.d0 + (z1-z2)**2)end function distfunction fx(r,x,y,z,eps) implicit none double precision :: fx,r,x,y,z,eps fx = 4.d0*eps*((12.d0/r**14.d0) - (6.d0/r**8.d0))*xend function fxfunction fy(r,x,y,z,eps) implicit none double precision :: fy,r,x,y,z,eps fy = 4.d0*eps*((12.d0/r**14.d0) - (6.d0/r**8.d0))*yend function fyfunction fz(r,x,y,z,eps) implicit none double precision :: fz,r,x,y,z,eps fz = 4.d0*eps*((12.d0/r**14) - (6.d0/r**8))*zend function fz",
    "present_kp": [
      "molecular dynamics"
    ],
    "absent_kp": [
      "computational physics",
      "time integration",
      "integration"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why did the program entry point become 'sub esp, 1c'?. i compiled the following c++ code with mingw and opened it in ollydbg 2.01. and the program stops at the following lines:cpu disasmaddress hex dump command comments00401570 /$ 83ec 1c sub esp,1c00401573 |. c<phone> mov dword ptr ss:[local.6],10040157a |. ff15 68814000 call dword ptr ds:[<&msvcrt.__set_app_ty00401580 \\. e8 fbfbffff call 00401180names in project1, item 20 address = <phone> section = .text type = export ordinal = name = <moduleentrypoint> comments =however, this is not what i want. i prefer when ollydbg stop at the following lines:cpu disasmaddress hex dump command comments004016b0 /$ 55 push ebp ; project1.004016b0(guessed void)004016b1 |. 89e5 mov ebp,esp004016b3 |. 83e4 f0 and esp,fffffff0 ; dqword (16.-byte) stack alignment004016b6 |. 83ec 10 sub esp,10004016b9 |. e8 a2050000 call 00401c60004016be |. c<phone> mov dword ptr ss:[local.4],offset 004050 ; /format => hello world!004016c5 |. e8 9e1f0000 call <jmp.&msvcrt.printf> ; \\msvcrt.printf004016ca |. b8 <phone> mov eax,0004016cf |. c9 leave004016d0 \\. c3 retnis that a bug? why did mingw set sub esp, 1c as the entrypoint? can i set ollydbg to start at the correct entrypoint?",
    "present_kp": [
      "ollydbg",
      "c++"
    ],
    "absent_kp": [
      "debugging"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "circle hough transform. improvement by knowing edge direction. i'm studying hough transform. i have fully understood the ht for detecting lines and successfully implemented it. i have more problems with the circle detection ht. given a circle, if the radius is known i have to work in a 2d parameter space (each point in the image space becomes a circle in the parameter space; the intersection of all the circles gives me the center of the image circle).if the radius is not known the parameter space becomes 3d.what i don't understand is why if i know the direction of the edges, even if my radius is unknown the parameter space lowers to 2d.",
    "present_kp": [],
    "absent_kp": [
      "image processing",
      "computer vision"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "samba share for a specific username not having a local account no password. so normally samba works by matching the username in microsoft windows to the username in linux, and then if the passwords for both accounts are the same then you get access to the folder share from linux. this is for a simple setup of samba-server in linux, where everything is local in linux and in smb.conf security = user.what i would like to do is set up a temporary samba share as a drop point so only one specific user from any microsoft windows computer can dump some files in there. i prefer not to make the folder via samba-server world-writeable and guest ok = yes so anyone can access it. but i also don't want to go through having to make this one specific microsoft windows user an account on my linux system because he will never need any access to the linux system other than this one time dumping a few files to my samba folder share.is there a way in smb.conf to create a folder share that will allow access by a specific username from microsoft windows, and not care about password?",
    "present_kp": [
      "samba"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "problem installing tor on kali linux. i'm trying to install tor on my kali linux 2016.1 (kali-rolling). when i type apt-get install tor in terminal, this error appears:reading package lists... donebuilding dependency tree reading state information... donepackage tor is not available, but is referred to by another package.this may mean that the package is missing, has been obsoleted, oris only available from another sourcee: package 'tor' has no installation candidatehow can i fix this and install tor?upd:i tried this: <url> - i added deb <url> wheezy main to sources file, but it didn't help at all, so i deleted this string and now it's in a default condition",
    "present_kp": [
      "apt",
      "kali linux",
      "tor"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how do you change your microsoft live global account information, like primary email address?. i've had my .net passport -> windows live id for a very long time. since that time, i've changed my primary email address and would really like to not have to use my email from highschool to still get access to everything in the microsoft world. my msdn account is tied to it, as well as skydrive and xbox live. can i change my login email to use a different one, without having to start a new account? if so, what are the steps i need to take and places i need to go to get that changed globally for all of the microsoft webapps that i use?",
    "present_kp": [
      "login",
      "windows live"
    ],
    "absent_kp": [
      "account management"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "where to handle packets between clients?. server: 192.168.1.1i want to theoretically disable that the clients can ping each other.can i use an iptables rule for it? e.g.:iptables -a forward -m iprange --src-range 192.168.1.2-192.168.1.255 --dst-range 192.168.1.2-192.168.1.255 -j dropis it true that i cannot filter traffic between the clients?? [or at least redirect these packets to e.g.: the router?]if i run tcpdump on the router [server] i can see that a client [192.168.1.201] is pinging another [192.168.1.162]# tcpdumptcpdump: warning: eth0: no ipv4 address assignedtcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on eth0, link-type en10mb (ethernet), capture size 96 bytes12:22:26.931343 ip 192.168.1.201 > 192.168.1.162: icmp echo request, id 12547, seq 141, length 6412:22:27.819373 ip 192.168.1.201 > 192.168.1.162: icmp echo request, id 12547, seq 142, length 6412:22:28.819640 ip 192.168.1.201 > 192.168.1.162: icmp echo request, id 12547, seq 143, length 64",
    "present_kp": [
      "iptables"
    ],
    "absent_kp": [
      "linux",
      "security",
      "firewall"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "can i send an email as a google groups owner/moderator?. i have a google group that requires me to approve all members. someone whom i am not sure of his identity asked to join. is it possible to send said person an e-mail from an owner/moderator address and not my personal address?",
    "present_kp": [
      "google groups"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "what are the licencing requirements for publishing and distributing an asp.net application. i started developing websites using php. i have read many times that php is free and open source which is an advantage over asp.net. however, due to my current job requirements, i had to switch to asp.net which is pretty good. now after developing few applications, i am wondering how can i publish them for free. as php is free and opensource, it was never an issue for me. what's bothering me is this.suppose i developed a web application using visual studio express (or professional version or any other version)do i need any special permission/license from microsoft to host my application on internetif i create a desktop application and want to distribute it for free on internet, is any license required.if no, then what are microsoft licenses all about and how is php free and asp is not freethanks!",
    "present_kp": [
      "asp.net"
    ],
    "absent_kp": [
      "licensing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "refactor this jquery datepicker code to be as small as possible. i am using jquery datepicker on 2 forms on the same page.i currently am using the following code (below), but as you can see, i am using the same parameters over again (showon, buttonimage, buttonimageonly, dateformat, & constraininput)is there a more efficient way to do this rather then repeating myself over and over again, over 4 methods?$('#startdate').datepicker( { showon: 'both', buttonimage: '/assets/images/calendar_symbol.png', buttonimageonly: true, dateformat: 'dd/mm/yy', constraininput: true, // all of the above is repeated again below mindate: new date(<?php echo date(y)?>, <?php echo date(m)-1?>, <?php echo date(d)?>), onclose: function() { $('#enddate').val($('#startdate').val()); } });$('#enddate').datepicker( { showon: 'both', buttonimage: '/assets/images/calendar_symbol.png', buttonimageonly: true, dateformat: 'dd/mm/yy', constraininput: true, // all the above repeated again.. mindate: new date(<?php echo date(y)?>, <?php echo date(m)-1?>, <?php echo date(d)?>) });$('#filter_startdate').datepicker( { showon: 'both', buttonimage: '/assets/images/calendar_symbol.png', buttonimageonly: true, dateformat: 'dd/mm/yy', constraininput: true,// and again... onclose: function() { $('#enddate').val($('#startdate').val()); } });$('#filter_enddate').datepicker( { showon: 'both', buttonimage: '/assets/images/calendar_symbol.png', buttonimageonly: true, dateformat: 'dd/mm/yy', constraininput: true // and again.... });i was thinking of using a if statement to check what was clicked (#startdate or #filter_startdate), and then breaking it down like that, but unsure if this would work or how to do this.",
    "present_kp": [],
    "absent_kp": [
      "javascript",
      "jquery ui"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to view metadata of local/downloaded .deb files in debian. while one can download a .deb package using apt download $package.deb there doesn't seem to be any way to see the metadata of that file. i mean by metadata something like -[$] aptitude show dgit package: dgit version: 3.10state: not installedpriority: optionalsection: develmaintainer: ian jackson <<email> alluncompressed size: 309 kdepends: perl, libwww-perl, libdpkg-perl, git-core, devscripts, dpkg-dev, git-buildpackage, liblist-moreutils-perl, coreutils (>= 8.23-1~) | realpath, libdigest-sha-perl, dput, curl, apt, libjson-perl, ca-certificates, libtext-iconv-perl, libtext-glob-perlrecommends: ssh-clientsuggests: sbuilddescription: git interoperability with the debian archive dgit (with the associated infrastructure) makes it possible to treat the debian archive as a git repository. dgit push constructs uploads from git commits dgit clone and dgit fetch construct git commits from uploads.hopefully there is a way to view the depends, recommends etc. i had viewed also using less in various forums to do the same thing but couldn't get it to work as well.",
    "present_kp": [
      "debian"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "list the files containing a particular word in their text. i would like to list the files recursively and uniquely that contain the given word. example: checking for word 'check', i normal do is a grep$ grep check * -rbut as there are many occurrence of this word, i get a lot of output. so i just need to list the filenames that contain the given search word. i guess some trick with find and xargs would suffice here, but not sure.any ideas?",
    "present_kp": [
      "find",
      "grep"
    ],
    "absent_kp": [
      "file search"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to display talairach image in afni correctly?. i am using afni to display talairach image, that is anat+tlrc.brik & anat+tlrc.head . but the afni only shows part of the image. it looks like this:afni is one of the softwares to display and analyze fmri images. i am sure that the talairach image i want to display is in correct format. i do not how to fix this problem? anyone can give me a help?",
    "present_kp": [
      "fmri"
    ],
    "absent_kp": [
      "cognitive neuroscience",
      "philosophy of mind"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "working with all leaves on a certain level of a b-tree. i want to work with a b-tree of any size. i want to do something with all leaves of the lowest depth $d$. then if a certain condition holds, i want to recursively consider the same condition for the leaves at depth $d-1$... and so on.what's the best performing solution with this behavoir?",
    "present_kp": [
      "tree"
    ],
    "absent_kp": [
      "ds.data structures"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "good, cheap microphone for speech / voice recording?. i'm looking for a good microphone for recording speech (eg. for podcasts, although it might be used for other things too), ideally for under $30. i'm looking for something that:has minimal hiss and reasonable sound quality for its price.would work with my current setup, which is a laptop + usb sound card.i know that $30 (or even less) is not much for a microphone, but i should be able to get something better then my current very cheap one.there are some fairly cheap ones on amazon (example), does anyone have experience with the sound quality of these? the page states that it is a condenser microphone, so does it require phantom power?any recommendations? any other tips?",
    "present_kp": [],
    "absent_kp": [
      "audio",
      "microphones",
      "audio quality",
      "audio recording"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "connect to pseudoterminal (same user) from lost session. i was running several large processes at once, and my pc froze (i suspect due to heat, its a laptop, and the fan was at maximum for several minutes just prior to locking up). the mouse cursor moved, but clicks had no effect. i could still hear sound from one of the processes, but it was visually frozen. i hit ctrl-alt-f2 as a last ditch effort to get it to do something before a hard reboot, and it switched to tty2. so, i switched back to 1, logged in, and ran startx.my who output:nexus@lithium ~ $ whonexus tty1 2014-02-26 11:14nexus tty7 2014-02-20 09:50 (:0)nexus pts/0 2014-02-26 10:21 (:0.0)nexus pts/2 2014-02-26 11:11 (:0)nexus pts/3 2014-02-26 11:23 (:1)i figured out the pts sessions are where i had the terminal emulator open. i would like to reconnect to pts/0, and be able to view and use (save and close) the process it is running. is this possible? just want to open a new terminal somehow and get it to take over or emulate or mirror the original term. i can still hear the music in the background from the game in pts/0, and firefox is still running under that tty as well, so that i can't run a new firefox process.",
    "present_kp": [
      "terminal",
      "tty"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to migrate old statistics to google analytics?. in our project we stored all users event data in our database for over one year , but it's not indexed.now we are going to use google analytics to store our analytics and analyze the report using google analytics dashboard.but before start using google analytics , i would like to emigrate all old statics (about 2 million events) to google analytics.for this matter i should use measurement protocol and it's limit allow me to transfer 2 million hits with no problem.but i didn't succeed to know how to set the time of the event. measurement protocol has queue time but google says :values greater than four hours may lead to hits not being processed.how it's possible to transfer 2 million events to google analytics with there event time ?thanks",
    "present_kp": [
      "google analytics",
      "analytics"
    ],
    "absent_kp": [
      "event tracking",
      "analytics api",
      "analytics events"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "assigning files of a group to another one. i want to delete a group using groupdel command, but before doing this i want to assign all files belonging to this group to another one is there a way to do this?i know that i can use find / -gid group_id to find files that belong to this groups and then manually assign them to new one. i seek a way to automatically do the task.",
    "present_kp": [
      "group"
    ],
    "absent_kp": [
      "linux"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "incorporating libs into module pattern. i have recently started using require.js (along with backbone.js, jquery, and a handful of other javascript libs) and i love the module pattern (here's a nice synopsis if you're unfamiliar: <url>). something i'm running up against is best practices on incorporating libs that don't (out of the box) support the module pattern. for example, jquery without modification is going to load into a global jquery variable and that's that. require.js recognizes this and provides an example project for download with a (slightly) modified version of jquery to incorporate with a require.js project.this goes against everything i've ever learned about using external libs - never modify the source. i can list a ton of reasons. regardless, this is not an approach i'm comfortable with.i have been using a mixed approach - wherein i build/load the traditional js libraries in a traditional way (available in the global namespace) and then using the module pattern for all of my application code. this seems okay to me, but it bugs me because one of the real beauties of the module pattern (no globals) is getting perverted.anyone else got a better solution to this problem?",
    "present_kp": [
      "javascript"
    ],
    "absent_kp": [
      "modules"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "calculating the biggest number of consecutive ones in a binary number. the following code calculates the biggest number of consecutive ones in a binary representation of a number. for example 6 -> 2 (110), 5 -> 1 (101):string numberasbinarystring = integer.tobinarystring(number);list<string> listofconsecutiveonestrings = arrays.aslist(numberasbinarystring.split(0));system.out.println(listofconsecutiveonestrings.stream() .max(comparator.comparing(string::length)) .map(onessequence -> onessequence.length()) .get());i wonder if there is a more elegant way of writing this especially since this solution involves quite a few conversions. any opinions are appreciated.",
    "present_kp": [
      "strings"
    ],
    "absent_kp": [
      "java"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "compare characters. i have two lines, which are saved in two variables. but it doesn't really matter, where they are saved. my question is, how do i compare each character from both lines?for exampleshellohlleoresult: true (h), false ... ,true (o)",
    "present_kp": [],
    "absent_kp": [
      "scripting"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "nesting a function within a logical statement in google sheets. is it possible to include a formula as the value_if_true within a logical statement in a cell in google sheets? for instance, i would like to subtract the two rightmost digits in two adjacent cells, if that value in e23 is greater than the value in d23. i know how to return a yes or no:=if(right(e23,2)>right(d23,2),yes,no)but can i perform an operation if the value is true? i've (blindly) tried: =if(right(e23,2)>right(d23,2),=right(e23,2)-right(d23,2),no)but of course, that returns an error. how would i perform the equivalent of that function for a true value?",
    "present_kp": [],
    "absent_kp": [
      "google spreadsheets"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "where is make_request_fn source code in linux kernel. in my study and examine linux kernel code i encountered with function make_request_fn. in this link it says that this function type definition is in include/linux/blkdev.h, line 211 :typedef void (make_request_fn) (struct request_queue *q, struct bio *bio);but did not mention where is its implementation as source code in a .c file.my question is where is this function source code?",
    "present_kp": [
      "linux kernel"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "request facebook to investigate which administrator made page post. i'm an administrator of a facebook page for an organisation which has multiple administrators. recently another administrator posted some false information on the organisations facebook page which has led to serious accusations of deliberate sabotage of another member of the board and has led to some very heated words being exchanged at meetings, etc. so far no one has admitted to making the post and i have feeling no one ever will. to put an end to the issue it would obviously help if we could identify who made the post in question.is there any way i can request that facebook check out who made this post? as a safety measure all other administrators have now been removed from the page and the post in question and has now been removed. i'd be very grateful for any help,thanks",
    "present_kp": [
      "facebook"
    ],
    "absent_kp": [
      "facebook pages"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to automatically accept epel gpg key. the very first time that i install a package from epel, i am prompted if i want to import a gpg key. notice how there are 2 'is this ok' prompts when installing redis? [root@us-devops-build02 yum.repos.d]# yum install redisloaded plugins: fastestmirrorloading mirror speeds from cached hostfile.. truncated for readabilitytotal download size: 213 kinstalled size: 668 kis this ok [y/n]: ydownloading packages:redis-2.4.10-1.el6.x86_64.rpm | 213 kb 00:00 warning: rpmts_hdrfromfdno: header v3 rsa/sha256 signature, key id 0608b895: nokeyretrieving key from <url> gpg key 0x0608b895: userid: epel (6) <<email> from : <url> this ok [y/n]: ythis causes puppet to fail on freshly-provisioned machines, unless i ssh in to the machine first and manually accept the installation of this key. why does epel need a key to be downloaded on the first installation of a package? how can i automatically install this key on my images so puppet won't fail?",
    "present_kp": [
      "rpm",
      "gpg"
    ],
    "absent_kp": [
      "package management"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "repackaging apk file using baksmali and smali. i am a student interested in android security. i was trying to modify a apk file using baksmali and smali. however, i am not able to run the repackaged app in my mobile. when i click on the icon it say unfortunately, test has stopped and exists. (even the icon of the app got changed, now i see default android icon instead of old real colorful icon of the app) what could be the reason for this ? actually i haven't even modified code of the apk file. i just unzipped apk to get dex file, then i converted it to smali using baksmali.jar, and then back to dex using smali.jar. finally zipped and signed. what i have done in detail:decompress the apk file$ unzip test.apkconvert 1classes.dex1 to smali$ baksmali -x classes.dex -o smaliclassesconverted the classes back to classes.dex (replaced old classes.dex, in fact i did not add any new code to smali file. i wanted to know whether this works first).$ smali smaliclasses -o classes.dexzip all the files to test.zip$ zip test.zip androidmanifest.xml classes.dex res meta-inf resourses.arscrename test.zip to test.apk$ mv test.zip test.apknow i believe i have to sign the apk again, please correct me if am wrong here.edited:java -jar signapk.jar testkey.x509.pem testkey.pk8 test.apk test-patched.apki tried to install the new repackaged apk. using adb shell. adb shell showed it successfully installed. however, i am not able to run the repackaged app in mobile. the app crashes when i click on it. it says unfortunately, test has stopped.why doesn't the repackaged app running ? i don't understand what i am missing here ?edited:i tried to repackage the same app using apktool. i extracted the smali files using it and repackaged. but why repackaging is not working with baksmali, smali, zip and signapk. is zipping the real problem in this procedure? i see the size of the app is reduced drastically when i zip it and rename it to .apk compared to the original apk file :|",
    "present_kp": [
      "android",
      "apk"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "map rental cost strategy. it is the case where data contains some field indicating what type of strategy should be picked.i mean, i often ignore solving the problem like shown below and duplicate the problems that strategy pattern is out to solve in the first place. is there a pattern for it or is it not viable at all in this case?recent example on how i have solved it:public class maprentalcoststrategy : imapper<equipmenttype, irentalcoststrategy>{ private readonly dictionary<equipmenttype, irentalcoststrategy> _knownrentalcoststrategy; public maprentalcoststrategy(imutablepriceconfiguration mutablepriceconfiguration) { _knownrentalcoststrategy = new dictionary<equipmenttype, irentalcoststrategy> { { equipmenttype.heavy, new heavyrentalcoststrategy(mutablepriceconfiguration)}, { equipmenttype.specialized, new specializedrentalcoststrategy(mutablepriceconfiguration)}, { equipmenttype.regular, new regularrentalcoststrategy(mutablepriceconfiguration)} }; } public func<equipmenttype, irentalcoststrategy> create => equipment => { if (!_knownrentalcoststrategy.containskey(equipment)) throw new argumentexception(); return _knownrentalcoststrategy[equipment]; };}where equipmenttype is an enum.maybe i should inject a service that returns this dictionary? in any case, it seems i am delegating the responsibility (in this case its object creation, it should not be mappers responsibility to deal with the instantiation of the dictionary).maybe i am on the wrong track? is there a better way?",
    "present_kp": [],
    "absent_kp": [
      "c#",
      "design patterns"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "ftp - user to upload file to different user's directory. i installed vsftpd and configured sftp.i have a standard user who is named, let's say, user1 with all perms. i don't want to give the password to my friend. i only want him to access a specific directory to upload files so server can run them.the file place is : /home/user1/uploadi created a user, with no shell login, named user1ftp. i changed the user home location to /home/user1/upload so when he logs in, he directly logs into the upload directory. he can upload and delete files in it. the problem is that when he uploads a file, the server cannot run it. his files' permission are: rw-r--r--i need to make sure that when he uploads a file, the permissions are rwxrwxr-xthese users are members of the same group.how can i do that?",
    "present_kp": [
      "permissions"
    ],
    "absent_kp": [
      "ftps"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how do i invoke a process with a certain java binary, and all processes called by that process?. i need to be able to call a certain process with a certain java binary. call that process foo. however, foo also invokes several other processes via java, but foo is a black box to me. when foo invokes those processes, it just runs java bar, java bat, etc. how do i tell foo and any processes called by foo, to use /full/path/to/java only? essentially i want to use update-alternatives for a single shell session and/or single process tree. os is centos7, shell is bash.setting the system java is out of the question, as other processes on the system need a different java binary.",
    "present_kp": [
      "java"
    ],
    "absent_kp": [
      "linux",
      "command line"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what are the benefits of using jasmine framework on existing applications?. i got one project which contains javascript code. no unit tests were written for javascript code.so it is feasible or beneficial to write unit test cases using jasmine framework for code?what are benefits or challenges may i face while doing this?",
    "present_kp": [
      "javascript"
    ],
    "absent_kp": [
      "unit testing",
      "frameworks"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how does word2vec handle the input word being in the context?. if word2vec encounters the same word multiple times in the same window, what occurs? obviously it is meaningless to decrease the distance between the vectors for the input word and the target word. but will the repetition strengthen the relationship between the repeated word and the context words?",
    "present_kp": [],
    "absent_kp": [
      "machine learning",
      "nlp",
      "word embeddings"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "google notified me that i have a sudden spike in 404's, but they are pages that don't exist. i'm pretty puzzled by this, so any direction would be helpful.the only thing hosted on this server is basically two word press instances with my personal site and another. everything's been great, but then i got an email letting me know that there are a bunch of new 404's.the urls look like spam pages, and i'm 100% sure that they don't exist in my wordpress instance.it appears to me that some spam site has just decided to link to a bunch of nonexistent pages on my site. is this a reasonable assumption? should i bother to look at this any further?the list looks like this:blog-forex/1182-gielisch+gcm+forex-cb.html40411/2/162blog-forex/6691-exchange+rates+of+the+market+forex+nigeria+today-52.html40411/2/163blog-forex/4918-%d8%a8%d9%88%d8%b1%d8%b5%d8%a9+%d9%88%d8%b8%d8%a7%d8%a6%d9%81+%d9%81%d9%8a+%d8%a7%d9%84%d9%8a%d9%85%d9%86-2b.html40411/2/164blog-forex/13789-one+hour+binary+options+strategy+explanation-f.html40411/2/165blog-forex/14381-breve+historia+de+la+bolsa+de+valores+de+caracas,+venezuela-d2.html40411/1/166blog-forex/1611-trade+in+the+market+forex+lessons-fc.html40411/2/167blog-forex/13522-vegas+wave+strategy+based+on+elliott+indicator+forex-4f.html40411/2/168blog-forex/5882-forex+currency+converter+togo-53.html40411/2/169blog-forex/10196-forex+beer+ads+1950's-c0.html40411/1/1610blog-forex/1505-meilleures+forex+sites+de+n%c3%a9gociation+en+c%c3%b4te+d'ivoire-89.html40411/1/1611blog-forex/12785-l%c3%a1ska+zar%c3%a1baj%c3%ba+v+forex+obchodovanie+na+slovenskom-78.html40411/2/1612blog-forex/10776-kotaksecurities+online+trading+account-16.html40411/2/1613blog-forex/4805-ko%c4%beko+burzov%c3%a9+trhy,+na+slovenskom-8a.html40411/1/1614blog-forex/13566-statistiky+kde,+jak+vyd%c4%9blat+pen%c3%adze+online+v+%c4%8cesk%c3%a9+republice-aa.html40411/1/1615blog-forex/14432-mercados+stock+exchange,+em+portugal-7b.html40411/1/1616blog-forex/5036-%e0%a6%ac%e0%a6%be%e0%a6%82%e0%a6%b2%e0%a6%be%e0%a6%a6%e0%a7%87%e0%a6%b6%e0%a7%87+%e0%a6%85%e0%a6%a8%e0%a6%b2%e0%a6%be%e0%a6%87%e0%a6%a8%e0%a7%87+%e0%a6%85%e0%a6%b0%e0%a7%8d%e0%a6%a5+%e0%a6%b9%e0%a7%8b%e0%a6%ae+%e0%a6%ab%e0%a7%8d%e0%a6%b0%e0%a6%bf-b.40411/2/16",
    "present_kp": [
      "404",
      "spam"
    ],
    "absent_kp": [
      "crawl errors"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to delete everything inside a directory, without an specific folder and it's content. my folder structure looks like that:./build./module/build./sourceall i want to keep is ./build and it's content.the command find . \\! -path ./build -delete does not delete ./build, but all of it's content.how to avoid that?",
    "present_kp": [
      "find"
    ],
    "absent_kp": [
      "shell"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what cannot be proven at compile-time about a computer program?. is there a turing-complete-ish programming language without runtime errors? like segmentation faults, memory leaks, race conditions, deadlock/livelock/starvation, etc.? with strongly typed languages, the compiler can detect null pointer exceptions and refuse to compile code. how much can we statically check? everything, or are there some things impossible to check?are there any language projects that are trying to do this (especially with concurrent and networked applications)?edit:i guess there are two sorts of behavior that i'm looking to avoid: unwanted erroneous behavior (deadlocks, etc), and runtime exceptions (divide-by-zero, etc).taking the example of divide-by-zero, can we create some computation x/y so that, whenever such a computation is done, we require y to be of the type t nonzero where t is integer, floating-point, or whatever for that overloaded operator?with deadlocks, and other concurrent scheduling hazards, is there a set of control structures which cannot create those hazards, or better yet, can detect those hazards at compile time (better because i think it might be more expressive)?",
    "present_kp": [],
    "absent_kp": [
      "programming languages",
      "type theory"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why bash disables autocompletion for some commands and how can i disable it?. it often happens to me that bash disables auto completion for certain commands, which forces me to add random symbol to name of command, use autocompletion and that fix the command name, which is rather annoying, for example:# i type:openvpn s<tab># nothing happens, so i add xopenvpnx s<tab># now this expands toopenvpnx somepath# same with ./configure or many other commands...is there a way to disable this so that autocompletion always works?",
    "present_kp": [
      "bash"
    ],
    "absent_kp": [
      "autocomplete"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why iterative-deepening-dfs requires o(b*d) memory?. after reading about iterative deepening depth-first search on wikipedia, i could understand that it just limits the depth upto which dfs can go in one iteration/call.however, i could not understand why it is said that it requires $o(b imes d)$ memory, where $b$ is fan-out of nodes i.e. children and $d$ is max-depth.since it is just doing dfs multiple times, it should also only need $o(d)$ memory. thinking more, i thought that may be the entire graph can not be put in memory and $o(b imes d)$ is the part of graph required to be kept in memory. but does that make dfs also have $o(b imes d)$ memory requirement ? but even this does not like a good argument, because since during traversal if we are fetching the new nodes from some storage, we can do so also with $o(d)$ memory.also the wikipedia page examples lead me to infer that it does not store visited[node_id] information about the nodes which stores whether node with id node_id is visited till now or not. this made me infer that graph is big.edit: after thinking further i think that it is right about $o(b imes d)$, since it is iterative-dfs rather than recursive-dfs. in recursive-dfs we usually take memory used by function call stack for granted. in iterative-dfs, we can find right memory-requirements. since, now we can not implicitly remember back-tracking, we have to put all children on the stack and hence if we are going to $d$ levels, we need $o(b imes d)$ memory.",
    "present_kp": [],
    "absent_kp": [
      "graphs",
      "graph traversal"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "construct a digraph given its in-degree and out-degree distribution. could anyone help me with this algorithmic problem:given the in and out degrees of a set of vertices, is it possible to determine if there exist a valid graph respecting this constraint? the graph can allow self loops but not parallel edges.here's an example: vertex a: in=1 out=1vertex b: int=2 out=2for which we can construct this graph:a => bb => ab => bhere's another example:vertex a: in=0 out=1vertex b: in=1 out=1here, we obviously cannot construct such graph.i have been scratching my head around this problem. for an undirected graph, there exist a simple algorithm to solve this problem but i cannot find any way to derive a solution for directed graphs.i have the intuition that we could find a matching algorithm in the bipartite graph representing the in and out edges of the graph and where each out-edge would be matched to an in-edge. however the usual approach can produce a graph with parallel edges.for example 1, a valid solution could be:a- a+: a => ab- b+: b => bb- b+: b => bwhich is not a valid graph.also, please note that i am more interested in determining if a valid solution exists. it's not necessary to provide or construct such solution.",
    "present_kp": [
      "graphs"
    ],
    "absent_kp": [
      "algorithms"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "lvm mirrored logical volume performance. i read that when mirroring a logical volume a log of some sort is used to keep the files in sync. you can either set it so this log is in a separate physical volume or in memory. i also read that if the log is in memory, the system has to resync the volumes.is there a performance hit for using the log in memory? for example, does it take much longer to boot the machine? is the data at risk while the resync is happening?i'm working with ubuntu 10.04 in this case. i believe it's lvm2. the clearest documentation i've found is the centos document here. i was also looking at this description.",
    "present_kp": [
      "ubuntu",
      "lvm"
    ],
    "absent_kp": [
      "linux"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "usb tuner driver not detected. i am using a dvb tuner, usb tuner. i have added driver to the kernel and it is being registered. but when the device is connected, it doesnt invoke the probe function. is there anything i have to add? like should the device driver be mapped to the device or something? and please explain how a driver is loaded, based on the type of usb device. the kernel i am using is linux 3.5.",
    "present_kp": [
      "usb",
      "dvb"
    ],
    "absent_kp": [
      "kernel modules"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "green website design. this is kindove a strange question, but...there was a site called blackle ( <url>) which claimed to save energy by using a black background (it doesn't: see here: <url>). however, blackle and it's idea of green website design interested me, and i was wondering if there are any ways to design an energy saving website that actually save energy. if anyone knows of any, please post them here. if nobody has any, then i guess there isn't a way to save energy through website design...",
    "present_kp": [
      "website design"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how do i transfer images from a fedora 20 computer to an iphone?. i have a laptop running fedora20. i have an iphone4s. i have installed the software suggested here which allows my laptop to read the iphone. i connect my phone to the laptop using a usb cable. i can see a bunch of files, including all the images that are on the iphone. i can transfer images from the iphone to the laptop.what i want to do is transfer images from the laptop to the iphone.when i copy and paste images i don' get any error message, but the images never appear on the iphone.how can i sync these images to the iphone?i do not have any other os available.(asking here because askdifferent, the apple stackexchange site is hopeless)",
    "present_kp": [
      "fedora",
      "files",
      "iphone"
    ],
    "absent_kp": [
      "synchronization"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why it is necessary to update /etc/default/grub, and not just save it?. i understand that the grub file contains some instructions that are required before booting such as: booting in text or booting in graphics mode. my question is why do i need update-grub? why just saving the file after making changes would not work?",
    "present_kp": [],
    "absent_kp": [
      "grub2"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "get the indices of a range elements by relative order of the values. i've just askedwhat's the idiomatic way to get the sort order of data, without actually applying it?(or see the original it was found to be a dupe on). so, i want to implement properly what seems to be the only suggested option - generating an array of indices and sorting it by the original data's relative order. here's my code (ignoring includes):template<typename raiterator, typename comparator, typename size = size_t>inline std::vector<size>sorted_indices(raiterator first, raiterator last, comparator comparator){ std::vector<size> result(last - first); std::iota(result.begin(), result.end(), 0); auto adapted_comparator = [&first, comparator](size lhs, size rhs) { return comparator(first[lhs], first[rhs]); }; std::sort(result.begin(), result.end(), adapted_comparator); return result;}template<typename raiterator, typename size = size_t>inline std::vector<size>sorted_indices(raiterator first, raiterator last){ using value_type = typename std::iterator_traits<raiterator>::value_type; auto comparator = [](const value_type& lhs, const value_type& rhs) { return lhs < rhs; }; return sorted_indices<raiterator, decltype(comparator), size>(first, last, comparator);}template<typename container, typename comparator, typename size = typename container::size_type>inline std::vector<size> sorted_indices(container& container, comparator comparator){ return sorted_indices<typename container::const_iterator, size>( std::begin(container), std::end(container), comparator);}template<typename container, typename size = typename container::size_type>inline std::vector<size> sorted_indices(container& container){ return sorted_indices<typename container::const_iterator, size>( std::begin(container), std::end(container));}have i covered all my bases or is there something i might be missing? also, how legitimate is it to return the vector of indices?",
    "present_kp": [
      "sorting",
      "iterator"
    ],
    "absent_kp": [
      "c++"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to subtract cbot grain prices. i had to calculate the difference between chicago board of trade grain futures prices using vb.net and came up with the following (admittedly clunky) solution. it's a little tricky because cbot prices, as posted, mix octal and decimal numbers. anyway, here is my code for you to enjoy and tear apart: public function grainsubtraction(byval numberone as double, byval numbertwo as double) as double ' this is a somewhat convoluted function to subtract cbot grain prices ... but it seems to work ' it assumes the numbers have no decimals - that is, if the price of corn is $4.75 3/4 per bushel, this number is represented as 4756 where the last digit (6) represents 6/8 ' basically, we perform octal subtraction on the smallest unit digit and regular decimal subtraction on the remaining digits ' i use arrays to do this so i can isolate each digit - there is probably a more efficient way to do this, though - i'm not a quant dim tempone as double = 0.0 dim temptwo as double = 0.0 dim inverted as boolean = false 'flags whether the second number is larger than the first dim answer as double = 0.0 ' this will be our ultimate response dim multiplier as double = 1.0 ' here, we compare the two numbers and invert them if the second number is largest then the first if numberone = numbertwo then return 0.0 if numberone < numbertwo then tempone = numbertwo temptwo = numberone inverted = true else tempone = numberone temptwo = numbertwo end if dim lengthone = cstr(tempone).length ' how many digits in number one? dim lengthtwo = cstr(temptwo).length dim i as integer = 0 dim errorflag as boolean = false 'let's us know whether we hit an error with this number dim numberlength as integer if lengthone > lengthtwo then numberlength = lengthone else numberlength = lengthtwo ' create our array dim numberonearray(numberlength) as integer dim numbertwoarray(numberlength) as integer dim answerarray(numberlength) as integer ' this will hold our answer as an array ' set array values to zero for i = 0 to numberlength - 1 numberonearray(i) = 0 numbertwoarray(i) = 0 next for i = 0 to lengthone - 1 numberonearray(i) = tempone mod 10 tempone = int(tempone / 10) next for i = 0 to lengthtwo - 1 numbertwoarray(i) = temptwo mod 10 temptwo = int(temptwo / 10) next i ' this is where we do the actual analysis for i = 0 to numberlength - 1 if i = 0 then ' we're doing octal subtraction here if numberonearray(i) >= numbertwoarray(i) then answerarray(i) = numberonearray(i) - numbertwoarray(i) else if i = numberlength then errorflag = true else numberonearray(i) = numberonearray(i) + 8 numberonearray(i + 1) = numberonearray(i + 1) - 1 answerarray(i) = numberonearray(i) - numbertwoarray(i) end if end if else ' we're doing decimal subtraction here if numberonearray(i) >= numbertwoarray(i) then answerarray(i) = numberonearray(i) - numbertwoarray(i) else if i = numberlength then errorflag = true else numberonearray(i) = numberonearray(i) + 10 numberonearray(i + 1) = numberonearray(i + 1) - 1 answerarray(i) = numberonearray(i) - numbertwoarray(i) end if end if end if next multiplier = 1 if errorflag = true then return 0.0 for i = 0 to numberlength - 1 answer = answer + (answerarray(i) * multiplier) multiplier = multiplier * 10 next if inverted = true then answer = -answer return answerend function",
    "present_kp": [
      "vb.net"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "google sheets automatically add rows based on condition of existing rows. i would like to have a formula on a google sheets spreadsheet that checks the content of the existing top nth rows, and adds extra rows based on the content of these.i read this question:how to automatically insert a new row and retain functions/formulas from last row?but it seems like the trigger here is when the spreadsheet is opened (onopen() function?), rather than a condition on existing rows.here is an example:<url> spreadsheet has a header, and similarly to the question posted above, i would like to add a new row by checking on the content of the existing ones.for example, given the fact that the users are going to be filling in the columns from bottom upwards, i would like a trigger that adds a new row 2 with entry452 / na / na / na, keeping the same formulas, etc. as entry451, and it does so when we are running out of empty rows. e.g., when there are less than 5 rows left with nas in column machine_type.",
    "present_kp": [],
    "absent_kp": [
      "google spreadsheets"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "enospc: why is 'btrfs balance' required? what does it do?. i've needed to run btrfs balance on my single device filesystem as i was getting enospc even though there was indeed free space.why does this need to be run?what does the balance actually do?",
    "present_kp": [
      "btrfs"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "change freebsd's fetch with aria2 or axel. freebsd uses fetch command in order to download source codes or binary packages it needs to compile or install.if you are on slow connection like caf or shared band this process become painful. fetch can't resume in most cases and fails so as the process trying to install or compile something. i want to know if it is possible to use aria2 or axel instead of fetch.",
    "present_kp": [
      "freebsd"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "super key (a.k.a. windows key) not working in i3wm (fedora 26). the super key does not work with i3wm when i configure it as the configured modifier key.the used configuration file is the one that is initially generated choosing the super/windows key instead of alt.xev recognises the key presses of the super key well enough. keypress event, serial 40, synthetic no, window 0x200001, root 0x289, subw 0x0, time <phone>, (38,142), root:(763,601), state 0x0, keycode 133 (keysym 0xff7e, mode_switch), same_screen yes, xlookupstring gives 0 bytes: xmblookupstring gives 0 bytes: xfilterevent returns: falsekeyrelease event, serial 40, synthetic no, window 0x200001, root 0x289, subw 0x0, time <phone>, (38,142), root:(763,601), state 0x2000, keycode 133 (keysym 0xff7e, mode_switch), same_screen yes, xlookupstring gives 0 bytes: xfilterevent returns: falseso the identifier of the super key is mode_switch. i found this question which talks about a very similar problem. however, the answer did not work on my case. using xmodmap (i know i should be using setxkbmap instead, but i couldn't find the equivalent for these commands) i did:xmodmap -e clear mod4which worked as expected. and then:xmodmap -e add mod4 = mode_switchwhich returned the following error:x error of failed request: badvalue (integer parameter out of range for operation) major opcode of failed request: 118 (x_setmodifiermapping) value in failed request: 0x17 serial number of failed request: 11 current serial number in output stream: 11questionwhat is the setxkbmap equivalent answer to the question i linked?how do i avoid the error xmodmap gave me?an answer to any of these could help me get the super key working.",
    "present_kp": [
      "fedora",
      "i3",
      "super key"
    ],
    "absent_kp": [
      "keyboard shortcuts",
      "tiling wm"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to input text into a new text file using nano from command line?. how to input text into a new text file using nano from command line?i would like the same as with the following, but using nano:echo 'hello, world.' >foo.txtresult:nano is not capable of handling non-interactive text input. echo is available in every linux/unix system, while nano is not installed by default in every linux/unix system. echo can be also used in shell scripts, too.conclusion:the most compatible solution is to use echo 'hello, world.' >foo.txtas solution to create a file and fill with input text non-interactively.",
    "present_kp": [
      "command line",
      "text",
      "input",
      "nano"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "do sql devs create sql queries using sql query designers?. i was just curious if sql devs write their code freehand or do they make use of the visual query designer to generate queries? in the majority of the cases, the query designer can create most of non-complex queries, no? (i'm a winforms dev just now getting started with sql server)",
    "present_kp": [
      "sql",
      "sql server"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "flow of control? or control of flow?. in most pl textbooks (even in language specifications) statments like if, while, for, break are categorized as flow of control statements. however, it is not consistent; i.e. sometimes the same text uses control-flow statement (note the dash), or control flow statment, i read sentences that says control flows...i understand those statements control the flow of execution, hence control of flow. but many texts (even wikipedia) says otherwise, implying control is something that flows in a programwhat is the formal, language-agnostic definition of this concept in cs, if any?",
    "present_kp": [],
    "absent_kp": [
      "terminology",
      "programming languages"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "gnupg 2.1.16 : changes concerning option --with-fingerprint. before importing a key from a file i want to check the fingerprint of the key. according to the instructions from the centos wiki i use the command gpg --quiet --with-fingerprint <path of key file>. if i use gnupg 2.1.16 (self compiled) or gnupg 2.1.17 (opensuse tumbleweed or archlinux (command gpg)) the output does not contain the key.if i use gnupg 2.1.15 (self compiled) or gnupg 2.1.13 (fedora (command gpg2)), the output contains the fingerprint as expected.how do i get the fingerprint with the newer gnupg versions?below further information on my tests:the used key file: <url> of gpg --quiet --with-fingerprint ./rpm-gpg-key-centos-7 (line breaks may be wrong)with gnupg 2.1.17: pub rsa4096 2014-06-23 [sc]uid centos-7 key (centos 7 official signing key) <<email> gnupg 2.1.16: pub rsa4096 2014-06-23 [sc]uid centos-7 key (centos 7 official signing key) <<email> gnupg 2.1.13: pub rsa4096 2014-06-23 [sc]6341 ab27 53d7 8a78 a7c2 7bb1 24c6 a8a7 f4a8 0eb5uid centos-7 key (centos 7 official signing key) <<email>",
    "present_kp": [
      "gpg",
      "gnupg",
      "fingerprint"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how would you explain http to a lay person. i got this question in one of the interviews i took. i started by explaining that it is like a set of rules that should be followed while moving resources around, but i did not feel convinced. is there any other analogy or take on this question ?",
    "present_kp": [
      "interview",
      "http"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to find out if i have been removed from a friend's yahoo! messenger's contact list. is there any way to know if someone has removed me from his/her yahoo! messenger friends list?",
    "present_kp": [
      "yahoo"
    ],
    "absent_kp": [
      "chat"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is there any performance penalty to having multiple segments for one volume in lvm?. we have a big drive that we've split up using lvm. we allocated one segment to the root filesystem and several other segments to other filesystems. then we wanted to add more space to the root filesystem, so we carved out another segment at the unallocated end and added it (meaning that the root filesystem is composed of two disjoint segments).lately we've been having some i/o performance issues and the thought is that the non-contiguous root volume could be to blame. we're considering moving things around to make the root volume one (contiguous) segment, in hopes of improving performance (the thought being that this will make it easier to access things).another school of thought is that lvm has this all sorted out and it's not going to make any difference (other than to make the graphic of the drive layout a bit more compact).what's the likely penalty we're paying for the two-segment volume?",
    "present_kp": [
      "filesystems",
      "performance",
      "lvm"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "leetcode 125. valid palindrome (better performance?). problem statementgiven a string, determine if it is a palindrome, considering only alphanumeric characters and ignoring cases.for example,a man, a plan, a canal: panama is a palindrome.race a car is not a palindrome.note:have you consider that the string might be empty? this is a good question to ask during an interview.for the purpose of this problem, we define empty string as valid palindrome.introduction of algorithmthe current implementation beats 87% c# submission, runtime: 112 ms, my last practice only beats 25.59% c# submission. i also value the good performance result and this practice just accidentally showed me a possible good design. i enjoyed the practice because i ran into time out-of-index error, so i put some defensive checking of index-out-of-range 3 times, the first one is begin < end in first nested while loop, the 2nd, 3rd checkings are added without hesitation. in detail, 2nd is end >= 0 in the second nested while loop, 3rd is if(start >= end) after the nested two while loops. and surprisingly the performance is much better than the one filtering nonalphanumeric characters first using o(n) time (n is the string's length). i am open to the advice of performance, coding guidelines, nested while, defensive checking etc. i did some study in june 2016 about coding strategies, because i failed my first practice of leetcode 125, i got so many options, use while/if, nested while, or flat code. i did not have a good understanding what is important. at the beginning, i say no to nested while loop, try to make the code more flat to left side, since i studied cyclomatic complexity and learn to lower the cyclomatic complexity of the function. please take a look the performance of this practice:the c# code passes leetcode online judge. using system;using system.collections.generic;using system.diagnostics;using system.linq;using system.text;using system.threading.tasks;namespace leetcode125_ispalindrome{ class validpalindrome { /* * leetcode 125: valid palindrome * <url> */ static void main(string[] args) { runtestcases(); } public static void runtestcases() { debug.assert(ispalindrome(aa)); debug.assert(ispalindrome(ab%ba)); debug.assert(ispalindrome(b%1*1b)); debug.assert(!ispalindrome(b%a*1b)); } /* * given a string, determine if it is a palindrom, considering * only alphanumeric characters and ignoring cases * time complexity: * o(n) * * empty string is valid palindrome */ public static bool ispalindrome(string rawstring) { if (rawstring == null || rawstring.length == 0) { return true; } // two pointers techniques int start = 0; int end = rawstring.length - 1; while (start < end ) { while (start < end && !isalphabeticanddigit(rawstring[start])) { start++; } while (end >= 0 && !isalphabeticanddigit(rawstring[end])) { end--; } if(start >= end) { return true; } char left = rawstring[start]; char right = rawstring[end]; if (toupper(left) != toupper(right)) { return false; } start++; end--; } return true; } /* * check if the char is alphabetic or digit * a-z * a-z * 0-9 */ private static bool isalphabeticanddigit(char anychar) { if (iscapitalcasealphabetic(anychar) || islowercasealphabetic(anychar) || isdigit(anychar)) { return true; } return false; } private static bool iscapitalcasealphabetic(char anychar) { var number = anychar - 'a'; return number >= 0 && number < 26; } private static bool islowercasealphabetic(char anychar) { var number = anychar - 'a'; return number >= 0 && number < 26; } private static bool isdigit(char anychar) { var number = anychar - '0'; return number >= 0 && number <= 9; } /* * assuming input char is alphabetic number, * output the capitical case char */ private static char toupper(char alphabeticchar) { int number = alphabeticchar - 'a'; if (number >= 0 && number < 26) { return (char)('a' + number); } return alphabeticchar; } /* * filter out non alphanumeric characters * and keep cases */ private static string fiteroutnonalphanumbericacharacters(string rawstring) { return string.concat(rawstring.where(c => isalphabeticanddigit(c) == true).toarray()); } }}",
    "present_kp": [
      "c#",
      "performance",
      "algorithm",
      "palindrome"
    ],
    "absent_kp": [
      "programming challenge"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "validate answers to online quiz. i'm working on building a website to help people practice english. in this practice, the answer in the blank needs to be validated on hitting enter.i'd like to know if there is a simpler/better way to write it since i need to use it for a lot of pages.html:<p>theres an energy here. at least, i think there is. is it just that i know this city to be so? that there can be skiing a half hour away on the north shore mountains <input class=fillblank type=text id=one placeholder=your answer> there is tennis in the city?</p><p>but then, <input class=fillblank type=text id=two placeholder=your answer> my walk i took this photo below.</p><p>the city is also lush which means that it rains on <input class=fillblank type=text id=three placeholder=your answer>. okay, maybe more than occasionally at this time of year.</p>css:.fillblank {width: 100px;border-radius: 0;border: none;background-color: #eff9f7;padding: 3px; text-align: center;}js: $(#vancouver).ready(function() { $(.fillblank).val(); });// vancouver quiz answer onevar one = document.getelementbyid(one);one.addeventlistener(keydown, function (e) {if (e.keycode === 13) {valone(e);}});function valone(e) {var one = document.getelementbyid(one).value;if (one === while) {document.getelementbyid(one).style.color = green;}else {document.getelementbyid(one).style.color = red;}}// vancouver quiz answer twovar two = document.getelementbyid(two);two.addeventlistener(keydown, function (e) {if (e.keycode === 13) {valtwo(e);}});function valtwo(e) {var two = document.getelementbyid(two).value;if (two === on) {document.getelementbyid(two).style.color = green;}else {document.getelementbyid(two).style.color = red;}}// vancouver quiz answer threevar three = document.getelementbyid(three);three.addeventlistener(keydown, function (e) {if (e.keycode === 13) {valthree(e);}});function valthree(e) {var three = document.getelementbyid(three).value;if (three === occasion) {document.getelementbyid(three).style.color = green;}else {document.getelementbyid(three).style.color = red;}}",
    "present_kp": [
      "quiz"
    ],
    "absent_kp": [
      "javascript"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "libxml2.so.2: cannot open shared object file: no such file or directory. i am using centos in which i had removed libxml2 accidentally now it was showing the following error as follows:there was a problem importing one of the python modules required to run yum. the error leading to this problem was:libxml2.so.2: cannot open shared object file: no such file or directoryplease install a package which provides this module, or verify that the module is installed correctly.it's possible that the above module doesn't match the current version of python, which is: 2.4.3 (#1, jun 18 2012, 08:55:31) [gcc 4.1.2 20080704 (red hat 4.1.2-52)]if you cannot solve this problem yourself, please go to the yum faq at: <url>",
    "present_kp": [
      "centos",
      "yum"
    ],
    "absent_kp": [
      "linux"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "can you force netcat to succeed only if all ports in port range succeed?. i use nc as part of a verification script, and check the output of each nc command listed against what the expected output should be.e.g.,nc -zvw1 serv1.host.com 443 | gawk '{print $7}'expected output: succeeded!the problem i'm facing is if i want to check a range of ports, while some tests may in fact return succeeded!, there's no guarantee that all ports returned as such. this is an issue for me because i compare cmd:output on a 1:1 basis, based off a configuration file that lists the commands and the expected output.instead of listing something like:nc -zvw1 serv1.host.com 443 | gawk '{print $7}' nc -zvw1 serv1.host.com 444 | gawk '{print $7}' nc -zvw1 serv1.host.com 445 | gawk '{print $7}'expected result: succeeded! expected result: succeeded! expected result: succeeded!i'd like to be able to force nc to fail if any ports in the range fail; so in this case my configuration could be condenced to:nc -zvw1 serv1.host.com 443-445 | gawk '{print $7}'expected result: succeeded!this is a long-winded description of a straight-forward question, unfortunately. apologies.",
    "present_kp": [
      "netcat"
    ],
    "absent_kp": [
      "linux"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "recommendations for setup of test environment. we host several sites on a cloud server. it constantly seems to be getting picked up by google, but our clients need to be able to access there in progress builds to add content. this brings with it several problems because we can't assign ips to block as most clients would be on a dynamic ip.is there any recommendations for any sort of portal system that would allow us to have quite alot of sites based off the portal domain and accessed seperately via a username and password. even if with the one password you could still view someone elses site?our setup is currently used for magento / wordpress sites and as such obviously needs to be php5.",
    "present_kp": [
      "php",
      "server"
    ],
    "absent_kp": [
      "testing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "any good alternatives to mediawiki for creating an 'intranet' type system?. i would like to install some form of intranet system to allow multiple users to add documentation to - mediawiki would suffice, i just wondered if anyone knew of any better alternatives (must be php).",
    "present_kp": [
      "php",
      "mediawiki",
      "intranet"
    ],
    "absent_kp": [
      "looking for a script"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is the most managable way to install a program from source in debian?. let's say that i've found a program that i want to install on my machine. it is only available as a source tarball that can be built with the usual ./configure; make; make install.while i could do make install in these instances, i'm likely not going to know where xyz file/command come from in a few years, and whether it is important. and who knows what will happen if i try to build a newer version of the program.what's the most common, manageable way of handling scenarios like this?",
    "present_kp": [],
    "absent_kp": [
      "software installation",
      "package management",
      "compiling"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "turbovnc server displays sad face when i login. hi all been at this a for a few days now and cant understand why i get the sad face after i login to vncviewer.details:i have two accounts root and user1 now i can ssh from a windows box and start a server with root login which in turn will let me login via vncviewer and work no problems. however if i try this method with my user1 account i get the sad face or black screen.server running details: ./vncserverdesktop 'turbovnc: rhel7pod1:1 (user1)' started on display rhel7pod1:1starting applications specified in /home/user1/.vnc/xstartup.turbovnclog file is /home/user1/.vnc/rhel7pod1:1.logi have changed the group to engineer which user1 is part of but i believe that the main issue here is permissions as root runs fine?if you need any information let me know and i will post accordingly.many thanksedit1:looking into the xvnc and when i run that command i get the following: ./xvnc turbovnc server (xvnc) 64-bit v2.0.2 (build 20160312) copyright (c) <phone> the virtualgl project and many others (see readme.txt) visit <url> for more information on turbovnc 20/09/2016 16:17:12 using auth configuration file /etc/turbovncserver-security.conf 20/09/2016 16:17:12 notice: pam-userpwd is a permitted auth method fatal server error: error: no authentication methods enabled!here is the config turbovncserver-security.conf # uncomment the following to turn on the user access control list whenever # the pam user/password authentication method is used. # (default: user acl is disabled) #enable-user-acl # this specifies the maximum idle timeout (in seconds) for all turbovnc # sessions started on this machine. the idle timeout is the amount of time # that a turbovnc session can remain idle (with no vnc viewer connections) # before it will automatically exit. if this value is set to a number greater # than 0, then all turbovnc sessions on this machine will use this idle timeout # value by default, and the user will only be allowed to override it with a # lower value. #max-idle-timeout = 86400 # this specifies the maximum desktop size for all turbovnc sessions started on # this machine. if a user attempts to start a session with a larger geometry # than this or to use remote desktop resizing to increase the desktop size to a # size larger than this, the desktop size will be clamped to this width/height. #max-desktop-size = 3200x1800 # uncomment the following to globally disable the automatic sending of # clipboard changes from turbovnc server sessions to their connected viewers. # (default: clipboard sending is allowed) #no-clipboard-send # uncomment the following to globally disable the automatic sending of # clipboard changes to turbovnc server sessions from their connected viewers. # (default: clipboard receiving is allowed) #no-clipboard-recv # uncomment the following to globally disable the ability to make reverse # vnc connections. # (default: reverse connections are allowed) #no-reverse-connections # uncomment the following to globally disable inbound remote connections to all # turbovnc servers started on this machine. this effectively forces ssh # tunneling to be used for all inbound turbovnc connections. # (default: inbound remote connections are allowed) #no-remote-connections# uncomment the following to globally disable the built-in http server in all # turbovnc servers started on this machine. #no-httpd # uncomment the following to globally disable x11 tcp connections to all # turbovnc servers started on this machine. #no-x11-tcp-connections # set pam-service-name to the name of the pam service that you will use to # to process pam user/password authentications from turbovnc. this service # name typically corresponds to a file in /etc/pam.d or to one or more lines in # /etc/pam.conf. # (default: turbovnc) pam-service-name = password-auth-ac # set the following to any combination of vnc, otp, pam-userpwd, or# none, separated by commas. if the following variable is enabled and # a particular authentication method is not listed in it, then users cannot # enable that authentication method by using xvnc command line arguments. this# variable also controls the order in which the corresponding authentication # capabilities are advertised to the turbovnc viewer. see the man pages and # user's guide for more information. # (default: vnc, otp, pam-userpwd, none) #permitted-auth-methods = vnc, otp, pam-userpwd permitted-auth-methods = pam-userpwdedit 1 endedit 220/09/2016 16:38:25 using auth configuration file /etc/turbovncserver-security.conf20/09/2016 16:38:25 enabled authentication method 'pam-userpwd'20/09/2016 16:38:25 advertising security type 'tight' to viewers20/09/2016 16:38:25 desktop name 'turbovnc: rhel7pod1:3 (user1)' (rhel7pod1:3)20/09/2016 16:38:25 protocol versions supported: 3.3, 3.7, 3.8, 3.7t, 3.8t20/09/2016 16:38:25 listening for vnc connections on tcp port 590320/09/2016 16:38:25 interface 0.0.0.020/09/2016 16:38:25 listening for http connections on tcp port 580320/09/2016 16:38:25 url http://rhel7pod1:580320/09/2016 16:38:25 interface 0.0.0.020/09/2016 16:38:25 framebuffer: bgrx 8/8/8/820/09/2016 16:38:25 maximum clipboard transfer size: <phone> bytes20/09/2016 16:38:25 vnc extension running!xlib: extension glx missing on display :3.gnome-session-is-accelerated: no hardware 3d support.xlib: extension glx missing on display :3.gnome-session-check-accelerated: helper exited with code 256xlib: extension glx missing on display :3.gnome-session-is-accelerated: no hardware 3d support.xlib: extension glx missing on display :3.gnome-session-check-accelerated: helper exited with code 256** (process:9464): warning **: software acceleration check failed: child process exited with code 120/09/2016 16:38:42 got connection from client 192.<phone>/09/2016 16:38:42 using protocol version 3.820/09/2016 16:38:42 enabling tightvnc protocol extensions20/09/2016 16:38:52 pixel format for client 192.168.103.17:20/09/2016 16:38:52 32 bpp, depth 24, little endian20/09/2016 16:38:52 true colour: max r 255 g 255 b 255, shift r 16 g 8 b 020/09/2016 16:38:52 no translation needed20/09/2016 16:38:52 using tight encoding for client 192.<phone>/09/2016 16:38:52 enabling full-color cursor updates for client 192.<phone>/09/2016 16:38:52 enabling cursor position updates for client 192.<phone>/09/2016 16:38:52 using jpeg subsampling 0, q100 for client 192.<phone>/09/2016 16:38:52 using jpeg quality 95 for client 192.<phone>/09/2016 16:38:52 using jpeg subsampling 0 for client 192.<phone>/09/2016 16:38:52 enabling lastrect protocol extension for client 192.<phone>/09/2016 16:38:52 enabling desktop size protocol extension for client 192.<phone>/09/2016 16:38:52 enabling extended desktop size protocol extension for client 192.<phone>/09/2016 16:38:52 enabling continuous updates protocol extension for client 192.<phone>/09/2016 16:38:52 enabling fence protocol extension for client 192.<phone>/09/2016 16:38:52 rfbprocessclientnormalmessage: ignoring unknown encoding -305 (fffffecf)20/09/2016 16:38:52 using tight compression level 1 for client 192.<phone>/09/2016 16:38:52 using 1 thread for tight encoding20/09/2016 16:38:52 continuous updates enabled20/09/2016 16:38:58 client 192.168.103.17 gone20/09/2016 16:38:58 statistics:20/09/2016 16:38:58 key events received 6, pointer events 4120/09/2016 16:38:58 framebuffer updates 1, rectangles 21, bytes <phone>/09/2016 16:38:58 lastrect markers 1, bytes 1220/09/2016 16:38:58 cursor shape updates 1, bytes 1220/09/2016 16:38:58 cursor position updates 1, bytes 1220/09/2016 16:38:58 tight rectangles 18, bytes <phone>/09/2016 16:38:58 raw equivalent 4.464012 mbytes, compression ratio 226.174799edit 2 end",
    "present_kp": [
      "vnc"
    ],
    "absent_kp": [
      "linux"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "which bash is executing?. i think i have configured bash as shell, but why is this happening?my shell is bash:# ls -al 'which sh' lrwxrwxrwx 1 root root 4 jul 12 03:25 /bin/sh -> bash error executing a script with sh# sh ubuntu/util.sh ubuntu/util.sh: line 32: 'test-build-release': not a valid identifierno error from bash# bash ubuntu/util.sh",
    "present_kp": [
      "bash",
      "shell"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "inventory consignment flow. not sure whether this is the right place to ask this question, but here goes..currently i have requirement to add support for consignment transaction in our inventory module. i have a very limited understanding of what consignment means in inventory, i.e. customer get stocks/products from seller without actually buying them, the product just resides in the customer's inventory and it's still owned by the seller. only when the customer actually buy the stocks then only will the ownership of the stock is transferred.the issue is i can't imagine how the data will be presented to both the customer and the seller. what i know is that i would need to deduct the stock from the seller's inventory when the customer raise a request to get the stock through consignment, but what about the 'ownership' of the stocks/products? does that mean i would need to create another column in my table to state that for each inventory it is owned by who?anywhere i can get information on how i should work out an inventory module like this?thanks.",
    "present_kp": [],
    "absent_kp": [
      "terminology",
      "enterprise architecture"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "man -t converts - to. man -t ls converts - to . is there a way i can tell man -t to not do that? i prefer having -, as the - is often part of examples where would be wrong (e.g. options).",
    "present_kp": [
      "man"
    ],
    "absent_kp": [
      "groff"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "first mvc router. i'm learning the mvc pattern and today i finished my first mvc routing class.hope you can give me some tipps and improvements.router.php<?phpnamespace system\\libraries;class router{ /** * store the given parameters * * @var array */ private $_param = []; /** * default action * * @var bool */ private $_action = 'index'; /** * store the given request * * @var array */ private $_request = []; /** * default controller * * @var bool */ private $_controller = 'index'; /** * constructor */ public function __construct() { $this->spliturl(); $this->setroute(); } /** * split the url * * @return void */ public function spliturl() { if (isset($_get['url']) && !empty($_get['url'])) { $this->_request = trim(filter_var(strtolower($_get['url']), filter_sanitize_url)); $this->_request = explode('/', rtrim($this->_request, '/')); } else { $this->_request = null; } } /** * check the request and load the controller * * @return void */ public function setroute() { if (isset($this->_request[0])) { if (file_exists(root . '/app/controllers/' . $this->_request[0] . '.php')) { $this->_controller = $this->_request[0]; } else { $this->_controller = 'error'; } } $this->_controller = 'app\\controllers\\' . ucfirst($this->_controller); $this->_controller = new $this->_controller; if (isset($this->_request[1])) { if (method_exists($this->_controller, $this->_request[1] . 'action')) { $this->_action = $this->_request[1]; } else { $this->_action = 'error'; } } unset($this->_request[0], $this->_request[1]); if (isset($this->_request[2])) { $this->_param = array_values($this->_request); } call_user_func_array([$this->_controller, $this->_action . 'action'], $this->_param); } /** * destory the variables * * @return void */ public function __destruct() { $this->_param = null; $this->_model = null; $this->_action = null; $this->_request = null; $this->_controller = null; }}?>",
    "present_kp": [
      "php",
      "mvc"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "what is redirecting mobile visitors to our website?. our site may have been hacked.it loads okay on desktop browsers, but on mobile devices it redirects to a porn site.i can't see anything in the source code or .htaccess which would be causing this.the site is wordpress based, and it used to use wordpress mobile edition, but that is now deleted, yet the symptom persists.can you see what is redirecting mobile visitors?",
    "present_kp": [
      "htaccess"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "simple age calculator. i have written an age calculator that takes a birthdate as input.i'd like a general review of this. i'm especially concerned about the message variable and the lines after the try/catch statement.namespace age{ class program { static void main(string[] args) { while (true) { try { console.write(enter your birtdate: ); datetime birthdate = datetime.parse(console.readline()); int days = (datetime.now.year * 365 + datetime.now.dayofyear) - (birthdate.year * 365 + birthdate.dayofyear); int years = days / 365; string message = (days >= 365) ? your age: + years + years : your age: + days + days; console.writeline(message); } catch { console.writeline(you have entered an invalid date. ); } console.writeline(exit? (y/n)); string uservalue = console.readline(); if (uservalue == y) { environment.exit(0); } } } }}",
    "present_kp": [
      "datetime"
    ],
    "absent_kp": [
      "c#",
      "beginner"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "multithreaded c# mvvm application architecture. i have an application built in c# using the mvvm pattern that will have 40-50 different tests that need to be run. i want to show each test as a list item with a progress bar of how far each test has progressed. i want to run multiple tests at one time and at some point in the future there may be dependencies where certain tests have to run before other tests run. i'm trying to figure out how to set up the threads to manage these requirements. i'm aware of the threadpool, but i'm not sure it will work for my requirements. what are the limitations of thread pool and is there a better way to go about building these requirements?",
    "present_kp": [
      "c#",
      "architecture",
      "mvvm"
    ],
    "absent_kp": [
      "multithreading"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "i know what i want to do, but i have a hard time coding it?. after doing some interviews, i've realized that i usually know what i want to do to solve a problem, but i often get my logic complicated (and introduce a lot of bugs) when i'm actually coding. for example, when i tried to code up insertion into a sorted singly linked list, this is what i knew i had to do:i had to check for the case where the list is currently nulli had to check if the element i want to insert is less than the current headto insert between nodes, we had to link one node to the new node, and the new node's next to the one that is supposed to be after.i had trouble inserting between nodes because i was actually supposed to check the next node's data and not the node which i had my pointer pointing to (so i could set current's next to the new node). i also struggled on this problem (note that this was coded on white board) because i was pressured under time and i think i made it more complicated in my mind that i should have. when i went back to this problem after the interview, it turns out the solution isn't complicated at all (as it shouldn't be). however, i just tend to complicate things and confuse myself.would anyone have any suggestions on how i could improve this vulnerability of mine?",
    "present_kp": [],
    "absent_kp": [
      "programming practices"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "deleting all lines after first occurrence of a string in a line. i have a large file test.txt like this example:foobefore ...before some line foo something interesting barafter some linesafter ...barhow do i create a new file with just the lines before the first occurrence of the string something interesting with basic bash commands like sed or grep (not awk, i need this on an embedded device without awk)?",
    "present_kp": [],
    "absent_kp": [
      "text processing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "are there any practical differences between a turing machine with a prng and a probabilistic turing machine?. say i plugged in a hardware true-random number generator (trng) to my computer, then wrote programs with output that depends on the trng's output. can it do anything non-trivial that a turing machine with a psuedo-random number generator can't do? (a trivial thing it can do would be generating truly random numbers)",
    "present_kp": [],
    "absent_kp": [
      "turing machines",
      "randomness",
      "probabilistic algorithms"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "viterbi algorithm for shortest path calculation. i have to write an essay about shortest path calculation with viterbi algorithm. since i am interested in finding the path with the least weight on the network graph, i am a little bit confused how to model the network since viterbi works only on trellis graphs. does viterbi require directed acyclic graph or it could be used on graph with cycles? on this link i found some hints how viterbi is used to compute shortest path. it seems like bellman ford algorithm to me, but there is some topological sorting in first step of the algorithm? why i need it?",
    "present_kp": [
      "graphs",
      "shortest path"
    ],
    "absent_kp": [
      "algorithms"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "small javascript library for ecmascript version detection. i wrote a library that detects the version of the ecmascript that is running on javascript, and that also allows to check support for few syntax-features.for the ecmascript version detection i run 4 checks, each check tests if javascript supports some features of a specific ecmascript version. until a check is successful, its specific ecmascript version is returned. if no check matches, the return is null. each check is just a detection for a specific ecmascript version. check 1 detects es7, check 2 detects es6, check 3 detects es5 and check 4 detects es3.for the syntax-features detection i basically have an object, which is pre-declared in my library's factory, containing raw codes to be evaluated, that's syntax-features mixed with others. in this detection i build a string, which will be a code to be evaluated. the string will always contain an anonymous function, inside this anonymous function will be pushed raw codes according to the syntax features i want (if the raw code for any of these syntax features isn't specified in the earlier object, the non-existent specific(s) feature(s) won't affect anything), with an extra empty statement (semicolon). finally, the result code will be evaluated. if an error is thrown, that means one or more of the features aren't supported, so we get false as detection result. else, the detection result is true.this is my library code:;(function (root, name, factory) { 'use strict'; if ((function === typeof define) && define.amd) define([exports], factory); else if (typeof exports === 'object') factory(exports); else factory(root[name] = {});})(this, 'esx', function (exports) { 'use strict'; var features = { arraycomprehensions: [for(_ of [0])_] , arrowfunction: (_=>_) , class: (class{}) , const: const c=true , defaultparams: (function(a=false){}) , destructuring: let {d}={a:true} , forof: for(var b of []) , generator: (function*(){}) , getter: ({get a(){}}) , label: l:0 , let: let o , reservedwords: ({catch:true}) , setter: ({set a(v){}}) , spread: [...[]] , stringinterpolation: '$\\{0}' , stringlinebreak: '\\ ' , super: ({b(){super.a}}) , yield: (function*(){yield true}) }; // exports.features = features; function evaluate (code) { try { eval(code); return true; } catch(e) { return false; } } /** * check if a set of features are supported. */ function supports () { var code = (function(){; var i = 0, len = arguments.length; for (; i < len; ++i) { var feature = arguments[i].tostring(); if (features.hasownproperty(feature)) code += features[feature] + ';'; } code += })(); return evaluate(code); } exports.supports = supports; function checkes7 () { return supports(arraycomprehensions); } function checkes6 () { var methods = 'function' === typeof object.assign && 'function' === typeof object.freeze; var syntax = supports( arrowfunction , class , const , forof , defaultparams , destructuring , super , yield ); return methods && syntax; } function checkes5 () { var methods = 'function' === typeof [].filter && 'function' === typeof function.prototype.bind && 'function' === typeof object.defineproperty && 'function' === typeof ''.trim && 'object' === typeof json; var syntax = supports(reservedwords); return methods && syntax; } function checkes3 () { return function === typeof [].hasownproperty; } /** * check for ecmascript version. */ exports.detectversion = function () { return checkes7() ? 7 : checkes6() ? 6 : checkes5() ? 5 : checkes3() ? 3 : null; };});my library can be used so, then:console.log(esx.detectversion() >= 6); // true hereconsole.log(esx.supports(arrowfunction, super));// ^^^^^^ true here (supports arrow functions and super)please give your constructive criticisms and suggestions.",
    "present_kp": [
      "javascript",
      "library"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "messy handler for multiple touch events in a game. i have a mobile game, utilizing cocos2d-x, that has a handler for touch events.what it does:main function that gets the touch.loop in module a. all sprites check if the touch coordinates are in the boundary of one of the sprite. if yes, call the function that associated with the action and break the loop.the same as 2 with module b.same as 2,3, with module c and so on.in some some parts of the loop there are boolean indicators that say:if boolind1 is true, ignore touches from module a.if boolind2 is true, ignore touches from module b.and so on...it's a very long and complex function, and i was wondering if i can rewrite this function with a better structure.void maingame::ontouchesbegan(const std::vector<touch*>& touches, event *event){ bool fontsigntouched = false; for( auto& touch : touches) { auto location = touch->getlocation(); //once the next level button is triggered to show all other touchs are disabled vector<node *> scorecontainerchildren = pscorecontainer->getchildren(); for (auto iter = scorecontainerchildren.begin(); iter != scorecontainerchildren.end(); ++iter) { node *childnode = *iter; if(childnode->gettag() == buttons_tags::sound_bt) { point thistouchscorecontainer = pscorecontainer->converttouchtonodespace(touch); sprite* psoundbt = static_cast<sprite*>(*iter); if(psoundbt->getboundingbox().containspoint(thistouchscorecontainer)) { pscorecontainer->setsoundbuttonspriteframe(true); break; } } if(childnode->gettag() == buttons_tags::points_container_node) { sprite* pcoinsbt = (sprite*)childnode->getchildbytag(buttons_tags::coins_img_bt); sprite* pcoinscountframebt = (sprite*)childnode->getchildbytag(buttons_tags::coins_count_frame); point thistouchpointscontainernode = childnode->converttouchtonodespace(touch); if(pcoinsbt->getboundingbox().containspoint(thistouchpointscontainernode)) { setpopupwindow(); break; } if(pcoinscountframebt->getboundingbox().containspoint(thistouchpointscontainernode)) { setpopupwindow(); break; } } } vector<node *> fontselectioncontainerchildren = pfontselectioncontainer->getchildren(); for (auto iter = fontselectioncontainerchildren.begin(); iter != fontselectioncontainerchildren.end(); ++iter) { node *childnode = *iter; if(childnode->gettag() == sign_tags::letter_sigh) { point thistouchpositionfontselection = this->converttouchtonodespace(touch); sign* psign = static_cast<sign*>(*iter); if(psign->getboundingbox().containspoint(thistouchpositionfontselection)) { settings::getinstance()->getsoundmanager().playeffect(font_to_solution); psolutioncontainer->setfontselectiontosulotionfont(psign); fontsigntouched = true; break; } } } vector<node *> thisselectionchildren = this->getchildren(); if(!fontsigntouched) { for (auto iter = thisselectionchildren.begin(); iter != thisselectionchildren.end(); ++iter) { node *childnode = *iter; if(childnode->gettag() == sign_tags::letter_sigh) { point thistouchpositionfontselection = this->converttouchtonodespace(touch); sign* psign = static_cast<sign*>(*iter); if(psign->getboundingbox().containspoint(thistouchpositionfontselection)) { settings::getinstance()->getsoundmanager().playeffect(solution_to_font); pfontselectioncontainer->removefrommainparantandsetinsprite(psign); break; } } if(childnode->gettag() == buttons_tags::next_bt) { point thistouchpositionfontselection = this->converttouchtonodespace(touch); sign* psign = static_cast<sign*>(*iter); if(psign->getboundingbox().containspoint(thistouchpositionfontselection)) { } } } } else { fontsigntouched = false; } }}",
    "present_kp": [],
    "absent_kp": [
      "c++",
      "event handling"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "architecture for mockable dal on large projects. i have recently been reading an article about creating a blog using asp.net and mvc, and in the article the user splits the data access layer into a separate class library, and creates an interface for this to allow the dal to be mocked for testing.this works great for small projects, but i am struggling to see how this will scale. for example in the article you end up with the following interface:public interface iblogrepository{ ilist<objects.post> posts(int pageno, int pagesize); int totalposts(bool checkispublished = true); ilist<objects.post> postsforcategory(string categoryslug, int pageno, int pagesize); int totalpostsforcategory(string categoryslug); objects.category category(string categoryslug); ilist<objects.post> postsfortag(string tagslug, int pageno, int pagesize); int totalpostsfortag(string tagslug); objects.tag tag(string tagslug); ilist<objects.post> postsforsearch(string search, int pageno, int pagesize); int totalpostsforsearch(string search); objects.post post(int year, int month, string titleslug); ilist<objects.category> categories(); ilist<objects.tag> tags(); ilist<objects.post> posts(int pageno, int pagesize, string sortcolumn, bool sortbyascending); void addpost(objects.post post);}there is then an associated .cs file with the implementation of this interface.how would you implement a similar architecture for a much larger project? for example the project i have at work consists of 25 controllers, each having as a minimum list, add, edit, view, delete and count. that would lead to an interface with 150+ functions.is this kind of architecture still suitable for larger projects, and if so how would you structure this to avoid having a single file implementing 150+ functions?",
    "present_kp": [
      "architecture",
      "asp.net",
      "mvc"
    ],
    "absent_kp": [
      "large scale project"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "forcing bracket to go under cursor. so i am trying out javascript on vim. it is great that it automatically indents the braces for me when i close them etc.. but i was typing some js and got into a bracket muddle.here is what i had:angular.module('functionalities', []).directive(entering, function() {_}})i have put an underscore to represent where my cursor was when i started typing, in this case it was on this line in between the two braces : .directive(entering, function() {_} when i press enter the closing brace stays behind my cursor, is there a way to get it to go to the line underneath my cursor and indent correctly? like this:angular.module('functionalities', []).directive(entering, function() { _ }})when i close braces, it automatically indents them in the right place for me, i am not sure if this due to a plugin i may have installed like surround.vim but in this kind of situation where i insert text before the brace, it put me into a muddle with my brackets.suggestions would be incredibly appreciated.",
    "present_kp": [],
    "absent_kp": [
      "vimrc"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what are the pros and cons of inter process communication done via sockets vs shared memory?. i understand that two of the many more options for inter process communication can be :shared memorysockets actually i saw these two options being exposed by intellij idea for debugging a java application . i want to know what are the pros and cons of each approach .",
    "present_kp": [
      "debugging",
      "memory",
      "sockets",
      "process"
    ],
    "absent_kp": [
      "language agnostic"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "applying the master theorem on merge sort. i found the proof below in a textbook. i would like to know why it is important for the proof that it uses $\\lceil rac{n}{2} ceil$ instead of just $ rac{n}{2}$? i know that you can't split into calls which aren't natural numbers, but how do i present this argument formally?the proof:the recursion splits the problem into two sub problems, each with at most $\\lceil rac{n}{2} ceil$ elements. therefore, we can apply the master theorem with $a = b = 2$. so, $\\log_b{a}$ = 1. the cost of splitting is $0$ comparisons, and that of combining is at most $n1$ comparisons. hence the cost of split/combine is $\\theta(n) = \\theta(n\\log_b{a})$, so we are in the second case of the master theorem, and therefore the total cost is $\\theta(n\\log n)$.",
    "present_kp": [
      "master theorem"
    ],
    "absent_kp": [
      "algorithm analysis",
      "runtime analysis",
      "sorting",
      "recurrence relation"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "displaying a scheduled job under a scheduled time. i use at to schedule jobs and atq to display the scheduled jobs. but it mildly irritates me in that i have to look up each job individually. i wrote a python script to display the scheduled job under the scheduled time. (in my world all the commands will begin 'task') so instead ofstafflinux$atq8 mon aug 18 09:00:00 2014 a joseph10 tue aug 19 09:00:00 2014 a joseph15 thu aug 21 09:00:00 2014 a joseph12 fri aug 22 09:00:00 2014 a joseph9 thu aug 21 09:00:00 2014 a joseph14 sat aug 30 09:00:00 2014 a joseph7 sun aug 17 09:00:00 2014 a joseph6 mon aug 18 09:00:00 2014 a joseph11 sat aug 30 09:00:00 2014 a josephstafflinux$my script producesstafflinux$./atscript.py 8 mon aug 18 09:00:00 2014 a josephtask buy a four-way plug adapter task see guy about meeting -----------------------------------------10 tue aug 19 09:00:00 2014 a josephtask bring back personal effects from office -----------------------------------------15 thu aug 21 09:00:00 2014 a josephtask book tickets for next week-----------------------------------------i'm looking for any feedback - particularly in terms of 'pythonic' style and any and all tricks i may have missed:#!/usr/bin/pythonimport osos.system(atq > attemp.txt) file = open(attemp.txt)line = file.readline()while line: number =line[:2] print line.strip() os.system(at -c + number+ | grep task) line=file.readline() print '-----------------------------------------'print lineos.system(rm attemp.txt)",
    "present_kp": [
      "python"
    ],
    "absent_kp": [
      "child process",
      "scheduled tasks"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is the correct location to create a new wineprefix?. i have a copy of wine installed on a debian server. and i have now set up my first application.the .exe is in /home/user/appdir/appname.exei have set up a .desktop file containing:exec=env wineprefix=/home/user/.wine wine /home/user/appdir/appname.exethis all works fine. i now need a second copy of my application, so i will need a new wineprefix.should i create the prefix as /home/user/.wine2, and put the second copy of the application as /home/user/appdir2/appname.exe ?i'm just wondering because i notice that firefox is also installed and seems to have it's own wineprefix in home/user/.local/share/wineprefixes/firefoxand all the files are installed under /home/user/.local/share/wineprefixes/firefox/dosdevices/c:/program files/mozilla firefoxis either of these considered right / wrong? will one or the other come back to bite me later on (bearing in mind i may have 4 / 5 copies of my application to set up.",
    "present_kp": [
      "debian",
      "wine"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "is there a tool for generating historical process oriented resource usage statistics like top output?. i've seen many tools like collectd, systats etc for recording general usage statistics, but i would like to find a tool for generating a historical graph of system resource consumption of the top 10 processes running in a system at sample intervals. think of like running top and taking the top 10 processes and saving to and rrd database to see what programs are putting a certain system under load over time. perhaps it's just a matter of configuration of something like systats, so perhaps there are some examples online of this.",
    "present_kp": [
      "statistics"
    ],
    "absent_kp": [
      "system information"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "vim and tmux are conflicting. how can i fix this?. i'm having a painful experience with tmux. i'm using solarized in vim but when i run tmux then my vim configuration does not work. it's as if solarized is not configured in my .vimrc. out of tmux all is fine! using tmux using vim hurts my sight.... what do i need to do?",
    "present_kp": [
      "vim",
      "tmux",
      "vimrc"
    ],
    "absent_kp": [
      "linux",
      "ssh"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "transforming a list of maps representing bands. i have the following code, it takes a list of maps representing bands and transforms the band properties via the pipeline_each method. as you can see in the code, i am dealing with a lot of options being returned when i modify the band data in the map - i am using an immutable map object and therefore need to return a new map each time the function is called.i am also not happy about the type of the bands data structure being a list[map[string, any]] - particularly the any part because i need to convert the values i get out of the maps to string types in order to modify them. i am stuck with this data structure though.how can i make this code better, specifically addressing my concerns above? object exercise { val bands: list[map[string, any]] = list(map(name -> sunset rubdown, country -> uk, active -> true), map(name -> women, country -> germany, active -> false), map(name -> a silver mt. zion, country -> spain, active -> true)) def set_canada_as_country(band: map[string, any]): map[string, any] = { map(name -> band.get(name).get, country -> canada, active -> band.get(active).get ) } def strip_punctuation_from_name(band: map[string, any]): map[string, any] = { map(name -> band.get(name).get.tostring.replace(., ), country -> band.get(country).get, active -> band.get(active).get ) //tostring above is kinda gross and could cause a class cast exception //do i have to handle all the case matching on some/none for all the above calls to get? } def capitalize_names(band: map[string, any]): map[string, any] = { map(name -> band.get(name).get.tostring.touppercase, country -> band.get(country).get, active -> band.get(active).get ) //same again - don't like the tostring } def pipeline_each(data: list[map[string, any]], fnlist: list[map[string, any] => map[string, any]]): list[map[string, any]] = { fnlist.foldleft(data) {(bandlist, currfn) => bandlist.map(currfn)} }}i execute the above code via the repl like so:exercise.pipeline_each(exercise.bands, list( exercise.set_canada_as_country, exercise.strip_punctuation_from_name, exercise.capitalize_names))",
    "present_kp": [],
    "absent_kp": [
      "scala"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "extracting part of speech (source and destinations) using text mining/nlp?. i need to extract the source and destination terms from the text documents using text mining / nlp / information retrieval ?example input:i am travelling from new york to london.i am heading towards playground from home.i will be going to sweden from boston.i was flying from school to home.the output can be as follows :s. no. | source | destination------ | ----------|------------ 1| new york | london 2| playground | home 3| sweden | boston 4| school | home",
    "present_kp": [
      "nlp",
      "text mining"
    ],
    "absent_kp": [
      "machine learning",
      "nltk",
      "stanford nlp"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to contribute to vim?. i have been using vim since 4 years. i am looking forward to contribute to the open source project vim. i would like a few tips on where to start and whom to contact to contribute to vim. help from people who have already contributed to vim would help a lot :). thanks in advance. ps - i am not even sure whether i can post this here but finally decided to post it here as this would be the place with the most vim enthusiasts to talk with.",
    "present_kp": [],
    "absent_kp": [
      "vim development"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "access google spreadsheet revision history through api. is there any way i could access google spreadsheet revision history through the api?the reason for that is i have a lot of sheets and simply relying on build-in revision history does not suffice to point me to the sheet where changes were made. i need to check all the sheets to figure out what change was made - highly inefficient approach.the output i would expect is:changes made at 01 jan 2010 by someuser sheet name: sheet1updates:[{address: c1,oldvalue: oldvalue,newvalue: newvalue},{address: b2,oldvalue: oldvalue,newvalue: newvalue}, ...]",
    "present_kp": [],
    "absent_kp": [
      "google spreadsheets",
      "google apps script"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how can i judge whether changing domain from .com to more expert style domain such as .org or .edu is worth it?. i have two problems with my current domain choice: it is com -ending (all related sites are under edu or org)it is meant for all people around the world but it contains a word that is different in different dialects of englishhow can i judge whether i should make a change to other domain? the domain is about 5 years old.",
    "present_kp": [],
    "absent_kp": [
      "domains"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why the extracted content differs?. i used binwalk to extract firmware.bin image, the extracted content is squashfs-root dir, and a separate dpc.squashfs file. then i used sasquatch utility to extract content of dpc.squashfs file, and i got the same squashfs-root directory. i compared content of both squashfs-root directories and find, that the squashfs-root dir extracted with binwalk have www and www_safe folders, whereas in squashfs-root dir extracted with sasquatch that folders are missing. why the extracted content differs?",
    "present_kp": [
      "firmware"
    ],
    "absent_kp": [
      "binary analysis",
      "decompilation",
      "linux",
      "unpacking"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "should http status codes be used to represent business logic errors on a server?. i'm at a bit of a crossroads with some api design for a client (js in a browser) to talk to a server. we use http 409 conflict to represent the failing of an action because of a safety lock in effect. the satefy lock prevents devs from accidentally making changes in our customers' production systems. i've been tasked with handling 409s a bit more gracefully on the client to indicate why a particular api call failed.my solution was to wrap the failure handlers of any of our ajax calls which will display a notification on the client when something fails due to 409 - this is all fine and works well alongside other 4xx and 5xx errors which use the same mechanism.a problem has arisen where one of our route handlers responds with 409s when encountering a business logic error - my ajax wrapper reports that the safety lock is on, whilst the client's existing failure handler reports what (it thinks) the problem is based on the body of the response. a simple solution would be to change either the handler's response or the status code we use to represent the safety lock.which brings me to my crossroad: should http status codes even be used to represent business logic errors? this question addresses the same issue i am facing but it did not gain much traction. as suggested in the linked answer, i'm leaning towards using http 200 ok with an appropriate body to represent failure within the business logic.does anyone have any strong opinions here? is anyone able to convince me this is the wrong way to represent failure?",
    "present_kp": [
      "api"
    ],
    "absent_kp": [
      "rest",
      "web"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what are the dangers of using mindfulness-based techniques for individuals suffering from schizophrenia?. i am unable to find much research on the application of mindfulnesss for those suffering from schizophrenia. a recent study by chien and lee (2013) focuses on the application of a psychoeducation program for chinese patients with schizophrenia. however, i do not have the pleasure of reading this but findings indicate that it can improve psychosocial functioning (<url>).though the study show promising results for alleviating symptoms in schizophrenic patients, i feel - prima facie - that there might be a risk in exacerbating symptoms by educating schizophrenic patients to adopt a non-judgmental awareness towards positive symptoms such as hallucinations and delusional thinking. i am a supporter of mindfulness-based interventions except i can't seem to find much literature on its limitations and risks for schizophrenia.as the neurological and biological evidence around schizophrenia is far from conclusive, i am narrowing this down to only psychological and ethical risks/dangers that could potentially arise from mindfulness practice and schizophrenic individuals. for clarity around the theory underpinning schizophrenia, i would refer to the theory of mind as the overarching philosophical foundation for understanding schizophrenia and interactions with mindfulness practice. a definition is given by pedersen et al (2012):theory of mind (tom), the ability to think about mental states such as thoughts and beliefs in oneself and others, is a complex cognitive function that requires the integration of information from multiple sources. substantial evidence has accumulated that patients with schizophrenia have impaired tom functions (sprong et al., 2007; bora et al., 2009) that result in social-interactive decits.on a cognitive perspective, what dangers does mindfulness pose to a schizophrenic individual's ability to discern their self-concept and their relations with the world? examplethis is purely for illustrative purposes.x suffers from schizophrenia and visual hallucinations. x does engage in mindfulness practice regularly. his/her mindfulness practice involves paying non-judgmental attention in the present moment. initially, x paid too much attention to the occasional visual hallucinations through his mindfulness exercises. this triggered a higher amount of distress and anxiety for x and caused him/her to believe in the 'reality' of the hallucination. it could be said that there may be a potential danger that existed in the initial stages of x's introduction to mindfulness practice.questionswhat are the dangers of using mindfulness-based techniques forindividuals suffering from schizophrenia?what are the mechanisms that make mindfulness practice effective when an individual is experiencing a delusional episode?edit:i have provided some more information around the scope of the question primarily around what type of 'dangers' could result from the interaction between mindfulness practice and those suffering from schizophrenia.referenceschien, w.t., lee, i.y.m. (2013). the mindfulness-based psychoeducation program for chinese patients with schizophrenia. psychiatric services 2013",
    "present_kp": [],
    "absent_kp": [
      "cognitive psychology",
      "abnormal psychology"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is there a way to link to a netflix movie at specific time?. is there a way to link to a netflix movie at a specific time, similar to how you can link to a youtube video at a specific time?",
    "present_kp": [
      "netflix"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "mounting mergerfs directory with bind mount under lxc shows nothing unless root or 555 permissions. so i'm using proxmox and have a few hard drives mounted to /mnt/hdd1, /mnt/hdd2 etci use mergerfs so that they all show up as one drive. i then use a bind mount so that i can access the dir when in various lxc containers. i have the folder permissions set to 550 however, when i try ls -la on the directory, all it returns is total 0. i can view the directory with sudo ls -la though so it does mount.changing the permissions to 555 lets me view the directory properly however i checked using id username and i am a member of the group the directory is owned by. also, if i mount /mnt/hdd1 for example using the same method, i can access the directory with permissions 550. i tried recreating the group.any ideas what is causing this?i can access this fine from the host as the same user. (again just a member of the group)my mergerfs settings in /etc/fstab are defaults,allow_other,use_ino,func.getattr=newest. using default_permissions instead of allow_other results in d?????????, but still works for root.",
    "present_kp": [
      "permissions",
      "lxc",
      "bind",
      "proxmox"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "ida, how to show ascii comment on immediate values?. today i used ida on a owned compiled software.i notice that ida show me comment (ascii char) on immediates values as on this screenshoot below :this is the first time i saw it automaticaly. what is the option the enable it ? most of the time i must add ascii value comment manually...regards",
    "present_kp": [
      "ida"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "seo question, getting info about websites. i don't know much about seo.i have a csv file with 200,000 links to websitesi want to add another field (or maybe couple of them) to each link in the csv file with page ranking of each link and maybe other interesting metrics and info about the link.i saw a free api from <url> i can maybe use to build a simple script, but it's limited to 3 links for second which will roughly take 1100 minutes or 18 hours to runany other ideas how to get this kind of simple metrics about each link ?thanks !",
    "present_kp": [
      "api",
      "seo",
      "links"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to make a window visible on all tabs. how i can make a window automatically visible on all tabs ? like nerdtree-tabs plugin: :tabsaba 1 nerd_tree_1 main.caba 2 nerd_tree_1 makefileaba 3 nerd_tree_1> readme.md",
    "present_kp": [],
    "absent_kp": [
      "vimrc",
      "vim windows"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "codewars: sum of pairs. i've been working on this task (see screenshot). here's my answer to it so far:var sum_pairs=function(ints, s){ //your code here var ptr_1 = 0, ptr_2 = 0, i = 0, j = 0, len = ints.length, j_min = len, arr_sum = []; for (i; i < len-1; i++){ for (j = i+1; j < len; j++){ if(ints[i] + ints[j] === s){ if (j < j_min){ j_min = j; arr_sum = [parseint(ints[i]), parseint(ints[j])] } } } }return ( (arr_sum.length === 0)? undefined : arr_sum ); }however, while the tests all pass, it throws an stderr about 'optimising the code':how might i be able to optimise my code further?",
    "present_kp": [],
    "absent_kp": [
      "javascript",
      "performance",
      "time limit exceeded"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "when changing the hostname of a server, can a cname be used with the old name for backwards compatibility?. we have a vps hosting a few of dozen sites at server.example.com. we're deploying 3 more vpss this week to cope with growth & want to change our naming structure.e.g. s1.example.com, s2.example.com, s3.example.com, etc.if we change the hostname of our current server, is there a way we can set an alias (like a cname) so that we don't have to change the config of every device that connects to us for mail services? currently almost 80 devices.",
    "present_kp": [
      "vps",
      "name"
    ],
    "absent_kp": [
      "dns"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "two-way anova table. i changed some methods used previously, and i am wondering if i need to do the multiple for loops over and over again. how do i convert my arrays to lists to use in nested for loops?using system;using system.collections.generic;using system.componentmodel;using system.data;using system.drawing;using system.linq;using system.text;using system.windows.forms;using system.io;using system.threading.tasks;using twowayanova;namespace twowayanovatable{public partial class twowayanovatable : form{ public twowayanovatable() { initializecomponent(); } private static readonly char[] separators = { ',', ' ' };private static double _atreatmentsumofsquares;private static double _btreatmentsumofsquares;private static double _interactionsumofsquares; private static double _errorsumofsquares; private static double _sumofsquares; private static double _ameantreatmentsumofsquares; private static double _bmeantreatmentsumofsquares; private static double _interactionmeansumofsquares; private static double _meanerrorsumofsquares; private static double _atreatmentdegreesoffreedom; private static double _btreatmentdegreesoffreedom; private static double _interactiondegreesoffreedom; private static double _errordegreesoffreedom; private static double _totaldegreesoffreedom; private static double _ateststatistic; private static double _bteststatistic; private static double _interactionteststatistic; private static double _apvalue; private static double _bpvalue; private static double _interactionpvalue; private static void processfile() { var lines = file.readlines(data.csv); var numbers = processrawnumbers(lines); var rowtotal = new list<double>(); var squarerowtotal = new list<double>(); var rowmean = new list<double>(); var totalelements = 0; var totalinrow = new list<int>(); var rowtotalsquarebyn = new list<double>(); var sumofsquareofblock = new list<double>(); foreach (var values in numbers) { var sumofrow = values.sum(); rowtotal.add(sumofrow); squarerowtotal.add(values.select(v => v * v).sum()); rowmean.add(sumofrow / values.count); totalinrow.add(values.count); totalelements += values.count; rowtotalsquarebyn.add(rowtotal.select(r => r * r / values.count).sum()); sumofsquareofblock = squarerowtotal - rowtotalsquarebyn; } var grandtotal = rowtotal.sum(); } int anum = 3, bnum = 3; double[] totalsumperblock = new double[bnum]; double[] totalsumofsquaresperblock = new double[bnum]; int[] blocktotalelements = new int[bnum]; double[] totalsquareperblockbyn = new double[bnum]; double[] blockmean = new double[bnum]; double[] sumofsquarestotalofblock = new double[bnum]; for (int i = 0; i < bnum; i++) { for (int j = 0; j < anum; j++) { totalsumperblock[i] += rowtotal[j + i * 3]; totalsumofsquaresperblock[i] += squarerowtotal[j + i * 3]; blocktotalelements[i] += totalinrow[j + i * 3]; sumofsquarestotalofblock[i] += sumofsquareofblock[j + i * 3]; } totalsquareperblockbyn[i] += totalsumperblock[i] * totalsumperblock[i] / blocktotalelements[i]; blockmean[i] = totalsumperblock[i] / blocktotalelements[i]; } double[] grandtotalallblocks = new double[bnum]; double[] grandblocksumofsquares = new double[bnum]; int[] grandnumberofelements = new int[bnum]; double[] grandsumofsquares = new double[bnum]; double[] grandblocksquaressumbyn = new double[bnum]; double[] grandblockmean = new double[bnum]; double finalsum = 0; double finalsumofsquaresrow = 0; int finalelements = 0; double finalsumofsquaresbyn = 0; double finalsumofsquares = 0; double finalmean = 0; for (int i = 0; i < bnum; i++) { for (int j = 0; j < anum; j++) { grandtotalallblocks[i] += rowtotal[i + 3 * j]; grandblocksumofsquares[i] += squarerowtotal[i + 3 * j]; grandnumberofelements[i] += totalinrow[i + 3 * j]; } grandblocksquaressumbyn[i] = grandtotalallblocks[i] * grandtotalallblocks[i] / grandnumberofelements[i]; grandblockmean[i] = grandtotalallblocks[i] / grandnumberofelements[i]; finalsum += grandtotalallblocks[i]; finalsumofsquaresrow += grandblocksumofsquares[i]; finalelements += grandnumberofelements[i]; finalsumofsquaresbyn = finalsum * finalsum / finalelements; finalsumofsquares = finalsumofsquaresrow - finalsumofsquaresbyn; finalmean = finalsum / finalelements; } for (int i = 0; i < numbers.count; i++) { _errorsumofsquares += sumofsquareofblock[i]; _interactionsumofsquares += rowtotalsquarebyn[i]; } for (int i = 0; i < bnum; i++) { _atreatmentsumofsquares += totalsquareperblockbyn[i]; _btreatmentsumofsquares += grandblocksquaressumbyn[i]; _interactionsumofsquares = _interactionsumofsquares - totalsquareperblockbyn[i] - grandblocksquaressumbyn[i]; } _interactionsumofsquares = (-1) * (_atreatmentsumofsquares - _btreatmentsumofsquares) + finalsumofsquaresbyn; _atreatmentsumofsquares -= finalsumofsquaresbyn; _btreatmentsumofsquares -= finalsumofsquaresbyn; _sumofsquares = _errorsumofsquares + _btreatmentsumofsquares + _interactionsumofsquares + _atreatmentsumofsquares; _atreatmentdegreesoffreedom = anum - 1; _btreatmentdegreesoffreedom = bnum - 1; _interactiondegreesoffreedom = _atreatmentdegreesoffreedom * _btreatmentdegreesoffreedom; _errordegreesoffreedom = (totalelements - 1) - _atreatmentdegreesoffreedom - _btreatmentdegreesoffreedom - _interactiondegreesoffreedom; _totaldegreesoffreedom = totalelements-1; _ameantreatmentsumofsquares = _atreatmentsumofsquares / _atreatmentdegreesoffreedom; _bmeantreatmentsumofsquares = _btreatmentsumofsquares / _btreatmentdegreesoffreedom; _interactionmeansumofsquares = _interactionsumofsquares / _interactiondegreesoffreedom; _meanerrorsumofsquares = _errorsumofsquares / _errordegreesoffreedom; _ateststatistic = twowayanovaclass.calculateteststatistic(_ameantreatmentsumofsquares,_meanerrorsumofsquares); _bteststatistic = twowayanovaclass.calculateteststatistic(_bmeantreatmentsumofsquares, _meanerrorsumofsquares); _interactionteststatistic = twowayanovaclass.calculateteststatistic(_interactionmeansumofsquares, _meanerrorsumofsquares); _apvalue = twowayanovaclass.calculatepvalue(_ateststatistic, _atreatmentdegreesoffreedom, _errordegreesoffreedom); _bpvalue = twowayanovaclass.calculatepvalue(_bteststatistic, _btreatmentdegreesoffreedom, _errordegreesoffreedom); _interactionpvalue = twowayanovaclass.calculatepvalue(_interactionteststatistic, _interactiondegreesoffreedom, _errordegreesoffreedom); tss = _atreatmentsumofsquares.tostring(); ess = _errorsumofsquares.tostring(); bss = _btreatmentsumofsquares.tostring(); iss = _interactionsumofsquares.tostring(); totss = _sumofsquares.tostring(); tdf = _atreatmentdegreesoffreedom.tostring(); bdf = _btreatmentdegreesoffreedom.tostring(); idf = _interactiondegreesoffreedom.tostring(); edf = _errordegreesoffreedom.tostring(); totdf = _totaldegreesoffreedom.tostring(); tms = _ameantreatmentsumofsquares.tostring(); bms = _bmeantreatmentsumofsquares.tostring(); ims = _interactionmeansumofsquares.tostring(); ems = _meanerrorsumofsquares.tostring(); ft = _ateststatistic.tostring(); fbk = _bteststatistic.tostring(); fin = _interactionteststatistic.tostring(); pt = _apvalue.tostring(); pbl = _bpvalue.tostring(); pi = _interactionpvalue.tostring(); } private void button2_click(object sender, eventargs e) { readfile(); display(); } private void display() { textboxtss.text = tss; textboxess.text = ess; textboxiss.text = iss; textboxbss.text = bss; textboxtotss.text = totss; textboxtdf.text = tdf; textboxedf.text = edf; textboxidf.text = idf; textboxbdf.text = bdf; textboxtotdf.text = totdf; textboxtms.text = tms; textboxems.text = ems; textboxbms.text = bms; textboxims.text = ims; textboxft.text = ft; textboxfb.text = fbk; textboxfi.text = fin; textboxpt.text = pt; textboxpb.text = pbl; textboxpi.text = pi; }}}",
    "present_kp": [],
    "absent_kp": [
      "c#",
      "performance",
      ".net",
      "statistics"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how do the internals of sudo work?. how does sudo work internally? how is it possible that it can become root without having the root password, unlike su? what syscalls, etc. are involved in the process? is it not a gaping security hole in linux (e.g. why couldn't i compile a heavily-patched sudo that just did whatever regular sudo did, but didn't ask for the unprivileged user's password)?i have read login and su internals. i have also read how is sudo intended to be used? but despite the title, they mainly deal with the differences between su and sudo.",
    "present_kp": [
      "sudo",
      "root"
    ],
    "absent_kp": [
      "privileges"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "moving to new domain. one my site has been hit by penguin penalty. without waiting for next penguin release i want to move my site to another domain. using 301 for pages could pass the penalty back to the new domain. i've done my best already by disavowing links.so what is the best way to divert traffic to my site without letting google know?i could ask visitors to click on a nofollow link for the moved page. or i could use javascript to set the link target instead of using href in <a tag. but google can figure this out.or i can use use a form submit button to be clicked to go the moved page.so what is the best way in this situation?",
    "present_kp": [],
    "absent_kp": [
      "google penguin algorithm"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "mvc widget optimization when accessing css and resources. so we're trying to re-imagine our web solution in an mvc fashion. going from an old webforms based solution to working with asp.net mvc with a bootstrap main menu and adding functionality in the forms of widgets using html.action() that calls a controller and action to fill in the information on that part of the page.we're now thinking of how the css and resources that we are going to be getting for each widget separately in their own controller can be effectively used in this scenario.if we only use html.action and letting a controller add our functionality to the page, we lose our connection to the page as a whole, and we could possibly load the resource package and css for the same type of objects over and over again. how would one solve this and make it possible for our web portal to have knowledge over which css and resource files have already been added to the solution?",
    "present_kp": [
      "asp.net",
      "mvc",
      "css",
      "resources"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "a function is lambda-2-definable iff it is hg computable and provably type correct in lambda-pred2. i'm having a problem regarding theorem 5.4.40.3 of barendregt's lambda calculi with types (1992), a chapter in handbook in logic in computer science. (i'm referring to the postscript version available on the web.)the theorem states that a function is $\\lambda$2-definable iff it is hg (herbrand-goedel) computable and provably type correct in $\\lambda$pred2. the author delegates the proof to a proceeding in focs, i.e., reasoning about functional programs and complexity associated with type disciplines (1983) by leivant. the corresponding theorem in the proceeding is theorem 3.1 and corollary 3.2, although they are stated in terms of provable type-correctness in $\\lambda$2, not in $\\lambda$pred2.the first problem i'm running into is that the definition of hg computability by barendregt doesn't make sense. imho, the equation in definition 5.4.39.1 should be replaced by$$\\mathit{hg} dash p:(f_n(t^\\sim) =_l (f(t))^\\sim).$$assuming this, i tried to prove the equivalent of theorem 3.1 (leivant) in terms of $\\lambda$pred2, but in vain. leivant's proof is informal and i have hard times formalizing it. (by using barendregt's notation, i'm pretty sure that if $b$ is the proof of $f$'s type correctness then $[b]$ is a $\\lambda$2-term that represents $f$, but i am unable to prove this.) besides, his proof deals with only one direction of the equivalence (provably type correct $\\rightarrow$ $\\lambda$2-definable) and the rest is left unstated. i suppose the opposite direction involves the formalization of sn, but i am unsure.i would be the most grateful for detailed explanation of theorem 5.4.40.3 by barendregt.",
    "present_kp": [
      "computability"
    ],
    "absent_kp": [
      "lo.logic",
      "pl.programming languages",
      "type theory",
      "lambda calculus"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "converting this nfa to turing machine. i'm asked to choose a dfa and convert it to nfa and then convert it to turing machine... i have done the first two parts as follows: dfa:--> nfa:--> turing machine:???i haven't found any explanation/tutorial on how to do this. so, a final solution containing a state diagram with an explanation on what steps were followed would be great.",
    "present_kp": [],
    "absent_kp": [
      "turing machines",
      "finite automata",
      "nondeterminism",
      "simulation"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "c itoa implementation. as an exercise, i limited my self to the man pages. i didn't look for a previous existing implementation because i wanted to see how well i could do on my own. i also tried to handle as many cases as i could think of. i am sure there are weaknesses so feel free to point them out. where can i improve my usage of the c standard library, etc.char *itoa(int i){ short digit_cnt; short index; void *ret; char digit; int tmp; errno = 0; feclearexcept(fe_all_except); if (i == 0) { digit_cnt = 1; } else { tmp = (i == int_min) ? (i + 1) : i; digit_cnt = floor(log10(abs(tmp))) + 1; if (errno || fetestexcept(fe_invalid | fe_divbyzero | fe_overflow | fe_underflow)) { return null; } } if (i < 0) { ++digit_cnt; } ++digit_cnt; // '' errno = 0; if ((ret = malloc(digit_cnt * sizeof(char))) == null || errno) { print_error_msg(errno); free(ret); return null; } // this made debugging easier. memset(ret, '0', digit_cnt * sizeof(char)); index = digit_cnt; ((char*)ret)[--index] = ''; tmp = i; do { digit = (char)((int)'0' + abs(tmp % 10)); ((char*)ret)[--index] = digit; tmp /= 10; } while (tmp != 0); if (i < 0) { ((char*)ret)[--index] = '-'; } return ret;}",
    "present_kp": [
      "c"
    ],
    "absent_kp": [
      "programming challenge",
      "reinventing the wheel"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how do i add functionality to an existing binary executable?. i want to add some functionality to an existing binary file. the binary file was created using gcc. do i need to decompile the binary first, even though i sufficiently understand the functioning of the program ? how should i go about adding the necessary code ?do i need any tools to be able to do this ?",
    "present_kp": [
      "c",
      "executable"
    ],
    "absent_kp": [
      "linux",
      "hll mapping"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "opposite of mutating. i am looking for an adjective; one that describes the opposite of mutating. i want to use it to mark functions, methods and method parameters (including the receiver) as not being mutated by the function, similar how the d language does it.ds approach works pretty well for parameters:class x { void foo(const int x, immutable char[] y) { // x is readonly // y is readonly and can be trusted not to change }}however, both immutable and const seem ugly when using them on the function/method itself to denote that the function does not modify the receiver object:// again dclass x { private int x; void foo() const { // x becomes readonly because the method is marked s such } void foo2() immutable { // same as foo(): x is readonly }}i am looking for a keyword with the same semantics as the above keywords in d; but as an adjective to be written before the function name:class x { private x: int non-mutating foo() { // x is readonly }}using immutable reads like the method would be immutable (such as final in java or non-virtual in c#/c++). i cannot use the keyword after the parenthesis closing the parameter list because thats where the return type is supposed to go:fun x() -> inti'm not entirely sure whether this is the correct place to ask this question. but since i did not find a programming language design se, i figuerd the people knowing many programming languages in this community could give me a few pointers :)",
    "present_kp": [
      "programming languages"
    ],
    "absent_kp": [
      "terminology"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "variable assignment in .bash_profile has no effect. i added the following to my .bash_profile:java_home=$java_home:/usr/java/jdk1.7.0export java_homehowever, when i echo $java_home it outputs an empty string. why weren't my changes to .bash_profile reflected in the shell?",
    "present_kp": [
      "bash",
      "profile"
    ],
    "absent_kp": [
      "environment variables"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "does the link we provide on various networking websites counts as a backlink?. does the blog/website address we add on various sites like facebook count as a backlink in the eyes of search engines, especially google?",
    "present_kp": [
      "google",
      "facebook"
    ],
    "absent_kp": [
      "seo",
      "backlinks"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "rice's theorem for non-semantic properties. rice's theorem tell us that the only semantic properties of turing machines (i.e. the properties of the function computed by the machine) that we can decide are the two trivial properties (i.e. always true and always false).but there are other properties of turing machines that are not decidable. for example, the property that there is an unreachable state in a given turing machine is undecidable$^{\\dagger}$.is there a similar theorem to rice's theorem that categorizes the decidability of similar properties? i don't have a precise definition. any known theorem that covers the example i have given would be interesting for me.$^\\dagger$ it is easy to prove that this set is undecidable using kleene's recursion/fixed point theorems.",
    "present_kp": [],
    "absent_kp": [
      "computability",
      "undecidability"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to activate the password prompt on a lock screen?. i am using linux mint 17 cinnamon. when i lock the screen it shows a dimmed version of my desktop background and the time. so when i want to unlock it, i usually move the mouse and the prompt for entering the password appears. however, sometimes it appears rather slowly and today it didn't appear at all. i saw my mouse moving, but no matter how much i moved the mouse, clicked and typed the password prompt wouldn't appear. i thought this might have to do with the fact that i use an external screen with my laptop lid closed, but after i opened the lid nothing changed and i was still not able to unlock it. finally, i restarted. so my questions: is this is a known issue with mint and is there a particular way to call the password prompt?",
    "present_kp": [
      "linux mint",
      "password"
    ],
    "absent_kp": [
      "screen lock"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why it converts to t. the and function in the lambda is:and = ( a. b. a b f)i have following expression:and t tthen the result will become t and i am asking me why? when i do the beta reduction, then i understand, why i've got t:( a . b . a b ( x . y . y)) ( x . y . x) ( x . y . x)( b . a b ( x . y . y))[a ( x . y . x)] ( x . y . x)( b . ( x . y . x) b ( x . y . y)) ( x . y . x)(( x . y . x) b ( x . y . y))[b( x . y . x)]( x . y . x) ( x . y . x) ( x . y . y)( y . x)[x ( x . y . x)] ( x . y . y)( y . ( x . y . x)) ( x . y . y)( x . y . x)[y( x . y . y)]( x . y . x)tbut when i shortened it like:t t fthen i've got f, because:t t ft ffwhat am i interpreting wrong?",
    "present_kp": [],
    "absent_kp": [
      "lambda calculus"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "using delegates to communicate between forms and networkcommunicator class. i have multiple forms, each of which would like to send message through the network, using my networkcommunicator class's sendmessage method.the solution i described here works, however it is ugly, and bludgeons some of the main concepts of oop paradigm. what i'm looking for (and i'm open to any suggestions) is basically to achieve the same goal but in a more elegant, and oop conforming solution. (maybe by events?)i'm using more than these forms, but for the example, these seemed to be enough.//using statementsnamespace examplespace{ public delegate void improvedsenderdelegate(string message); public class loginform :form { private networkcommunicator ncom = new networkcommunicator(); public ncom { get { return ncom; } } private void login() { mainform mf = new mainform(); mf.owner = this; //other things i do etc. this.hide(); mf.show(); } }//endofloginform public class mainform :form { public improvedsenderdelegate isd; private void load () { (this.owner as loginform).ncom.subscribetosendmessage(this); } private void i_want_to_send_a_message(string message) { isd(string); } }//endofmainform public class networkcommunicator { public void subscribetosendmessage(object sender) { if (sender is mainform) { (sender as mainform).isd += new improvedsenderdelegate(sendmessage); } } private void sendmessage(string message) {/*magic*/} }/endofnetworkcommunicator}",
    "present_kp": [
      "delegates"
    ],
    "absent_kp": [
      "c#",
      ".net"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why we prefer vif if we can find multicollinearity from correlation matrix as well?. why we prefer vif if we can find multicollinearity from correlation matrix as well? what is the exact logic behind it.thanks for the help.",
    "present_kp": [],
    "absent_kp": [
      "predictive modeling",
      "regression"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "where do i access the book once i register on kindle?. if i buy a book on kindle how do i access the book is this done by going into my amazon account and access the book directly?",
    "present_kp": [
      "amazon",
      "kindle"
    ],
    "absent_kp": [
      "kindle cloud reader"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "scalar by n component vector multiplication faster than o(n)?. is there a way to multiply scalar by vector faster than just multiplying each element of the vector by that scalar?it feels to me that there should be some exploit to do that. after all we will multiply two vectors elementwise in n steps. the original problem is simpler as it's only scalar by vector. shouldn't we be able to use the sparsness in our advantage?",
    "present_kp": [
      "multiplication"
    ],
    "absent_kp": [
      "time complexity",
      "optimization"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why do bloom filters work?. let's say i am using bloom filters to create a function to check if a word exists in a document or not. if i pick a hash function to fill out a bit bucket for all words in my document. then if for a given number of words, wouldn't the whole bit bucket be all 1s? if so then checking for any word will return true? what am i missing here?",
    "present_kp": [
      "bloom filters"
    ],
    "absent_kp": [
      "data structures",
      "probabilistic algorithms",
      "dictionaries"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "google blink (new webkit fork): meaning of moving dom into javascript?. from the blink blog:finally wed like to explore even larger ideas like moving the entire document object model (dom) into javascript.what does this mean? does it mean webkit's dom is currently not coded in javascript but in some other language? does it mean that they want to expose more public accessors to the dom? or what?",
    "present_kp": [
      "javascript",
      "dom"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "can i get header numbering in google docs?. is there a way to automatically number headings in google docs?it used to be possible with css but this feature isn't supported anymore in the new google docs version.",
    "present_kp": [],
    "absent_kp": [
      "google documents"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "where can i find good link exchange programs and forums?. googling didnt worked in finding good link exchange programs.has anyone of you tried an automatic link exchange program?my sites page rank has fallen from 3 to 1.please help",
    "present_kp": [],
    "absent_kp": [
      "links",
      "backlinks"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "designing a dfa binary string with exactly 2 occurrences of 010?. hi i'm having difficulty in designing a dfa binary string with exactly 2 occurrences of 010?",
    "present_kp": [],
    "absent_kp": [
      "finite automata"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why is formatting a usb as fat32 quicker than formatting as ext3?. i often use gparted to format my usbs. whenever i format a usb as fat32, it takes just a few seconds to complete. whenever i format a usb as ext3, it takes a few minutes to complete. why is there such a large difference in the speeds of formatting the usb in these different formats?",
    "present_kp": [
      "gparted",
      "ext3",
      "fat32"
    ],
    "absent_kp": [
      "filesystems"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "methods for silhouette feature extraction. let's suppose i have an image of a person and its corresponding background. using opencv i can relatively easily get a silhouette of the person from the image. i am doing a project on human recognition using silhouettes and the only things i came up with for feature extraction are so called granlund coefficients derived from fourier coefficients and hu moments which provide me with features i can send to various classifiers i have. what i am further curious about is what other methods are there, if any, for feature extraction from silhouettes? edit: in accordance with the first commentator, i will expand my question. as i said, i have tried with granlund coefficients and hu moments, both of them are well documented standard techniques which you can find in opencv documentation and ieee xplore digital library (granlund coefficients). also, i didn't say anything specific about the features, i just want to know about various methods for silhouette feature extraction, other than the two mentioned above.",
    "present_kp": [],
    "absent_kp": [
      "reference request",
      "pattern recognition"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "correct form of the ggx geometry term. i'm trying to implement a microfacet brdf in my raytracer but i'm running into some issues. a lot of the papers and articles i've read define the partial geometry term as a function of the view and half vectors: g1(v, h). however, when implementing this i got the following result: (bottom row is dielectric with roughness 1.0 - 0.0, top row is metallic with roughness 1.0 - 0.0)there's a weird highlight around the edges and a cut-off around n.l == 0. i couldn't really figure out where this comes from. i'm using unity as a reference to check my renders so i checked their shader source to see what they use and from what i can tell their geometry term is not parametrized by the half vector at all! so i tried the same code but used to macro surface normal instead of the half vector and got the following result:to my untrained eye this seems way closer to the desired result. but i have the feeling this is not correct? the majority of the articles i read use the half vector but not all of them. is there a reason for this difference?i use the following code as my geometry term:float raytracer::geometryggx(const vector3& v, const vector3& l, const vector3& n, const vector3& h, float a){ return g1ggx(v, h, a) * g1ggx(l, h, a);}float raytracer::g1ggx(const vector3& v, const vector3& h, float a){ float nov = util::clamp01(cml::dot(v, h)); float a2 = a * a; return (2.0f * nov) / std::max(nov + sqrt(a2 + (1.0f - a2) * nov * nov), 1e-7f);}and for reference, this is my normal distribution function:float raytracer::distributionggx(const vector3& n, const vector3& h, float alpha){ float alpha2 = alpha * alpha; float noh = util::clamp01(cml::dot(n, h)); float denom = (noh * noh * (alpha2 - 1.0f)) + 1.0f; return alpha2 / std::max((float)pi * denom * denom, 1e-7f);}",
    "present_kp": [
      "shader",
      "brdf",
      "microfacet"
    ],
    "absent_kp": [
      "pbr"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how can i show a terminal shell's process tree including children?. when a script is launched from command prompt the shell will spawn a subprocess for that script. i want to show that relationship between terminal level process and its children using ps in a tree style output.how can i do this?what i have tried so farfile: script.sh#!/bin/bashps -f -p$1then i invoke the script from the command line passing in the process id of the terminal shell:$ ./script.sh $$what i want is something like thistop level (terminal) shell process./script.shprocess for ps command itself user pid [..]ubuntu 123 -bashubuntu 1234 \\_ bash ./script.shubuntu 12345 \\_ ps auxf what im getting is: pid tty stat time command14492 pts/24 ss 0:00 -bash",
    "present_kp": [
      "shell",
      "ps"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "unexpected processor/core counts for amd processor. i wanted to determine if a machine has hyperthreading enabled or not. to determine this, i used advice i found online, which operates as follows:physical_proc_count = 'grep physical id /proc/cpuinfo | sort -u | wc -l'logical_proc_count = 'grep processor /proc/cpuinfo | wc -l'core_count = 'grep core id /proc/cpuinfo | sort -u | wc -l'the idea is that if the number of logical processors is twice the number of cores, hyperthreading is enabled.in my naivete at the time, i was oblivious to the fact that amd processors don't have hyperthreading. i knew that they don't have the exact technology called hyper-threading, but i mistakenly believed that they had a functional equivalent. so i ran the script on a machine with four amd opteron processor 6276, and the output was:4 physical cpus64 logical cpus8 cores per physical cpu8 cores per physical cpu == 32 cores. yet there are 64 logical cpus. therefore i concluded that the machine had hyperthreading enabled. this was further compounded by the fact that the processor flags in /proc/cpuinfo include the ht flag. which i have since learned stands for hypertransport in the case of amd chips --- doh!a co-worker noticed my mistake and stepped in and politely informed me that amd processors don't have hyperthreading. i still don't understand why i got the above numbers that i did.checking the tech specs of the processor, it says there are actually 16 cores per physical cpu: <url> the output looks to me like there is hyperthreading. where did i go wrong? how can i parse /proc/cpuinfo to get the true counts for all amd and intel (ht and non-ht) chips?",
    "present_kp": [
      "cpu",
      "amd",
      "hyperthreading"
    ],
    "absent_kp": [
      "linux",
      "x86"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "are arithmetic circuits weaker than boolean?. let $a(f)$ denote the minimum size of a (non-monotone) arithmetic $(+, imes,-)$ circuitcomputing a given multilinear polynomial$$ f(x_1,\\ldots,x_n)=\\sum_{e\\in e}c_e\\prod_{i=1}^n x_i^{e_i}\\,,$$ and $b(f)$ denote the minimum size of a (non-monotone) boolean $(\\lor,\\land, eg)$ circuitcomputing the boolean version $f_b$ of $f$ defined by:$$ f_b(x_1,\\ldots,x_n)=igvee_{e\\in e}\\ igwedge_{i\\colon e_i eq 0} x_i\\,.$$are polynomials $f$ known for which $b(f)$ is smaller than $a(f)$?if we consider monotone versions of circuits -- no minus $(-)$ and no not $( eg)$ gates -- then $b(f)$ can be even exponentially smaller than $a(f)$: take, for example, the shortest s-t path polynomial $f$ on $k_n$;then $b(f)=o(n^3)$ and $a(f)=2^{\\omega(n)}$. but what happens in the non-monotone world? of course, big gaps cannot be known just because we do not have large lower bounds on $a(f)$. but perhaps there are at least some small gaps known? note (15.03.2016) in my question, i do not specified how large coefficients $c_e$ are allowed. igor sergeev remembered me that, for example, the following (univariate) polynomial $f(z)=\\sum_{j=1}^m 2^{2^{jm}} z^j$ has $a(f)=\\omega(m^{1/2})$ (strassen and people of his group). but $b(f)=0$ for this polynomial, since $f_b(z)=z$. we can obtain fron $f$ a multivariate polynomial $f'(x_1,\\ldots,x_n)$of $n=\\log m$ variables using using kronecker substitution. associate with every exponent $j$ a monomial $x_j=\\prod_{i:a_i=1}x_i$, where $(a_1,\\ldots,a_n)$ are the 0-1 coefficients of the binary representation of $j$. then the desired polynomialis $f'=\\sum_{j=1}^m c_j x_j$, and we have that $$a(f')+n\\geq a(f)=\\omega(m^{1/2})=2^{\\omega(n)}.$$ but the boolean version of $f'$ is just an or of variables, so $b(f')\\leq n-1$, and we have an even exponential gap. thus, if magnitude of coefficients can be triple-exponential in the number $n$ of variables then the gap $a(f)/b(f)$ can be shown to be even exponential. (actually, not the magnitude itself -- more the algebraic dependence of the coefficients.)this is why the real problem with $a(f)$ is the case of small coefficients (ideally, only 0-1). but in this case, as joshua recalled, the lower bound $a(f)=\\omega(n\\log n)$ of strassen and baur (with 0-1 coefficients) remains the best what we have today.",
    "present_kp": [
      "lower bounds",
      "arithmetic circuits"
    ],
    "absent_kp": [
      "cc.complexity theory",
      "circuit complexity"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to make text in blogger non-clickable?. i have recently started using blogger to write blogs. i wanted to make my text non clickable in blogger so nobody can copy that material from my blog.i add the material using a compose button. its more or less similar to writing in microsoft word. blogger automatically generate html code of it.i think that their should be a script that i should insert in the html code that will make my code non clickable. but i don't know much about html. what should i do to achieve this task?",
    "present_kp": [
      "blogger"
    ],
    "absent_kp": [
      "blogger themes"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "dealing with gnu stow conflicts. what is the recommended way of dealing with gnu stow conflicts? i tried to stow readline-6.2 and got the following warning:> stow readline-6.2loading defaults from /home/josh/.stowrcwarning! stowing readline-6.2 would cause conflicts: * existing target is stowed to a different package: share/info/dir => ../../../stow_dir/stow_2.2.0_canonical_paths/share/info/dirall operations aborted.the clash is with the package stow_2.2.0_canonical_paths which is the package dir for stow (this is because i bootstrapped stow).here is is what that conflicting target contains:> cat ~/local/share/info/dir this is the file .../info/dir, which contains thetopmost node of the info hierarchy, called (dir)top.the first time you invoke info you start off looking at this node.file: dir, node: top this is the top of the info tree this (the directory node) gives a menu of major topics. typing q exits, ? lists all info commands, d returns here, h gives a primer for first-timers, memacs<return> visits the emacs manual, etc. in emacs, you can click mouse button 2 on a menu item or cross reference to select it.* menu:system administration* stow: (stow). gnu stow.what would be the recommended way of resolving this conflict?",
    "present_kp": [
      "readline",
      "stow"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "changing a numerical value in a file with vim from a script. i need to change the numerical value of a variable stored in a text file from the command line.i've tried to script it with bash, but only got so far:#!/bin/bashclearvim the_generator.c(this is hopefully where i modify the the_generator.c file).-c 'wq'",
    "present_kp": [
      "vim"
    ],
    "absent_kp": [
      "shell script",
      "text processing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why was ffmpeg removed from debian?. currently, ffmpeg is missing from apt packages when using the stable versions of debian and ubuntu.there are numerous resources (example from superuser, another one from debian's documentation and the one from askubuntu) which explain how to install it in a different (and more complex) way than a simple apt-get install ffmpeg.what i wonder is why the package is not there in the first place?from what i understood, avconv is a fork of ffmpeg and is a de facto standard for debian similar distributions. meanwhile, ffmpeg is not abandoned: the website mentions no intention to close the project in profit of avconv, despite the fact that the leader of ffmpeg left the project.so:why ffmpeg was plainly removed from apt packages, instead of keeping it and simply adding avconv?is there a reason (other than the fact that it became more difficult to install ffmpeg) to stop using it?",
    "present_kp": [
      "debian",
      "ffmpeg"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "low coupling processing big quantities of data. usually i achieve low coupling by creating classes that exchange lists, sets, and maps between them. now i am developing a java batch application and i can't put all the data inside a data structure because there isn't enough memory. i have to read and process one chunk of data and then going to the next one. so having low coupling is much more difficult because i have to check somewhere if there is still data to read, etc.what i am using now is:source -> process -> persistthe classes that process have to ask to the source classes if there are more rows to read.what are the best practices and or useful patterns in such situations?i hope i am explaining myself, if not tell me.",
    "present_kp": [
      "java",
      "coupling"
    ],
    "absent_kp": [
      "design patterns",
      "programming practices",
      "patterns and practices"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how can i change the default usage rights settings of the google images search?. i would like the default setting to be labeled for reuse. how can i make this setting apply permanently?",
    "present_kp": [],
    "absent_kp": [
      "google image search"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why is the code section called a text section?. the section of an executable that contains code is sometimes called the .text section. in segmented memory architectures, a segment mapped as code is sometimes called a text segment. the unix error message text file busy (etxtbsy) means this file is a program that is being executed.how did text come to mean executable (machine) code? an ideal answer would:explain the connection between the word and its meaning;provide a citation for the origin or at least the history of the term;give some idea of which communities use it.",
    "present_kp": [
      "history"
    ],
    "absent_kp": [
      "terminology",
      "file structure",
      "linking"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "quantum algorithms based on transforms other than fourier transforms. in quantum computation and quantum information by nielsen and chuang they say that many of the algorithms based on quantum fourier transforms rely on the coset invariance property of fourier transforms and suggests that invariance properties of other transforms might yield new algorithms.has there been any fruitful research on other transforms?",
    "present_kp": [
      "quantum information"
    ],
    "absent_kp": [
      "quantum computing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what kind of users stories should be written in the initial stages of a project?. when just starting a project, you have nothing---no ui, no data layer, nothing in between. thus, a single story like users should be able to view their foos will entail a lot of work. once you have that story, one like users should be able to edit their foos is more realistic, but that first story will involve setting up a ui layer, a presentation logic layer, a domain logic layer, and a data access layer.this doesn't fit with my concept of tasks: to me, i'd rather have something like the following tasks:show dummy data for a user's foos in html, derived from javascript objects.set up a presentation logic layer, and connect the javascript objects to it.set up a domain logic layer, and connect the presentation logic layer to it.set up a data access layer, and connection the domain logic layer to it.do all of these fall under the single story above? if so, i feel like stories are not a terribly useful framework in the early stages of a project. if so, that's fine---i just want to make sure i'm not missing something, since i'm really trying to learn this agile methodology as best i can.",
    "present_kp": [
      "agile"
    ],
    "absent_kp": [
      "project management"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "efficiently compute a complete set of representatives of cyclotomic cosets of $2$ modulo $2^n-1$. let $n > 1$. for an integer $k$ with $0 \\leq k \\leq 2^n-2$ the cyclotomic class $c_k$ of $2$ modulo $2^n-1$ is the set given by $$c_k = \\{k, k\\cdot 2, k \\cdot 2^2 , \\ldots, k\\cdot 2^{n-1} \\pmod {2^n-1}\\},$$where the entries are reduced modulo $2^n-1$. note that either $c_k = c_i$ or their intersection is empty. a set $\\{a_1, \\ldots, a_m\\}$ of integers in $[0, 2^n-1]$ is called a complete set of representatives of the classes if $c_{a_j} eq c_{a_i}$ whenever $i eq j$ and $$igcup_{i=1}^m c_{a_i} = \\{0, 1, \\ldots, 2^n-1\\}. $$i am interested in, given $n$, efficiently computing the corresponding complete set of representatives of the cyclotomic classes. any ideas? i'm hoping to be able to compute this for large enough $n$. thanks!",
    "present_kp": [],
    "absent_kp": [
      "efficiency"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "google analytics content drilldown displays folders which i do not have in my domain. assume that i have the following folders in the root of my domain: programming, codesnippets, wordreminder. i am using google analytics content drilldown to see the statistics.apart from the above three i can see folders which i do not have: /pinshop123/, /pinshop/, /scripts/, /pro/ and so on...why such folders appear? is any site using my site in iframe or javascript?",
    "present_kp": [
      "google analytics"
    ],
    "absent_kp": [
      "subdomain"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "access elements in string array - stray @ symbol. i have created an array (3 elements), and each element contains a comma delimited string.the array was created by reading a file line by line - the file contains fields output from a database.i have written the code below in order to iterate through each string element, as though it were too an array.it works, apart from one strange error, the last element in the array has an additional @ symbol(also declare -a didn't work, though i've made sure i'm using bash 4 on my mac)i=0declare -a itemswhile read line do items[i]=$line ((i++)) done < test_file1.txtdeclare -a item_arrfor item in ${items[@]} do item_arr=($item[@]) doneecho ${item_arr[4]}://${item_arr[1]}:${item_arr[3]}@${item_arr[2]}/control/configinfooutput is:https[@]://192.168.1.152:username@pwd/control/configinfowhy is the @ symbol printing out?am i wasting my time, should i have used awk instead? otherwise, it feels like a fairly straightforward way, but that could be as i'm relatively inexperienced. potentially the most items i may need to initialise in this would be 1 to 200.the purpose of my code is to create a curl request to obtain some config_info, the user name, password, ip address, protocol are all pulled from a database in order to build a curl request for each item.thanks in advance!",
    "present_kp": [
      "bash",
      "array"
    ],
    "absent_kp": [
      "gawk"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "should i use piping or redirection for input to 'sort'. i want to doawk -f , '{print $1 }' inputfile1.txt | sort -u > distinctoutput.txtis this good performance-wise ? or should i redirect / use a temp file since i think it is better to make sort wait till awk is done dumping the complete output? or is this taken care internally?(aix 6.1)",
    "present_kp": [
      "sort"
    ],
    "absent_kp": [
      "io redirection",
      "pipe"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "any concerns with using high uid numbers (3000+) on rhel5+?. the legacy systems has uid numbers up until the low 2000s. in implementing a new user management system, i am looking into options to avoid uid collisions.one option is to just have a list of uids that cannot be re-assigned. i am currently looking into that.another much easier option would be to use a higher range (3000+). any concerns i should keep in mind?this would be on rhel5, rhel6 and rhel7.",
    "present_kp": [
      "rhel"
    ],
    "absent_kp": [
      "users"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "iptables redirect outside requests to 127.0.0.1. i have a service running on 127.0.0.1 with port 2222. i need to forward all requests to 192.168.2.2:2222 (outside ip) only from subnet 192.168.1.0/24 to 127.0.0.1:2222.i'm trying to use this, but it's not working.$ iptables -t nat -i prerouting -p tcp -d 192.168.1.0/24 --dport 2222 -j dnat --to-destination 127.0.0.1:2222how can i get this to work?upd: edit address scheme.",
    "present_kp": [
      "iptables"
    ],
    "absent_kp": [
      "port forwarding",
      "network interface"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "shared source file between two parts of project. i have the following sort of directory structure:part1 build srcpart2 build srcnow i have a header file that i would like to include in both parts of these projects, it will contain constants and some macro definitions as well as some utility functions, and i want these to be synchronized between the two parts. what would be the generally acceptable way to format my directory structure?something like so?part1 build srcpart2 build srcshare srcand set up my make files in the build directories to also look in ../../share/srcor is there a more correct way of doing this?",
    "present_kp": [
      "c",
      "directory structure"
    ],
    "absent_kp": [
      "c++"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how can i buffer a pipe with minimal added latency?. i am connecting to a debian system over a serial connection and streaming real time data from a pressure sensor. originally i used:python3.6 pressure_streamer.pythe data is being produced at 32.1kb/s and it needs to be buffered in case the reader is busy. i can add a buffer with the pv command:python3.6 pressure_streamer.py | pv --quiet --buffer-size 10mwith pv the data appears bunched up into 8k writes in the steady state. this adds ~250ms of latency.is there a way to buffer which adds less latency?",
    "present_kp": [
      "python3",
      "pv"
    ],
    "absent_kp": [
      "shell"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "preferred form for error detection and loop termination conditions?. in this stackoverflow anwer i dimly recall being taught that it's better to use as wide a condition as possible to terminate a loop, rather than testing for an exact termination condition.i.e. use:while (x < 10) rather thanwhile (x != 10)however i cannot recall (or find) any formal basis or rationale for this.is this actually written down anywhere as best current practise?",
    "present_kp": [],
    "absent_kp": [
      "formal methods"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "deterministic cfl closure property homomorphism. i tried to research the following question with no results: can you find one example where the following holds true:let l = {xxxxxxx} be a deterministic-context-free language and let h(...) = xxxxx be a homomorphism.then the homomorphism on that language l h(l) ist not a deterministic-cfg anymore.",
    "present_kp": [],
    "absent_kp": [
      "reference request",
      "fl.formal languages",
      "context free languages"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "taking arbitrary length input in c. instead of having to use something like char buf[100500] and hope that no possible user input could be longer than 100500 bytes, i decided to make the following function:char* input (file* in, size_t size){ char * input_str = null; int c; size_t len = 0; /*initial allocation*/ input_str = malloc(size); if (!input_str) return null; while ((c = fgetc(in)) != eof && c != ' ') { input_str[len++] = c; /*allocate more room if needed*/ if (len == size) { input_str = realloc(input_str, size += 16); if (!input_str) return null; } } input_str[len++] = 0; return realloc(input_str, len);}my questions about this code:is leaving it up to the calling function to free memory a bad idea?is there a way to increase performance for this (memory pool, etc)? i don't like making so many syscalls...in general, is there a better way to go about this?",
    "present_kp": [
      "performance",
      "c",
      "io"
    ],
    "absent_kp": [
      "memory management"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how do i make myself an admin and delete old members/staff?. my seven-strong team and i are using trello through trello.com. unfortunately, several staff members have left in the past 12 months, including the one who set up the account, and none of us seem to have admin status nor know how to log in as one who does. how can i set one of us up as the new admin so we can go in and delete inactive members?",
    "present_kp": [
      "trello"
    ],
    "absent_kp": [
      "account management",
      "user accounts"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to open a local url (webpage) on the command line. on macos x i can runopen /some/path/index.htmland this would open the page index.html with the default software that handles .html files. is there something similar on ubuntu linux? i have used gnome-open in the past, but if there is no gnome installed, this command fails, of course.gnome-open /some/path/index.htmlis there a generic open this file with the default application on linux?",
    "present_kp": [
      "command line"
    ],
    "absent_kp": [
      "scripting"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "runtime of euclidean algorithm. given two $n$-bits numbers $a$ and $b$, i am not sure on how to find the runtime of the euclidean algorithm for finding the $\\gcd$ of $a,b$. the problem (for me) in here is that apart from the size of $a$ and $b$, i don't feel like i have any other information that will help me to know what is the runtime of the algorithm. i saw somewhere that the number of rounds is $\\log_2 a = n$, but, assuming this is correct, i have no intuition as for why it is such? my question is for an explanation to how much time it takes to calculate the $gcd$ (as a function of $n$)? is it linear in $n$?",
    "present_kp": [],
    "absent_kp": [
      "algorithm analysis",
      "runtime analysis",
      "number theory"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to think about things when it's the right time?. is there a good way to train that?to be able to pause thinking about something that you can't change right now, until it's actually time. that might be one day, one week or one month later.",
    "present_kp": [],
    "absent_kp": [
      "wandering mind"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how close is cdebconf able to serve as a drop-in replacement for debconf?. most of the internet information around cdebconf is circa 06', or i was just unable to find its location. please mention if recent documentation, or faq, is available. is cdebconf able to serve as a drop-in replacement for debconf? as of 06' it apparently was not, but it's been about 6 years from that time. i would assume the status has changes since then, but by how much?note: please refrain from answering these questions, plenty of available grounded opinions exist.why the community needs cdebconf?why debconf needs a replacement?",
    "present_kp": [],
    "absent_kp": [
      "ubuntu",
      "debian",
      "package management",
      "software installation"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "implementing a zipwithindex predicate in prolog. i want to implement a predicate of the form:zipwithindex(?list1, ?list2)which is true when the elements of list2 are the same as in list1 but are paired with the index of the element. this is what i have:zipwithindex([],i,s,s).zipwithindex([h|t],i,s0,s):- append(s0,[h/i],s1), i1 is i+1, zipwithindex(t,i1,s1,s).zipwithindex(list1,list2):- zipwithindex(list1,0,[],list2).is there a better way to implement this?",
    "present_kp": [
      "prolog"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how can i remove aliasing in an image without introducing blur?. i am trying to reduce aliasing in an image using some filters. i tried the gaussian variant of filters to remove the said high frequency patterns in the image, but i feel it's a bit too much of a blur. i need a filter that can help me do some kind of anti-aliasing without doing too much blur. i have searched a lot and unfortunately the results are not fruitful.",
    "present_kp": [
      "image"
    ],
    "absent_kp": [
      "image processing",
      "antialiasing",
      "filtering"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "sum of subset of 5 numbers equals 0. i have a task to print all zero subsets of 5 numbers, input from the console.i have succeeded in implementing a working code, but it seems it is quite complex (it is like inception) and if the count of the numbers is to be greater, it would be rather pointless to use such a method.using system;using system.collections.generic;using system.linq;class zerosubset{ static void main() { int number; int[] numbers = new int[5]; bool result = false; for (int i = 0;i <=4; i++) { here: ; console.writeline(input number {0}, i + 1); if (int.tryparse(console.readline(), out number)) { numbers[i] = number; } else { goto here; } } if (numbers[0] == 0 && numbers[1] == 0 && numbers[2] == 0 && numbers[3] == 0 && numbers[4] == 0) { result = true; console.writeline(string.join(+, numbers) + = 0); return; } for (int firstnum = 0; firstnum <= 3; firstnum++) { for (int secondnum = firstnum + 1; secondnum <= 4; secondnum++) { if (numbers[firstnum] + numbers[secondnum] == 0) { result = true; console.writeline({0} + {1} = 0, numbers[firstnum], numbers[secondnum]); } } } for (int firstnum = 0; firstnum <= 2; firstnum++) { for (int secondnum = firstnum + 1; secondnum <= 3; secondnum++) { for (int thirdnum = secondnum + 1; thirdnum <= 4; thirdnum++) { if (numbers[firstnum] + numbers[secondnum] + numbers[thirdnum]== 0) { result = true; console.writeline({0} + {1} + {2} = 0, numbers[firstnum], numbers[secondnum], numbers[thirdnum]); } } } } for (int firstnum = 0; firstnum <= 1; firstnum++) { for (int secondnum = firstnum + 1; secondnum <= 2; secondnum++) { for (int thirdnum = secondnum + 1; thirdnum <= 3; thirdnum++) { for (int fourthnum = thirdnum + 1; fourthnum <= 4; fourthnum++) { if (numbers[firstnum] + numbers[secondnum] + numbers[thirdnum] + numbers[fourthnum]== 0) { result = true; console.writeline({0} + {1} + {2} + {3} = 0, numbers[firstnum], numbers[secondnum] , numbers[thirdnum], numbers[fourthnum]); } } } } } if (numbers.sum() == 0) { result = true; console.writeline(string.join(+, numbers) + = 0); } if (result == false) { console.writeline(no zero subsets); } }}i am asking for a more simple approach to this matter.",
    "present_kp": [
      "console"
    ],
    "absent_kp": [
      "c#",
      "array"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is berkeley packet filter ported to linux?. i am doing some research to figure out what distro's of linux contain kernel packet filtering and are compatible with bpf. <url> two articles lead me to believe there is a package somewhere taht includes the libraries, and binaries?i am specifically looking for the pfctl command like i have in freebsdthanks",
    "present_kp": [
      "linux",
      "pf"
    ],
    "absent_kp": [
      "networking",
      "firewall"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "problem during installing kali linux. i want to install kali linux on my hard disk. i already have ubuntu/win7 dual boot. when i start installing kali linux everything goes right, choosing language, root's pass word ..., but after the system finished coping system's files i got a weird black screen saying system is going for halt (or something like that i can't remember well because it was too short).after that i lost the grub, i got an error saying file not found grub-rescue >. i retried but the same problem came up, so i installed ubuntu again and everything is fine right now. i would like to know the reason why i cant have kali linux on my hard drive though it works awesome in live mode.p.s i disabled wireless card from bios menu because the installation stops at detecting network interfaces step.",
    "present_kp": [
      "kali linux"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "implementation of inheritable pimpl idiom and mvc pattern. i am trying to write an object template that use pimpl and mvc pattern to encapsulate everything not specific to the object. the following is how i tried to attempt it. i am sure that the code smells (if not stinks) but can't tell what's wrong with the design.i would like to know what better alternatives with which i can do this.#include <iostream>using namespace std;class ancestor{}; //every object using the template inherits this class//use a template as the type of impl is only known at compile timetemplate <typename impl> class obj : public ancestor { public: //dependency injection for easy tdd obj(impl *impl = new impl()) : m_pimpl(impl) {}; ~obj() {delete m_pimpl; m_pimpl = nullptr;}; protected: impl *m_pimpl;};// well a template is used for the same reason as for obj<>template <typename model> struct mvccontroller { model *m_pmodel;};// same reason for a templatetemplate <typename model, typename controller> struct mvcimpl { model *m_pmodel; controller *m_pcontroller; // leave alone the view for simplicity //constructor mvcimpl(model *m = new model(), controller *c = new controller()) : m_pmodel(m), m_pcontroller(c) {m_pcontroller->m_pmodel = m;};};struct somebasemodel{};if i want to create an object called myobj with a public api void foo():struct mymodel : somebasemodel { // pod};struct mycontroller : public mvccontroller<mymodel> { void foo() {cout << mycontroller: foo << endl;};};struct myimpl : public mvcimpl<mymodel, mycontroller> {};class myobj : public obj<myimpl> { public: // inherit mybaseobj to call its default ctor/dtor if no customization is needed myobj(myimpl *impl = new myimpl()) : obj<myimpl>(impl) {}; void foo();};void myobj::foo() {m_pimpl->m_pcontroller->foo();};if i want myderivedobj to inherit myobj and add a void bar():struct myderivedcontroller : public mycontroller { void bar() {cout<<myderivedcontroller: bar<<endl;};};struct myderivedimpl : public mvcimpl<mymodel, myderivedcontroller> {};// virtual inheritance is used as both inherited class inherits from ancestorclass myderivedobj : virtual public myobj, virtual public obj<myderivedimpl> { using obj<myderivedimpl>::m_pimpl; public: myderivedobj(myderivedimpl *impl = new myderivedimpl()) : obj<myderivedimpl>(impl) {}; // add any new api here void bar();};void myderivedobj::bar() {m_pimpl->m_pcontroller->bar();};int main() { auto m2 = new myderivedobj(); m2->foo(); m2->bar(); return 0;}output:mycontroller: foomyderivedcontroller: barby using the above implementation i am trying tohide all pimpl and mvc specific code in the templateshide all generic object behaviors in ancestorleave the possibility for inheritance",
    "present_kp": [
      "mvc"
    ],
    "absent_kp": [
      "c++"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how can i get a list of all indexed pages for my domain?. we're in the process of migrating an outdated e-commerce cart to magento. part of this process will entail configuring proper 301 redirects. i'm aiming to get a list of all indexed pages to throw our solution against and resolve as many potential issues as possible before going live.ideally i'd just need a csv containing the uris indexed by the search engines for our domain.looking at a similar question here, it seems there is no simple way to export this data from either google or bing's webmaster tools, given that this cart has tens of thousands of products (and thus tens of thousands of indexed pages).i've run across a few other 3rd party utilities such as screaming frog and web-based ones like searchenginegenie and internetmarketingninjas, but i've never used any of them and am hesitant to start throwing additional traffic at our site unless i know we'll get what we need out of it.has anyone out there used these tools to do something similar, or found some way to retrieve more than the top 1000 records from gwt (or something similar from bing)?",
    "present_kp": [
      "301 redirect",
      "magento"
    ],
    "absent_kp": [
      "google search console",
      "indexing",
      "google index"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "programmatically detect the ansi escape code supported by terminal. i am playing around with shell scripts that use ansi codes and found that for one reason or another different escape codes are supported depending on your terminal/os.in some cases i get a dump of unparsed gunk unexpectedly, which i'm assuming means my terminal (on mac os) doesn't support that escape code used, despite having read in a number of places that these mean the same thing:27 = 033 = 0x1b = ^[ = \\ein searching i found this question about detecting slash-escaped support.the selected answer sniffs the $term value to detectcase $term in (|color(|?))(([ekx]|dt|(ai|n)x)term|rxvt|screen*)*) ps1=$'\\e\\]0;$generated_window_title'$ps1esacbut i wonder how reliable that is.is there a standard way to check for escape code support (primarily for bash), or is that script pretty much the run of the mill?alternatively, what escape code can i use to 'guarantee' the mostwide-spread support?what about echo expansion -e?what are general best practices in terms of portability/availability/distribution for scripts that use or reference control codes?i'm pretty green to deeper *nix nuances and terminal sessions, so be gentle. this is a nice read too for anyone else looking for info.",
    "present_kp": [
      "terminal"
    ],
    "absent_kp": [
      "escape characters",
      "ansi term"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "validation extensions v2. i have still another version of my validation extensions. i've reworked it and added some new features. it doesn't relay on expression trees any more but as a compensation the same extensions can be used for unit testing.the base class is still the validationcontext:public class validationcontext<targ>{ private string _membername; public targ argument { get; internal set; } public string membername { get { if (!string.isnullorempty(_membername)) return _membername; var memberexpression = expression().body as memberexpression; return memberexpression.member.name; } set { _membername = value; } } internal func<expression<func<targ>>> expression { get; set; } public virtual validationcontext<targ> validate<texception>( func<targ, bool> predicate, params object[] args) where texception : exception { if (!predicate(argument)) { throw (texception)activator.createinstance(typeof(texception), args); } return this; }}however in this version it is the one that throws exceptions after checking a validation rule. the validate method can be overriden in a derived class which allows to create a new validation context for unit testing. it is now able to throw a different kind of exception that the unit testing environment will notice. it forwards the original exception as the inner one.public class unittestingvalidationcontext<targ> : validationcontext<targ>{ public override validationcontext<targ> validate<texception>( func<targ, bool> predicate, params object[] args) { try { if (!predicate(argument)) { throw (texception)activator.createinstance(typeof(texception), args); } return this; } catch (exception inner) { // cannot throw this in linqpad //throw new assertfailedexception throw new exception(this is a test., inner); } }}as a result the validations extensions have changed a little and have gotten a new test method and refactored validations:public static class validations{ // crates validation context for normal usage public static validationcontext<targ> validate<targ>( this targ arg, string membername) { return new validationcontext<targ>() { argument = arg, membername = membername }; } // create validation context for unit-testing public static unittestingvalidationcontext<targ> test<targ>( this targ arg, string membername) { return new unittestingvalidationcontext<targ>() { argument = arg, membername = membername }; } // validations don't throw exceptions anymore but tell the context how to do it public static validationcontext<targ> isnotnull<targ>( this validationcontext<targ> context) { return context.validate<argumentnullexception>(arg => arg != null, context.membername, test message.); } public static validationcontext<string> isnotnullorempty( this validationcontext<string> context) { return context.validate<argumentnullexception>(arg => !string.isnullorempty(context.argument), context.membername, test message.); }}examples:var foo = (string)null;// normal usage like for method parameters etc.foo.validate(foo).isnotnullorempty(); // bam!// in unit-testingfoo.test(foo).isnotnullorempty(); // bam!some more examples showing how i mean to use the extensions (this is a special case where i wrap the validation in an action because it looks nice when alls tests look the same):[testmethod]public void isnullpasses(){ new action(() => ((string)null).validate().isnull()).test().doesnotthrow();}[testmethod]public void isnullthrows(){ new action(() => .validate().isnull()).test().throws<argumentexception>();}",
    "present_kp": [
      "unit testing",
      "validation"
    ],
    "absent_kp": [
      "c#",
      "extension methods",
      "framework"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "setting up and syncing a developer (local) version and a public version of a website. what is the best and most efficient way to set up two versions of the same website?i want one version that is online and open for everyone to use but i also want a developer version where i can develop and test new things instead of doing that against the ftp.currently i have mamp setup on my developing machine and i use git for version control.in mamp/xampp the location for the files are localhost/example/ and on the ftp it is the host's (dreamhost's in this case) file structure. something like /home/username/example.com/. that results in having multiple versions of pretty much the same file. this isn't that efficient but i can't come up with a good solution for it. are there any?it would be perfect if i wanted to add a new feature to the website i could do that in mamp and when i commit the changes with git it uploads the changes to the ftp.",
    "present_kp": [
      "ftp"
    ],
    "absent_kp": [
      "web development",
      "websites",
      "project structure"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "asynchronous socket.io for lots of concurrent messages. we have a web application which currently operates like this on a typical view/page:the front has to display 100+ previews (in the form of base64 images)each of this preview is built on-demand by the backend when the front requests ita pool of 8 standard ajax requests are running in queue until all previews are loadedlet me illustrate this with a picture:sometimes the user does something that modifies some of the previews and those (and only those) have to be requested again by the front. in this case it goes like this:at this moment, waiting for 100+ previews to load initially can take a long time (between 30 and 50 seconds), which is very annoying.before coming to the actual question, let me state a few points:each preview is totally independent from the otherseach preview can be built almost instantly by the back-endeach preview's base64 payload weighs virtually nothing (a few kb)establishing an http connection actually takes most of the timeof course, no more than around 8 ajax requests can work concurrentlyif the user leaves to another view while 8 requests are being handled, the browser will wait to have some room in his ajax queue before loading elements from the new view, which is very annoying (the new view can remain empty for many seconds before something happens)we can't bulk all the previews in one big request because sometimes (and it's not predictable), it happens that a particular preview takes a lot of time (several seconds) to built, and we don't want to penalize the other previews which could be loaded much fasterso, my questions are:could using a socket dramatically improve connection time so that the app is much more responsive at initial loading and in case of modifications in previews?could messages exchanged in this socket be asynchronous so that if we ask for 100+ previews at the same time, they will all be loaded very quickly? and if so, is there a maximum number of concurrent messages in the socket?can all the concurrent messages for which the front is currently waiting for, be instantly dropped if the user exits this view to visit another one? or does it even matter if there are virtually no concurrent messages limit?we're using emberjs for the front with emberdata and flask for the back.",
    "present_kp": [
      "async"
    ],
    "absent_kp": [
      "concurrency",
      "websockets",
      "ember.js"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "getting wifi ssid on ios in swift. i've seen a lot of objective-c implementations but i'd like to do it in swift.i did it like this and it seems to work just fine. does anyone have comments and/or improvements to make?import systemconfigurationclass envdata: nsobject { class func getssid() -> string { var currentssid = let interfaces = cncopysupportedinterfaces() if interfaces != nil { let interfacesarray = interfaces.takeretainedvalue() as [string] if interfacesarray.count > 0 { let interfacename = interfacesarray[0] as string let unsafeinterfacedata = cncopycurrentnetworkinfo(interfacename) if unsafeinterfacedata != nil { let interfacedata = unsafeinterfacedata.takeretainedvalue() as dictionary! currentssid = interfacedata[ssid] as string } else { currentssid = } } else { currentssid = } } else { currentssid = } return currentssid }}",
    "present_kp": [
      "ios",
      "swift"
    ],
    "absent_kp": [
      "networking"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "teaching neural net via video footage. is it possible to train a neural network to learn something via video footage?in other words, if i have a video teaching me how to draw an animal from scratch, can i then use this video to teach the computer to draw the animal in the same way?edit:video footage is essentially a sequence of images, any image processing capabilities available to us through machine learning are possible when applied to videos using a sequencial network (lstm, rnn etc.)so i guess the difficult part becomes mapping the activity to an action like moving a pen or something",
    "present_kp": [],
    "absent_kp": [
      "neural networks"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is the meaning partitioned signatures?. context: boneh & boyen devised a transformation for these class of signatures which increases the security to stronger notion of unforgeability. it would be nice if it is an example. thanks!reference paper: strongly unforgeable signature based on cdh assumption",
    "present_kp": [],
    "absent_kp": [
      "cr.crypto security"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "can not connect internet from lan. i have the below setups. node1 & node2 are in a lan. both use the eth0 nic to connect to lan. both are debian systems.additionally node1 is connected to the internet using wlan0 nic. expectation is that both node1 & node2 can reach internet.with this setup, i am not able to access internet. the kernel ip showed the default rule to the routerbasically i am encountering with two issues.issue 1 :on node 1with this setup, the node1:kernel ip table shows the default rule via eth0. hence connecting to router1/lan.i tried ip route del & add new default rule device wlan0. ip table looks good(at least what i expected default rule via wlan0). but still, no internet connection.question 1: is this the right way to delete & add the default rule to change the system behaviour to which dev it should choose to send the packets out. or some additional config i need.now with curiosity, i just flipped the dev. means the wlan0 nic is connected now to lan & eth0 is to the router2(external net).ip table shows default rule to eth0. as this time eth0 connected to router2, i got the internet connection. could ping, browse & all.question2: is eth0 always used for default rule. how can i change to other nic. here wlan0.issue 2:on node 2however this success could not last longer as from node-2, i do not get internet connection from node-2.it's static configuration, the gw is mentioned as ip addr of node-1. question3: is this correct. or do i need to do any snat or masqrade kind of config on node1 to be able to or the router-2 should take care of that.. what i am missing here to get the internet connection.i am adding some clarity to the above scenario. & also clarification to some comments as i am not yet authorized to comment.internet access to/from node2(if more systems on lan also) should via node-1 only.the lan router(router-1) is dhcp enaled and all nodes are client to it.same with the internet router-2.so in this case do i need to do any special configuration on node-1.on node-1, ip route : default via dev eth0on node-2, ip route : default via dev eth0.another tricky question (but less important, i can live with it by using eth0 to my internet connected router), what i have already asked is : why the default is via eth0 always. how can i force it to use wlan0(in my case) connected gw. my del/add of default is not working.",
    "present_kp": [
      "lan"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "one way diff between 2 directories. i am looking to perform a diff between 2 directories, both of which contain may files and subdirectories.i am hoping to perform a one-way diff between a source and target directory that will return only the following:files that exist in source but not in targetfiles that have changes in source that do not exist in targetso this would exclude:files that exist in the target but not in the sourceany additions that have been made to files in the target directory that do not conflict with a line in the sourceany changes related to whitespaceis this possible? i am using cygwin.",
    "present_kp": [
      "directory",
      "diff"
    ],
    "absent_kp": [
      "recursive"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "vc dimension calculation for intervals. as i see in ml course a vc dimension calculation is very theoretical. what is the vc-dimension of intervals in r? the target function is specifieed by an interval, and labels any example positive i it lies inside that interval.answers:vc-dim = 2. a set of two points can be shattered, since there's only a single block of positive examples that could lie within the interval. but no set of 3 points can be shattered, because it can not be labeled in alternating +; - ; + order.so i'm get stuck with meaning of interval. for example, it means {(a, b) | a is lower than b, a,b is real number} has vc-dim = 2?any idea or solution would be highly appreciated.",
    "present_kp": [
      "vc dimension"
    ],
    "absent_kp": [
      "machine learning",
      "artificial intelligence",
      "data mining",
      "pattern recognition"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "updating the desktop icons after moving a manually-installed program. i'm new here. i've installed a program manually and now i've moved the program files to a new location. how do i update the path of the program for my old launcher icons to work properly?-- without symlinking.i'm using elementary os.",
    "present_kp": [],
    "absent_kp": [
      "software installation",
      "executable",
      "pantheon"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is it possible to protect facebook page from takeover by other administrator?. if many people have admin access to facebook page one of them can delete page, or remove other administrator.is it possible to protect from that situation in some way (ie. master admin or password)?",
    "present_kp": [
      "facebook"
    ],
    "absent_kp": [
      "security"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "are there sex differences in inter-rater consistency of attractiveness ratings of opposite-sex faces?. is there any research that assesses whether males or females judge faces of the opposite sex with a greater internal consistency? that is, is the standard deviation of attraction ratings for a specific opposite sex face on average smaller when looking at a specific gender?",
    "present_kp": [
      "sex differences"
    ],
    "absent_kp": [
      "social psychology",
      "physical attraction",
      "reliability"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "add a set of querystring params to a url. example cases:need to add x=1, y=2 and querystring variables to the following urls:<url> usageget_link(array('x' => 1, 'y' => 2, 'z' => 'string'));function get_link( $my_params ){ $param_querystring = ; $http_host = $_server['http_host']; //-- get the part of the url before the querystring, if applicable $url = explode( '?', $_server['request_uri'] ); $request_uri = $url[0]; $querystring = $url[1]; foreach ( $my_params as $param_key => $param_value ) { $param_querystring .= $param_key . '=' . $param_value; } if ( empty( $querystring ) ) { //-- generates foo.com/blah?x=1&y=2&z=string if no //-- querystring was present $link = $request_uri . '?' . $param_querystring; } else { //-- generates foo.com/blah?a=1&b=2&x=1&y=2&z=string if a=1&b=2 //-- querystring was already present. $link = $request_uri . $querystring . '&' . $param_querystring; } return $link;}",
    "present_kp": [
      "url"
    ],
    "absent_kp": [
      "php"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "can i use plesk to grant site access to more than one user per site and multiple sites per user?. i am new to parallels plesk control panel and i have the reseller account of it. here i want to ask about user management. let say i have website a, b, c, d, and e hosted on my dedicated server. is it possible that i create a few user accounts to manage website for file & database management like:user 1 manage website a, b, and c.user 2 manage website a, d, and e. user 3 manage website b, c, and e.is it possible to do that? if can how?",
    "present_kp": [
      "plesk",
      "management"
    ],
    "absent_kp": [
      "users"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how do linux distributions check the cd/dvd-rom integrity?. for example in debian wheezy installation cd there is a check the cd-rom(s) integrity menu option in debian-installer main menu. checking the integrity of cd takes few minutes. how are such tests usually done? by simply reading all the files from cd and in case thee are no read errors from file-system, the cd is valid? or does it involve some sort of hash calculation and comparison?",
    "present_kp": [
      "debian"
    ],
    "absent_kp": [
      "debian installer"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how could i flip content of a binary file with bash commands. i was wondering if it's possible to do bitwise conversion on a binary file, with bash commands, (in my case bitwise negation).",
    "present_kp": [
      "bash",
      "binary"
    ],
    "absent_kp": [
      "shell script"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why does sudo work on linux but not android?. i have not had the chance to read enough about android, linux, or unix to answer this myself. sudo works on a linux machine but doesn't work on android unless you root the mobile device (e.g. samsung gt-n8013). why does the mobile device require to be rooted, but not the typical linux install?the context of my question is related to <url> is there any way for a program to ask to run as root on android, the same way you have escalation of privileges to run as administrator on windows? if you think this question should be on its own thread, i can create one)",
    "present_kp": [
      "sudo",
      "root",
      "android",
      "privileges"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "polling loop - always a bad decision?. i had a need to isolate some long-running blocking calls in a background thread of my app. i also needed to keep this thread running indefinitely, because com would complain if some object i created in that thread were accessed from another thread. i then needed to control what these objects did from my main ui thread, in a guaranteed-fifo manner.the solution i came up with was kind of activeobject-reminiscent; a polling loop running as the dowork handler of a backgroundworker, which accepted encapsulated command objects representing the work to perform from a thread-safe concurrentqueue. here's the vitals://basic implementation of the command pattern; no serialization/persistence needed,//just need to be able to encapsulate some work and defer it.internal class command{ private action action; public command(action action) { this.action = action; } protected command() { } public virtual void execute() { action(); }}//generic variation accepts a single parameterinternal class command<t>:command{ private t param; private action<t> action; public command(action<t> action, t param) { this.action = action; this.param = param; } public override void execute() { action(param); }}...//event handlers run in the ui thread will enqueue() command objects//containing the delegates that should be run in the background threadconcurrentqueue<command> queuedcommands = new concurrentqueue<command>();//the bgw dowork handler; runs indefinitely until the ui that needs it is dispose()dprivate void mainbackgroundloop(object sender, system.componentmodel.doworkeventargs e) { command command; bool commandavailable; do { commandavailable = queuedcommands.trydequeue(out command); if (commandavailable) command.execute(); else thread.sleep(100); //<-- here's your code smell } while (!e.cancel); }now, this functions beautifully. but, the use of thread.sleep() in any so/cr post usually gets called out as a code smell. i get why; this loop requires cpu to try to pull commands out of the queue at least 10 times a second, which will eat cpu for an idle thread. but, given there are other reasons why the thread has to keep running, this seems acceptable.thoughts?",
    "present_kp": [],
    "absent_kp": [
      "c#",
      "multithreading"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "suggestions for alternative 3d space partition tessellation, different from voronoi and delaunay. i have a system of mono-disperse spheres inside a cubic box. i am studying the volume distribution inside the sample, after tessellating it with either voronoi and delaunay tessellations. i am interested on some properties which should not depend on the tessellation.currently, i am comparing with the values obtained from voronoi and delaunay. i would like to know if you are familiar with another space partition approach (it is important that the final sum of the individual cells add up to the total volume, and the cells should be disjoint). furthermore, in case you know another kind of tessellation, do you also know a library which already implements it, preferable in c/c++ or python?some variations, like laguerre partitions, coincide with my current voronoi approach since the spheres are mono-disperse. another candidate will be the centroidal voronoi tessellation, although i have not found yet a library to do that (although it could lead to evenly spaced cells which does not reflect the disorder inside the system, which is not desirable).for a given tessellation, the spheres' centers are used to perform the tessellation. in the case of voronoi, each cell wil enclose the sphere. thanks in advance for your kind help.note: i have asked this at stackoverflow. i was suggested to ask here, but i do not have enough points to migrate a question. the original question link is : <url>",
    "present_kp": [],
    "absent_kp": [
      "computational geometry"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "in scite (scintilla), how can i create syntax-highlighting for my own custom language?. i want to create a custom language, with its own custom syntax highlighting. notepad++ (a windows scite/scintilla based text-editor), allowed me to create a custom language, and now, in linux, i want to reproduce the same thing. i need(?) to use skite/scintilla because unless someone knows otherwise, it is the only plain text editor which can display different size fonts in the same text-file (eg. default-font= 12pt, comment-font=24pt). i used the comments font to display a complex script(alphabet) in a larger font.please let me know if there is any other plain text editor which does this. i assume this is a feature of scite/scintilla (and not of notepad++).some of the magic is possible/probably(?) done in files such ase:/usr/share/scite/.properties notepad++ has a gui interface to set up a new language/syntax,...but i could use some direction on exactly how to go about it in scite. thanks...",
    "present_kp": [
      "fonts",
      "size",
      "highlighting"
    ],
    "absent_kp": [
      "editors"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "retrieving location coordinates from sql database and plotting on a map android gmaps. i am new to android development. i am currently working on an android app that fetches changing location coordinates from a sql database, and plots on a map view at regular intervals. but i think this code can be made more efficient, as i am unable to fetch locations after a certain time and can't figure out how to solve it.i am attaching the code of which contains 2 classes. the first is mapsactivity.java that displays the locations on maps, and the second is bgtask.java. it is inside mapsactivity.java and extended from asynctask for fetching data from the database. there is a thread also for regular fetching of data from database and plotting on maps.here is my mapsactivity.java:package com.maps.saury.mybus_demo;import android.manifest;import android.content.pm.packagemanager;import android.os.asynctask;import android.os.bundle;import android.os.handler;import android.os.message;import android.support.v4.app.activitycompat;import android.support.v4.app.fragmentactivity;import android.view.view;import android.widget.button;import android.widget.toast;import com.google.android.gms.maps.cameraupdatefactory;import com.google.android.gms.maps.googlemap;import com.google.android.gms.maps.onmapreadycallback;import com.google.android.gms.maps.supportmapfragment;import com.google.android.gms.maps.model.bitmapdescriptorfactory;import com.google.android.gms.maps.model.latlng;import com.google.android.gms.maps.model.marker;import com.google.android.gms.maps.model.markeroptions;import java.io.bufferedreader;import java.io.ioexception;import java.io.inputstream;import java.io.inputstreamreader;import java.net.httpurlconnection;import java.net.malformedurlexception;import java.net.url;public class mapsactivity extends fragmentactivity implements onmapreadycallback { private googlemap mmap; string data_string; string lat,lng; double d_lat,d_lng; public static handler handler; public static thread thread1; public static boolean status=false; public static button button; public static latlng latlng,latlng_currentloc; public static byte count=0; public static marker now; @override protected void oncreate(bundle savedinstancestate) { button=(button)findviewbyid(r.id.button2); super.oncreate(savedinstancestate); setcontentview(r.layout.activity_maps); // obtain the supportmapfragment and get notified when the map is ready to be used. supportmapfragment mapfragment = (supportmapfragment) getsupportfragmentmanager() .findfragmentbyid(r.id.map); mapfragment.getmapasync(this); startact(); } @override public void onmapready(googlemap googlemap) { mmap = googlemap; if (activitycompat.checkselfpermission(this, manifest.permission.access_fine_location) != packagemanager.permission_granted && activitycompat.checkselfpermission(this, manifest.permission.access_coarse_location) != packagemanager.permission_granted) { return; } mmap.setmylocationenabled(true); } public void showloc(view v){ mmap.movecamera(cameraupdatefactory.newlatlng(latlng)); mmap.animatecamera(cameraupdatefactory.zoomto(16)); } public void exitmap(view v){ status=false; finish(); } public void startact(){ count=0; status=true; thread1=new thread(new loopclass()); thread1.start(); handler=new handler(){ @override public void handlemessage(message msg) { toast.maketext(getapplicationcontext(),count= +count,toast.length_short).show(); new bgtask().execute(); } }; } class bgtask extends asynctask<void, void, string> { @override protected void onpreexecute() {} @override protected string doinbackground(void... params) { string locurl = <url> try { url url = new url(locurl); httpurlconnection http = (httpurlconnection) url.openconnection(); inputstream is = http.getinputstream(); bufferedreader br = new bufferedreader(new inputstreamreader(is)); stringbuilder sb = new stringbuilder(); while ((data_string = br.readline()) != null) { sb.append(data_string + ); } br.close(); is.close(); http.disconnect(); return sb.tostring().trim(); } catch (malformedurlexception e) { e.printstacktrace(); } catch (ioexception e) { e.printstacktrace(); } return null; } @override protected void onpostexecute(string s) { int i=0; while(s.charat(i)!=' '){i++;} lat=s.substring(0,i-1); lng=s.substring(i+1); d_lat= double.parsedouble(lat); d_lng= double.parsedouble(lng); latlng = new latlng(d_lat,d_lng); if(now!=null) now.remove(); if(count==0) { mmap.animatecamera(cameraupdatefactory.newlatlng(latlng)); count++; } else { now=mmap.addmarker(new markeroptions().position(latlng).icon(bitmapdescriptorfactory.fromresource(r.drawable.bus)).title(my loc)); } } } class loopclass implements runnable{ @override public void run() { while(true){ if(status==false)break; message msg=message.obtain(); msg.arg1=8; handler.sendmessage(msg); try { thread.sleep(2000); } catch (interruptedexception e) { e.printstacktrace(); }} } }}",
    "present_kp": [
      "java",
      "android"
    ],
    "absent_kp": [
      "google maps"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "understanding subtle differences between addressing modes in x86. i'm trying to have a whole picture of all the possible addressing modes of x86 instructions. starting from this i studied the intel ia-32 reference and multiple secondary references found online.i'd like to understand them correctly, so here's my doubts:mod == 0b11: direct value contained in register is accessed, quite clearmod != 0b11: these are all indirect values, with optional 8 bit or 16 bit displacement to the final value, so we refer to the value contained in the computed address.my doubts:the 16 bit displacement is signed or unsigned? eg mov ax, [si + 40000] vs mov ax, [si - 1000]what is exactly the case mod == 0b00 & r/m == 0b110? it' just a indirect absolute value, eg mov cl, [1234h], which masm compile as 8b0e3412: mov cx, word ptr ds:0x1234are all these indirect addressing always relative to a segment? from the refence it sounds like that in 16 bit mode everything is always relative to ds, unless bp is contained in the indirect address, in that case ss is used (or a specific segment override is used). so basically [bp+si+10h] always means ss:[bp+si+10h] where ss segment is shifted by 4 bits to the left.which is the exact role of 67h prefix in this context? if i use a 67h prefix it's like switching the table of 16 bit addressings with the 32 bit addressing and viceversa? (according to the current executing mode).and what about 66h? does it just change the size of data moved between 16 bits and 32 bits? eg, forcing 32 bit operand size means that a 32 bit register will be selected and always 4 bytes of memory will be fetched from the indirect address and vice versa?and now the 32-bit addressing modesmod == 0b11: direct value, as for 16 bit, quite clearmod == 0b00 && r/m == 0b101: raw value as for the 16 bits addressing casemod != 0b11 && r/m == 0b100: the r/m doesn't specify a register but the sib mode, so we can specify a base register + an index register + a scale valuehere everything is enough clear, i was just wondering, as for 16 bits if displacement in 32 bits are signed or unsigned? sib and displacement can be combined easily if i understand it correctly, eg [eax + ebx*2 + 10] will generate a mod == 01 with specific sib byte and additional single byte for signed displacement. are these values considered absolute in a flat memory space or segments must be considered here too?",
    "present_kp": [
      "x86"
    ],
    "absent_kp": [
      "assembly"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how are google plus emails interactive?. i received a mail from google+ saying that someone i knew recently joined google+. the email had an interactive red-box which allowed me to add the user to my circles. as far as i am aware, emails do not include running javascript. how is this achieved?screenshots:the original emailon hoverfinal result",
    "present_kp": [
      "email",
      "google plus"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how do i check if fftw installed correctly?. i tried to install fftw on my system (ubuntu 12.04) using the rather unclear instructions on this website, and now i want to check if it worked. (fftw is a c subroutine library for computing the discrete fourier transform in one or more dimensions.) from the website:you can also type make check to put the fftw test programs through their paces.the problem is that the website doesn't say what the output is supposed to look like. i tried this and the output was a lot of lines like executing /home/petur/fftw-3.3.4/tests/bench -o nthreads=2 --verbose=1 --verify 'ok10bx6bx6e11x13b' --verify 'ik10bx6bx6e11x13b' --verify 'obrd7x13v16' --verify 'ibrd7x13v16' --verify 'ofrd7x13v16' --verify 'ifrd7x13v16' --verify '//obcd7x13v16' --verify '//ibcd7x13v16' --verify '//ofcd7x13v16' --verify '//ifcd7x13v16' --verify 'obcd7x13v16' --verify 'ibcd7x13v16' --verify 'ofcd7x13v16' --verify 'ifcd7x13v16' --verify 'okd10bv127' --verify 'ikd10bv127' --verify '//obr240' --verify '//ibr240' --verify '//ofr240' --verify '//ifr240' --verify 'obr240' --verify 'ibr240' --verify 'ofr240' --verify 'ifr240' --verify '//obc240' --verify '//ibc240' --verify '//ofc240' --verify '//ifc240' --verify 'obc240' --verify 'ibc240' --verify 'ofc240' --verify 'ifc240' --verify 'ok11760e00' --verify 'ik11760e00' --verify 'obr33v31' --verify 'ibr33v31' --verify 'ofr33v31' --verify 'ifr33v31' --verify '//obc33v31' --verify '//ibc33v31' --verify '//ofc33v31' --verify '//ifc33v31' --verify 'obc33v31' --verify 'ibc33v31'each followed by tens of lines likeok10bx6bx6e11x13b 1.5604e-07 1.85166e-05 1.52953e-07which doesn't tell me much. the program notified me that the fftw transforms passed basic tests and that fftw threaded transforms passed basic tests.is that enough to know that fftw installed successfully? i don't know if it's relevant, but i checked the contents of ~/fftw-3.3.4/config.log and it contains some lines that suggest that there's been some errors, for example:conftest.c:88:24: error: expected expression before ')' tokenconftest.c:118:18: error: expected '=', ',', ';', 'asm' or '__attribute__' before 'newvar'i think i should also mention that /usr/local/lib/ now contains the fileslibfftw3f.a libfftw3f.la libfftw3f_threads.a libfftw3f_threads.ladoes that mean i can remove the directory ~/fftw-3.3.4 or is that still necessary?",
    "present_kp": [
      "c"
    ],
    "absent_kp": [
      "software installation"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "kile freezes running pdflatex. when i run kile i get the message:qstatusbar::insertwidget: index out of range (4), appending widgetand i am not able to compile from kile. while i press pdflatex inside kile, it freezes. what can i do to debug/fix this? i am using suse linux 11.4",
    "present_kp": [
      "kile"
    ],
    "absent_kp": [
      "compiling"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "javascript/jquery string handling. recently i wrote some code to take two arrays of data and create a title. each array could have 0-n elements. however, the multiple if statements seem messy to me. is there a nicer way to do this? perhaps one that harnesses the power of jquery better?var campaign_names = data[campaign_names];var list_names = data[list_names];var title;if (campaign_names && campaign_names.length > 1) { campaign_names = campaign_names.join(, );} else if (campaign_names) { campaign_names = campaign_names[0];}if (list_names && list_names.length > 1) { list_names = list_names.join(, );} else if (list_names) { list_names = list_names[0];}if (campaign_names && list_names) { title = campaign_names + & + list_names;} else if (campaign_names) { title = campaign_names; } else if (list_names) { title = list_names;}",
    "present_kp": [
      "javascript",
      "jquery"
    ],
    "absent_kp": [
      "strings"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "destroywindow() non-existant windows. what i am working on has sets of window controls. controls are constantly being either destroyed, or created.void changecontrol() {// remove all controls, existing, or nothwnd = getdlgitem(hwnd, idc_btn_fooa);destroywindow(hwnd); hwnd = getdlgitem(hwnd, idc_btn_foob);destroywindow(hwnd); hwnd = getdlgitem(hwnd, idc_btn_fooc);// add the control wantedhwndbutton = createwindow(button, start, ws_visible|ws_child|bs_ownerdraw|bs_pushbutton, left, top, button_width, button_height, hwnd, (hmenu)idc_btn_foob, getmodulehandle(null), null);}test case:dword errorresult;testhwnd = getdlgitem(hwnd, idc_btn_start); if (testhwnd) { destroywindow(hwnd); }else { errorresult = getlasterror(); }i am wondering if this method is safe, or if i should be checking if a control exists before calling destroywindow().",
    "present_kp": [],
    "absent_kp": [
      "c++",
      "winapi"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "image processing pipeline. i'm currently toying around with some computer vision algorithms and always wanted to learn more about templates, so i came up with the idea to build a templated image processing pipeline.i want the pipeline to be built of several task blocks which i want to plug into each other sequentially. for a specific block, its input and output types are fixed. i ultimately want to be able to inherently control for a block to only accept the next block if the output type of the current is identical to the input type of the next. block_n block_n+1 _______________ _________________ int | | str | | float--->| str = f1(int) |------>| float = f2(str) |-------> |_______________| |_________________|consequently, block_n+2 would have to accept float as input and may have an arbitrary output type. this is what i was able to come up with:abstract base classes:template<class i>class abstractinput {public: virtual ~abstractinput() { } virtual void call( i _input ) = 0;};template<class i, class o>class abstractpipelinetask : public abstractinput<i> {public: typedef abstractinput<o>* ptr; abstractpipelinetask( abstractinput<o>* _nexttask = null) : m_nexttask ( _nexttask ) { } virtual void call( i _input ) { m_input = _input; m_output = executetask( m_input ); if ( m_nexttask ) m_nexttask->call( m_output ); } virtual o executetask( i& _input ) = 0;protected: abstractinput<o>* m_nexttask; i m_input; o m_output;};example implementation:class str2intpipelinetask : public abstractpipelinetask< std::string, int > {public: str2intpipelinetask( abstractinput<int>* _nexttask = null ) : abstractpipelinetask( _nexttask ) { } virtual ~str2intpipelinetask() { } virtual int executetask( std::string& _input ) { return atoi( _input.c_str() ); }};class int2intpipelinetask : public abstractpipelinetask< int, int> {public: int2intpipelinetask( abstractinput<int>* _nexttask = null ) : abstractpipelinetask( _nexttask ) { } virtual ~int2intpipelinetask() { } virtual int executetask( int& _input ) { return ( _input * _input ); }};class int2floatpipelinetask : public abstractpipelinetask< int, float > {public: int2floatpipelinetask( abstractinput<float>* _nexttask = null ) : abstractpipelinetask( _nexttask ) { } virtual ~int2floatpipelinetask() { } virtual float executetask( int& _input ) { return ( static_cast<float>( _input ) / 42.f ); }};examples:int _tmain(int argc, _tchar* argv[]){ int2floatpipelinetask* pipetask2 = new int2floatpipelinetask( ); int2intpipelinetask* pipetask1 = new int2intpipelinetask( pipetask2 ); str2intpipelinetask* pipetask0 = new str2intpipelinetask( pipetask1 ); pipetask0->call( 42 ); return 0;}what i would really like to know besides some general advice is:how can i get rid of the raw pointers abstractinput<o>* m_nexttask? i tried changing the typedef of ptr to std::shared_ptr<abstractinput<o>> but that broke the derived classes constructors.is it possible to get rid of abstractinput?",
    "present_kp": [
      "template"
    ],
    "absent_kp": [
      "c++",
      "polymorphism"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how do i make the value of one field change based on changes made to another field in cognito forms?. can i create a drop down box that shows a code (in this case a trip code) and link it to another box where the name of the trip corresponding to that code would be shown?",
    "present_kp": [
      "cognito forms"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "schema.org webpage structure for single page site. i am building a single page portfolio site with wordpress for a client which is divided into sections that would traditionally be separate pages like - about, portfolio, contact. can i nest multiple schema.org webpages within one global webpage like the following markup:<body> <main itemscope itemtype=<url> <article itemscope itemtype=<url> </article> <article itemscope itemtype=<url> </article> <article itemscope itemtype=<url> </article> </main></body>",
    "present_kp": [
      "schema.org"
    ],
    "absent_kp": [
      "html",
      "microdata",
      "single page application"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "bash variable checker function. i am struggling to figure out a way to reduce code duplication. specifically when checking if a variable is set. my code currently looks like this:# webadmin enabledif [ -f ${servercfgfullpath} ]; then webadminenabled=$(grep controlpanelenabled ${servercfgfullpath} | sed 's/^.*value=//' | cut -f1 -d\\) if [ ! -n ${webadminenabled} ]; then webadminenabled=not set fielse webadminenabled=\\e[0;31munavailable\\e[0mfi# webadmin portif [ -f ${servercfgfullpath} ]; then webadminport=$(grep controlpanelport ${servercfgfullpath} | tr -cd '[:digit:]')fiif [ ! -n ${webadminport} ]; then webadminport=0fiin summery the code first checks that a config file is available if not displays unavailable to the user. if config file is available will attempt to get specific info from the config file to the user, should that fail it will display not set instead.not really an issue if only used a few times however this is duplicated many times and although works it is far from ideal as it gets very hard to read. serverconfigpath variable i believe is easy enough to improve however i would like to create a function or a way to simply set a variable to not set if the variable was unable to get the information it needed.any ideas or pointers on how to streamline this code?here is the specific file with the offending code <url>",
    "present_kp": [
      "bash",
      "variable"
    ],
    "absent_kp": [
      "shell script"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is data received after transmission immediately stored and can that be recovered?. when i send a message from one device to another, during the process during which the other device is receiving the data, is that data stored temporarily on some place on the device or is it immediately stored on the hard-drive of the device. and while using tcp when you receive an ack does this mean that that data has been stored on the hard drive or is it stored when everything is received. for example, i am sending an essay to a server which makes a download link for another person and then send that link back to me, for sharing. now during the process of sending that essay on the server i cancel halfway, so can i determine whether the essay was stored on its main drive or not (just an example).",
    "present_kp": [],
    "absent_kp": [
      "communication protocols"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "clear the cache of the last visited page. i have an html page that has shows if a server is online of offline via an iframe transcluding a php file. if someone clicks on a link, like to turn the server on or off and presses the back button on the browser, the browser loads a cached version of the page and still displays the server as off unless the page is refreshed. i know there are other ways to go about this but i would like for just that page to not be cached so that when a user reaches that page via the back button, it will have to redownload the page.how can this be done? is it even possible?putting header(cache-control: no-cache, must-revalidate); in the php file did not work.",
    "present_kp": [
      "cache"
    ],
    "absent_kp": [
      "browsers",
      "cache control"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is there a way to bulk follow all usernames in a text list?. we've been collecting twitter handles for contacts in our crm systems for a while now. we're finally joining the 21st century and are deciding to start a twitter account.is there a way (perhaps a handy program) for me to feed in a list of twitter usernames and get my account to follow each of them?",
    "present_kp": [
      "contacts",
      "twitter"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "can i remove *-dev packages if i'm not compiling apps from source code?. i rarely compile source code myself and mostly install stuff using apt-get. so, can i safely uninstall all those *-dev files? or are some apps reliant on them?",
    "present_kp": [],
    "absent_kp": [
      "debian",
      "package management",
      "development"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how can i debug my pseudocode algorithm?. the algorithms for the problem i am working on become more and more complex as i try to improve their performance. they already span several pages with cases and sub-cases, and will probably become even longer. i am worried that there might be mistakes that are difficult to notice.as a programmer, i am used to writing detailed test-cases to test my programs, but the algorithm is written in pseudo-code (it is not easy to implement). what ways can you recommend for testing the algorithm?",
    "present_kp": [
      "algorithms"
    ],
    "absent_kp": [
      "software testing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "can server errors cause major traffic drop from google?. my web server was down for about 30 min. the next day (30~ hours from the event) i noticed major drop in traffic from google. the normal traffic to my site is about 2500 unique/day and after the event it dropped to 400/day. other than this event nothing happened. can this be the reason?",
    "present_kp": [
      "google",
      "server"
    ],
    "absent_kp": [
      "crawl errors"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "classifying samples based on marker gene expression. i have a few sets of marker genes that i can classify rna-seq samples using semi-supervised clustering. i would like to automate the process, however, i am struggling to find the ideal algorithm that could generate some kind of score for marker gene set from a given sample. i presume that this is a standard analyses in many groups but i am not sure which method(s) are yielding good results in practice.",
    "present_kp": [],
    "absent_kp": [
      "rna seq",
      "classification"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "predicting a word using word2vec model. given a sentence:when i open the ?? door it starts heating automaticallyi would like to get the list of possible words in ?? with a probability.the basic concept used in word2vec model is to predict a word given surrounding context.once the model is build, what is the right context vectors operation to perform my prediction task on new sentences? is it simply a linear sum?model.most_similar(positive=['when','i','open','the','door','it','starts' ,'heating','automatically'])",
    "present_kp": [],
    "absent_kp": [
      "nlp",
      "predictive modeling",
      "word embeddings"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "vim resets columns variable after ctrl+z. i start up vim using the following command:vim -u none -u none -c ':set columns=40 nu'then, i push vim to the foreground......and return.for some reason, vim has changed the columns variable to the width of the window without me telling it to do that.any idea how to fix this? note that i did not manually resize the window after ctrl+z",
    "present_kp": [],
    "absent_kp": [
      "bash",
      "macos"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why doesn't parallelism necessarily imply non-determinism?. i'm a student reading a book on threads. and i got when i got to non-deterministic and parallel programs, i got a bit confused. i hope you can help me out. i understand the difference between concurrency and parallelism. i get that concurrent programs are non-deterministic depending on the precise timing of events. but parallelism doesn't necessarily imply non-determinism - as said in the book. why is that? does that imply that it's all dependent on the languages that support parallelism. which implies that these languages should execute parallel programs in a deterministic manner? another question that i have is that the timing of events of concurrent programs depend on what exactly on what exactly? the architecture of the machine?",
    "present_kp": [
      "concurrency"
    ],
    "absent_kp": [
      "parallel computing",
      "nondeterminism"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "bash script to replace server short names in a file with fqdn. i have a rather large config file that has server short names in it like this:[host 1]host = server1[host 2]host = server2[host 3]host = server3i need to replace all the server short names with the fq long names that will be outputted from a bash script i've already written, lookup.sh. can i work some sed or awk magic to accomplish this task?",
    "present_kp": [
      "bash",
      "sed",
      "awk"
    ],
    "absent_kp": [
      "shell script",
      "scripting"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is the difference between * and *.* while copying?. what is the difference between following 2 commands?cp -rp /dir1/*.* /dir2/cp -rp /dir1/* /dir2/",
    "present_kp": [
      "cp"
    ],
    "absent_kp": [
      "shell",
      "shell script",
      "command line"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "after unlocking our not provided keywords, we aren't seeing the actual keywords in google analytics. we are trying to improve our seo ranking but we notice a problem or maybe a discrepancy in what the google analytics data is showing us.we use this step by step for not provided:<url> the result after 2 years is :we should be getting actual keyword right?a brief backgroundwe use wordpress with yoast premiumwhat are we doing wrong? or this is normal?",
    "present_kp": [
      "google analytics",
      "wordpress",
      "not provided"
    ],
    "absent_kp": [
      "google search console"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "replay feature (repeat / rewind). this occurred to me playing some games where there is no live rewind feature. basically what i'm looking for is an application which records the screen, and - when pressing a shortcut - rewinds what it has recorded in the last 10 seconds and then jumps back to normal.is there an app which can do this? or is this achievable in another way?",
    "present_kp": [],
    "absent_kp": [
      "x11",
      "recording",
      "screencasting"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "using syslog from a kernel module. i am writing a linux kernel module to report statistics and send some log messages. can i use syslog to accomplish this job? or is printk the only way?",
    "present_kp": [
      "linux",
      "kernel",
      "syslog"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "printing infinite loop using pthread. i'm trying to implement a c program using pthreads which prints 1 2 3 4 5 in an infinite loop. i have used conditional variables and mutex to synchronize the pthreads. i'm able to print 1 2 3 4 5 without race condition using conditional variables and mutexes. but the problem occurs when i try to make it an infinite loop. can anyone please review my code and suggest the edits i have to make inorder to get an ouptut like 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 ...below is the code which i have tried implementing.#include <stdio.h>#include <stdlib.h>#include <pthread.h>static int count=0, count2=0, count3=0, count4=0,count5=1;pthread_cond_t c1 = pthread_cond_initializer;pthread_cond_t c2 = pthread_cond_initializer;pthread_cond_t c3 = pthread_cond_initializer;pthread_cond_t c4 = pthread_cond_initializer;pthread_cond_t c5 = pthread_cond_initializer;pthread_mutex_t m = pthread_mutex_initializer; //mutex variablevoid *func1(void *a){while(1){ pthread_mutex_lock(&m); while(count5 == 0){ pthread_cond_wait(&c5, &m); } printf(1 ); count = 1; count5 = 0; pthread_cond_signal(&c1); pthread_mutex_unlock(&m);}}void *func2(void *b){while(1){ pthread_mutex_lock(&m); while(count == 0){ pthread_cond_wait(&c1, &m); } printf(2 ); count2 = 1; pthread_cond_signal(&c2); pthread_mutex_unlock(&m);}}void *func3(void *c){while(1){ pthread_mutex_lock(&m); while(count2 == 0){ pthread_cond_wait(&c2, &m); } printf(3 ); count3 = 1; pthread_cond_signal(&c3); pthread_mutex_unlock(&m);}}void *func4(void *d){while(1){ pthread_mutex_lock(&m); while(count3 == 0){ pthread_cond_wait(&c3, &m); } printf(4 ); count4 = 1; pthread_cond_signal(&c4); pthread_mutex_unlock(&m);}}void *func5(void *e){while(1){ pthread_mutex_lock(&m); while(count4 == 0){ pthread_cond_wait(&c4, &m); } printf(5 ); count=0; count2=0; count3=0; count4=0; count5=1; pthread_cond_signal(&c5); pthread_mutex_unlock(&m);}}int main(int argc, char **argv){ pthread_t thread[5]; pthread_create(&thread[0], null, func1, null); pthread_create(&thread[1], null, func2, null); pthread_create(&thread[2], null, func3, null); pthread_create(&thread[3], null, func4, null); pthread_create(&thread[4], null, func5, null); for(int i=0; i<5; i++) pthread_join(thread[i], null); pthread_mutex_destroy(&m); pthread_cond_destroy(&c1); pthread_cond_destroy(&c2); pthread_cond_destroy(&c3); pthread_cond_destroy(&c4); pthread_exit(null); return 0;}",
    "present_kp": [
      "c",
      "pthreads"
    ],
    "absent_kp": [
      "posix"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "custom syntax file for fix data doesn't work. i tried to create a custom syntax file to read fix messages. note: i am certain my custom files are being loaded by vim.the file format is sequences of number=value pairs separated by the unprintable 0x1 character. i wanted a few of the pairs (the ones with important data) to turn colors when i open the file in vim. i don't care about which colors they turn, as long as they are all different and are easily distinguishable from each other.specifically, i want the tags associated with the following numbers to be highlighted: 44, 52, 54, 60, 9717.here's my syntax file (~/.vim/syntax.vim)if version >= 600 if exists(b:current_syntax) finish endifelse syntax clearendifsyn case ignoresyn match fixprice \\|44[=][^|]\\+\\|syn match fixsendingtime \\|52[=][^|]\\+\\|syn match fixtransacttime \\|60[=][^|]\\+\\|syn match fixcorrelation \\|9717[=][^|]\\+\\|syn match fixside \\|54[=][^|]\\+\\|syn match fixprice 44syn match fixsendingtime |52syn match fixtransacttime \\|60syn match fixcorrelation \\|9717syn match fixside \\|54 define the default highlightingif version >= 508 || !exists(did_fix_syntax_inits) if version < 508 let did_fix_syntax_inits = 1 command -nargs=+ hilink hi link <args> else command -nargs=+ hilink hi def link <args> endif hilink fixprice label hilink fixsendingtime number hilink fixtransacttime include hilink fixcorrelation string hilink fixside identifier delcommand hilinkendiflet b:current_syntax = fixsample line of data:8=fixt.1.1^a9=380^a35=x^a49=cme^a56=0^a34=<phone>^a52=20130626135401572^a1128=9^a268=3^a279=0^a269=2^a48=42025^a22=8^a270=-224^a271=1^a273=135401000^a274=2^a451=32^a1020=23^a83=523628^a5799=1^a5797=1^a279=0^a269=2^a48=17311^a22=8^a270=12356^a271=1^a273=135401000^a277=1^a451=-395^a1020=201377^a83=<phone>^a5797=1^a279=0^a269=2^a48=122816^a22=8^a270=12580^a271=1^a273=135401000^a274=2^a277=1^a451=-427^a1020=31^a83=<phone>^a5797=2^a75=20130626^a10=001^anote that the ^a characters are the 0x1 unprintable characters, you can fix this file with::%s/\\^a/<01>/where <01> is entered with ctrl+v followed by 001.",
    "present_kp": [],
    "absent_kp": [
      "syntax highlighting",
      "regular expression"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "editor for markdown tables. is there any tool for linux which makes it easier to generate and edit tables for markdown?",
    "present_kp": [],
    "absent_kp": [
      "software rec",
      "editors",
      "wiki"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "bind mounting across different file systems. does the command mount -o bind allow mounting a folder from a different file system (vfat, ntfs) to a folder in linux native partition?",
    "present_kp": [
      "linux",
      "mount",
      "bind mount"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "server code to save uploaded files. this code basically saves an uploaded file to the server. i am wondering if there is anything that i can do to tighten this code up. i am very new to f# so i'm still having trouble breaking away from the c# way of doing things./// create file paths/// returns a tuple (server path * link path * file number)let createpath (file : httppostedfilebase) = // server directory path let serverdirpath = httpcontext.current.server.mappath(@~/uploads) // get the file name let origfilename = file.filename // get the extension let extension = path.getextension(origfilename) // get the file size in bytes let filesize = file.contentlength // directory check let pathexists() = directory.exists(serverdirpath) // create directory let createdir() = if not (pathexists()) then directory.createdirectory(serverdirpath) |> ignore createdir() // find current file name let findcurrentfilename() = // check if row exist let rowcount = query{ for row in db.uploads do select row count } // get file number let filenumber = if rowcount < 1 then 1 else query{ for row in db.uploads do select (row.filenumber + 1) head } // final path let finalservpath = serverdirpath + @\\ + filenumber.tostring() + extension // download link let linkpath = finalservpath.replace(serverdirpath + @\\, @~/uploads/) finalservpath, linkpath, filenumber findcurrentfilename()/// save file to server and path to db.let saveupload (file: httppostedfilebase) (title : string) = // create the path including filename let servpath, linkpath, filenumber = createpath file // save file to server file.saveas(servpath) // create new row for db table let newupload = new dbschema.servicetypes.uploads(title = title, filepath = servpath, size = file.contentlength.tostring(), filenumber = filenumber, linkpath = linkpath) // insert new row insertrowin db.uploads newupload // save to db savetodb()",
    "present_kp": [
      "f#",
      "http"
    ],
    "absent_kp": [
      "beginner"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is the proof-theoretic significance of the existence of a brown-palsberg self-interpreter for system $f_\\omega$?. in a self-interpreter for f-omega, brown and palsberg construct for each term of each type $x:t$ a representation $ar x : \\box t$ (a metatheoretical function which can't be represented in the language), and an interpretation function $u_t : \\box t o t$. this is reminiscent of gl provability logic, which in particular abstracts how pa can represent its own proof theory. but while the representation looks like the axiom $dash t \\implies dash \\box t$, the second $dash \\box t o t$ is inconsistent with gl. so what sort of gadget is this, in logical terms? can it be used at all to talk about the metatheory of f-omega?what would a representation operation that satisfies the axioms of gl look like?",
    "present_kp": [
      "proof theory"
    ],
    "absent_kp": [
      "type theory"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "configure splitflap screensaver. how do i configure the splitflap screen-saver? i was able to figure out all documented parameters like -no-spin or -no-wander. yet i haven't found a way to specify a text to show or select a site to fetch text from. there is a -mode text but no way to add text.so tl;dr how to prevent the screensaver from sayingerror loading urlhttp:/fridge.ubuntu.comnode/feed: 404 notfound(yes, there is only on slash after http: and no slash after .com)edit:the screensaver in question is located in /usr/lib/xscreensaver/ and the executable is called splitflap. i'm using linux mint 18.1 with mate and the screensaver came with the os.",
    "present_kp": [
      "screensaver"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "lower bound on worst case pancake number?. given n pancakes, for each permutation we can compute the minimum number of pancake flips. if we take the maximum over all possible permutations, we get the worst case pancake number $p_n$.i think i can prove that $p_n \\geq n$. my argument is that i can start from the sorted pancake, and do a anti-sorting of the pancake by first flipping at position n, then position at n-1, n-2, etc.for example, the case of n=5 would yield 34251.sorting such a pancake would take at best n steps.am i doing something wrong?",
    "present_kp": [
      "sorting"
    ],
    "absent_kp": [
      "lower bounds"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "when importing a key during a yum installation, where is that key stored?. i'm installing a piece of software for which i had to use a third-party repository. after adding the repository with rpm -uvh http://[site], i installed with yum install [package]. one of the prompts that came up during installation read:importing gpg key 0xabcabcab fname lname <<email> from /etc/pki/rpm-gpg/rpm-gpg-key-somewhere-fnameis this ok [y/n]: ydoes anyone know where this key is stored? i'm finished with that particular repo and don't want the additional repo/key lingering unnecessarily on my machine. i've already tried gpg --list-keys as both a regular user and root, but the key isn't listed there.as a work around would simply removing the repo also remove the key associated with it?",
    "present_kp": [
      "yum",
      "rpm",
      "gpg"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "kerberos error after archlinux installation. i've installed archlinux successfully on my laptop yesterday. to do this, i've followed the beginner's guide along with the raid guide. i could do every step without problem, except the one for the time zone (i got an error with ln -s and /etc/localtime). one other point differs from my installation : on the raid webpage, when you have to edit the hooks section, mine is different from the one on the pages. i understood that i only had to had mdadm_udev.however, when i try to boot my computer, archlinux seems booting fine. it looks like the os loads modules without problem, then i got the following failed :[failed] failed to start packet filtering framework.see 'systemct1 status iptables.service' for details.[failed] failed to start ipv6 packet filtering framework.see 'systemct1 status ip6tables.service' for details.then some modules are load again and then i got the last failure :[failed] failed to start kerberos 5 kdc.see 'systemct1 status krb5-kdc.service' for details.this error seems weird because before it comes, i have some lines like this :[ ok ] started kerberos 5 kdc. stopping kerberos 5 kdc...[ ok ] stopped kerberos 5 kdc. starting kerberos 5 kdc.after this, i have a black screen that looks like the terminal which asks me my login. does any have an idea how to fix this or is this normal ?ps : if you need all the step of the loading screen i can try to film the screen and type it here.edit : i don't know if this is a consequence of the failure of this module but i can't log in as root.",
    "present_kp": [
      "kerberos"
    ],
    "absent_kp": [
      "arch linux"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "iostat report huge writes to drives that's not even mounted. root@host [~]# fsck /home2fsck from util-linux-ng 2.17.2e2fsck 1.41.12 (17-may-2010)/dev/sdb1: clean, <phone>/91578368 files, 545<phone> blocksroot@host [~]# fsck /home4fsck from util-linux-ng 2.17.2e2fsck 1.41.12 (17-may-2010)/dev/sdd1: clean, <phone>/91578368 files, 759<phone> blocksfsck returns no errorroot@host [~]# lsof /home4root@host [~]# lsof /home2lsof returns no userroot@host [~]# mount/dev/mapper/volgroup-lv_root on / type ext4 (rw,relatime,usrjquota=quota.user,jqfmt=vfsv0)proc on /proc type proc (rw)sysfs on /sys type sysfs (rw)devpts on /dev/pts type devpts (rw,gid=5,mode=620)tmpfs on /dev/shm type tmpfs (rw,rootcontext=system_u:object_r:tmpfs_t:s0)/dev/sda1 on /boot type ext4 (rw)/dev/mapper/volgroup-lv_home on /home type ext4 (rw,relatime,usrjquota=quota.user,jqfmt=vfsv0)/dev/sdc1 on /home3 type ext3 (rw,relatime)none on /proc/sys/fs/binfmt_misc type binfmt_misc (rw)/usr/tmpdsk on /tmp type ext3 (rw,noexec,nosuid,loop=/dev/loop0)/tmp on /var/tmp type none (rw,noexec,nosuid,bind)sunrpc on /var/lib/nfs/rpc_pipefs type rpc_pipefs (rw)root@host [~]# iostat -xklinux 2.6.32-279.19.1.el6.x86_64 (host.buildingsuperteams.com) 01/06/2013 _x86_64_ (16 cpu)avg-cpu: %user %nice %system %iowait %steal %idle 18.91 0.02 39.17 20.22 0.00 21.67mount shows that there is sdd1 and sdb1 is not mounteddevice: rrqm/s wrqm/s r/s w/s rkb/s wkb/s avgrq-sz avgqu-sz await svctm %utilsda 0.16 11.93 1.35 3.30 53.91 59.92 48.95 0.10 21.87 3.70 1.72sdb 0.49 219.57 22.00 99.14 224.17 1275.44 24.76 7.44 61.38 7.45 90.24sdd 0.46 226.39 23.26 92.71 260.61 1277.34 26.52 0.67 5.77 7.71 89.40sdc 0.00 1.79 0.28 0.05 5.03 7.38 74.28 0.00 14.34 2.05 0.07dm-0 0.00 0.00 1.45 14.91 53.66 59.50 13.83 1.56 95.36 1.06 1.73dm-1 0.00 0.00 0.04 0.10 0.18 0.41 8.00 0.00 21.25 2.44 0.04dm-2 0.00 0.00 0.01 0.00 0.05 0.01 8.49 0.00 7.32 1.84 0.00iostat report huge writeswhat would the reason be? i will replace the hard disk anyway. but this puzzles me to no endthis caused a server crash already. i unmout the drive.iostat -x 1 shows empty, which is what's expected. so all this time i saw past data?",
    "present_kp": [
      "io",
      "fsck",
      "lsof"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "what language features would i need to remove from a real programming language to make it decidable?. let's say that i want to restrict certain features of a common programming language--for instance, c--such that the result is decidable, and thus no longer turing-complete. what language features, at minimum, would i need to remove to make a given programming language decidable?for the purpose of this question, let's assume the following:the original language in question is imperative.we're not restricting memory in any meaningful way.the result isn't hideously crippled (e.g. no subsets of c that only consist of printf calls).",
    "present_kp": [],
    "absent_kp": [
      "computability",
      "programming languages",
      "undecidability",
      "halting problem",
      "turing completeness"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "openswan established nated tunnel but, i can't ping remote end's lan pc. i am establishing site to site tunnel.192.168.2.0/24===10.103.6.40<10.103.6.40>---10.103.6.1...10.103.6.29<10.103.6.29>===192.168.1.0/24;local ip: 10.103.6.40local lan: 192.168.2.0/24remote lan is behind 10.103.6.29 (router) has lan 10.1.1.0/24.remote ip: 10.1.1.44 (behind 10.103.6.29, i have enabled vpn passthrough)remote lan: 192.168.1.0/24now, tunnel is established, but i can not ping remote lan's machine on 192.168.1.2.log of ipsec:mar 4 16:34:45 ng authpriv.warn pluto[9431]: listening for ike messagesmar 4 16:34:45 ng authpriv.warn pluto[9431]: adding interface br0/br0 192.168.2.1:500mar 4 16:34:45 ng authpriv.warn pluto[9431]: adding interface br0/br0 192.168.2.1:4500mar 4 16:34:45 ng authpriv.warn pluto[9431]: adding interface eth3/eth3 10.103.6.40:500mar 4 16:34:45 ng authpriv.warn pluto[9431]: adding interface eth3/eth3 10.103.6.40:4500mar 4 16:34:45 ng authpriv.warn pluto[9431]: adding interface lo/lo 127.0.0.1:500mar 4 16:34:45 ng authpriv.warn pluto[9431]: adding interface lo/lo 127.0.0.1:4500mar 4 16:34:45 ng authpriv.warn pluto[9431]: adding interface lo/lo ::1:500mar 4 16:34:45 ng authpriv.warn pluto[9431]: loading secrets from /etc/ipsec.secretsmar 4 16:34:46 ng authpriv.warn pluto[9431]: ng #1: initiating main modemar 4 16:34:46 ng daemon.err ipsec__plutorun: 104 ng #1: state_main_i1: initiatemar 4 16:34:46 ng authpriv.warn pluto[9431]: ng #1: received vendor id payload [openswan (this version) 2.6.38 ]mar 4 16:34:46 ng authpriv.warn pluto[9431]: ng #1: received vendor id payload [dead peer detection]mar 4 16:34:46 ng authpriv.warn pluto[9431]: ng #1: received vendor id payload [rfc 3947] method set to=115 mar 4 16:34:46 ng authpriv.warn pluto[9431]: ng #1: enabling possible nat-traversal with method rfc 3947 (nat-traversal)mar 4 16:34:47 ng authpriv.warn pluto[9431]: ng #1: transition from state state_main_i1 to state state_main_i2mar 4 16:34:47 ng authpriv.warn pluto[9431]: ng #1: state_main_i2: sent mi2, expecting mr2mar 4 16:34:47 ng authpriv.warn pluto[9431]: ng #1: nat-traversal: result using draft-ietf-ipsec-nat-t-ike (macos x): peer is natedmar 4 16:34:47 ng authpriv.warn pluto[9431]: ng #1: transition from state state_main_i2 to state state_main_i3mar 4 16:34:47 ng authpriv.warn pluto[9431]: ng #1: state_main_i3: sent mi3, expecting mr3mar 4 16:34:47 ng authpriv.warn pluto[9431]: ng #1: received vendor id payload [can-ikev2]mar 4 16:34:47 ng authpriv.warn pluto[9431]: ng #1: main mode peer id is id_ipv4_addr: '10.103.6.29'mar 4 16:34:47 ng authpriv.warn pluto[9431]: ng #1: transition from state state_main_i3 to state state_main_i4mar 4 16:34:47 ng authpriv.warn pluto[9431]: ng #1: state_main_i4: isakmp sa established {auth=oakley_preshared_key cipher=aes_128 prf=oakley_md5 group=modp1024}mar 4 16:34:47 ng authpriv.warn pluto[9431]: ng #1: dead peer detection (rfc 3706): enabledmar 4 16:34:47 ng authpriv.warn pluto[9431]: ng #2: initiating quick mode psk+encrypt+tunnel+pfs+up+ikev2allow+sareftrack {using isakmp#1 msgid:f5d74d20 proposal=aes(12)_128-md5(1)_128, aes(12)_192-md5(1)_128, aes(12)_256-md5(1)_128, aes(12)_128-sha1(2)_160, aes(12)_192-sha1(2)_160, aes(12)_256-sha1(2)_160, 3des(3)_192-md5(1)_128, 3des(3)_192-sha1(2)_160 pfsgroup=oakley_group_modp1024}mar 4 16:34:49 ng authpriv.warn pluto[9431]: ng #2: dead peer detection (rfc 3706): enabledmar 4 16:34:49 ng authpriv.warn pluto[9431]: ng #2: transition from state state_quick_i1 to state state_quick_i2mar 4 16:34:49 ng authpriv.warn pluto[9431]: ng #2: state_quick_i2: sent qi2, ipsec sa established tunnel mode {esp=>0x9143bd7d <0x1cb79f9a xfrm=aes_128-hmac_md5 natoa=none natd=10.103.6.29:4500 dpd=enabled}tunnel is established successfully.local configuration (at 10.103.6.40 side):config setup nat_traversal=yes oe=off protostack=netkeyconn ngpassthrough left=192.168.2.1 right=0.0.0.0 leftsubnet=192.168.2.0/24 rightsubnet=192.168.2.0/24 authby=never type=passthrough auto=routeconn ng right=10.103.6.29 rightsubnet=192.168.1.0/24 left=10.103.6.40 leftsubnet=192.168.2.0/24 leftnexthop=10.103.6.1 auto=start leftid=10.103.6.40 rightid=10.103.6.29 #x_rightdynamic=yes authby=secret compress=no failureshunt=drop dpddelay=15 dpdtimeout=60 dpdaction=restart pfs=yes ike=aes128-md5-modp1024,aes192-md5-modp1024,aes256-md5-modp1024,aes128-sha1-modp1024,aes192-sha1-modp1024,aes256-sha1-modp1024,3des-md5-modp1024,3des-sha1-modp1024,aes128-md5-modp1536,aes192-md5-modp1536,aes256-md5-modp1536,aes128-sha1-modp1536,aes192-sha1-modp1536,aes256-sha1-modp1536,3des-md5-modp1536,3des-sha1-modp1536,aes128-md5-modp2048,aes192-md5-modp2048,aes256-md5-modp2048,aes128-sha1-modp2048,aes192-sha1-modp2048,aes256-sha1-modp2048,3des-md5-modp2048,3des-sha1-modp2048 esp=aes128-md5,aes192-md5,aes256-md5,aes128-sha1,aes192-sha1,aes256-sha1,3des-md5,3des-sha1remote configuartion (at 10.1.1.44 side):config setup nat_traversal=yes oe=off protostack=netkeyconn ngpassthrough left=192.168.1.1 right=0.0.0.0 leftsubnet=192.168.1.0/24 rightsubnet=192.168.1.0/24 authby=never type=passthrough auto=routeconn ng right=10.103.6.40 rightsubnet=192.168.2.0/24 left=10.1.1.44 leftsubnet=192.168.1.0/24 leftnexthop=10.1.1.1 auto=start leftid=10.103.6.29 rightid=10.103.6.40 #x_rightdynamic=yes authby=secret compress=no failureshunt=drop dpddelay=15 dpdtimeout=60 dpdaction=restart pfs=yes ike=aes128-md5-modp1024,aes192-md5-modp1024,aes256-md5-modp1024,aes128-sha1-modp1024,aes192-sha1-modp1024,aes256-sha1-modp1024,3des-md5-modp1024,3des-sha1-modp1024,aes128-md5-modp1536,aes192-md5-modp1536,aes256-md5-modp1536,aes128-sha1-modp1536,aes192-sha1-modp1536,aes256-sha1-modp1536,3des-md5-modp1536,3des-sha1-modp1536,aes128-md5-modp2048,aes192-md5-modp2048,aes256-md5-modp2048,aes128-sha1-modp2048,aes192-sha1-modp2048,aes256-sha1-modp2048,3des-md5-modp2048,3des-sha1-modp2048 esp=aes128-md5,aes192-md5,aes256-md5,aes128-sha1,aes192-sha1,aes256-sha1,3des-md5,3des-sha1firewall rule on 10.103.6.29:-a prerouting -p udp -m udp --dport 500 -j dnat --to-destination 10.1.1.44:500 -a prerouting -p udp -m udp --dport 4500 -j dnat --to-destination 10.1.1.44:4500 i have two pc's at both end. on local side pc has ip 192.168.2.2 and at remote side pc has ip 192.168.1.2.i can ping remote router (192.168.1.1) but, can't access/ping remote lan pc (192.168.1.2).help me please.",
    "present_kp": [
      "vpn",
      "ipsec"
    ],
    "absent_kp": [
      "networking"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "on finding coin jams. the problem can be found here and is essentially asking us to generate j many numbers containing n many digits, where these digits have to be either 0 or 1. the catch is that the these numbers that we generate have to be composite when interpreted as being written in base i, where i ranges from 2 to 10, inclusive. furthermore, these digits have to begin and end with a 1. my solution was to randomly generate candidate numbers and test for compositeness across the bases. however, i am wondering if perhaps there is a better way to solve the problem (with the interest of speed)? for perusal, my code is as follows (note that i believe it to be correct, but slowy):import sysimport random# converts a list of 1's and 0's interpreted in base i, to base 10 equivalentdef base_i_to_base_10(binary, i): return sum([binary[j]*(i**(len(binary)-j-1)) for j in range(len(binary))])# checks primality of a number# if prime, return -1# if composite, return the divisor that indicated sodef isprime(num): for i in range(2,int(num**0.5)+1): if num%i==0: return i return -1# function to check if a number (whose digits are 0 and 1) is prime# primality is tested by converting the number into bases 2 through 10# if prime, we return []; representing that no divisors exist# if composite, we return a ['some divisor'] as proofdef notprime(binary): thefactors = [] # iterates over bases 2 through 10 for i in range(2,11): num = base_i_to_base_10(binary, i) # number after converting to base i divisor = isprime(num) if divisor==-1: return [] # the number was prime in base i else: thefactors += [divisor] # the number is not prime in base i return thefactors # the number is composite in bases 2 through 10# reads in how many test cases there arecasenums = int(sys.stdin.readline().strip( ))case = 0 # initalize case count to zero# read lines from standard inputfor line in sys.stdin: case += 1 # each line contains two numbers # n is the number of digits the binary number contains # by binary we mean made up of 1's and 0's, and not necessarily in base 2 # j is the number of binary digits that are not prime in any of the bases # 2 through 10 and begin and end with 1, that we would like to generate n, j = [int(i) for i in line.strip(' ').split(' ')] print case #{}: .format(case) # print out test case number valid = [] # list of valid binary numbers satisfying the requiremnts seen = [] # list of binary numbers we've considered already # randomly generate a candidate binary number binary = [1]+[int(random.random() > 0.5) for i in range(n-2)]+[1] # a list of divisors, one from each base 2 through 10 # if the number is not prime in atleast one of the above bases, divs = [] divs = notprime(binary) # while we haven't generated j many valid binary numbers, keep searching while (len(valid)<j): # if the randomly generated binary number isn't one we've seen already # and if it is prime across all bases 2 through 10 # record the binary number as valid if ( not (binary in seen) and len(divs)==9): # dispay binary number and some of its divisors num = ''.join([str(i) for i in binary]) + for i in divs: num += str(i)+ num = num.strip(' ') print num valid += [binary] # mark binary number as valid seen += [binary] # mark the binary number as already seen # generate a new binary number to consider, and some of its divisors binary = [1]+[int(random.random() > 0.5) for i in range(n-2)]+[1] factors = notprime(binary)",
    "present_kp": [
      "random"
    ],
    "absent_kp": [
      "python",
      "algorithm",
      "programming challenge"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to measure the cross-entropy of prior and posterior distribution with re-parameterization trick?. given a conditional prior $p_{ heta}(z|x)$ and a posterior distribution $p_{\\phi}(z|x,y)$. i want to prameterize both of them using deep neural network. and the objective function of $ heta$ is $\\max_{ heta} e_{p_{\\phi}}\\log p_{ heta}(z|x)$where $z$ is a vector of size $k$.since the objective function is equivalent to $h(p_{\\phi})+kl(p_{\\phi}||p_{ heta})$.thus we can just take the kl-divergence term into account when we want to take the gradient of the objective function with respect to $ heta$. but i have no idea about what should be following. i wonder how can i use the re-parameterization trick to construct a differentiable estimator with respect to $ heta$? thanks in advance!",
    "present_kp": [],
    "absent_kp": [
      "machine learning",
      "deep learning",
      "optimization",
      "bayesian"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "supply input to a program with a password prompt. is there any linux command which supplies multiple enter keys to another process which is running in a terminal till the other process dies?for example consider the command yes ' ' | ./ssr-sim prepare. i need a command on the lhs side of the pipe(|). for example instead of yes ' ' on the left of |, another command which supplies multiple enter keys to the ./ssr-sim prepare on the right side.i wanted to automate enter keys inside a shell script and the command ./ssr-sim prepare produces output which asks for password multiple number of times and the user needs to manually press the enter key every time (no need to give password here, only press enter each time when it asks for password). i need to automate that pressing of enter key part.the command yes ' ' | ./ssr-sim prepare is not working.",
    "present_kp": [
      "shell script",
      "terminal"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "multiple independent random number streams. having multiple streams of pseudo-random numbers known to be independent and with a uniform distribution i want to do monte carlo simulations in parallel.in other words, one thread will have a full-period independent and uniformly distributed stream of pseudo-random numbers. each thread will consume these numbers in four different functions (a,b,c,d).my concern is about the distribution across threads for each function. thread.1 func_a.1, thread.2 func_a.2... and so on. do i still need to make sure this distribution is indeed uniform across func_a1, func_a2, etc? failing to do so can make my simulation have flaws?in summary,if i start using the pseudo-random numbers in a random fashion, rejection sampling. can i still be sure of the normal distribution among the different parts?",
    "present_kp": [
      "monte carlo"
    ],
    "absent_kp": [
      "pr.probability",
      "randomness",
      "dc.parallel comp",
      "na.numerical analysis"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "delete all inodes besides one under all instances of dir0/dir1 under var/www/html, in one command. i have the dir var/www/html and under it there are a few website dirs (say, about 5).all of the 5 website dirs have an internal path dir0/dir1.how could i bulk delete all inodes inside this path (besides one inode named he_il.mo), but in one command?i ask about one command since i have the following block of 3 commands that works, but i would like to go minimal as much as i can with this:(find /var/www/html/*/dir0/dir1/ ! -name 'he_il.mo' -type f -exec rm -f {} + find /var/www/html/*/dir0/dir1/ -type f -exec rm -d {} +find /var/www/html/*/dir0/dir1/ -type f -exec rm -l {} + )if i do * instead of f i get should contain only a letter.if i do i instead of f, i get a unknown argument.",
    "present_kp": [],
    "absent_kp": [
      "command line"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is it stable to use epel and rpmforge in the same time?. so after a fresh scientific linux 6.3 desktop install, i:yum install rpmforge-release && yum updateand yum install epel-release.noarchthen: yum install wine gparted cups-pdf && echo 'includepkgs=nss-mdns wine* gparted cups-pdf' >> /etc/yum.repos.d/epel.repook! after this i: vi /etc/yum.repos.d/epel.repoincludepkgs=nss-mdns wine* gparted cups-pdfso this is how i install a fresh wine (+cups-pdf) and gparted. q: is this a stable thing to do? i mean can this cause problems in the future? (that there are some epel packages installed, but i use a whitelist after installing the mentioned few packages to help stability - because i only need epel for the few things gparted/wine/cups-pdf). will these stepes cause problems in the future?",
    "present_kp": [
      "yum",
      "rpm",
      "scientific linux"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "idapython: change struct member content name. tl;dri have an array of structs, i'd like to iterate over it and change fields content, (name, values), upon simple conditions.i can get the offset of the members, but i can't modify them due to the aforementioned error.i have a the following structure in ida:load:86b70b00 22 f6 dc 86 24 f6 dc+stru_86b70b00 commands_array <asc_86dcf622, aqueryorsetansr, 2, \\load:86b70b00 86 02 00 00 00 89 f6+ ; data xref: sub_85c73bbc+42oload:86b70b00 d2 85 df f6 d2 85 00+ ; load:off_85c73d78oload:86b70b00 00 00 00 00 00 00 00+ sub_85d2f688+1, sub_85d2f6de+1>; 0 ; s ...load:86b70b00 3f f6 dc 86 45 f6 dc+ commands_array <0, 0, 0x86dcf63f, byte_nothing, sub_0+2>; 1load:86b70b00 86 02 00 00 00 00 00+ commands_array <0, 0, 0, sub_0, asc_86dcf646>; 2load:86b70b00 00 00 00 00 00 00 00+ commands_array <adial, sub_0+3, 0x85caeca9, sub_85caecac+1,\\load:86b70b00 00 00 00 00 00 00 00+ a_end__2>; 3i would like to iterate over it and change the name of each of its membersby examining each member.getting the offset to the structure member was sort of easy, id = getstrucidbyname(struct_name)print 0x%08x % (((base + (struct_size * indx)) + getmemberoffset(id, func)))however, whenever i try to makename(offset, new_name)i get the following error:can't rename byte as 'func' because this byte can't have a name (it is a tail byte).i guess i'm doing something wrong here, but i can't seem to find the right way to access ida structures and handle them properly.help would be appreciated and compensated by points, beer and re-se fame.",
    "present_kp": [
      "idapython"
    ],
    "absent_kp": [
      "idapro plugins"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "ssh key based login bypasses password policies. i noticed that logins via an ssh key bypass the ldap password policies(password ageing, password warning, and password lockout due to failedattempts, etc). is there any way to force key-based ssh logins torespect the password policies?my clients are old (rhel 4), so installing sssd is not an option.",
    "present_kp": [
      "ssh",
      "password",
      "ldap"
    ],
    "absent_kp": [
      "security",
      "openldap"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "neuroevolution is it not supervised learning?. if i compare back-propagation to feed-forward neuro-modulation, the latter is unsupervised (requires no labeled data set).but putting it into a genetic algorithm to refine topology and weights, the ga will require fitness function, which means you need a labeled data set to compare with.would that renders ff neuro-modulation a supervised learning in that case?is there any way to get unsupervised learning (i have no labeled datasets) using neuroevolution?",
    "present_kp": [
      "unsupervised learning"
    ],
    "absent_kp": [
      "neural networks",
      "genetic algorithms"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is it fair to use shortened url to mention and give links of live projects in my resume?. i have done or was part of some mobile projects that are uploaded in their respective markets and currently live, and i am already mentioning status as live for respective projects.one of my friend -who is actually a web developer- has recently suggested me to put the urls of the application if the project is live. and i am taking this suggestion seriously. but i have found that some urls are really long and they desperately need to be shortened. so my question is : is it fair to shorten all the urls of the live application? or just shorten those which are so long.in case i shorten all the urls, can any employer take this point negatively? thanks edit : after reading your response i think i need to clarify one thing.... the applications i have developed are mobile applications and they are uploaded in their respective markets, like blackberry app store and android markets. it means a lot for an application accepted to be sold in market. and i want to put the url of these market sites. the links are there for a long time, so there is no threat that the link goes offline in a year or two.....",
    "present_kp": [
      "resume"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "minimizing the products of variables. my problem maximize$$\\min_{i} \\{\\ c_i \\cdot \\prod_{j \\in a(i)} {x_{j}} \\prod_{j \\in b(i)} {y_{j}} \\} $$subject toegin{align}&\\sum_{j \\in c(k)} x_{j} = 1,\\ orall k \\&l \\leq x_{j} \\leq u,\\ orall j\\&\\sum_{j \\in d(k)} y_{j} = 1+p,\\ orall k\\&y_{j} \\in \\{1,\\ p\\},\\ orall j\\end{align}where $c_i, l, u, p$ are positive constants less than 1, and sets $a, b, c, d$ are also given. what i have tried i have tried to use ncpol2sdpa to relax the polynomial programming into a semi-definite programming and call the sdpa solver to solve it. the objective is replaced with a new variable $ \\ f$, and the following constraints are added$$\\ f \\leq c_i \\cdot \\prod x_{j} \\prod y_{j}, orall i\\, .$$each discrete variable $y_i$ is replaced by $y_i = p \\cdot (1-z_i) + z_i$, where $z_i = \\{0,\\ 1\\}$ or equivalently, $z_i^2-z_i=0$.however even for a small problem (5 $x$'s and 8 $y$'s), it took hours for ncpol2sdpa to relax and sdpa to solve the relaxed problem. the level of relaxation is set to be 3.i also tried to relax the constraint on $y$ to be continues, but still quite slow.i wonder if i did anything wrong with both tools? or is there a better method/solver for this problem?",
    "present_kp": [],
    "absent_kp": [
      "optimization",
      "nonlinear programming",
      "nonconvex"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what drops faster from the google index? a page that 301 redirects to another one or one that gives a 404 error?. what kind of page drops faster from google's index? one that is 301 redirected to another page or one that gives a 404 status?in regards to the first option i would expect the new page to take the place of the redirected one. however, will this happen faster than google would drop a 404 page from their index?thank you",
    "present_kp": [
      "google",
      "404"
    ],
    "absent_kp": [
      "indexing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "need new secure host. possible duplicate:how to find web hosting that meets my requirements? my website is being attacked too much lately and my provider is unresponsive. logs show dozens of attempts to run mysql setup.php scripts from various locations, then the logs vanish. time to find another host.i lack the skill to config and run my own server -- actually, i'd have no problem setting it up and running it, i'm just not current on linux security -- so i must rely on a competent host.can anyone recommend a linux-based hosting service with solid security and reasonable bandwidth rates? i will be selling a program so there will be a large number of downloads.i am in canada, so usa or canada is preferred. all advice is welcome. thanks.",
    "present_kp": [
      "web hosting",
      "linux"
    ],
    "absent_kp": [
      "looking for hosting"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "graph theory problem (name unknown). i am trying to solve the following kind of problem. i do not know if there is already a name for this, or a solution; however, i'm willing to bet there is. i was hoping someone could point me in the direction of implementing a solution for it, or at least tell me the name of the problem?suppose a traveler has a certain amount of gold coins, and some bronze coins. he must start a city a, and go to city b, then city c, and finally city d.there are two (or more) roads that pass from a to b, labeled ab1, and ab2 (etc.).road ab1 has a toll of 5 gold coins, and ab2 has a toll of 5 bronze coins.roads from city b to c: bc1 has a toll of 10 coins, which can be either currency. bc2 requires 8 gold coins.roads from c to d have some sort of similar setup.assuming that we know how much money the traveler has, and that there is no possible exchange between bronze and gold coins: is there a method to determine if the traveler has enough money to pass through from a to b, to c, and finally to d?this is the kind of problem i need a solution to... is there a name for this problem? is there a solution to this problem (other than brute force)? i'm assuming this is a similar problem to flow problems, but i don't know quite how to approach it.",
    "present_kp": [],
    "absent_kp": [
      "graph traversal"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "can i get sued over duplicating another program's ui?. if i see something i like in another application, and create something similar from the ground up in a commercial product, could i get sued over it? of course it would be styled to look differently, however most of the functionality would be the same since the functionality is what i liked about it. it also would not be the entire system, just a part of one.i live in the us.",
    "present_kp": [],
    "absent_kp": [
      "copyright"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "feh: maintain zoom setting. when browsing through the images in a directory with feh, the magnification can be changed by use of the up/down arrow keys, but the setting is lost as soon as the next image in the series is loaded. is there a way to apply the magnification setting selected for image i to image i+1? if not, is this possible in another lightweight image viewer?",
    "present_kp": [
      "images",
      "feh"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "basic php factory pattern. i have an object, kbarticle, that originally in it's constructor, would try to return null if the sql query that retrieves the data for its required properties came up with 0 rows. a friend told me that constructors shouldn't return null, and that i should look into the factory pattern. i did and i see the benefits, so i'm wondering how i can implement this better/properly.quick overview: knowledgebase.php is the page an end-user sees, it calls a function in factory.php which calls the constructor in kb_fnc.php and then it ends up creating the object back on knowledgebase.php if things all go well.knowledgebase.php<?php if ($kb = factory::make('kbarticle', $_get['a'])) { $kb->echonavtrail();} else { die('failed constructor');}?>factory.phpnote: it disappoints me that apparently the functions in here have to be public instead of private, because my factory method is static? this seems not ideal, or am i incorrect?<?phprequire_once('kb/kb_fnc.php');class factory { public static function make($obj, $params) { $succ = false; switch($obj) { case('kbarticle'): return trynewknowledgebasearticle($params); break; } } }function trynewknowledgebasearticle($id) { try { $kb = new kbarticle($id); } catch (exception $e) { return null; } return $kb; }?>kb_fnc.phpnote: is throwing an exception here smart? in my quick tests yesterday, i couldn't access the custom string on the exception after it was chained back, so it seems not that usefulat least not without knowing how to log that string to console at the end or something, for debugging purposes.class kbarticle { function __construct($id) { //do sql if (odbc_num_rows($res) > 0) { //fill in properties with data from table } else { throw new exception('kba constructor failed sql query'); } }}my biggest questions are regarding:does this look sensible?can i access that custom exception string in my factory class, so i can ultimately encode it in json for a console.log for easier debugging if things go awry down the line?the functions in my factory class being public instead of encapsulated as private bothers me.",
    "present_kp": [
      "php"
    ],
    "absent_kp": [
      "design patterns"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "validate text field to hold only digits. is there anyway to check whether a textbox field contains just number and not alphabetic or special characters? i have the need to have the user enter a player's uniform number, but need to allow the user to enter 00 through 99. the 00 entry can't be converted to 0 so using a number box doesn't work.",
    "present_kp": [],
    "absent_kp": [
      "cognito forms"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how can i find the list of all of vim's default keybindings?. where can i find a list of all the default vim key bindings/mappings? something like this alphabetical listing for vi, but specifically for vim, not vi.i know that i can use :map, :imap, :nmap, etc, however these show me the mappings based on my current configuration. i'm aware of this question, but don't believe it to be a duplicate because it is about the current mappings based upon the user's configuration. i'm specifically looking for the default mappings.how do i see the defaults?",
    "present_kp": [
      "key bindings"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how can i remove lines under a sub heading in a file using linux commands. i have a file as below . in which i want to remove lines under a particular sub heading alone. i tried using some sed and awk commands but i couldn't get it. can any one help to crack this with some linux commands.[first attempt]a=10b=20[second attempt]a=20b=20[third attempt ] a=30b=50i want to remove lines under '[second attempt]' sub heading alone. the output should be as below. i just want to remove the contents under the sub heading and optionally the removed lines can be replaced by one blank line[first attempt]a=10b=20[second attempt][third attempt ] a=30b=50",
    "present_kp": [
      "sed",
      "awk"
    ],
    "absent_kp": [
      "text processing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "recreating strstr in c. i'm trying to recreate strstr in c, my code works but i think it is too long, and it needs some improvement.#include <stdlib.h>static int find_str_indx(char *str, int index, char *str_to_find){ int i; int temp; int found_index; while (str[index] != '') { if (str[index] == *str_to_find) { temp = index; i = 0; found_index = index; while (str_to_find[i] != '' && str[index] != '') { if(str_to_find[i] != str[index]) found_index = -1; i++; index++; } if (found_index == -1) index = temp; } index++; } return (found_index);}char *my_strstr(char *str, char *to_find){ char *found_str; int found_index; int i; found_str = (char*)malloc(sizeof(found_str)); found_index = find_str_indx(str, 0, to_find); if (found_index != -1) { i = 0; while (str[found_index] != '') { found_str[i] = str[found_index]; found_index++; i++; } } else found_str = null; return (found_str);}updatei have rewritten the code to get rid of allocating memory inside strstr, as advised by ratchet freak and jerry coffin.char *my_strstr(const char *haystack, const char *needle){ size_t needle_len; needle_len = strlen(needle); while (*haystack) { if (*haystack == *needle) { if (!strncmp(haystack, needle, needle_len)) return ((char *)haystack); } haystack++; } return (null);}",
    "present_kp": [
      "c"
    ],
    "absent_kp": [
      "strings",
      "reinventing the wheel"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "release/change management - best aproach. i asked this question an year ago in stackoverflow and never got a good answer. since programmers seems to be a better place to ask it, i'll give it a try...what is the better way to work with release management? more specifically what would be the best way to release packages?for example, assuming that you have a relatively stable system, a good quality assurance process (qa), etc. how do you prefer to release new versions?let's assume that we are talking about a mid to large centralized web system (no clients), in-house development. this system can be considered vital for a corporate operations.i have a tendency to prefer to do this by releasing packets at regular intervals, not greater than 1 to 3 months. during this period, i will include into the package,fixes and improvements and make the implementation in production environment only once.but i've seen some people who prefer to place small changes in production, but with a greater frequency.the claim of these people is that by doing so, it is easier to identify bugs that have gone through the process of qa: in a package with 10 changes and another with only 1, it is much easier to know what caused the problem in the package with just one change... what is the opinion came from you?",
    "present_kp": [
      "release",
      "change"
    ],
    "absent_kp": [
      "project management"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "psu recommendation for new casual gaming setup. i'm looking for a psu recommendation for a setup below:corsair 650d chassisintel i5 4690k cpuasrock extreme6 moboradeon r9 290 gpu16gb ram (2x 8gb)1x ssd 256gb1x 2tb sata hdd2x 1tb sata hdd as raid1 and to have some wiggle room to oc.price range below $200",
    "present_kp": [
      "gaming"
    ],
    "absent_kp": [
      "power supply"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "in facebook, is it possible to delete multiple friends?. i'm looking for a way to delete multiple friends. i created my facebook account years ago, when i was still in high school, and it was a thing to basically add everybody in the school. now, 5 years later, ive got over 3000 friends. i want to remove all these people and bring my list down to two digits. creating a whole new account seems redundant and counter-productive.if there is no way to do this, could somebody kindly point me in the right direction to creating said option myself? i have html5 and css experience. my javascript isn't the best but, i learn more and more everyday.",
    "present_kp": [
      "facebook"
    ],
    "absent_kp": [
      "facebook timeline"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "date - years prior to 1901 are treated as invalid. i have date (gnu coreutils) 7.1 installed on my system.if i try to check dates prior to 14-dec-1901, i get an invalid date error. for example, $ date -d 1901-12-13 date: invalid date '1901-12-13' $ date -d 1901-12-14 sat dec 14 00:00:00 est 1901what should i do to make the date utility to treat years prior to 1901 as valid?i receive similar errors for dates after 19-jan-2038",
    "present_kp": [
      "date"
    ],
    "absent_kp": [
      "shell script"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "continue and break for static analysis. i know there have been a number of discussions of whether break and continue should be considered harmful generally (with the bottom line being - more or less - that it depends; in some cases they enhance clarity and readability, but in other cases they do not).suppose a new project is starting development, with plans for nightly builds including a run through a static analyzer. should it be part of the coding guidelines for the project to avoid (or strongly discourage) the use of continue and break, even if it can sacrifice a little readability and require excessive indentation? i'm most interested in how this applies to c code.essentially, can the use of these control operators significantly complicate the static analysis of the code possibly resulting in additional false negatives, that would otherwise register a potential fault if break or continue were not used? (of course a complete static analysis proving the correctness of an aribtrary program is an undecidable proposition, so please keep responses about any hands-on experience with this you have, and not on theoretical impossibilities)thanks in advance!",
    "present_kp": [
      "c",
      "static analysis"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "in what area is lisp's macro better than ruby's ability to create dsl. one of things that makes ruby shine is the ability to create domain specific languages better, likesinatrarspecrakeruby on rails' activerecordthough one can duplicate these libraries in lisp through macro, i think ruby's implementation is more elegant. nonetheless, i think there are cases that lisp's macro can be better than ruby's, though i could not think of one.so, in what area is lisp's macro better than ruby's ability to create dsl, if any?updatei've asked this because modern programming languages are approaching the lisp singularity, likec got macro expansion preprocessor, though very primitive and prone to errorc# has attributes, though this is a read-only, exposed through reflectionpython added decorator, that can modify the behavior of the function (and class for v 3.0), though feels quite limited.ruby tmtowtdi that makes elegant dsl, if care is applied, but in ruby way. i was wondering if lisp's macro is only applicable to special cases and that the other programming language features are powerful enough to raise the abstraction to meet the challenges today in software development.",
    "present_kp": [
      "ruby",
      "lisp",
      "dsl"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "open source login solution. authentication is such a general problem, which most websites have to implement. there are a few commercial solutions, but all lack sufficient functionality to customize the registration process. therefore, i am looking for an open-source alternative. i am using php and with postgresql as database, but as far as i understand one could utilize authentication solutions using other technologies and integrate them into our site in various ways. therefore, i am looking for such solutions in any technology apart from those requiring microsoft infrastructure... i would prefer open source solution, which have already implemented the following features:has password recovery procedureusername is the email address of the userhas remember me functionailty (meaning that the user is logged inautomatically without seeing the login page)email address verificationgoogle has gotten me nowhere on this and neither a search on this site...",
    "present_kp": [
      "authentication",
      "open source"
    ],
    "absent_kp": [
      "linux"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "ext4 directory lists as a file. i'm working with a large (8tb) ext4 filesystem in linux. after a power outage, the filesystem wouldn't mount. fsck is taking weeks to complete, but i ran testdisk and i'm able to see my partition and the files and directories of the root. the problem is the directory that contains most of the data on the disk is in a certain folder that when listed in testdisk lists as a directory. the d at the beginning of the file permissions is not set. is there any way to fix this or browse to the directory? can i search for the directories that sit below this unusable directory? thanks for your help!",
    "present_kp": [
      "linux",
      "ext4"
    ],
    "absent_kp": [
      "filesystems",
      "raid",
      "inode"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to search collection of podcasts (.mp3 files)?. i have a collection of podcasts, in particular the talkingmachines podcasts (human conversation about machine learning), see <url> are 22 episodes, each 1 hour long. mp3 files are available on my local disk.now when i want to search the audio files for the episode where they talk about, say, the kmeans++ algorithm , what should i do?specifically, i want to search for that episode where a male voice (ryan) talks about kmeans++ in the first half of the podcast. again, what should i do? is there a desktop tool available that can do this search? should i upload the files to some webservice that can do audio recognition?",
    "present_kp": [
      "search",
      "audio recognition"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "can changes in memory (stack or hex dump) be highlighted as are register changes in ollydbg?. in ollydbg, if an instruction causes a register to change, it is highlighted red in the registers window in the cpu view.is it possible to have the same happen in the hex dump or stack windows? of course the area of memory being watched would have to have limits, perhaps only what is seen, or between some limits?",
    "present_kp": [
      "ollydbg"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "kindly review this class. what i can think of improving is:show stack on screen to fail gracefully i.e. improved catch part of try..catchmake the try part shorter to improve readabilitymake imports with * to improve readability so you don't have to import the same package twicesource code://source is probably public domain since prv.se is a public authority:package se.prv.pandora.arendeprocess.actionhandler;import java.util.list;import org.apache.log4j.logger;import se.prv.framework.forms.iformdata;import se.prv.framework.forms.iformpattern;import se.prv.framework.general.action;import se.prv.framework.general.isessionhandler;import se.prv.pandora.arendeprocess.daos.ansokandao;import se.prv.pandora.arendeprocess.entity.ansokan;import se.prv.pandora.arendeprocess.exceptions.pandoradaoexception;import se.prv.pandora.arendeprocess.forms.namnsokningformpattern;import se.prv.pandora.arendeprocess.general.pandoraactionhandler;import se.prv.pandora.arendeprocess.obj.ansokaninfo;import se.prv.pandora.arendeprocess.obj.arendeprocesssessiondata;import se.prv.pandora.arendeprocess.obj.arendesearchadmin;import se.prv.pandora.arendeprocess.obj.arendesokinfo;import se.prv.pandora.arendeprocess.util.arendecomparatormanager;import se.prv.pandora.arendeprocess.util.arendeprocesslistcomparator;import se.prv.pandora.arendeprocess.util.arendesoklistcomparator;import se.prv.pandora.arendeprocess.util.lookuphelper;import se.prv.pandora.arendeprocess.util.menumanager;import se.prv.pandora.arendeprocess.util.pandoraansokanhandler;import se.prv.pandora.arendeprocess.util.pandorafieldconstants;/** * @author niklas rosencrantz (adbnro) * */public class arendesokningactionhandler extends pandoraactionhandler { private arendeprocesssessiondata sessiondata; private ansokandao ansokandao; private int initialsort = arendeprocesslistcomparator.inkom_first; private final static logger logger = logger.getlogger(arendesokningactionhandler.class); protected iformdata getformdata() { return null; } protected iformpattern getpattern() { return namnsokningformpattern.getinstance(); } public arendesokningactionhandler(){ ansokandao = lookuphelper.lookup(se.prv.pandora.arendeprocess.daos.ansokandao.class); } protected void performaction(isessionhandler sessionhandler, action action) { string returnpage = null; list<ansokaninfo> ansokaninfolist = null; list<ansokaninfo> sortedlist = null; arendesokinfo arendesokinfo = null; arendecomparatormanager compmanager = null; arendesearchadmin admin = new arendesearchadmin(); pandoraansokanhandler ansokanhandler = new pandoraansokanhandler(); try { sessiondata = (arendeprocesssessiondata) sessionhandler.getsessiondata(); menumanager menumanager = sessiondata.getmenumanager(); returnpage = menumanager.getlatestdestination(action.getactiontarget()); arendesokinfo = sessiondata.getarendesokinfo(); if(arendesokinfo != null) { if(arendesokinfo.getarendecomparatormanager() != null) { compmanager = arendesokinfo.getarendecomparatormanager(); } else { compmanager = new arendecomparatormanager(); } } else { arendesokinfo = new arendesokinfo(); compmanager = new arendecomparatormanager(); } iformdata formdata = sessiondata.getformdata(); //ansokaninfo.seteditpersoninfo(null); if(action.getactioncommand().equalsignorecase(choose)){ // todo: ska valideras //savegrunduppgifter(formdata, ansokaninfo, ansokan); returnpage = sessiondata.getlatestaction().getcurrpage(); //arendeprocess_grunduppgifter.jsp } else if(action.getactioncommand().equalsignorecase(search)){ logger.info(i search); string search_arende = formdata.getvalue(pandorafieldconstants.field_search_arende); list<ansokan> ansokningar = ansokandao.search(search_arende); sessiondata.setarendesokninglista(ansokanhandler.getansokaninfofromansokan(ansokningar)); returnpage = sessiondata.getlatestaction().getcurrpage(); } compmanager.setlastsortnr(initialsort); arendesoklistcomparator comp = new arendesoklistcomparator(initialsort); sortedlist = sortformlist(sessiondata.getarendesokninglista(), comp); admin.setresultlist(sortedlist); admin.setnbofhits(sortedlist.size()); arendesokinfo.setarendesearchadmin(admin); arendesokinfo.setansokaninfolist(ansokaninfolist); arendesokinfo.setarendecomparatormanager(compmanager); sessiondata.setarendesokinfo(arendesokinfo); action.setreturnpage(returnpage); } catch (exception e) { logger.error(arendesokningactionhandler: performaction() , e); throw new pandoradaoexception(e, performaction(), error. transaction must be rolled back: + e.getmessage()); } } list<ansokaninfo> sortformlist(list<ansokaninfo> resultlist1, arendesoklistcomparator comp) { java.util.collections.sort(resultlist1, comp); return resultlist1; }}",
    "present_kp": [
      "java",
      "search"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "bash script have problems depending on desktop environment. how to fix this?. system: ubuntu 16.04.1 xenial xerushi, i'm having a strange problem with my bash script used to control the gpu fan speed.this script changes the gpu fan speed if the gpu passes to another temperature range. code:#!/bin/bashinterval=5while true; docurrent_temp='nvidia-settings -query gpucoretemp| grep gpu | perl -ne 'print $1 if /gpucoretemp.*?: (\\d+)./;''if (($current_temp < 45)); then nvidia-settings -a [gpu:0]/gpufancontrolstate=1 -a [fan:0]/gputargetfanspeed=40elif (($current_temp > 45)) && (($current_temp < 60)); then nvidia-settings -a [gpu:0]/gpufancontrolstate=1 -a [fan:0]/gputargetfanspeed=50elif (($current_temp > 60)) && (($current_temp < 65)); then nvidia-settings -a [gpu:0]/gpufancontrolstate=1 -a [fan:0]/gputargetfanspeed=60elif (($current_temp > 65)) && (($current_temp < 70)); then nvidia-settings -a [gpu:0]/gpufancontrolstate=1 -a [fan:0]/gputargetfanspeed=70elif (($current_temp > 70)) && (($current_temp < 75)); then nvidia-settings -a [gpu:0]/gpufancontrolstate=1 -a [fan:0]/gputargetfanspeed=80elif (($current_temp > 75)); then nvidia-settings -a [gpu:0]/gpufancontrolstate=1 -a [fan:0]/gputargetfanspeed=100fisleep $intervaldonein unity de, gnome and kde, this script works flawlessly using ./nvautofan.sh, but this script presents an error if i call the same command in xfce4 (using bash -x ./nvautofan.sh for debugging):gnome debug output (expected):+ interval=5+ true++ perl -ne 'print $1 if /gpucoretemp.*?: (\\d+)./;'++ grep gpu++ nvidia-settings -query gpucoretemp+ current_temp=37 # proper output+ (( 37 < 45 ))+ nvidia-settings -a '[gpu:0]/gpufancontrolstate=1' -a '[fan:0]/gputargetfanspeed=40'attribute 'gpufancontrolstate' (linux-rig:0[gpu:0]) assigned value 1.attribute 'gputargetfanspeed' (linux-rig:0[fan:0]) assigned value 40.+ sleep 5# ...nvidia-settings -query gpucoretemp output (gnome)attribute 'gpucoretemp' (linux-rig:0.0): 39. 'gpucoretemp' is an integer attribute. 'gpucoretemp' is a read-only attribute. 'gpucoretemp' can use the following target types: x screen, gpu.attribute 'gpucoretemp' (linux-rig:0[gpu:0]): 39. 'gpucoretemp' is an integer attribute. 'gpucoretemp' is a read-only attribute. 'gpucoretemp' can use the following target types: x screen, gpu.xfce4 debug output (non-functional)+ interval=5+ true++ nvidia-settings -query gpucoretemp++ grep gpu++ perl -ne 'print $1 if /gpucoretemp.*?: (\\d+)./;'+ current_temp= # an error appeared here+ (( < 45 ))nvautofan.sh: line 15: ((: < 45: syntax error: operand expected (error token is < 45)+ (( > 45 ))nvautofan.sh: line 17: ((: > 45: syntax error: operand expected (error token is > 45)+ (( > 60 ))nvautofan.sh: line 19: ((: > 60: syntax error: operand expected (error token is > 60)+ (( > 65 ))nvautofan.sh: line 21: ((: > 65: syntax error: operand expected (error token is > 65)+ (( > 70 ))nvautofan.sh: line 23: ((: > 70: syntax error: operand expected (error token is > 70)+ (( > 75 ))nvautofan.sh: line 25: ((: > 75: syntax error: operand expected (error token is > 75)+ sleep 5# ...nvidia-settings -query gpucoretemp output (xfce)attribute 'gpucoretemp' (linux-rig:0.0): 38. 'gpucoretemp' is an integer attribute. 'gpucoretemp' is a read-only attribute. 'gpucoretemp' can use the following target types: x screen, gpu.so, facing this problem, i tried to workaroud this using sh ./nvautofan.sh, but i discovered an error that creates files in my script folder (script continues non-functional):+ interval=5+ true+ nvidia-settings -query gpucoretemp+ grep gpu+ perl -ne print $1 if /gpucoretemp.*?: (\\d+)./;+ current_temp=nvautofan.sh: 15: nvautofan.sh: cannot open 45: no such file+ nvautofan.sh: 15: nvautofan.sh: : permission denied+ nvautofan.sh: 17: nvautofan.sh: : permission denied+ nvautofan.sh: 19: nvautofan.sh: : permission denied+ nvautofan.sh: 21: nvautofan.sh: : permission denied+ nvautofan.sh: 23: nvautofan.sh: : permission denied+ nvautofan.sh: 25: nvautofan.sh: : permission denied+ sleep 5# ...how can i solve this problem in order to execute this script in xfce properly?thank you.",
    "present_kp": [
      "linux",
      "bash",
      "ubuntu"
    ],
    "absent_kp": [
      "shell script"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "hiding away complexity with sub functions. i am having a discussion on code style, and it's starting to sound like a matter of taste. i strongly believe otherwise, so i'm writing this to get your opinion and learn from your arguments for and against.the issue is that of hiding complexity by replacing a bunch of lines of code with a method. this discussion is unrelated to reusable functions, and i'm trying to make the argument for simple code organization, not for factoring out common functionality.so, take this example in loose java:public void purchase(customer customer, set<item> items, warehouse warehouse){ int credit = customer.getcredit(); int orderprice; for(item item: items){ orderprice+=items.getprice(); } if(orderprice + customer.getpendingpayments() > customer.getcredit()) { warehouse.reservestock(items); transportcenter.allocatetransport(); customer.bill(items, orderprice); }}as opposed to this:public boolean iscreditok(customer customer, set<item> items){ int credit = customer.getcredit(); int orderprice; for(item item: items){ orderprice+=items.getprice(); } return orderprice + customer.getpendingpayments() > customer.getcredit();}public void purchase(customer customer, set<item> items, warehouse warehouse){ if(iscreditok(customer, items)){ warehouse.reservestock(items); transportcenter.allocatetransport(); customer.bill(items, orderprice); }}the whole discussion now is: would you have created the iscreditok method or would you have left if inline? when do you do one or the other? does it depend, in a general case, on:the length of the function to extracthow many sub-functions and sub-subfunctions iscreditok will end up havingagain, this is not done because iscreditok will be used often, but because it makes the code easier to read (in my opinion).can i have your comments? thanks!note: i have found this question to be related: is there such a thing as having too many private functions/methods?",
    "present_kp": [],
    "absent_kp": [
      "readability",
      "encapsulation",
      "coding style"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "variadic template class that stores a vector of each type given. i'm exploring the possibility to generalise a class of mine that stores some entities. i like it to be able to keep entities of several type, given as template arguments. to explore the possibilities i wrote the following class, that does exactly that. i'm however unsure if anything that i do here is a) save and b) as easy as it gets. i'm not very deep into metaprograming. i'm looking for tips how to improve and pitfalls.#include <iostream>#include <tuple>#include <vector>template<typename... ts>class a{ template<typename t> using container = std::vector<t>;public: template<typename t> void add(t e) { std::get<container<t>>(m_tupel).emplace_back(e); } template<typename t> container<t>& get() { return std::get<container<t>>(m_tupel); }private: std::tuple<container<ts>...> m_tupel;};int main() { a<int, bool, double, float> a; a.add(3); a.add(33); a.add(3.14); a.add(333); a.add(31.4); const auto& cdouble = a.get<double>(); const auto& cint = a.get<int>(); for(auto i : cdouble) std::cout<<i<< ; std::cout<< ; for(auto i : cint) std::cout<<i<< ;}",
    "present_kp": [
      "variadic"
    ],
    "absent_kp": [
      "c++",
      "collections",
      "template meta programming",
      "c++17"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why do i still need to run shutdown as sudo after this?. i'm running linux mint 17.3.by default, running shutdown or other commands like reboot, poweroff requires root privilege.so i added the following linelesaff_b all=(all) nopasswd: /sbin/poweroff, /sbin/reboot, /sbin/shutdownto /etc/sudoers. i know it still requires that i run these commands as sudo but it won't ask for password, so i create an alias for each of them.it worked fine the first time, but then it stopped working, i now need to run these commands as sudo and type my password. i know there are others ways of running these commands without sudo, but why doesn't it work whereas it should, and most importantly, why did it work one time ??",
    "present_kp": [
      "sudo",
      "reboot"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "built-in context-and-input-to-output-hashing in compilers. why doesn't (open source) compilers contain builtin funtionality for (shared) caching and reuse-fetching (using sha1-hash of compiler-version, build-flags, target-platform and inputs) of executable output objects (elf or coff) similar to what ccache does and scons?many large software projects would benefit enormously in required build-disk-space and -time if this was present in, say gcc.",
    "present_kp": [
      "compiler",
      "caching",
      "hashing"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how long does it take for backlinks to appear if you submit the page yourself?. i have some new backlinks to my website online on other websites. i submitted the pages using webmaster tools > submit url so that googlebot would crawl the pages faster.this was 24 hours ago and i checked the backlinks in moz site explorer and in webmaster console but they still haven't showed up.how long would this usually take for webmaster tools? and for open site explorer would this take the same amount of time or longer?is this the best way to check for backlinks?",
    "present_kp": [],
    "absent_kp": [
      "google analytics",
      "google search console"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "do kernel threads seem to interfere with full tickless mode?. i'm trying to disable local timer interrupts for low latency. i have full tickless mode enabled in the kernel config and i have the boot parameter nohz_full set for the cores in question. however when i look at the interrupt count through /proc/interrupts i see the local timer interrupts counting up 1000 times a sec per core meaning full tickless isn't working. the tickless mode documentation says that for tickless to work that only one running process needs to be on that core. when i look at top, i see the following under a given core (core 1 in this example): 19 root rt 0 0 0 0 s 0.0 0.0 0:00.00 1 watchdog/1 20 root -2 0 0 0 0 s 0.0 0.0 0:02.15 1 rcuc/1 21 root rt 0 0 0 0 s 0.0 0.0 0:00.04 1 migration/1 22 root -2 0 0 0 0 s 0.0 0.0 0:00.25 1 ksoftirqd/1 23 root rt 0 0 0 0 s 0.0 0.0 0:00.00 1 posixcputmr/1 24 root 20 0 0 0 0 s 0.0 0.0 0:00.00 1 kworker/1:0 25 root 0 -20 0 0 0 s 0.0 0.0 0:00.00 1 kworker/1:0hi do know that some of these are kernel threads. are these the reason why my full tickless mode isn't working?",
    "present_kp": [
      "kernel"
    ],
    "absent_kp": [
      "linux",
      "performance"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to add a virtual core to linux system. here a virtual core means another cpu core that does not exist physically but can exist just as another core and can be assigned processes naturally by the system.is it even possible to do this?please help.",
    "present_kp": [
      "linux",
      "cpu"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "selection of k elements of given ranks. given a procedure/function $select(s,r)$ - which selects element of rank r from set swhich uses at most $|s|. constant$ comparisonswe design another function $multiselect(s,r)$with $r =\\{r_1<r_2<...<r_k\\}$returns $x =\\{x_1<x_2<...<x_k\\}$ such that rank of $x_i$ is $r_i$what is the minimum no of comparisons for this function?from wikipedia i found the upper bound for it -but while utilising this for deriving the answer i got it totally messed up!i tried using algorithm to utilise select(s,r) repeatedly each timeeach time decreasing the search space by logn ... but i guess the answer could be foundin a more concise mathematical way.by the way, answer given was - $constant.|s|(1+logr)$",
    "present_kp": [],
    "absent_kp": [
      "complexity theory",
      "algorithm analysis"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is parameter tying more common in rnns than in regular nns?. from what i can see it seems like rnns favour using a backprop through time method which i haven't seen really applied to other neural networks.can someone explain the significance of tying the weights and why it is necessary to be done for backprop through time?",
    "present_kp": [
      "neural networks"
    ],
    "absent_kp": [
      "machine learning"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "page vs page table entry. im studying for os-finals and i cant figure out the difference.a page is a chunk of addresses e.g 0-4095. this maps to 4kb of memory.this page is 4kb big. but according to the litterature the page table entry is around 4 bytes big. i thought page table entry are the same as a page.",
    "present_kp": [],
    "absent_kp": [
      "virtual memory"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "exiting infinte loop. this code gets into an infinite loop. how can in troubleshoot it?pgm=ot_run_hourlypid=$$timestamp='date +%y%m%d%h%m%s'master_date_stamp_begin='date +%d'master_time_stamp_begin='date +%t'dir=/var/opt/gogd/ship/na/log#logfiles /var/opt/gogd/ship/na/log/ot_run_hourly*.loglog_file=${dir}/${pgm}_${timestamp}_${pid}.logcounter=1date +info: %m/%d/%y %t : step 1 : start | tee -a ${log_file}while [ $counter -le 5 ]do date +info: %m/%d/%y %t : step 2 : counter $counter | tee -a ${log_file} counter ='expr $counter + 1' sleep 60donedate +info: %m/%d/%y %t : step 3 : complete | tee -a ${log_file}/var/opt/gogd/ship/scripts",
    "present_kp": [],
    "absent_kp": [
      "bash"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "google webmaster tools keeps listing errors about urls that don't apear on my site. google webmaster tools keeps listing errors on my site about urls such as <url> 93/f/w8kgntn9x5eppxkrkzo=the problem is that i don't have links with such urls and that gwt doest indicate the where it found the link. is there a way to find where those urls come from?",
    "present_kp": [
      "google"
    ],
    "absent_kp": [
      "google search console"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "installing openssl on r. i'm trying to install openssl inside r, using install.packages('openssl'), but i'm getting an error message:* installing *source* package openssl ...** package openssl successfully unpacked and md5 sums checkedfound pkg-config cflags and libs!using pkg_cflags= using pkg_libs=-lssl -lcrypto -ldl -lz ------------------------- anticonf error ---------------------------configuration failed because openssl was not found. try installing:* deb: libssl-dev (debian, ubuntu, etc)* rpm: openssl-devel (fedora, centos, rhel)* csw: libssl_dev (solaris)* brew: openssl (mac osx)if openssl is already installed, check that 'pkg-config' is in yourpath and pkg_config_path contains a openssl.pc file. if pkg-configis unavailable you can set include_dir and lib_dir manually via:r cmd install --configure-vars='include_dir=... lib_dir=...'--------------------------------------------------------------------error: configuration failed for package openssl* removing /usr/local/lib64/r/library/opensslopenssl seems to be installed on the system:$ openssl version -aopenssl 1.0.2f 28 jan 2016built on: reproducible build, date unspecifiedplatform: linux-x86_64options: bn(64,64) rc4(16x,int) des(idx,cisc,16,int) idea(int) blowfish(idx)compiler: gcc44 -i. -i.. -i../include -fpic -dopenssl_pic -dopenssl_threads -d_reentrant -ddso_dlfcn -dhave_dlfcn_h -wa,--noexecstack -m64 -dl_endian -o3 -wall -dopenssl_ia32_sse2 -dopenssl_bn_asm_mont -dopenssl_bn_asm_mont5 -dopenssl_bn_asm_gf2m -dsha1_asm -dsha256_asm -dsha512_asm -dmd5_asm -daes_asm -dvpaes_asm -dbsaes_asm -dwhirlpool_asm -dghash_asm -decp_nistz256_asmopenssldir: /usr/local/bin/miniconda2/ssli have tried to set pkg_config_path to where openssl.pc is located but with no luck.some help would be much appreciated!i'm running suse11 sp2",
    "present_kp": [
      "openssl",
      "suse",
      "r"
    ],
    "absent_kp": [
      "pkg config"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "adding hyperlinks in a google form. how can i add hyperlinks in google form?",
    "present_kp": [],
    "absent_kp": [
      "google forms"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is autism caused by genetics?. is autism and asperger's syndrome caused by genetics or some other factor such as the environment? i am aware about the neanderthal theory of autism, asperger and adhd, but am not interested in this due to it appearing too broad and to me seems based off of opinions like preferring the cold to the heat which is not always the case, a the term lack of social life seem's very broad and not clear nor specific in the aspects of socializing that they are not good with. the aspect of having big brains is also very broad and does not cover in any detail about what it means by that, however if it means intelligence then this is a development from back when we needed to hunt for food as we started to create things and fleeing particular pray on sight became instinct. the section about being meat eaters does not conclude anything as many autistic people are vegans the same as neurotypicals so does not lead us anywhere. the section about woman being more submissive sexually is also not conclusive as most autistic people are asexual so there is no real dominance by both genders as most do not even partake in sexual activities. the graphs included are also questionable as they all seem to be in favor for aspies as they score higher on everything which i would say for things like the chances of having ocd they would be very much correct as the chances of having this when on the spectrum is 40 times more likely. a lack of organization is mentioned which to me does not seem right i feel that we over plan and organize. these category's specified do not cover everyone on the spectrum so therefore is not conclusive.",
    "present_kp": [
      "autism"
    ],
    "absent_kp": [
      "abnormal psychology",
      "aspergers"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "linux spdif input not working. i bought this product , the problem is the spdif input not workinglinux 4.4.0-28-generic #47-ubuntu smp fri jun 24 10:09:13 utc 2016 x86_64 x86_64 x86_64 gnu/linux$ lspci | grep -i audio00:03.0 audio device: intel corporation xeon e3-1200 v3/4th gen core processor hd audio controller (rev 06)00:1b.0 audio device: intel corporation 9 series chipset family hd audio controller03:01.0 audio device: creative labs sb x-fi04:00.1 audio device: nvidia corporation gf108 high definition audio controller (rev a1)alsamixer",
    "present_kp": [
      "audio",
      "alsa"
    ],
    "absent_kp": [
      "pulseaudio"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "google spreadsheet not automatically updating on published page. i have a spreadsheet that i share at work that does not update on the published webpage link that i send out. if new info in input, the webpage never updates even though it says it is set to every 5 minutes. any insight would be appreciated.",
    "present_kp": [],
    "absent_kp": [
      "google spreadsheets"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "i feel unprepared to start my first job out of college... how can i improve?. i just graduated from university with a degree in computer science/engineering and was fortunate enough to land a job working in the pharmaceutical industry as a developer. my title is system developer i, which will require the following skills:0-3 years development experience with c# .net and sql serverexperience writing t-sql and stored proceduresxmljavascripti learned of the company at a job fair and was not aware that i would need those skills for the job until after they wanted to set up an interview with me. the interview consisted of talking about my background, an almost-too-simple logic test, and a couple sql questions that anyone with any experience should be able to answer. i was honest with them and indicated that i had absolutely no experience with sql server, .net, xml, or javascript, but they offered me the position anyway. of course, i accepted it, but i am now extremely worried that my skills will not be up to snuff. i fully realize from reading lots of coding horror, stack overflow, and the daily wtf that a degree in comp sci in no way prepares me to be a software developer; i further realize that i will be a monumental noob in the presence of people who have been doing this for years. i feel like the only thing that makes up for my lack of development experience and programming knowledge are my social skills, innate writing ability, and humility (at least compared with some of my co-graduates who fancied themselves to be the next steve jobs... barf) you will never find me being the prima donna constantly complaining about the system, the language, etc... i just want to do my job like i'm told, work 9 - 5, and go home with my paycheck feeling like i'm competent. if that requires home-study, i'm more than willing because i do love programming and computer science.so far, i've familiarized myself a bit on using sql server management studio, gave myself a refresher on basic sql, and started learning more about c# and .net using introducing visual c# 2010 by adam freeman from apress. can anyone recommend anything else i can do in the meantime to:a. chill the ** out and enjoy my new job without worrying so much about getting canned for incompetenceb. improve my understanding of design patterns and oopc. get the low-down on writing t-sql in the most efficient way possiblethanks everyone.",
    "present_kp": [
      "c#",
      ".net",
      "sql server"
    ],
    "absent_kp": [
      "object oriented",
      "tsql"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what's the best way to inject science into everyday programming?. i work better when i have some firm foundations. this helps me to solve everyday problems better, because i can generalize them. the thing i find in programming is so many things are black boxes. logic is scientific, but commands are not. a command in a language is just something that does something. it is very hard to work out what its logic is, without understanding the underlying hardware. i learned brainfuck for a little bit, and enjoyed it very much, but it still has commands. these commands are packages of functionality that cannot really be understood without understanding all the lower levels of logic, unfortunately. what i would like to know is if there are any mathematical or logic principles that help with being able to generalise programming problems?",
    "present_kp": [],
    "absent_kp": [
      "computer science"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "remove duplicate entry from file1 if entry present in file2. i want to delete duplicate entry e from file1 if e is also present in file2.input file1:x1 y1x2 y2x3 y3x4 y4y1 x1x5 y5y3 x3x6 y6x5 y5input file2:y1 x1y2 x2y3 x3y4 x4x1 y1y5 x5x3 y3y6 x6x5 y5desired output:x1 y1x2 y2x3 y3x4 y4x5 y5x6 y6i have used following shell script:awk 'fnr==nr { lines[nr,col1] = $1 lines[nr,col2] = $2 lines[nr,line] = $0 next } (lines[fnr,col1] != $1) {($1 in lines) print lines[fnr,line] next}' file1.txt file2.txtbut it is giving following output:x1 y1x2 y2x3 y3x4 y4y1 x1x5 y5y3 x3x6 y6",
    "present_kp": [
      "awk"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "fedora 18 live cd is hanging on boot. when i boot up fedora 18 (fedora-18-x86_64-live-desktop.iso) in a virtual machine, i just get this screen:i've verified the sha sum, and i've tried starting it up in basic graphics mode, yet i still get the same problem.that's all i get, there is nothing else.what is wrong?",
    "present_kp": [
      "fedora",
      "boot",
      "virtual machine"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how do i conditionally add items to the statusline?. as a simple example, let's say i want to show the line number only when :spell is set. i would assume the following would work, but does not.%{&spell ? %l : }",
    "present_kp": [
      "statusline"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "counter example to graph coloring heuristic using bfs. i am considering the following heuristic for the graph coloring problem (i.e. to color a graph $g$ using a minimal number of colors so that no two adjacent vertices have the same color):explore the vertices of $g$ in the order that they would be explored by a bfs search (with arbitrary starting vertex) and assign each vertex the lowest numbered color not yet used for one of its neighbors.since i don't think this algorithm is correct, i am trying to find a counterexample where coloring a graph in this way does not yield a coloring with the minimal number of colors. does anyone know of such an example?",
    "present_kp": [],
    "absent_kp": [
      "algorithms",
      "graphs",
      "correctness proof",
      "colorings"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "conditional check for iptables. i am trying to check whether specific rules in iptables exists or not.#!/bin/bashif iptables -l -n | grep -- accept tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:8880; then echo accept tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:8880 exists else echo accept tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:8880 does not existfiif iptables -l -n | grep -- accept tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:80; then echo accept tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:80 exists else echo accept tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:80 does not existfii am checking below two rules:accept tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:8880accept tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:80because those two rules already exists and conditions run the true case, terminal also outputs the result of the grep in conditional check unnecessarily. grep somehow does not output for false case.how can i prevent the grep to output for true case? and how can i combine those separate two if conditionals into a single or conditional?btw, my iptables is old version and can not use -c argument.",
    "present_kp": [
      "bash",
      "grep",
      "iptables"
    ],
    "absent_kp": [
      "shell script"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "where can i find recent snowfall totals for a city?. i'm trying to find a place that shows how much snow has fallen in the last few days. reports from amateur weather stations (such as in-home or at schools) are fine. if possible, i'd like to know what it is for my city or zip code, or a neighboring city or zip code, rather than for the metro area.i've been able to find plenty of similar information that doesn't work; i'm not looking for any of the following:ski resort totalssnow accumulations (i'd have to know how much snow was on the ground before, and how much snow melted to get what i'm looking for.)simple precipitation totals (as far as i understand, this really only tells how wet the snow was, or the amount of water if you melted down the snow.)snowfall totals that don't go past the last 24 hours (that won't catch storms that last several days)i've tried wolfram alpha, local news sites, and a few weather sites like weather.com, weather underground, and accuweather. however, i wouldn't be surprised if i'm just not looking in the right place there.",
    "present_kp": [
      "weather"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "is it *ever* okay to catch stackoverflowerror in java?. i used to think that it's not, but yesterday i had to do it. it's an application that uses akka (an actor system implementation for the jvm) to process asynchronous jobs. one of the actors performs some pdf manipulation, and because the library is buggy, it dies with a stackoverflowerror every now and then.the second aspect is that akka is configured to shutdown its whole actor system if any jvm fatal error (e.g. stackoverflowerror) is caught.the third aspect is that this actor system is embedded inside a web app (for wtf-ish, legacy, reasons), so when the actor system is shut down, the web app is not. the net effect is that on a stackoverflowerror our job processing application becomes just an empty web app.as a quick fix i had to catch the stackoverflowerror being thrown, so that the thread pool of the actor system isn't torn down. this lead me to think that maybe it's sometimes okay to catch such errors especially in contexts like this? when there's a thread pool processing arbitrary tasks? unlike an outofmemoryerror i can't imagine how a stackoverflowerror can leave an application in an inconsistent state. the stack is cleared after such an error, so computation can go on normally. but maybe i'm missing something important.also, let it be noted that i'm all for fixing the error in the first place (as a matter of fact i have already fixed an soe in this same app a few days ago), but i really don't know when this kind of situation might arise.why would it be better to restart the jvm process instead of catching the stackoverflowerror, mark that job as failed, and continue with my business?is there any compelling reason to never catch soes? except best practices, which is a vague term that tells me nothing.",
    "present_kp": [
      "java",
      "jvm",
      "stackoverflow"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "seo effects of linking to subdomain. i have <url> as a main site. then i have <url> as the subdomain. i'm cautious to link to the subdomain from the main site. i did this before and droped 50% of traffic from the main site. also, google webmaster tools showed over 3000 incoming links from hollywood nose to the subdomain / forum. i'm assuming google frowned on this and saw it as spam or something of that nature. all i would want is link to the subdomain in the top nav bar of the main site. is this safe? will i get an overabundance of incoming links resulting in an seo drop? does anyone have any experience in this matter? thank you.",
    "present_kp": [
      "seo",
      "subdomain",
      "links"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how does chronic stress improve memory processes?. there seems to be more information on the detrimental effects of stress on memory.there is evidence to the contrary.stress effects on memory: an update and integration.schwabe l, et al doi: 10.1016/j.neubiorev.2011.07.002it is well known that stressful experiences may affect learning and memory processes. less clear is the exact nature of these stress effects on memory: both enhancing and impairing effects have been reported.i am interested in the research available on stress positively effecting memory.what are the biological reasons for stress enhancing memory processes?there is a related question can the brain work better in stressful conditions?; i am interested more in chronic stress. an examination of the effects of acute stress on memory, i believe, warrants a separate discussion.",
    "present_kp": [
      "memory"
    ],
    "absent_kp": [
      "neurobiology",
      "cognitive neuroscience"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what are the advantages of a nomultilib system?. i see that my linux operating system has support for a nomultilib system. as far as i can tell, that means that no 32-bit packages will be installed on the system, meaning the entire system will be 64-bit.i can find a lot of information about what a nomultilib system is and how to change to one, but there's one question i can't find an answer to:is there any benefiet to running a nomultilib system?",
    "present_kp": [],
    "absent_kp": [
      "64bit",
      "32bit"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "i am taking the web server for a domain offline - but keeping the domain - what should i do with the dns 'a' record?. i have a few domains where i plan to take the vps server that hosts the content offline.i will be keeping the domains.what is the general best practice regarding the dns a records?i'm assuming once i delete the vps the hosting company will reassign the ip address of the vps to another server, and if my domains dns records are still pointing back to my old server, it might look like i am hotlinking my domains back to that old server!also out of consideration to the person taking over the ip address server i dont want them to have to deal with the constant bots probing my domains (although much of that traffic probably is directed straight at the ip address itself and not the domains - which i dont control).is there some kind of proper dns entry you can assign for an a record when the domain doesnt have a webserver?",
    "present_kp": [
      "dns"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "return the element of collection if the collection contains only one element. recently i have found in my code several places where i was first gathering some solutions, and then continued processing them only if the solution was unique (solution collection contained only one element). following code is an attempt to solve this in a more functional manner. implicit class getonlyone[a](val coll: iterable[a]) { def getonlyone = { if (coll.isempty) none else if (coll.tail.isempty) coll.headoption else none } }the function can be used like:seq(1).getonlyoneseq(1,2).getonlyoneset(1).getonlyoneis there anything missing to be idiomatic scala, or to be more elegant?",
    "present_kp": [
      "scala"
    ],
    "absent_kp": [
      "iterator"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "start server from terminal using a script. i've a server nodejs server.js and i want it to restart as soon as it crashes but only if previous restart time is more than a second. this will prevent my email being flooded if the server exits immediately.here is the start_server.sh which restarts it. i've placed it in @reboot in crontab and it starts quite fine when the server boots.but how can i run start_server.sh from the terminal and close the terminal later and it should work just fine without exiting?i've tried nohup start_server.sh but that does not work. i also want to redirect the stdout to a log file.start_server.sh#!/bin/shset -xcount=0while :doif test $count -lt 40thenmail -s server.js started just now <email> < /dev/nullficount='expr $count + 1'date1='date +%s' #get unix timecd /home/login/railways/nodejsecho starting time is $date1 /usr/bin/nodejs server.js >> server.logdate2='date +%s' #get unix timeif test $date1 -eq $date2then msg=exiting as process has failed echo $msg if test $count -lt 40 then mail -s $msg <email> < /dev/null fi sleep 10fiecho restarting the processsleep 1done",
    "present_kp": [],
    "absent_kp": [
      "ubuntu"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "thread safe static collection, is this code safe. a small caching utility, can you tell if it is thread safe?public static class cacheservice{ private readonly static object locker = new object(); private static dictionary<string, dictionary<string, object>> data = new dictionary<string, dictionary<string, object>>(); public static void insert(string partition, string key, object value) { lock (locker) { if (!data.containskey(partition)) { var newval = new dictionary<string, object>(); newval.add(key, value); data.add(partition, newval); } else { if (!data[partition].containskey(key)) { data[partition].add(key, value); } else { data[partition][key] = value; } } } } public static bool keyexists(string partition, string key) { return data[partition] != null && data[partition][key] != null; } public static v get<v>(string partition, string key) { return (v)data[partition][key]; }}",
    "present_kp": [
      "cache"
    ],
    "absent_kp": [
      "c#",
      "thread safety"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "api calls documentations in ida pro (fortran). i am reversing an application and found that it has many calls like:for_write_int_fmt_xmit((__int64)&v1041, 9069724ll, (char *)&v707, v166);result = for_write_seq_lis((unsigned __int64)&v46);for_dealloc_allocatable(*(void **)(retaddr - 296));i am using ida pro however i would like to know what these functions do and their api documentation.can anyone help me please?there are a lot of them in the application and i think they should be in one manual but failed to findfor_closefor__close_argsfor__close_defaultfor__close_procfor__desc_ret_itemfor__key_desc_ret_itemfor__desc_test_itemfor__desc_zero_length_itemfor__this_image_number_or_zerofor__io_returnfor__issue_diagnosticfor__get_msgfor_emit_diagnostic",
    "present_kp": [
      "ida"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "mail server spoofing prevention. i was in the mail logs yesterday when i noticed the following messages.. only there were more like 10,000 odd hits...jun 21 10:47:10 exi-svr-2 dovecot: pop3-login: disconnected: user=, method=plain, rip=67.228.94.206, lip=xxx.xxx.xxx.xxxjun 21 10:47:10 exi-svr-2 dovecot: pop3-login: disconnected: user=, method=plain, rip=67.228.94.206, lip=xxx.xxx.xxx.xxxi added 67.228.94.206 to my firewall like soiptables -i rh-firewall-1-input -s 67.228.94.206 -j dropservice ip tables savethe attack stopped straight away, however in the process it managed to successfully obtain a user account and started spoofing with it.i deleted that user account however it appears that it is still being spoofed as i am getting flooded with bounce emails from various mailserversjun 22 15:08:08 exi-svr-2 postfix/smtp[27219]: connect to vahoo.com[216.151.212.175]: connection refused (port 25)jun 22 15:08:07 exi-svr-2 postfix/smtp[27158]: connect to mail.gamdak.co.za[196.215.56.13]: connection refused (port 25)jun 22 15:08:07 exi-svr-2 postfix/smtp[27169]: a72a61068460: to=, relay=none, delay=33839, delays=33839/0.13/0.51/0, dsn=4.4.1, status=deferred (connect to keywordranking.com[208.87.35.105]: connection refused)jun 22 15:08:07 exi-svr-2 postfix/smtp[27169]: connect to keywordranking.com[208.87.35.105]: connection refused (port 25)jun 22 15:08:07 exi-svr-2 postfix/smtp[27179]: 40a9c1068515: to=, relay=none, delay=32038, delays=32038/0.22/0.19/0, dsn=4.4.1, status=deferred (connect to graintech-makeway.com[50.116.103.74]: connection refused)not entirely sure how to go about fixing this and the preventive measures i need to take to stop this happening going forward?i have read elsewhere that this kinda thing is unavoidable and can pretty much ignored by not trapping these messages on the log... obviously i'm not entirely comfortable with this.im running centos 5.6postfix, dovecot, amavis, spamassassin, clamav",
    "present_kp": [
      "centos",
      "postfix"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "usb serial cable (prolific) not working with ftdi_sio driver on rpi-buildroot image. i'm using a raspberry pi to control a led matrix display with a prolific usb to serial cable (067b 2303). using the default raspian image it works perfectly, however with a custom buildroot image (using rpi-buildroot) i'm unable to configure or use it.here are my steps thus far:linux recognizes the device but does not automatically load any drivers or attach it to /dev/ttyusbx, so i use:modprobe ftdi_siowhich yields:usbcore: registered new interface driver usbserialusbcore: registered new interface driver usbserial_genericusbserial: usb serial support registered for genericusbcore: registered new interface driver ftdi_siousbserial: usb serial support registered for ftdi usb serial devicei don't see anything at /dev/ttyusb* so i echo to new_id with prod and vendor ids:echo 067b 2303 > /sys/bus/usb-serial/drivers/ftdi_sio/new_idwhich yields:ftdi_sio 1-1.2:1.0: ftdi usb serial device converter detected usb 1-1.2: detected fibu232am ftdi_sio ttyusbo: unable to read latency timer: -32 ftdi_sio ttyusbo: unable to write latency timer: -32 usb 1-1.2: ftdi usb serial device converter now attached to ttyusb0when i try to change baud rate with:stty -f /dev/ttyusb0 115200which fails with:ftdi_sio ttyusbo: ftdi_set_termios failed to set databits/stopbits/parity ftdi_sio ttyusbo: ftdi_set_termios urb failed to set baudrate ftdi_sio ttyusbo: urb failed to clear flow control ftdi_sio ttyusbo: failed to get modem status: -32ftdi_sio ttyusbo: ftdi_set_termios urb failed to set baudrateftdi_sio ttyusbo: urb failed to clear flow controlftdi_sio ttyusbo: failed to get modem status: -32ftdi_sio ttyusbo: error from flowcontrol urb i'm new to the cross-compilation so might be missing something obvious in my build config, but i found nothing for usb serial to configure. any insight would be greatly appreciated, thanks!",
    "present_kp": [
      "usb",
      "raspberry pi",
      "buildroot"
    ],
    "absent_kp": [
      "serial port"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "programmaticaly finding the landau notation (big o or theta notation) of an algorithm?. i'm used to search for the landau (big o, theta...) notation of my algorithms by hand to make sure they are as optimized as they can be, but when the functions are getting really big and complex, it's taking way too much time to do it by hand. it's also prone to human errors.i spent some time on codility (coding/algo exercises), and noticed they will give you the landau notation for your submitted solution (both in time and memory usage).i was wondering how they do that...how would you do it?is there another way besides lexical analysis or parsing of the code?this question concerns mainly php and or javascript, but i'm opened to any language and theory.",
    "present_kp": [
      "algorithms",
      "big o"
    ],
    "absent_kp": [
      "complexity",
      "static analysis",
      "big theta"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "liquid layout out of proportion. i am using a liquid layout on my website, but when i view in in my browser i find that when i change the browser window size, the images are stretched out of proportion. can anybody help?",
    "present_kp": [],
    "absent_kp": [
      "html",
      "css",
      "browsers"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "allowing rename but not moving to other directory. users can upload/delete/rename files via vsftp server and everything works well except they can also move files across directories. is it possible to allow renaming of files but disallow moving them around?example,/ftp/work/xls/list.xls # can be deleted/renamed but should not moved to ie. 'doc'/ftp/work/doc/list.doc # same thing, should not be moved elsewhere",
    "present_kp": [
      "rename",
      "ftp"
    ],
    "absent_kp": [
      "permissions",
      "security",
      "vsftpd"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is there any way to distinguish between ll(k) and lr(k) grammar?. i am recently studying about compilers designing. i came to know about two types of grammar one is ll grammar and other is lr grammar.we also know the facts that every ll grammar is lr that is ll grammar is a proper subset of lr grammar. first one is used in top-down parsing and the second one is used in bottom-up parsing. but is there any way to so that we can say that a given grammar is ll or lr?",
    "present_kp": [
      "compilers"
    ],
    "absent_kp": [
      "formal grammars",
      "parsers",
      "lr k",
      "ll k"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how can an open source project attract new contributors?. one of the most common things that i see in open source projects is that they are maintained by a single developer. in my opinion, this often leads to abandoned projects due to the lack of time (or motivation) from the creator of the project and (if the project gains popularity) too many feature requests for a single person to deal with.on the other hand, many open source projects have several programmers but do not have any designer...which ends with:good programs that are not very user-friendly and therefore have a very small user baselack of originality in those projects, which results in a slower gain of popularity among usersare there any thumb rules or recommendations to attract more contributors to join an open source project?",
    "present_kp": [
      "contributor"
    ],
    "absent_kp": [
      "project management"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "example for converting 3sat to unary 2dfa. i have been told (generic explanation) that 3sat could be converted to unary 2dfa with endmarkers (assuming each variable of 3sat as a prime number and then for each clause creating a cycle in unary 2dfa).can someone please provide an example of the above with explanation of the process (a small example, say 3-4 clauses would be fine). tried it but have not background in unary 2dfa.",
    "present_kp": [],
    "absent_kp": [
      "np complete"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "first kde session. i was curious, so i installed kde alongside gnome (which i use primarily, for the matter).i am running arch linux, and i installed kde using yaourt, by the following command:yaourt -syua # update databases yaourt -s kde --force # install kde itselfi installed everything yaourt suggested for kde, and i didn't get any errors.so, i rebooted the computer, selected kde plasma in the gdm environment select menu. i entered the password and gdm accepted it.however, when gdm starts kde, i see a wallpaper with a white transparent rectangle with a picture of a hard disk on the left. the mouse is visible and i can move it, but that's it. nothing happens after that.is this normal or do i have to re-install kde ?",
    "present_kp": [
      "arch linux",
      "kde",
      "gdm",
      "yaourt"
    ],
    "absent_kp": [
      "desktop"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "ui (specifically flex) automation testing. i was wondering whether the community utilizes automation tests to the extent of ui? is there some common best practices for testing ui?also - the product i'm working on is flex-based. i would love to get ideas how to test it.mainly i want to test:data appears on the screen as expectedfield validations (for example: field is not a number) are working as expectedalignment of fieldsand so on",
    "present_kp": [
      "automation",
      "ui",
      "flex"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "to which debian package does /etc/nsswitch.conf belong?. i tried to find out to which package /etc/nsswitch.conf belongs on my debian machine, but dpkg --search /etc/nsswitch.conf won't tell me. does anyone know?",
    "present_kp": [
      "debian",
      "nsswitch"
    ],
    "absent_kp": [
      "linux",
      "apt",
      "package management"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "cannot execute binary file. i'm trying to install jasperreports server 4.1 on linux ubunto (release 11.04) and have following message:-bash: ./jasperreports-server-cp-4.1.0-linux-x64-installer.run: cannot execute binary fileis any idea what should i do in order to insall jasper?previously, i installed jasperreports server 4.0 on linux red hat without any problem. is it related specifically to linux ubuntu?here are the output for the commands uname and ls -al:uname -alinux 64-cncrclinrpts 2.6.38-11-generic-pae #48-ubuntu smp fri jul 29 20:51:21 utc 2011 i686 i686 i386 gnu/linuxls -al jasperreports-server-cp-4.1.0-linux-x64-installer.run-rwxr-xr-x 1 root root 329844862 2011-09-09 09:11 jasperreports-server-cp-4.1.0-linux-x64-installer.run",
    "present_kp": [
      "linux"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "criticize my threaded image downloader. i'm going to be working on a much larger version of this program and i just wanted to see if there was anything i should change in my style of coding before i made anything larger.if it wasn't obvious, this code goes through each comic on a softer world and downloads each comic.oh and with larger projects, should i try and fit my main() function into a class or is having a large main() function normal with programs?from beautifulsoup import beautifulsoup as bsfrom urllib import urlretrievefrom urllib2 import urlopenfrom os import path, mkdirfrom threading import threadfrom queue import queueclass worker(thread): def __init__(self, queue): self.queue = queue self.url = '<url> thread.__init__(self) def run(self): while 'running': number = self.queue.get() soup = bs(urlopen(self.url + number)) match = soup.find('p', {'id' : 'thecomic'}).find('img') filename = match['src'].split('/')[-1] urlretrieve(match['src'], 'comic/' + filename) self.queue.task_done()def main(): queue = queue() if not path.isdir('comic'): mkdir('comic') for number in xrange(1, 977): queue.put(str(number)) for threads in xrange(20): thread = worker(queue) thread.setdaemon(true) thread.start() queue.join()main()",
    "present_kp": [],
    "absent_kp": [
      "python",
      "multithreading"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "httpurlconnection response code handling. this snippet from a downloader callable handles http status codes. i need critique on both the style (for or do-while loop better here?) and functionality. should i manage the delay differently? do i need to handle interruptedexception specifically?httpurlconnection connection = null;boolean connected=false;outer:for(int retry=0;retry<=retries&&!connected;retry++){ if(retry>0) {log.warning(retry +retry+/+retries);thread.sleep(retry_delay_ms);} connection = (httpurlconnection)entries.openconnection(); connection.connect(); switch(connection.getresponsecode()) { case httpurlconnection.http_ok: log.fine(entries + **ok**);connected=true;break; // fine, go on case httpurlconnection.http_gateway_timeout: log.warning(entries+ **gateway timeout**);break;// retry case httpurlconnection.http_unavailable: system.out.println(entries+ **unavailable**);break;// retry, server is unstable default: log.severe(entries+ **unknown response code**.);break outer; // abort }}connection.disconnect();log.severe(aborting download of dataset.);",
    "present_kp": [
      "http"
    ],
    "absent_kp": [
      "java",
      "error handling"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to do local fft on huge 3d vector data cell mesh and visualize it spatially?. simulation type:i'm running a simulation with the oommf micromagnetics packagehttp://math.nist.gov/oommf/ where are magnet is represented by a mesh of 3 million cells, it gets excited by a magnetic pulse, the single cells oscillate and data output for every cell is x, y, z direction and absolute magnetic field value of every component over time, so a mini-magnet. now i want to do a fft on m_z(t) for every cell and later visualize this magnet over time while cells with m_z(t) showing common oscillation modes/eigenfrequencies get same color. data output format:it is data binary 4 and as said x, y, z and absolute field value for every component is given for all time steps (below 300-500 time steops)the manual says:the binary representations are ieee floating point in network byte order (msb). to insure that the byte order is correct, and to provide a partial check that the le hasn't been sent through a non 8-bit clean channel, the first datum is a predened value: <phone>.0 (hex: 49 96 b4 38) for 4-byte mode, and 123456789012345.0 (hex: 42 dc 12 21 83 77 de 40) for 8-byte mode. the data immediately follow the check value. the structure of the data depends on whether the \\meshtype declared in the header is \\irregular or \\rectangular. for irregular meshes, each data element is a 6-tuple, consisting of the x, y and z components of the node position, followed by the x, y and z components of the field at that position. ordering among the nodes is not relevant. the number of nodes is specied in the \\pointcount line in the segment header. for rectangular meshes, data input is field values only, in x, y, z component triples. these are ordered with the x index incremented first, then the y index, and the z index last. this is nominally fortran order, and is adopted here because commonly x will be the longest dimension, and z the shortest, so this order is more memory-access ecient than the normal c array indexing of z, y, x. the size of each dimension is specied in the \\xnodes, ynodes, znodes lines in the segment header.i use a rectangluar mesh herequestion: the fft can be done using something like mathematica or matlab for a single cell and (m_z/ number, time/number) data list .dat file, but a data binary 4 file for a single time step for 3 million cells is up to 20 mb big, workstation with 30 gb ram and 12 cores available. the manual above mentions fortran (to my knowledge not so easy to learn language although very fast). what is the way to achieve the results i'm looking for and described above? from my point of view i face the problem to do the fft in reasonable time with a script process for around 3 million cells and then transforming that data into a spectrogram (to see what different eigenfrequencies and higher harmonics exist, also this can be done by using the average m_z (summed over all single cells) and simply fft it) but also have a output format of that data binary 4 format after fft that can be used to feed a visualization software that can read such mesh data and color code it, show single 2d slices of the 3d object. i read that mathematica can deal with binary 4 and offers high performance, but as this is completely new land to me, i first want to ask given above constraints and wishes, what is the best way and software/programming language to achieve this in reasonable time (few weeks)?",
    "present_kp": [
      "matlab",
      "mesh"
    ],
    "absent_kp": [
      "data visualization",
      "fourier analysis",
      "data analysis"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "compare file dates from two directories. i have two directories with files with the same name.basically i want to replace dir1/file.txt with dir2/file.txt if the creation date of dir1/file.txt is more recent. but i think i'm missing something in the if condition.#!/bin/bashfor i in /dir1/*; do namefirr=$(basename $i) dateinput=$(date -r $i) dateoutput=$(date -r /dir2/$namefirr) if [ $dateinput -ge $dateoutput ]; then cp -u $i /dir2/$namefirr fidone",
    "present_kp": [
      "date"
    ],
    "absent_kp": [
      "file comparison"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "gnome keyring manager and gnupg. i get thisgpg: warning: the gnome keyring manager hijacked the gnupg agent.gpg: warning: gnupg will not work proberly - please configure that tool to not interfere with the gnupg system!when i use gnupg-agent with mutt. but the warning doesn't give me clue how to resolve the issue, nor could i find the info from gnome documentation.",
    "present_kp": [
      "gnome"
    ],
    "absent_kp": [
      "gnome3",
      "pgp"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "buffer size for capturing packets in kernel space?. going through the man page of tcpdump, it seems kernel can drop the packets if the buffer is full. i was wondering if:that size is configurable and/orwhere can i see the size for my distro?from the man page (for easy reference):packets ''dropped by kernel'' (this is the number of packets that were dropped, due to a lack of buffer space, by the packet capture mechanism in the os on which tcpdump is running, if the os reports that information to applications; if not, it will be reported as 0).",
    "present_kp": [
      "kernel",
      "buffer",
      "tcpdump"
    ],
    "absent_kp": [
      "networking"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "reading all bytes from a file. i'm basically trying to write a helper function that reads a whole file and returns the data and the number of bytes read.can you tell me if is correctly written and used?#include <iostream>static char * readallbytes(const char * filename, int * read){ ifstream ifs(filename, ios::binary|ios::ate); ifstream::pos_type pos = ifs.tellg(); int length = pos; char *pchars = new char[length]; ifs.seekg(0, ios::beg); ifs.read(pchars, length); ifs.close(); *read = length; return pchars;}int _tmain(int argc, _tchar* argv[]){ const char * filename = polar00.map; int read ; char * pchars = readallbytes(filename, &read); delete[] pchars; return 0;}",
    "present_kp": [],
    "absent_kp": [
      "c++",
      "file system"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how should i access a third party api which requires a key?. i am accessing a third-party api. it requires a key, which is the same key for all of my users. currently, my application includes that key in the client-side code, and calls the third party api directly. so any user can decompile my code, and get the key tied to my application. (bad, right?)i am guessing that i need to have a server between the client and third-party. and the client makes a request to the server, which has the key. the server then makes the request to the third party, and returns the results to the client. is this the right approach?if this is the correct approach, would i build the server and run it on aws? or do third-party tools for this use already exist? (i saw aws gateway, but that appeared to be about building apis, not accessing them.) how should i have a user access the third party, without giving them the secret key which is shared among all users?",
    "present_kp": [
      "api"
    ],
    "absent_kp": [
      "third party libraries",
      "keys"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "canonical tags for separate mobile urls. i have a drupal website serving mobile pages from different urls (starting from /mobile). according to google recommendations i should use the canonical tag to map desktop and mobile pages. right now i did this in case i serve the same node (e.g: node/123 and mobile/node/123) but should i do this for other pages as well that are equivalent but share a different content?for example do i need to map the desktop and mobile homepages even if they don't have the same content at all?",
    "present_kp": [
      "mobile"
    ],
    "absent_kp": [
      "canonical url"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to retrieve the third-party library licenses. i am looking for a way to automatically collect all the third-party library licences that my project is using. currently i am collecting by hand the licences on github.so far , i don't have a clear idea how to get a 3rd party library licence automatically.what is the most reliable way to get the 3rd party licence ?small ideas :most github projects contain a licence text. ex : <url> . but can you map a dependency 'com.squareup.dagger\ud83d\udde1\ufe0f1.2.2' with its github url ?most jvm artifacts are found on mvnrepository . i don't know if mvnrepository.com list the licence.the .jar files may contain licence text . how to extract it ?related :what is the best practice for arranging third-party library licenses paperwork?",
    "present_kp": [],
    "absent_kp": [
      "licensing",
      "open source",
      "builds",
      "maven",
      "gradle"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "npm install couchbase on linux machine failing. i am trying to move node js application from windows machine to linux machine for qa environment and i am having trouble installing couchbase module on linux machine. it ran fine on windows machine but i've tried this install on 4 different linux machine and i get this error message when i runnpm install couchbase --no-bin-linksi am on precise32 with vagrant for this logs. couchbase@2.1.3 install /vagrant/mbo <phone>/node_modules/couchbase prebuild --installsh: 1: prebuild: not foundnpm err! linux 3.2.0-23-generic-paenpm err! argv /home/vagrant/.nvm/versions/node/v5.3.0/bin/node /home/vagrant/.nvm/versions/node/v5.3.0/bin/npm install couchbase --no-bin-linksnpm err! node v5.3.0npm err! npm v3.3.12npm err! file shnpm err! code elifecyclenpm err! errno enoentnpm err! syscall spawnnpm err! couchbase@2.1.3 install: 'prebuild --install'npm err! spawn enoentnpm err!npm err! failed at the couchbase@2.1.3 install script 'prebuild --install'.npm err! make sure you have the latest version of node.js and npm installed.npm err! if you do, this is most likely a problem with the couchbase package,npm err! not with npm itself.npm err! tell the author that this fails on your system:npm err! prebuild --installnpm err! you can get their info via:npm err! npm owner ls couchbasenpm err! there is likely additional logging output above.npm err! linux 3.2.0-23-generic-paenpm err! argv /home/vagrant/.nvm/versions/node/v5.3.0/bin/node /home/vagrant/.nvm/versions/node/v5.3.0/bin/npm install couchbase --no-bin-linksnpm err! node v5.3.0npm err! npm v3.3.12npm err! path npm-debug.log.6258c2ba9fb733156e17534450091effnpm err! code etxtbsynpm err! errno -26npm err! syscall renamei have posted npm-debug i got from this command to pastebinhttp://pastebin.com/ttpfcsyf",
    "present_kp": [
      "node.js",
      "npm"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "starvation condition in cpu/process scheduling?. let's say os is using preemptive cpu scheduling. a process p1 gets a chance to run for some time and then next higher priority process comes and preempts p1. after that higher priority processes are coming regularly and p1 never gets chance. is this condition called starvation or not?",
    "present_kp": [
      "process scheduling"
    ],
    "absent_kp": [
      "operating systems"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what are the best ways to measure end user web site performance?. i know of a couple tools to measure end user web site performance and i'm wondering what else is out there. the two major ones i know of are yslow and google's page speed.",
    "present_kp": [
      "performance"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how do you upload an image in sourceforge.wiki?. there is an image icon in the edit barbut, it just injects markdown into your code that needs the image url. nothing is uploaded. what should i do?ps i have spotted that their documentation/create new page contains an image with url <url> please note the <page name>/attachment/image subfolder. so, there should be a way to attach image files to your pages. but how? i do not see any button.",
    "present_kp": [
      "upload",
      "sourceforge"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "association rule mining interpretation. i am trying to create a association using apriori algorithm.the data contains around 33000 records.below is the sample of the dataid code1 191 581 1112 192 1112 1673 123 793 854 965 196 587 127 187 407 487 857 867 135in r this data is:structure(list(id = c(1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 5, 6, 7, 7, 7, 7, 7, 7, 7), code = c(19, 58, 111, 19, 111, 167, 12, 79, 85, 96, 19, 58, 12, 18, 40, 48, 85, 86, 135)), .names = c(id, code), row.names = c(na, -19l), class = data.frame)using the following code i tried to build the association# creating a string of codes based on the idlibrary(dplyr)asso2 = asso %>% group_by(patient_id) %>% summarise(hi = tostring(hcc_id))# converting into transactionslibrary(arules)fact <- data.frame(lapply(asso2,as.factor))trans <- as(fact, 'transactions') # applying apriorrules = apriori(trans, parameter = list(supp = 0.001, conf = 0.001,target = rules))rulesinspect(rules)i am getting totally 96 rules like below with empty lhs and i trying to understand whether we cannot make any rules from this data or am i missing anything here. since i am novice in this, i would like to get some help.# lhs rhs support confidence lift# 1 {} => {hi=19, 96, 108} 0.001021696 0.001021696 1 # 2 {} => {hi=176} 0.001021696 0.001021696 1 # 3 {} => {hi=88, 108} 0.001021696 0.001021696 1 # 4 {} => {hi=72} 0.001051746 0.001051746 1 # 5 {} => {hi=88, 96} 0.001051746 0.001051746 1 # 6 {} => {hi=108, 112} 0.001081796 0.001081796 1 # 7 {} => {hi=84} 0.001081796 0.001081796 1 # 8 {} => {hi=100, 103} 0.001111846 0.001111846 1 # 9 {} => {hi=18, 108, 111} 0.001111846 0.001111846 1",
    "present_kp": [
      "r"
    ],
    "absent_kp": [
      "machine learning",
      "association rules"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "which group should own /var/www/html?. i have a centos 7 vm setup on my windows 7 host.i have installed vsftpd and ftp and can now successfully connect from the host with filezilla, but the user i log in as doesn't have permissions to write to /var/www/html and so i cannot upload files there. /var/www/html is owned by user:root and group:root.i know i shouldn't add my user to the root group. instead, should i change the group which owns the directory to another (e.g. make one like www-admins) and add my user to that group? i am fairly new to linux and so am wary about changing permissions on directories...",
    "present_kp": [
      "permissions"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "php form generator class. i have made the following classes and objects.it was made to create forms programattically easily. it should produce highly readable, valid, and clean html.care to give me some feedback? :)<url> code:<?phpnamespace forms;/** * this file is supposed to give a good way of generating forms programmatically with php. *//* * warning! warning! warning! warning! warning! warning! warning! warning! warning! warning! warning! warning! * ----------------------------------------------------------------------------------------------------------- * none of the strings you see in the following classes are escaped/secured against any kind of * sql injections, xss attacks, or any other sort of attack for that matter! for your own safety * implement the necessary protections on any strings you use in these classes before entering them! * ----------------------------------------------------------------------------------------------------------- * warning! warning! warning! warning! warning! warning! warning! warning! warning! warning! warning! warning! * @package classes *//** * all containing nodes should implement this. */interface contains_nodes { /** * @abstract * * @param node $node * * this method will add a node to the node list of the implementing object. */ public function add(node $node);}node/** * base object for all nodes. */class node { /** * @var string $element will hold the element name (or tag name) */ public $element; /** * @var array $attribute_list will hold all of the rest of the form's attributes such as id, class and whatnot. */ public $attribute_list = array(); /** * @var bool $self_contained signifies whether the node is self contained. self contained nodes do not get a closing tag. */ public $text; public $self_contained = false; const self_contained = true; const not_self_contained = false; const tab = ; /** * @param string $element * @param string $text * @param bool $self_contained * @param array $attribute_list * * general constructor for nodes. should be overridden regularly. */ public function __construct($element, $text = , $self_contained = false, array $attribute_list = array()) { $this->element = $element; $this->self_contained = $self_contained; $this->text = $text; $this->attribute_list = $attribute_list; } /** * @return string * * general string generator for nodes. should be overridden regularly. */ public function __tostring() { //open element $result = <{$this->element}; //node list $result .= $this->string_attribute_list(); //close element $result .= >; if (!$this->self_contained && !empty($this->text)) { $result .= $this->text; } if (!$this->self_contained) { $result .= </{$this->element}>; } return $result; } /** * @return string */ public function string_attribute_list() { $result = ; if (!empty($this->attribute_list)) { foreach ($this->attribute_list as $attr => $value) { $result .= {$attr}=\\{$value}\\; } } return $result; }}form/** * the form object, will describe a single form. * after constructing it is done, it should format into a cool, simple, valid, readable, html code. */class form extends node implements contains_nodes { public $element = form; public $self_contained = false; /** * @var node[] $node_list this will hold all of the nodes in the form. including fields and inputs. */ public $node_list; /** * @var string $action will hold the action for the form. this is a required field. */ public $action; /** * @var string $method will hold the form submission method for the form. defaults to post. */ public $method = form::method_post; const method_post = 'post'; const method_get = 'get'; /** * @param string $action page to which the form submits. * @param string $method post or get, will throw an exception otherwise. * @param array $attribute_list miscellaneous attributes for the form element. */ public function __construct($action, $method = self::method_post, array $attribute_list = array()) { $this->action = $action; $this->method = $method; $this->attribute_list = $attribute_list; if (($method != self::method_get) && ($method != self::method_post)) { throw new \\exception(form method must be either post or get); } } /** * @param node $node node to add. * * @return form to not break the chain */ public function add(node $node) { $this->node_list[] = $node; return $this; } /** * @return string format and stringify the form. */ public function __tostring() { //open tag, usually <form ...> $result = <{$this->element} action=\\{$this->action}\\ method=\\{$this->method}\\; //attribute list $result .= $this->string_attribute_list(); //close opening tag $result .= >; //loop through the nodes foreach ($this->node_list as $node) { $result .= . self::tab . str_replace( , . self::tab, $node->__tostring()); //the replace is to keep the code indented and formatted properly. } //close form element $result .= </{$this->element}>; return $result; }}input/** * class to describe a single input node. */class input extends node { public $element = input; public $self_contained = true; public $type; public $label; public $name; public $default_value; public $label_before_input = true; /** * @param string $type * @param string $name * @param label $label * @param string $default_value * @param array $attribute_list * * constructor for input node. */ public function __construct($type, $name, label $label, $default_value = , array $attribute_list = array()) { $this->type = $type; $this->name = $name; $this->label = $label; $this->default_value = $default_value; $this->attribute_list = $attribute_list; } /** * @return string formatted input html. */ public function __tostring() { //label element open (usually <label $result = <{$this->label->element}; //begin attribute list $result .= $this->label->string_attribute_list(); //close opening tag $result .= >; //if we want label before the input... if ($this->label_before_input) { $result .= . self::tab . $this->label->text; } //begin input element usually <input ... $result .= . self::tab . <{$this->element} type=\\{$this->type}\\ name=\\{$this->name}\\; //default value if (!empty($this->default_value)) { $result .= value=\\{$this->default_value}\\; } //attribute list $result .= $this->string_attribute_list(); //close input element $result .= >; //if we want label after input if (!$this->label_before_input) { $result .= . self::tab . $this->label->text; } //close label element (usually </label> $result .= </{$this->label->element}>; /* * final result should look like: * <label [label-attributes]> * [text-if-before] * <input type=[type] name=[name] value=[value] [input-attributes] * [text-if-after] * </label> */ return $result; }}label/** * class to describe a single label node. * labels should be contained inside of inputs on a 1:1 relationship. */class label extends node { public $element = label; /** * @param string $text * @param array $attribute_list */ public function __construct($text, array $attribute_list = array()) { $this->text = $text; $this->attribute_list = $attribute_list; } /** * @return string returns label text only. labels can only be part of inputs. */ public function __tostring() { return $this->text; }}fieldset/** * class to describe a single fieldset node. * * @implements contains_nodes */class fieldset extends node implements contains_nodes { public $element = fieldset; /** * @var node[] */ public $node_list; public $legend; /** * @param $legend * * construct a fieldset element */ public function __construct($legend) { $this->legend = $legend; } /** * @param node $node * * @return fieldset to not break the chain * * add a node to the fieldset. */ public function add(node $node) { $this->node_list[] = $node; return $this; } /** * @return string generate a formatted node list of the fieldset. */ public function __tostring() { //open element (usually <fieldset) $result = <{$this->element}; //attribute list $this->string_attribute_list(); //close opening tag $result .= >; //legend text $result .= . self::tab . <legend>{$this->legend}</legend>; //loop through node list foreach ($this->node_list as $node) { $result .= . self::tab . str_replace( , . self::tab, $node->__tostring()); //the replace is to keep the code indented and formatted properly. } //close fieldset tag $result .= </{$this->element}>; return $result; }}buttonclass button extends node { public $element = button; public function __construct($text, array $attribute_list = array()) { parent::__construct($this->element,$text, node::not_self_contained, $attribute_list); }}submitclass submit extends button { public $type = submit; public function __construct($text = submit, array $attribute_list = array()) { $attribute_list[type] = $this->type; parent::__construct($text, $attribute_list); }}testing/* * testing begins! */$form = new form(index.php, form::method_get);$field = new fieldset(fieldset);$field->add(new input(text, name, new label(input inside of fieldset)));$form ->add( new input( text, //type test, //name new label(testing input, array(class => label)), //label woot, //default value array(id => test) //attribute list )) ->add(new node(hr, , node::self_contained)) ->add(new submit(go));echo $form . ;echo <pre>;echo print_r($form);echo </pre>;",
    "present_kp": [
      "php",
      "form"
    ],
    "absent_kp": [
      "object oriented"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "turning structural code into object oriented code. this is a bit experimentation from my part because i had written a lot of procedural code back in my school days hence i have trouble in thinking in oop way i.e. to find proper classes and relationship between them. i know i cannot do this every time but i need to create some kind of correlation which may help me understand if i am thinking in right direction or not.below is the code of a parking lot allocation problem:/* a parking lot can have many slots which has different size.a slot can be occupied by at most two vehicles.checkin: * find the slot for the vehicle based on sizecheckout: * calculate total fare based on vehicle size and durationquestions: * design bottom up or top down i.e. vehicle, slot or parkinglot */ // slot details, 3 slotscapacity = [4, 2, 2];// vehicle details, 4 vehiclessizes = [2, 1, 2, 2];parked = [1, 0, 1, 1];start = ['001', nil, '002', '003'];end = ['005', nil, '006', '008'];// slot vehicle relationvehicleslot = [0, nil, 0, 1];function checkin (vehicle) { for slot, size in capacity: if capacity[slot] >= size[vehicle]: start[vehicle] = currenttimestamp; vehicleslot[vehicle] = slot; capacity[slot] -= size[vehicle]; return true; return 'no slot empty'}function checkout (vehicle) { if (vehicleslot[vehicle] == nil): return 'invalid checkout' slot = vehicleslot[vehicle]; capacity[slot] += size[vehicle]; end[vehicle] = currenttimestamp; vehicleslot[vehicle] = nil; return pricingalgo(vehicle);}function pricingalgo (vehicle) { // size * duration var duration = (end[vehicle] - start[vehicle]); return sizes[vehicle] * duration;}the code may not be completely correct but it shows what i am trying to do at least. i need to know if there is a way to evolve this code to a good oop program?",
    "present_kp": [
      "object oriented"
    ],
    "absent_kp": [
      "programming practices",
      "object oriented design",
      "domain driven design"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to print colored text using a black and white printer?. i printed a pdf containing colored text using lp file.pdf. i expected the text to show up in grey or black, but it was not printed at all.is this expected behaviour? can i change a setting somewhere to handle color in some sensible manner?my printer is an oki 4100.",
    "present_kp": [
      "printer"
    ],
    "absent_kp": [
      "printing",
      "cups"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "maximize the expected number of losers - is it np-hard?. i am trying to find a reduction for a problem that seems np-hard:let me start from a toy example. consider 3 elements, $a$, $b$, and $c$.you want to choose two pairs out of the three pairs and compare the two elements of each pair. there is a true underlying ordering of the elements, which we don't know. our objective is to pick the comparisonsthat will maximize, in expectation, the number of elements that will lose at least one comparison. for example, lets assume that we choose $(a,b)$ and $(b,c)$. there are four possibilities:(1) $a>b$ and $c>b$ with probability $ rac{2}{6}$ ($2$ out of the $3!$ possible permutations respect $a>b$ and $c>b$)(2) $a>b$ and $c<b$ with probability $ rac{1}{6}$(3) $a<b$ and $c>b$ with probability $ rac{1}{6}$(4) $a<b$ and $c<b$ with probability $ rac{2}{6}$in the case of (1), only $b$ loses at least one comparison, in (2), $b$ and $c$ lose at least one comparison, and so on. the expected number of elements losing at least one comparison is:$ rac{2}{6} * 1 + rac{1}{6} * 2 + rac{1}{6} * 2 + rac{2}{6} * 2 = 1 rac{2}{3}$note that the outcome of each comparison is based on the true ordering, which we don't know..in this toy example, any two pairs give the same expected number of losers. in its general form, the problem states that we have $n$ elements, and we can ask for $b$ comparisons (the problem becomes interesting when $b> rac{n}{2}$). making it even more general, we can have the outcome of previous comparisons as input and try to maximize the losers out of those elements that still haven't lost any comparison. i am interested in the second, more general, case but i feel that even without the previous comparisons' outcome, the problem is still np-hard.the most helpful result i have found until now, is that counting linear extensions of a partially ordered set is #p-complete [1]. i would greatly appreciate any ideas about a reduction, or any obvious reduction that i can not see.thank you![1] brightwell, graham r.; winkler, peter (1991), counting linear extensions, order 8 (3): 225242, paper",
    "present_kp": [],
    "absent_kp": [
      "cc.complexity theory",
      "graph theory",
      "np hardness",
      "reductions",
      "directed acyclic graph"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how do i install a printer with cups on chrome os?. i installed neverware cloudready on an old dell workstation i had lying around. because my printer, a samsung ml-1665, does not have wireless printing, i want to try to take advantage of the fact that chrome os now has cups preinstalled.chrome os only detects hp usb printers at the moment, unfortunately.i downloaded the linux print and scan driver from samsung and copied the required filter (rastertospl) and ppd (samsung_ml-1660_series.ppd).i tried using the web interface, but attempting to enable it resulted in an internal server error.i also tried using the lpadmin command, and it appeared to work, as lpstat reported the printer as idle and functional. however, when sending a test page using the lp command, nothing was printed, despite lpstat reporting that the job was received a split second after i sent and that the printer was again idle afterwards.i am unsure what other information would be useful. i am new here, so please inform me if i am not following proper etiquette or if i should include additional details.edit:localhost / # lp hirequest id is 1665-1 (1 file(s))localhost / # lpstat -p 1665printer 1665 now printing 1665-1. enabled since wed 15 mar 2017 07:25:51 pm pdtcolor manager: no profiles specified in ppdlocalhost / # lpstat -p 1665printer 1665 now printing 1665-1. enabled since wed 15 mar 2017 07:25:51 pm pdtcolor manager: no profiles specified in ppdlocalhost / # lpstat -p 1665printer 1665 is idle. enabled since wed 15 mar 2017 07:25:59 pm pdtcolor manager: no profiles specified in ppdedit:i was able to run the samsung install script by remounting /home/chronos/user with the exec and rw flags. however, there was one error: cups restart failed.i also remounted / with the same flags, which allowed me to enable the web interface with cupsctl. however, i could not connect to localhost:631 in the browser. a test with openssl gave the following:localhost uld # openssl s_client -connect localhost:631connect: connection refusedconnect:errno=111edit:new error. i think installing the driver using the samsung script configured a bunch of stuff i don't know about, as setting the printer up with lpadmin and printing a test page now yields the following:localhost uld # lpstat -p 1printer 1 now printing 1-1. enabled since thu 16 mar 2017 05:48:13 pm pdtfailed to load ppm imagethis seems to have to do with ghostscript being missing.edit: it seems that this has nothing to do with ghostscript, as i installed it, and nothing changed.",
    "present_kp": [
      "printing",
      "cups",
      "chrome os"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to install git for centos?. i tried several suggestions from this answer but none of them worked.[root@308321 sugar-dev]# yum install gitloaded plugins: fastestmirrorloading mirror speeds from cached hostfile * base: mirror.ubiquityservers.com * epel: <url> * extras: mirrors.usinternet.com * updates: mirrors.serveraxis.netsetting up install processresolving dependencies--> running transaction check---> package git.x86_64 0:1.8.2.1-1.el5 will be installed--> processing dependency: perl-git = 1.8.2.1-1.el5 for package: git-1.8.2.1-1.el5.x86_64--> processing dependency: perl(term::readkey) for package: git-1.8.2.1-1.el5.x86_64--> processing dependency: perl(git) for package: git-1.8.2.1-1.el5.x86_64--> processing dependency: perl(error) for package: git-1.8.2.1-1.el5.x86_64--> processing dependency: libssl.so.6()(64bit) for package: git-1.8.2.1-1.el5.x86_64--> processing dependency: libexpat.so.0()(64bit) for package: git-1.8.2.1-1.el5.x86_64--> processing dependency: libcurl.so.3()(64bit) for package: git-1.8.2.1-1.el5.x86_64--> processing dependency: libcrypto.so.6()(64bit) for package: git-1.8.2.1-1.el5.x86_64--> running transaction check---> package compat-expat1.x86_64 0:1.95.8-8.el6 will be installed---> package git.x86_64 0:1.8.2.1-1.el5 will be installed--> processing dependency: libcurl.so.3()(64bit) for package: git-1.8.2.1-1.el5.x86_64---> package openssl098e.x86_64 0:0.9.8e-17.el6.centos.2 will be installed---> package perl-error.noarch 1:0.17015-4.el6 will be installed---> package perl-git.x86_64 0:1.8.2.1-1.el5 will be installed--> processing dependency: perl(:module_compat_5.8.8) for package: perl-git-1.8.2.1-1.el5.x86_64---> package perl-termreadkey.x86_64 0:2.30-13.el6 will be installed--> finished dependency resolutionerror: package: perl-git-1.8.2.1-1.el5.x86_64 (epel) requires: perl(:module_compat_5.8.8)error: package: git-1.8.2.1-1.el5.x86_64 (epel) requires: libcurl.so.3()(64bit) you could try using --skip-broken to work around the problem you could try running: rpm -va --nofiles --nodigest[root@308321 sugar-dev]# yum -y install git --disablerepo=updatesloaded plugins: fastestmirrorloading mirror speeds from cached hostfile * base: mirror.ubiquityservers.com * epel: <url> * extras: mirrors.usinternet.comsetting up install processresolving dependencies--> running transaction check---> package git.x86_64 0:1.8.2.1-1.el5 will be installed--> processing dependency: perl-git = 1.8.2.1-1.el5 for package: git-1.8.2.1-1.el5.x86_64--> processing dependency: perl(term::readkey) for package: git-1.8.2.1-1.el5.x86_64--> processing dependency: perl(git) for package: git-1.8.2.1-1.el5.x86_64--> processing dependency: perl(error) for package: git-1.8.2.1-1.el5.x86_64--> processing dependency: libssl.so.6()(64bit) for package: git-1.8.2.1-1.el5.x86_64--> processing dependency: libexpat.so.0()(64bit) for package: git-1.8.2.1-1.el5.x86_64--> processing dependency: libcurl.so.3()(64bit) for package: git-1.8.2.1-1.el5.x86_64--> processing dependency: libcrypto.so.6()(64bit) for package: git-1.8.2.1-1.el5.x86_64--> running transaction check---> package compat-expat1.x86_64 0:1.95.8-8.el6 will be installed---> package git.x86_64 0:1.8.2.1-1.el5 will be installed--> processing dependency: libcurl.so.3()(64bit) for package: git-1.8.2.1-1.el5.x86_64---> package openssl098e.x86_64 0:0.9.8e-17.el6.centos.2 will be installed---> package perl-error.noarch 1:0.17015-4.el6 will be installed---> package perl-git.x86_64 0:1.8.2.1-1.el5 will be installed--> processing dependency: perl(:module_compat_5.8.8) for package: perl-git-1.8.2.1-1.el5.x86_64---> package perl-termreadkey.x86_64 0:2.30-13.el6 will be installed--> finished dependency resolutionerror: package: perl-git-1.8.2.1-1.el5.x86_64 (epel) requires: perl(:module_compat_5.8.8)error: package: git-1.8.2.1-1.el5.x86_64 (epel) requires: libcurl.so.3()(64bit) you could try using --skip-broken to work around the problem you could try running: rpm -va --nofiles --nodigest[root@308321 sugar-dev]# yum install git --disableexcludes=main --skip-brokenloaded plugins: fastestmirrorloading mirror speeds from cached hostfile * base: mirror.ubiquityservers.com * epel: <url> * extras: mirrors.usinternet.com * updates: mirrors.serveraxis.netsetting up install processresolving dependencies--> running transaction check---> package git.x86_64 0:1.8.2.1-1.el5 will be installed--> processing dependency: perl-git = 1.8.2.1-1.el5 for package: git-1.8.2.1-1.el5.x86_64--> processing dependency: perl(term::readkey) for package: git-1.8.2.1-1.el5.x86_64--> processing dependency: perl(git) for package: git-1.8.2.1-1.el5.x86_64--> processing dependency: perl(error) for package: git-1.8.2.1-1.el5.x86_64--> processing dependency: libssl.so.6()(64bit) for package: git-1.8.2.1-1.el5.x86_64--> processing dependency: libexpat.so.0()(64bit) for package: git-1.8.2.1-1.el5.x86_64--> processing dependency: libcurl.so.3()(64bit) for package: git-1.8.2.1-1.el5.x86_64--> processing dependency: libcrypto.so.6()(64bit) for package: git-1.8.2.1-1.el5.x86_64--> running transaction check---> package compat-expat1.x86_64 0:1.95.8-8.el6 will be installed---> package git.x86_64 0:1.8.2.1-1.el5 will be installed--> processing dependency: libcurl.so.3()(64bit) for package: git-1.8.2.1-1.el5.x86_64---> package openssl098e.x86_64 0:0.9.8e-17.el6.centos.2 will be installed---> package perl-error.noarch 1:0.17015-4.el6 will be installed---> package perl-git.x86_64 0:1.8.2.1-1.el5 will be installed--> processing dependency: perl(:module_compat_5.8.8) for package: perl-git-1.8.2.1-1.el5.x86_64---> package perl-termreadkey.x86_64 0:2.30-13.el6 will be installed--> finished dependency resolutionpackages skipped because of dependency problems: compat-expat1-1.95.8-8.el6.x86_64 from base git-1.8.2.1-1.el5.x86_64 from epel openssl098e-0.9.8e-17.el6.centos.2.x86_64 from base 1:perl-error-0.17015-4.el6.noarch from base perl-git-1.8.2.1-1.el5.x86_64 from epel perl-termreadkey-2.30-13.el6.x86_64 from base[root@308321 sugar-dev]# git-bash: git: command not found[root@308321 sugar-dev]#[root@308321 sugar-dev]# uname -alinux 308321.oliveyou.net 2.6.32-220.2.1.el6.x86_64 #1 smp fri dec 23 02:21:33 cst 2011 x86_64 x86_64 x86_64 gnu/linux",
    "present_kp": [
      "centos",
      "yum"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how can i reformat tabular data using awk?. i have a file contain following output.ora.abc.db 1 online online servera open 2 online online serverb openora.xyz.db 1 online online servera open 2 online online serverb open 2 online online serverc openi want to format this file in following way, but i want to use shell script only, i believe we can use awk but i don't have logic abc abc1 online serveraabc abc2 online serverbxyz xyz1 online serveraxyz xyz2 online serverbxyz xyz3 online serverc",
    "present_kp": [
      "awk"
    ],
    "absent_kp": [
      "text processing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "'$xauthority' appears from 'nowhere' on su+tmux. when i switched from su+bash to su+tmux+zsh i noticed that i get $xauthority variable defined as /root/.xauthxxxxxx where xxxxxx are 6 random alphanumeric characters. with previous configuration x worked with root flowlessly but now i need to copy ~username/.xauthority to $xauthority.variable is nowhere defined (i checked .zshrc, /etc/profile*, /etc/profile.d/* etc.).# envterm=screenshell=/usr/bin/tmuxuser=toortmux=/tmp//tmux-0/default,6495,3path=/sbin:/bin:/usr/sbin:/usr/binpwd=/rootshlvl=2home=/rootlogname=toordisplay=:0.0xauthority=/root/.xauthuszll4colorterm=gnome-terminal_=/bin/envoldpwd=/rooteditor=vimvcs_info_msg_0_=vcs_info_msg_1_=edit:% echo $xauthority /home/mpiechotka/.xauthority% su password:# echo $xauthority /root/.xauthuszll4# ls $xauthorityls: cannot access /root/.xauthuszll4: no such file or directory# cat .tmux.conf set -g default-command /bin/zshset -g default-shell /bin/zshsu is aliased to su - toor and it opens tmux as shell. toor is an alias of root with different shell.ps. i just discovered that it appears on normal su as well. it did not some time ago...edit 2 set-enviroment didn't help eitheredit 3 xhost +localhost did not helped but xhost + (disabling all control) did helped.",
    "present_kp": [
      "shell",
      "su",
      "tmux"
    ],
    "absent_kp": [
      "login"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "code letters as numbers. the script bellow is asking the user to enter something then it print the entry as an output:#!/bin/bash printf \\e[31mtype/enter something: $pdir\\e[m ; read -e userinput if [[ -n $userinput ]] then pdir=$userinput fi printf \\e[96myour code is: $pdir\\e[m ;my questions are:how can the previous code read only letters in userinput? how can i convert my entry userinput as an output numbers for example if i enter john how can i get an output numbers.",
    "present_kp": [
      "bash"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "is it possible to force a group member to run something allowed by sudoers as himself only?. the sudoers line is%game_servers all= nopasswd:/usr/bin/renicewhich allows group members renice any process run by any user without pw, but i'd like to allow group members renice their own processes only, to negative value.i couldn't spot the answer from man sudoers, from where i got the idea to change all=(root:root) to all=, which proved to be bad ideaall=() is syntax error.",
    "present_kp": [
      "sudo",
      "nice"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "domain renewal/transfer gandi.net. i'm in kind of a difficult situation here: i have a domain (a .at domain) and transfered it to gandi.net a couple of months ago. now it's time to renew my domain but when i tried to do the renewal it just said the renewal could not be done online. after that i got an invoice from the nic.at (registry for .at domains) which stated that gandi wouldnt pay any more invoices for my domain. i can't access the domain over the gandi.net website so i cant change any dns/whois information. i can't transfer my domain back to gandi since the mail address in the whois was an @contact.gandi.net which doesnt exist anymore since the domain isn't listed anymore in my gandi account.i contacted the support of gandi and got no answer what so ever(for over 1 week now). the domain is about to run out + i have to pay the invoice from nic.at if i don't have it transfered within the next couple of weeks.now my question is: what do you suggest my next step is towards regaining control over my domain? did anyone experience simular issues with gandi.net?thanks for the help guys, i'm desperate for ideas/suggestions",
    "present_kp": [
      "domains"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "usb keyboard doesn't work with debian installer. i have a wired usb keyboard. i've written the latest netinstall wheezy image on my usb pendrive (using dd). it boots but the installer doesn't recognize the keyboard! (the same pendrive works on my laptop) i am guessing my motherboard is no longer supported by this installer. how can i make the keyboard work with the installer?",
    "present_kp": [
      "debian",
      "usb",
      "keyboard"
    ],
    "absent_kp": [
      "system installation"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is the effect of a lone backtick at the end of a command line?. i accidentally typed in cd ' into terminal today and terminal didstrange things.it put a > signed on the next line followed by my cursor like it wanted some input. no matter what i entered continued to do the same thing until i terminated the command.out of curiosity what happened? was this a bug or a feature?",
    "present_kp": [
      "command line"
    ],
    "absent_kp": [
      "shell"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "replacement of particular number in unix files. i want to make a comand that can replace particular number in unix file sysytem for eg:-zar zec zda-9 2 34 8 -95 6 7i have zar, zec, zda as my header and -9 in zda must be replaced by +9. what command should i write so it will change in particular that number without changing others value like -9 present in zar field.",
    "present_kp": [],
    "absent_kp": [
      "linux",
      "shell script",
      "sed",
      "awk"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "partitioning for dual boot of two linux systems. i would like to have dual boot of two linux distributions on my laptop.one stable (mint 18 that i found to work well with yoga 3 pro) and one that would be probably changing often (i want to try different distros like fedora or opensuse, without destroying my stable working environment).i wonder how i should do partitioning of boot partitions here. i would like to do it in the easiest possible way that would allow to remove second linux easily.i was thinking about having two boot partitions, one for each linux.during installation of main linux i could choose to put boot loader into mbr, and for second one into the proper partition. but then i would have to update mint's grub everytime i install new test distro, would that be a good solution? also i am not sure if i can have two efi partitions.what would be the most stable and safe option here?update 1first of all i did more research and realized that i was confused about few things. i didn't know that device for boot loader installation [1] option during mint installation is completely ignored during installation in uefi mode [2] (1). also poor naming in ubuntu installer (2) made me believe that esp is doing the job of /boot partion, not the mbr.knowing that i am thinking of following partition scheme (256gb ssd drive):scheme:/dev/sda1 efi system partition fat32 /boot/efi 512mb (esp partion)/dev/sda2 ext2 /boot 512mb (boot for mint)/dev/sda3 ext2 512mb (boot for other)/dev/sda4 lvm2 ( / for each linux, shared home, swap )and then during installation of each linux i would make 4 mounting points of correct partitions:/boot/efi/boot/swapis that reasonable? and do i understand correctly that option for choosing device for boot loader installation during mint installation is redundant in efi mode and i should not worry about it anymore? and do i understand correctly that now shared esp will just have a config to start loading grub from a boot partition it got set up as default?update 2i am going with a scheme i proposed above. however creating partitions /dev/sda1 - /dev/sda3 through gparted resulted in some errors in mint installer. i repeated the process by destroying this partitions and creating them again from a mint installer and it went smooth. /dev/sda4 i created before running mint installer in gparted and created local volumes from terminal. this tutorial on lvm was very helpful on that [4].update 3after installing mint i proceed with installing fedora (3), after that system by default booted into fedora but in bios i was able to choose ubuntu or fedora and each of them worked well.i change in bios to boot first from mint and then from mint i executedsudo grub-mkconfig -o /boot/grub/grub.cfgwhich allows me basically to boot both linuxes now.and because i made assumption that other linux is for testing purpose i more or less achieved what i wanted. if i will remove fedora and install in its place for example opensuse i could probably simply execute above command again to obtain stable boot system.comments(0) i am keeping that question updated all the time just in case some one may find it useful in the future.(1) i did installation on a different computer some time ago that had two hard drives (separate devices). /dev/sda was fully meant for windows 10 and i wanted to install mint on /dev/sdb. despite the fact that i selected device for boot loader installation as /dev/sdb it found esp on the other drived and used that partition for booting.(2) efi system partition (esp) is named in mint (ubuntu) installer as efi boot partition [3].(3) i had to be super careful with choosing mount points and partitioning her.links:[1] <url>] <url>] <url>] <url>",
    "present_kp": [
      "partition",
      "dual boot",
      "uefi"
    ],
    "absent_kp": [
      "linux mint"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "my queue implementation is correct?. i have read clrs introduction to algorithms book, now i'm on elementary data structure chapter. i read about queue concept and try to implement in c.i have tested a lot it work fine, but i have a little doubt on its working. please tell me this implementation is correct or not. #include <stdio.h>#include <stdlib.h>#define max 4struct queue { int array[max]; int head; int tail; };void enqueue(struct queue* p,int data) {// error checking for overflow if((p->tail == max-1 && p->head == 0 ) || (p->head == p->tail+1)) { printf( queue is full!! ); return; } p->array[p->tail] = data; if(p->tail == max-1) p->tail = 0; else p->tail = p->tail+1; printf( %d data is added,data);}int dequeue(struct queue* p) { if(p->head == p->tail) { printf( queue is empty!!); return -1; } int x = p->array[p->head]; if(p->head == max-1) p->head = 0; else p->head = p->head+1; printf( %d data is removed,x); return x;}void showqueue(struct queue* p) { int i = 0; i = p->head; printf(p->head = %d & p->tail = %d,p->head,p->tail); printf( queue elements:); while(i != p->tail) { printf(%d ,p->array[i]); if(i == (max-1)) i = 0; else i = i+1; }}int main() { struct queue* q; q = (struct queue*) malloc(sizeof (struct queue)); if(q == null) { printf(memory allocation failed); return -1; } q->head = 1; q->tail = 1; /* enqueue(q,1); showqueue(q); dequeue(q); showqueue(q); enqueue(q,1); enqueue(q,1); showqueue(q); enqueue(q,1); enqueue(q,1); enqueue(q,1); showqueue(q); printf(head = %d & tail = %d ,q->head,q->tail);*/ enqueue(q,98); enqueue(q,20); enqueue(q,16); enqueue(q,2);// enqueue(q,1);// enqueue(q,2); showqueue(q); dequeue(q); dequeue(q); showqueue(q); enqueue(q,5); showqueue(q); enqueue(q,3); showqueue(q); enqueue(q,10); showqueue(q); dequeue(q); showqueue(q); enqueue(q,12); dequeue(q); showqueue(q); enqueue(q,2); dequeue(q); showqueue(q); enqueue(q,1); showqueue(q); dequeue(q); showqueue(q); dequeue(q); showqueue(q); dequeue(q); showqueue(q); dequeue(q); showqueue(q); return 0;}",
    "present_kp": [
      "c",
      "queue"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "mapping protein sequences on an abscissa?. i have asked this question in the stack forum but i think here, it's more appropriate.what would you do to represent the distribution of small sequences (<=40 residues) according to their position in their initial protein?i have several sequences as below. the 1st column is the number of the current sequence. the second column is the start position and the 3rd column is the stop position of the current sequence in its initial protein.1 18 34 2 39 55 3 30 46 4 20 36 5 22 46 6 22 46 7 25 50 8 33 50 9 46 63those sequences do not come all from the same protein, they come from different proteins which have different length.what would be the best idea to map those sequences on an abscissa to see if they are more located at the beginning of a protein or more at the end or more in the middle, considering that the proteins don't all have the same length?i wrote an algorithm that map those sequences on the abscissa, according to their start and stop position but the problem is that the graph can't be interpreted since proteins have different length. my graph shows that sequences are more are the beginning of proteins but this is only due to the fact that some proteins are shorter than others, so this is an issue. anyone has a better idea for this?thanks in advance.",
    "present_kp": [],
    "absent_kp": [
      "statistics",
      "mapping strategy"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "header function with increasing number in bash. i would like to have a function in bash that i can use in some install scripts to announce that the next paragraph is starting.a simple solution (with colors) would beheadline(){ echo -e \\e[1;34m########################################### echo -e ########## \\e[1;37m$* echo -e \\e[1;34m###########################################\\e[0m}but how do i add an increasing number in it?",
    "present_kp": [
      "bash"
    ],
    "absent_kp": [
      "shell script"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "jquery plugin that makes a slider out of an list. this is one of my very first jquery plugins. in short: it makes a slider out of an <ul> list filled with <li> items containing <img>s. just follow this jsfiddle link to see it in action.i therefore wanted to know if there were any flaws in my code. i would like it be reviewed by anyone who would be willing to let me know of any major problem in it.so here's the html:<ul id=slider> <li><img src=<url> alt=slider_1/></li> <li><img src=<url> alt=slider_2/></li> <li><img src=<url> alt=slider_3/></li> <li><img src=<url> alt=slider_4/></li> <li><img src=<url> alt=slider_4/></li></ul>and the jquery plugin code:;(function($) { $.fn.extend({ sdslider: function(options) { if (options && typeof(options) == 'object') { options = $.extend({}, $.sdslider.defaults, options); } // be sure the plugin is attached to only one domelement if($(this).length == 1) { new $.sdslider(this, options); } else if($(this).length > 1) { console.error($.messages.severalelements); } else if($(this).length == 0) { console.error($.messages.zeroelement); } return this; } }); // error messages to be displayed in the console $.messages = { imgandlimismatch: '[jquery.sdslider] error: the number of list items and the number of <img src> mismatch.', severalelements: '[jquery.sdslider] error: several dom element have been detected. are you sure you're using the right selector?', noimgfound: '[jquery.sdslider] error: no <img> tag was found within the <li>.', zeroelement: '[jquery.sdslider] error: couldn't find the slider. it seems you have targetted no element.' }; $.sdslider = function(el, option) { $(window).on('load', function() { var options = option || $.sdslider.defaults , $li = $(el).find('> li') , $img = $li.find('> img') , imgsrcs = [] , imgheights = [] , imgwidths = [] , $wrapper , i , index; if(!$img.length) { console.error($.messages.noimgfound); return; } // mark the first slide as active $(el).find('> li:first-child').addclass('active'); // add border if asked to if(options.border && typeof(options.border) === 'object') { $img.css('border', options.border.width + 'px solid ' + options.border.color); } $img.each(function(i) { $(this).attr('data-image', i); imgsrcs.push($(this).attr('src')); imgheights.push($(this).outerheight()); imgwidths.push($(this).outerwidth()); }); // if each li has an img if(imgsrcs.length === $li.length) { maxheight = math.max.apply(math, imgheights); maxwidth = math.max.apply(math, imgwidths); // build the wrapper div $wrapper = $('<div>').attr('id', 'sdslider-wrapper').css({ width: maxwidth, height: maxheight }); $(el).wrap($wrapper); // add arrows if asked to if(options.arrows) { $.sdslider.addarrows(el, options, $wrapper); } // add dots controls if asked to if(options.controls) { $controlwrapper = $.sdslider.addcontrols(el, options); // on click on the dots $controlwrapper.find('> span').on('click', function(e) { // make slider slide i = $(this).index(); $.sdslider.appear(el, options, i); }); } // autostarting the slider if asked to if(options.autostart.active) { window.setinterval(function() { index = $(el).find('> li.active').index(); if(index + 1 < $img.length) { $.sdslider.appear(el, options, index + 1); } else { $.sdslider.appear(el, options, index - $img.length + 1); } }, options.autostart.delay); } } else { console.error($.messages.imgandlimismatch); } return; }); }; // function to add the dots control on the bottom of the slider $.sdslider.addcontrols = function(el, options) { var $li = $(el).find('> li') , $controlwrapper = $('<div>') .addclass('control-wrapper') .css({ 'text-align': 'center', 'bottom': '35px', 'background-color': 'rgba(170, 170, 170, 0.5)' }) , $controls; $li.each(function(i) { $controls = $('<span>').attr('data-image', i); if(!i) { $controls.addclass('active'); } $controlwrapper.append($controls); }); $(el).after($controlwrapper); return $controlwrapper; } // function to add the controlling left and right arrows on the sides $.sdslider.addarrows = function(el, options, $wrapper) { var classes = 'sdarrow fa-4x fa' , $left = $('<i />').addclass(classes + ' sdarrow-left fa-arrow-circle-left disabled').css('left', 0) , $right = $('<i />').addclass(classes + ' sdarrow-right fa-arrow-circle-right').css('right', 0) , $img = $(el).find('> li').find('> img') , index; ; $(el).after($left).before($right); // if right arrow is clicked $right.on('click', function() { index = $(el).find('> li.active').index(); if(index + 1 < $img.length) { // if this is not the end of the slider // make slider go right $.sdslider.appear(el, options, index + 1); } }); // if left arrow is clicked $left.on('click', function() { index = $(el).find('> li.active').index(); if(index - 1 >= 0) { // if this is not the beginning of the slider // make slider go left $.sdslider.appear(el, options, index - 1); } }); return; }; // function to make the slider slide (where 'i' is the index to go to) $.sdslider.appear = function(el, options, i) { var activeimgindex = $(el).find('> li.active').index() , animation = {} , gap = 0 , $li = $(el).find('> li') , $img = $li.find('> img') , $left = $(el).parent('div').find('i.sdarrow-left') , $right = $(el).parent('div').find('i.sdarrow-right'); // if the slider is not currently sliding if(!$li.is(':animated')) { $li.removeclass('active').eq(i).addclass('active'); if(activeimgindex < i) { // going right gap = i - activeimgindex; animation['left'] = '-=' + ($li.find('> img').eq(i).outerwidth() * gap) + 'px'; } else { // going left gap = activeimgindex - i; animation['left'] = '+=' + ($li.find('> img').eq(i).outerwidth() * gap) + 'px'; } // slider animation $li.each(function() { $(this).animate(animation, { duration: options.duration }); }); // add disabled class to arrows if needed to if(options.arrows) { if(i + 1 === $img.length) { $right.addclass('disabled'); } else { $right.removeclass('disabled'); } if(i === 0) { $left.addclass('disabled'); } else { $left.removeclass('disabled'); } } // add active class to corresponding dot $('.control-wrapper') .find('> span') .removeclass('active') .eq(i) .addclass('active') ; } return; }; // plugin default options $.sdslider.defaults = { autostart: { active: false, delay: 1000 }, border: { color: '#000', width: 0 }, controls: true, arrows: true, duration: 1000 };})(jquery);the code to initiate the plugin is as such:jquery(function($) { $('#slider').sdslider({ autostart: { // an object containing: active: false, // a boolean indicating whether to start the slider automatically delay: 1000 // and a integer specifying the delay between each slide }, border: { // an object containing: color: '#f00', // the color of each image border as a string width: 0 // the width in px of each image border as an integer }, controls: true, // a boolean to specify whether to display the dots controlling each slide arrows: true, // a boolean to specify whether to display the left and right arrows duration: 500 // the duration in ms of each animation as a integer });});you can see a more complete readme on my github account.i would also like to know if my code is not cross-browser. please feel free to spare no details about this point and how it can be fixed.my general concerns are about following the best practices in order to write a good jquery plugin so i'm looking for any suggestions on how to improve this code.",
    "present_kp": [
      "jquery",
      "animation",
      "plugin"
    ],
    "absent_kp": [
      "javascript",
      "portability"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to visually document interactive user interfaces for fast consumption. i'm writing documentation for some interactive ajax user interface but instead of writing everything in words i would rather provide diagrams that are faster consumed than whole paragraphs of text. i did provide page navigation flow diagram but that's very high level before you drill down to individual pages and their inner workings.examplesuppose i have to document a typical search panel:is collapsed by default and can be expanded on clickhas basic search fields that provide basic search criteria over my model entityhas advanced fields as well that show when one clicks a particular link which expands search with additional fieldshas two actions search and reset that are pretty much self explanatory, but have particular behaviours attached to themi know you will say that this already is documentation, but it's usually not this simple and trivial. search panel is just an example.i can see that most of my interfaces (or interface components) need two types of documentation:properties related to contained controls (i.e. drop down field with these and those values of which that particular one is selected by default)actions and behaviours that describe what happens when user interacts with controls related to those propertieswhat would be the best diagram or documentation format to describe these kind of interactive interfaces so that it's easy and quick to consume by implementers?",
    "present_kp": [
      "documentation",
      "diagrams"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "social choice, arrow's theorem and open problems ?. last few months i started to lecture myself on social choice, arrow's theorem and related results.after reading about the seminal results, i asked myself about what happens with partial order preferences, the answer is in the paper of pini et al.: aggregating partially ordered preferences: impossibility and possibility results. then, i wondered if it is possible of finding a characterization of admissible social choice functions. and again someone did it (complete characterization of functions satisfying the conditions of arrows theorem by mossel and tamuz). i won't give a full list, but any of the problems related to social choice i can think of where all solved in the last 5 years :(so, do you know if there exists a survey on what was done recently in the field and what was not done?another question is: are you aware of complexity and social choice related problems (for instance the complexity of finding the largest subset of users that are compatible for at least one social choice function, or this kind of question).",
    "present_kp": [],
    "absent_kp": [
      "co.combinatorics",
      "gt.game theory",
      "boolean functions",
      "social sciences"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is making my own copyright licence safe?. i've seen various open source libraries (actually i've seen it for assets as well) having a home-baked license in the following manner :someguy's license:1. you can use this code freely in commercial projects and modify it as you wish, but not sell it2. if you want to sell a modified version, drop me an email first, or give credits to meedit:the above example is ambiguous, so i am giving another one, i want to know if 3 lines of license will hold some ground:someguy's license:1. you can use this code in a commercial project as a 3rd party library2. you can't sell it as a derivative worki know that such license is not polished at all, for example the creative commons set of licenses seem to be short, but actually have some large legal stuff underneath it, but i wonder if at least some level of protection can be gained with a hobby license like that ?my question is, could this hold any ground in the court, or would the corporative lawyers of the company x tear it apart ?",
    "present_kp": [
      "open source",
      "legal",
      "copyright"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "merging multiple lines selectively?. i have a large file in which a header line is followed by a string of characters at multiple times. eg:file1vhbfbjbdsbvvfjbsvsvjbvhjbvjsvkjbvjbnvkvjvfile2dfhgdgffdghggsvkjbvjbnvkvjvi want to keep the header file same, but merge all characters from line 2 onwards into a single line. can anyone suggest something?",
    "present_kp": [],
    "absent_kp": [
      "files",
      "scripting"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "do i need a specific page title when a user drills down by country in a list of items?. i have list with doctors sorted by country. when a user chooses germany they will see doctors from germany. when that happens, i change url e.g. 'example.com/list?=germany' but i do not change <title> of the page. the title for germany is same as the whole list or as it is for doctors from italy. is this fine or do i need have special <title> for each choice? will google think that using the same title indicates thin content or spam?",
    "present_kp": [
      "google",
      "title"
    ],
    "absent_kp": [
      "seo"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "awesome wm wibox visible just in one tag. i want to leave visible wibox1 only one of my tags.i try use this code:your_tag:add_signal(property:selected, function(tag) your_wibox.visible = tag.selectedend) but no have success.this is my config: wibox1 = wibox({width = 190 , height = 30, x = 40, y = 180, screen = 1 }) wibox1.visible = false onlinef = blingbling.text_box({ height = 30, width= 100, v_margin = 5, font=xirod, font_size = 10, text_color = #00bfff, background_text_border = #000000,}) onlinef:set_text( offline attacks ) wibox1:set_widget(onlinef) awful.key({ modkey }, h, function () wibox1.visible=true end), awful.key({ modkey }, h, function () wibox1.visible=false end),tags = { names = { ' web', ' analistas', ' sniffing', ' engenharia', ' hardware', ' system', ' anony me', ' ataques', }, layout = { layouts[10], --web layouts[2], --multimidia layouts[8], --hacking layouts[10], --free layouts[4], --free layouts[5], --linux layouts[10], --editing layouts[1], --process } }",
    "present_kp": [
      "awesome"
    ],
    "absent_kp": [
      "window manager",
      "lua"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "encrypted flashdrive. i'm looking for a secure usb flash drive that has built-in 256-bit (or greater) encryption.requirements:at least 16gbusb 3.0completely encrypted without third-party software.having either actual buttons on the flash drive (for a pin), or having to type in a password when plugged into a computer is fineworks on both windows (vista, 7, 8.1, 10) and on gnu/linuxable to change/customize password or pincheaper it is, the better, but no actual limit for the priceunable to delete files/format without the password/pin",
    "present_kp": [
      "usb",
      "encryption",
      "flash drive"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to calculate communication waste in scrum ?. communication waste is the time spent in communication between team members in the following cases:delivering features which are not needed by the customerdelivering partial features (analysis, design, or testing is done, but the feature is canceled before it is completed and delivered)any other type of communication should be productive, even if it can be squeezed a bit.can any one give an advice how to calculate these 2 items mentioned above thanks",
    "present_kp": [
      "scrum",
      "communication"
    ],
    "absent_kp": [
      "real time"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "building and getting a form of objects that have multiple properties. i'm building a form dynamically in a web app where i'm using product_id to keep track which product i'm reading and then product_<id>_property to grab the value of it. then in my view i end up with this:for product_id in request.post.getlist('product_id'): productplacement.objects.create( product = product.objects.get(id=product_id), board = board, x = request.post.get('product_{0}_x'.format(product_id)), y = request.post.get('product_{0}_y'.format(product_id)), width = request.post.get('product_{0}_width'.format(product_id)), height = request.post.get('product_{0}_height'.format(product_id)), rotation = request.post.get('product_{0}_rotation'.format(product_id)), z = 0 )it's not very pretty, is there a nicer more readable way of doing it?",
    "present_kp": [],
    "absent_kp": [
      "python",
      "html",
      "django"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "should i worry about performance, even if there is no loss of convenience?. assume an application that shows a data table. the data is loaded from a database when the program is initialized.each value in the table is represented by an input field, where each keypress in one of those fields causes a refresh:the data is written to the databasethen, each value in the table is refreshed by reassigning the models to the data my concern here is that each value is getting updated in order to recompute a summary, even those that weren't touched at all.yet, the application runs fast, the update is not causing any inconvenience to the user. should i refactor my application nevertheless?",
    "present_kp": [
      "performance"
    ],
    "absent_kp": [
      "architecture",
      "refactoring"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "must i store a private key in a file?. i have a db-driven application that needs to communicate with another server via ssh. the web app can generate a keypair and give the user the pubkey, or it can accept the unencrypted private key in a field on a web form. it then encrypts the key with reversible encryption and stores it in its database.when the web app goes to communicate with the ssh server, i need to unencrypt the private key and provide it to ssh some way or another. i could create a secure temporary file with mktemp, point ssh at it, and then destroy the file, but that's extra work that exposes the key in a new way that seems like it shouldn't be necessary. i've looked at man ssh, man ssh-agent and man ssh-add, but there doesn't seem to be any way to use the key without adding it to a file first. what am i missing?",
    "present_kp": [
      "ssh"
    ],
    "absent_kp": [
      "key authentication",
      "ssh agent"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to grep-inverse-match and exclude before and after lines. consider a text file with the following entries:aaabbbcccdddeeefffggghhhiiigiven a pattern (e.g. fff), i would like to grep the file above to get in the output:all_lines except (pattern_matching_lines u (b lines_before) u (a lines_after)for example, if b = 2 and a = 1, the output with pattern = fff should be:aaabbbccchhhiiihow can i do this with grep or other command line tools?note, when i try:grep -v 'fff'-a1 -b2 file.txti don't get what i want. i instead get:aaabbbcccdddeeefff----fffggghhhiii",
    "present_kp": [
      "grep"
    ],
    "absent_kp": [
      "text processing",
      "awk",
      "sed"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to add a link to delicious and associate tags by sending an email?. why would you want to use email? i'd like to add links by email, so i can add to delicous an article i am reading offline from my iphone, in instapaper or read it later.on ping.fm ping.fm only provides a partial solution as it unfortunately doesn't allow you to set tags.",
    "present_kp": [
      "email",
      "delicious"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "do i need to wipe and reinstall after possible hacker attack. yesterday i noticed that the xterm title i use changed suddenly to something strange.so i suspect that someone hacked my linux laptop (i'm using arch).i changed the wifi parameters (pass, essid) and changed my password on the computer.i also reinstalled all packages to overwrite files that may have been changed.i tried to take a look at the changed files, logs and so on, but i don't have experience in security, so it was like walking in the dark ;) and of course i didn't found any signs of intrusion.do you think i should wipe the system and install from scratch?",
    "present_kp": [
      "security"
    ],
    "absent_kp": [
      "arch linux"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "requesting quality analysis test cases up front of implementation/change. recently i have been assigned to work on a major requirement that falls between a change request and an improvement. the previous implementation was done (badly) by a senior developer that left the company and did so without leaving a trace of documentation.here were my initial steps to approach this problem:considering that the release date was fast approaching and there was no time for slip-ups, i initially asked if the requirement was a must have. since the requirement helped the product significantly in terms of usability, the answer was if possible, yes.knowing the wide-spread use and affects of this requirement, had it come to a point where the requirement could not be finished prior to release, i asked if it would be a viable option to thrash the current state and revert back to the state prior to the ex-senior implementation. the answer was most likely: no.understanding that the requirement was coming from the higher management, and due to the complexity of it, i asked all usability test cases to be written prior to the implementation (by qa) and given to me, to aid me in the comprehension of this task. this was a big no-no for the folks at the management as they failed to understand this approach. knowing that i had to insist on my request and the responsibility of this requirement, i insisted and have fallen out of favor with some of the folks, leaving me in a state of baffledness.basically, i was trying a test-driven approach to a high-risk, high-complexity and must-have requirement and trying to be safe rather than sorry. is this approach wrong or have i approached it incorrectly?p.s.: the change request/improvement was cancelled and the implementation was reverted back to the prior state due to the complexity of the problem and lack of time. this only happened after a 2 hour long meeting with other seniors in order to convince the aforementioned folks.",
    "present_kp": [],
    "absent_kp": [
      "agile",
      "tdd",
      "requirements",
      "requirements management"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "validating conditions for enabling sign-up button. on an authentication view, there are some inputs, like password, email, etc and a sign up button which is enabled only if theses inputs are not empty.so what i have is this function: func cansignupbuttonbeenabled() -> bool { let password = self.passwordtextfield.text.stringbyreplacingoccurrencesofstring( , withstring: ) return (!self.emailtextfield.text.isempty && !self.passwordtextfield.text.isempty && !self.firstnametextfield.text.isempty && !self.lastnametextfield.text.isempty && countelements(password) >= 4)}and after each character entered by the user, i set on the enable property of my button the result of this function.so i was wondering if there is a cleaner solution to do it?",
    "present_kp": [],
    "absent_kp": [
      "form",
      "swift"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "can the find command work more efficiently to delete many files?. i want to delete old files in a directory that has a huge number of files in multiple subdirectories.i am trying to use the following - after some googling it seems to be the recommended and efficient way:find . -mindepth 2 -mtime +5 -print -deletemy expectation is that, this should print a file that satisfies the conditions (modified more than 5 days ago and satisfies the mindepth condition) and then delete it, and then move on to the next file.however, as this command runs, i can see that the find's memory usage is increasing, but nothing has been printed (and therefore i think nothing has been deleted yet). this seems to imply that find is first collecting all files that satisfy the conditions and after traversing the whole filesystem tree, it will print and then delete the files.is there a way to get it to delete it right away after running the tests on the file? this would help do the clean up incrementally - i can choose to kill the command and then rerun it later (which would effectively resume file deletion). this does not seem to happen currently because find has not begun deleting anything until its done traversing the gigantic filesystem tree. is there any way around this?edit - including requested data about my use case:the directories i have to clean up have a maximum depth of about 4; regular files are present only at the leaf of the filesystem. there are around about 600 million regular files, with the leaf directories containing at most 5 files. the directory fan-out at the lower levels is about 3. the fan-out is huge at the upper levels. total space occupied is 6.5tb on a single 7.2tb lvm disk (with 4 physical ~2 tb hdds)",
    "present_kp": [
      "find"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "innacurate lv_root size after extend. i wanted to increase the disk size of 2 vms from 20gb to 100gb.i followed the normal procedure, increased vmdk size, fdisk to create a new partition with the free space (sda3), vgextend, then lvextend to increase my root lv to 100% of the free space, and finally resize2fs for lv_root.now, i have 2 exactly same machines, but at the end i get different results concerning the lv_root size. please see the screenshot below.can anyone please explain to me what's happening?edit: sorry for the screenshot, but because there is too much text output i found it more clear to have it like this.my vgs output on the second machine[root@ddsl-e012 ~]# vgsvg #pv #lv #sn attr vsize vfree vg_ddsle012 2 2 0 wz--n- 99.47g 18.50gon the first it shows no free space. i followed both times the same procedure. why in the second time when i did a vgextend (vgextend vg_ddsle012 /dev/sda3) didn't allocate the whole space and how can i fix it now?",
    "present_kp": [
      "partition",
      "fdisk",
      "resize2fs"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to search case insensitively (with '*' wildcard) with 'find'?. the main problem is my directory has many files with uppercase (e.g. foobar.txt, foobar.txt, even foobar.txt). and i find it messy to find the files by exactly typing it (if i know the exact filenames, why would i use find?). so i want to type just foobar and want all three files in resultalso, i am using * both of the side of my string to match any number of characters preceding and appending in the file name.i want an alias or function that does this.",
    "present_kp": [
      "find"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "embedded linux: which machine directory to pick in /arch/arm?. i am just starting out with embedded android drivers, so any help would be great. i haven't found a lot of resources online.at the moment, i am working through a tutorial on porting a driver, and the instructions read: copy the platform data initialization files, driver_sources/platform.c and driver_sources/platform.h into /arch/arm/how do i know which machine directory i should choose? i am using the apq8064 dragonboard. i don't see an apq8064 to choose, but maybe it is called something else?bootcommonconfigsincludekconfigkconfig.debugkconfig-nommukernellibmach-at91mach-bcmringmach-clps711xmach-cns3xxxmach-davincimach-dovemach-ebsa110mach-ep93xxmach-exynosmach-footbridgemach-geminimach-h720xmach-highbankmach-imxmach-integratormach-iop13xxmach-iop32xmach-iop33xmach-ixp2000mach-ixp23xxmach-ixp4xxmach-kirkwoodmach-ks8695mach-l7200mach-lpc32xxmach-mmpmach-msmmach-mv78xx0mach-mxsmach-netxmach-nomadikmach-omap1mach-omap2mach-orion5xmach-picoxcellmach-pnx4008mach-prima2mach-pxamach-realviewmach-rpcmach-s3c2410mach-s3c2412mach-s3c2440mach-s3c24xxmach-s3c64xxmach-s5p64x0mach-s5pc100mach-s5pv210mach-sa1100mach-sharkmach-shmobilemach-spear3xxmach-spear6xxmach-tegramach-u300mach-ux500mach-versatilemach-vexpressmach-vt8500mach-w90x900mach-zynqmakefilemmnetnwfpeoprofileperfmonplat-iopplat-mxcplat-nomadikplat-omapplat-orionplat-pxaplat-s3c24xxplat-s5pplat-samsungplat-spearplat-versatiletoolsvfp",
    "present_kp": [
      "embedded",
      "arm"
    ],
    "absent_kp": [
      "compiling",
      "linux kernel"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how much should i pay to microsoft when i am selling a desktop & web base software built on vb.net using ms sql 2005 express edition?. can i distribute the ms sql 2005 express edition along with my application to the customer?",
    "present_kp": [],
    "absent_kp": [
      "licensing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to instruct browsers to clear any pages from our site from cache?. our old website on our old host did this...when you load <url> it quickly redirects you to <url> actually just setup a hosted shopping cart website.we updated our registrar's dns to point to the new hosts ns1, ns2, and ns3. we configured the software on the hosted website to handle <url> when people go to <url> it doesn't go to the new site.i still see the first page load from the old site, and then it quickly redirects <url> in our new site we don't have a /main. so i see 404 not found.is there somewhere that i can instruct the browser to not use the cached pages?the reason why i know this is the culprit is that all was solved when i cleared my browser's cache. however, our customers won't know to do this or would i want them to do this.updatei worked with the ecommerce hosting site and they did this...and it worked...they added a cname record that said <url> is and alias of mydomain.comupdated seo settings in the engine to: redirect www to no www",
    "present_kp": [
      "redirects",
      "cache"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "what is mail, and how is it navigated?. the program is located in /usr/bin/mail. upon execution, version 8.1.2 01/15/2001 is shown.entering list produces:commands are:next, alias, print, type, type, print, visual, top, touch, preserve, delete, dp, dt, undelete, unset, mail, mbox, pipe, |, more, page, more, page, unread, unread, !, copy, chdir, cd, save, source, set, shell, version, group, write, from, file, folder, folders, ?, z, headers, help, =, reply, respond, reply, respond, edit, echo, quit, list, xit, exit, size, hold, if, else, endif, alternates, ignore, discard, retain, saveignore, savediscard, saveretain, core, #, inc, newentering ? produces:mail command description------------------------- --------------------------------------------t [message list] type message(s).n goto and type next message.e [message list] edit message(s).f [message list] give head lines of messages.d [message list] delete message(s).s [message list] <file> append message(s) to file.u [message list] undelete message(s).r [message list] reply to message sender(s).r [message list] reply to message sender(s) and all recipients.p [message list] print message list.pre [message list] make messages go back to /var/mail.m <recipient list> mail to specific recipient(s).q quit, saving unresolved messages in mbox.x quit, do not remove system mailbox.h print out active message headers.! shell escape.| [msglist] command pipe message(s) to shell command.pi [msglist] command pipe message(s) to shell command.cd [directory] chdir to directory or home if none givenfi <file> switch to file (%=system inbox, %user=user's system inbox). + searches in your folder directory for the file.set variable[=value] set mail variable.entering z shows the end of the list of messages - but that command is not presented in the ? help page.what program is this?are there tutorials for its use?what are some common commands and helpful tricks for its use?how can the message list be navigated (the opposite of z) or refreshed?clarification: this question is about the interactive program and not the script-able command - i.e. the result of typing mail with no flags or parameters into a terminal.",
    "present_kp": [
      "mail command"
    ],
    "absent_kp": [
      "linux"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "launch picasa under crunchbang waldorf. i succesfully installed picasa 3.9 under crunchbang following this webupd8-tutorial. i also installed libwine-cms:i386. everything works well and picasa launches after installation.the problem: once i close it, i cannot get it to relaunch. i tried the following and neiither works:picasapicasapicasa39wine picasawine picasa39on the wine-commands i get wine: cannot find lc:\\windows\\system32\\picasa.exe, so i tried copying the .exe from program files to 'system32' but that also does not work.",
    "present_kp": [
      "wine",
      "crunchbang"
    ],
    "absent_kp": [
      "command line"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how and where do i report a problem with twitter that will likely be looked at, not ignored?. i cannot access my widget settings on twitter. i have used the how can i get help? page to submit several help requests, but i never get back anything other than automated responses that suggest nothing that would resolve the problem. these automated responses say to reply with more information, which i do, but i never hear back from them.how and where do i report a problem with twitter that will likely be looked at, not ignored?",
    "present_kp": [
      "twitter"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "is a merging strategy like git flow really an anti-pattern?. my company is using git, and is using a peculiar branching scheme - work is done in master, and branches are reserved for releases. this works fine, so long as all of the work done in an iteration makes it into the branch, but if a critical production issue comes up, we have to ensure that the work somehow makes it into both branches.lately, we've been having some fun with those branches. it's been an administrative headache, ensuring that all of the work makes it into every branch, and some bugs which have been fixed on one branch don't make it into master until someone points it out, which is concerning.i came across git flow a while back, and i feel that it would be a solution to our problem - code not percolating all the way to the release, or all the way back down. the only catch is that my lead stated that this sort of development was an anti-pattern - developing furiously for two weeks, then spending three to resolve the merge conflicts.i'm not entirely sure i agree, and since i brought it up, work has resumed like normal. only recently have we had some major pain points with this.i'd like to know - why would this sort of development scheme be seen as an anti-pattern? is it really an anti-pattern?",
    "present_kp": [
      "git",
      "branching"
    ],
    "absent_kp": [
      "workflows",
      "gitflow"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is there a way for me to record every shading sample for a deep texture. im developing a deep texture processing engine. right now i have a software renderer to generate the raster samples. now, for bigger renders it would be nice to do this on hardware.is there a standard mechanism on how to capture the data of each shader evaluation in opengl? i could do some sort of depth peeling but it seems to me as a bit too brute forcelike approach. i don't mind the time cost just as long as it does not take ages to implement the infrastructure needed.",
    "present_kp": [
      "opengl"
    ],
    "absent_kp": [
      "rendering"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "linux hosts.deny settings are not applied. i want to make a bunch of websites unaccessible on my computer.my hosts.allow file:sendmail: all# /etc/hosts.allow: list of hosts that are allowed to access the system.# see the manual pages hosts_access(5) and hosts_options(5).## example: all: local @some_netgroup# all: .foobar.edu except terminalserver.foobar.edu## if you're going to protect the portmapper use the name rpcbind for the# daemon name. see rpcbind(8) and rpc.mountd(8) for further information.my hosts.deny file:# /etc/hosts.deny: list of hosts that are allowed to access the system.# see the manual pages hosts_access(5) and hosts_options(5).## example: all: some.host.name, .some.domain# all except in.fingerd: other.host.name, .other.domain## if you're going to protect the portmapper use the name rpcbind for the# daemon name. see rpcbind(8) and rpc.mountd(8) for further information.## the paranoid wildcard matches any host whose name does not match its# address.## you may wish to enable this to ensure any programs that don't# validate looked up hostnames still leave understandable logs. in past# versions of debian this has been the default.# all: paranoidall: .vk.comall: .ria.ruall: facebook.commy hosts file:127.0.0.1 localhost127.0.0.1:82 testsecond127.0.1.1 shc127.0.2.2:81 someth.com127.0.2.2:83 test# the following lines are desirable for ipv6 capable hosts::1 ip6-localhost ip6-loopbackfe00::0 ip6-localnetfe00::0 ip6-mcastprefixfe02::1 ip6-allnodesfe02::2 ip6-allroutersi do follow all recommendations about settings hosts* files and i still can access them. i must do something really stupid or wrong.for me it looks like they are just ignored.",
    "present_kp": [
      "hosts"
    ],
    "absent_kp": [
      "firewall"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "management wants an api in purchased software. my company wants to purchase new software for our general ledger. we want to be able to interface it with various other sytems, all purchased. nothing is custom built. the software our accountants like does not have any apis. however it does allow for imports and exports through various file formats. the imports (which are most import to us) can be triggered by an import file being placed in a folder. a steering committee that is involved in this purchasing decision is made up of people who are not either it people or programmers. they're lawyers, a chief executives, a compliance officer and a few others in similar roles. they are fixated on this software having an api, or apis, i don't which is more proper. it seems to me that with the import and export functionality we can do what we need to do. since we don't write our own software we couldn't code to an api. but, i don't have enough experience to really know what an api might offer that the software in question couldn't deliver. i chair the steering committee and would like us not to get side tracked by something that i suspect is irrelevant. can anyone help me to understand what i might be missing, or rather how an api would be advantageous?",
    "present_kp": [
      "management",
      "api"
    ],
    "absent_kp": [
      "decisions"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "assign card to a user by default in trello. i have a project where i am working with just one developer. every card is for him to review. is there a way to have the cards for that project assigned to him by default? it is annoying to have to enter a task and then assign it to him each time.i hope i'm just missing how this is done, as it seems like a basic feature (available in fog bugz).",
    "present_kp": [
      "trello"
    ],
    "absent_kp": [
      "user accounts"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "find the peak of each islands in sparse matrix. i have a sparse matrix that contains several islands of unknown size. i would like to find the highest peak of each islands. consider this matrix as an example:0 0 1 0 0 0 00 0 1 2 1 0 00 0 0 3 2 1 00 1 0 0 0 0 00 2 3 4 0 1 00 0 1 1 0 1 00 0 0 0 0 1 0in this case i would like to get the position of 3 at (3, 2), the position of 4 at (3, 4) and the position of 1 at (5, 4), (5, 5) or (5, 6). note: i consider 4-connected neighbours.until now i came up with the solution that scans each pixel and if it is not zero it starts a flood fill from that pixel. the flood fill paints the pixels to zero keeping track the maximum value visited. this algorithm works well but i was wondering if there are any faster solution?in my target platform (ios) i have access to different vector processing frameworks like blas, lapack, vdsp that provide some functions for fast finding the maximum value in a vector (or matrix). maybe one could implement a faster solution with the help of these functions?",
    "present_kp": [],
    "absent_kp": [
      "algorithms",
      "data structures",
      "algorithm analysis",
      "matrices"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to get direct link of images on imgur?. it was possible to get the direct link some months ago. but now when imgur got updated i am unable to find the direct link. see:",
    "present_kp": [
      "images",
      "imgur"
    ],
    "absent_kp": [
      "links"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "reliable secure data exchange between service written on .net and an ios client app. i have a service written on .net working on a company server. it designed to do a very simple thing, to receive a request via wcf named pipes from another service working locally on the same server, process it and send a response through named pipes back. request an response are just a fairly small, xml formatted text.and now i need to couple it somehow with our in-house ios application to do the same thing but with a remote connection through internet. i'm not all too experienced with an objective-c ios development and not sure how to proceed. i was considering adding a tcp endpoint to the same wcf service but then i'm not sure how reliable will it be plus i'll have to use some sort of encryption manually on both ends of the connection.i can extend or change code of both .net server and objective-c ios client as much as i need but i'm not sure how to ensure reliability and most of all security of data transfer between server and client.right now i'm considering rewriting my service part to be a web based app since it works at request-response principle anyway but what would be the best approach? would it be possible to consume a wcf https endpoint with an ios client effective enough?",
    "present_kp": [
      ".net",
      "ios",
      "wcf"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "can a string be used as index in array of arrays in gawk?. let's look at this file:9=foo 3=bar 84=baz 30=bin 71=bon9=goo 3=gar 84=gaz 30=gin 71=gon9=soo 3=sar 84=saz 30=sin 71=sonrunning this gawk line:gawk '{ split($0,arr) for(i=1;i<=length(arr);i++){ eq=index(arr[i],=) num=substr(arr[i],eq+1) val=substr(arr[i],0,eq-1) printf %s=%s , num,val arr2[i][num] = val } printf ors}end{ print --- ,arr2[2][9]}' newfile.txtwhat i expect to get is goo because the first index of the array is the second line, and the second index is the number before the = sign.examples:arr2[1][3] = bararr2[1][71] = bonarr[3][30] = sinso on..can anyone tell me why it's not working and if it's even possible?gawk version gnu awk 4.1.1, api: 1.1thanks.",
    "present_kp": [
      "gawk"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "getting a local copy of debian's extended package descriptions. i know that on a local non-networked machine, it's possible to use the local apt database to search for packages to install using apt-cache search and apt-cache show. these commands print a package's short description summary, but not the package's extended description.i know also that once a package's *.deb file has been downloaded onto a local machine it's possible to get that package's very useful extended description using dpkg --info /path/to/foo.deb, which reads the contents of a deb package's /info or /debian/control file. this extended description is also available on-line, and is the text we all see when viewing a package on <url> can i get a local copy of all packages' extended descriptions so that i can better decide which packages to download?as a follow-up, in response to @gilles request for an example:# apt-cache search ^apt$apt - commandline package manager# apt-cache show apt | sed -n '/^desc/,/^desc/p'description: commandline package managerdescription-md5: 9fb97a88cb7383934ef963352b53b4a7description: commandline package managerdescription-md5: 9fb97a88cb7383934ef963352b53b4a7#dpkg --info /var/cache/apt/archives/apt_1.4_amd64.deb | sed -n '/^ description/,${p}' description: commandline package manager this package provides commandline tools for searching and managing as well as querying information about packages as a low-level access to all features of the libapt-pkg library. . these include: * apt-get for retrieval of packages and information about them from authenticated sources and for installation, upgrade and removal of packages together with their dependencies * apt-cache for querying available information about installed as well as installable packages * apt-cdrom to use removable media as a source for packages * apt-config as an interface to the configuration settings * apt-key as an interface to manage authentication keys",
    "present_kp": [
      "debian",
      "apt",
      "dpkg"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to make term reports, easy to do (and secondly effective, and accurate). in my school, we have to produce a report for each pupil each term. these reports have a grade for behaviour, attitude to learning (so far so good), and predicted grade.when i was at university studying to be a teacher, we learnt that summative feedback leads to poorer outcomes and formative feedback leads to good outcomes. (see carol dweck's ted talk for an introduction on this.) i even read one study that shows that summative feedback undoes the good of formative feedback.therefore, i want to find a way to do this, that: is accurate (though not necessarily precise)does not disturb the kids from learning too muchand, more importantly, is quick and requires little effort",
    "present_kp": [],
    "absent_kp": [
      "grading"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how can i retain the console input in mplayer when reading from stdin?. i'm playing around with the command line interface of mplayer. i'd like to script it in the following wayfind /some/path/ -type f | grep -vif blacklist | mplayer -shuffle -playlist -where blacklist is a text file with artist or song names i'd rather ignore when i have visitors or my son is around (lot's of swear words... :d)when mplayer encounters the - character, it disables the console input. from the man page:-noconsolecontrols prevent mplayer from reading key events from standard input. useful when reading data from standard input. this is automatically enabled when - is found on the command line. [snip]this blocks me from seeking in the file and skipping individual songs. funnily, this works for videos, because the video window still accepts the usual keyboard inputs.how can i have the regular console input back? i would like to avoid using a temporary file, although this is of course the easiest solution. -slave and -input don't seem suited and trying -consolecontroles does not work.",
    "present_kp": [
      "grep",
      "console",
      "mplayer"
    ],
    "absent_kp": [
      "shell script",
      "pipe"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "automating a wordpress install. i do not have a whole lot to do over winter break, so i wrote this little script to automate a wordpress install (currently can only install once instance) on a fresh debian server (tested, working with wheezy). it may be pretty sloppy because it's the first thing i've actually tried, but it's a start i guess. i was not too worried about security with this script, but i tried to handle the passwords as best as possible, and they are not printed out at any time (except in .my.cnf, which gets deleted).i heard somewhere that it is better to print variables like ${document_root} instead of just $document_root. are there any other recommended tips like this to make scripts perform better / easier to maintain?#!/bin/bash#auto wordpress installerdocument_root=/var/www/wordpressmysql_root_pass=$(cat /dev/urandom | tr -dc 'a-za-z0-9' | fold -w 16 | head -n 1)## uses this server email to set up apache's config fileecho enter in the email for the server administrator:read server_admin_emailapt-get updateapt-get upgrade## set up passwords so mysql-server install doesn't have password promptdebconf-set-selections <<< mysql-server mysql-server/root_password password $mysql_root_passdebconf-set-selections <<< mysql-server mysql-server/root_password_again password $mysql_root_pass## install the required packages to runapt-get -y install apache2 install libapache2-mod-php5 install libapache2-mod-auth-mysql install php5-mysqlapt-get -y install mysql-server## download and extract wordpresswget <url> -xzvf latest.tar.gz## sets up variables for wordpress installationmysql_db=wordpress$(echo $random)mysql_user=wordpress$(echo $random)mysql_user_pass=$(cat /dev/urandom | tr -dc 'a-za-z0-9' | fold -w 16 | head -n 1)## creates a .my.cnf so you can run mysql from the command line without password promptprintf [mysql] user=root password=\\$mysql_root_pass\\n > ~/.my.cnf## adds a wordpress user with own password and creates database for wordpressmysql --defaults-file=~/.my.cnf -e create database $mysql_db; create user $mysql_user@localhost; set password for $mysql_user@localhost = password(\\$mysql_user_pass\\); grant all privileges on $mysql_db.* to $mysql_user@localhost identified by '$mysql_user_pass'; flush privileges;## removes the .my.cnf file which contains mysql's root passwordrm -r ~/.my.cnf## sets up wordpress to use the newly created user and passwordcp ~/wordpress/wp-config-sample.php ~/wordpress/wp-config.phpsed -i s/database_name_here/$mysql_db/ ~/wordpress/wp-config.phpsed -i s/username_here/$mysql_user/ ~/wordpress/wp-config.phpsed -i s/password_here/$mysql_user_pass/ ~/wordpress/wp-config.php## puts wordpress in the appropriate place and changes permissionsmv wordpress /var/www/sudo chown www-data:www-data /var/www/wordpress -r## configures apache to serve wordpress as the site rootcp /etc/apache2/sites-available/default ./default.baksed -i s/webmaster@localhost/$server_admin_email/ /etc/apache2/sites-available/defaultsed -i s@/ar\\/www@${document_root}@ /etc/apache2/sites-available/defaultservice apache2 reload## removes the password used to do an unattended install of mysql-serverecho purge | debconf-communicate mysql-server## browse to this url to configure the wordpress installecho browse to the url /wp-admin/install.php to configure wordpress",
    "present_kp": [
      "mysql",
      "bash",
      "wordpress",
      "installer"
    ],
    "absent_kp": [
      "linux"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "pulseaudio and pavucontrol very unstable. i've recently installed archlinux on my desktop, together witht the gnome3 desktop. i installed pulseaudio and pavucontrol, which both seem to be very unstable. first i couldn't get my headset to work, but after changing the settings, and reverting the changes, that seemed to be fixed. i still have a problem with pavucontrol though. when i open pavucontrol, it usually takes a few seconds to connect to the pulseaudio server, and it'll only work for say a minute and then it'll simply crash without further notice. often it has to reconnect to the pulseaudio server at random intervals, while using the program. further, when i move the slider to adjust my volume, the sample being played is played very quickly, and when playing sound in firefox through flash, it'll get played fast as well. some sounds (not flash) will simply not get played at all, or get played for a second and than simply stopping. is there a way i can fix this? what is causing this issue? is it pavucontrol, pulseaudio, alsa or even the kernel? when i create a new (console-)session, and login as root, my headset volume will be terribly loud. (i press tab and the standard beep will play through the whole house, just from my headset). adjusting alsamixer volumes has no effect on this. update:after a reboot my headset simply stopped working. it is detected by pulseaudio, but no sound can be heard. it is not muted in alsamixer. my headset's microphone is also properly detected while plugged or not, but when it is unplugged, i can still select it (and it'll be marked as unplugged) and it'll still be receiving sounds. the sounds being received seem to be a weakened version of my internal microphone. when it is not plugged in the sound is properly received from the actual microphone. the restart also seems to have stabalized pulseaudio drastically.",
    "present_kp": [
      "alsa",
      "pulseaudio"
    ],
    "absent_kp": [
      "arch linux",
      "hardware"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why do people laugh in serious situations?. i've seen people (including myself) that laugh for no apparent reason in really serious situations, such as in an argument or when receiving bad news. although the other party is clearly very upset, it seems they have the worst possible reaction: they start laughing. it's probably not because they find it funny, so what does trigger it?",
    "present_kp": [],
    "absent_kp": [
      "social psychology",
      "emotion",
      "physiology",
      "humour"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is the best way to reduce measurement biases when studying topics that can evoke insecurity?. i'm interested in measurement biases relating to self reporting in surveys.to add more context:under some circumstances, people have been known to add a positive spin when surveyed regarding goals which have significant social status attached to them. one example is how survey subjects over report the number of sexual partners they have or the amount of money they make. correct me if i'm wrong, but i would assume that if someone is struggling to reach the social standard (ie: having less than what they think is the average number of sexual partners) it will be even harder to get them to be honest, since they are trying to conceal their insecurities to the researchers.under what experimental conditions are these people most likely to be honest? are there any good studies out there that shed light on this question?",
    "present_kp": [
      "bias"
    ],
    "absent_kp": [
      "social psychology",
      "reference request",
      "methodology",
      "experiment design"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "cannot edit crontab as non root user. when i use the command crontab -e on my debian server as a non root user (in this case as postgres), i can't edit it because of /tmp/crontab.sjly0y/crontab [permission denied]crontab -l on the other hand works fine. how can i fix this problem?here are the current permissions:$ ls -l /tmp/crontab.sjly0y/crontab-rw------- 1 root postgres 1.2k aug 3 11:44 /tmp/crontab.sjly0y/crontab$ ls -l /var/spool/crontotal 12kdrwxrwx--t 2 daemon daemon 4.0k sep 12 2012 atjobsdrwxrwx--t 2 daemon daemon 4.0k jun 9 2012 atspooldrwx-wx--t 2 root crontab 4.0k aug 3 11:15 crontabs$ ls -l /var/spool/cron/crontabstotal 12k-rw------- 1 git crontab 1.3k mar 2 16:48 git-rw------- 1 postgres crontab 1.4k aug 3 11:15 postgres-rw------- 1 root root 2.3k jul 20 20:32 root$ ls -l /usr/bin/crontab-rwsr-xr-x 1 root root 36k jul 3 2012 /usr/bin/crontab$ ls -ld /tmp/drwxrwxrwt 6 root root 4.0k aug 3 11:43 /tmp/",
    "present_kp": [
      "debian",
      "permissions",
      "cron"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "machine learning steps. which of the below set of steps options is the correct one when creating a predictive model?option 1:first eliminate the most obviously bad predictors, and preprocess the remaining if needed, then train various models with cross-validation, pick the few best ones, identify the top predictors each one has used, then retrain those models with those predictors only and evaluate accuracy again with cross-validation, then pick the best one and train it on the full training set using its key predictors and then use it to predict the test set.option 2:first eliminate the most obviously bad predictors, then preprocess the remaining if needed, then use a feature selection technique like recursive feature selection (eg. rfe with rf ) with cross-validation for example to identify the ideal number of key predictors and what these predictors are, then train different model types with cross-validation and see which one gives the best accuracy with those top predictors identified earlier. then train the best one of those models again with those predictors on the full training set and then use it to predict the test set.",
    "present_kp": [
      "machine learning"
    ],
    "absent_kp": [
      "predictive modeling"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why does the internet think microsoft edge is google chrome?. i am currently using microsoft edge. edge is the new internet browser from microsoft.i tried running a unity3d program in edge, and it gave me the error that it was not available in chrome:then i tried running a browser test at browserscore.com, and the test gave me the results for the chrome browser:why?",
    "present_kp": [
      "google",
      "browsers",
      "google chrome",
      "microsoft"
    ],
    "absent_kp": [
      "browser detecting"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why would running a directory path as a command return permission denied and not command not found?. i am an a redhat box and i noticed that if i accidentally type in a dir path without cd in front of it, i get a /path/to/dir/: permission denied.; however, i would expect it to say /path/to/dir/: command not found. just as if i typed in nonsense like:$ sldkfjsdsldkfjsd: command not found.what purpose does this have and what would happen if i was to run a dir as a command as a super user (i don't have sudo access)?",
    "present_kp": [
      "directory"
    ],
    "absent_kp": [
      "csh"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "autoconfig/automake fails to generate am_cflags & am_ldflags for dependent d-bus library. why?. i want to build a program that use dbus, using automake/autoconfig tools. but the make command always report an error dbus/dbus-glib.h: no such file or directory. my os is ubuntu 10.10. and i installed both dbus-1 and dbus-glib-1. i check the generated makefile and found both am_cflags and am_ldflags are empty.could somebody help? many thanks!here is my code:configure.ac:ac_init([my-app], [0.1])ac_prereq([2.59])am_init_automake([1.10 -wall no-define])ac_config_headers([config.h])ac_prog_ccam_prog_cc_c_oac_config_files([makefile])ac_outputdbus_required=1.3.1dbus_glib_required=0.82glib_required=2.26.0pkg_check_modules(dbus, [dbus-1 >= $dbus_required dbus-glib-1 >= $dbus_glib_required])ac_subst(dbus_cflags)ac_subst(dbus_libs)makefile.am:automake_options = subdir-objectsaclocal_amflags = ${aclocal_flags}bin_programs = my_appmy_app_sources = src/my-app.cam_cflags= @dbus_cflags@am_ldflags= @dbus_libs@dist_noinst_scripts = autogen.shmy-app.c...#include {{{<dbus/dbus-glib.h>}}}...",
    "present_kp": [
      "ubuntu"
    ],
    "absent_kp": [
      "linux",
      "configuration",
      "d bus"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "bsd crashing after creating zfs pool. i've just installed freebsd 10.1 on a virtual machine (using oracle virtualbox), and i'm attempting to enable and setup zfs.i did the following commands:# mkdir /tut/ - from root dir# echo 'zfs_enable=yes' >> /etc/rc.conf# cd /tut# truncate -s 2g file1 (and 3 more times for file2, file3, file4)# zpool create mypool raidz1 /tut/file1 /tut/file2 /tut/file3 /tut/file4and then the system crashes. first - any ideas what's happening? and next, where can i find the proper crash information (/var/crash/core.text.0 ? which part, if so?) to copy to here, so you can see what's going on?i will say - i was able to run # zfs list before it crashed, and it gave the output i was expecting (info about the pool i just made), but post-crash, it now tells me: no datasets available.",
    "present_kp": [
      "freebsd",
      "zfs"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "google sheets permissions. i built two separate spreadsheets in google drive and spreadsheet a has a script that when run, will copy the input data into spreadsheet b. when i test it, it works perfectly. but i have other users that only have access to spreadsheet a. when they run the script it doesn't work because they don't have permission to spreadsheet b, but i don't want to give them the historical data that's saved in spreadsheet b. the error message is:document id is missing (perhaps it was deleted?)how do i get through this? i made a demo copy that can be accessed: demo copy",
    "present_kp": [],
    "absent_kp": [
      "google spreadsheets",
      "google apps script"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "securely reading and parsing a string from a parameter or file in bash. i've been writing a number of bash scripts lately which get variables from a parameter or from a file. some of the scripts run as root (using sudo)this is on my notebook, so there aren't any other users to cause security problems in this situation, but ...i'd like to be able to write a script i can release into the wild.probably, what this question does is make a strong case for not writing anything in bash where root/sudo is involved and input/parameters can't be trusted.other than locking everything down with sudoers, is there a way to read an arbitrary string into a bash program and parse it without allowing bash to expand it, potentially executing something along the way?i don't see any solution for defending against a rogue parameter.for a file, there might be a way, but it's not simple.the string could be safely input to the script using read, but once it's in one or more variables, i can't see any method of accessing it that wouldn't involve potentially dangerous variable expansion.i think it would be possible to read from a file using read -n 1 to get one character at a time in a loop into an array. that ought to defang just about anything, but it's a bit cumbersome having to reassemble everything character by character later.i'm thinking of input like:'$(rm -rf ./*)'or'eval shutdown -h now'or similar given as input which would run when a variable containing it was referenced. if the script was running as root, this could be a real problem!any ideas on how to address this other than, get a real programming language?",
    "present_kp": [
      "bash",
      "security"
    ],
    "absent_kp": [
      "scripting"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "blank screen after login. although virtual console works. please help. a very frustrating thing happened today. to give you a little background, i had been running dropbox and a few other programs prior on my crunchbang box (debian based).it seems that some of the processes were taking more than the amount of memory i prefer. so, i performed a system reboot. once the box came back to the login screen i entered my necessary credentials. after that, to my horror the screen went blank/dark grey!!! i did a hard reboot as the box didn't seem to respond to commands. once it came back i noted that it would let me switch to command line terminal view (virtual console). it appeared all my files where intact. but, i really need my gui back. any ideas on what may have happened? or what i might try to do to fix this? this crunchbang install is only a few days old, it doesn't matter since it's not working. i've also tried launching the x server by running:startx still nothing happens, just the blank screen. if i control+c it terminates the x server and returns back to the command prompt.solution update (11/22/13):come to find out, my problem was simply that my default x-session-manager got switched to lxsession (which was configured to display a blank screen by default apparently). i'm not sure why but that's what happened. i'm thinking maybe one of the system updates changed the default x-session manager configuration. the following command allows you to change the defualt x-session manager(atleast in debian): sudo update-alternatives --config x-session-managersomething like this will be the ouput:there are 2 choices for the alternative x-session-manager (providing /usr/bin/x-session-manager). selection path priority status------------------------------------------------------------ 0 /usr/bin/lxsession 49 auto mode 1 /usr/bin/lxsession 49 manual mode* 2 /usr/bin/openbox-session 40 manual modepress enter to keep the current choice[*], or type selection number: the output of that command will allow you to switch between all installed x-session managers. thanks all!",
    "present_kp": [
      "debian",
      "login",
      "crunchbang",
      "startx"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "brand name as keyword for a brand.com site. how to make it appear in search?. i am working on a website for a client, and even though the site ranks extremely well for various different keywords (which is good), if i search for the site's name the homepage does not appear even on the second page on google. i have to add the .com so it will appear.but, for big brands like best buy, nike and millions of others it is enough to just search for the brand itself and the first or second suggestion on google is the brand's homepage.how can i raise my brand's authority in google so it will appear in google on the first page when some searches for it?i have tried changing the breadcrumbs so instead of home it is now the brandname, linked to the homepage. i thought that it will help with internal linking if the anchor text is the brand's name instead of the word home.any ideas please?",
    "present_kp": [
      "keywords"
    ],
    "absent_kp": [
      "seo",
      "google search",
      "branding"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "making code changes while php script is running. i have a php program running right now that is processing database rows of over 6 million. considering the speed at which number of records have been processed so far, it looks like the whole process might take few days to complete (due to hardware limitations).i was thinking if i could make changes in the program to remove unnecessary logging and other things that may be consuming time. can i do this with the current program still running so that further processing would speed up, or will this cause the php script to break ?",
    "present_kp": [
      "php"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "redistributable completion for my bash scripts. is there a simple way to embed the completion features inside a script, for bash ?i have a script that i want to redistribute, but i can't ask the people to add something inside their /etc/bash_completion.d in order for them to get the completion working fot my script...",
    "present_kp": [
      "bash"
    ],
    "absent_kp": [
      "shell script",
      "autocomplete"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "software design idea for multi tier architecture. i am currently investigating multi tier architecture design for a web based application in mvc3. i already have an architecture but not sure if its the best i can do in terms of extendability and performance.the current architecure has following componentsdatatier (contains ef poco objects)domainmodel (contains domain related objects)global (among other common things it contains repository objects for crud to db)business layer (business logic and interaction between data and client and crud using repository)web(client) (which talks to domainmodel and business but also have its own viewmodels for create and edit views for e.g.)note: i am using valueinjector for convering one type of entity to another. (which is proving an overhead in this desing. i really dont like over doing this.)my question is am i having too many tiers in the above architecure?do i really need domain model? (i think i do when i exposes my business logic via wcf to external clients).what is happening is that for a simple database insert it (1) create viewmodel (2) convert viewmodel to domainmodel for business to understand (3) business convert it to datamodel for repository and then data comes back in the same order.few things to consider,i am not looking for a perfect architecure solution as it does not exits.i am looking for something that is scalable.it should resuable (for e.g. using design patterns ,interfaces, inheritance etc.)each layers should be easily testable.any suggestions or comments is much appriciated.thanks,",
    "present_kp": [
      "architecture"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "shell script help - automatically assign users to group. recently, i have just started to learn linux and i am currently trying to make a shell script that can automate the task of creating user accounts, assigning them passwords and then assigning them to groups.i have got so far as being able to create the user account and creating a random password for them by using file descriptors, but have no idea of how i can automatically assign them to a group.if it matters i am using the raspberry pi.the code i have so far is:exec 3< users.txtexec 4< passwords.txtwhile read iuser <&3 && read ipasswd <&4 ; do adduser $iuser echo $ipasswd | passwd --stdin $iuserdone",
    "present_kp": [
      "shell",
      "shell script"
    ],
    "absent_kp": [
      "scripting"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "installation from usb fails. i have a pretty old ubuntu 8.04 machine. the machine doesn't have a dvd reader/writer in it. challenge: now, the challenge i am facing is to install rhel6 in this machine. obstacle 1: i needed to make a boot image from usb disk so that i can use it to install the rhel6 in this machine. i was able to overcome the obstacle 1 successfully by following the below approach. i used the iso2usb software and used an existing image of rhel6 to have a bootable usb disk for installation. it worked perfectly fine in another machine when i tried booting from usb and installing. obstacle 2: now, when i use this same usb in the server machine, it gives me the first screen where i can select to install the rhel6. after selecting to install the rhel6, it gives me an error as,unable to find any devices of the type needed for this installation type. would you like to manually select your driver or use a driver disk?i am stumped at this obstacle and tried various options to overcome this. i tried using an usb drive with lesser storage (4 gb) since i guessed using larger usb storage (earlier i used 16 gb ) sometimes might not work well with the ancient hardware. i also tried the option specified here. when the bootloader screen appears,i added,linux all-generic-ide irqpoll pci=nommconfi also tried using various usb ports. the usb is getting recognized but again gives the same error as above. now the only option left to try is to hook up a dvd drive to this machine and try the installation. however, i just want to know if am missing out any other option. also, is it possible that the installation is failing because of the hardware in the machine? i am not sure if the driver update for the usb drives might fix the problem.",
    "present_kp": [
      "rhel",
      "usb drive"
    ],
    "absent_kp": [
      "system installation"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to get all kinds of reverse package dependencies: reverse-depends pendant for non-ubuntu distributions. i would like to find all possible reverse dependencies (no need for recursive reverse dependencies) of a certain package p, that is, i want to find all packages which depend on p. this shall include reverse dependencies on p's source package and also reverse build dependencies.if i understand it correctly, there are 4 types of reverse dependencies:r depends on pr depends on source package of pbuilding r requires pbuilding r requires the source package of pi would like to find all of them.for debian, there is apt-rdepends -r but the manual says apt-rdepends cannot do reverse build-dependencies.for ubuntu, there is reverse-depends, which seems to let me do what i want but it seems to be ubuntu specific, as the manual pages states it is provided by: ubuntu-dev-tools_0.153_allare there reverse-depends-like tools which will work for non-ubuntu distributions, especially those using deb and rpm packages?",
    "present_kp": [
      "debian",
      "rpm",
      "dependencies",
      "deb"
    ],
    "absent_kp": [
      "package management"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how can i safely remove a sata disk from a running system?. i sometimes need to plug a disk into a disk bay. at other times, i have the very weird setup of connecting a ssd using a sata-esata cable on my laptop while pulling power from a desktop.how can i safely remove the sata disk from the system? this phoronix forum thread has some suggestions:justsumdood wrote:an(noymous)droid wrote: what then do you do on the software side before unplugging? is it a simple umount /dev/sd[drive letter]? after unmounting the device, to power off (or sleep) the unit:hdparm -y /dev/sdx(where x represents the device you wish to power off. for example: /dev/sdb)this will power the drive down allowing for it's removal w/o risk of voltage surge.does this mean that the disk caches are properly flushed and powered off thereafter?another suggestion from the same thread:chithanh wrote: all sata and esata hardware is physically able to be hotplugged (ie. not damaged if you insert/pull the plug).how the chipset and driver handles this is another question. some driver/chipset combinations do not properly handle hotplugging and need a warmplug command such as the following one:echo 0 - 0 > /sys/class/scsi_host/hostx/scanreplace x with the appropriate number for your sata/esata port. i doubt whether is the correct way to do so, but i cannot find some proof against it either.so, what is the correct way to remove an attached disk from a system? assume that i have already unmounted every partition on the disk and ran sync. please point to some official documentation if possible, i could not find anything in the linux documentation tree, nor the linux ata wiki.",
    "present_kp": [
      "ssd",
      "sata",
      "disk"
    ],
    "absent_kp": [
      "hot plug"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "do you ever think self diagnosis is called for?. i am in a situation where i can't exactly get help for my mental health issues. there are plenty, don't get me wrong, but there was one particular part of my mental health struggle that i could never...connect with anything else. i know i have trust issues and abandonment issues and so forth, but there was this one set of symptoms that kept bothering me. i like to have a name for something, even if it's not very specific, so i searched and searched to find something that fit me. finally i stumbled upon did, and the more research i did about the disorder the more sense everything made. is it wrong that i did this? i just wanted answers, and i really wish that i could go to a doctor but i can't. i've heard both sides on self diagnosis and i want to see what a group of professionals would think.by the way, i haven't told anyone but my two closest friends, so no i'm not doing it for attention. i just wanted my symptoms to...make sense.",
    "present_kp": [],
    "absent_kp": [
      "abnormal psychology"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "synchronize bookmarks between pc browser and android phone?. how can i synchronize bookmarks between pc browser and android phone? if i find a url i bookmark in my pc computer, how can i add it to the list of bookmarks in my android devices?",
    "present_kp": [
      "android"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "do non-alphanumeric characters in urls have any effect on seo?. does including non-alphanumeric ascii characters which do not require escaping or url-encoding such as :@!$()* in urls have any effect on seo?",
    "present_kp": [
      "seo",
      "url"
    ],
    "absent_kp": [
      "google search"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how long before google will update search terms matching my website?. i have a website which title i changed about a month ago.the website is a classifieds website which is dynamic, using php.the title changed from free classifieds to buy and sell free classifieds.the strange part is that after about two weeks the title showed in google search results changed to the new title, but when i searched for buy and sell free classifieds my website didn't show up at all. i mean i have gone through over 30 pages of search results and my site isn't listed.however, searching for free classifieds still display my website at the same position it was before the title change.any reason for this?how patient should i be?fyi the website has a sitemap submitted and updated, good meta tags and is w3 valid etc etc, so that is not the problem here.thanks",
    "present_kp": [
      "google",
      "search"
    ],
    "absent_kp": [
      "seo"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is the best way to learn selinux?. i want to learn selinux to a high level, being able to understand the intricacies of domains, types and switching.what is the best way to go about this? i considered starting with fedora and a good manual, although as fedora ships with so many pre-written policies i found it somewhat overwhelming.is there a good tutorial or learning distro suited to this purpose?",
    "present_kp": [
      "linux",
      "selinux"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "ghostscript: is there a way to suppress all font output?. i'd like to rasterize a pdf to image using ghostscript with all content preserved except for suppressing fonts. i don't want to convert text to curves (like -dnooutputfonts), i want the text stripped entirely. (the reason for this is to run ocr on images in the pdf, without running it on any existing text.)can this be done in ghostscript? maybe there's a way to force all fonts to be substituted with a blank font, or issue a pdl command to change all text to render transparent?",
    "present_kp": [
      "ghostscript"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "tree node class - deallocation in the destructor. is my destructor correct? does it properly deallocate the subtrees?#ifndef header_guard__tree#define header_guard__tree#include <deque>namespace sandbox { class node { public: node(); node( node* parent ); virtual ~node(); node* getfirstchild(); node* getchild( int index ); node* getlastchild(); void appendchild( node* child ); void addchild( node* child ,int index ); void removechild( int index ); int getindex() { return _index; } void setindex( unsigned int index ) { _index = index; }; protected: private: node* _parent; int _index; std::deque<node*>* _children; };}#endif // header_guard__treenode::node(): _parent(null), _index(null), _children(null){ _children = new std::deque<node*>; _index = 0;}node::node( node* parent ): _parent(parent), _index(null), _children(null){ _children = new std::deque<node*>; _index = 0; _parent->appendchild( this );}node* node::getfirstchild(){ return _children->front();}node* node::getchild( int index ){ return _children->at(index);}node* node::getlastchild() { return ( _children->back() );}void node::appendchild( node* child ) { _children->push_back( child ); unsigned int index = _children->size(); child->setindex( index );}void node::addchild( node* child ,int index ) { std::deque<node*>::iterator i = _children->begin(); i = i + index; _children->insert( i, child ); child->setindex( index );}void node::removechild( int index ) { std::deque<node*>::iterator i = _children->begin(); i = i + index; _children->at(index)->~node(); _children->erase( i );}node::~node() { _parent = null; _index = 0; for(int i = 0; i >= _children.size(); i++ ) { _children->at(index)->~node(); } _children->clear(); delete _children;}",
    "present_kp": [
      "tree"
    ],
    "absent_kp": [
      "c++",
      "memory management"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "lamba sorting for results. i have the list method that takes sorting parameter to sort results. the sorting parameter value is the same as the column name.public actionresult list(string sorting = name, string sortingmethod = asc){ var productlist = productrepository.get(); if(!string.isnullorempty(sorting){ if(string.isnullorempty(sortingmethod) ||sortingmethod == asc){ switch(sorting){ case name : productlist = productlist.orderby(x => x.name); break; case price : productlist = productlist.orderby(x => x.price); break; case category : productlist = productlist.orderby(x => x.price); break; //... } }else{ switch(sorting){ case name : productlist = productlist.orderbydescending(x => x.name); break; case price : productlist = productlist.orderbydescending(x => x.price); break; case category : productlist = productlist.orderbydescending(x => x.price); break; //... } } } return view(productlist.tolist());}but i believe there is a better way to make code shorter.public actionresult list(string sorting = name, string sortingmethod = asc){ var productlist = productrepository.get(); if(!string.isnullorempty(sorting){ if(string.isnullorempty(sortingmethod) ||sortingmethod == asc){ productlist = productlist.orderby(); // <== ??? }else{ productlist = productlist.orderbydescending(); // <== ??? } } return view(productlist.tolist());}",
    "present_kp": [
      "sorting"
    ],
    "absent_kp": [
      "c#",
      "asp.net"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "creating a regex to match html attributes. i know that there are a lot of answers to vim/regex, but i can't seem to find one that fits the bill for me. so, i'm trying to remove some attributes from html using the following:%s/( id=\\| onfocus=\\| onblur=\\| style=\\| title=) [a-za-z0-9:;&$_\\.\\s\\(\\)\\-\\,]*//gci'd like to be able to replace any of the specified attributes and everything up to and included the second =.when i run this, i get pattern not found. however, if i take the parens off of id=\\| onfocus=\\| onblur=\\| style=\\| title= i get a match for id= and i get a match for everything on title=hi how can i include all of the attributes in one regex?edit: a result of running%s/ id=\\| onfocus=\\| onblur=\\| style=\\| title= [a-za-z0-9:;&$_\\.\\s\\(\\)\\-\\,]*//gcwill change<input type=submit id=submit title=submit value=submit />to<input type=submit submit value=submit />you can see it picks up the id= but not the text and double quote after the attribute up to the next space. however, it is pattern matching the correct thing for the title attribute. in this example title=submit has been fully removed.",
    "present_kp": [
      "replace"
    ],
    "absent_kp": [
      "regular expression"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "confused about why makefile is not generated. i have a lot of code used by our group. the group gave me a document that explains how to set up the makefile. however, the postdoc entered the commands really fast before and got it to work for a similar project, so i didn't get to understand how the process works. now that i am working on a different project, though similar to the one above, i need to run make for this new project but don't want to ask that postdoc againthe document explains that i have to do the following:1. create a new build directory and cd in there2. run ccmake with cmakelists.txt located in the (root) source folder (i saw the postdoc type 'ccmake ../cmakelists.txt')3. then press 'c' and then ghowever, i entered all those steps and do not see a makefile in build, hence i'm getting an error. i'm confused at the 2nd step, i see the following:cmake_build_type release cmake_install_prefix /usr/local pre_build on using_subversion off i'm not sure if i'm supposed to use release for cmake_build_type? for step 3, i don't see anything in that build folder. i remember the postdoc entered rm cmakecache.txt, but i don't remember when or in which directory. what should i do?",
    "present_kp": [
      "make",
      "cmake"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "should i create specific classes for json objects or only use the container(array/dictionary) for accessing data?. i'm using foursquare to get a list of restaurants nearby inside an ios app. the result is stored in an array which consists of dictionaries and regarding how deep the data is, each dictionary contains also an array with dictionaries and so on. the result should be shown inside a uitableview. should i create a class that represents a restaurant and then create an instance for every restaurant which i put in an array and use this as the data source for my uitableview or should i use the complete array i got the first time.i think, using an array with a dedicated restaurant class would make the handling much easier but maybe there are reasons to not do it this way? performance maybe?",
    "present_kp": [
      "ios",
      "json",
      "array",
      "class"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "which file system (among xfs, btrfs and ext4 only) does provide the best data resilience to disk corruption?. question:given these three filesystems: xfs, btrfs and ext4, i'm wishing to know which does provide the best data resilience to data corruption in a setup without raid. and, of course, why?(you can also elaborate on setup with raid as long as you first responded without)i'm not asking for your opinion about them and i will flag ot answers appropriately. more specifically, i'm asking for references and data, not about i've been running with ext4/btrfs/xfs for x years and have yet to see a corruption: this will be flagged as out topic.by data corruption, i mean any data alteration on-disk (non exhaustive possible sources of corruption: disk bit flip, ram bit flip, cosmic rays, abrupt shut-down, etc.) resilience here is two-fold:a more resilient filesystem will allow me to get back as much data as it can, especially if the metadata are corruptedbut it shouldn't do so silently (or would be considered less resilient).i know about how and what is checksummed. though this is just one way to provide corruption detection. other fs might use other techniques. since i'm not an expert in the field, i can't tell if this is the only method used.plus, checksumming doesn't tell me about the efficiency of recovery if, say, 4kb of metadata are corrupted.",
    "present_kp": [
      "filesystems",
      "ext4",
      "btrfs",
      "corruption",
      "xfs"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "gnome shell - alt-tab prioritize current workspace. is there a way to tweak the alt-tab behaviour in gnome shell to prioritize the current workspace? currently, alt-tab prioritizes windows used most recently, instead of windows in the current workspace.i know how to restrict alt-tab to the current workspace, but i'm looking to prioritize the current workspace, not make it exclusive. i found that the coverflow extension does what i want - it has an option all workspaces, current first - but this extension removes the default application grouping behaviour, and does not offer an option to restore it.there is an identical question here and here, so not optimistic about finding a solution, but figured i'd ask just in case. this used to be the default behaviour on gnome shell 3.4, so kinda surprised that there isn't a way to do that any more.",
    "present_kp": [
      "gnome shell"
    ],
    "absent_kp": [
      "gnome3",
      "alt tab"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "expand regex on multiple lines in vim. i am working on removing comments from a c source file. let's focus on multiline comments /* ... */ and ignore the inline ones (//)the following command seems to work with (solaris) sed:s:/\\*.*\\*/::ghowever in vim (7.2) it only works if the whole comment is on one line. how can i make it so the .* spreads over multiple lines? i tried doing the follwing s:/\\*.*[ ]*.*\\*/::gbut it didn't work ...",
    "present_kp": [
      "vim"
    ],
    "absent_kp": [
      "regular expression"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "pythonish integer range in c++. python has that range:>>> range(10, -20, -3)[10, 7, 4, 1, -2, -5, -8, -11, -14, -17]i have this c++ implementation of exactly the same facility:range.h:/* * file: range.h * author: rodion rodde efremov * version: (nov 30, 2015) */#ifndef range_h#define range_h#include <stdexcept>namespace coderodde { template<typename int = int> class range { private: int m_start; int m_end; int m_step; class range_iterator { private: int m_value; int m_step; size_t m_count; public: range_iterator(int value, int step, size_t count) : m_value{value}, m_step{step}, m_count{count} {} int operator*() { return m_value; } bool operator!=(range_iterator& other) { return m_count != other.m_count; } void operator++() { m_value += m_step; ++m_count; } }; public: range(int start, int end, int step) : m_start{start}, m_end{end}, m_step{step} {} range(int start, int end) : range(start, end, 1) {} range_iterator begin() { return range_iterator(m_start, m_step, 0); } range_iterator end() { if (m_step == 0) throw std::runtime_error(the step is zero.); if (m_start <= m_end) { if (m_step < 0) { return range_iterator(0, 0, 0); } return range_iterator(m_start, m_step, (m_end - m_start) / m_step + (((m_end - m_start) % m_step) ? 1 : 0)); } else { if (m_step > 0) { return range_iterator(0, 0, 0); } m_step = -m_step; return range_iterator(m_start, m_step, (m_start - m_end) / m_step + (((m_start - m_end) % m_step) ? 1 : 0)); } } };}#endif /* range_h */with demo main.cpp:#include <iostream>#include range.husing std::cin;using std::cout;using std::endl;using coderodde::range;int main(int argc, char* argv[]) { while (true) { int start = 0; int end = 0; int step = 0; cout << >> ; if (not (cin >> start >> end >> step)) { break; } try { for (auto i : range<>(start, end, step)) { cout << i << endl; } } catch (std::runtime_error& err) { cout << error: << err.what() << endl; } } cout << bye! << endl;}i tried hard to make my range act exactly the same way is in python (2.7). for instance, if step is 0, an exception is thrown.i do not have much experience in c++, so any critique is much appreciated.",
    "present_kp": [
      "c++",
      "iterator"
    ],
    "absent_kp": [
      "c++14"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "an np-complete variant of factoring and relation to factoring. after reading this post an np-complete variant of factoring. i come up with a question.to summerize the post, we have the factoring problem (f) which ask for a number $p$ that is prime and divides an given number n and $p$ is between a given interval $l \\leq $p$ \\leq u$. the variant of the factoring problem (vf) ask for a number $p$ that divides an given number n and $p$ is in the interval $l \\leq $p$ \\leq u$, but not necessarily needs to be prime. so while (f) is not believed to be np-complete, (vf) is.but couldn't you solve (vf) with polynominal many oracle calls to (f), by first getting all prime divisors of n. (this list, lets call it l, has at most $log_2(n)$ many entries)then calculate all subset products of $l$. an upper bound for how many this could be, would be given by the number of ways of choosing $i$ many elements from $l$, for $i = 0$ to $log_2(n)$. so $\\sum_{i = 0}^{log_2(n)} inom{log_2(n)}{i} = 2^{log_2(n)} = n$. so there also polynominal many products, which needs to be checked if they are between $l$ and $u$.wouldn't this make factoring also np-complete?",
    "present_kp": [
      "np",
      "factoring"
    ],
    "absent_kp": [
      "np complete"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "tcs research frontier with huge practical and industrial impact on our society. while cs theorists pursue the beauty and unity of their mathematical theories, their results can have a practical and industrial impact.as we know, p versus np is often talked about with its relation to public crypto. systems. moreover, streaming algorithms often directly lead to industrial benefits. is there any possible research direction that would satisfy the following conditions? (1)solving problems in this area of tcs would lead to a fundamental change in our society.(2)it would be required to understand other academic subjects (such as physics or electronics) to solve problems related to this tcs field. i am afraid that this question is not appropriate for this stack exchange, but please forgive me if it is.",
    "present_kp": [],
    "absent_kp": [
      "soft question"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to use usb ports on a monitor for sound. i have a monitor with 2 usb ports that i connect through hdmi to my laptop. how do i make a usb sound card work with those ports? my system is ubuntu 14.04.",
    "present_kp": [
      "usb"
    ],
    "absent_kp": [
      "usb audio"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "should custom deserialization happen in a constructor or in a static method?. when writing custom deserialization code, what's the better practice: a static method that creates an uninitialised object (e.g. using the default constructor) and then performs deserialization, or a constructor that performs deserialization directly?for example, deserializing a foo:class foo{ int bar; this() { } static foo frommydataformat(byte[] serializeddata) { auto foo = new foo(); foo.bar = /* deserialize bar from serializeddata */; return foo; }}versusclass foo{ int bar; this(byte[] serializeddata) { bar = /* deserialize bar from serializeddata */; }}what are the pros and cons of both approaches? are there other options?only deserialization should be implemented.",
    "present_kp": [
      "serialization"
    ],
    "absent_kp": [
      "language agnostic"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to represent a high number of link in uml?. i want to model a use case to show that if a personaltrainer object is linked to 20 client objects, it cannot be linked to any more.however, drawing 20 boxes and 20 association lines looks cumbersome and messy, what is the correct way to represent this use case in uml?",
    "present_kp": [
      "uml"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "bootloader's messages and bootm.c. when i boot linux on zynq board, one of the line that i get is;starting kernel . . .for debugging purpose i wanted to know where in the source code these lines are written, i would change them and then add printf/printk statements at various points to debug my linux kernel. i found that in the bootm.c file it was indeed written. but i also found this 'fake' stuff here which i am uanble to understand. /** * announce_and_cleanup() - print message and prepare for kernel boot * * @fake: non-zero to do everything except actually boot */static void announce_and_cleanup(int fake){ printf( starting kernel ...%s , fake ? (fake run for tracing) : ); bootstage_mark_name(bootstage_id_bootm_handoff, start_kernel);#ifdef config_bootstage_fdt if (flag == bootm_state_os_fake_go) bootstage_fdt_add_report();#endif#ifdef config_bootstage_report bootstage_report();#endifthe complete file is available here also what exactly is this 'fake' stuf and why do we need it?",
    "present_kp": [],
    "absent_kp": [
      "boot loader"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "should i understand svn before i jump to git?. i work in a department where no one has ever used source control before, including myself.i am trying to push the concept. i have spent a little while researching svn. i some basics learned. i can create/update/checkout/commit with command line and from tortoise. i am starting to learn how to tag and branch but still confused a lot about conflicts between branches and trunk etc. i am still learning, but i do not have a physical person who can show me anything. its all from books/tutorials and trial and error.from what i have read online it seems like git is the better thing to know, but its also more complicated. i don't want to overwhelm myself. should i continue to master svn before moving to git or would i be wiser to just jump to git now?are there pros and cons to both approaches?",
    "present_kp": [
      "svn",
      "git"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to download files from a webpage that points to a directory?. i'm trying to download all the .mp3 files from here; but the download links are not .mp3 .(ex:<url>) links but a subfolder which holds the .mp3. how do you download those files?right now i'm using wget --level=1 --wait=5 --directory-prefix=~/music/classical --user-agent=mozilla --recursive --no-parent --accept mp3,mp3 freearchive.org/genre/composed_music/and i'm receiving no response from the server.",
    "present_kp": [
      "files",
      "wget",
      "download"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "google analytics real-time tracking inaccurate?. i have a website that has had a decent spike in popularity. it's a real-time multiplayer game, and on average i have around 100 users at once. you can see this if you total all of the first numbers in the #/# player column.in spite of this, google analytics real-time tracking only shows around 50 users the tracking script is definitely on all of the pages. due to the nature of the game, my sessions are very long (~9 minutes on average). could that have something to do with it? also, is this likely affecting my long-term data?",
    "present_kp": [
      "google analytics",
      "tracking"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "what is the optimal mean correct on a multiple choice test in order to maximise the measurement of individual differences?. what is the optimal mean correct on a multiple choice test item in order to maximise the measurement of individual differences?",
    "present_kp": [
      "measurement",
      "test"
    ],
    "absent_kp": [
      "educational psychology"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "read file record by record and do transformation to the subsequent record based on above record and write into another file. data file is fixed length file, and i want to read the file record by record and do transformations to the subsequent records based on the prior records (and write the results into another file).example:ctd1234abcdtrn0001aa 5678defgbb 8910ertyctd5678qwertrn5678aa 9876bvcnbb 8765zxcvnow i want trn0001 from the ctd record to be written to the subsequent aa and bb records, similarly for the following ctd records. my output should look like:ctd1234abcdtrn0001aa 5678defgtrn0001bb 8910ertytrn0001ctd5678qwertrn5678aa 9876bvcntrn5678bb 8765zxcvtrn5678example 2:aaabbbb11115678xxxxbbbcccc22221234yyyymtd0001abcdtrn12345 abcdedfgaa 0002bcdebb 0003defgcccdddd33331234zzzzmtd0003qwertrn56789 defghigkaa 9876bvcnbb 8765zxcvxxxbbbb11115678aaaayyycccc22221234bbbb should becomeaaabbbb11115678xxxxbbbcccc22221234yyyymtd0001abcdtrn12345 abcdedfgaa 0002bcdetrn12345bb 0003defgtrn12345cccdddd33331234zzzzmtd0003qwertrn56789 defghigkaa 9876bvcntrn56789bb 8765zxcvtrn56789xxxbbbb11115678aaaayyycccc22221234bbbb can you please help me on this, how can you achieve this using unix shell scripting?",
    "present_kp": [
      "shell",
      "shell script"
    ],
    "absent_kp": [
      "text processing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how do i publish my stuff to be visible in google reader player?. i see many nice pix etc. @ google reader play.is there a way i can share my own pix / vids / stuff so it should publicly be visible there too?",
    "present_kp": [
      "google reader"
    ],
    "absent_kp": [
      "publishing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is the role of the public folder in mvc web applications?. must all the controllers, models and views be placed in the private folders? if so, what are the roles of the public folder? what are the correct terminologies for such roles in computer science? how does the gui relate with such concepts?otherwise, does the controllers and/or views have to be distributed across the private and public folders? in that case, the scripts that are directly invoked via post, are just interfaces, or controller interfaces, or view interfaces?",
    "present_kp": [
      "web applications",
      "mvc",
      "gui"
    ],
    "absent_kp": [
      "design patterns",
      "directory structure"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "drawing an archimedean spiral using pillow. from rosetta code:the archimedean spiral is a spiral named after the greek mathematician archimedes. it can be described by the equation: $$r=a+b heta$$ with real numbers \\$a\\$ and \\$b\\$.here is my attempt to draw it in python (using pillow):this module creates an archimdean spiral.from math import cos, sin, pifrom pil import image, imagedrawdef translate(point, screen_size): takes a point and converts it to the appropriate coordinate system. note that pil uses upper left as 0, we want the center. args: point (real, real): a point in space. screen_size (int): size of an n x n screen. returns: (real, real): translated point for pillow coordinate system. return point[0] + screen_size / 2, point[1] + screen_size / 2def draw_spiral(a, b, img, step=0.5, loops=5): draw the archimdean spiral defined by: r = a + b*theta args: a (real): first parameter b (real): second parameter img (image): image to write spiral to. step (real): how much theta should increment by. (default: 0.5) loops (int): how many times theta should loop around. (default: 5) draw = imagedraw.draw(img) theta = 0.0 r = a prev_x = int(r*cos(theta)) prev_y = int(r*sin(theta)) while theta < 2 * loops * pi: theta += step r = a + b*theta # draw pixels, but remember to convert to cartesian: x = int(r*cos(theta)) y = int(r*sin(theta)) draw.line(translate((prev_x, prev_y), img.size[0]) + translate((x, y), img.size[0]), fill=1) prev_x = x prev_y = yif __name__ == '__main__': image_size = 300, 300 img = image.new('1', image_size) draw_spiral(1, 2, img) img.save('spiral.png')the program outputs this image:",
    "present_kp": [
      "python"
    ],
    "absent_kp": [
      "graphics"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "use basename to parse a list of paths held in a file. i'm running mac osx and trying to use the command line to find the number of files i have with the same name. i tried to use the following command:find ~ -type f -name * -print | basename | sort | uniq -d > duplicate_filesit doesn't work! when i do the following:find ~ -type f -name * -print > duplicate_filesthen duplicate_files does contain the paths of all my files. so i think the issue is with basename - it doesn't accept standard input. i then tried the following:basename $(find ~ -type f -name * -print) > duplicate_filesbut again that doesn't seem to work. search on the internet doesn't seem to yield much joy. any thoughts most welcome.",
    "present_kp": [
      "find",
      "osx",
      "duplicate",
      "basename"
    ],
    "absent_kp": [
      "filenames"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "move a todo-list to other card?. is it possible to move a todo-list from one card to another? it seems difficult even to copy/paste the list.",
    "present_kp": [
      "todo"
    ],
    "absent_kp": [
      "trello"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to compute $\\delta u$ on the boundary of the biharmonic equation?. let $u$ be the answer of a pde.is there any relationship between $u, rac{\\partial u} {\\partial n}$ and $\\delta u$.i have the values of $u$ and $ rac{\\partial u} {\\partial n}$ on $\\partial \\omega$ but i need the value of $\\delta u$ on the boundary.i think it's impossible, but is there anyone who know how to do this?in fact i want to solve biharmonic equation by convert it into two poisson problems:$$\\delta^2u=f$$$$u=g_1$$$$ rac{\\partial u} {\\partial n}=g_2$$.using $\\delta u=w$ leads to $$\\delta u=w,$$$$ u=g_1 ~~on~~\\partial \\omega$$$$\\delta w=f ,$$$$w=\\delta u-c( rac{\\partial u} {\\partial n}-g_2)~~on~~\\partial \\omega$$so at the first i have to use an initial guess for $\\delta u$on boundary. but by this way the accuracy is low. and sometimes it is dependent to initial guess.here $c$ is a small constant.",
    "present_kp": [
      "pde"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "fenics: solving linear system with blockmatrix and blockvector. i have set up a linear system using a blockmatrix and blockvector, where blocks arise from assembled systems and self introduced linear systems. but i see that the solve doesn't take these entities as valid inputs. how can solve such a system which has been constructed using these objects? also will it be possible to separate out parts which correspond to different blocks (something akin to the split() method used for mixedfunctionspaces ) ?below is a minimal code for what i seek to do inside an iteration eventually. i'd appreciate any comments regarding the quality of the code so far: please suggest improvements if you think some things can be done more elegantly. for now i am struggling to get the code working but this input will also go a long way.from dolfin import *from mshr import *from petsc4py import petscn = 128alpha = 1e-3mesh = unitsquaremesh(n,n)v = functionspace(mesh, 'cg', 1)u = trialfunction(v)v = testfunction(v)yd = expression('exp(-50*(pow(x[0]-0.5,2) + pow(x[1]-0.5,2)))')a = inner(grad(u),grad(v))*dxbc = dirichletbc(v, constant(0.0), on_boundary)z = yd*v*dx a, z = assemble_system(a,z,bc) at = assemble(adjoint(a)) bc.apply(at) m = assemble(u*v*dx) bc.apply(m)n = v.dim() ap = am = zap.zero()am.zero()ap_old = apam_old = amgamma = 1as = as_backend_type(a)as.zero()dvec = (ap+am)dvec*=(gamma)as.set_diagonal(dvec)h = blockmatrix(2,2)h[0, 0] = ah[1, 0] = ash[0, 1] = -1*mh[1, 1] = atg = blockvector(2)g[0] = alpha*gamma*(ap-am)g[1] = -1*zx = blockvector(2)solve(h, x, g) # evidently, this is where it throws an error(y,p) = split(x)",
    "present_kp": [
      "petsc",
      "fenics"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "extracting info from blocks inside text file. i have a text file which has blocks likedir1/dir2/dir3/name_run_number1:line1_run_number1_part1line2_run_number1_part2line3_run_number1_part3...each block is separated with a blank line and there is the : in the header of each one while each block has a unique number1 after run_ suffix which is also present in the lines inside the blockwhat i want to do is for each block, extract the number1 as shown in the first line and then for the lines below count from 1-20 and give a message if a partx line is missing. any bash or python would be finethanks",
    "present_kp": [],
    "absent_kp": [
      "text processing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how do i avoid the leaning toothpick syndrome in awk?. when matching patterns with / in them, having to escape the / quickly becomes unwieldy, ugly:/\\/usr\\/share\\/man\\//with sed, perl or vim, i would use a different delimiter for the regex, say ::sed '\\:/usr/share/man/: do something'perl -ne 'print if m:/usr/share/man/:':g:/usr/share/man/: do something vimhow can i avoid this awk? something like:awk ':/usr/share/man/: {do something}'the best i can think of is to use a variable:awk -v pat='/usr/share/man/' '$0 ~ pat {do something}'but that is very verbose compared to the sed/perl/vim method.of course, there might be other ways to match paths like /usr/share/man/, but that's not the only place where / could appear in a pattern.",
    "present_kp": [
      "awk"
    ],
    "absent_kp": [
      "regular expression"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to get the most out of an annoying project. i was recently assigned a task of managing a small project.the project has some technological aspects but the job is mainly doing managerial things: scheduling, making sure people do what they are suppose to, going to a lot of meetings, etc. basically, it's not how the average programmer or even a team leader would like to spend his time.since i am not going to sneak out of this, i have found myself wishing i could teleport myself to some point of time after the project has ended. after pondering about it some more, i reached the conclusion that i hate working like that and i can't allow myself to work on a project that makes me think this way. i have to make the most out of this project.so my goal is to make the most out of this unwanted project. how do i do that?i thought about writing every day a what i have learned today list, that will help me see clearly that i am learning from this and not wasting my skills.do you have any recommendations on what can i do?",
    "present_kp": [],
    "absent_kp": [
      "project management"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "started with ubuntu, apt-get upgrade, now i have trisquel?. i was considering moving to trisquel (although, to be clear, i never downloaded anything related to it), so i'm not necessarily heartbroken, but i'm very unsure of what happened with this.i used ubuntu 14.04lts for my work computer, and i decided to run an upgrade. when the upgrade was finished, i noticed that a lot of ubuntu related things didn't work. i found out i now have trisquel gnu/linux.i'm guessing i should probably back-up and do a fresh install, but does anybody have any insight on how this little bit of weirdness happened? i've never upgraded and woke up with a new distro before.",
    "present_kp": [
      "ubuntu",
      "upgrade",
      "trisquel"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "unbalanced parenthesis correction. i'm prepping for a coding interview, and am examining the task of correcting unbalanced parentheses. the finger tree seems to be the right data structure.as a proof of concept i've used data.sequence to test my idea. if this is the right direction to go, i'll write more specialized finger tree code.the code works on the few test cases i have tried. feedback appreciated.{-# language viewpatterns #-}module parenthesis whereimport basicprelude hiding (concat,null,empty)import data.sequence hiding (length)import data.foldable hiding (length,null)data debit = rpbalanceparens :: string -> stringbalanceparens str = go str [] empty where go :: string -> [debit] -> seq char -> string go [] [] (null -> true) = [] go [] [] parens = data.foldable.tolist parens go ('(':xs) [] (null -> true) = go xs [rp] (singleton '(') go (')':xs) [] (null -> true) = go xs [] (fromlist ()) go ('(':xs) debit parens = go xs (rp:debit) (parens |> '(') go (')':xs) [] parens = go xs [] corrected where corrected = ('(' <| parens) |> ')' go (')':xs) (rp:debit) parens = go xs debit (parens |> ')') go (_:xs) debit parens = go xs debit parens go [] (rp:debit) parens = go [] debit (parens |> ')')example:balanceparens ))((())()balanceparens )))((()))",
    "present_kp": [],
    "absent_kp": [
      "haskell",
      "functional programming"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "grepping foo and bar. i am looking for a command options for grep to find files with the occurences of foo and bar.grepping withgrep -r -e foo -e bar .shows files which have only foo or only bar and files which have foo and bar.is it possible with grep to find only files which have both foo and bar (and display the lines that match either foo or bar or both in those files only)?example:echo foo > file1echo bar > file2(echo foo;echo;echo bar) >file3echo barfoo > file4the grep cmd:grepcmd -r -e foo -e bar ../file3:foo./file3:bar./file4:barfoo",
    "present_kp": [
      "grep"
    ],
    "absent_kp": [
      "search"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "best practices for when a 3rd party dependency breaks?. i am working in a php project, and our 3rd party dependencies are managed with packagist via composer.json. pretty standard stuff. sometimes we run into a situation where a given plugin has some issue, such that we would like to continue to use the plugin but the issue makes it a liability or just straight up unusable.one idea might be to make a pull request to fix the issue, but that involves a. the repo is actively monitored and b. we actually know how to fix the issue. we may have neither. if we have (b) but not (a) we could fork the repo with our own fix? or we could modify the file in the vendor directory and commit it to the repo? both prospects seem unappealing. most recently the php-whois package had an issue, because it used some terribly asinine code:if (empty($r)) { if ($hasreg) $r['registered'] = 'no';}if $r is unset, or ='' or =0 or =null or a number of other situations, empty($r) will be true and $r['registered'] = 'no' will at the very least trigger a warning, or, as of php ~7.1+, trigger a fatal error.what is the best course of action? find a new php-whois package? make the (easy) fix ourselves (just do $r=[];) and commit to our own repo? something else? this is a tricky situation that i don't find myself in often, but it has come up more than once.using laravel 5.3, in case that's relevant.",
    "present_kp": [],
    "absent_kp": [
      "third party libraries"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why do people want to have children?. sooner or later most people will want to have children. what are some of the most common psychological reasons leading people to want to have children? what are their motivations and expectations?",
    "present_kp": [],
    "absent_kp": [
      "social psychology",
      "developmental psychology",
      "cross cultural psychology"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "water pouring challenge. i'm trying to write code to solve water pouring problem in clojure. the following code is strongly influenced by martin odersky's solution lectured in his coursera course, functional programming principle in scala. he first created a class pouring with parameter capacity, which is accessible everywhere in the class. though he made the best use of case class in his solution, i just used plain clojure map to represent the moves.i tried to solve the problem in the same way, and end up with the following code:(ns pouring)(declare capacity init-state)(defn empty [state glass] (assoc state glass 0))(defn fill [state glass] (assoc state glass (capacity glass)))(defn pour [state [from to]] (let [amt (min (state from) (- (capacity to) (state to)))] (-> state (assoc from (- (state from) amt)) (assoc to (+ (state to) amt)))))(defn change [state move] (if state (cond (:empty move) (empty state (:empty move)) (:fill move) (fill state (:fill move)) (:pour move) (pour state (:pour move))) (change init-state move)))(defn moves [capacity] (let [glasses (range (count capacity))] (lazy-cat (map (fn [g] {:empty g}) glasses) (map (fn [g] {:fill g}) glasses) (for [from glasses, to glasses :when (not= from to)] {:pour [from to]}))))(defn extend-path [path move] {:history (conj (:history path) move) :end-state (change (:end-state path) move)})(defn extend ([paths explored] (if-let [more (for [path paths next-path (map #(extend-path path %) (moves capacity)) :when (not (contains? explored (:end-state next-path)))] next-path)] (lazy-cat paths (extend more (conj explored (map #(:end-state %) more)))))) ([] (extend #{{:history [], :end-state init-state}} #{init-state})))(defn init [c] (def capacity c) (def init-state (vec (repeat (count c) 0))))(defn solve [capacity target] (init capacity) (first (for [path (extend) :when (some #(= % target) (:end-state path))] path)))the part i don't like is init function. most clojurians says that using def inside a function is bad. if i don't use init function, perhaps i have to add parameter capacity to almost every function in the code, which looks not pretty.i was considered using nested function.(defn solve [capacity target] (defn empty ...) (defn fill ...) (defn pour ...) (defn extend ...) ...)however, using defn inside function is also bad, and the functions defined inside are actually not local functions, which are accessible outside the outer function.yes, there's letfn in clojure. i can put every function except solve into letfn. but in this case, the code is not very readable. i think letfn is only for short (which can be expressed in 1 or 2 lines) functions.perhaps there are better ways to do this. any suggestions?",
    "present_kp": [
      "clojure"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "separating command arguments from flags. say i have a command that takes flags and arguments, like this:mycommand --foo bar arg1 arg2(the foo flag has a value of bar.)but what if the foo flag can optionally be supplied with no argument?mycommand --foo arg1 arg2how do i prevent arg1 being considered the value for the foo flag?",
    "present_kp": [
      "arguments"
    ],
    "absent_kp": [
      "shell",
      "command line"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to change bugzilla change privacy notice?. how can i change the privacy notice in both the site and email messages? my instance is not public facing. i don't like this:privacy notice: bugzilla is an open bug tracking system. activity on most bugs, including email addresses, will be visible to the public. we recommend using a secondary account or free web email service (such as gmail, yahoo, hotmail, or similar) to avoid receiving spam at your primary email address.",
    "present_kp": [
      "privacy"
    ],
    "absent_kp": [
      "bug trackers"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "does image mime-type affect seo?. i have uploaded images as mime-type application/octet-stream. they are served well if sourced from an image tag but chrome for example cannot open them directly, always asks for download. the question is, does the wrong mime-type affect findability and seo?",
    "present_kp": [
      "seo",
      "images"
    ],
    "absent_kp": [
      "google image search",
      "content type"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "anyone know of any parallelizing compilers following the approach of the dragon book?. in compilers: principles, techniques, & tools, aho et al describe an approach for optimizing for parallelism (chapter 11 in the second edition). is anyone aware of any existing compilers which follow that approach?",
    "present_kp": [
      "compiler",
      "parallelism"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "is there any software like ip blocker for remix os?. is there any software like ip blocker for remix os, which blocks sites except white list? (i only want to use local network and want to block others.)",
    "present_kp": [
      "ip"
    ],
    "absent_kp": [
      "software rec"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "monitor mirroring. i need to mirror a remote monitor from a server running centos 6.5. with vnc i'm able to connect to the :0 display (that it's the one it is shown on the remote server monitor) but when i change user on the server without logging out it obviously opens a new display and the local screen became useless. are there any options to overcame this issue ? starting vnc before logging could achieve this ? unfortunately i run into a bug (i've asked on a centos forum) and i can't start vnc that way because i'll need to upgrade some packages that could cause some stability issues.if this works good to know (i can use vnc on other servers) but other options are really appreciated.",
    "present_kp": [
      "centos",
      "vnc"
    ],
    "absent_kp": [
      "remote desktop"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "application of histogram of oriented gradients in colored image. i recently learned about face recognition with deep learning here. one of the approach involved is histogram of oriented gradients which is used for face detection as follows (short summary) :convert image to black and whitelook at every pixel in image and detect surrounding pixeldraw and arrow in direction where surrounding pixels are getting darkerrepeat the whole process this results in hog version of image ,something like this :question can we use similar method in colored images like how does rgb varies at pixel level or how does pixels vary in colored images to improve the accuracy of face detection and similarity.purpose face comparison is a bit tedious in this approach in terms of accuracy. so, i want to find some sort of hash function/value(just using as layman term) to derive unique value from every image. this might improve face comparison easier.additionally , sharing any already implemented approach in this direction will be highly appreciated.",
    "present_kp": [
      "deep learning"
    ],
    "absent_kp": [
      "machine learning",
      "image classification"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to run a cronjob in my regular environment?. both the shell and environment variables that cronjobs are run in is completely different from the ones presented to me in gnome-terminal. how can i run a cronjob under the same circumstances as if i had run it in the terminal?my current solution is running the cronjob env display=:0.0 gnome-terminal -e my-command, but this pops up a gnome-terminal, which isn't really acceptable.",
    "present_kp": [
      "environment variables",
      "cron"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "generating word list base on possible numbers/characters via crunch in kali linux. i'm new to crunch. i'm trying to generate a list containing 8 character long words.i've triedcrunch 8 8 > pw.txtit freezes my computer because it doesn't have enough space for that. is there a way to create a word list based on a certain number/character that i might have use to take less time/space to create ? possible password a2016010b2016010c2016010...z2016010any hints / suggestions on this will be much appreciated !",
    "present_kp": [
      "kali linux",
      "password"
    ],
    "absent_kp": [
      "text processing",
      "compression"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "silently start task in background. i know that you can use this to start a process in the background, without getting the notification when the process is put in the background and when it is done like so:(command &) &> /dev/nullhowever this removes the option to trap when the process finishes (trap child_done chld).how can i have both?",
    "present_kp": [
      "process",
      "trap"
    ],
    "absent_kp": [
      "zsh"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to download latest version of cmake using tar xzf?. when i did cmake --version i got this -cmake version 2.6-patch 4",
    "present_kp": [
      "tar",
      "cmake"
    ],
    "absent_kp": [
      "software installation"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to see the contents of a file i deleted, but that a process is still writing to. i started a very long running job (expected to take 6 days to finish), and want to see its output, so i did:$ nohup ./thejob.sh > out.txtwhen i need to see the job progress i tail - f the file.but the out.txt file was growing too much and i deleted the file and created it again.$ rm out.txt$ touch out.txtafter this, no output is sending to the file. i think the job lost the bind to the file. i can see the job is running by using top but i can't see its progress anymore.is there any way to see it again?",
    "present_kp": [
      "nohup"
    ],
    "absent_kp": [
      "linux",
      "io redirection",
      "deleted files"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "checking proper english sentences of user's input with grammar using c++. write a program that checks if a sentence is correct according to the english grammar in 6.4.1. assume that every sentence is terminated by a full stop (.) surrounded by whitespace. for example, birds fly but the fish swim . is a sentence, but birds fly but the fish swim (terminating dot missing) and birds fly but the fish swim. (no space before dot) are not. for each sentence entered, the program should simply respond ok or not ok. hint: dont bother with tokens; just read into a string using >>.#include <string>#include <iostream>using namespace std;string str;void putback(string s) { str = s;}string verb() { if (str == rules || str == fly || str == swim) { return str + ' '; } else return ;}string noun() { if (str == birds || str == fish || str == c++) { return str + ' '; } else return ;}string article() { if (str == the) { string temp = str; cin >> str; return temp + ' '; } else return ;}string conjunction() { if (str == and || str == or || str == but) { return str + ' '; } else return ;}string l;string sentence() { l += article(); if (noun() == ) { cerr << not ok; return ; } else { l += noun(); cin >> str; string temp = verb(); if (temp == ) { cerr << not ok; return ; } else { l += temp; cin >> str; if (str == ) { cerr << not ok; return ; } else if (str == .) { l += .; return l; } else { if(conjunction() == ) { cerr << not ok; } else { l += str + ' '; cin >> str; } return sentence(); } } }}int main() try { string sen = ; while (cin) { cin >> str; sen = sentence(); if (str == .) { cout << sen << ' '; } } //keep_window_open();}catch (exception& e) { cerr << error: << e.what() << ' '; //keep_window_open(); return 1;}catch (...) { cerr << oops: unknown exception! ; //keep_window_open(); return 2;}the grammar:sentence: noun verb // e.g., c++ rules article noun verb sentence conjunction sentence // e.g., birds fly but fish swimconjunction: and or butarticle: thenoun: birds fish c++verb: rules fly swimthis is perfectly working code. is there anything i can improve on for my code? do you think using global variables in this case is okay? even for whether i have proper variable names is worth commenting. this question is from this webpage, exercise 6 at the very bottom.",
    "present_kp": [
      "c++",
      "grammar"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "easiest improvement on first-fit for bin packing algorithm. see the interactive example here. first-fit on the left, optimal on the right.i know that in general, optimal bin-packing is np-hard, so i'm not looking for a perfect solution. i'm looking for the lowest cost improvement over the current solution.the problem is the one most of the web pages on the internet have: how to best pack a set of rectangles into an enclosing rectangle, preserving order, with no overlaps. actually the enclosing rectangle is only a fixed width, it expands to arbitrary height needed to fit all the rectangles, but we still want everything aligned along the top. real web browsers do it by relying on a rather restrictive set of rules, one of the most significant of which is that the items are presented in a given order and that order must be maintained when laying out the items in the same way text is laid out, which for english is left-to-right, top-to-bottom.for some cases, though, such as a set of images, the order could be changed somewhat in order to achieve better packing. we do want to limit the reordering, so that an item ends up no more than n places out of position, but we don't need n to be zero.a web library called packery implements this with a basic first-fit algorithm. with gravity pulling the rectangles to the top-left, it places the first item where it will fit, which of course is the top-left of the enclosing rectangle. it then places the next item where it will fit, and so on, until all the items are placed. this works very well when the rectangles are all related by small integers, e.g. 1x1, 1x2, 1x3, 2x1, 2x2, 2x3, 3x1, 3x2, 3x3, with the enclosing rectangle being some integer width, and there are enough small rectangles. in our example the outer rectangle is 3 units wide. our inefficient packing algorithm on the left becomes optimal on the right with the addition of just 2 more rectangles that match the voids. so the question is: is there a low-cost way to detect the inefficient packing and backtrack or make some other alteration to get to the efficient packing in the middle? keep in mind that we want to preserve order as much as possible, so the optimization of pre-sorting the list so that the largest rectangles are first is not acceptable.",
    "present_kp": [],
    "absent_kp": [
      "dynamic programming",
      "greedy algorithms"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "identify missing files in a folder. i have a process creating the following files, which share a similar pattern.file_1.txtfile_2.txt...file_1000.txt...file_1901.txtfile_1902.txtbut, there are only 1890 files in the folder. i would like to know if there is a way to identify the missing files from the list of files sharing a pattern.",
    "present_kp": [
      "files"
    ],
    "absent_kp": [
      "regular expression",
      "filenames"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "software design for a class utilizing many different services. i am trying to model a design for a c# project. trying to follow the solid principles, this is what i've got so far:sdkclient class which has three (for this explanation) important properties (iauthprovider, ihttpprovider and ifileservice). the sdkclient should be decoupled, so i am always free to exchange the auth class or http class. that's why i am only using interfaces (for later di).but the real problem lays in the services. so far i was using the client purely for so called fileservices, which share the same behaviour. now i am not sure what to do. i am supposed to use new services, which don't share anything in common with fileservices.so my question is, is there a way to generalize it to something like iservice, or is this the wrong way, are maybe some factories the right solution here, or shouldn't the service be in the sdkclient?i am pretty inexperienced in software design, that's why i am hoping for really neat solutions/ideas.if my explanation wasn't sufficient, just ask me questions, about the points you couldn't understand.",
    "present_kp": [
      "c#",
      "design",
      "interfaces"
    ],
    "absent_kp": [
      "design patterns",
      "architecture"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how can i get a regular expression to only match an empty string? (in a .htaccess file). i try to make a regular expression in a .htaccess file, that matches only an empty string.i have tried many things, but it seems like its impossible. for example, i tried ^$, but it's looking for that will always exist in a string.so i seek answers to it all possible.if possible, i would like to hear how to extend such a regular expression together.here is the content of my .htaccess file: rewriteengine onrewriterule ^$ <url> [r=302,l]rewriterule ^guestbook.html$ <url> [r=301,l,ne]rewriterule ^sites/guestbook.html$ <url> [r=301,l,ne]rewriterule ^guestbook$ <url> [r=301,l,ne]rewriterule ^(\\w+).html$ <url> [r=301,l,ne]rewriterule ^sites/(\\w+).html$ <url> [r=301,l,ne]rewritecond %{https} onrewriterule ^(\\w+)$ ?site=$1.html [l]rewritecond %{https}| offrewriterule (.*) <url>%{request_uri} [r=301,l,ne]",
    "present_kp": [
      "htaccess",
      "regular expression"
    ],
    "absent_kp": [
      "redirects",
      "apache",
      "mod rewrite"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "raviart-thomas elements global definition and compact support. as per the suggestion by christian in the comments here, as part of my continuing quest to understand the raviart-thomas (rt) elements i'd like to know how exactly the rt elements are defined globally, and in particular how they have compact support. for rt0 on the reference square, one of the basis functions is $\\mathbf{\\phi}(\\mathbf{x}) = rac{1}{4}\\langle 1 + x, 0 angle^t$. this function is only dependent on $x$ so it is non-zero over all elements above and below the reference square. since the rt are $h$(div) conforming, i suppose there is no need to enforce continuity in the solution, or the basis functions. as i understand it, this means that we could simply set $\\mathbf{\\phi}$ to be $\\mathbf{0}$ outside some domain. as a concrete example, given the edge numbering below, (i assume there is one basis functions per edge for rk0, but this may be wrong) what basis functions are non-zero over the middle element (a)?as a separate question, for langrangian elements of order $k$ we choose the finite dimensional subset of $h^1$ to be the set of all piece-wise continuous polynomials of order $k$. for the rt elements of order $k$ we take the subspace of $h$(div):$$p_{k+1,k} imes p_{k,k+1}$$as defined in the answer to my last question. does this space have a name?",
    "present_kp": [],
    "absent_kp": [
      "pde",
      "finite element"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "extracting attachments from mail that is coming to my postfix inbox. how do i extract attachments from mails arriving at my postfix mailbox and preferably send the content of the attachment to another email account? i am interested in being able to acquire/read the content of the attachment on ubuntu terminal.",
    "present_kp": [
      "email",
      "postfix"
    ],
    "absent_kp": [
      "fetchmail"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "flipping bits python implementation. problem statementyou will be given a list of 32 bits unsigned integers. you are required to output the list of the unsigned integers you get by flipping bits in its binary representation (i.e. unset bits must be set, and set bits must be unset).input formatthe first line of the input contains the list size t, which is followed by t lines, each line having an integer from the list.constraints\\$1t100\\$\\$0integer<2^{32}\\$output formatoutput one line per element from the list with the requested result.solutionfor _ in range(int(raw_input())): n = int(raw_input()) n = n & 0xffffffff # 32 bit representation print n ^ 0xffffffffhint: <url>",
    "present_kp": [
      "python"
    ],
    "absent_kp": [
      "programming challenge",
      "python 2.7",
      "bitwise"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "should mobile webpages have hreflang links to non-mobile pages?. my site has multilingual links, which are specified like this on non-mobile pages: <link rel=alternate hreflang=en href=<url> /> <link rel=alternate hreflang=jp href=<url> /> <link rel=alternate hreflang=ko href=<url> />in addition, these non-mobile pages link to a mobile version: <link rel=alternate media=only screen and (max-width: 640px) href=/mobile/page />now the question is about what links should be in the mobile page, which isn't translated to different languages now. is this enough: <link rel=canonical href=/page/>or should i also have the same group of hreflangs that point to non-mobile pages?",
    "present_kp": [
      "mobile",
      "hreflang"
    ],
    "absent_kp": [
      "canonical url"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "looking for a webhost that offers both linux and windows. possible duplicate:how to find web hosting that meets my requirements? to deploy my .net web-application i need both windows (for frontend) and linux (for database) virtual servers (vps). what windows+linux hoster would you recommend?of course, i am not interested in windows or linux-specific hosters. also i found some mix hosters who provides low prices for linux but high for windows or vice versa.",
    "present_kp": [
      "web hosting"
    ],
    "absent_kp": [
      "looking for hosting"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "windows-like visual style for gnome/cinnamon?. i'm secretly trying to replace a windows machine with linux for a girl and make it look like windows the way it is actually possible to. i was unable to find appropriate apps like panel and menu launcher for that purpose except some gnome themes. what i need is a panel and a menu launcher for linux that can simulate windows xp/7. if there is any, do please link it.",
    "present_kp": [
      "linux",
      "theme"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "maintaining users in a dovecut user database. i learn bash scripting occasionally from time to time, now i am working on a simple bash script that serves to maintain dovecot user database. the general purpose of the script is able to list, add or delete user entries in the specified file. please add some comments about the code - common mistake and etc. i want to learn more about code simplicity and avoiding bad habits.#!/bin/bashuserdb=/etc/dovecot/usersmailboxmap=/etc/postfix/virtual/virtual-mailbox-maps.cf## add user functionadd_user () {# reading usernamewhile [[ -z $username ]]; do echo -n please enter username: read usernamedone# reading domain namewhile [[ -z $domain ]]; do echo -n please enter domain name: read domaindone# reading password and generating hash with doveadm pwwhile [[ -z $password ]]; do echo -n please enter user password: read passworddoneif [[ -n $password ]]; then userpass=$(doveadm pw -p $password -s sha512-crypt)fi# adding provided credentials to dovecot userdbecho adding user credentials to userdb...echo $username@$domain:$userpass::: >> $userdb# adding provided user@domain to postfix mailbox mapecho adding user credentials to postfix map...echo $username@$domain $domain/$username >> $mailboxmap# hashing the mapif [[ -f $mailboxmap ]]; then postmap hash://$mailboxmap echo hashing map is done!else echo postfix map not exist or have been moved to another directory! exit 1fiexit 0}## list all users functionlist_users () {if [[ -f $userdb ]]; then awk -f: '/./'' {print $1} ' $userdbelse echo user database not exist or have been moved to another directory! exit 1fi}## delete user functiondelete_user () {if [[ -n $deluser ]]; then sed -i /^$deluser$/d $userdb $mailboxmap && echo user account have been deleted!else echo user not found! exit 1fi}case $1 in add|-a) add_user ;; list|-l) list_users ;; delete|-d) shift deluser=$1 delete_user ;; *) echo unknown command ;;esac",
    "present_kp": [
      "bash"
    ],
    "absent_kp": [
      "linux",
      "email"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "contains method in lambda expression. i use this method to extract table name and field name from formula.public list<string> extractfieldsfromformula(string formula){ list<string> formulas = new list<string>(); formulas.addrange(formula.split(new char[] {'{', '}'}).where(f=>!f.contains(+) && !f.contains(-)&&!f.contains(/)&&!f.contains(*)&& !f.contains(()&& !f.contains()) && f.trim()!=string.empty)); return formulas;}my formula is like this:{city.a}+5*(2/{city.b})and result of method is a list :city.acity.bhow can i write this code better than this?!!",
    "present_kp": [],
    "absent_kp": [
      "c#",
      "linq"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is it decidable whether the langauge accepted by a reversal-bounded counter machine is deterministic?. i'm wondering if anyone can point me to either an algorithm or an undecidability proof for the following question:given a non-deterministic reversal-bounded multicounter machine $m$,is there some deterministic reversal-bounded multicounter machine $m'$such that $l(m) = l(m')$?for anyone interested, reversal-bounded counter machines are described at lengthin oscar ibarra's paper about them.",
    "present_kp": [
      "decidability"
    ],
    "absent_kp": [
      "reference request",
      "fl.formal languages",
      "computability",
      "automata theory"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how do i mount an external sd card without using sudo?. i wrote a bash-script with the intention to back up a specific (truecrypt-)file onto a sd-card (and a folder shared over the cloud) using the rsync command. the problem was that the sd-card usually wasn't mounted yet and thus the synchronization didn't work.so i wrote a script that mounts the card, syncs the file and should unmount it again. script:#!/bin/bashsudo mkdir /media/user_name/sdcardsudo mount -t vfat label=sdcard /media/user_name/sdcardrsync -avi /home/user_name/dokumente/file_to_sync /media/user_name/sdcardrsync -avi /home/user_name/dokumente/file_to_sync /home/user_name/cloudboxsudo umount /media/user_name/sdcardsudo rmdir /media/user_name/sdcard while the synchronization to the cloudbox works properly, the sync to the sdcard does not. furthermore the umount and the rmdir commands both are not executed.the result being that the following time i use the script, there is an error because the directory already exists. when all commands are executed manually and in succession in the terminal, the rsync command (with sudo) to the sdcard gives an error message:user_name@host ~ $ sudo rsync -av ~/dokumente/file_to_sync /media/user_name/sdcard/file_to_syncsending incremental file listfile_to_syncrsync: chown /media/user_name/sdcard/.file_to_sync.tk2rnm failed: operation not permitted (1)sent 524,416,101 bytes received 127 bytes 61,696,026.82 bytes/sectotal size is 524,288,000 speedup is 1.00rsync error: some files/attrs were not transferred (see previous errors) (code 23) at main.c(1183) [sender=3.1.0]strangely, if i mount the sdcard by double clicking on the desktop icon instead of using sudo mount ..., i can synchronize the file without any problems.what is wrong with my script? i have tried several modifications, but none worked (better).why is there a difference between the mounting by double clicking and the mounting in the terminal and what is it? how could i reproduce the double-click-command in the terminal?i have been trying to get this to work for quite some time now and forums and tutorials etc. could not help me out. so i'd appreciate any tiny little bit of help anyone can give...thanksmy system: linux mint 17 (originally cinnamon but changed to xfce) on a lenovo t440p with core i7vpro, 250 gb ssd, 12 gb ram (and the worst touchpad i have ever encountered)",
    "present_kp": [
      "mount",
      "rsync",
      "sd card"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "oh-my-zsh overriding my function?. i use zsh with oh my zsh and i am trying to define a function called git, as such:function git() { echo i'm happy because i am executed! }i have placed the function definition in $zsh/custom/general.zsh.everything else in this file works (i have a bunch of aliases there) except this function.running which git outputs:git () { case $1 in (commit|blame|add|log|rebase|merge) exec_scmb_expand_args $_git_cmd $@ ;; (checkout|diff|rm|reset) exec_scmb_expand_args --relative $_git_cmd $@ ;; (branch) _scmb_git_branch_shortcuts ${@:2} ;; (*) $_git_cmd $@ ;; esac}removing git from plugins=( ... ) didn't work. trying to find this function in oh my zsh yielded no results. i read the source code of oh-my-zsh.sh, and it seems the custom directory is loaded after all of omz's files, so it didn't make any sense to me, that when i placed my function at the bottom of .zshrc it worked.any ideas on how to keep the function in the custom folder? i would like to keep things organized.",
    "present_kp": [
      "zsh",
      "oh my zsh"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "difference between creative commons and gpl?. why shouldn't i release code under a creative commons license?for me the cc-by-sa sounds very nice and is quite more readable than a whole gpl license, so i would like to know why its such a bad idea to release code with this license.",
    "present_kp": [
      "gpl"
    ],
    "absent_kp": [
      "license compatibility",
      "cc by sa"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "script to insert line into files in sub-dirs. i have a sub-dir tree with varying number of branches and most ofthe branches contain .cpp files (many of them). i have a header-fileat the root of the tree that i want to have as a#include <constructed-relative-path-to-root>/headerfile.has the first line of each .cpp.an alternative to constructed-relative-path-to-root would be thehard-coded path, which would have to be adjusted whenever the project is relocated)a second alternative would be to copy the content of the header-file in at the top of each .cpp file i have no idea how to write such a script. can anyone please help?",
    "present_kp": [],
    "absent_kp": [
      "linux",
      "scripting"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "does the peripheral nervous system processes information like central neurons do?. neural coding deals with the problem on how neurons or a network of neurons processes a stimulus and creates a response in form of electrical action potentials. information then might be encoded in the rate of the action potentials or their timing.my questions now are:do nerves, e.g. in the arm of a human, merely relay any information, or do they also process incoming signals in any way? as a result, the electrical signal would then differ when measured at the beginning and the end of a nerve.can a nerve be stimulated at one point with a certain electrical signal as input (with varying rate etc. to encode information) and can the output be measured at another point? if i understood correctly, this is done at a electroneuronography (enog) only that the electrical stimulus is not varying in rate or other properties.any links to research that deals with similar things would be great.",
    "present_kp": [],
    "absent_kp": [
      "neurobiology",
      "measurement",
      "neurophysiology",
      "electrophysiology",
      "experimental neuroscience"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "setting up proxy settings on debian. during installation of debian i was asked for a standard proxy string in the form of http://user:password@host:port/, which i entered. apt-get ran and retrieved files during update, and now debian is installed. however, when i tried running sudo apt-get install ..., i get an error message containing the message could not resolve proxy_host where proxy_host is the host i entered during installation. are there other places where i have to set up the proxy information?",
    "present_kp": [
      "debian",
      "proxy"
    ],
    "absent_kp": [
      "http proxy"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "unit tests for database abstraction layer. i previously asked this question about creating an algorithm for joining records from multiple databases, and since then, i have built a rudimentary version of it (which doesn't allow for inter database table associations to keep it simple for the moment), and now i am trying to test it. so far, i have tested doing joins, custom columns, where and having clauses, order by's, and limits, and they seem to all work perfectly. but, most of my applications that do database queries are quite simple, with just a few joins, a couple of custom columns, and a few other bits and pieces of information, so i was wondering if there was a nice series of really complex queries i could pass through it to ensure that it can really do everything that would be needed by 99% of applications? i know its a bit broad, but even just a handful of complicated queries would be fantastic to test it out.",
    "present_kp": [
      "database"
    ],
    "absent_kp": [
      "unit testing",
      "jquery"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to search for viber app files. i am trying to perform filesystem extraction on viber app. my findings show that viber contains a folder named databases which contained files of interest, namely, viber_messages, viber_data and viber_call_log.db. hence first i made sure that my android device is connected by using the command adb devices, then typed adb shell. then i decided to search for the aforementioned files by typing find / -name along with the names of the folder and files one after the other but the response i got was not found. i also tried to root it by typing su but i got the same responsewhat am i doing wrong? i have attached an image of what i did",
    "present_kp": [
      "shell"
    ],
    "absent_kp": [
      "filenames"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "perl script to match case law references. i am very new to perl and decided to work on a simple script that could solve a problem i encounter in my day to day work. the purpose of the code that follows is to search over a body of text and extract english case law publication references. there are quite a few flavours of reference style, so i have several expressions looking for different reference structures. i'm very much at the hooray, i've managed to make it work-stage on the perl learning curve, but know enough to recognise that this code is pretty hideous. there are two main areas i'm aiming to improve:getting the source text from a file, rather than plonking itdirectly into the code. getting the output to write to a output.txt file.an examples of obvious poor practice is that i have used strict or warnings. the interpreter didn't like various aspects of the code. i'd be really grateful for some feedback on this very modest first attempt to use perl in an applied way. it is pretty hideous and i'm sure there are endless ways in which this could be achieved more elegantly.the code#!/usr/bin/perl# paste text to review below $search_text = <<eod; in salomon v a salomon and co ltd [1897] ac 22, the house of lords held that these principles applied as much to a company that was wholly owned and controlled by one man as to any other company. in macaura v northern assurance co ltd [1925] ac 619, the house of lords held that the sole owner and controller of a company did not even have an insurable interest in property of the company, although economically he was liable to suffer by its destruction. lord buckmaster, at pp 626-627 said:eodprint ----------------------------- ;print case references found in text;print ----------------------------- ;# find ncitsprint ***** ncits... ;while ($search_text =~ m/((\\(|\\[)\\d{4}(\\)|\\]))(\\s+((ewhc)|(ewhc\\s+admin)|(cat)|(ewca)|(ewca\\s+civ)|(ewca\\s+crim)|(ewcop)|(ewfc)|(ewfc\\s+b)|(ewpcc)|(ukhl)|(ukiat)|(ukpc)|(uksc)|(csoh)|(csih)|(nica)|(iesc)|(iecca)|(ieca)|(iehc)|(ukut))\\s+\\d+)/ig) { print $1$4 ;}# find wlrprint ***** wlr references... ; while ($search_text =~ m/((|\\[)\\d{4}(\\)|\\]))(\\s+\\d+\\s+((wlr))\\s+\\d+)/ig) { print $1$4 ;}# find ac with vol numberprint ***** appeal cases references... ; while ($search_text =~ m/((|\\[)\\d{4}(\\)|\\]))(\\s+\\d+\\s+((ac))\\s+\\d+)/ig) { print $1$4 ;}# find ac with no vol number while ($search_text =~ m/((|\\[)\\d{4}(\\)|\\]))(\\s+((ac))\\s+\\d+)/ig) { print $1$4 ;}# find qb with vol numberprint ***** queen's bench cases cases... ; while ($search_text =~ m/((|\\[)\\d{4}(\\)|\\]))(\\s+\\d+\\s+((qb))\\s+\\d+)/ig) { print $1$4 ;}# find qb with no vol numberwhile ($search_text =~ m/((|\\[)\\d{4}(\\)|\\]))(\\s+((qb))\\s+\\d+)/ig) { print $1$4 ;}print ***** external references... ;while ($search_text =~ m/((\\(|\\[)\\d{4}(\\)|\\])){0,1}(\\s*\\d+\\s+((tlr)|(tlr\\s+\\(pt\\s+1\\))|(tlr\\s+\\(pt\\s+2\\))|(lgr)|(cr\\s*app\\s*r)|(cr\\s*app\\s*r\\s*\\(s\\))|(ll\\s*l\\s*rep)|(lllr)|(tc)|(flr)|(bclc))\\s+\\d+)/ig) { print $1$4 ;}# find niqbprint ***** ni queen's bench cases cases... ;while ($search_text =~ m/((|\\[)\\d{4}(\\)|\\]))(\\s+((niqb))\\s+\\d+)/ig) { print $1$4 ;}# find nzlrprint ***** nzlr references... ; while ($search_text =~ m/((|\\[)\\d{4}(\\)|\\]))(\\s+\\d+\\s+((nzlr))\\s+\\d+)/ig) { print $1$4 ;}# find all erwhile ($search_text =~ m/((|\\[)\\d{4}(\\)|\\]))(\\s+\\d+\\s+((all er))\\s+\\d+)/ig) { print $1$4 ;}# find ecrprint ***** ecr references... ;while ($search_text =~ m/((|\\[)\\d{4}(\\)|\\]))(\\s+((ecr))\\s+\\d+)/ig) { print $1$4 ;}# old style iclr refsprint ***** older volume references... ;while ($search_text =~ m/(\\d+\\s+((app\\s*cas)|(ch\\s*d)|(cpd)|(ex\\s*d)|(p.d.)|(q.b.d.))\\s+\\d+)/ig) { print $1$4 ;}",
    "present_kp": [
      "perl"
    ],
    "absent_kp": [
      "beginner",
      "regex"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "finding minimum array integer. i'm trying to solve one simple task and on first look it's working.any advice on how i can optimize this code will be appreciated.a quick description of the task would be:you have a file from which you read the integers.the first line has two values:the array widththe number of test casesthe second line has actual array values.after that, you have \\$n\\$ rows all containing the array range which should be checked.\\$n =\\$ the number of test cases.here's a link to my code<?php$handle = fopen(php://stdin, r);$getfirstrow = fgets($handle);$firstrow = explode( , $getfirstrow);$firstrow1 = intval($firstrow[0]);$firstrow2 = intval($firstrow[1]);$getstring = fgets($handle);for ($z=0;$z<$firstrow2;$z++){$getrange = fgets($handle);$getragearr = explode( , $getrange);$i=intval($getragearr[0]);$j=intval($getragearr[1]);$arrresult = array();$lane = explode( , $getstring);for ($p=$i;$p<=$j;$p++){array_push($arrresult, $lane[$p]);}$result = min($arrresult);echo $result.php_eol;}",
    "present_kp": [
      "php"
    ],
    "absent_kp": [
      "algorithm",
      "programming challenge"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "change/edit text of whiptail gauge. i know that i can change the progress of a whiptail --gauge using something like:{ for ((i = 0 ; i <= 100 ; i+=20)); do sleep 1 echo $i done} | whiptail --gauge please wait while installing 6 60 0but i am wondering whether it is possible to edit/modify the text of the whiptail box (so change the please wait while installing text to something else.my current solution is to bring up a new whiptail box, but there is a noticeable flicker between the old one closing and the new one opening. if you can't update the text of a whiptail box, is it possible to reduce/remove this flicker instead?",
    "present_kp": [
      "whiptail"
    ],
    "absent_kp": [
      "bash",
      "progress information"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "pick algorithm with runtime in o(n) vs. (n) vs. (\\log n ). you are given three algorithms, $a$, $b$, and $c$ with the following time complexities in the worst case $o(n)$, $\\theta(n)$, and $\\omega(\\log n )$, respectively.assume that you have to choose exactly one algorithm, what would you choose as the best algorithm? explain your answer.i think i cannot choose $c$ because i cannot guarantee that the complexity is very large, i.e., $2^n$.i would choose $a$ and not $b$. why? well $a$ could have a very low complexity since its complexity is $o(n)$ so complexity of $a$ is less than $c\\cdot n$. but, $b$ is always bounded from above and from below by $c\\cdot n$.is my answer correct?",
    "present_kp": [],
    "absent_kp": [
      "algorithm analysis",
      "asymptotics",
      "runtime analysis"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "removing extensions in a column. i have a file like thisilmn_1343291 tgtgttgagagcttctcagactatccacctttgggtcgctttgctgttcg nm_001402.5ilmn_1343295 cttcaacagcgacacccactcctccacctttgacgctggggctggcattg nm_002046.3ilmn_1651209 tcacggcgtacgccctcatggggaaaatctccccggtgactttcaggtcc nm_182838.1i want to remove the numeric extensions from the end in the 3rd column so that my output file looks like thisilmn_1343291 tgtgttgagagcttctcagactatccacctttgggtcgctttgctgttcg nm_001402ilmn_1343295 cttcaacagcgacacccactcctccacctttgacgctggggctggcattg nm_002046ilmn_1651209 tcacggcgtacgccctcatggggaaaatctccccggtgactttcaggtcc nm_182838how can i do it on command line preferably using awk? i can do this in perl but i am pretty sure there is a single command line to do it.",
    "present_kp": [
      "awk"
    ],
    "absent_kp": [
      "text processing",
      "sed"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "take the value of another column if the query column is between two column values. so i have a file with column that has information on salary (eg. 2674 dollars) -indi salary sam 2674 john 6375 max 9547and another file with the scale of the salary (column1&colum2) on its corresponding rank (column3)salary_min salary_max rank2000 4000 deputy4000 6000 secretary6000 8000 assistant8000 10000 managernow i would like to assign those individuals from file 1 based on the salary to their corresponding rankso the final output would be indi rank sam deputy john assistant max managerhow can i achieve this in linux on the same dataset type but with larger data?",
    "present_kp": [],
    "absent_kp": [
      "text processing",
      "numeric data"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is there an isomorphism between (subset of) category theory and relational algebra?. it comes from big data perspective. basically, many frameworks (like apache spark) compensate lack of relational operations by providing functor/monad-like interfaces and there is a similar movement towards cats-to-sql conversions (slick in scala). for instance, we need natural join (assuming no repetitions on indexes) for element-wise multiplication of vectors from sql-perspective, which could be considered as zip + map(multiply) (spark's mlib, however, already has elementwiseproduct) in category theory's applications. simply saying (following examples are in scala):the referenced subcase of join can be thought as applicative functor (over sorted collection), which in its turn gives us zip: list(1,2,3).ap(list(2,4,8).map(a => (b: int) => a * b)) --> (list(1,2,3) zip list(2,4,8)).map(x => x._1 * x._2). moreover, we can induce it to some other joins, assuming some preprocessing (groupby operator or just surjection, or generally - an epimorphism).other joins and selection can be thought as monad. for instance, where is just: list(1,2,2,4).flatmap(x => if (x < 3) list(x) else list.empty) --> list(1,2,2,4).filter(_ < 3)data itself is just adt (gadt too?), which in its turn looks like a simple set-category (or more generally speaking - cartesian-closed), so it should (i suppose) cover set-based operations (due to curry-howard-lambek itself) and also operations like rename (at least in practice).aggregation corresponds to fold/reduce (catamorphism)so, what i'm asking is can we build an isomorphism between (maybe subset of) category theory and (the whole) relational algebra or is there something uncovered? if it works, what exact subset of categories is isomorphic to relalgebra?you can see that my own assumptions are quite broad, but formal solutions like curry-howard-lambek correspondence for logic-cats-lambda are more precise, so actually, i'm asking for a reference to an accomplished study (that shows a direct relationship) with more examples in scala/haskell to better understand how to approach bigdata itself.",
    "present_kp": [
      "relational algebra",
      "category theory"
    ],
    "absent_kp": [
      "curry howard"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "debug mkfs.ext3 command output. i was creating a new file system in my external hdd. while formatting, i had to format this partition to the remaining available partition which is somewhere around 850gb. now, i created an ext3 file system in this partition. this is the output of my mkfs.ext3 command. mkfs.ext3 /dev/sdb3mke2fs 1.41.3 (12-oct-2008)filesystem label=os type: linuxblock size=4096 (log=2)fragment size=4096 (log=2)52060160 inodes, 208234530 blocks10411726 blocks (5.00%) reserved for the super userfirst data block=0maximum filesystem blocks=42949672966355 block groups32768 blocks per group, 32768 fragments per group8192 inodes per groupsuperblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, <phone>, <phone>, <phone>, <phone>, <phone>, 20480000, 23887872, 71663616, 78675968, 102400000writing inode tables: done creating journal (32768 blocks): donewriting superblocks and filesystem accounting information: donecan someone help me debug the information as am not clear on what these values actually represent?",
    "present_kp": [
      "ext3",
      "mkfs"
    ],
    "absent_kp": [
      "filesystems"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what does actually reasonable mean when we say reasonable model of computation?. i have seen in many text when the author says reasonable model of computation. what does it really mean?",
    "present_kp": [],
    "absent_kp": [
      "turing machines"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "entity component system in c++. i've written an entity component system using c++ for my game engine. i'm still inexperienced so i've probably made a lot of mistakes. thus i've decided to ask here for an honest review.the complete code is fairly large (although not too large imho) so it wouldn't quite fit here. still if anyone here wishes to browse through it and give me some advice you can find the code on github.however the point of this topic was to present a few dubious design choices and code parts which i'm not completely sure about.design:there are many ecs designs but i've chosen the one which was the easiest to grasp logically. current design is based on entities being component containers, and systems being component processors. basically entity contains a unique id, a bitset which represents a key for systems (with each bit representing a component type) and a map of components. components on the other hand contain only data, no methods besides constructors. and finally systems contain a bitset which represents a lock where each bit represents a component which are registered(subscribed might be a better word) to the system. now while i really like this design because it's easy to understand, it does contain a double nested loop, and if a component contains a collection then it leads to a triple nested loop. i'm worried that for a game which requires fast updates this might cause performance issues. should i redesign the system? what would be a better design?code:there are many things i'm worried about in regards to code. first is that i've got a nested for -> if -> for -> if loop. which, in my opinion, is an essence of bad code. and also the fact that it's also copied code (used twice).void world::update(){ for( auto& ecsystem : m_systems ) { if( ecsystem.second->enabled() ) //if system is enabled { std::bitset< 64 >& lockbits = ecsystem.second->getlockbits(); for( auto& entity : m_entities ) { if( ( entity->getkeybits() & lockbits ) == lockbits ) { ecsystem.second->update( entity->getrelevantcomponents( lockbits ) ); } } } }}void world::draw(){ for( auto& ecsystem : m_systems ) { if( ecsystem.second->enabled() ) { std::bitset< 64 >& lockbits = ecsystem.second->getlockbits(); for( auto& entity : m_entities ) { if( (entity->getkeybits() & lockbits) == lockbits ) { ecsystem.second->draw( entity->getrelevantcomponents( lockbits ) ); } } } }}both methods (update() and draw()) must have 0 parameters (though i'm planning on adding a time step parameter to update method later on) and i thought that i might somehow create a single method which iterates over all systems and entities. but since the end call to each systems update or draw method is made from within the system itself, i can't think of a way to make the iteration more ambiguous. second is that getrelevantcomponents call. i've got a separate class called componentprovider which provides relevant components to users of the system. basically to make sure that whoever is creating new systems can't use components which aren't registered to that system. the way getrelevantcomponents( bitset<64> lockbits ) works is that it creates an instance of componentprovider class, then adds the pointers of entity's relevant components and then returns that componentprovider instance.componentprovider entity::getrelevantcomponents( std::bitset< 64 >& lockbits ){ componentprovider provider; for( auto& mapitem : m_componentmap ) { auto key = mapitem.first; if( lockbits.test( key ) ) { provider.m_relevantcomponentmap.insert( std::make_pair( key, mapitem.second) ); } } return provider;}now what worries me the most in this code is that i'm returning an object instanced in the method itself. the result of the method call is thrown as a parameter to another method. i thought that i should make an rvalue assignment operator overload so that the values won't be copied but instead swapped, however i've been told that i should use the copy-swap idiom instead and leave it to rvo to handle the optimizations. is that really a good idea?third is the fact that i'm using shared pointers a lot. i'm afraid of shared pointer overhead as well as slowdowns since new shared pointers are being created and destroyed every update cycle. should i use raw pointers instead and just be very careful of memory leaks?fourth is that i'm using a map to contain all systems since i need to be able to remove specific systems and not allow adding more systems of the same type, but i also need to iterate over all systems each update cycle so i'm worried if it was really a good idea to use a map( instead of an unordered map since as far as i know normal map is contiguous ) instead of a vector. i've also read that unordered_map is faster than normal map in every case so i'm not sure at what i should use really.those are my largest worries right now. please let me know if my post is hard to understand and i'll try to explain things better.",
    "present_kp": [
      "c++",
      "entity component system"
    ],
    "absent_kp": [
      "c++11"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "meaning of (meta)heuristic methods. for optimization, from wikipedia:in computer science, metaheuristic designates a computational method that optimizes a problem by iteratively trying to improve a candidate solution with regard to a given measure of quality. metaheuristics make few or no assumptions about the problem being optimized and can search very large spaces of candidate solutions. however, metaheuristics do not guarantee an optimal solution is ever found. many metaheuristics implement some form of stochastic optimization.other terms having a similar meaning as metaheuristic, are: derivative-free, direct search, black-box, or indeed just heuristic optimizer. several books and survey papers have been published on the subject.i wonder how to tell whether an optimization method is metaheuristic or not? for example, (1) is the simplex method for linearprogramming metaheuristic? (2) are the majority of nonlinearprogramming methods such as gradient descent, lagrangian multipliermethod, penalty methods, interior point methods (barrier methods),metaheuristic? (3) are all gradient-free methods, such as neldermead method or downhill simplex method, metaheuristic?what are some optimization methods that are not metaheuristic?more generally (going beyond optimization) for problem solvingtechniques, from wikipedia:heuristic refers to experience-based techniques for problem solving, learning, and discovery. where an exhaustive search is impractical, heuristic methods are used to speed up the process of finding a satisfactory solution. examples of this method include using a rule of thumb, an educated guess, an intuitive judgment, or common sense.in more precise terms, heuristics are strategies using readily accessible, though loosely applicable, information to control problem solving in human beings and machines.i wonder how to understand the meaning of heuristic?how can i tell whether a problem solving,learning, and discovery technique is heuristic or not? what are some problem solving,learning, and discovery techniques that are not heuristic?thanks and regards!",
    "present_kp": [
      "optimization",
      "heuristics"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "running commands on user login?. first before anything i have installed a ubuntu 16.10after i installed it i have a tty1 open. i have installed xorg and xfwm4 (x window manager)if i do following:tty1: logintype: startxtty2: logintype: export display=:0type: chromium-browsertty1: (chromium browser will be there andopen)how can i automate such a thing without doing it manually every time? it should happen after login, but only if user login is happened in tty1. i don't want reserve the other ttys.",
    "present_kp": [
      "ubuntu",
      "tty",
      "display",
      "window"
    ],
    "absent_kp": [
      "linux"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "mei 0000:00:16.0: init hw failure. when i boot up i get the following errors: mei 0000:00:16.0: init hw failure. mei 0000:00:16.0: initialization failed.(they don't prevent the bootup, i'm just curious as to what they are and if i shouldbe worried).i googled of course, but i only found reference to supermicro motherboards on servers. i have simply a home laptop (lenovo thinkpad s540) and my motherboard vendor simply shows as lenovo.more googling lead me to believe it's something to do with intel vpro technology, an interface that uses the intel me hardware features to enable an interaction between high- and low-level hardware systems in a system. with this intel feature, the administrators can now handle the tasks without intervention of the human beings.according to /sys/devices/pci0000:00/0000:00:16.0, vendor is 0x8086 and device is 0x9c3a, which google translate as 0x9c3a intel management engine interface driver and 0x8086 intel corporation.could anyone shed more light on mei , and if possible what this error is regarding?",
    "present_kp": [
      "boot"
    ],
    "absent_kp": [
      "ubuntu"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why does it take up to several minutes to clean a listening tcp port after a program dies?. if i kill a program that is listening on a tcp port, it takes up to several minutes until the port is reclaimed by the system and usable again. i've seen several q/a mentioning this phenomenon, but without an explanation. why does that happen, why doesn't the system reclaim the port right away? does it also happen on another systems, such as windows or mac?",
    "present_kp": [
      "tcp"
    ],
    "absent_kp": [
      "networking",
      "resources"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "rsync --delete did not remove file in destination while with ssh. on my rhel6 pair of servers, i tested the rsync from server-1 to server-2 with ssh. everything was good as expected. however, when i moved a file under a directory, a/x.file to b/ of /home/user on server-1, repeated the same rsync with --delete (or similar options) did not remove x.file on a/ and backup x.file to b/ on server-2. here is the rsync script: rsync -avhu -e ssh --delete home/user/ remote_user@remote_host:/home/user/. ssh was set up without passphrase. i tried different order of those options, none of them work, meaning that x.file was still under a/ at destination (server-2), not on b/. what went wrong?",
    "present_kp": [
      "ssh",
      "rhel",
      "rsync"
    ],
    "absent_kp": [
      "centos"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "encapsulating business logic that goes beyond validations in mvc. i'm pretty clear business logic goes into models. but on this question, they refer to business logic as pretty much low level validation (if the user signs up, ask for their e-mail).what about business logic that goes beyond validation? for example, right now i'm working on a project where the user is able to generate a quotation after a form submission. once submitted, the application will process several data (that are not related to said form) and create some more database rows. this process involves 3 different model entities, which clearly indicates placing the code for this process inside any of those model entity classes would be wrong. right now i'm using unit of work pattern to encapsulate crud operations (database and mock for testing), but i have somewhere read about some called service pattern, that encapsulates said unitofwork and possibly processes like described above.what's the pattern used for this kind of logic?",
    "present_kp": [
      "mvc",
      "business logic"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "removing null / empty fields. just wanted to check with you could this be done better:awk -f '{ for (i = 1; i <= nf; i++) { if ($i != null) { printf(%s%s, $i, fs); } } printf( );}' file1the goal is to print only non-null fields. for example:echo testrecord001 null null age 29 null null name john | awk -f '{ for (i = 1; i <= nf; i++) { if ($i != null) { printf(%s%s, $i, fs); } } printf( );}'will print out: testrecord001 age 29 name john",
    "present_kp": [
      "null",
      "awk"
    ],
    "absent_kp": [
      "shell",
      "unix"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "start several fish shells in different directories?. in order to be able to launch my work environment with a single command, i want to start several urxvt windows running fish shell in different folders. however, i found no obvious way of having fish run a startup command (e.g. a cd) and not exit afterwards. has anyone figured out how to do start fish in a particular directory without making it a default in config.fish?",
    "present_kp": [
      "shell",
      "fish"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "web app to convert images to web friendly formats of .gif or.png. i am looking for a web app in which i can upload my images and convert them to web-friendly versions such as gif/png? something similar to when you upload photos to facebook and they are automatically converted into gifs.",
    "present_kp": [
      "photos"
    ],
    "absent_kp": [
      "webapp rec"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "ios external dac. i'm thinking about purchasing an external dac for my iphone to improve volume and quality of music i listen to when out and about.i understand when i upgrade from the 6 to the iphone 7, i will be forced to use an external dac if i want to keep the same headphones i'm using now? betron b750s.any dac recommendations? i don't want something which drains the iphones battery.been looking at dragonfly black/reds, anyone using one of these?",
    "present_kp": [],
    "absent_kp": [
      "smartphones"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "validating user supplied input. i was assigned a task to fix the sql injection flaw reported by a code analysis tool. i am not the original author of the code. i am fairly knowledgeable with sql.public int executenonquery(string query, sqlparameter[] parameters){ using (sqlcommand command = createcommand(query, parameters)) { int rowsaffected = command.executenonquery(); log.debug(rows affected: {0}, rowsaffected); return rowsaffected; }}and the tool has reported the injection flaw at the line number 3. the executenonquery is being called from so many places.here is one such sample:public void adddatafrom(iworkflowstepexecutionstorage storage, bool inversesourceandtargetside){ workflowstepexecutionstorage storageimpl = storage as workflowstepexecutionstorage; if (storageimpl == null) { throw new invalidoperationexception(); } const string querytemplate = @insert into [$mappingdictionarytable](sourceobjectid, sourceobject, targetobjectid, targetobject, linkedobjecttypespairid) select sourcetable.objectid, sourcetable.systementry, targettable.objectid, targettable.systementry, @linkedobjecttypespairid from [$mappingtable] mt inner join [$sourceimportstoragetable] sourcetable on mt.$sourceidcolumn = sourcetable.id inner join [$targetimportstoragetable] targettable on mt.$targetidcolumn = targettable.id ; querybuilder querybuilder = new querybuilder(querytemplate); querybuilder.addparameter($mappingdictionarytable, this.mappingdictionarytable.tablename); querybuilder.addparameter($mappingtable, storageimpl.mappedtable.tablename); querybuilder.addparameter($sourceimportstoragetable, inversesourceandtargetside ? (storageimpl.targetimportstorage as importexecutionstorage).importtable.tablename : (storageimpl.sourceimportstorage as importexecutionstorage).importtable.tablename); querybuilder.addparameter($targetimportstoragetable, inversesourceandtargetside ? (storageimpl.sourceimportstorage as importexecutionstorage).importtable.tablename : (storageimpl.targetimportstorage as importexecutionstorage).importtable.tablename); querybuilder.addparameter($sourceidcolumn, inversesourceandtargetside ? mappedtable.targetidcolumn.name : mappedtable.sourceidcolumn.name); querybuilder.addparameter($targetidcolumn, inversesourceandtargetside ? mappedtable.sourceidcolumn.name : mappedtable.targetidcolumn.name); string query = querybuilder.getparametrizedquery(); sqlparameter[] sqlparams = new[] { new sqlparameter(@linkedobjecttypespairid, storageimpl.runconfiguration.linkedtypespairconfiguration.id) }; using (iqcdbconnection dbconnection = this.connectionprovider.createdbconnection()) { dbconnection.executenonquery(query, sqlparams); }}here is the createcommand code:private sqlcommand createcommand(string commandtext, sqlparameter[] parameters) { sqlcommand retval = this.connection.createcommand(); retval.commandtext = commandtext; retval.commandtimeout = this.commandstimeout; retval.parameters.addrange(parameters); return retval; }is there any injection risk in this code? if yes, how do i recognize it?",
    "present_kp": [
      "sql injection"
    ],
    "absent_kp": [
      "c#"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "reading data.data/nscr1000 files in macos. there is a great question/answer here on the topic: how to read nscr1000 data files?the missing link i can't quite figure out is how to use the information in the windows.plist to decrypt the data in the data.data file? i assume the nswindowid field (as outlined by cimarron in the answer to the previously mentioned question) allows us to look up a corresponding nswindowid in the windows.plist file, and that structure also has an nsdatakey property that seems to hold a key (in base64 maybe?). i also know from the previous answer that we're dealing with 128-bit aes encryption, and that several records are present in the data.data file and the data must be extracted into it's own file for decryption, but no matter how i try to use the key i can't get decryption to work.how does one use all this information to decrypt?",
    "present_kp": [
      "decryption"
    ],
    "absent_kp": [
      "osx"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "upgrade openssl from 0.9.8w to 1.0.0i or 1.0.1a on linux server. i want to upgrade openssl from 0.9.8w to 1.0.0i or 1.0.1a.i am usingos.name linux os.version 2.6.18-164.9.1.el5 any pointers or links would be very helpful.thanks in advance.",
    "present_kp": [
      "linux",
      "openssl"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "what is alternative of initlog (present in rhel5) but deprecated in rhel6?. i am using initlog tool in a cron file to write the output of ntpq -p command in syslog..but initlog command has been deprecated in rhel6.now, i want know the alternative of initlog command in rhel6.",
    "present_kp": [
      "rhel",
      "cron"
    ],
    "absent_kp": [
      "linux",
      "shell"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "impractical problems in p. possible duplicate:polynomial-time algorithms with huge exponent/constant in many texts you find statements like 'the class p characterizes the problems that are efficiently solvable. even though $n^{100}$ algorithms are in p, these don't occur in practice'.today i came across a $n^{120}$ algorithm for a problem where, apparently, no better algorithm is known: recognizing map graphs reference.then there are algorithms with huge constants like the graph minor algorithm.do you know of any other problems where the best known algorithm is polynomial, but terribly impractical?",
    "present_kp": [],
    "absent_kp": [
      "ds.algorithms",
      "reference request",
      "big list"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "which are the most accredited tests for measuring personality traits?. i'm lacking of an academic background. i thought there were only 4 or 16 personality traits. but a fast search on google is showing a huge number: according to this link they should be 638.is this information correct? which are the most accredited tests for measuring personality traits?",
    "present_kp": [
      "personality",
      "test"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "timers to use various task(s). i am developing a c# program, which performs various tasks in parallel threads. i created a base task and let all tasks derived from this class execute. i just want to know if this a viable approach which won't cause unforeseen consequences if various tasks run in conjunction.public abstract class basetask{ private readonly timer _tasktimer = new timer(); // derived classes to define thier own time intervals protected abstract double timerinterval { get; set; } public void start() { // set default timer -- 10 seconds timerinterval = 10000; _tasktimer.autoreset = true; _tasktimer.interval = timerinterval; _tasktimer.elapsed += executetask; _tasktimer.start(); } public void stop() { _tasktimer.stop(); } public abstract void executetask(object sender, elapsedeventargs e);}// derived task class class derivedtask : basetask { protected override double timerinterval { get { return 5000; } set { } } public override void executetask(object sender, elapsedeventargs e) { // do my derived task operations here } } // now in main.. static void main() { derivedtask dtask = new derivedtask (); _dtask .start(); }",
    "present_kp": [
      "c#",
      "timer"
    ],
    "absent_kp": [
      "multithreading"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "failure to connect debian nis client to opensuse 12.1 server. i'm trying to connect a debian 7.0 client to a opensuse 12.1 server. the nisdomainname is correct, the /etc/group, /etc/passwd and /etc/shadown all have the +::: lines, ypbind connects, but the authentication, both via ssh or directly on the machine, fails. the log /var/log/auth.log on the debian machine only shows that the authentication password failed. the nis server works fine with other opensuse machines.",
    "present_kp": [
      "debian",
      "opensuse",
      "nis"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "gnu gpl dual license with cc-by-nd?. in a project that i am working on, i would like to use the gnu gpl software license, but i also don't want to have any possible problems distributing my software.for instance, there were some reports that gnu gpl apps were removed from the apple app store because the gnu gpl is not consistent with their terms of distribution.in order to preempt such a possibility, i am planning to dual-license my code under two licensesgnu gpl v2, or later at your discretioncc-by-nd 4.0the thought process is, if some platform decides for some obscure legal reason, gpl software is not allowed, i can say fine, i will distribute this release of the code to you under cc-by-nd. but this also doesn't weaken my commitment to free-software, because if someone wants to fork the project, they have to use either gnu gpl, or retain the dual licensing. if they retain only cc-by-nd, then they can no longer make modifications.does this make sense? will this work the way i think it does? is there a simpler or better way? note that if this works, then i would prefer it to using mit / bsd license.do any other projects pursue a similar strategy? i don't know of one, and i'm not a lawyer.clarification based on comment discussion belowso, it's quite possible that i misunderstand what the issue was with apple and gpl. but i thought the issue was that, apple imposes terms like when you buy the app, you can only install it on one device, and this is considered to run afoul of gpl which says no downstream restriction. iiuc, gpl software can be sold -- they can decide to sell or not sell, and then they distribute or they don't. but when they decide to distribute, they can't put further restrictions on use of the software. iiuc, cc-by-nd allows any downstream restrictions, its just no one can make modifications.basically what i think that i want is, an escape hatch that allows people to impose downstream restrictions when they distribute, provided they give up the right to make further modifications. since i think that's basically still consistent with free software. it's possible that i shouldn't want that, i don't really know. but that's what makes sense to me at the moment.",
    "present_kp": [
      "gpl",
      "apple app store"
    ],
    "absent_kp": [
      "cc by nd"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "strange error message during centos 6.5 installation regarding disk containing bios raid metadata?. while attempting to use an old 40gb hdd to setup a kind of junky though usable athon 1ghz system i ran into the following error during the hdd identification/partitioning phase of the installation:warning: disk sda contains bios raid metadata, but is not part of any recognized bios raid sets. ignoring disk sdahow can i work around this issue?",
    "present_kp": [
      "centos",
      "raid"
    ],
    "absent_kp": [
      "system installation"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "distributed file system that works well with multiple small files. hi my use case is quite specific.i have 20 windows 7 machines constantly creating files in my storage; around 98% of these files are 2.1 mb. on average we create 24 million files every 3 days, and this number may increase in the near future as we may need to add new clients to our system.i do not modify files (just create, read, copy and delete).i have seen reiser4, which looks promising, but i also would like to have the capability to replicate the files across multiple storage nodes across the network, so i can have a fault tolerance system in place.any suggestion?",
    "present_kp": [],
    "absent_kp": [
      "filesystems",
      "distributed filesystem",
      "reiserfs"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to show that all 2-terminal sp-graphs are o(log n)-outer-planar. i am trying to show that every 2-terminal sp-graph is $o(\\log(n))$-outer-planar for a challenge question on my assignment. in particular, i am trying to prove this by induction on the number of combinations (either a series or a parallel combination). this is what i have so far.the base case on $0$ combinations is that we just have some edge $(s, t)$. clearly this is outer-planar and so $o(\\log(n))$-outer-planar as well (in this case, $\\log(2) = 1$, so this does hold).now we do the inductive step. given some 2-terminal outer-planar graph $g = (v, e)$, we will show that it is $o(\\log(n))$-outer-planar, where $|v| = n$. since $g$ is a 2-terminal outer-planar graph, it was either the result of combining two graphs $g_1$ on $n_1$ vertices and $g_2$ on $n_2$ vertices by series or by parallel. so we have two cases two consider.the first case is when $g_1$ and $g_2$ were combined by series. by induction, $g_1$ is $o(\\log(n_1))$-outer-planar and $g_2$ is $o(\\log(n_2))$-outer-planar. since $g$ was combined by series, it has $n = n_1 + n_2 - 1$ vertices, where $n_1, n_2 \\ge 2$. let $n_0 = \\max(n_1, n_2)$. then $g$ is $o(\\log(n_0))$-outer-planar since we have not added any layers to $g_1$ or $g_2$ by combining them. it follows that $g$ is $o(\\log(n))$-outer-planar since $n_0 \\le n$.in the second case, $g_1$ and $g_2$ are combined in parallel, meaning that $g$ has $n = n_1 + n_2 - 2$ vertices. again by induction, $g_1$ is $o(\\log(n_1))$-outer-planar and $g_2$ is $o(\\log(n_2))$-outer-planar. however, this time we cannot conclude that the number of layers around $g$ is the same.this is where i am stuck. i understand that we are at-most doubling the amount of vertices in $g$ by combining them in parallel, but i don't know how to use that to argue that the number of layers in $g$ is $o(\\log(n))$. any help would be appreciated since i've been stuck on this for several days now.edit: now that the deadline has passed for my assignment, i would love to have a solution so that i can prepare for my midterm exam (solutions to bonus/challenge problems are never posted).terminologyseries-parallel graphs (sp-graphs) are constructed in the following way. a two-terminal graph is a graph with two distinguished nodes $s,t$. a single edge connecting $s$ and $t$ is an sp-graph. if $(g_1,s_1,t_1)$ and $(g_2,s_2,t_2)$ are two sp-graphs, then their serial composition is an sp-graph, and their parallel composition is an sp-graph. the serial composition is obtained by identifying $t_1$ with $s_2$; the new terminals are $s_1$ and $t_2$. the parallel composition is obtained by identifying $s_1$ and $s_2$, and $t_1$ and $t_2$; the new terminals are $s_1=s_2$ and $t_1=t_2$. sp-graphs are the minimal class of graphs generated by these operations. see wikipedia for more on this class.$k$-outerplanar graphs: a planar embedding (i.e. an embedding without crossings) is $1$-outerplanar if all of the vertices belong to the unbounded face. for $k \\gt 1$, a planar embedding is said to be $k$-outerplanar if removing the vertices on the outer face results in a $(k 1)$-outerplanar embedding. a graph is $k$-outerplanar if it has a $k$-outerplanar embedding.",
    "present_kp": [],
    "absent_kp": [
      "graph theory"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "suurballe's algorithm: proof of correctness. i was reading about suurballe's algorithm on wikipedia, for the shortestedge-disjoint paths problem, i.e. given nodes $s$ and $t$ finding a pair of paths between these nodes, whose accumulated weight is minimal.i understand why the output consists of two $s$-$t$ paths, and the relationship between this and augmenting path flow algorithms on an intuitive level, however i fail to understand why the two paths the algorithm chooses necessarily minimize the weight.i would appreciate if somebody could explain why the algorithm is correct. i don't have access to suurballe's original paper and i can't follow the paper by suurballe and tarjan.",
    "present_kp": [
      "algorithms"
    ],
    "absent_kp": [
      "graphs",
      "shortest path",
      "correctness proof"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why is the available space in my $home decreasing?. my os is ubuntu 12.04. as system monitor shows, free for my $home is 1.6gib, but available is 222.7 mib and is keeping slowly decreasing from around 750 mib yesterday. i am not aware that i am adding more data to $home. so i would like to know how to find out the reason that causes that, e.g.,is there some running application that is using the space of $home as temporary storage place and therefore can be freed up?how to find out which directory or file is increasing its size?to ignacio~/.xsession-errors is only 21330862 bytes. some of its last content is > ** (zeitgeist-datahub:2116): warning **: recent-manager-provider.vala:133: desktop file for> file:///windows-d/academic%20discipline/study%20objects/areas/formal%20systems/logic/generalize%20to%20when%20inference%20is%20uncertain/uncertainlize%20deductive%20logic/statistics/general/kalbfleisch/vol2/all.djvu> was not found, exec: plugin-container, mime_type: image/vnd.djvu> > ** (zeitgeist-datahub:2116): warning **: recent-manager-provider.vala:133: desktop file for> file:///windows-d/academic%20discipline/study%20objects/areas/formal%20systems/logic/generalize%20to%20when%20inference%20is%20uncertain/uncertainlize%20deductive%20logic/statistics/general/montgomery/prob%20and%20stat%20in%20eng/4ed.pdf> was not found, exec: plugin-container, mime_type: application/pdfto karlson$ df -khfilesystem size used avail use% mounted on/dev/sda7 21g 15g 4.8g 76% /udev 949m 12k 949m 1% /devtmpfs 383m 988k 382m 1% /runnone 5.0m 0 5.0m 0% /run/locknone 956m 184k 956m 1% /run/shm/dev/sda2 71g 47g 24g 68% /windows-c/dev/sda3 110g 101g 9.9g 92% /windows-d/dev/sda6 27g 26g 223m 100% /home/dev/sda1 1.2g 658m 543m 55% /media/system_drv",
    "present_kp": [
      "ubuntu",
      "home"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "trying to use an old pc for home theater pc. i'm trying to use some fairly old hardware (purchased ~2000) to have hooked up to my tv full time and i'm looking for advice on which distro to use.hardware specs: pentium 4 1.9 ghz765 mb ramnv11 [geforce2 mx/mx 400] graphics cardmore available if needed, not sure what else to list thoughmachine requirements: capable of streaming music/video (a functional browser)easy installable to play out to a large screen (~30 inches)capable of playing dvd's/music from harddrive.extras that would be nice: ssh support, able to run as a lamp, able to run as a file server. based on my machine requirements and hardware specs, is this possible? which distro would you recommend? i would ideally use ubuntu server+gui but i've been having trouble with the install and i think it may be due to the aging hardware.",
    "present_kp": [],
    "absent_kp": [
      "lightweight"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is remembering a list of words an example of semantic or episodic memory?. i'm having a hard time making a clear distinction between semantic and episodic memory. when presented with idealized examples, i find the breakdown clear; describing a trip to paris is an example of episodic memory, describing the structure of a cell is an example of semantic memory.however, when it comes to experimental settings, things suddenly start to look more murky. for example, say that one is presented 20 words in a sequential fashion and then is asked to repeat as many of these as possible. are we dealing with episodic memory then? does it make any difference if one is forced to recollect them in the same order as they appeared? many researchers seem to consider this situation an example of episodic memory (just as an example, see van der helm et al., 2011), and indeed, the man who invented the concept of episodic memory also did so at the time of its conception (tulving, 1972; p.390).however, later, tulving has revised his original definition. in a more recent article (tulving, 2002) he writes: i had been wrong in 1972 when i had assumed that the traditional, ebbinghaus-inspired, study/test laboratory experiments of verbal learning and memory had dealt with episodic memory. they had not. two important features of episodic memory were missing.one had to do with the contents of what the subjects in the experiments had to learn. episodic memory is about happenings in particular places at particular times, or about what, where, and when [...] traditional laboratory experiments, however, were almost invariably concerned with what. subjects are asked, what do you remember of the presented material? they report their knowledge in tests such as free recall, cued recall, or recognition. subjects memory for where and when was hardly ever examined.the other missing feature was what i referred to in elements as recollective experience, or conscious awareness of what had happened in the past. in traditional experiments the experimenter assumes that the overt behavioral response reflects the subjects mental state; that is, that behavior is a faithful index of cognition. the reasoning goes something like this: surely, if the subject recognizes an item in a recognition test, it means that he remembers it from the list, that is, that he has a conscious recollection of the items occurrence in the study list. how could it possibly be otherwise?as subsequent history showed, it could be otherwise. research on implicit memory [...], or so-called nonconscious memory [...], has overwhelmingly proved that one and the same behavioral response in a study/test experiment could represent conscious awareness of the retrieved items experimental history as readily as it could represent total lack of such awareness.so which one is it? are there any large disagreements within the community of what episodic memory actually is? are word list tests seen as something that at least captures a small part of episodic memory? if remembering a list of words isn't an example of episodic memory, then what is it an example of?references:van der helm, e.; gujar, n.; nishida, m; walker, m.p. (2011).sleep-dependent facilitation of episodic memory details. plos one, 6(11).tulving, e. (1972). episodic and semantic memory. in e. tulving and w. donaldson (eds.), organization of memory (pp. 381-402). new york: academic press.tulving, e. (2002). episodic memory: from mind to brain. annual review of psychology, 53, 1-25.",
    "present_kp": [
      "memory",
      "episodic memory",
      "semantic memory"
    ],
    "absent_kp": [
      "long term memory"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is the minimal number of states for the dfa?. let the regular expression $r = ((a^*\\cup \\emptyset \\cup arepsilon^*)^*b)^*$ above $\\sigma = \\{a,b,c,d\\}$. what is the minimal number of states for a dfa accepting this regex?$1$$2$$4$$5$ or moreso for my understanding, this regex is equivalent to $(a^*b)^*$. i was able to build the following dfa, nevertheless, i know the answer is $2$. how is it possible?",
    "present_kp": [],
    "absent_kp": [
      "regular languages",
      "automata",
      "finite automata",
      "regular expressions"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "pycrypto aes-256 ctr wrapper secure for public use?. i have tried to write a more user-friendly aes ctr wrapper with pycrypto, but i'm not sure if it's safe enough. i mean, by default there is iv=1 for ctr and in documentation it's said that iv is ignored anyway, so i'm not sure if i should use it or not (or if it's even necessary rather than overkill).this code should be for files on a local disk, nothing to do with networking. the code saves initial part of a counter i.e. secret.secret to a separate file together with the encrypted one (two files). counter (or rather parts of it) in the script is incremented always by 1 and if it comes to 255 (which seems to throw some error), then it resets to 0 and starts again, so for each block for file stream there should be a unique counter (go print it in the while loop), therefore each block should be safe enough if only user knows the key and the counter incrementing isn't something obvious as +1, but rather something more complex such as working with % and remainders.is my code safe to use in public? of course this is only an example, counter incrementing is much more complex than +1.is my code still safe if i reveal how counter is incrementing and someone has access to .ctr file if he/she doesn't know the key(32)?note: the key(32) is generated with user's input combined with a machine's constant + salt and other stuff. user has to input password for each usage of the code and user's input isn't stored anywhere.import osimport arrayfrom crypto.cipher import aesclass secret(object): def __init__(self, secret=none): if secret is none: secret = os.urandom(16) self.secret = secret self.reset() def counter(self): for i, c in enumerate(self.current): if c + 1 == 255: self.current[i] = 0 else: self.current[i] = c + 1 return self.current.tostring() def reset(self): self.current = array.array('b', self.secret)class vial(object): def __init__(self, key): self.key = key def encrypt(self, text, counter_path): secret = secret() with open(counter_path, 'wb') as f: f.write(secret.secret) crypto = aes.new(self.key, aes.mode_ctr, counter=secret.counter) encrypted = crypto.encrypt(text) return encrypted def decrypt(self, text, counter_path): with open(counter_path, 'rb') as f: load_secret = f.read() secret = secret(load_secret) crypto = aes.new(self.key, aes.mode_ctr, counter=secret.counter) decrypted = crypto.decrypt(text) return decrypted def encrypt_stream(self, input, output): secret = secret() counter_path = os.path.splitext(output.name)[0] + '.ctr' with open(counter_path, 'wb') as f: f.write(secret.secret) crypto = aes.new(self.key, aes.mode_ctr, counter=secret.counter) while true: data = input.read(4096) if not data: break data = crypto.encrypt(data) output.write(data) def decrypt_stream(self, input, output): counter_path = os.path.splitext(input.name)[0] + '.ctr' with open(counter_path, 'rb') as f: counter_read = f.read() secret = secret(counter_read) crypto = aes.new(self.key, aes.mode_ctr, counter=secret.counter) while true: data = input.read(4096) if not data: break data = crypto.decrypt(data) output.write(data)if __name__ == '__main__': key32 = '<phone>' * 3 + 'qw' vial = vial(key32) path = os.path.abspath(os.path.dirname(__file__))+'/vial_test.ctr' enc = vial.encrypt(16 * 'a', path) print 'enc: ', enc, len(enc) vial = vial(key32) path = os.path.abspath(os.path.dirname(__file__))+'/vial_test.ctr' dec = vial.decrypt(enc, path) print 'dec: ', dec, len(dec) vial = vial(key32) finput = open('encrypt_me.png', 'rb') foutput = open('im_encrypted.png', 'wb') vial.encrypt_stream(finput, foutput) finput.close() foutput.close() vial = vial(key32) finput = open('im_encrypted.png', 'rb') foutput = open('im_decrypted.png', 'wb') vial.decrypt_stream(finput, foutput) finput.close() foutput.close()",
    "present_kp": [
      "aes"
    ],
    "absent_kp": [
      "python",
      "cryptography"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "do all linux distributions use the same cryptographic hash function?. do all linux distributions use the same cryptographic hash function?if yes, is it provided with the kernel itself?edit:- i refer to the function mainly used for storing user login passwords.",
    "present_kp": [
      "password"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to echo an escaped string. how can i echo an escaped string that contains $ in bourne shell?user@server:~$ cat test.sh #!/bin/shecho $1user@server:~$ ./test.sh \\$sad\\$test$sad$testi want it to return an escaped version like this:user@server:~$ ./test.sh \\$sad\\$test\\$sad\\$testi have tried doing tricks with sed and awk but no luck. any guidance would be greatly appreciated.",
    "present_kp": [
      "shell",
      "sed",
      "awk"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "neural networks : can i use both sigmoid and tanh as activation functions?. in a neural network architecture can i use the sigmoid function in some layers and the tanh function in the others? is it a good choice?",
    "present_kp": [
      "neural network"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "chmod 600 fails silently. here is what happens:$ chmod 600 foobar.txt$ ls -ltotal 1-rwx------ 0 sampablokuper sampablokuper 13 feb 19 21:00 foobar.txtwhy is the last line not reading as follows?-rw------- 0 sampablokuper sampablokuper 13 feb 19 21:00 foobar.txtn.b. this is occurring on a server of which i am not the sysadmin. the server is running linux kernel 3.8.0-33-generic under the following os:$ cat /etc/*-releasedistrib_id=ubuntudistrib_release=12.04distrib_codename=precisedistrib_description=ubuntu 12.04.3 ltsmcs linux 2013/2014 (x86_64)version = 2013name=ubuntuversion=12.04.3 lts, precise pangolinid=ubuntuid_like=debianpretty_name=ubuntu precise (12.04.3 lts)version_id=12.04",
    "present_kp": [
      "linux",
      "ubuntu",
      "chmod"
    ],
    "absent_kp": [
      "permissions"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "confused about udevadm usage. the ultimate goal here is to turn on/off touchpad on mouse plug, soi'm trying to get some property of my mouse and my touchpad from udev database, using udevadm but i don't get how this working and unfortunately the manpage isn't clear enough to me$ lsb_release -ircdistributor id: debianrelease: 8.4codename: jessiehere is the kind of information i'm looking for :kernel==input16 subsystem==input driver== attr{name}==bluetooth laser travel mouse attr{phys}==5c:e0:c5:9d:63:fd attr{uniq}==00:07:61:ec:be:5c attr{properties}==0from here i have tried this :$ udevadm info -a /sys/devices/pci0000\\:00/0000\\:00\\:1c.3/0000\\:03\\:00.0/usb2/2-1/2-1\\:1.0/0003\\:1ea7\\:<phone>/input/input25/mouse1/'and i'm getting thisunknown device, absolute path in /dev/ or /sys expected.if i monitor, i get this result:$ udevadm monitor -k -s inputmonitor will print the received events for:kernel - the kernel ueventkernel[<phone>] remove /devices/pci0000:00/0000:00:1c.3/0000:03:00.0/usb2/2-1/2-1:1.0/0003:1ea7:<phone>/input/input25/mouse1 (input)kernel[<phone>] remove /devices/pci0000:00/0000:00:1c.3/0000:03:00.0/usb2/2-1/2-1:1.0/0003:1ea7:<phone>/input/input25/event11 (input)kernel[<phone>] remove /devices/pci0000:00/0000:00:1c.3/0000:03:00.0/usb2/2-1/2-1:1.0/0003:1ea7:<phone>/input/input25 (input)kernel[<phone>] add /devices/pci0000:00/0000:00:1c.3/0000:03:00.0/usb2/2-1/2-1:1.0/0003:1ea7:<phone>/input/input26 (input)kernel[<phone>] add /devices/pci0000:00/0000:00:1c.3/0000:03:00.0/usb2/2-1/2-1:1.0/0003:1ea7:<phone>/input/input26/mouse1 (input)kernel[<phone>] add /devices/pci0000:00/0000:00:1c.3/0000:03:00.0/usb2/2-1/2-1:1.0/0003:1ea7:<phone>/input/input26/event11 (input)so i have also tried this:$ udevadm info -a -p /sys/devices/pci0000\\:00/0000\\:00\\:1c.3/0000\\:03\\:00.0/usb2/2-1/2-1\\:1.0/0003\\:1ea7\\:<phone>/input/input25/and this $ udevadm info -a -p /devices/pci0000\\:00/0000\\:00\\:1c.3/0000\\:03\\:00.0/usb2/2-1/2-1\\:1.0/0003\\:1ea7\\:<phone>/input/input25/and get this resultsyspath not foundthe only way i manage to get some property is using this command:$ udevadm info --query=all --name=/dev/input/mouse1and i get this but, i don't have the attribute i'm looking for(ie attr{name})p: /devices/pci0000:00/0000:00:1c.3/0000:03:00.0/usb2/2-1/2-1:1.0/0003:1ea7:<phone>/input/input26/mouse1 n: input/mouse1 s: input/by-id/usb-1ea7_2.4g_wireless_mouse-mouse s: input/by-path/pci-0000:03:00.0-usb-0:1:1.0-mouse e: devlinks=/dev/input/by-id/usb-1ea7_2.4g_wireless_mouse-mouse /dev/input/by-path/pci-0000:03:00.0-usb-0:1:1.0-mouse e: devname=/dev/input/mouse1 e: devpath=/devices/pci0000:00/0000:00:1c.3/0000:03:00.0/usb2/2-1/2-1:1.0/0003:1ea7:<phone>/input/input26/mouse1 e: id_bus=usb e: id_input=1 e: id_input_mouse=1 e: id_model=2.4g_wireless_mouse e: id_model_enc=2.4g wireless mouse e: id_model_id=0064 e: id_path=pci-0000:03:00.0-usb-0:1:1.0 e: id_path_tag=pci-0000_03_00_0-usb-0_1_1_0 e: id_revision=0200 e: id_serial=1ea7_2.4g_wireless_mouse e: id_type=hid e: id_usb_driver=usbhid e: id_usb_interfaces=:030102: e: id_usb_interface_num=00 e: id_vendor=1ea7 e: id_vendor_enc=1ea7 e: id_vendor_id=1ea7 e: major=13 e: minor=33 e: subsystem=input e: usec_initialized=77840674so clearly i've a misunderstanding on how to query udev to get the attribute of a device.hope i'm clear enough if anyone has an idea where i'm mistaking any input is welcome !thanks !matth.",
    "present_kp": [
      "debian",
      "udev"
    ],
    "absent_kp": [
      "linux",
      "command line"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "understanding client service discovery mechanism. i'm trying to understand how to implement service discovery pattern in my architechture. i understand that each instance of one service must register itself in the service registry when starts and then refresh the registration each, for instance, 30 seconds. say i have a purchases service that needs to read some data from clients service. both are rest based services.do i need to query the clients service's url in the service registry each time it needs to make a request against it? doesn't it have performance consequences?",
    "present_kp": [],
    "absent_kp": [
      "architectural patterns",
      "microservices"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "where to host my own dokuwiki on a stick. i have a dokuwiki on a stick (basically dokuwiki only ... uhmm, it's on a stick) in which i've been gathering ... stuff (my gawd, is this guy specific or what? :) and now i'd like to put it somewhere on a web. what would be a good place to host it so i can access it from anywhere?is it possible maybe to host it on google apps of my company, and how would that process go?i'd welcome all experiences and advices on the subject.btw, i've no idea whether this should go here or on s.user.",
    "present_kp": [
      "dokuwiki"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how do you know if it's psychosomatic or not?. it's in your head is often true and often offensive. but as your head can invent any number of psychological and physiological symptoms how can you distinguish whether something is or isn't psychosomatic?my naive stance (which i'm looking to correct) so far is that if the symptoms are too vague or too arbitrary it's psychosomatic. this probably has some indirect legitimacy but is entirely anecdotal. for example something too arbitrary would be: experiencing pain in your feet after standing in water in a shower, but standing in a bathtub, pool, lake, or ocean doesn't cause it.",
    "present_kp": [],
    "absent_kp": [
      "perception",
      "abnormal psychology",
      "psychiatry",
      "well being"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "program that tells if a date is valid in c. i'm reading a book about c programming, at the end of each chapter it has some training exercise and one of them was to make a program that tells if a date is valid or not, the code below is what i did and i would like to know if it can be improved?#include <stdio.h>int main(){int dd,mm,yy;printf(write a date(day/month/year):);scanf(%d/%d/%d,&dd,&mm,&yy);int bissextile=(yy%4==0)?1:0;if (dd>31 || dd<1 || mm>12 || mm<1) printf(invalid date. ); else if((mm==2 && dd<30 && bissextile) || (mm==2 && dd<29 && bissextile==0)) printf(valid date. ); else if((mm==4 || mm==6 || mm==9 || mm==11)&& dd<31) printf(valid date. ); else if(dd<31 && mm!=2) printf(valid date. ); else printf(invalid date. );}",
    "present_kp": [
      "c"
    ],
    "absent_kp": [
      "beginner",
      "datetime",
      "validation"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "bowling game kata in scala - pattern match. this is a functional approach to problem described by standard bowling game kata - to implement the rules of calculating a score in a bowling game. the inspiration was the implementation by uncle bob: <url> game consists of 10 frames as shown above. in each frame the player has two opportunities to knock down 10 pins. the score for the frame is the total number of pins knocked down, plus bonuses for strikes and spares.a spare is when the player knocks down all 10 pins in two tries. the bonus for that frame is the number of pins knocked down by the next roll. a strike is when the player knocks down all 10 pins on his first try. the bonus for that frame is the value of the next two balls rolled.in the tenth frame a player who rolls a spare or strike is allowed to roll the extra balls to complete the frame. however no more than three balls can be rolled in tenth frame.any feedback of my pattern-match-with-recursion implementation f bowling game kata will be greatly appreciated:import org.scalatest.{flatspec, matchers}class bowlinggametest extends flatspec with matchers { sanity test should pass in { true shouldbe true } gutter game should have no points in { val score = bowlinggame.count(seq.fill(20)(0)) score shouldbe 0 } all ones should return sum of points in { val score = bowlinggame.count(seq.fill(20)(1)) score shouldbe 20 } spare should get bonus from next throw in { val points: seq[int] = seq(5, 5, 8, 0) ++ seq.fill(16)(0) val score = bowlinggame.count(points) score shouldbe 26 } multiple spares should get bonus from each next throw in { val points: seq[int] = seq(5, 5, 8, 2, 7) ++ seq.fill(15)(0) val score = bowlinggame.count(points) score shouldbe 42 } strike should get bonus points from two next throws in { val points: seq[int] = seq(10, 5, 2, 0) ++ seq.fill(16)(0) val score = bowlinggame.count(points) score shouldbe 24 } multiple strikes should get bonus points from two next throws in { val points: seq[int] = seq(10, 10, 5, 2, 0) ++ seq.fill(15)(0) val score = bowlinggame.count(points) score shouldbe 49 } spare in last round should get bonus points from a single additional throw in { val points: seq[int] = seq.fill(18)(0) ++ seq(5, 5, 8) val score = bowlinggame.count(points) score shouldbe 18 } strike in last round should get bonus points from two additional throws in { val points: seq[int] = seq.fill(18)(0) ++ seq(10, 5, 3) val score = bowlinggame.count(points) score shouldbe 18 } strike in last round and in first additional throw should get no bonus points from additional throws in { val points: seq[int] = seq.fill(18)(0) ++ seq(10, 10, 3) val score = bowlinggame.count(points) score shouldbe 23 } maximum game should get bonuses in each round in { val score= bowlinggame.count(seq.fill(12)(10)) score shouldbe 300 } partial game should return score so far in { bowlinggame.count(seq(1,2,3)) shouldbe 6 }}object bowlinggame { def count(points: seq[int]): int = { def counthelper(ps: seq[int], score: int): int = ps match { case nil => score case left :: nil => score + left case lastroundstrike :: additional1 :: additional2 :: nil => score + lastroundstrike + additional1 + additional2 case strike :: next :: nextnext :: _ if strike == 10 => counthelper(ps.tail, score + strike + next + nextnext) case sparel :: sparer :: next :: pss if sparel + sparer == 10 => counthelper(next :: pss, score + sparel + sparer + next) case left :: right :: pss => counthelper(pss, score + left + right) } counthelper(points, 0) }}",
    "present_kp": [
      "recursion",
      "scala"
    ],
    "absent_kp": [
      "unit testing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "symbolic linking to ~/ instead of ~/[path/file]. i moved all of my dot files out of~/ in to ~/data/dotfilesi then did ln -s ~/data/dotfiles/[filename] ~/my intention was ln -s ~/data/dotfiles/[filename] ~/[filename]this included my .vimrc , .bashrc and othersnow when i open a terminal session or open vim i get too many levels of symbolic links.i'm unsure if i delete the symbolic link from my home directory or from the directory the files live in. any help would be appreciated. as per request:jsw:dotfiles jsw$ ls -l ~/.vimrc ~/data/dotfiles/.vimrclrwxr-xr-x 1 jsw staff 6 1 aug 13:54 /users/jsw/.vimrc -> .vimrc-rw-r--r-- 1 jsw staff 11044 31 jul 10:51 /users/jsw/data/dotfiles/.vimrc",
    "present_kp": [
      "dot files"
    ],
    "absent_kp": [
      "symlink"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to remap (at the software level) the fn and ctrl keys in an acer laptop?. i have an acer aspire v5 laptop for home use whose keyboard's lower-left corner has the keys arranged like thisctrlfnaltthis differs from, and imo is less comfortable than, the arrangements i find in every other keyboard i use, namely thesefnctrlalt (my work laptop)ctrlalt (my desktops at home and at work)i really would like the acer's ctrl and fn keys to switch places, so to speak.1the bios for this acer, afaict, offers no option for flipping the fn and ctrl keys.2is there some way that i can switch these keys by remapping them at the software level?1 the acer's different layout is more than just an annoyance. it causes me to make frequent errors (as i press fn when in fact i want to be pressing ctrl). moreover, after i become habituated to the acer, and then switch to my work laptop, i start making the same sort of error all over again. these errors are often disorienting, especially if i'm working quickly, and they have the potential for being costly.2 my work laptop's bios does offer such an option, but, ironically, perhaps because i am an emacs user, i find my work laptop's key arrangement to be the more ergonomically comfortable to the two, by far. since i use my work laptop far more than the acer, i am very reluctant to make it less sound ergonomically by availing myself of its bios option.",
    "present_kp": [
      "keyboard"
    ],
    "absent_kp": [
      "debian",
      "xorg",
      "keyboard shortcuts"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "javafx design writes to static field from constructor. in javafx, the lifecycle of an application is quite different than swing. your constructor must take no arguments so that the application class can create an instance of your object. once the instance has been created, it calls start(stage), which gives you a stage for you to put your user interface on. this may be fine for some people, but i want to access instance variables of my application class (which happens to be called ljgm), and the only way i thought i could do this was to have a single, static instance of my ljgm object, and have a static instance() method which returns the instance. whenever the constructor is called, instance is set to this. there has to be a better way to do this.public class ljgm extends application { /** the instance of {@link ljgm}. */ private static ljgm instance; /** * gets the only instance of ljgm. * * @return the instance of ljgm. */ public static ljgm instance() { return instance; } // (other variables here) /** * instantiates a new ljgm object. */ public ljgm() { ljgm.instance = this; // (initialize other variables) } /* * (non-javadoc) * * @see javafx.application.application#start(javafx.stage.stage) */ @override public void start(stage primarystage) throws exception { // (add gui to the primarystage here) } /** * main method. * * @param args * the arguments to pass to the application. */ public static void main(string[] args) { launch(args); }}here is the application javadoc in case you need it.",
    "present_kp": [
      "java",
      "constructor",
      "javafx"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "debian: umount /media/usb0: operation not permitted. i recently moved from ubuntu 14.10 to debian testing. under ubuntu i had a shell script for backing up my home folder to a usb stick. the last line of the script unmounts the usb stick. adapting it to debian's file system, it should be:umount /media/usb0(or umount /media/usb which links to /media/usb0). but when i execute the script, this last line throws umount: /media/usb0: umount failed: operation not permitted. obviously, with sudo it works, but i'm wondering why i need to be sudo here (was not necessary on ubuntu). any ideas how to circumvent that, so how to unmount the usb stick without being root? (the remaining part of the script runs flawlessly). updatethis is my /etc/fstab:# <file system> <mount point> <type> <options> <dump> <pass>/dev/mapper/sklar--vg-root / ext4 errors=remount-ro 0 1# /boot was on /dev/sda1 during installationuuid=a90bee04-e08a-4a86-8465-762aca5719a4 /boot ext2 defaults 0 2/dev/mapper/sklar--vg-swap_1 none swap sw 0 0/dev/sdb1 /media/usb0 auto rw,user,noauto 0 0",
    "present_kp": [
      "debian",
      "shell script",
      "mount"
    ],
    "absent_kp": [
      "unmounting"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "throwing exceptions in application configuration providers. simple question: what is the best/common practice regarding to throwing errors for application configuration providers?given is a simple key/value-based configuration source:class configuration string getvalue(key); void setvalue(key, value);or more extended, with the ability to add/remove key/value elementsclass configuration string getvalue(key); void setvalue(key, value); void addkeyvaluepair(string key, string value); void removekeyvaluepair(string key);not throwing exceptions on methods like getvalue() has the benefit of not interrupting the application flow and making code easier to read because of a lack of exception handling etc. it also allows to use null-conditional operators (maybe monads) or null-coalescing operators for e.g. setting defaults:var value = configuration.getvalue(mykey) ?? hello world;return null simply tells the user: 'whoops, got nothing to return since nothing is set/there'.however..if i'd avoid throwing exceptions, what would happen if i try to add a key/value pair which is already available? or set the value with a key that isn't there in the underlying storage? or removing a key/value pair that isn't available? would it be obvious for the user that simply nothing happens when we tries to remove a none-existing key pair?what is the prefered approach for application configuration?",
    "present_kp": [
      "exceptions",
      "exception handling",
      "configuration"
    ],
    "absent_kp": [
      "c#",
      "programming practices"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "running fetchmail with socks proxy. i checked the manual of fetchmail, all it mentioned was socks_conf, but what is the format of that config file?and how do i know if my fetchmail is configured with --with-socks, fetchmail -v doesn't say anything related.",
    "present_kp": [
      "proxy",
      "socks",
      "fetchmail"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "will it be a wrong idea to have in <body>?. in below code i placed internal style sheet with in body tag, instead of having in head.for single-page-application i am considering to do this for styles that is only applicable to that page alone, rather than having separate pagespecific.css file.is there a scenario where this has downside as i am not putting the same in head section?<!-- mypartial.html starts here --> <!-- like to keep styles unique to this html right here in this file --> <div> <style> body { background-color: red; } #mytext { color: white; } </style> <span id='mytext'>hello</span></div><!-- mypartial.html ends here -->",
    "present_kp": [
      "html",
      "css"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how do i test ftplugin scripts efficiently. say i am writing a plugin and constantly editing scripts in ftplugin. how do i efficiently test the changes? i can't simply source the editted file because they are filetype-specific. for example, if there is a lineset buftype=nofilein the script, and i source it, then whatever buffer i'm in will no longer be writable into a file...i can open another vim instance to test the change, but that is not very efficient because i have to reopen one whenever i make changes to the ftplugin scripts.",
    "present_kp": [
      "filetype"
    ],
    "absent_kp": [
      "vimscript"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how group has access for something?. example: in debian if user want to have access to journalctl without using root credentialas he must be added to systemd-journal group. /bin/journalctl is owned by root and group root so how it works ? how systemd-journal group has access and how to edit this permissions. i am not talking about permisisons to files and folders but maybe it comes down to that.",
    "present_kp": [
      "permissions",
      "group"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "torrent to direct-link converter. which are the websites which help to direct download bit-torrent files?one such service is torrific what torrific does is, when we give the url of a .torrent file, it will download the file on its server and give us the direct link for the downloaded file.since torrific has a bandwidth limit of 10gb per day i am looking for other free alternatives",
    "present_kp": [],
    "absent_kp": [
      "webapp rec",
      "bittorrent"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is this the optimal way to write find these ids in a list of objects using linq?. i've got a situation where i have a list of organizations (code to follow), and a user has a list of organizationids (ints). i want to filter down the full list of organizations using the user's organizationid list for a dropdown list. the query works, but it feels like there may be a more optimal way to write it. is there, or am i already pretty much optimized?public class organization{ public int id { get; set; } public string division { get; set; } public bool isactive { get; set; } public bool isdefault { get; set; } public string name { get; set; }}queryvm.locations = _organizations.getorganizations().where(o => ((applicationuser)user).organizationidlist.any(u => u == o.id)).select(org => new selectlistitem { text = org.division + - + org.name, value = org.id.tostring() }).orderby(item => item.text);",
    "present_kp": [
      "linq"
    ],
    "absent_kp": [
      "c#",
      "optimization"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "'seen' message status on facebook website and messenger app. some of the messages i have sent to facebook friends are appearing with 'seen' stamp if i open the messages on facebook.com while they are unseen on the messenger app. which one is the right one? it seems to me that messenger is right because on facebook, all the times when the message is seen is one minute after sending the message.",
    "present_kp": [
      "facebook"
    ],
    "absent_kp": [
      "facebook chat"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how should i troubleshoot a round error in commcare?. i have set up a commcare form in the form builder, when i try to deploy i receive the following message at the top of the screen: validation error: the round function was provided the incorrect number of arguments:2. it expected 1 arguments.the error also points to which form the error is stemming from, but not where in the form it is being triggered. what should i look for in the form to debug this error?",
    "present_kp": [
      "commcare"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "config to re-enable gtk2 tear-off menus?. following an apt-get update on debian jessie/sid, several gtk2 applications no longer have tear-off menus enabled.i'm aware that this is a deprecated capability of gtk (and disagree strongly with the gnome team's rational for deprecating the feature), but if this is a config issue (and i've dug it up repeatedly in the past), how would i re-enable it?gconf-editor doesn't seem to affect the behavior.",
    "present_kp": [
      "gtk2"
    ],
    "absent_kp": [
      "gui",
      "desktop"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to make diagnosis of these symptoms using pet scan, fmri, and eeg?. the symptoms are muscle weakness, slow reaction time, visual perception issues, seizures, hearing loss, weight gain, general sense of pain and delusions. using the above scanning techniques, a doctor concluded that there are problems in emission of neurotransmitters and secretion of hormones as well as myelin sheath of some neurons, occipital lobe, and temporal lobe. could you tell me how the doctor used the scanning tools to reach to his conclusion? i totally have no idea how the doctor learned that there's problem in emission of neurotransmitter by using scanning techniques.",
    "present_kp": [],
    "absent_kp": [
      "neurobiology",
      "neuroimaging"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to go back to original top panel in debian mint xfce?. changes to the top panel in lmde w/ xfce have corrupted the formatting; everything is flush left now. unable to get the time, window switcher and other items to go flush right. how do i revert to the original panel layout so i can start over? ps: do not use sudo prepended before risto's steps below.",
    "present_kp": [
      "debian",
      "xfce"
    ],
    "absent_kp": [
      "gnome panel"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "shortening method based on an argument name. as you can see below, i have a method which executes statements based on the first letter of a component firing an itemevent:public void itemstatechanged(itemevent ie) { if(ie.getsource() == rradiobutton) { currentscale = r; ((defaulteditor) rspinner.geteditor()).gettextfield().requestfocus(); } else if(ie.getsource() == gradiobutton) { currentscale = g; ((defaulteditor) gspinner.geteditor()).gettextfield().requestfocus(); } else if(ie.getsource() == bradiobutton) { currentscale = b; ((defaulteditor) bspinner.geteditor()).gettextfield().requestfocus(); } //...}is it possible to easily shorten the method and make it more automatic?if the argument was an actionevent i could use getactioncommand to assign the value for currentscale but the same number of lines of code would be needed first to setactioncommand for every component.i could also use setname and getname for components but this will result it the same issue as described above.also it would be good to be able to automatically point to a specific spinner based on a specific radiobutton as you can see in the attached source code.maybe there is a possibility to use reflection for this but i don't know if this could work.",
    "present_kp": [],
    "absent_kp": [
      "java"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "auto forwarding emails in gmail. i have set up a new gmail address and i have put several addresses in for any incoming emails to be forwarded to automatically. my problem is that it only seems to be selecting the top email address, and not auto forwarding to them all.is there any way i can do this?",
    "present_kp": [
      "gmail",
      "email"
    ],
    "absent_kp": [
      "email forwarding"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "stuck due to knowing too much. note more discussion at <url> i have a relatively simple development task, but every time i try to attack it, i end up spiraling in deep thoughts - how could it extending the future, what are the 2nd generation clients going to need, how does it affect non functional aspects (e.g. performance, authorization...), how would it best to architect to allow change...i remember myself a while ago, younger and, perhaps, more eager. the me i was then wouldn't have given a thought about all that - he would've gone ahead and wrote something, then rewrote it, then rewrote it again (and again...). the me today is more hesitant, more careful.i find it much easier today to sit and plan and instruct other people on how to do things than to actually go ahead and do them myself - not because i don't like to code - the opposite, i love to! - but because every time i sit at the keyboard, i end up in that same annoying place.is this wrong? is this a natural evolution, or did i drive myself into a rut?fair disclosure - in the past i was a developer, today my job title is a system architect. good luck figuring what it means - but that's the title.wow. i honestly didn't expect this question to generate that many responses. i'll try to sum it up.reasons:analysis paralysis / over engineering / gold plating / (any other too much thinking up-front can hurt you).too much experience for the given task.not focusing on what's important.not enough experience (and realizing that).solutions (not matched to reasons):testing first.start coding (+ for fun)one to throw away (+ one api to throw away).set time constraints.strip away the fluff, stay with the stuff.make flexible code (kinda opposite to one to throw away, no?).thanks to everyone - i think the major benefit here was to realize that i'm not alone in this experience. i have, actually, already started coding and some of the too-big things have fallen off, naturally.since this question is closed, i'll accept the answer with most votes as of today. when/if it changes - i'll try to follow.",
    "present_kp": [],
    "absent_kp": [
      "programming practices",
      "productivity"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "write-only permission for a directory doesn't allow to rename (move) files inside?. in fact, i would like to ask more general question -- what does write permission for a directory allow you to do exactly? -- but let's approach it with a concrete example.it is a long question, if you are in a hurry read the bold -- it should cover the main part.different sources (nice question, one more, grymoire's) say something similar to the following on the directory permissions:r, read -- reading of the directory's content (filenames inside)w, write -- changing the directory's attributes (e.g. modification time) and creating/renaming/removing entries insidex, search -- accessing files inside, you have access to the inode of the file, hence you can reach it's actual contentmy problem is with the description of w. what directory's attributes does it give you access to? i cannot create/rename/remove a file inside a directory with only write permission:i make a directory (tdir/) and a file inside (afile), chmod -x-r tdir/, mv tdir/afile tdir/af, rm tdir/afile, touch tdir/newfile -- all fail with permission denial, unless i set x permission to the directory as well.and x alone doesn't give you the permission to create/rename/remove files inside the directory.in order to do that you need both x and w.but touch tdir does change the modification time of the directory with w only.i would rephrase the sources above this way for the compliance with the issue: a directory's r allows you to see the filenames inside, but no access to the actual file (to the inode); x gives you access to the inodes of the files (which means you can see their permissions and, according to it, have access to the contents), but you still cannot change anything in the directory; directory is actually some sort of a file and to change something in it you need the w permission.thus, when you are changing something in the directory you need w permission. if your change requires inodes of the files in the directory -- you need x as well.it explains why you cannot remove a file inside a directory with w only: when removing a file you need to reduce the link count of the inode by 1 -- you need to know the inode -- thus you need x for the directory.but why do you need x for creating (you could ask the system to create a file without exposing the inode?) and renaming/moving the file (when you move a file you don't change it in any way, you only change the records inside the directories and their inode counts?)?maybe it is just an implementation thing? i.e. indeed you don't need the inode for renaming/creating files -- you need only filenames and w permission; but inode and filename constitute one record in the directory; thus changing the filenames = changing the records = kind of accessing the inodes.and also what attributes do directories have besides modification time, permissions and files records? what else in the directory can you change with w only?",
    "present_kp": [
      "permissions",
      "directory"
    ],
    "absent_kp": [
      "filesystems"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "searching has_and_belongs_to_many relations for article categories. i have article and category, with a has_and_belongs_to_many relation between the two.i want to use article.in_categories(category_ids) to get any article that is in any of the category_ids passed, and i want to handle the possibility of category_ids being nil in a way that all article items are returned. the category_ids value is coming from a form, and i want the user to be able to ignore the category selection part of the form if they do not want to limit their selection by that criteria.i currently have this in place on the article model:def self.in_categories(category_ids = nil) category_ids.blank? ? all : includes(:categories).where(categories: { id: category_ids })endif i do not have the check for category_ids being nil, then it will only return the article items that are not in any category.i do not want to manually set category_ids = [1,2,3] (for example) in the method call, because a category can be added/removed.is there a better way of handling this than the way i already do?",
    "present_kp": [],
    "absent_kp": [
      "ruby",
      "ruby on rails"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how do you keep track of your past projects?. after having hard time trying to remember details of my past projects several years ago i was wondering how programmers usually keep track of that?this kind of information comes handy with job interviews etc.do you write down technologies used, challenges faced etc to some kind of jumbo cv? or do you just trust your memory?",
    "present_kp": [
      "interview",
      "project"
    ],
    "absent_kp": [
      "resume"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "using second ip on eth1. i have a /26 static ip in my network and i want to assign a second ip to a machine which already have one ip on eth0. the os centos 6.6 64bit.i added another ethernet adapter (card) to the machine and removed the 70-persistent-net.rules so network cards can be detected and added automatically after reboot.rebooted the os and ran ifup eth1:root@host [~]# ifup eth1/sbin/ifup: configuration for eth1 not found.usage: ifup so i made a copy of ifcfg-eth0 to ifcfg-eth1 and replaces eth0 to eth1 and hardware id (mac).content of ifcfg-eth1:hwaddr=00:50:56:be:57:c2name=eth1gateway=xxx.xxx.xxx.1dns1=8.8.8.8dns2=8.8.4.4domain=google.comdevice=eth1onboot=yesuserctl=nobootproto=staticnetmask=255.255.255.192ipaddr=xxx.xxx.xxx.5peerdns=noipv6init=nocheck_link_down() { return 1;}content of ifcfg-eth0:hwaddr=00:50:56:be:7b:88name=eth0gateway=81.17.30.1dns1=8.8.8.8dns2=8.8.4.4domain=google.comdevice=eth0onboot=yesuserctl=nobootproto=staticnetmask=255.255.255.192ipaddr=81.17.30.37peerdns=noipv6init=nocheck_link_down() { return 1;}and ran service network restart:shutting down interface eth0: [ ok ]shutting down loopback interface: [ ok ]bringing up loopback interface: [ ok ]bringing up interface eth0: determining if ip address xxx.xxx.xxx.37 is already in use for device eth0... [ ok ]bringing up interface eth1: determining if ip address xxx.xxx.xxx.5 is already in use for device eth1... [ ok ]xxx.xxx.xxx.37 is still up but xxx.xxx.xxx.5 is not responding. did i do anything wrong?",
    "present_kp": [
      "centos"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "copying files to fuchsia / magenta kernel. how to copy files (e.g. .o or .dart) from your host (linux machine) to the fuchsia / magenta kernel? or is there any way to prepare your files -> save them to respective fuchsia / kernel directory -> compile the kernel -> and when i boot, my files are there inside the kernel. i have followed this link: copying files to and from magenta / including additional userspace files but failed to complete the same.edit:these two options are listed (link which i posted earlier) in their official tutorial, but i am not able to follow them.question a: copying files to and from magenta (already figured out)with local link ipv6 configured, the host tool ./build-magenta-arch/tools/netcp can be used to copy files.# copy the file myprogram to magentanetcp myprogram :/tmp/myprogram# copy the file myprogram back to the hostnetcp :/tmp/myprogram myprogramusing this option i am able to copy files from my linux machine to fuchsia kernel, but when i restart the kernel, my transferred files are gone. how to transfer files permanently? question b: including additional userspace files$builddir/tools/mkbootfs -o extra.bootfs @/path/to/directoryecho issue.txt=/etc/issue > manifestecho etc/hosts=/etc/hosts >> manifest$builddir/tools/mkbootfs -o extra.bootfs manifestkindly help me understand this work-flow.",
    "present_kp": [
      "fuchsia",
      "magenta kernel"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "cannot use ntfs drive as non-root user. i have an ntfs partition which i am trying to mount as read write in fedora 22. using nautilus as root allows me to create folders, files etc. however when i use my non root account, i get an error while copying to data the destination is read-only error.i have changed the permission and ownership of the whole mount point to my non-root account and even specified the user and group ids in the mount options.my mount options: nosuid,nodev,nofail,x-gvfs-show,remove_hiberfile,rw,fmask=0000,dmask=0000,uid=1000,gid=1000",
    "present_kp": [
      "files",
      "fedora"
    ],
    "absent_kp": [
      "linux"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to install precompiled ath9k driver?. i have a ubuntu and i have cross compiled an ath9k driver for an arm architecture. the problem is a can't make the install work as i want to work.readme says the following:building for external kernelsif you have a kernel you do not have installed but yet want to build the compat-wireless-2.6 drivers for it you can use this syntax:make klib=/home/mcgrof/kernels/linux-2.6.23.9 klib_build=/home/mcgrof/kernels/linux->2.6.23.9if you have a kernel installed, which is not your currently running kernel (e.g. via distro updates; plus its corresponding kernel-dev package), you can use this syntax:make klib=/lib/modules/2.6.30.6-53.fc11.x86_64and to install to your system's root path for the non-running kernel:make klib=/lib/modules/2.6.30.6-53.fc11.x86_64 kmodpath_arg='install_mod_path=' installmy build command is the following:make arch=arm cross_compile=/home/roncsak/building/toolchain-arm-linux-gnueabihf/gcc-lw-arm-linux-gnueabihf-4.7-2013.03-20130313_linux/bin/arm-linux-gnueabihf- klib=/home/roncsak/building/linux/linux-3.0.35-boundary klib_build=/home/roncsak/building/linux/linux-3.0.35-boundary(building is a success.)according to readme i should install like this:make arch=arm cross_compile=/home/roncsak/building/toolchain-arm-linux-gnueabihf/gcc-arm-linux-gnueabihf-4.7-2013.03-20130313_linux/bin/arm-linux-gnueabihf- klib=/home/roncsak/building/linux/linux-3.0.35-boundary klib_build=/home/roncsak/building/linux/linux-3.0.35-boundary kmodpath_arg='install_mod_path=/home/roncsak/building/targetfs/rfs/lib/modules/3.0.35' installunfortunatelly my install won't work. the result is the following:make -c /home/roncsak/building/linux/linux-3.0.35-boundary m=/home/roncsak/building/ath9/compat-wireless-3.1.1-1 modulesmake[1]: entering directory '/home/roncsak/building/linux/linux-3.0.35-boundary' building modules, stage 2. modpost 9 moduleswarning: ewma_init [/home/roncsak/building/ath9/compat-wireless-3.1.1-1/net/mac80211/mac80211.ko] undefined!warning: ewma_add [/home/roncsak/building/ath9/compat-wireless-3.1.1-1/net/mac80211/mac80211.ko] undefined!make[1]: leaving directory '/home/roncsak/building/linux/linux-3.0.35-boundary'make -c /home/roncsak/building/linux/linux-3.0.35-boundary m=/home/roncsak/building/ath9/compat-wireless-3.1.1-1 install_mod_dir=updates install_mod_path=/home/roncsak/building/builddir/targetfs/rfs/lib/modules/3.0.35/ \\ modules_installmake[1]: entering directory '/home/roncsak/building/linux/linux-3.0.35-boundary' install /home/roncsak/building/ath9/compat-wireless-3.1.1-1/compat/compat.ko install /home/roncsak/building/ath9/compat-wireless-3.1.1-1/drivers/net/wireless/ath/ath.ko install /home/roncsak/building/ath9/compat-wireless-3.1.1-1/drivers/net/wireless/ath/ath9k/ath9k.ko install /home/roncsak/building/ath9/compat-wireless-3.1.1-1/drivers/net/wireless/ath/ath9k/ath9k_common.ko install /home/roncsak/building/ath9/compat-wireless-3.1.1-1/drivers/net/wireless/ath/ath9k/ath9k_htc.ko install /home/roncsak/building/ath9/compat-wireless-3.1.1-1/drivers/net/wireless/ath/ath9k/ath9k_hw.ko install /home/roncsak/building/ath9/compat-wireless-3.1.1-1/net/mac80211/mac80211.ko install /home/roncsak/building/ath9/compat-wireless-3.1.1-1/net/rfkill/rfkill-regulator.ko install /home/roncsak/building/ath9/compat-wireless-3.1.1-1/net/wireless/cfg80211.ko depmod 3.0.35warning: /home/roncsak/building/builddir/targetfs/rfs/lib/modules/3.0.35//lib/modules/3.0.35/updates/net/wireless/cfg80211.ko needs unknown symbol rfkill_unregisterwarning: /home/roncsak/building/builddir/targetfs/rfs/lib/modules/3.0.35//lib/modules/3.0.35/updates/net/wireless/cfg80211.ko needs unknown symbol rfkill_blockedwarning: /home/roncsak/building/builddir/targetfs/rfs/lib/modules/3.0.35//lib/modules/3.0.35/updates/net/wireless/cfg80211.ko needs unknown symbol rfkill_destroywarning: /home/roncsak/building/builddir/targetfs/rfs/lib/modules/3.0.35//lib/modules/3.0.35/updates/net/wireless/cfg80211.ko needs unknown symbol rfkill_resume_pollingwarning: /home/roncsak/building/builddir/targetfs/rfs/lib/modules/3.0.35//lib/modules/3.0.35/updates/net/wireless/cfg80211.ko needs unknown symbol rfkill_pause_pollingwarning: /home/roncsak/building/builddir/targetfs/rfs/lib/modules/3.0.35//lib/modules/3.0.35/updates/net/wireless/cfg80211.ko needs unknown symbol rfkill_set_hw_statewarning: /home/roncsak/building/builddir/targetfs/rfs/lib/modules/3.0.35//lib/modules/3.0.35/updates/net/wireless/cfg80211.ko needs unknown symbol rfkill_allocwarning: /home/roncsak/building/builddir/targetfs/rfs/lib/modules/3.0.35//lib/modules/3.0.35/updates/net/wireless/cfg80211.ko needs unknown symbol rfkill_registerwarning: /home/roncsak/building/builddir/targetfs/rfs/lib/modules/3.0.35//lib/modules/3.0.35/updates/net/wireless/cfg80211.ko needs unknown symbol rfkill_set_sw_statewarning: /home/roncsak/building/builddir/targetfs/rfs/lib/modules/3.0.35//lib/modules/3.0.35/updates/net/mac80211/mac80211.ko needs unknown symbol ewma_addwarning: /home/roncsak/building/builddir/targetfs/rfs/lib/modules/3.0.35//lib/modules/3.0.35/updates/net/mac80211/mac80211.ko needs unknown symbol ewma_initwarning: /home/roncsak/building/builddir/targetfs/rfs/lib/modules/3.0.35//lib/modules/3.0.35/updates/net/rfkill/rfkill-regulator.ko needs unknown symbol rfkill_unregisterwarning: /home/roncsak/building/builddir/targetfs/rfs/lib/modules/3.0.35//lib/modules/3.0.35/updates/net/rfkill/rfkill-regulator.ko needs unknown symbol rfkill_destroywarning: /home/roncsak/building/builddir/targetfs/rfs/lib/modules/3.0.35//lib/modules/3.0.35/updates/net/rfkill/rfkill-regulator.ko needs unknown symbol rfkill_allocwarning: /home/roncsak/building/builddir/targetfs/rfs/lib/modules/3.0.35//lib/modules/3.0.35/updates/net/rfkill/rfkill-regulator.ko needs unknown symbol rfkill_registermake[1]: leaving directory '/home/roncsak/building/linux/linux-3.0.35-boundary'mkdir: cannot create directory '/usr/lib/compat-wireless/': permission deniedmake: *** [install-scripts] error 1how should i do that right?",
    "present_kp": [
      "linux",
      "drivers"
    ],
    "absent_kp": [
      "modem"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "which standard copyleft licences can be applied to unity assets?. i would like to release a framework for unity under a copyleft licence but have some concerns.my goal is for people to be able to use the framework to develop their own projects and only be required to release any changes made to the framework back to the community i.e. they should not be required to publish their project's 'content' as open source.some of my concerns:the gnu lgpl looks like it might be a good fit but has been written with a heavy focus on linking and object code. while the unity editor can import a dll into a project i believe that for most platforms the final executable will be staticly linked. i can't see how anyone receiving the framework could use it in a distributed game and not trigger the viral clauses if under the lgpl. some of this framework will be non-code files (prefabs, audio visual media files etc) that i would like to be covered by a similar, or preferably the same, licence.i am also concerned that anti-drm clauses in some licences may impact adoption, i do not want the licence to restrict use on any platform.while i am not planning to release via the asset store i do not want to restrict that option for the future. the asset store provider agreement states:5.10.4 provider represents and warrants that its assets shall not contain any software licensed under the gnu general public license or gnu limited (lesser) general public license, or any other license with terms that include a requirement to extend such license to any modification or combined work and provide for the distribution of the combined or modified products source code upon demand so that customer content becomes subject to the terms of such open source license; or (ii) any software that is a modification or derivative of any software licensed under the gnu general public license, limited (lesser) public license, or license with terms similar thereto so that customer content become subject to the terms of such open source license.which of the standard copyleft licences are suitable for a unity asset?",
    "present_kp": [
      "copyleft",
      "drm",
      "assets",
      "lgpl"
    ],
    "absent_kp": [
      "license recommendation"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "example of logarithmic length witness being easier to verify than find. an easy observation is that if a problem $a$ is decidable by a polynomial-time nondeterministic program using $o(\\log n)$ nondeterministic bits (i.e., all witnesses are logarithmic in length), then $a \\in \\mathsf{p}$.if one then asks the question, is it easier to verify a witness than to find one? for such problems, and one considers all polynomial running times equivalent, then the answer is no, since one can find such witnesses in polynomial time by searching through all potential witnesses.but what if we consider fine-grained distinctions between polynomial running times? i'm wondering if there is a concrete example of a natural problem in $\\mathsf{p}$ that has logarithmic-length witnesses that are easier to verify than to find, where easier means a smaller polynomial running time.for example, known algorithms for perfect matching in graphs take polynomial time, but more than $o(n)$ time on a graph with $n$ nodes. but given a set of $n/2$ pairs of nodes (a witness), it is easy to verify in time $o(n)$ that it is a matching. however, the matching itself requires at $\\omega(n)$ bits to encode.is there some natural problem that achieves a similar (apparent) speedup in verification versus finding, in which the witness has logarithmic length?",
    "present_kp": [],
    "absent_kp": [
      "cc.complexity theory"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "hour averaging program. i am looking for tips on improving my short program. i am using system(pause) because this was for an assignment. code:#include <iostream>#include <string>#include <iomanip>using namespace std;void display(string* , int* , double*, int);void percentage(int*, double*, int);void intro(string*, int*, int&);void highest(int&, string&, double*, string*, int);int main(){ int students; string names[10]; int hours[10]; double percents[10]; int highest; string most; intro(names, hours, students); percentage(hours, percents, students); display(names, hours, percents, students); highest(highest, most, percents, names, students); system(pause);}void intro(string* names, int* hours, int& students){ int team; cout << a team is made up of atleast 2 students. how many students are on the team?: ; cin >> team; cout << enter student's first name and the hours worked on the final project: << endl; for (int i = 0; i < team; i++) { cout << i + 1 << : ; cin >> names[i] >> hours[i]; } students = team;}void display(string* names, int* hours, double* percent, int students){ cout << setw(20) << students; cout << setw(20) << hours worked; cout << setw(20) << % of total hours; cout << endl; cout << ------------------------------------------------------------ << endl; for (int i = 0; i < students; i++) { cout << setw(20) << names[i]; cout << setw(9) << hours[i]; cout << setw(16) << percent[i]; cout << endl; }}void percentage(int* hours, double* percent, int students){ int total(0); for (int i = 0; i < students; i++) { total += hours[i]; } for (int i = 0; i < students; i++) { percent[i] = double(hours[i]) / total * 100; }}void highest(int& highest, string& most, double* percent, string* names, int students){ highest = percent[0]; for (int i = 0; i < students; i++) { if (highest < percent[i]) { highest = percent[i]; most = names[i]; } } cout << most << worked the most hours. << endl;}",
    "present_kp": [],
    "absent_kp": [
      "c++",
      "homework"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "command to check cpu, memory, i/o and networking for a process. i'm searching for a single osx command to check, at one time:cpu %memory %i/o operations numbernumber of packets in and outfor a specific process.i think top gives me only informations about cpu and memory for a single process, isn't it?is there a solution?",
    "present_kp": [
      "process",
      "osx",
      "io",
      "top"
    ],
    "absent_kp": [
      "terminal"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "rails-api gem, is there such thing as an api only application?. i've built a few api's using the complete rails stack. in each project there have been multiple uses for rails core features. each of the api has had management screens for monitoring usage, managing authentication keys, etc. is there such thing as an api without a management front end?",
    "present_kp": [
      "api"
    ],
    "absent_kp": [
      "web applications",
      "web services",
      "ruby on rails",
      "api design"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to write this pseudocode where condition. this question is extension of this question about listing questions of directories. i get a problem that i need to understand the original section of the text. current outputwhere you see the second question does not make sense. it was included in the subsection takayasu arteritis in the directory rheumatology:\\subsection{takayasu arteritis}egin{question}{you get a patient. what do you notice first in this patient?}absence of peripheral pulse.\\end{question}lorem ipsum. egin{question}{what was the first takayasu case?}young woman in asia with red vessels in the eye. so special eye diagnosis done. affects eye.\\end{question}where the current output is by this excellent code\\section{rheumatology} egin{question}{you get a patient. what do you notice first in this patient?}absence of peripheral pulse.\\end{question}egin{question}{what was the first takayasu case?}young woman in asia with red vessels in the eye. so special eye diagnosis done. affects eye.\\end{question}but i would like it to be\\section{rheumatology} \\subsection{takayasu arteritis}egin{question}{you get a patient. what do you notice first in this patient?}absence of peripheral pulse.\\end{question}egin{question}{what was the first takayasu case?}young woman in asia with red vessels in the eye. so special eye diagnosis done. affects eye.\\end{question}my proposal is to include subsections if there are questions within the subsection. my pseudocodelook for the environment egin{question}...\\end{question}.see the subsection of the question (it locates above the question). if there is no subsection, leave blank. if there are many questions within one subsection, put only one subsection. terdon's code#!/usr/bin/env bash## avoid errors if a directory has no *tex filesshopt -s nullglobdirectories=(cardiology rheumatology surgery);## change this to set whichever options you want.printf %s %s \\documentclass{yourclass} egin{document}for directory in ${directories[@]}do ## reset the counter, avoid empty sections. c=0; for file in $directory/*tex do let c++ [ $c -eq 1 ] && printf %s \\section{$directory} ## extract the wanted lines perl -lne '$a=1 && print if /\\begin{question}/; print if $a==1; $a=0 if /\\end{question}/;' $file echo donedoneecho \\end{document}where i think the logic of these lines should be changed ## extract the wanted lines perl -lne '$a=1 && print if /\\begin{question}/; print if $a==1; $a=0 if /\\end{question}/;' $file echo which uses a regex to find all questions in a file, and ignores the subsections of the questions. example of data (little different from the last case!)\\subsection{takayasu arteritis}egin{question}{you get a patient. what do you notice first in this patient?}absence of peripheral pulse.\\end{question}egin{question}{what was the first takayasu case?}young woman in asia with red vessels in the eye. so special eye diagnosis done. affects eye.\\end{question}fever of unknown origin can be used when you do not know what is causing the disease. % show cases in medscape and ask class. aneurysms. \\subsubsection{treatment}egin{question}{what you should always include in takayasu treatment? what are the symptoms?}blood pressure.aneurysms which will burst without treatment. so blood pressure decreasing drugs like beta blockers along in combination with other drugs.\\end{question}egin{question}{when is the checkup of the takayasu arteritis?} only once per year. you could expect every month like normally in this kind of diseases.but only once per year.\\end{question}where you can ignore subsubsections in the application.how can you write the pseudocode?bug in terdon's code 14.10.2014 foundexample of data which causes the bug\\subsection{3}a 55 y.o male says that for the last year ... egin{question}{what is the pathogenetic mechanism of his complains?} \\end{question}which parsed by terdon's code gives exactly the same result which is wrong, since the sentence a 55 y.o... should not be there anymore in the final result.if there is a enter between the subsection and the body text, then correct output. however, this cannot be assumed.the cause of this bug is the windows symbols, moved here.",
    "present_kp": [
      "perl"
    ],
    "absent_kp": [
      "text processing",
      "scripting",
      "latex"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what exactly is the function piping into the other function in this fork bomb :(){ :|: & };:?. there are a couple of questions related to the fork bomb for bash :(){ :|: & };: , but when i checked the answers i still could not figure out what the exactly the part of the bomb is doing when the one function pipes into the next, basically this part: :|: . i understand so far, that the pipe symbol connects two commands by connecting the stdandard output of the first to the standard input to the second, e.g. echo turkeys will dominate the world | sed 's/s//'.but i do not get it what the first function is pushing through its standard out, which gets pushed into the second one, after all there are no return values defined inside the function, so what is travelling through the human centipede if the man at the beginning has an empty stomach?",
    "present_kp": [
      "pipe",
      "function",
      "fork"
    ],
    "absent_kp": [
      "command line"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "orange discretize using rank values. i am newbie to this world and i have been lucky on finding orange!on my tests i want to discretize using a top rank, i mean: i want to check which are the top n most common values, keep them and assign another value for all the others.i am not sure about which is the best way to accomplish this.any suggestion?thanks in advance",
    "present_kp": [
      "orange"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "where and how do i obtain and install certificates to a new installation of openssl?. i have just compiled openssl and the lynx web browser. i would like to browse https websites. it is my understanding that i need to get certificates from the cas those sites use and that i also need some sort of certificate of my own. the web browser calls openssl properly, but i can't browse the internet because i don't have an initiali list of ca certificates.does anyone have documentation that explains how to get these certificates and get openssl to consume them?thanks",
    "present_kp": [
      "openssl",
      "ssl",
      "browser",
      "certificates"
    ],
    "absent_kp": [
      "configuration"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is it possible ( or how) to dump source code/machine code from memory?. i am actually working for the side of software protection. but in order to properly protect my source code ( be it .net or c++), i would need to understand what is achievable ( and not achievable) from hacker's point of view .i can protect/obfuscate/encrypt/virtualize my code all the way i want, but when the source code is loaded into the memory, it will have to be unprotected in order to run. so at this point of time, can hacker dump the source code out from the memory, and thus recover my source code in full, unadulterated fashion? if yes, how this can be done? using what tools?edit: from my finding, it seems that this is doable, and there are protection tools that can prevent this, such as agile.net:method level code encryption - encrypts all the msil code and keeps it in a secure storage. when the assembly is loaded agile.net binds to the .net runtime engine and manages decrypting the msil on a per method basis. agile.net creates a runtime environment that executes the original msil code by decrypting one method at a time, this important virtue minimizes the exposure of msil code in memory thus prevents dumping the code from physical memory.but i have no idea how, can anyone shed some lights on this?",
    "present_kp": [
      "c++",
      "memory",
      ".net"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "explanation on chown(1) posix spec. the posix spec for the chown utility mentions in its rationale section about the chown user:group syntax (formerly chown user.group) (emphasis mine):the 4.3 bsd method of specifying both owner and group was included in this volume of posix.1-2008 because:there are cases where the desired end condition could not be achieved using the chgrp and chown (that only changed the user id) utilities. (if the current owner is not a member of the desired group and the desired owner is not a member of the current group, the chown() function could fail unless both owner and group are changed at the same time.)i thought the user:group syntax was a convenience thing. now the above implies there are things you can do with chown user:group that you can't with chgrp group; chown usernow that text doesn't make sense to me. in 4.3bsd, only root could change the owner a file so in any case there's no restriction in what he can do.sysv and a few other systems allow (or used to allow) the owner of a file to change the user and group of a file to anything, but even in those system, that text above doesn't make sense to me. ok, if one does do a chown someone-else the-file, one cannot do chgrp something-else the-file afterwards because one is no longer the owner of the file, but there's nothing preventing him/her from doing the chgrp first (staying the owner of the file) and chown after, and that's not what the text above is exactly saying.i don't understand what the and the desired owner is not a member of the current group has to do with the problem.so what are the conditions for which the chown() function could fail unless both owner and group are changed at the same time, and on what system?",
    "present_kp": [
      "posix",
      "chown"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "thick models vs. business logic, where do you draw the distinction?. today i got into a heated debate with another developer at my organization about where and how to add methods to database mapped classes. we use sqlalchemy, and a major part of the existing code base in our database models is little more than a bag of mapped properties with a class name, a nearly mechanical translation from database tables to python objects.in the argument, my position was that that the primary value of using an orm was that you can attach low level behaviors and algorithms to the mapped classes. models are classes first, and secondarily persistent (they could be persistent using xml in a filesystem, you don't need to care). his view was that any behavior at all is business logic, and necessarily belongs anywhere but in the persistent model, which are to be used for database persistence only.i certainly do think that there is a distinction between what is business logic, and should be separated, since it has some isolation from the lower level of how that gets implemented, and domain logic, which i believe is the abstraction provided by the model classes argued about in the previous paragraph, but i'm having a hard time putting my finger on what that is. i have a better sense of what might be the api (which, in our case, is http restful), in that users invoke the api with what they want to do, distinct from what they are allowed to do, and how it gets done.tl;dr: what kinds of things can or should go in a method in a mapped class when using an orm, and what should be left out, to live in another layer of abstraction?",
    "present_kp": [
      "orm",
      "abstraction",
      "business logic"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "join two bezier curves so that the result is two-times continuously differentiable. i have a task to join two bezier curves, so that the resulting curve is two-times continuously differentiable.i have the cubic bezier curve c with control points:c0 = (1,1)c1 = (3,4)c2 = (7,5)c3 = (8,2)i shall continue this curve c with curve d from control point c3 to a control point d3 = (12,1) so that this curve is two-times continuously differentiable.first task: determine control points d0, d1, d1 for the new curve.second task: specify a pecewise defined formula for the new curve g(v) with v out of [0,1] that passes through c0,c3 and d3. thus connect curves c and d in v = 1/2.third task: prove by calculation that the transition between c and d is two-times continuously differentiable.regarding first task: i don't know how to determine the points. can someone help to do this? the rest of the tasks is then maybe something easier to do for me.",
    "present_kp": [
      "bezier curve"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "is it possible to create an open group for a facebook page?. this group should be an integrated part of the facebook page (cf. page events) and be represented as a tab in the tab menu.does facebook provide such a feature for facebook pages?",
    "present_kp": [
      "facebook",
      "facebook pages"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "qpcr: why is fold change and standard deviation calculated after transformation?. i am analyzing data from a quantitative polymerase chain reaction (qpcr) using r. after cleaning the raw data, it looks something like this:> dput(x)structure(list(reporter = c(fam, fam, fam, fam, fam, fam, fam, fam, fam, fam, fam, fam, vic, vic, vic, vic, vic, vic, vic, vic, vic, vic, vic, vic), number = c(1l, 2l, 3l, 4l, 5l, 6l, 7l, 8l, 9l, 10l, 11l, 12l, 1l, 2l, 3l, 4l, 5l, 6l, 7l, 8l, 9l, 10l, 11l, 12l), a = c(22.19, 22.24, 22.5, 22.54, 22.6, 22.59, 23.07, 23.46, 22.43, 22.74, 24.09, 23.91, 24.52, 25.03, 25.25, 25.82, 25.13, 24.71, 25.34, 25.85, 25.25, 26.15, 25.81, 25.29), b = c(21.72, 21.78, 22.86, 22.73, 19.88, 20.07, 21.06, 21.06, 20.96, 21.11, 19.46, 19.43, 24.75, 24.69, 25.64, 25.19, 23.76, 23.69, 24.35, 25.05, 24.1, 23.81, 22.81, 23.13), c = c(21.37, 21.56, 20.07, 20.01, 21.17, 21.08, 20.54, 20.36, 33, na, na, na, 23.91, 24.31, 23.61, 23.88, 24.33, 24.31, 23.37, 23.53, 33, na, na, na), e = c(26.26, 27.33, 25.93, 26.56, 25.76, 23.03, 24.72, 25.27, 24.43, 24.31, 26.98, 23.33, 24.04, 25.02, 25.1, 25.1, 24.68, 25.48, 25.87, 26.22, 25.35, 25.36, 25.11, 25.98), f = c(25.81, 26.9, 25.58, 26.61, 25.06, 21.85, 23.59, 24.04, 23.19, 23.19, 25.17, 20.8, 24.12, 24.26, 25.32, 25.25, 24, 23.78, 24.7, 24.48, 23.52, 23.87, 23.05, 23.05), g = c(26.12, 27.02, 24.08, 25.15, 25.99, 23.18, 24.2, 24.05, 33, na, na, na, 23.47, 23.45, 23.7, 23.74, 24.46, 24.19, 23.56, 23.53, 33, na, na, na)), .names = c(reporter, number, a, b, c, e, f, g), class = c(tbl_df, tbl, data.frame), row.names = c(na, 24l))i think that each well measures two genes: target which is 12 different genes (color fam), and the reference or housekeeping gene, gapdh (color vic). also, control is triplicate a, b, c (3 x 12 wells), and treatment is e, f, g (3 x 12 wells). i have created a function ddct_ for analyzing the data with the ddct algorithm (livak & schmittgen, 2001). it takes one argument x which is a data.frame of the form exemplified above. ddct_ <- function(x) { # subset x by control/treatment and target/reference # then, calculate ct averages for each triplicate tc <- x %>% select(reporter, number, a, b, c) %>% filter(reporter == fam) %>% rowwise() %>% mutate(ct_avg = mean(c(a, b, c), na.rm = true)) rc <- x %>% select(reporter, number, a, b, c) %>% filter(reporter == vic) %>% rowwise() %>% mutate(ct_avg = mean(c(a, b, c), na.rm = true)) tt <- x %>% select(reporter, number, e, f, g) %>% filter(reporter == fam) %>% rowwise() %>% mutate(ct_avg = mean(c(e, f, g), na.rm = true)) rt <- x %>% select(reporter, number, e, f, g) %>% filter(reporter == vic) %>% rowwise() %>% mutate(ct_avg = mean(c(e, f, g), na.rm = true)) # normalize ct of the target gene to the ct of the reference gene dct_control <- tc$ct_avg - rc$ct_avg dct_treatment <- tt$ct_avg - rt$ct_avg # normalize dct of the treatment group to the dct of the control group ddct <- dct_treatment - dct_control # calculate fold change fc <- 2^(-ddct) # calculate avg and sd dct_control_avg <- mean(dct_control) dct_control_sd <- sd(dct_control) dct_treatment_avg <- mean(dct_treatment) dct_treatment_sd <- sd(dct_treatment) # create output df <- data_frame( sample = 1:12, dct control = dct_control, dct treatment = dct_treatment, ddct = ddct, fold change = fc, dct control avg = dct_control_avg, dct control sd = dct_control_sd, dct treatment avg = dct_treatment_avg, dct treatment sd = dct_treatment_sd ) %>% round(2) write.csv(x = df, file = result.csv) saverds(object = df, file = result.rds) df}however, i am not a biochemist or molecular biologist, so i am unsure of the basic concepts involved and the overall approach. so, my questions are:is the function correct?why is it important to calculate fold change and standard deviation after normalization?",
    "present_kp": [
      "r",
      "normalization"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "what should i use columns for, and what should i use rows for in sql?. in sql, what's the standard convention for what rows and columns should be used for? for instance, if i need to make a table of users and their data, which axis (rows or columns) should be the users labels and which axis should be the users data labels?edit: the free heroku plan only allows a maximum of 10000 rows on the sql table, so will it make a difference if i switch the table axises around and use a very large number of columns instead of rows?",
    "present_kp": [
      "sql",
      "heroku"
    ],
    "absent_kp": [
      "database",
      "database design"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is it possible to import (paste) images into draw.io?. while i love the rich image set readily available to me, i'd like to include custom images (my drawings, photos...) in draw.io documents. is it possible?i've tried ctrl+v with a clipboard image but with no success.",
    "present_kp": [
      "images",
      "draw.io"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "filtering issues without certain tag on github. does the current github issue search engine provide any way for users to search for reports without certain tag?classical use-case would be having a granular grouping such as [bug], [enhancement], [feature] etc. i'd like to search for all items without the [bug] tag.",
    "present_kp": [
      "search",
      "github"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how should a robots.txt be for a wordpress site in order to be mobile-friendly ?. i run a site built on wordpress and my question kind of sums up what i am looking for. there has been an update on google according to which mobile-friendliness of a site is now considered as a ranking factor. since my site traffic has taken a significant dive i took a mobile-friendly-test. although my site resizes pretty well in mobile devices, the test result still says my site is not mobile-friendly. on a closer look, the result also says - this page uses 20 resources which are blocked by robots.txt. and on further reading of this page i found that blocked resources are a part of the mobile-seo. it seems like googlebot needs access to css and js files of a site in order to determine the mobile-friendliness of a site. on the other hand i read long back that for a wordpress site wp-content folder should be blocked. so currently i have my robots.txt as given below. i am wondering on what would be the correct contents in a robots.txt if it is a wp-site, so my site could be read by the mobile-friendly tool as a complying site. also is it okay to allow access to entire wp-content folder ? this is what my robots.txt looks like.user-agent: *disallow: /cgi-bin/disallow: /jwp/disallow: /wp-admin/disallow: /wp-includes/disallow: /wp-content/disallow: /trackback/disallow: /feed/disallow: /comments/disallow: /xmlrpc.phpdisallow: /*?sdisallow: /search/allow: /wp-content/themes/mysite/style.csssitemap: <url>",
    "present_kp": [
      "robots.txt"
    ],
    "absent_kp": [
      "googlebot mobile"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "find out how much all tmp directy occupy. how can i know the size of all tmp folders for all users. i tried du -h /home/*/tmpbut it did not add them all together.",
    "present_kp": [],
    "absent_kp": [
      "disk usage"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "keeping your text entry fields secure. i've noticed over the past few days someone has been entering text like the following into my comment fields :<a href=<url> unendurable imperfections <a href=<url> side effects</a> eolith acidimeter what is this person trying to achieve, and is there anything i need to do to ensure they don't do anything malicious through my text entry fields?editit looks like this site belongs to the brazilian government. other sites that have been linked don't even exist. hence i am a bit skeptical that this is just link spamming..",
    "present_kp": [],
    "absent_kp": [
      "security"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "beating naive dynamic programming: examples similar to integer partitions?. let $p(n)$ denote the number of partitions of $n\\in\\mathbb{n}$ (briefly, number of ways to split a pile of $n$ stones into $\\geq1$ unordered nonempty parts). the classical dynamic programming algorithm to find $p(n)$ is to construct a square table $a$ where$$a_{i,j} = ext{partitions of $i$ where each part is $\\leq j$}$$and recursively fill it using the rules $$a_{i,j} = a_{i,j-1} ext{ if }j>i$$and $$a_{i,j} = a_{i,j-1} + a_{i-j,j} ext{ otherwise}$$with the appropriate conventions for the corner cases. then $p(n)=a_{n,n}$. this takes $o(n^2)$ operations on integers (let's say we're looking for $p(n)$ modulo some big number, so the size of the numbers is $o(1)$). we can optimize the memory down to $o(n)$ by noticing that we only need the previous column to find the next one.however, the pentagonal number theorem due to euler says that $$p(n) = \\sum_{k\\in\\mathbb{z}:g_k\\leq n}(-1)^{k+1}p(n-g_k)$$where $g_k = k(3k-1)/2$ are the pentagonal numbers. this allows us to recursively build a one-dimensional list of the $p(i)$, using only $o(n\\sqrt{n})$ operations, since the above sum contains $o(\\sqrt{n})$ terms.my question: i was wondering if there are other examples of natural problems for which there is such a surprising speed-up over the standard dynamic programming algorithm, or if this is more of an isolated case complexity-wise.",
    "present_kp": [
      "dynamic programming"
    ],
    "absent_kp": [
      "cc.complexity theory",
      "ds.algorithms"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "semantic action in parser combinators. i have a parser that can be defined as int = a:/[0-9]+/ {parseint(a)}. it is supposed to match an int and convert it into a number. there can also be a parser like sum = a:int b:int {a+b}as you can see, it is a series of tokens, which means that they must be parsed as an array of tokens. when we start such an array, i allocate a scope for the variables. the trouble however is that the chain includes the semantic action. parsing 1 2 with int int would produce [1,2] and semantic action a+b, which is also a parser, replaces that with [3], three in the array, because sum is a chain of parsers and its result must be an array. but, normal user wants pure result 3. how do i solve the problem?",
    "present_kp": [
      "parser combinator"
    ],
    "absent_kp": [
      "semantics"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "l contains the concatenations of all k-bit long strings. why is it decided in pspace(loglogn)?. (this exercise is from computation complexity: a conceptual perspective by oded goldreich):for any k $\\in \\mathbb{n}$, let $w_k$ denote the concatenation of all k-bit long strings (in lexicographic order) separated by *'s (i.e, $w_k = 0^{k-2}00*0^{k-2}01*0^{k-2}10*...*1^k$). show that the set $s = \\{w_k:k\\in\\mathbb{n}\\} \\subset \\{0,1,*\\}$ is decidable in double-logarithmic space.the basic idea of the solution, if my understanding of it is correct, is:iterate from i to k until the structure of the input suits some $w_k$, for example $\\{0,1\\}^k*\\{0,1\\}^k*...*\\{0,1\\}^k$.for each pair of adjacent binary numbers, make sure $b_{i+1} = b_i + 1$according to the guideline in the book, step 2 takes space $o(log(k))$.why do i need $log(k)$ space to check if $b_{i+1} = b_i + 1$? can't it be done in constant space?",
    "present_kp": [],
    "absent_kp": [
      "complexity theory",
      "complexity classes",
      "space complexity"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to reliably get the pid associated to a window?. i decided to get the completely unused pause key on my keyboard to actually pause things so i wrote a pause <pid> shell function to toggle the state a process and assigned pause $(xdotool getwindowfocus getwindowpid) to the key with sxhkd.the problem is that xdotool and other utilities i have tried rely on the _net_wm_pid property which is really unrealiable in my setup: i use firejail for the majority of programs and in that case it seems to always return the pid 3 while for games and other binaries running in a chroot environment the property it's not even set.is there any other method to associate a pid to a window? the x.org server must have this information, or does it?",
    "present_kp": [
      "process",
      "window"
    ],
    "absent_kp": [
      "xorg"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "transfer my m.website to my responsive website without losing my se ranking. until last month i was using two versions of my website: one for desktop and one for mobile. (m.example.com / example.com) a few weeks ago i made my desktop version responsive.in order to redirect my search engine traffic, i simply added a .htaccess on my mobile version which redirects m.example.com/path toexample.com/path. i also let the headers links rel=canonical and rel=alternate on both versions.after a few weeks, my m.website is still showing up in the search results and the traffic is still counted in my m.website google search console.i would like to know what are the next steps to definitely tell google than my website has moved to a responsive design without losing my ranking.",
    "present_kp": [
      "transfer"
    ],
    "absent_kp": [
      "seo",
      "serps"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "algorithm for grouping identical neighbors in a list. i have a list that i want to reduce to a smaller list by grouping identical neighbors. this list has many many redundant entries. example list:1,1,1,1,1,2,2,2,3,3,1,1,1,2,2,2,2,2,2,2,2,2,4,4,4,4,1,1,1,1,1,1after 'compression'1,2,3,1,2,4,1is there an algorithm for doing this that is faster than o(n)? in other words, faster than just looking at the next neighbor?while (1) n = 0, m = 1 if (list[n] == list[m]) remove list[m] else n = m m = m + 1 if (m > list.size) breakif not, what about this problem makes it o(n)?",
    "present_kp": [],
    "absent_kp": [
      "algorithms",
      "time complexity",
      "data compression",
      "lists"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "coming up with a valid ranking algorithm for articles. i'm creating a website which will publish one or more articles a week.i'd like to be able to rank the articles based on popularity. in order to do this, i need a formula to calculate a score for each article. this formula should reward popular articles (those that receive many views), while also bringing new articles closer to the top in order to keep the content fresh and interesting.the website doesn't support voting on the articles. the readers will be able to comment and i can have the site track the amount of unique views the article gets.my attempti have come up with a way to calculate a score for each article but i'm not sure if it's the right way to do it, or if it will work at all.this formula calculates the score:score = unique_views / ( hours_since_release * 4 )the formula keeps articles with many views afloat, while also making sure that they'll eventually subside to leave space for new articles.i wrote a small program to test this formula. here is the output of one run. this script generates between 0 and 2 articles every day, and simulates their score as time progresses.when an article is created, it chooses a random number (m) between 100 and 1000, representing its popularity. every day, the script adds a random number of views between 0 and m to each article.the actual views shown in the output (v) are the result of this formula:view_count = ( 10 * log10( view_count ) ) ^ 2i used this to make sure the view count doesn't keep growing forever. instead, its growth will slow down as time goes, as it might in real life.the values a and s in the output represent respectively the age in days of the article and its score, based on the formula shown above.this formula causes some interesting behavior: if you look at the game tomb raider: anniversary, it spawns with a big mnumber, which means it's going to be very popular. it gets to the top on the first day, goes down in the next 2 days and then comes back to the top for 3 days in a row due to the amount of views it gets.this seems to be the behavior i'm looking for, but i'm not sure if it will work in practice, or even if this is the right approach at all.you can find the source of my rough prototype here if you'd like to run it yourself. here comes the question: how can you come up with and test ranking algorithms so that they satisfy the conditions of the problem, using the available variables?secondary question: is my ranking algorithm appropriate, and would it work in the situation i described?edit: from the links in maythesource.com's answer, i just realized there's also the issue of score gaming. would only considering unique visits somehow prevent gaming, or is it a weak countermeasure?",
    "present_kp": [
      "algorithms",
      "articles"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "reading command prompt output into variables - gvim and windows. first off, i'm writing this for my setup at work, which is entirely on windows. i'd imagine it would be quite a bit easier if i was able to use a proper terminal.i've written a function in my vimrc that lets me run a syntax checker for the obscure programming language we use here. i'd like to read the output from that syntax checker into a variable in the function, which i can then use to set some linenumber marks or output to a temp buffer (like syntastic).i browsed through the helpfile and found redir =>, but it doesn't seem to work like i'd expect it to. syntax check for progress =========================function! checksyntaxcustom() if (&ft=='progress') redir => progress_output :execute '0read !c:\\progress\\openedge11_3in\\_progres.exe -1 -b -mm 16384 -pf c:\\progress\\openedge11_3\\startup.pf -p c:\\code\\custom\\syntax.p -param %:p' redir end :echom progress_output endifendfunction seems like i can't manipulate progress_output. ideally i'd like it to return a string that i can then use with :execute, but right now it just pastes the command prompt output to the line below the cursor. i'm fairly new to vimscript so i could be completely wrong on what redir should be used for. any advice on how to accomplish what i'm looking to do?",
    "present_kp": [
      "vimscript",
      "gvim"
    ],
    "absent_kp": [
      "microsoft windows"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "mutt: how to categorize incoming emails. i have a gmail account, mutt is configured to get the mail through imap. yesterday i subscribed to a mailing list and now my personal emails are mixed up with the ones from the list.the list emails are addressed to me and <email>. how can i tell mutt to move all such emails to a separate file, so they wouldn't be mixed with my emails. but i still could read them, opening that file?",
    "present_kp": [
      "email",
      "mutt",
      "imap"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "using sounds in java. i'm trying to apply sound today, and the goal in mind was a simple violin tuner. the actionlistener seems repetitive/ how might i optimize it (what i tried just broke everything)? i also noticed the sounds bleed into each other. i don't mind that so much for the different keys (though i'm thinking of a toggle option for this) but it also is the case for the same key, which is undesirable. i'm not sure how to handle that./* author: luigi vincenta simple violin tuner, emits g, d, a, and e sounds to facilitate violin tuning*/import javax.swing.*;import javax.sound.sampled.*;import java.awt.*;import java.awt.event.*;import java.io.*;public class violintuner { public static void main(string[] args) { jframe frame = new jframe(violin tuner); // get and set icon for the program imageicon icon = new imageicon(images/icon.png); frame.seticonimage(icon.getimage()); // buttons final jbutton g_key = new jbutton(g); final jbutton d_key = new jbutton(d); final jbutton a_key = new jbutton(a); final jbutton e_key = new jbutton(e); // listener to play sounds on click actionlistener violinsounds = new actionlistener() { @override public void actionperformed(actionevent e) { if (e.getsource() == g_key) { try { audioinputstream audioinputstream = audiosystem.getaudioinputstream(new file(sounds/g.wav).getabsolutefile()); clip clip = audiosystem.getclip(); clip.open(audioinputstream); clip.start(); }catch(exception x) { x.printstacktrace(); } } else if (e.getsource() == d_key) { try { audioinputstream audioinputstream = audiosystem.getaudioinputstream(new file(sounds/d.wav).getabsolutefile()); clip clip = audiosystem.getclip(); clip.open(audioinputstream); clip.start(); }catch(exception x) { x.printstacktrace(); } } else if (e.getsource() == a_key) { try { audioinputstream audioinputstream = audiosystem.getaudioinputstream(new file(sounds/a.wav).getabsolutefile()); clip clip = audiosystem.getclip(); clip.open(audioinputstream); clip.start(); }catch(exception x) { x.printstacktrace(); } } else if (e.getsource() == e_key) { try { audioinputstream audioinputstream = audiosystem.getaudioinputstream(new file(sounds/e.wav).getabsolutefile()); clip clip = audiosystem.getclip(); clip.open(audioinputstream); clip.start(); } catch(exception x) { x.printstacktrace(); } } } }; // register buttons to listener g_key.addactionlistener(violinsounds); d_key.addactionlistener(violinsounds); a_key.addactionlistener(violinsounds); e_key.addactionlistener(violinsounds); // buttons to panel jpanel p = new jpanel(); p.add(g_key); p.add(d_key); p.add(a_key); p.add(e_key); // panel to frame frame.add(p); frame.setdefaultcloseoperation(jframe.exit_on_close); frame.setsize(225, 75); frame.setlocationrelativeto(null); frame.setvisible(true); }};",
    "present_kp": [
      "java",
      "audio"
    ],
    "absent_kp": [
      "beginner",
      "event handling"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "100% open source linux distro. do 100% open source linux distro's exist? i.e. distros which contain absolutely no closed source components anywhere at all? apparently distros like ubuntu contains bits and pieces which are closed source.please note, i am not asking for 100% free software based linux distribution, i am specifically asking for 100% open source linux distributions, distributions which have absolutely nothing within them which is closed source.",
    "present_kp": [
      "distros",
      "open source"
    ],
    "absent_kp": [
      "distribution choice",
      "opensource projects"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "vehicle routing problem over manhattan distances. i am looking for references to the variant of the vehicle routing problem over manhattan distance metric where the aim is to optimize the number of tours starting at the depot.is the following problem np-hardproblem :the distance constrained version of the vehicle routing problems over manhattan distance metric considering an complete $n imes n$ grid or a grid where some edges are missing ?the grid graph is a weighted one.",
    "present_kp": [],
    "absent_kp": [
      "cc.complexity theory",
      "reference request",
      "graph theory",
      "np hardness",
      "optimization"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "jre8 installation error on aix server 7.1. i am trying to install java jre 8 on my aix server 7.1. under the /tmp/java_install directory i have the jre installation files, and i am using smitty install_all to install this jre.# pwd/tmp/java_install# ls.toc 1 java8_64.jre installp.log# smitty install_allon smitty, the very first option (input device / directory for software) i have selected [./] and chosen the jre8 package on the other option: software to install.after accepting the license agreement i started the installation process, but unfortunately i get an error like this:command: failed stdout: yes stderr: nobefore command completion, additional instructions may appear below.[more...60]inurest: failure on system call to execute command /usr/sbin/restbyname -s -xyaqf/datafs/jre8/java8_64.jre -z /tmp/inutmptmnaea/sorted.al.installp: the installation has failed for the usr part of the following filesets: java8_64.jre 8.0.0.402installp: cleaning up software for: java8_64.jre 8.0.0.402 finished processing all filesets. (total time: 26 secs).please mount volume 2 on /datafs/jre8/java8_64.jre... and press enter to continue inurest: error in restoring filesinurest: failure on system call to execute command /usr/sbin/restbyname -s -xyaqf/datafs/jre8/java8_64.jre -z /tmp/inutmptmnaea/sorted.al.finished processing all filesets. (total time: 27 secs).+-----------------------------------------------------------------------------+ summaries:+-----------------------------------------------------------------------------+installation summary--------------------name level part event result-------------------------------------------------------------------------------java8_64.jre 8.0.0.402 usr apply failedjava8_64.jre 8.0.0.402 usr cleanup success",
    "present_kp": [
      "aix"
    ],
    "absent_kp": [
      "software installation"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is the recommended way to retrieve host and repository from git url. think of a utility like gerrit-sh which uses git config options to determine where the gerrit host is located. the context for my question is this function get_repo_remote_config(). so i want to retrieve the repository and more important the host name of the git url configured in .git/config.the method assumes you have your username, host, port and repo all inside your git config. which ignores that you can use ssh aliases.what is a recommended way to eithertell the host and repo for a git urlfetch the antialiased url based on the git url which might contain ssh aliasesfor (1) i can think of splitting on the first : or / with some additional magic.for (2) i could think of something like in this question: awk.both approaches would work, but isn't there a more convenient way to do that? isn't there a utility for some ssh/url calculation?thanks in advance.",
    "present_kp": [
      "ssh",
      "git",
      "url"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "mapping scancodes to keycodes. the archwiki article map scancodes to keycodes statesmapping scancodes to keycodes is universal and not specific to linux console or xorg [...]while the archwiki article extra keyboard keys (which the former article suggests to read) statesnote that the keycodes are different for linux console and xorg.which of the two is true? or am i getting something wrong and it is no contradiction at all?",
    "present_kp": [
      "xorg",
      "keyboard"
    ],
    "absent_kp": [
      "keyboard layout"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "combining binary features with textual ones. i have a binary feature that i want to use it with textual features i.e. unigrams. i use logistic regression and tf/idf for representing text. so i simply add a unique feature, say ss or oo, to text of each instances. but in practice, i see adding more number of these features to instances, say two oo or ss or more get me a better results. what is the reason? how these weights improve the classification results? should not logistic regression can get more weights to this features instead of weighting them by hand?",
    "present_kp": [
      "classification",
      "logistic regression"
    ],
    "absent_kp": [
      "text mining"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "do we have a responsiblity to improve old code?. i was looking over some old code that i wrote. it works, but it's not great code. i know more now than i did at the time, so i could improve it. it's not a current project, but it's current, working, production code. do we have a responsibility to go back and improve code that we've written in the past, or is the correct attitude if it ain't broke, don't fix it? my company sponsored a code review class a few years ago and one of the major takeaways was that sometimes it's just good enough; move on.so, at some point should you just call it good enough and move on, or should you try to push projects to improve old code when you think it could be better?",
    "present_kp": [],
    "absent_kp": [
      "code quality",
      "code reviews"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "quasi-polynomial time algorithm for permutation group isomorphism. is there a known $n^{lpha \\log n+o(1)}$ algorithm for permutation group isomorphism? here $n$ is the size of the group, and the isomorphism must be a permutational isomorphism.my hope for such an algorithm comes from reading a blog post on the group isomorphism problem and its comments. because any group of size $n$ has a generator set of size at most $\\log_2 n$, maybe a permutation group will even have a strong generator set of size at most $\\log_2 n$ (and hopefully that would already be sufficient for a quasi-polynomial algorithm). the comments mention two papers that relate the complexity of the problem of permutational isomorphism of permutation groups to the complexity of the problem of isomorphism of semisimple groups, and also mention earlier work on the permutational isomorphism problem.a comment by ben barber on a related question indicates that permutation group isomorphism can be reduced to graph isomorphism, which is not really surprising, but nice to know nevertheless.",
    "present_kp": [
      "graph isomorphism"
    ],
    "absent_kp": [
      "cc.complexity theory",
      "gr.group theory"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "closest pair of points between two sets, in 2d. i have two sets $s,t$ of points in the 2-dimensional plane. i want to find the closest pair of points $s,t$ such that $s \\in s$, $t \\in t$, and the euclidean distance between $s,t$ is as small as possible. how efficiently can this be done? can it be done in $o(n \\log n)$ time, where $n = |s|+|t|$?i know that if i'm given a single set $s$, then it's possible to find the closest pair of points $s,s' \\in s$ in $o(n \\log n)$ time using a standard divide-and-conquer algorithm. however, that algorithm doesn't seem to generalize to the case of two sets, because there's no connection between the distance between the two closest points within $s$ or $t$ vs. the distance between the two closest points across those sets.i thought of storing the set $t$ in a $k$-d tree, then for each $s \\in s$, using a nearest-neighbor query to find the closest point in $t$ to $s$. however, the worst-case running time of this might be as bad as $o(n^2)$ time. there are results saying that if the points of $t$ are randomly distributed, then the expected running time for each query is $o(\\log n)$, so we'd obtain an algorithm with expected running time $o(n \\log n)$ if we were guaranteed that the points are randomly distributed -- but i'm looking for an algorithm that will work for any collection of points (not necessarily randomly distributed).motivation: an efficient algorithm would be useful for this other question.",
    "present_kp": [],
    "absent_kp": [
      "algorithms",
      "computational geometry"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "adding a pattern to each field of the following line. i need some help here please. i have the following input:mx04a;dmx04a; dmx04a; lmx04a; lmx04a;-17.2; -15.3; -14.3; -13.6;-16.8; -15.4; -16.0; -15.3;lh36a;dlh36a; dlh36a;-11; -117.2;-11; -17.5;i want to get this output mx04a:dmx04a; mx04a:dmx04a; mx04a:lmx04a; mx04a:lmx04a; -17.2; -15.3; -14.3; -13.6; -16.8; -15.4; -16.0; -15.3; lh36a:dlh36a; lh36a:dlh36a; -11; -117.2; -11; -17.5;",
    "present_kp": [],
    "absent_kp": [
      "text processing",
      "sed"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to get switch mac address if it's a layer2 one?. arp -adoesn't show it. how can i get the mac of it then? (on ex.: from a linux)",
    "present_kp": [
      "mac address"
    ],
    "absent_kp": [
      "networking"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "does oxytocin reduce brain activity?. in the book psychologie by richard j. gerrig, philip zimbardo i read there are studies claiming that: die gegenwrtige forschung legt nahe, dass das hormon eine facettenreiche rolle bei persnlichen und sozialen prozessen spielt (ishak et al., 2011) translation: recent studies suggest that the hormone plays a diverse role in personal and social processesanother study was carried out in order to examine the effects of oxytocin on trust. in this study, half of subjects where given a dose of oxytocin, and the other half not. furthermore, they carried out fmrt scans to examine the areas on which the oxytocin had an effect on. the study showed that: die scans ergaben, dass oxytozin-teilnehmer weniger aktivitt in gehirnregionen aufwiesen, die wie amygdala mit furchtreaktionen in zusammenhang stehen. (baumgartner et al., 2008)translation: the scans suggested that participants who were given a dose of oxytocin showed less brain activity in regions which are, like the amygdala, associated with fear. unfortunately, i do not have access to the paper where this seems to be described (baumgartner et al. 2008).to what extent does oxytocin reduce brain activity and in which regions?baumgartner, t., heinrichs, m., vonlanthen, a., fischbacher, u., & fehr, e. (2008, may 22). oxytocin shapes the neural circuitry of trust and trust adaptation in humans.",
    "present_kp": [
      "fear",
      "oxytocin"
    ],
    "absent_kp": [
      "neurobiology"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "haskell and lisp vs. haskell or lisp. i currently code with c, c++, and python. i'm wanting to pick up a functional programming language, and right now i'm leaning toward haskell. i do not want to start a haskell vs lisp war here; what i want to know is this: if i learn haskell primarily for exposure to functional programming, what benefits, if any, will i gain from later learning lisp?",
    "present_kp": [
      "learning",
      "functional programming",
      "haskell",
      "lisp"
    ],
    "absent_kp": [
      "paradigms"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "seemingly inconsistent behavior for ln & ln -s. as we all know, the ln command creates a link, with the default being a hard link and the -s option creating a symlink. the general syntax is ln [-s] old new, where old is the file you are linking to and new is the new file you are creating. hard links can not be created for directories, as a hard link could be created between folders inside each other & i suppose computers do not yet have the resources to check for this without a serious slowdown.when creating the link, the path of both files must be written out, and can be absolute or relative. you can mix relative & absolute filepaths, i.e. have a relative path for the new file/folder & an absolute path for the old one. when creating a hard link with a relative path, the paths of both files are relative to the current folder, while for a symbolic link the path of the linked-to file/folder is relative to its parent folder but the path of the old file/folder is relative to the current folder. why this is is relative to my question.for example, say we am in the home folder, /home/user, also known as ~, and create 2 folders, new and new2, with the file file in the folder new. if we try ln -s new/file new2/file, the result is a broken link from ~/new2/file to the currently nonexistent ~/new2/new/file. however, if we instead run ln -s ../new/file new2/file, we get the expected result, which is a link from ~/new2/file to ~/new/file.so, my question:why is the file path for the old file/folder of a symlink relative to its parent, while the other 3 paths (hard link old, new files, symlink new file/folder) are relative to the current folder?all this is on fedora, but i'm sure it applies to most unix-based os's.edit e carter young seems to hit the nail on the head with regard to my 2nd question (as well as my 1st question, which was wrong anyway). it seems that for a symbolic link, the target doesn't have to exist yet, so the system has to make its path relative to the link rather than the current directory. however, why can't the shell parse out that path when you're running the command, rather than forcing the user to figure out what the path is & enter it him/herself? the shell seems to parse pretty well, so is this a case of legacy issues? performance issues? what?",
    "present_kp": [
      "symlink",
      "hard link",
      "ln"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "getting capabilities of my cd/dvd drive when wodim --devices doesn't work. i used to be able to run either commands successfully on my fedora 14 thinkpad t410 laptop:$ wodim --scanbus$ wodim --deviceshowever since upgrading to fedora 19 this no longer seems to work and i'm not sure why. are there any alternative methods for getting the drives capabilities from wodim?",
    "present_kp": [],
    "absent_kp": [
      "linux",
      "hardware",
      "data cd"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "wireless connection connecting and disconnecting all the time. i am running ubuntu 11.04 on my laptop and i use a 3g usb modem for my internet connection.i wanted to share this internet connection with my girlfriend's laptop that is running window xp. basically, i created the the wireless connection on my laptop and now it just connects and disconnects all the time. why is it doing this? i am connected to the internet via the 3g usb modem while i am also trying to connect the wireless set up, could that be the reason? is there anything i should do on the windows computer during the wireless set up?",
    "present_kp": [
      "ubuntu"
    ],
    "absent_kp": [
      "wifi",
      "connection sharing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "can message passing help in gi related problems?. can message passing algorithms like those used in <url> be useful in showing gi testing is in p?note message passing is prominent in ai and has been tried in decoding problem for linear codes which is an np hard problem and natural obstructions have been identified. however gi cannot be np hard under some reasonable hypothesis and possibly message passing could be a reasonable approach.",
    "present_kp": [],
    "absent_kp": [
      "big picture",
      "big list"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "resizing xterm window kills django running in it. i have a django instance running in a terminal window. any time i resize the window, the django process dies ungracefully, leaving no record of it in the error_log file.i tested it within ratpoison, xmonad and mwm. both xterm and rxvt-unicode are affected. hell, even putty is affected!",
    "present_kp": [
      "terminal",
      "django"
    ],
    "absent_kp": [
      "x11"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "ubuntu machine randomly reboots. i have a ubuntu machine which randomly reboots itself:last rebootreboot system boot 2.6.32-042stab12 sat mar 25 23:48 - 12:42 (11:53)reboot system boot 2.6.32-042stab12 wed mar 22 18:28 - 12:42 (3+17:14)reboot system boot 2.6.32-042stab12 sat mar 18 05:10 - 12:42 (8+06:32)reboot system boot 2.6.32-042stab12 thu mar 16 13:52 - 12:42 (9+21:49)reboot system boot 2.6.32-042stab12 thu mar 16 09:50 - 13:52 (04:02)i am trying to figure out why and stop it. i pasted the logs to pastebin here.i am not doing these reboots. also, it seems like there is nothing going on (like a crash, etc.) which would cause the server to reboot.",
    "present_kp": [
      "ubuntu",
      "reboot"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to exclude traffic with a url query string from google analytics using google tag manager. i tried:create variable: variable type: urlcomponent type: queryquery key: ignoreme, url source: page url/defaultcreate trigger:trigger type: page viewthis trigger fires on: some page viewsfire this trigger when an event occurs and all of these conditions are true: ignoreme does not equal 1however, when i go to example.com?ignoreme=1, the google analytics still counts it as a visit and shows me as a visitor. i have published the changes in gtm and deleted my browser cookies.",
    "present_kp": [
      "google analytics",
      "google tag manager",
      "query string"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "is lvm mirroring supported on rhel 5?. we have sw md raid1 on a rhel5. raid devices come from multipath currently.question: is it possible to convert this setup to lvm mirroring? does rhel 5 supports mirroring in lvm? just like in a raid1? or raid1 like lvm mirroring is only from rhel 6?",
    "present_kp": [
      "lvm"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "should jpeg encoding take into account gamma?. when an image is encoded using jpeg, the rgb pixels are first encoded into yuv, and then the uv channels are downsampled. before actually doing the dct and encoding the coefficients, jpeg doesn't gamma correct the y channel. is this correct? shouldn't we determine the dct coefficients that will most affect our viewed result?",
    "present_kp": [
      "gamma"
    ],
    "absent_kp": [
      "compression"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is the controller in django mvc?. learning django mvc and the way i thought of it is:models are the database tables represented in django as python classes.views are the html returned from function in views.py.controllers are the actual functions themselves in views.py invoked from a http request. however i read on wikipedia (at the time of writing): ... a regular-expression-based url dispatcher (controller).i would have thought the mapping of urls to functions as routing - not the controller. but perhaps i am wrong - i guess i got my ideas because is asp mvc the functions that handle the requests are contained in classes called contollers...",
    "present_kp": [
      "mvc",
      "django"
    ],
    "absent_kp": [
      "terminology"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to change a single line in multiple files. i have ~750 .php files in the same directory that contain the lineinclude(path/to/file.php);i would like to change this line in every file torequire_once(path/to/file.php);what would be an efficient method to do so? so far i have tried the following sed command with no luck: sed 's#include(path/to/file.php);#require_once(path/to/file.php);#' *.php",
    "present_kp": [
      "sed",
      "php"
    ],
    "absent_kp": [
      "regular expression"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why kernel modesetting, instead of privilege separation?. kernel modesetting was kind of painful to get on linux at first, but now it's pretty awesome to have. i mean, x not need to run as root? high-res hardware accelerated consoles? cool stuff.problem is, a lot of unix platforms don't have modesetting kernel drivers of any sort. so hardware that relies on kms is now mostly limited to linux.my question: why actually implement this in the kernel?if hardware access is needed to set the screen resolution, why not use a separate privileged daemon, or a small setuid binary? that would maintain the advantage of separating out the privileged code, and letting the display server run as limited user; while getting rid of the special driver requirement, and making cross-unix support easier. right? or am i missing something significant here?",
    "present_kp": [
      "kernel",
      "drivers"
    ],
    "absent_kp": [
      "xorg"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "edit distance in sublinear space. what is the best known complexity for computing the exact edit distance between two strings of the same length using working space which is sublinear in the size of the input? i assume the input is stored in some read-only format. is this a previously studied problem?to make the question a little more specific, how about $\\theta(\\sqrt{n})$ space where $n$ is the length of each input string.edit. following the answer of david eppstein, it seems a good question is simply if the edit distance can be found in polynomial time and $\\theta(\\sqrt{n})$ space. any lower bounds would also be interesting.",
    "present_kp": [
      "edit distance"
    ],
    "absent_kp": [
      "ds.algorithms",
      "space time tradeoff"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "does mvc pattern negate other design patterns?. so usually when working with the mvc you have a controller that controls the input a model that process it and makes it ready for the user and a view that display the result to the user.now when creating this pattern you seperate the code into their relevant place. for instance the controller code goes into the controller, the gui code goes into the view and so on.now my question is if we look at all of the design patterns out there for instance the observer pattern. how would you apply such pattern to a code structure that already implements the mvc pattern? for that case many of the other patterns aswell such as composite, factory and command pattern?doesnt the structure of the mvc pattern make it harder to implement other good pratice design patterns?",
    "present_kp": [
      "design patterns"
    ],
    "absent_kp": [
      "object oriented"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "analyzing time series association. i am pretty new for time series analysis and i would like to share one of my research questions.here is the graph:apparently, both 1 and 2 are time series data. 1 represents monthly humidity and 2 represents a morbidity rate. i would like to analyze if the morbidity rate (2) is associated with seasonal humidity (1), what method is the best in this scenario? also, from the visualization, it seems like the morbidity peak is about 3 months after the peak of humidity. can anyone give me some suggestions of this question?thank you for any replies!",
    "present_kp": [
      "time series"
    ],
    "absent_kp": [
      "data mining",
      "statistics"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "debian 8 no wlan0 on xps-13 intel 8260 (rev 3a). freshly installed debian 8 on a dell xps - developer edition laptop, and running the 3.16.0-4 kernel since the newer one doesn't boot (dont care to fix that at the moment).sudo lspci -nnk | grep -ia2 net lists the following:3a:00.0 network controller [0280]: intel corporation wireless 8260 [8086:24f3] (rev 3a) subsystem: intel corporation device [8086:0050]3b:00.0 unassigned class [ff00]: realtek semiconductor co., ltd. device [10ec:525a] (rev 01)i downloaded the firmware (tried both iwlwifi-8000c-13.ucode and iwlwifi-8000c-17.ucode) and copied it to /lib/firmware, rebooted and nothing changed.if i try to install using apt-get install firmware-iwlwifi, my usb-ethernet adapter stops working as well.edit: also, my driver (8000-17.ucode) isn't even listed in the firmware-iwlwifi packed here:<url> when trying make menuconfigroot@001:/linux-4.1.26# hostcc scripts/kconfig/mconf.oin file included from scripts/kconfig/mconf.c:23:0:scripts/kconfig/lxdialog/dialog.h:38:20: fatal error: curses.h: no such file or directory#include curses_loc ^compilation terminated.scripts/makefile.host:108: recipe for target 'scripts/kconfig/mconf.o' failedmake[1]: *** [scripts/kconfig/mconf.o] error 1makefile:541: recipe for target 'menuconfig' failedmake: *** [menuconfig] error 2",
    "present_kp": [
      "debian",
      "firmware",
      "iwlwifi"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "online and parallizeable set intersection algorithm. i have problem that is reducible to the following:from a collection of stacks, find all items whose keys are on all stacks.my current solution to this problem is to just pop things off as quickly as possible store the items in the language's set type, compute the intersection every once in a while (the stacks are refilling constantly), and use that set, repeat.the issue here is that the equivalent of the pop operation is expensive (i can do this at most 4k times a second), and it kills the chance of multithreading (as pops are destructive).is there a way of doing this such that i can multithread the intersection bit? i cannot practically use shared memory or requeuing items. the threads can communicate with each other via pipes/sockets if need be, but such communication should be kept to a minimum (not really interesting in starting up some whole separate client/server thing just for this part of my application)as an idea of what i am dealing with, there can be up to 15 million items on the queues at any one time. by the time its over all items will have a match.ideas?clarification: these are message stacks (well queues really but the same thing) over the network. a pop is a consume operation over network, and push is a publish. thats why its so slow. since the issues is one of latency rather than bandwidth, multithreading can potentially help. i am not cpu bound. i have ~10 stacks, and up to 10 million items at a time. i have no control over the implementation of the stacks.",
    "present_kp": [],
    "absent_kp": [
      "algorithms",
      "sets",
      "online algorithms"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to install dropbox (and have dropbox file manager integration) without nautilus?. running other desktop environments than unity or gnome with a different file manager than nautilus, it might be a good idea to install dropbox without nautilus. but advice on dropbox involves installing nautilus too.nautilus has conflicts with some file managers, as it takes over the desktop (workspace) and the file manager integration of many programs.in xubuntu with thunar this is a big problem. also with cinnamon and nemo in linux mint nautilus changed the desktop and took over integration with firefox. but it seemed ok on elementary os. fewer problems on lubuntu too, if i am not mistaken.i want to ask for other people's experience on installing dropbox without nautilus on various linux systems and find if there's one single way to do it or system specific ways etc.trying to follow this advice on linux mint (sudo apt-get install dropbox python-gpgme) it had installed nautilus already!update after answer on linux mint.as i am already using other linux distros and will in the future, i expect other solutions for a few other distros, de, and file managers. i will also update my answer to add them if i can.",
    "present_kp": [
      "nautilus",
      "dropbox",
      "thunar",
      "nemo"
    ],
    "absent_kp": [
      "software installation"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "quicksort implementation using filter() to perform partitioning. from operator import ge as greater, lt as lesserdef qsort(l): if len(l) <= 1: return l pivot = l[0] sublist = lambda op: [*filter(lambda num: op(num, pivot), l[1:])] return qsort(sublist(lesser))+ [pivot] + qsort(sublist(greater))it seems like an overkill to have two lambdas in the sublist definition. how can i refactor the sublist lambda function to not include lambda at all?",
    "present_kp": [
      "lambda"
    ],
    "absent_kp": [
      "python",
      "algorithm",
      "python 3.x",
      "quick sort"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how do i escape spaces when using bash history interaction?. is it possible to escape a space for input on a command? i want to do this:!git\\ clonei love using ! to run past commands but not being able to use the space often limits the functionality.",
    "present_kp": [
      "bash"
    ],
    "absent_kp": [
      "command history",
      "escape characters"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "senti_classifier in sentiment analysis. any one has any idea about senti_classifier .all i can get is a module named senti_classifier and i have downloaded that in my local directory .",
    "present_kp": [
      "sentiment analysis"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how concerned about load balancing do i need to be?. i'm currently working on a project which will have client applications (desktop & android & web) communicate with varies processes running on a server(s).my question is how much should i be thinking about load balancing and the like?all projects which i have worked on before consisted of creating applications for internal use within companies so the number of connections was alwyas very low (probably no more than 50 at a time).however in my own time i decided to start creating a piece of accounting software, just because. so what i have ended up with is a wpf desktop application, and asp.net mvc web site and an android app which all connect to various processes/data running on my server.on the server there is an sql database (ms), various wcf services for communication from client to server and a few services running cleanup stuff.this works fine when i'm testing and its just me connected. however i have no idea how it will scale if a few people start connecting at once. i'm not even sure if i will release the software, let alone anyone use it! however as part of a learning exercise how should i be developing the system so that it could take on a large load and not crash/delay.i'm aware this could be a very open question and could be subjective to a lot of things, but as i say i have no experience in this department and it's something i would like to learn.",
    "present_kp": [],
    "absent_kp": [
      "scalability",
      "load testing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what does negative log likelihood mean?. i have a data set which has continuous independent variables and a continuous dependent variable. to predict the dependent variable using the independent variables, i've run an ensemble of regression models and tried to compare them against each other. here are the results for reference:i can interpret what the r-squared value / coefficient of determination for each of those models means. however, i can't understand what the negative log likelihood means. especially, why is it infinity for linear regression and boosted decision tree, and a finite value for a decision forest regression?edit:data description: the data that went into these three models is all continuous independent variables and a continuous dependent variable. there are a total of 542 observations and 26 variables. these 542 variables are split 70 - 30 to get training and testing datasets. therefore, the training dataset has 379 observations and 26 variables; the testing dataset has 163 observations and 26 variables.no missing data.edit 2 possible explanation - (click here): apparently, linear regression and boosted trees in azure ml don't calculate the negative log-likelihood metric - and that could be the reason that nll is infinity or undefined in both cases.",
    "present_kp": [
      "regression"
    ],
    "absent_kp": [
      "ensemble modeling"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "delta rpms and presto with yum. when you use delta rpm's with presto or without even, do they rely on you having the original rpm on hand, or do they download both the original and the difference between the original and then construct it? (specifically with yum)does presto or any other system need to be keeping a cache of all installed rpm's for delta rpms to work? do delta rpms work without presto, or are the integrated?thanks in advance :-)",
    "present_kp": [
      "rpm"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "working with multi-dimensional functions. how would you represent functions of type $[-1, 1]^n o \\mathbb r \\;$ for moderate $n$? how would you integrate them?for small $n$ (1-2) such functions can be represented as histograms, vectors in some base, etc. for really large $n$ one employs other techniques, like neural networks or for integration monte-carlo methods. but what if i have to deal with $n= \\overline{6 \\ldots 12} \\;$ ? i need to be able to compute such functions (for later integration to $\\mathbb r$), but how to store, how to represent them?having 100 points for each dimension will lead to $100^6 = 10^{12}$, whereas gib is around $10^9$ bytes.",
    "present_kp": [],
    "absent_kp": [
      "high dimensional",
      "data storage"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "making a selection in a dropdown based on several possible values. i'm making a chrome extension that lets you choose a size of an item and check out quickly.i have a value called large in sizepref array in my chrome.storage. and when it calls that value, it will choose an option in a dropdown based on that. however, possible labels are large, l, m/l or l/xl (large being the most often) with the value changing each time. what is the most efficient way for me to ensure that it makes the correct selection (assuming the speed of doing so is the highest priority)?this is my current code which just chooses l if large doesn't exist. this code works without errors, but i want it to be improved.chrome.storage.sync.get('sizepref', function(items) { // get size preferences from storage var sizepref = items.sizepref.top1; // set size to a var var sizeval = $(#size option).filter(function() { return $(this).html() == sizepref; }).val(); var sizeval2 = $(#size option).filter(function() { return $(this).html() == l; }).val(); if (sizeval !== undefined) { $(#size).val(sizeval); } else { $(#size).val(sizeval2); } });the targeted dropdown code is as follows:<select id=size name=size><option value=25243>small</option><option value=25244>medium</option><option value=25245>large</option><option value=25246>xlarge</option></select>",
    "present_kp": [
      "array"
    ],
    "absent_kp": [
      "javascript",
      "jquery",
      "google chrome"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "google sites page never shows up in google search organic results?. i use google sites (i.e.: <url> ) as a convenient way to maintain up-to-date info on several residential properties, info that's often requested by my property agents, its been around for about 1 year, but i still can never get it to appear in organic google search results or bing, even if i search the specific keywords such as the street names. i submitted the url manually to search engines, knowing that my sites page probably has very few incoming links. is this expected behavior?the content of my page has simple formatted text, and outgoing links to picasa/g+/imgur photo albums.am i doing something wrong or do all googlesites pages have poor organic search rank?thank you very much.",
    "present_kp": [
      "search engines",
      "google search",
      "google sites"
    ],
    "absent_kp": [
      "seo",
      "pagerank"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to share the look back video from facebook?. on facebook, there is a link to view a video looking back on your facebook history. apparently, some people have shared this video with their friends, however i and someone else cannot figure out how to do so. this url is not unique, it's a global url which shows the video of who's logged in. people say there's supposed to be a share link, but there are no links whatsoever. and, i can't share the url because it would show them their own videos.how do i share this video with friends on facebook?without using a screen recorder to capture it",
    "present_kp": [
      "facebook"
    ],
    "absent_kp": [
      "facebook lookback"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "opensolaris vs. debian for xen hypervisor vm server. all.i've just purchased a computer that i'd like to repurpose as a lab server. it's a six-core amd processor with 8 gigs of ram (16 gig max) and a terabyte hard drive. my idea is to hose windows, install a unix-based system on it, and use it to host a lot of virtual guests.i've been looking at my virtualization options, and i've decided to go with xen - i've never used it before, but it looks like it has good support and i've heard good things about it. (my only experience with virtual hosts has been with vmware on debian.)debian would be my first choice as an os because i'm familiar with it's setup and package management tools, and i know that xen is supported on debian. however, opensolaris has zfs, which is pretty compelling.has anyone had any experience doing something like this? i'm torn between going with what i know (debian) or trying my hand with opensolaris just for the zfs filesystem (which is, from what i understand, very awesome). does xen even run on opensolaris? all of my searches have turned up information about opensolaris as a virtual guest, not a virtual host.discussion and experiences are welcome. if this is a bad question, or if i need to give more details, please let me know. thanks!edit: virtualization options are welcome, as well. xen looks interesting, but i am by no means tied to it. since this will be a personal lab server, i'm willing to try different methods.",
    "present_kp": [
      "debian",
      "xen",
      "opensolaris"
    ],
    "absent_kp": [
      "virtual machine"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "failure tolerant factor coding. there are a lot of ml-algorithms which cannot directly deal with categorical variables. a very common solution is to apply binary (dummy-) coding to still properly handle the categorical nature of the data.very often e.g. in sk-learn or apache-spark the actual dummy-coder can only handle numeric values. so label-encoding needs to be performed beforehand.in a real live ml-scenario, the fitted model will encounter new and formerly not known data. usually, such a label-encoder (string-indexer) for spark has the option to either skip (ignore) a row of data which contains any unknown value or to throw an error. if multiple values require coding this can lead to a big loss of new data.are there any approaches which tolerate up to x new values per row and still properly evaluate the fitted pipeline?an example for spark string-indexing + dummy-coding is shown below.val df = spark.createdataframe(seq( (0, a), (1, b), (2, c), (3, a), (4, a), (5, c))).todf(id, category)val indexer = new stringindexer() .setinputcol(category) .setoutputcol(categoryindex) .fit(df)val indexed = indexer.transform(df)val encoder = new onehotencoder() .setinputcol(categoryindex) .setoutputcol(categoryvec)val encoded = encoder.transform(indexed)encoded.select(id, categoryvec).show()<url>",
    "present_kp": [
      "binary",
      "encoding"
    ],
    "absent_kp": [
      "scikit learn",
      "apache spark",
      "labels"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how i could pass variable inside awk match?. i have an awk command.i need to use i variable but my command does not work when i do.fechaname: 1,firstname: gdrgo, xxxxx: john, xxxxx: john, xxxxx: john, xxxxx: john, xxxxx: john, lastname: 222,dfgfechaname: 2,xxxxx: john, firstname: beto, xxxxx: john, xxxxx: john, xxxxx: john, lastname: 111,xxxxx: john,fechaname: 4,xxxxx: john, firstname: beto, xxxxx: john, xxxxx: john, xxxxx: john, lastname: 111,xxxxx: john,fechaname: 4,xxxxx: john, xxxxx: john, firstname: beto2, xxxxx: john,lastname: 555, xxxxx: john,xxxxx: john,fechaname: 5,xxxxx: john, xxxxx: john, firstname: beto2, xxxxx: john,lastname: 444, xxxxx: john,xxxxx: john,fechaname: 4,firstname: gdrgo, xxxxx: john, xxxxx: john, xxxxx: john, xxxxx: john, xxxxx: john, lastname: 222,dfgfechaname: 7,xxxxx: john, xxxxx: john, firstname: beto2, xxxxx: john,lastname: 444, xxxxx: john,xxxxx: john,when i use 5 instead of i it worksawk -v ofs='' -v fs='name: ' '{ for( i=2; i<=7; i++ ) if( match($2, /5/) ) print $0 }' sumacomandothis is my command awk -v ofs='' -v fs='name: ' '{ for( i=2; i<=7; i++ ) if( match($2, /**i**/) ) print $0 }' sumacomandoawk -v ofs='' -v fs='name: ' '{ for( i=2; i<=7; i++ ) if( match($2, /i/) ) print $0 }' sumacomando",
    "present_kp": [
      "awk"
    ],
    "absent_kp": [
      "text processing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "file structure for storing pixels that changed from one image to another. have two images with the same dimensions.i want to create a file that contains information on what pixels are different between the two images. the file should also tell me the value of the new pixel (integer).an example of such file is230,<phone>,920550,1500which tells me that the pixel at index 230 changed to 8528. the one at 291 is now value 920. and the pixel at 550 has value 1500.i already have the means to get all such data (the indexes where they change and their new integer values). my problem is the storage of such info, which needs to be as small as possible and of course easy to iterate when my program needs to read this data back.what i got currently is the following file format:[pixel color], [index], [index], [index], ...[pixel color], [index], ...[pixel color], [index], [index], ...basically, each line represents a pixel color that has changed, and then separated by commas are the indexes of the pixels that will change to such color.is there a more suitable/efficient file structure to achieve this kind of data storage?",
    "present_kp": [
      "data",
      "storage",
      "file structure"
    ],
    "absent_kp": [
      "data structures",
      "image processing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "get errors when aptitude update. i get the following errors when aptitude update. these errors are after non-error updates. what do i do to fix this, but not remove the ppas, etc?err <url> packages 404 not foundign <url> translation-en_caign <url> translation-enfetched 520 kb in 2s (206 kb/s)w: failed to fetch <url> unable to find expected entry 'main/binary-back/packages' in release file (wrong sources.list entry or malformed file)w: failed to fetch <url> unable to find expected entry 'main/binary-back/packages' in release file (wrong sources.list entry or malformed file)w: failed to fetch <url> unable to find expected entry 'non-free/binary-back/packages' in release file (wrong sources.list entry or malformed file)w: failed to fetch <url> unable to find expected entry 'non-free/binary-back/packages' in release file (wrong sources.list entry or malformed file)w: failed to fetch <url> unable to find expected entry 'import/binary-back/packages' in release file (wrong sources.list entry or malformed file)w: failed to fetch <url> unable to find expected entry 'main/binary-back/packages' in release file (wrong sources.list entry or malformed file)w: failed to fetch <url> unable to find expected entry 'main/binary-back/packages' in release file (wrong sources.list entry or malformed file)w: failed to fetch <url> unable to find expected entry 'main/binary-back/packages' in release file (wrong sources.list entry or malformed file)w: failed to fetch <url> unable to find expected entry 'main/binary-back/packages' in release file (wrong sources.list entry or malformed file)w: failed to fetch <url> unable to find expected entry 'iceweasel-release/binary-back/packages' in release file (wrong sources.list entry or malformed file)w: failed to fetch <url> unable to find expected entry 'non-free/binary-back/packages' in release file (wrong sources.list entry or malformed file)w: failed to fetch <url> unable to find expected entry 'main/binary-back/packages' in release file (wrong sources.list entry or malformed file)w: failed to fetch <url> 404 not founde: some index files failed to download. they have been ignored, or old ones used instead.e: couldn't rebuild package cachew: duplicate sources.list entry <url> wheezy/main amd64 packages (/var/lib/apt/lists/http.debian.net_debian_dists_wheezy_main_binary-amd64_packages)w: duplicate sources.list entry <url> wheezy/contrib amd64 packages (/var/lib/apt/lists/http.debian.net_debian_dists_wheezy_contrib_binary-amd64_packages)w: duplicate sources.list entry <url> wheezy/non-free amd64 packages (/var/lib/apt/lists/http.debian.net_debian_dists_wheezy_non-free_binary-amd64_packages)w: duplicate sources.list entry <url> wheezy/main i386 packages (/var/lib/apt/lists/http.debian.net_debian_dists_wheezy_main_binary-i386_packages)w: duplicate sources.list entry <url> wheezy/contrib i386 packages (/var/lib/apt/lists/http.debian.net_debian_dists_wheezy_contrib_binary-i386_packages)w: duplicate sources.list entry <url> wheezy/non-free i386 packages (/var/lib/apt/lists/http.debian.net_debian_dists_wheezy_non-free_binary-i386_packages)w: you may want to update the package lists to correct these missing files",
    "present_kp": [
      "debian",
      "apt",
      "aptitude"
    ],
    "absent_kp": [
      "crunchbang"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what % can be regarded as normal not viewed traffic reported in awstats?. recently statistics for my site is about 50% not viewed traffic. this seems much as many robots are blocked. what % of not viewed traffic can be regarded as normal?is my site under attack when 50% of traffic is generated not by normal users using their browsers?what can be done to reduce not viewed traffic (i am using nginx server)?",
    "present_kp": [
      "awstats"
    ],
    "absent_kp": [
      "web crawlers",
      "spam",
      "web traffic"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "utility functions for supporting memoization for functions. i've got a couple of utility functions to support memoization for functions with anywhere between 0 to 8 arguments:public shared function mize(of tresult)(byval input_f as system.func(of tresult)) as system.func(of tresult) dim is_new = true dim result as tresult return function() if is_new then result = input_f() is_new = false end if return result end functionend functionpublic shared function mize(of targ1 as structure, tresult)(byval input_f as system.func(of targ1, tresult)) as system.func(of targ1, tresult) dim map = new system.collections.generic.dictionary(of targ1, tresult) return function(arg1 as targ1) dim result as tresult if map.trygetvalue(arg1, result) then return result result = input_f(arg1) map.add(arg1, result) return result end functionend functionpublic shared function mize(of targ1 as structure, targ2 as structure, tresult)(byval input_f as system.func(of targ1, targ2, tresult)) as system.func(of targ1, targ2, tresult) dim map = new system.collections.generic.dictionary(of system.tuple(of targ1, targ2), tresult) return function(arg1 as targ1, arg2 as targ2) dim args = system.tuple.create(arg1, arg2) dim result as tresult if map.trygetvalue(args, result) then return result result = input_f(arg1, arg2) map.add(args, result) return result end functionend functionpublic shared function mize(of targ1 as structure, targ2 as structure, targ3 as structure, tresult)(byval input_f as system.func(of targ1, targ2, targ3, tresult)) as system.func(of targ1, targ2, targ3, tresult) dim map = new system.collections.generic.dictionary(of system.tuple(of targ1, targ2, targ3), tresult) return function(arg1 as targ1, arg2 as targ2, arg3 as targ3) dim args = system.tuple.create(arg1, arg2, arg3) dim result as tresult if map.trygetvalue(args, result) then return result result = input_f(arg1, arg2, arg3) map.add(args, result) return result end functionend functionpublic shared function mize(of targ1 as structure, targ2 as structure, targ3 as structure, targ4 as structure, tresult)(byval input_f as system.func(of targ1, targ2, targ3, targ4, tresult)) as system.func(of targ1, targ2, targ3, targ4, tresult) dim map = new system.collections.generic.dictionary(of system.tuple(of targ1, targ2, targ3, targ4), tresult) return function(arg1 as targ1, arg2 as targ2, arg3 as targ3, arg4 as targ4) dim args = system.tuple.create(arg1, arg2, arg3, arg4) dim result as tresult if map.trygetvalue(args, result) then return result result = input_f(arg1, arg2, arg3, arg4) map.add(args, result) return result end functionend functionpublic shared function mize(of targ1 as structure, targ2 as structure, targ3 as structure, targ4 as structure, targ5 as structure, tresult)(byval input_f as system.func(of targ1, targ2, targ3, targ4, targ5, tresult)) as system.func(of targ1, targ2, targ3, targ4, targ5, tresult) dim map = new system.collections.generic.dictionary(of system.tuple(of targ1, targ2, targ3, targ4, targ5), tresult) return function(arg1 as targ1, arg2 as targ2, arg3 as targ3, arg4 as targ4, arg5 as targ5) dim args = system.tuple.create(arg1, arg2, arg3, arg4, arg5) dim result as tresult if map.trygetvalue(args, result) then return result result = input_f(arg1, arg2, arg3, arg4, arg5) map.add(args, result) return result end functionend functionpublic shared function mize(of targ1 as structure, targ2 as structure, targ3 as structure, targ4 as structure, targ5 as structure, targ6 as structure, tresult)(byval input_f as system.func(of targ1, targ2, targ3, targ4, targ5, targ6, tresult)) as system.func(of targ1, targ2, targ3, targ4, targ5, targ6, tresult) dim map = new system.collections.generic.dictionary(of system.tuple(of targ1, targ2, targ3, targ4, targ5, targ6), tresult) return function(arg1 as targ1, arg2 as targ2, arg3 as targ3, arg4 as targ4, arg5 as targ5, arg6 as targ6) dim args = system.tuple.create(arg1, arg2, arg3, arg4, arg5, arg6) dim result as tresult if map.trygetvalue(args, result) then return result result = input_f(arg1, arg2, arg3, arg4, arg5, arg6) map.add(args, result) return result end functionend functionpublic shared function mize(of targ1 as structure, targ2 as structure, targ3 as structure, targ4 as structure, targ5 as structure, targ6 as structure, targ7 as structure, tresult)(byval input_f as system.func(of targ1, targ2, targ3, targ4, targ5, targ6, targ7, tresult)) as system.func(of targ1, targ2, targ3, targ4, targ5, targ6, targ7, tresult) dim map = new system.collections.generic.dictionary(of system.tuple(of targ1, targ2, targ3, targ4, targ5, targ6, targ7), tresult) return function(arg1 as targ1, arg2 as targ2, arg3 as targ3, arg4 as targ4, arg5 as targ5, arg6 as targ6, arg7 as targ7) dim args = system.tuple.create(arg1, arg2, arg3, arg4, arg5, arg6, arg7) dim result as tresult if map.trygetvalue(args, result) then return result result = input_f(arg1, arg2, arg3, arg4, arg5, arg6, arg7) map.add(args, result) return result end functionend functionpublic shared function mize(of targ1 as structure, targ2 as structure, targ3 as structure, targ4 as structure, targ5 as structure, targ6 as structure, targ7 as structure, targ8 as structure, tresult)(byval input_f as system.func(of targ1, targ2, targ3, targ4, targ5, targ6, targ7, targ8, tresult)) as system.func(of targ1, targ2, targ3, targ4, targ5, targ6, targ7, targ8, tresult) dim map = new system.collections.generic.dictionary(of system.tuple(of targ1, targ2, targ3, targ4, targ5, targ6, targ7, targ8), tresult) return function(arg1 as targ1, arg2 as targ2, arg3 as targ3, arg4 as targ4, arg5 as targ5, arg6 as targ6, arg7 as targ7, arg8 as targ8) dim args = new system.tuple(of targ1, targ2, targ3, targ4, targ5, targ6, targ7, targ8)(arg1, arg2, arg3, arg4, arg5, arg6, arg7, arg8) dim result as tresult if map.trygetvalue(args, result) then return result result = input_f(arg1, arg2, arg3, arg4, arg5, arg6, arg7, arg8) map.add(args, result) return result end functionend functionthe problem is that the memoization algorithm is repeated 6 times (the functions that accept functions with 2 arguments to 8 arguments basically have the same algorithm).i'm clearly violating dry here, and i want to modify the code above to not violate dry.i've tried shifting those repeating algorithm into a separate function, but since vb.net/c# does not allow system.delegate as a generic constraint, i'm out of ideas as to how i would do it.what should i do and how can i improve my code above?",
    "present_kp": [
      ".net",
      "vb.net",
      "memoization"
    ],
    "absent_kp": [
      "design patterns"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "decide if a specific turing machine halts on a specific string. can you always decide if a specific turing machine accepts a specific string?i started thinking about this after reading an answer to this question, rice's theorem vs turing completeness, which seams to you can. if this is true, intuitively, it seams it would be implied that halt could be decided, which is obviously not true. otherwise what is the difference?",
    "present_kp": [],
    "absent_kp": [
      "computability",
      "undecidability",
      "halting problem"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "facebook: list of recently added friends?. is there a way to see what friends i recently added? it used to be possible using edit friends but this part has changed it seems.became friends with stories are set to not appear on my wall.it seems to possible be using the api but is there an easier way?",
    "present_kp": [
      "facebook"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "are hashtags recommended for facebook?. are hashtags recommended for facebook posts ? i have seen less use of hash tags by major brands in their business pages but the hash tag trend confuses me. are they recommended in terms of seo or whatever?",
    "present_kp": [
      "seo",
      "facebook"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "efficient assembly of finite element matrix(coupled equations case). i noticed this post, where spalloc and sparse are recommanded for efficient assembly in matlab. i personally use sparse assembling for simple cases.however, when it comes to the case of coupled pde, say , 3-pde coupled, then the scalar unknow becomes a 3x3 tensor, in this case i can't figure out a way to exploit sparse(), and have to use for loops to assemble.when the final assembled sparse matrix is as large as 30k*30k(on pc), the assembling process becomes really slow(~10min), while the final matrix solving step is still fast(~less than 3 seconds).is there any suggestion? any generic solution is appreciated(not necessarily matlab)",
    "present_kp": [
      "finite element",
      "matlab",
      "assembly"
    ],
    "absent_kp": [
      "finite difference",
      "linear programming"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "should i use a layer between service and repository for a clean architecture - spring. i'm working in an architecture, it is going to offer a rest api for web client and mobile apps. i'm using spring(spring mvc, spring data jpa, ...etc). the domain model is coded with jpa specification. i'm trying to apply some concepts of clean architecture (<url>). not all, because i'm going to keep the jpa domain model.the actual flow through the layers is this:front end <--> api service -> service -> repository -> dbfront end: web client, mobile appsapi service: rest controllers, here i use converters and dto's and call servicesservice: interfaces with implementations and they contain business logicrepository: repository interfaces with automatically implementations(done by spring data jpa) which contatin crud operations and maybe some sql queriesmy doubt: should i use an extra layer between service and repository?i'm planning this new flow:front end <--> api service -> service -> persistence -> repository -> dbwhy to use this persistence layer? as it says in the clean architecture article i would like to have a service implementation(business logic or use case) that access an agnostic persistence layer. and not changes will be needed if i decide to use another data access pattern, for example if i decide to stop using repository.class productserviceimpl implements productservice { productrepository productrepository; void save(product product) { // do business logic productrepository.save(product) }}so i'm thinking use a persistence layer like this:class productserviceimpl implements productservice { productpersistence productpersistence; void save(product product) { // do business logic productpersistence.save(product) }}and implementation of the persistence layer like this:class productpersistenceimpl implements productpersistence { productrepository productrepository; void save(product product) { productrepository.save(product) }}so i only need to change the implementations of persistence layer, left the service without changes.coupled with the fact that the repository is related with the framework.what do you think? thanks.",
    "present_kp": [
      "architecture",
      "persistence",
      "services"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "why does coq have prop?. coq has a type prop of proof irrelevant propositions which are discarded during extraction. what are the reason for having this if we use coq only for proofs. prop is impredicative, so prop : prop, however, coq automatically infers universe indexes and we can use type(i) instead everywhere. it seems prop complicates everything a lot.i read that there're philosophical reasons for separating set and prop in luo's book, however, i didn't find them in the book. what are they?",
    "present_kp": [
      "coq"
    ],
    "absent_kp": [
      "dependent type"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "shell script with directory input. i'm writing a shell script that is supposed to take in one parameter (a directory name) and display how many files, directories, readable files, writable files and executable files are in that directory. if a parameter isn't given when you run it, its supposed to display an error message and abort. if the given parameter doesn't exist it should also display an error message and abort. otherwise it should display the above info. i cannot for the life of me get it to run. here is what i have, please help!: #!/bin/csh $1 set file=$1 if ($file==0)then echo usage: assignment6.sh <directory_name> exit0 else if (-e $file && -r $file) then echo number of directories: 'ls | wc -w' echo number of files: 'ls -d */ | wc -w' echo number of readable files: 'find * -type f -or -type d -maxdepth 0 -perm +u=r | wc -w' echo number of writable files: 'find * -type f -or -type d -maxdepth 0 -perm +u=w | wc -w' echo number of executable files: 'find * -type f -or -type d -maxdepth 0 -perm +u=x | wc -w' else if (! -e $file) echo no such directory. exit 0 endif endif exit 0",
    "present_kp": [
      "shell script",
      "csh"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "could amp's version of a webpage's url affect the traffic of the original one?. it seems that google amp is really important when designing a webpage.recently i've researched information about amp, but still i'm confused.our company has a webpage <url> i have a questions before adding amp.(1) if i design two versions of this page, amp and non-amp, and mobile users will see the url of our website is https:// <url> will this affect the traffic of the original one? (<url>), because the amp version's url has the google.com as the domain, and not example.com.",
    "present_kp": [
      "google",
      "amp"
    ],
    "absent_kp": [
      "seo",
      "domains"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "allowing outgoing emails that will be delivered to localhost only. i operate a linux system where i give out free linux shell accounts to people for educational purposes. unfortunately, while doing so it's expected to meet abusive users who will keep sending spam emails to other servers such as google, zoho, etc and hence will get the ip of the server blocked. what i would like to do is allow the users on the system to send messages within localhost only. this means that when a user tries to send out an email to an external domain name, gmail for example, the request will be refused. however, if the user tries to send an email to another user on localhost (example: giovanni@localhost), the message will be sent. i don't mind receiving emails from other servers, but i don't want my server to send emails to other servers. how can i do so?i'm running centos 6.5 with postfix installed. how can i configure this? any suggestion will be hugely appreciated!",
    "present_kp": [
      "email",
      "postfix"
    ],
    "absent_kp": [
      "configuration"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "python proper abstract class and subclassing with attributes and methods. the goal of the code below is to have an abstract base class that defines simple methods and attributes for the subclasses. this is part of an application that provides the code base for others to develop their own subclasses such that all methods and attributes are well implemented in a way for the main application to use them. the application shall never create more than one object of each subclass, since they are supposed to implement everything correctly to be called upon. ideally, instantiating objects of the subclasses would not even be necessary, but is needed as it is in order to check if the abstract methods are implemented.however, python seems to be different and awkward when it comes to classes in comparison with other programming languages (initialization, attributes, properties), and i am not very sure if the solution below is the most appropriate. it seems too much complicated to achieve the aforementioned purpose. in addition, another function (not in the code below) must be made to properly check (make sure that they are of a certain type) the attributes of the subclasses.any thoughts?from abc import abc, abstractmethodclass abstract_attribute(object): taken from <url> def __get__(self, obj, type): # now we will iterate over the names on the class, # and all its superclasses, and try to find the attribute # name for this descriptor # traverse the parents in the method resolution order for cls in type.__mro__: # for each cls thus, see what attributes they set for name, value in cls.__dict__.items(): # we found ourselves here if value is self: # if the property gets accessed as child.variable, # obj will be done. for this case # if accessed as a_child.variable, the class child is # in the type, and a_child in the obj. this_obj = obj if obj else type raise notimplementederror( %r does not have the attribute %r (abstract from class %r) % (this_obj, name, cls.__name__)) # we did not find a match, should be rare, but prepare for it raise notimplementederror( %s does not set the abstract attribute <unknown>, type.__name__)class base(abc): # attr1 and attr2 must be properly (be of a certain type) implemented by subclasses # abstract_attribute() checks if the attribute is present in the subclass attr1 = abstract_attribute() attr2 = abstract_attribute() @abstractmethod def method(): pass # a class property that does not change from instance to instance @property def prop(self): return self.prop @prop.setter @classmethod def prop(self, prop): self.prop = propclass sub(base): # attr1 and attr2 must be implemented otherwise and exception is thrown upon accessing them attr1 = something attr2 = somethingelse # method must be implemented otherwise an exception is thrown upon instantiation def method(): print(must be implemented by subclasses)",
    "present_kp": [
      "python",
      "properties"
    ],
    "absent_kp": [
      "object oriented",
      "inheritance",
      "reflection"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "can facebook's birthday list be expanded to include upcoming birthdays?. is there a way to expand the list of birthdays on facebook? i use to notice people's birthdays because their names and birthday were listed. facebook has moved the list up the page, condensed it, and only shows today's birthdays. is there any way to at least show upcoming birthdays and the persons' birth date?",
    "present_kp": [
      "facebook"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how can i set a folder undeletable but have r/w permissions to it's content. i have a directory which i'd like to mark as not deleteable, but have all the usual permissions of read/write/delete on all subdirectories and files. how can i achieve this?i tried setting the undeletable flag via chattr but it only works on files and apparently doesn't even prevent deleting, but allows restoring the file. setting the immutable flag prevents deleting the dir but also prevents changing anything in it.",
    "present_kp": [
      "permissions",
      "directory"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to stop application from being suspended by ctrl+z?. currently i'm running dvtm inside a terminal, and vim inside dvtm. when i press ctrl+z intending to suspend vim, dvtm got suspended instead. i didn't have this problem with screen or tmux, so i think it must be dvtm doing something wrong (or not doing something right). how can i fix that?update: i was wrong, this is not a problem with dvtm. indeed i was using the dtach+dvtm combo and wrongly assumed that dvtm was at fault. the problem is really with dtach.",
    "present_kp": [
      "suspend",
      "dtach"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "are filename expansion and pathname expansion the same thing in bash?. bash reference manual sometimes mentions filename expansion and sometimes but less often pathname expansion.i have been thinking they are the same concept. but the same manual mentions the two without defining the second, which makes me uncertain. so are they the same or different?",
    "present_kp": [
      "bash"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how is wordnet curated?. i use princeton's wordnet in nlp applications. i always read that it is human-curated. but how is it curated? who decides that words belong in a synset? how does the process work? wikipedia does not give many details. how does it work?",
    "present_kp": [],
    "absent_kp": [
      "natural language processing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to edit second occurrence of duplicate lines?. i'm looking for a way to find all duplicate lines which are not not a color and add is a color at the end of second occurrence of them.here is a diff -y of what i'm talking about.orginal file - final resaultpink pinkpink | pink is a colornot a color not a colornot a color not a color violet violetviolet | violet is a colornot a color not a colornot a color not a colororange orangeorange | orange is a colornot a color not a color",
    "present_kp": [],
    "absent_kp": [
      "text processing",
      "awk",
      "sed"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "separating front end from back end - tomcat. i'm currently working with a company that uses java / tomcat / spring for the back end of our web applications. as a front-end developer, i'm feeling more and more strongly that the back end should be a separate project from the front end, for a few reasons: 1) building the project - many modern projects build with grunt and other front-end tools. the front-end build goals (concatenation, minification, front end tests, pre-processing css) are completely different from the back end goals (compiling java code, passing back end tests, deploying to a continuous integration server)2) deployment: when i commit a javascript or html file to our git repo, it doesn't make sense that our continuous deployment tool should rebuild and redeploy the whole project, but it has no way to tell the difference between a front-end commit and a back-end commit.so, how would you restructure a project that previously automatically deployed from a single war file, living in a single git repo, to make a separate back end and front end? can i actually do this, given that we use spring framework?right now, my files live in a mixture of the /webapp/web-inf directory (for html pages / velocity templates) and the /webapp/resources directory for everything else (js, css, images). i'm slightly confused about how i should go about this when tomcat deploys a huge bulk of files to the /target directory, as well? if i keep a separate git repo for front end files, won't it be wiped out if i re-deploy the back end project?(originally i come from an apache background, where it seemed so nice and simple...)",
    "present_kp": [
      "java",
      "spring",
      "front end",
      "tomcat"
    ],
    "absent_kp": [
      "project structure"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why don't large popular sites such as so use keyword and description meta headers?. why don't stack overflow, wikipedia or msdn etc. include keyword and description meta tags in their page headers?for example google indexes the first paragraph as a brief description. is this a technical point in web design and seo?",
    "present_kp": [],
    "absent_kp": [
      "html"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "robots.txt and pattern matching. adding this to my robots.txtuser-agent: *disallow: /*action=*$how does robots not recognizing wild cards handle this?",
    "present_kp": [
      "robots.txt"
    ],
    "absent_kp": [
      "googlebot"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "when is the minimum spanning tree for a graph not unique. given a weighted, undirected graph g: which conditions must hold true so that there are multiple minimum spanning trees for g?i know that the mst is unique when all of the weights are distinct, but you can't reverse this statement. if there are muliple edges with the same weight in the graph, there may be multiple msts but there might also be just one: in this example, the graph on the left has a unique mst but the right one does not.the closest i could get to finding conditions for non-uniqueness of the mst was this:consider all of the chordless cycles (cycles that don't contain other cycles) in the graph g. if in any of these cycles the maximum weighted edge exists multiple times, then the graph does not have a unique minimum spanning tree. my idea was that for a cycle like thiswith n vertices, you can leave out exactly one of the edges and still have all of the vertices be connected. therefore, you have multiple choices to remove the edge with the highest weight to get a mst, so the mst is not unique.however, i then came up with this example:you can see that this graph does have a cycle that fits my condition: (e,f,g,h) but as far as i can see, the minimum spanning tree is unique:so it seems like my condition isn't correct (or maybe just not completely correct). i'd greaty appreciate any help on finding the necessary and sufficient conditions for the non uniqueness of the minimum spanning tree.",
    "present_kp": [
      "spanning trees",
      "minimum spanning tree"
    ],
    "absent_kp": [
      "graphs",
      "graph theory",
      "weighted graphs"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "should i use hardlinks for my sites-enabled folder instead of softlinks?. every article i find about web servers suggest creating a sites-available and sites-enabled directory within apache/nginx/etc. then, using symbolic (soft) links, create a link from the available to the enabled folder.why use symbolic links rather than hardlinks? with hardlinks, you can move the original file (rename it) as needed without needing to recreate the link. you can still delete the sites-enabled file without ruining anything, and the user/group permissions in any sane setup will be the same for both folders.can i safely use hardlinks instead of softlinks? or is there a downside to hardlinks i'm not seeing? the major upside for me is not having to worry about recreating a symlink if i move/rename the original file.",
    "present_kp": [
      "symlink"
    ],
    "absent_kp": [
      "filesystems",
      "webserver",
      "hard link"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "universal memcomputing machines (umm). this paper on memcomputing seems like a really big deal, but it doesn't seem to be particularly popular. they prove that their umm can solve np problems in p, although they don't claim p = np. in their next paper, they go on to build a physical model of this umm with moderate success. does this mean p=np?",
    "present_kp": [],
    "absent_kp": [
      "complexity theory",
      "computability",
      "turing machines",
      "computer architecture"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how would i design an interface such that it's clear which properties may change their value, and which will remain constant?. i am having a design issue regarding .net properties.interface ix{ guid id { get; } bool isinvalidated { get; } void invalidate();}problem:this interface has two read-only properties, id and isinvalidated. the fact that they are read-only, however, is by itself no guarantee that their values will remain constant.let's say that it were my intention to make it very clear thatid represents a constant value (which may therefore be safely cached), while isinvalidated might change its value during the lifetime of an ix object (and so shouldn't be cached).how could i modify interface ix to make that contract sufficiently explicit?my own three attempts at a solution:the interface is already well-designed. the presence of a method called invalidate() allows a programmer to infer that the value of the similarly-named property isinvalidated might be affected by it.this argument holds only in cases where the method and property are similarly named.augment this interface with an event isinvalidatedchanged:bool isinvalidated { get; }event eventhandler isinvalidatedchanged;the presence of a changed event for isinvalidated states that this property may change its value, and the absence of a similar event for id is a promise that that property will not change its value.i like this solution, but it's a lot of additional stuff that may not get used at all.replace the property isinvalidated with a method isinvalidated():bool isinvalidated();this might be too subtle a change. it's supposed to be a hint that a value is computed freshly each time which wouldn't be necessary if it was a constant. the msdn topic choosing between properties and methods has this to say about it:do use a method, rather than a property, in the following situations. [] the operation returns a different result each time it is called, even if the parameters do not change.what kind of answers do i expect? i am most interested in entirely different solutions to the problem, along with an explanation how they beat my above attempts.if my attempts are logically flawed or have significant disadvantages not yet mentioned, such that only one solution (or none) remains, i would like to hear about where i went wrong.if the flaws are minor, and more than one solution remains after taking it into consideration, please comment.at the very least, i would like some feedback on which is your preferred solution, and for what reason(s).",
    "present_kp": [
      "design",
      ".net",
      "properties"
    ],
    "absent_kp": [
      "c#"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to show progress when loading initrd?. i have a complete root fs built into initrd, so it's almost 500mb. it takes a while to load to memory and during this time, the screen has no indication of the progress. i would like to know if there is a way show progress as initrd loaded to memory? (such as pxe boot loading kernel will have '.' per 1mb.) i'm using grub legacy, and i can patch the source if needed.",
    "present_kp": [
      "boot",
      "initrd",
      "grub legacy"
    ],
    "absent_kp": [
      "progress information"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "awk + how to convert csv names to one field names. i have the following csv filemore names.csverik kastelo , roman flot , david krish , timoty klon derek matue , jenfaer loper , kris dovalo sara paula , boris miue , kami san-toto , benjaman hurato , lim paulo...so i want to convert this csv file to the following example formatplease advice how to do it with awk , or perl one liner or elseerik kastelo roman flot david krishtimoty klon derek matue jenfaer loper kris dovalo sara paula boris miue kami san-toto benjaman hurato lim paulo",
    "present_kp": [
      "awk",
      "perl",
      "csv"
    ],
    "absent_kp": [
      "linux",
      "text processing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "extract tomcat parameters from ps output. i'm trying to write a little diagnostic tool and one of the functions i want from it is that i want to run is: ps axu | grep tomcat | grep -v grep | awk '{ print $<list of desired fields> }' i want to return the userid that started tomcat and the following parameters that were passed when tomcat started:-javaagent:/usr/share/tomcat7/<someagent> -dcatalina.base=/var/lib/tomcat7 - dcatalina.home=/usr/share/tomcat7 -djava.io.tmpdir=/tmp/tomcat7-tomcat7-tmpi want those fields passed into an array which i can then use to tell the user something about the tomcat install. the problem is i can't find a way of formatting the command to force the fields to be what i need them to be to make awk work. at this point i'm thinking of just taking the output, writing it to a temp file, then pulling what i want out using regex. i know there's a way to do this in bash but i have not had any success getting there from here.",
    "present_kp": [
      "ps"
    ],
    "absent_kp": [
      "shell script",
      "text processing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how do i see when a systemd service was started/stopped/restarted?. i have a service (written by myself) running on a debian (jessie) server, and the service's own logs happen to indicate that it restarted at a particular time. there is no indication of a segfault or other crash, so i am now trying to figure out if the application somehow silently failed and got respawned by systemd, or whether a user purposely restarted the service via systemctl.the shell history doesn't show such activity, but that is not conclusive because of export histcontrol=ignoreboth and because an ssh session might have just timed out, preventing a previous login's bash history from being written to disk. the server was not rebooted at the time.but i would expect that systemd itself should keep a log indicating when a service was purposely restarted. to my surprise i was unable to find any documentation (e.g. for journalctl) on how to get such logs.some other posts (e.g. where is / why is there no log for normal user systemd services?) seem to indicate that there should be log messages like this:jan 15 19:28:08 qbd-x230-suse.site systemd[1]: starting chatty.service...jan 15 19:28:08 qbd-x230-suse.site systemd[1]: started chatty.service.but i don't see such log messages on my system.is there a way to find out when systemd services were started, stopped or restarted?edit: it seems the typical problem people might run into is that they run journalctl as a non-privileged user. this is not the case for me, i have been operating as root the whole time. in response to a comment, running grep systemd /var/log/syslog gives me only this:jun 6 09:28:35 server systemd[22057]: starting paths.jun 6 09:28:35 server systemd[22057]: reached target paths.jun 6 09:28:35 server systemd[22057]: starting timers.jun 6 09:28:35 server systemd[22057]: reached target timers.jun 6 09:28:35 server systemd[22057]: starting sockets.jun 6 09:28:35 server systemd[22057]: reached target sockets.jun 6 09:28:35 server systemd[22057]: starting basic system.jun 6 09:28:35 server systemd[22057]: reached target basic system.jun 6 09:28:35 server systemd[22057]: starting default.jun 6 09:28:35 server systemd[22057]: reached target default.jun 6 09:28:35 server systemd[22057]: startup finished in 59ms.jun 6 09:37:08 server systemd[1]: reexecuting.",
    "present_kp": [
      "systemd"
    ],
    "absent_kp": [
      "journald"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "kernel panic hardware error - what hardware is responsible?. i'm having kernel panics every other day. below is a transcript of a photo of the console.how do i determine what hardware is responsible for the problem? (or is it software)ubuntu 12.04 lts server[hardware error]: cpu:2 mc0_status[-|ue|-|-|addrv|uecc]: 0xb400200055000145[hardware error]: mc0_addr: 0x0000000164fe77b0[hardware error]: data cache error: data/tag dwr error.[hardware error]: cache level: l1, tx: data, mem-tx: dwr[hardware error]: cpu:3 mc0_status[-|ue|-|pcc|addrv|cecc]: 0xb66b400000000135[hardware error]: mc0_addr: 0x0000000164fe77b0[hardware error]: data cache error: data/tag dwd error.[hardware error]: cache level: l1, tx: data, mem-tx: drd[hardware error]: cpu 3: machine check exception: 4 bank 0: b66b400000000135[hardware error]: tsc bc02bd350de4 addr 164fe7bb0[hardware error]: processor 2:100f42 time <phone> socket 0 apic 3 microcode 10000c6[hardware error]: cpu:3 mc0_status[-|ue|-|pcc|addrv|cecc]: 0xb66b400000000135[hardware error]: mc0_addr: 0x0000000164fe77b0[hardware error]: data cache error: data/tag dwd error.[hardware error]: cache level: l1, tx: data, mem-tx: drd[hardware error]: machine check: invalidkernel panic - not syncing: fatal machine check on current cpushutting down cpus with nmihere is some additional logging that i found:kernel: [<phone>] ------------[ cut here ]------------kernel: [<phone>] warning: at /build/buildd/linux-lts-quantal-3.5.0/net/sched/sch_generic.c:255 dev_watchdog+0x272/0x280()kernel: [<phone>] hardware name: ms-7576kernel: [<phone>] netdev watchdog: eth0 (r8169): transmit queue 0 timed outkernel: [<phone>] modules linked in: nfsd nfs lockd fscache auth_rpcgss nfs_acl sunrpc xfs vesafb radeon ttm drm_kms_helper snd_hda_codec_hdmi snd_hda_codec_realtek snd_hda_intel drm snd_hda_codec wmi i2c_algo_bit snd_hwdep snd_pcm snd_timer snd soundcore snd_page_alloc lp shpchp r8169 sp5100_tco i2c_piix4 firewire_ohci parport firewire_core kvm_amd edac_core k10temp edac_mce_amd serio_raw kvm mac_hid microcode crc_itu_t raid10 raid456 async_pq async_xor xor async_memcpy async_raid6_recov raid6_pq async_tx raid1 raid0 multipath linear pata_atiixpkernel: [<phone>] pid: 0, comm: swapper/3 tainted: g m 3.5.0-23-generic #35~precise1-ubuntukernel: [<phone>] call trace:kernel: [<phone>] <irq> [<ffffffff81052c9f>] warn_slowpath_common+0x7f/0xc0kernel: [<phone>] [<ffffffff81052d96>] warn_slowpath_fmt+0x46/0x50kernel: [<phone>] [<ffffffff815a05b2>] dev_watchdog+0x272/0x280kernel: [<phone>] [<ffffffff8101be03>] ? native_sched_clock+0x13/0x80kernel: [<phone>] [<ffffffff810702d0>] ? __queue_work+0x330/0x330kernel: [<phone>] [<ffffffff815a0340>] ? pfifo_fast_dequeue+0xe0/0xe0kernel: [<phone>] [<ffffffff815a0340>] ? pfifo_fast_dequeue+0xe0/0xe0kernel: [<phone>] [<ffffffff81062ce6>] call_timer_fn+0x46/0x160kernel: [<phone>] [<ffffffff815a0340>] ? pfifo_fast_dequeue+0xe0/0xe0kernel: [<phone>] [<ffffffff81064632>] run_timer_softirq+0x132/0x2a0kernel: [<phone>] [<ffffffff810a4105>] ? ktime_get+0x65/0xe0kernel: [<phone>] [<ffffffff8105ba88>] __do_softirq+0xa8/0x210kernel: [<phone>] [<ffffffff810ab264>] ? tick_program_event+0x24/0x30kernel: [<phone>] [<ffffffff816a841c>] call_softirq+0x1c/0x30kernel: [<phone>] [<ffffffff81016245>] do_softirq+0x65/0xa0kernel: [<phone>] [<ffffffff8105be6e>] irq_exit+0x8e/0xb0kernel: [<phone>] [<ffffffff816a8d5e>] smp_apic_timer_interrupt+0x6e/0x99kernel: [<phone>] [<ffffffff816a7aca>] apic_timer_interrupt+0x6a/0x70kernel: [<phone>] <eoi> [<ffffffff8103ff56>] ? native_safe_halt+0x6/0x10kernel: [<phone>] [<ffffffff8101c993>] default_idle+0x53/0x1f0kernel: [<phone>] [<ffffffff8101d8a9>] cpu_idle+0xd9/0x120kernel: [<phone>] [<ffffffff8167b237>] start_secondary+0xc3/0xc5kernel: [<phone>] ---[ end trace ef52dc6dad6ceea1 ]--- # dmidecode -t 1 -t 3 -t 4 | egrep '(manufacturer|product|serial|socket|version)'manufacturer: micro-star international co.,ltdproduct name: ms-7576version: 1.0serial number: to be filled by o.e.m.manufacturer: micro-star international co.,ltdversion: 1.0serial number: to be filled by o.e.m.socket designation: cpu1manufacturer: amdversion: amd phenom(tm) ii x4 b50 processorserial number: to be filled by o.e.m.",
    "present_kp": [
      "ubuntu",
      "kernel panic"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "generating combinations of n elements in groups of k. i've written this program that writes all the combinations (without repetition) of n elements in groups of k.i think the code is good, but i like to know if you have some better (or faster) solutions.the elements to combine are always the number from 0 to n-1, the program calls a function (newcombo()) for each generated combination.eg: using n=6 and k=4 the program generates the following output:0 1 2 3 0 1 2 4 0 1 2 5 0 1 3 4 0 1 3 5 0 1 4 5 0 2 3 4 0 2 3 5 0 2 4 5 0 3 4 5 1 2 3 4 1 2 3 5 1 2 4 5 1 3 4 5 2 3 4 5 the code is below:#include <stdio.h>void newcombo(int *a,int n,int k);void newcombo(int *a,int n,int k){ int i; for(i=0;i<k;i++) printf(%d ,a[i]); puts();}int main(int argc,char *argv[]){ int a[30]; int n,k,p; do { printf(insert n and k - eg: 5 3: ); if (scanf(%d %d,&n,&k)!=2) return 1; if (k>n) puts(k shall be <= n !); if (k==0 || n==0) puts(k and n shall not be 0!); if (k>30) puts(k shall be <=30); } while (k>n || k==0 || n==0 || k>30); p=0; a[0]=-1; do { if (++a[p]>n-k+p) { p--; } else { if (p<k-1) { a[p+1]=a[p]; p++; } else { newcombo(a,n,k); } } } while(p>=0); return 0;}",
    "present_kp": [
      "c"
    ],
    "absent_kp": [
      "performance",
      "combinatorics"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why do the descriptions of github repos not always show up in the repositories tab?. when i find a github project that i like, i often go and look at other projects by the same author.generally people have a multitude of projects, so i skim down through the list, reading names and descriptions. i'm much more likely to actually look at the repository if the description is displaying. however, sometimes the description does not display on wildly popular repositories:as you can see, vim-pathogen is wildly popular, but the description is not showing. this is in spite of the fact that the repository has a description, and in fact seems to be his most popular repository:what's going on here? why do the descriptions get dropped off in the repositories tab?",
    "present_kp": [
      "github"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "people can see with whom i shared a post, when i post it with custom sharing. usually i post status or links with custom sharing. e.g. i do post a status keeping 10-15 people in custom list. now, if one of them hovers the mouse on the gear or custom sharing icon, they can see other people with whom i've shared the post (which i don't want).is this by default? because if i see custom posts from my friends, i can't see this behavior. so, it looks like problem with my profile.can anyone please tell me how to disable this from settings?",
    "present_kp": [],
    "absent_kp": [
      "facebook"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "modified strategy design pattern. i've started looking into design patterns recently, and one thing i'm coding would suit the strategy pattern perfectly, except for one small difference.essentially, some (but not all) of my algorithms, need an extra parameter or two passed to them.so i'll either need topass them an extra parameter when i invoke their calculate method orstore them as variables inside the concretealgorithm class, and be able to update them before i call the algorithm.is there a design pattern for this need / how could i implement this while sticking to the strategy pattern?i've considered passing the client object to all the algorithms, and storing the variables in there, then using that only when the particular algorithm needs it. however, i think this is both unwieldy, and defeats the point of the strategy pattern.just to be clear i'm implementing in java, and so don't have the luxury of optional parameters (which would solve this nicely).",
    "present_kp": [
      "java",
      "design patterns",
      "strategy"
    ],
    "absent_kp": [
      "interfaces"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "grep command to list the files not owned by a user. using the grep command display, all files not owned by you in your home directory?",
    "present_kp": [
      "grep",
      "command"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "left quotient of a regular language follow-up. hello i already asked this question here left quotient of a regular languagebut i asked from anonymous profile and now i couldn't comment on this. so i ask here for more details...question:show that for any language $l ^*$ and any dfa $a = \\langle \\sigma, q, q_0, \\delta, f angle$, the left quotient $l \\diagdown l (a)$ is a union of languages $l_q = \\{v | \\delta(q,v) \\in f\\}$ for selected states $q \\in q$, and explain what are those selected states.as the answer i got this:by definition,$$l \\diagdown l(a) = igcup_{x \\in l} \\{ w : \\delta(q_0,xw) \\in f \\}.$$the idea now is to use the identity$$\\delta(q_0,xw) = \\delta(\\delta(q_0,x),w).$$you take it from here.but i still can't see why from the answer? could you please show me some simple example? i cannot figure this out from this answer. thanks.",
    "present_kp": [],
    "absent_kp": [
      "formal languages",
      "regular languages",
      "finite automata"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "generating a random string of characters and symbols. after coding this, i was wondering if there are any ways in which i can improve upon it. it generates a random string of characters and symbols the same length as what is entered by the user. it uses a separate list for each char list but i am sure it is using a very long-winded method!import random, sysletters = [a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z]lettersnum = [a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,0,1,2,3,4,5,6,7,8,9]letterssym = [a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,#,*, , $, +, -, .]lettersnumsym = [a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,0,1,2,3,4,5,6,7,8,9,#, *, , $, +, -, .]def generator(length, num, sym, caps): count = 0 password = try: test = int(length) except valueerror: error(length not valid number) if num.lower() == yes and sym.lower() == yes: merge = lettersnumsym elif num.lower() == yes: merge = lettersnum elif sym.lower() == yes: merge = letterssym while count <= int(length): password = password + merge[random.randint(0, len(merge) - 1)] count = count + 1 if caps.lower() == uppercase: password = password.upper() elif caps.lower() == lowercase: password = password.lower() print(password:,password)def error(error): print(error) sys.exit(0)running = truewhile running == true: length = input(how long do you want the password?) numbers = input(do you want it to include numbers?) symbols = input(do you want it to include symbols?) capital = input(do you want it to be uppercase or lowercase??) generator(length, numbers, symbols, capital) restart = input(do you want restart?) restart = restart.lower() if restart in (yes, y, ok, sure, ): print(restarting ----------------------------------) else: print(closing down) running = false",
    "present_kp": [
      "random"
    ],
    "absent_kp": [
      "python",
      "optimization",
      "python 3.x"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "x problems on fedora 15. i just upgraded from f13 to f15 using the installation dvd and it looked all went fine. when i first booted into f15, gdm was having troubles starting up. i tried removing xorg and reinstalled and issued xorg -configure. it came up with screens found, but none have usable configuration. fatal error: no screens foundattachments:xorg.0.log - error log/root/xorg.conf.new - file created when doing xorg -configure. /etc/x11/xorg.conf - my old xorg.conf file i was using for my intel card. originally got from the nvidia proprietary installer.i'm curious about these lines[ 117.519] (ii) loading /usr/lib/xorg/modules/drivers/vesa_drv.so[ 117.519] refusing to touch device with a bound kernel driver[ 117.519] (ww) falling back to old probe method for vesai just want to know if this can be corrected, before i go do a fresh install. i never thought an upgrade would be such a pain in the bottom. edit:i just found these line in my dmesg[ 5.144255] systemd[1]: /usr appears to be on a different file system than /.this is not supported anymore. some things will probably break (sometimes even silently) in mysterious ways. consult <url> for more information.and the above link mentions there are 23 obvious cases during an f15 install. could this be a reason for my x problems? if so, how do i merge my /usr back to /?the complete dump of dmesg, just in case it happens to give you enough clues.",
    "present_kp": [
      "fedora",
      "xorg",
      "upgrade"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "server-side rendering for search engines only (seo). i have a heavily javascript based web app which i want to make it indexable.i know that google bot and others agents can already see javascript, but i don't trust they will capture my content correctly and the problem is even bigger with asynchronous javascript. for examples: there are sections that are loaded only when the user scrolls down. there are also crawlers that can't see javascript, like facebook's crawler.can i just send pre-rendered html, using phantomjs or other headless browser, to the google bot? this would be exactly what my end-users sees, but my fear is i may suffer penalties for serving different content for the bots.",
    "present_kp": [
      "seo",
      "javascript"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "is there a field of research based around music and emotion in the brain?. i'm looking for research surrounding the scientific study of music and emotion and its effects on the brain. in particular, are there known parts of the brain which activate when playing or listening to music? is there a specific field of study based around music and cognitive science?",
    "present_kp": [
      "emotion",
      "music"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "nfs over tcp not available from (my server ip) - network is unreachable & permission denied. i am trying to boot linux mint from a server i set up in virtualbox. server is running in linux mint, and the client that i'm testing on (also virtualbox) is disk-less and booting over network. after splash screen vmlinuz and initrd are received correctly, until i get the following error:any info on what might be going wrong?",
    "present_kp": [
      "virtualbox",
      "nfs"
    ],
    "absent_kp": [
      "dhcp",
      "tftp"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to transfer a rootfs folder?. i just created a rootfs with debootstrap in a debian jessie virtualbox vm.now i would like to transfer this to the host, by first compressing the directory to a tar archive, so i can upload it and download it again on the host.i want the tar archive to preserve permissions and make an exact copy of the rootfs. so i used tar -czpvf --one-file-system like the arch linux wiki suggests.however i get tar: exiting with failure status due to previous errors, those previous errors seem to tar: $path: cannot open: permission denied errors.i am afraid that running tar as root will modify the ownership/permissions of the files and directories in the rootfs folder. how do i preserve the permissions and ownership of this rootfs when compressing it to a gzipped tar?",
    "present_kp": [
      "tar",
      "debootstrap"
    ],
    "absent_kp": [
      "root filesystem"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "transfering properties from subsets of $x^*$ to subsets of $x^{\\omega}$ by using the topology induces by cantor space. a language $l \\subseteq x^*$ is non-counting of order $n > 0$ iff for all $u,v, w \\in x^*$$$ uv^nw \\in l \\leftrightarrow uv^{n+1} w \\in l.$$a $\\omega$-language (set of infinite sequences) $l \\subseteq x^{\\omega}$ is non-counting of order $n > 0 $ iff for all $u,v \\in x^*, \\eta \\in x^{\\omega}$$$ uv^{n}\\eta \\in l \\leftrightarrow uv^{n+1} \\eta \\in l.$$when talking about languages over finite words, i.e. subsets $l$ of $x^*$ every regular, noncounting languages of order $n > 0$ fullfills the following property: let $u,v \\in x^*$ with$$ p_n(u) = p_n(v), i_n(u) = i_n(v), s_n(u) = s_n(v)$$and $u \\in l$ (where $p_n, s_n, i_n$ denote the prefix, suffix and infixes of length $n$) then also $v \\in l$, meaning if two words coincide in their prefix, suffix and infixes of fixed length $n > 0$ then they are either both in $l$ or both not in $l$.now when considering subsets of infinite words (and so just prefix and infixes), i.e. $l \\subseteq x^{\\mathbb n}$, then this does not hold any longer. for consider $l = x^* 0^{\\omega}$, then $l$ is regular and non-counting for each $n > 0$. fix some $n > 0$ and consider$$ \\xi = 0^n1^n0^{\\omega} \\quad \\mbox{ and } \\quad \\eta = 0^n1^n0^n1^{\\omega}$$then $p_n(\\xi) = p_n(\\eta)$ and $i_n(\\xi) = i_n(\\eta)$ but $\\xi \\in l$ and $\\eta\\notin l$.the space $x^{\\omega}$ is naturally topologized as the cantor space, i.e. the space with the common-prefix-metric, or equivalently generated by the basis $v\\cdot x^{\\omega}$, $v \\in x^*$. now the set $l = x^* 0^{\\omega}$ is not closed in cantor space, for example $1^n0^{\\omega}$ is in $l$, but its limit $1^{\\omega}$ is not in $l$. so i conjecture that if $l$ is closed then the property from finite sets also hold for infinite sequences, i.e. if $l$ is regular and non-counting, then if two words coinside in their prefix and infixes of some specified length, then they are either both in $l$ or not both. a language $l$ is closed in cantor space iff if it has the property that if $\\xi$ is such that every prefix of $\\xi$ is the prefix of some words from $l$, then $\\xi \\in l$.i conjecture that for closed sets $l$, that if $l$ is regular and non-counting, then membership could be decided by comparing prefix and infixes, but i am stuck with the proof, so any hints or suggestions for me?edit: there is a certain problem with the way it is stated, instead of locally testable use star-free, and instead of just presuppose a set to be closed i will presuppose it to be open or closed, so that the proposition is stable under complementation (which are the star-free and non-countable languages), see the comments.",
    "present_kp": [
      "topology"
    ],
    "absent_kp": [
      "fl.formal languages",
      "automata theory",
      "regular language"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "server and desktop crash. i work for a server hosting company, and have found a number of client servers throw out this into the system log (/var/log/syslog or /var/log/messages) when the system crashes:^@^@^@^@^@^@^@i've also experienced this on my home system, which leads me to believe that this is not focused on hardware, but feels more like a panic.i've never been able to diagnose this issue before, so i would highly appreciate any assistance on this.if you do require hardware specs, then i'll be able to provide you with the specs of my home pc.",
    "present_kp": [
      "syslog"
    ],
    "absent_kp": [
      "linux",
      "kernel panic"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "unable to access console on ubuntu 16.04 arm guest emulated on intel x86 64. i am unable to access the console of the ubuntu arm guest. i am using qemu and created the guest through virt-manager, my host is an intel x86 64 running ubuntu 16.04 lts. the guest runs fine but the virt-manager console window is blank. i have downloaded the ubuntu arm 16.04 cloud image and have extracted the initrd image and kernel from the cloud image. guest kernel: vmlinuz-4.4.0-64-generic-lpaeguest cpu : arm vexpress a9.guest initrd: initrd.img-4.4.0-64-generic-lpaekernel parameters: console=ttyama0 115200environment: kernel 4.8.0-34-lowlatency #36~16.04.1-ubuntu smp preempt wed dec 21 19:45:45 description: ubuntu 16.04.1 ltsqemu emulator version 2.5.0 (debian 1:2.5+dfsg-5ubuntu10.6)image: ubuntu-16.04-server-cloudimg-armhf.tar.gz",
    "present_kp": [
      "qemu",
      "arm"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "setting up a shop as a facebook app. yesterday, a friend told me that facebook allows users to host a shop as an app.is this allowed now?where can i find information about this?",
    "present_kp": [
      "facebook"
    ],
    "absent_kp": [
      "facebook apps",
      "shopping"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is the difference between /dev/sda and /dev/hda?. sometimes internal harddrives are available as /dev/sda (with subsequent media located in /dev/sdb etc), and other times available as /dev/hda. what is the difference between the two?does it vary between linux distributions, or is it based on the computer hardware, or perhaps the harddrive type?",
    "present_kp": [],
    "absent_kp": [
      "hard disk",
      "devices"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "virtualbox resume from hibernate does not mount shared folder. i am facing the problem that after returning from hibernate, it does not mount the shared folder anymore. as the following shows in /media folder:lrwxrwxrwx 1 root root 6 may 2 2014 cdrom -> cdrom0drwxr-xr-x 2 root root 4096 may 2 2014 cdrom0d????????? ? ? ? ? ? sf_dropboxd????????? ? ? ? ? ? sf_google_drivetherefore the ln -s in home folder returns unable to access it.drwxr-xr-x 2 ethanlim ethanlim 4096 may 2 2014 desktopdrwxr-xr-x 3 ethanlim ethanlim 4096 nov 9 16:31 documentsdrwxr-xr-x 2 ethanlim ethanlim 4096 nov 10 01:15 downloadslrwxrwxrwx 1 root root 17 aug 15 20:35 dropbox -> /media/sf_dropboxlrwxrwxrwx 1 root root 22 aug 23 00:15 googledrive -> /media/sf_google_drivedrwxr-xr-x 2 ethanlim ethanlim 4096 may 2 2014 musicdrwxr-xr-x 2 ethanlim ethanlim 4096 may 2 2014 picturesdrwxr-xr-x 2 ethanlim ethanlim 4096 may 2 2014 publicdrwxr-xr-x 5 ethanlim ethanlim 4096 aug 22 22:59 settingsdrwxr-xr-x 2 ethanlim ethanlim 4096 may 2 2014 templatesdrwxr-xr-x 2 ethanlim ethanlim 4096 may 2 2014 videosdoes anyone face this issue?",
    "present_kp": [
      "virtualbox",
      "hibernate"
    ],
    "absent_kp": [
      "shared folders"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "gst123 does not play files in right order when playing back folders/. i have the following problem with the gstreamer player gst123:when i'm playing back folders gst123 does not play the files in alphabetical order. when i have my files ordered like this:01.mp302.mp303.mp3i would expect the player to play the files back in this order, which it doesn't. i also did not really grasp which order it is actually using. gst123 does have some options to not play in order: -z, --shuffle shuffle playlist before playing. -z, --random play files in random order forever.i'm not using any of these.does anyone know this problem or a way to fix it?i'm using gst123 version: 0.3.3-1+b3the gstreamer plugins that i'm using are:gstreamer1.0-plugins-ugly/testing,now 1.12.2-1 i386gstreamer1.0-plugins-good/testing,now 1.12.2-1 i386gstreamer1.0-plugins-base/testing,now 1.12.2-1 i386gstreamer1.0-plugins-bad/testing,now 1.12.2-1 i386for some reasons i was not able to playback mp3 after some upgrades so i also installed:gstreamer1.0-fluendo-mp3i'm using a:debian gnu/linux buster/sidkernel: linux 4.11.0-1-686-pae",
    "present_kp": [
      "gstreamer"
    ],
    "absent_kp": [
      "audio"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "understanding udev rules and permissions in libusb. i had a problem with my scanner. xsane worked only as root. using it as normal user it didn't find any devices. adding the user to the groups saned or scanner didn't help. finally i solved the problem by changing # 'libusb' device nodessubsystem==usb, env{devtype}==usb_device, mode=0664to# 'libusb' device nodessubsystem==usb, env{devtype}==usb_device, mode=0666in /lib/udev/rules.d/50-udev-default.rules however i do not understand why this works and if it has any negative side effects. i guess that this gives write permissions to all users on any usb device, but i don't know if and why this is correct and why this solves the scanner problem.could anyone explain in detail why this works and wether it has any side effects.my system is: ubuntu 12.04, the scanner is a hp psc 1200 all in one device hplip is installed from the repository.",
    "present_kp": [
      "permissions",
      "usb",
      "udev"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "return compound structure from getter method in dto. suppose the following dto class. which of the two getters breaks encapsulation least?class foo { public: /*return the most primitive type. caller do not need to worry about what array is*/ const rectype* databeginget() const; const rectype* dataendget() const; /*return a reference to the array. caller do not need to call two methods before iteration */ const array<rectype>& dataget() const {return m_data;} private: array<rectype> m_data; };",
    "present_kp": [
      "encapsulation"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to write a very simple wrapper that provides default parameters?. given a program that requires some parameters, e.g. program -in file.in -out file.out, what would be the simple-most approach to write a bash script that could be called with or without any of these parameters and use default values for each?script -in otherfile would run program -in otherfile -out file.out,script -out otherout -furtherswitch would run program -in file.in -out otherout -furtherswitch etc.",
    "present_kp": [
      "bash",
      "parameter"
    ],
    "absent_kp": [
      "shell script"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what are the differences between event sourcing and service layer pattern?. i am reading a book on architecting enterprise applications. in this book the event sourcing pattern is introduced which can be used as the command part of a command and query responsibility segregation (cqrs) architecture. event sourcing is described by martin fowler as:the fundamental idea of event sourcing is that of ensuring every change to the state of an application is captured in an event object, and that these event objects are themselves stored in the sequence they were applied for the same lifetime as the application state itself.i am used to applications that use the service layer pattern, which i would describe as: changes in the ui call a method, which then triggers the appropriate service call in a middleware layer which delegates the call to the backend where information is updated and notifies the caller of the result. this description of a service layer does, to me, not seem very different from event sourcing. what are the differences between event sourcing and the service layer pattern?",
    "present_kp": [
      "architecture",
      "event sourcing"
    ],
    "absent_kp": [
      "architectural patterns",
      "services"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "add existing models to a relation using nested attributes. i needed to add existing models to a has-many relation using nested attributes so i overwrote thumbnails_attributes=:class gallery < activerecord::base has_many :thumbnails, dependent: :nullify accepts_nested_attributes_for :thumbnails def thumbnails_attributes=(thumbnails_attributes) ids = thumbnails_attributes.map { |t| t['id'] } (ids - thumbnail_ids).each do |id| thumbnail = thumbnail.find id thumbnails << thumbnail end super(thumbnails_attributes) endendclass thumbnail < activerecord::base belongs_to :galleryendgiven the following data (as fixture):thumbnails.ymlone: gallery: onetwo: gallery:galleries.ymlone: name: mystringi can now run:galleries(:one).update(thumbnails_attributes: ['id' => thumbnails(:two).id])and galleries(:one).thumbnails will return both thumbnails.while it works pretty well, i wanted to ask for reviews and ways to improve/clean this code.the only issue i have is with include? (which is used by assert_includes in my tests):galleries(:one).update(thumbnails_attributes: ['id' => thumbnails(:two).id])galleries(:one).thumbnails.include? thumbnails(:two) # falsegalleries(:one).thumbnails # [#<thumbnail one>, #<thumbnail two>]galleries(:one).thumbnails.include? thumbnails(:two) # trueit looks like i have to reload the relation for include? to work, but i haven't found a fix nor a way to properly test it.galleries(:one).thumbnails.to_a.include? thumbnails(:two) works, include? doesn't appear to load the association.",
    "present_kp": [],
    "absent_kp": [
      "ruby",
      "unit testing",
      "ruby on rails",
      "active record"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "parallel scientific computation software development language?. i want to develop a parallel scientific computation software from scratch. i want some thoughts on which language to start. the program involves reading/writing data to txt files and doing heavy computations in parallel, with many lu factorizations and the use of sparse linear solvers. the candidate solutions i was thinking are fortran 2003/2008 with openmp or co-array, c++ with openmp cilk+ or tbb, python. any other, documented, suggestions are welcome! i know very well c, fortran and java (in that order). i've done some scripting in python but basic stuff.i know fortran is very fast, but, hard to maintain and parallelize. c++ is said to be slow unless you use external libraries etc python i like, but is it realistic to write a full scale, industrial level software upon?the software needs to be able to handle big amounts of data and be effective with scientific computations. the performance is of the essence.for the background, i already have a working software written in fortran. many people were involved in development over many years and the code is really dirty. maintaining and parallelizing the code has proved a nightmare and i'm thinking of alternatives.petros",
    "present_kp": [
      "python",
      "c++",
      "fortran"
    ],
    "absent_kp": [
      "hpc",
      "simulation"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "infosec reverse engineering course. did anyone take infosec institute's reverse engineering course? i'd like to know if it's worth my money, i didn't find feedback about the course on the web. thank you.for the people downvoting my post...why is that? because of the tags? using a tag is mandatory and i couldn't post with more appropriate tags like institute or company or something like that. the question is pertinent because it's about an institute that teaches reverse engineering. so i really don't get why the downvotes...don't you want to know how good a r$2000 reverse engineering course is before taking it??",
    "present_kp": [],
    "absent_kp": [
      "disassembly",
      "program analysis"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "unidentified address in gmail. many times, when i copy&paste an email address into the to field in a gmail message, it refuses to send the email, saying that the address has not been identified. this is very frustrating. is there any solution other than just manually typing the address?",
    "present_kp": [
      "gmail"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "replacing a spreadsheet formula with its result when the result satisfies a condition. in google sheets i'm setting up a sheet with a column of formulas referencing a cell in which there is an hourly rate. that hourly rate can occasionally change, but i don't want previous calculations to be affected, only future calculations (ie, cells further down the column).a great script solution was posted here: how do you replace a formula with its result?with this script from red red wine:function freezeoutput(){ var sheet = spreadsheetapp.getactivespreadsheet().getsheetbyname(sheet); var range = sheet.getrange(a1:a10); range.copyto(range, {contentsonly:true});}but because formulas that have not yet been triggered show as $0.00 in my column, those cells are converted from formulas to 0's.is there any way to make this script work on a cell only if the formula result is > 0?",
    "present_kp": [],
    "absent_kp": [
      "google spreadsheets",
      "google apps script"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "run a vim macro from inside a bash script. in a bash script i want to edit a file. is there a way to write a macro in vim, save it and then call it in bash script?",
    "present_kp": [
      "bash",
      "vim"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "google type image gallery layout with flexbox. im trying to create the same gallery with expander very similar to google images layout where it expands when you click on the tile.i am however trying to do this with flexbox, but running into an issue where i need to have the expanded section full screen width. the problem is that it aligns with the sibling. i made a fiddle to show what im trying to achieve and what the problem is.fiddle of flexbox gallery layoutherewith the current code layout <div class=wrapper><div class=header> <h1>header section</h1></div><div class=content> <div class=item> <div class=item-inner> <div class=item-header>header contnet</div> <div class=item-body>body content</div> </div> <div class=item-expander hidden>content of expander</div> </div> <div class=item> <div class=item-inner> <div class=item-header>header contnet</div> <div class=item-body>body content</div> </div> <div class=item-expander viible>content of expander</div> </div> <div class=item> <div class=item-inner> <div class=item-header>header contnet</div> <div class=item-body>body content</div> </div> <div class=item-expander hidden>content of expander</div> </div> <div class=item> <div class=item-inner> <div class=item-header>header contnet</div> <div class=item-body>body content</div> </div> <div class=item-expander hidden>content of expander</div> </div> <div class=item> <div class=item-inner> <div class=item-header>header contnet</div> <div class=item-body>body content</div> </div> <div class=item-expander hidden>content of expander</div> </div> <div class=item> <div class=item-inner> <div class=item-header>header contnet</div> <div class=item-body>body content</div> </div> <div class=item-expander hidden>content of expander</div> </div> <div class=item> <div class=item-inner> <div class=item-header>header contnet</div> <div class=item-body>body content</div> </div> <div class=item-expander visible>content of expander</div> </div> <div class=item> <div class=item-inner> <div class=item-header>header contnet</div> <div class=item-body>body content</div> </div> <div class=item-expander hidden>content of expander</div> </div> <div class=item> <div class=item-inner> <div class=item-header>header contnet</div> <div class=item-body>body content</div> </div> <div class=item-expander hidden>content of expander</div> </div></div>",
    "present_kp": [
      "layout"
    ],
    "absent_kp": [
      "css",
      "html5"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "reverse groups challenge. i'm trying to do the reverse groups challenge on codeeval:given a list of numbers and a positive integer k, reverse the elements of the list, k items at a time. if the number of elements is not a multiple of k, then the remaining items in the end should be left as is.my logic is functional, but when i try to submit my solution is says that it timed out at 10 seconds. how can i improve the speed here?file.open(argv.first).readlines.each do |line| values, k = line.split(';') k = k.to_i values = values.split(',') current = values.shift(k) str = while current.count % k == 0 do current.reverse_each{ |val| str += #{val},} current = values.shift(k) end str << #{current.join(',')} puts strendother attempt:open(argv[0]).readlines.each do |line| values, k = line.chomp.split(';') k = k.to_i values = values.split(',') current = values.shift(k) str = while current.count % k == 0 do current.reverse_each{ |val| str << #{val},} current = values.shift(k) end str << #{current.join(',')} puts strend",
    "present_kp": [
      "io"
    ],
    "absent_kp": [
      "ruby",
      "programming challenge",
      "comparative review",
      "time limit exceeded"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "harddrive remaining space does not compute. i have a server which has a handful of services (apache, mysql, dns, nagios), and have been having issues with the harddrive filling up. upon further review, i noticed that the percentage used according to df doesn't make any sense. used space plus available space does not equal total space. at the time i first became aware of this (thanks to a nagios notification), the available space was less than 1 gb out of our 40 gb harddrive. according to du and df used column, the actual used was 11gb. that's a difference of 28gb. at the moment it's fluctuating only a little, but sometimes it fluctuates at over 1000 bytes per second. see here:$ df / && sleep 5 && df /filesystem 1k-blocks used available use% mounted on/dev/vzfs 41943040 <phone> 29195724 29% /filesystem 1k-blocks used available use% mounted on/dev/vzfs 41943040 <phone> 29173016 29% /this is a vps that i don't have physical access to. it's running xfs, so a normal reboot fsck isn't an option either. it doesn't do it constantly, and log files don't seem to indicate anything going on. iotop doesn't even show any activity at all. twist, we have people actively using this machine so we can't just format.output of df -a:$ sudo df -afilesystem 1k-blocks used available use% mounted on/dev/vzfs 41943040 <phone> 29064180 29% /proc 0 0 0 - /procsysfs 0 0 0 - /sysnone 524288 4 524284 1% /devnone 0 0 0 - /dev/ptsnone <phone> 12 <phone> 1% /var/tmpfsnone 0 0 0 - /proc/sys/fs/binfmt_misc/etc/named 41943040 <phone> 29064180 29% /var/named/chroot/etc/named/var/named 41943040 <phone> 29064180 29% /var/named/chroot/var/named/etc/named.rfc1912.zones 41943040 <phone> 29064172 29% /var/named/chroot/etc/named.rfc1912.zones/etc/rndc.key 41943040 <phone> 29064172 29% /var/named/chroot/etc/rndc.key/usr/lib64/bind 41943040 <phone> 29064164 29% /var/named/chroot/usr/lib64/bind/etc/named.iscdlv.key 41943040 <phone> 29064156 29% /var/named/chroot/etc/named.iscdlv.key/etc/named.root.key 41943040 <phone> 29064148 29% /var/named/chroot/etc/named.root.key",
    "present_kp": [
      "df"
    ],
    "absent_kp": [
      "linux"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is it possible to make the buyer pay the fee when accepting payments via paypal?. when accepting payments via paypal is it possible to make the buyer pay the fee?",
    "present_kp": [
      "paypal"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "is there any limit on the number of speed tests from a given ip on <url>. does the website <url> enforce any limit on the number of speed tests from a given ip? (e.g. max 1000 tests per day)",
    "present_kp": [],
    "absent_kp": [
      "speedtest.net"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how do i find the max and min value of an array in 3n/22 comparisons?. so i'm using this method to find the min and max value of an array simultaneously where i split the array into n/2 and n/2 parts. i then keep splitting each part until i have either a pair of numbers or a single number.what i'm trying to do now is the same thing but i'm trying to come up with a method that will always use 3n/22 comparisons. the method above doesn't use 3n/22 comparisons each time. so i just want to visualize how this is done on an array before i start to programming the method.",
    "present_kp": [
      "comparison"
    ],
    "absent_kp": [
      "algorithms",
      "search algorithms",
      "arrays"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "java is not actually a pure object oriented programming language since it needs primitives why?. here my question it is said that java is not actually a pure object oriented programming language since it needs primitives i want to know that how data types can affect to java be an pure object oriented langugae ?",
    "present_kp": [
      "java"
    ],
    "absent_kp": [
      "java ee",
      "java8"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "passwordless login (from physical console and from others users). how to configure an user (in debian) to login without password ? an user without password needs to login in physical terminals (ttys - not pty) either using su - user from another user, without being asked for password.(secure isn't a requirement in this environment)",
    "present_kp": [
      "users",
      "login",
      "password"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "use awk interactively through a pipe. looking at this question, i've noticed that awk can't read userinput if the file is being piped into standard input, but it doesbehave as expected if reading input from a file given as a commandline parameter.for instance, if you have the following begin block in your awk script:begin { printf enter input: getline var < -}if you run it like awk -f ./script.awk file.txt, it will ask promptfor user input, and then will proceed processing file.txt. however,if you run like cat file.txt | awk -f ./script.awk, i suppose awkwill interpret what it is getting from the pipe as the user's input(so getline will fill var with the first line of file.txt).is there a way to make awk behave like it is reading from a file, but being used in a pipe?i can use a temporary file, sure, but this is far from elegant.",
    "present_kp": [
      "awk",
      "pipe"
    ],
    "absent_kp": [
      "shell"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "are ipv6 link-local addresses assigned by the kernel or by a userspace program. when ipv6 is enabled for an interface, it will get a link-local address assigned automatically based on the mac of the network interface.but who assigns this address? is it done in the kernel, or by some userspace program that also sets up the interface?ideally, i would also be interested in a link to the actual source code where it is done.",
    "present_kp": [
      "kernel",
      "network interface",
      "ipv6"
    ],
    "absent_kp": [
      "networking"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what are dry, kiss, solid, etc. classified as?. is something like dry a design pattern, a methodology, or something in between? they do not have specific implementations that could neccessarily be demonstrated(even if you can easily demonstrate a case not using something like kiss... see the daily wtf for a plethora of examples), nor do they fully explain a development process like a methodology generally would. where does that leave these types of rule of thumb's?",
    "present_kp": [
      "methodology"
    ],
    "absent_kp": [
      "design patterns",
      "terminology"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "pointer ownership tips. in some libraries i've seen, a lot of emphasis is put into change of ownership of pointers, like for example a class method that allocates and returns a pointer to an object while relinquishing its ownership to the caller.my question is: how would you design if you had to code something with the constraint that no pointer ownership gets transferred, ever ?",
    "present_kp": [
      "design",
      "pointers"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "determining if a word is an anagram of another. the question was to see if a word is an anagram of another word.how would you improve this? it returns true if an anagram, or false otherwise.# write the function is_anagramdef is_anagram(test, original): if len(test) != len(original): return false for letter in test.lower(): if letter not in original.lower(): return false for letter in original.lower(): if letter not in test.lower(): return false return true",
    "present_kp": [],
    "absent_kp": [
      "python",
      "strings"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "inferences about branching in tsp algorithm. i am building a program that uses branching-and-bounding to find an optimal path in a complete graph (the heuristic, faster algorithm is the second part). i have to begin and end at node 0. i was given a large hint that eliminating hopeless branches early in the algorithm will lead to an overall much faster runtime. i have tried several things: greedily selecting the smallest edge as either the first or last edge in the tour, branching into two subsets of solutions with one branch being solutions including the smallest available edge and the other being solutions that exclude the largest available edge. neither have proved to be worth any salt, and i was wondering if there were any clever optimizations to branch-and-bound i could implement to help speed up my program. any help will be appreciated!edit: any advice on computing better bounds and how to use them effectively is also welcomed.",
    "present_kp": [
      "optimization"
    ],
    "absent_kp": [
      "graph traversal"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "quicksort and middle pivot. i am having a head ache understanding quicksort with middle pivot. i found lot of explanations about using left most or right most, but not many about a middle one.can i safely assume these?:if left and right pointers meet at the same position, means that theelement at that position is at its final sorted position, so i can split the list in two without including that element (list1 length + list2 length = list length -1).if left and right cross each other ( so left > right), means that noelement is at its final sorted position yet, so i must split the list in two using left and right as boundaries ( list1 length + list2 length = list length).is this right?thanks.update : the reason why i want to use a middle pivot, is to implement the median algorithm that increases qs speed. in this techique, the pivot is selected by approximating the list middle value: <url>",
    "present_kp": [],
    "absent_kp": [
      "algorithms",
      "sorting"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "possible to communicate between two networks without bridging?. i have two interfaces connected to a real machine, eth0 and vboxnet2 (a virtualbox bridged network).eth0 is sitting on 192.168.15.x and is connected to my adsl modem. vboxnet2 is on 192.168.200.x.kernel ip routing tabledestination gateway genmask flags metric ref use ifacedefault 192.168.15.1 0.0.0.0 ug 0 0 0 eth0192.168.15.0 * 255.255.255.0 u 1 0 0 eth0192.168.200.0 * 255.255.255.0 u 0 0 0 vboxnet2should i be able to ssh from (say) 192.168.15.100 to 192.168.200.4? my understanding of the routing table is that it says if you see any packets destined for 192.168.200.x, send them to the vboxnet2 interface. doesn't seem like i need nat, iptables magic, or bridged networking for that. but it isn't working (no route to host).long time linux user, but networking details have fallen off the mental stack in the last few years as networkmanager started to do everything...",
    "present_kp": [
      "networking"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "training data batch size. i'm building a function to pass training information for my tensorflow model.it is similar to keras:model.fit(features, labels, epochs=150, batch_size=10)the dataset in use is pima indians which contains exactly 768 records.in keras when i pass epochs=150, batch_size=10 it runs fine. in my model, since at epoch 78 there is no more data (78 * 10), we start feeding empty information.what would be the correct behavior when training a model and number of epochs already consumed all data for model. the obvious answer would be stop training or continue feeding data to model and restart counter and start passing information from beginning. any feedback is appreciated. def import_data(filename, batch_size, stride): if filename: dataset = np.loadtxt(filename, delimiter=,) # expand dimensions of labels if stride > -1: start = batch_size else: raise valueerror('invalid batch size') features = dataset[:, 0:8] labels = np.expand_dims(dataset[:, 8], axis=1) return features[start:stride], labels[start:stride]_batch_size = 10for step in range(_num_epochs): # data print step batch_x, batch_y = import_data(filename, _batch_size * step, _batch_size * (step + 1)) print batch_x, batch_y",
    "present_kp": [
      "dataset",
      "tensorflow",
      "training"
    ],
    "absent_kp": [
      "python"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "given a low resolution video, is it possible to create a higher resolution image. this question is purely theoretical as i've not been able to find anyone who's done such a thing, so my question is - is it possible, and if not why?say you have a short video (cctv) of a face, or number (car licence) plate, and that video is of too poor quality to discern who that person is or what the number plate says, would it be possible to combine details from several frames of video to interpolate and thus make a clearer image than can be gained from a single frame?surely some frames would contain detail that others lack (due to pixel boundaries, shadows etc)? are there any examples of machine learning or cv that do this?",
    "present_kp": [],
    "absent_kp": [
      "image processing",
      "computer vision"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "dd command indicates not enough disk space - trying to format sd card for raspberry pi. i have been trying to format an sd card with the lastest debian jessie-lite image for use with raspberry pi. when using the dd command, it states that there is no space left on device after copying 10 megs. i have searched se and have tried to use various answers to questions but i always end up back at the same place. below are the outputs of dd, fdisk, df and ls commands that may be of interest. /dev/sdb is the sd carddd bs=4m if=/home/user/downloads/2017-02-16-raspbian-jessie-lite.img of=/dev/sdbdd: error writing /dev/sdb: no space left on device3+0 records in2+0 records out10485760 bytes (10 mb) copied, <phone> s, 760 mb/sfdisk -l /dev/sdbdisk /dev/sdb: 10 mib, <phone> bytes, 20480 sectorsunits: sectors of 1 * 512 = 512 bytessector size (logical/physical): 512 bytes / 512 bytesi/o size (minimum/optimal): 512 bytes / 512 bytesdisklabel type: dosdisk identifier: 0xdbcc7ab3device boot start end sectors size id type/dev/sdb1 8192 137215 129024 63m c w95 fat32 (lba)/dev/sdb2 1<phone> <phone> 1.3g 83 linuxls -al /dev/sdb*-rw-r--r-- 1 root root <phone> mar 3 22:04 /dev/sdbbrw-rw---- 1 root disk 8, 17 mar 3 22:05 /dev/sdb1brw-rw---- 1 root disk 8, 18 mar 3 22:05 /dev/sdb2brw-rw---- 1 root disk 8, 19 mar 3 22:05 /dev/sdb3df -hfilesystem size used avail use% mounted on/dev/sda1 226g 7.3g 207g 4% /udev 10m 10m 0 100% /devtmpfs 1.6g 9.3m 1.6g 1% /runtmpfs 3.9g 112k 3.9g 1% /dev/shmtmpfs 5.0m 4.0k 5.0m 1% /run/locktmpfs 3.9g 0 3.9g 0% /sys/fs/cgrouptmpfs 792m 4.0k 792m 1% /run/user/119tmpfs 792m 8.0k 792m 1% /run/user/1000",
    "present_kp": [
      "raspberry pi",
      "raspbian",
      "dd",
      "fdisk"
    ],
    "absent_kp": [
      "disk usage"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "vsftpd closes conntion with code 421 when listing directory content. i am currently struggling with some kind of connection problem with vsftpd.i am using an arch linux and have vsftpd version 3.0.3 installed.i don't use iptables or anything. when i connect to my ftp server, i am successfully able to log in and browse directories. i am also able to up- and download files.the ftp user is chrooted into his home directory, and all subdirectories are owned by the ftp user.the current structure:|- home_dir |- dir1 |- dir2 |- dir3 |- dir3 |- dir4 |- ...i am able to cd into dir1, do a ls and up / download files.i am also able to cd into dir2, but here comes the tricky part:i can perfectly upload files into dir2, but when i do a ls, the server is closing the connection saying: 421 service not available, remote server has closed connection.i figured out, that the cause are several of the subdirectories of dir2. i figured out, that the existence of 8 of all subdirectories are causing the connection to be closed. once i remove them, the listing works fine. all names are containing lower letters from a to z, no special chars.my ftp config:log_ftp_protocol=yesanonymous_enable=nouserlist_enable=yesuserlist_file=/etc/vsftpd.user_listuserlist_deny=nopasv_addr_resolve=yespasv_address=mydomain.compasv_enable=yespasv_min_port=40020pasv_max_port=40030local_enable=yeswrite_enable=yeslocal_umask=002dirmessage_enable=yesxferlog_enable=yesconnect_from_port_20=yeschown_uploads=yeschown_username=myuserftpd_banner=welcome.chroot_local_user=yesand that's what the servers ftp-log says:wed dec 28 13:55:06 2016 [pid 2] connect: client <client_ip>wed dec 28 13:55:06 2016 [pid 2] ftp response: client <client_ip>, 220 welcome.wed dec 28 13:55:07 2016 [pid 2] ftp command: client <client_ip>, user myuserwed dec 28 13:55:07 2016 [pid 2] [myuser] ftp response: client <client_ip>, 331 please specify the password.wed dec 28 13:55:07 2016 [pid 2] [myuser] ftp command: client <client_ip>, pass <password>wed dec 28 13:55:07 2016 [pid 1] [myuser] ok login: client <client_ip>wed dec 28 13:55:07 2016 [pid 3] [myuser] ftp response: client <client_ip>, 230 login successful.wed dec 28 13:55:07 2016 [pid 3] [myuser] ftp command: client <client_ip>, opts utf8 onwed dec 28 13:55:07 2016 [pid 3] [myuser] ftp response: client <client_ip>, 200 always in utf8 mode.wed dec 28 13:55:07 2016 [pid 3] [myuser] ftp command: client <client_ip>, pwdwed dec 28 13:55:07 2016 [pid 3] [myuser] ftp response: client <client_ip>, 257 / is the current directorywed dec 28 13:55:07 2016 [pid 3] [myuser] ftp command: client <client_ip>, type iwed dec 28 13:55:07 2016 [pid 3] [myuser] ftp response: client <client_ip>, 200 switching to binary mode.wed dec 28 13:55:07 2016 [pid 3] [myuser] ftp command: client <client_ip>, pasvwed dec 28 13:55:07 2016 [pid 3] [myuser] ftp response: client <client_ip>, 227 entering passive mode (<server_ip>,156,86).wed dec 28 13:55:07 2016 [pid 3] [myuser] ftp command: client <client_ip>, listwed dec 28 13:55:07 2016 [pid 3] [myuser] ftp response: client <client_ip>, 150 here comes the directory listing.wed dec 28 13:55:08 2016 [pid 3] [myuser] ftp response: client <client_ip>, 226 directory send ok.wed dec 28 13:55:09 2016 [pid 3] [myuser] ftp command: client <client_ip>, cwd /dir1wed dec 28 13:55:09 2016 [pid 3] [myuser] ftp response: client <client_ip>, 250 directory successfully changed.wed dec 28 13:55:09 2016 [pid 3] [myuser] ftp command: client <client_ip>, pasvwed dec 28 13:55:09 2016 [pid 3] [myuser] ftp response: client <client_ip>, 227 entering passive mode (<server_ip>,156,92).wed dec 28 13:55:09 2016 [pid 3] [myuser] ftp command: client <client_ip>, listwed dec 28 13:55:09 2016 [pid 3] [myuser] ftp response: client <client_ip>, 150 here comes the directory listing.wed dec 28 13:55:09 2016 [pid 3] [myuser] ftp response: client <client_ip>, 226 directory send ok.wed dec 28 13:55:11 2016 [pid 3] [myuser] ftp command: client <client_ip>, cwd /dir1/dir2wed dec 28 13:55:11 2016 [pid 3] [myuser] ftp response: client <client_ip>, 250 directory successfully changed.wed dec 28 13:55:11 2016 [pid 3] [myuser] ftp command: client <client_ip>, pasvwed dec 28 13:55:11 2016 [pid 3] [myuser] ftp response: client <client_ip>, 227 entering passive mode (<server_ip>,156,87).wed dec 28 13:55:11 2016 [pid 3] [myuser] ftp command: client <client_ip>, listwed dec 28 13:55:11 2016 [pid 3] [myuser] ftp response: client <client_ip>, 150 here comes the directory listing.the log always stops after 150 here comes the directory listing.. i tried it several times.and this is, what i see, when i use the ftp command on my terminal:$ ftp -n 127.0.0.1connected to 127.0.0.1.220 welcome.ftp> user myuser331 please specify the password.password:230 login successful.ftp> ls200 port command successful. consider using pasv.150 here comes the directory listing.drwxr-xr-x 4 1000 1000 4096 dec 28 13:27 dir1226 directory send ok.ftp> cd dir1250 directory successfully changed.ftp> ls200 port command successful. consider using pasv.150 here comes the directory listing.-rwxr-xr-x 1 1000 1000 469504 dec 23 03:03 file_1-rw-r--r-- 1 1000 1000 186 feb 19 2016 file_2-rw-r--r-- 1 1000 1000 121856 dec 23 00:16 file_3-rw-r--r-- 1 1000 1000 118272 feb 12 2016 file_4-rw-r--r-- 1 1000 1000 88232 mar 22 2016 file_5-rw-r--r-- 1 1000 1000 64680 mar 22 2016 file_6-rw-r--r-- 1 1000 1000 101032 mar 22 2016 file_7-rw-r--r-- 1 1000 1000 6144 mar 22 2016 file_8-rw-r--r-- 1 1000 1000 899584 mar 22 2016 file_9drwxrwxr-x 42 1000 1000 4096 dec 28 11:38 dir2226 directory send ok.ftp> cd dir2250 directory successfully changed.ftp> pwd257 /dir1/dir2 is the current directoryftp> ls200 port command successful. consider using pasv.150 here comes the directory listing.421 service not available, remote server has closed connectionthanks a lot for help.",
    "present_kp": [
      "arch linux",
      "ftp",
      "vsftpd"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "replace strings in a file. i had to create an executable to search and replace strings in a file. this is to be used in my installer for text file manipulation. i have various placeholders in configuration files that i need to replace with proper values after querying the end user system.i have written the following code. this being my first real experience with c#, i do not expect the code to be any good. i have come up with this code based upon a couple of hours of reading msdn and stackoverflow. i needed someone to review this. there is no one greater than the community to do this.please review this and let me know what modification are necessary.i am specifically concerned about the exception handling.i have included two try-catch blocks.my logic:if i have all operations under one try-catch block, the failure of one process will block the execution of all others. that is, if my text = regex.replace(text, args[1], args[2]);has an exception and if i have writelog(args[0], args[1], args[2], strexception, intstatus);also in the same try block, it will not execute and i will not have any logs.also, if i have the writelog(args[0], args[1], args[2], strexception, intstatus);in the catch block, any exception in that method cannot be caught. yes i can have try-catch in the writelog method but i see no problems in my approach too.so first, i have a try-catch for file manipulation and then another for log write.please ignore the log path(hardcoded as d:).class program{ static int main(string[] args) { //args[0] = the file that needs modification //args[1] = the string to replace //args[2] = the string with which to replace args[1] int intstatus = 0; string strexception = ; //get out immediately if no/improper arguments are found if (args.length < 3) { intstatus = 1; return intstatus; } try { streamreader streamreader = new streamreader(args[0]); string text = streamreader.readtoend(); streamreader.close(); text = regex.replace(text, args[1], args[2]); streamwriter streamwriter = new streamwriter(args[0]); streamwriter.write(text); streamwriter.close(); } catch (exception ex) { //text file manipulation failed strexception = ex.tostring(); intstatus = 2; } try { writelog(args[0], args[1], args[2], strexception, intstatus); } catch { //log write failed if (intstatus == 0) { intstatus = 3; //if intstatus = 1 when we get here, no need to modify the value(its a complete failure) //if intstatus = 0 when we get here, make intstatus = 2 so as to clearly distinguish a log write failed //error from a text file manipulation failed error } } return intstatus; } //the new logger for the exe static void writelog(string arg0, string arg1, string arg2, string exception, int status) { string strgreplogfilename = string.format(d:\\templogs\\grep-{0:yyyy-mm-dd_hh-mm-ss-tt}.log, datetime.now); streamwriter greplog = new streamwriter(strgreplogfilename, true); greplog.writeline(argument 1: + arg0); greplog.writeline(argument 2: + arg1); greplog.writeline(argument 3: + arg2); greplog.writeline(status: + status.tostring()); greplog.writeline(exception: + exception); greplog.close(); }}edit 1:it is an absolute pity that i cannot mark multiple answers.",
    "present_kp": [
      "c#",
      "strings",
      "exception handling",
      "file"
    ],
    "absent_kp": [
      "beginner"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "sed command with option -n and '$='. i want to print number of lines in a file using the sed command. i have the following line for that, please can any one explain in detail.sed -n '$=' myfile.txt",
    "present_kp": [
      "sed"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to sort by multiple columns?. i have a tab-separated file, and i want to sort it by its columns 9, 14, and 16. by this i mean that all rows that agree at column 9 should be sorted by column 14, and among these, those that also agree on column 14 should be sorted by column 16. (all columns should be sorted ascendingly, but the first two should be sorted alphabetically, and the remaining one numerically.)i've tried various forms of (gnu) sort, but i don't get the desired sort order. the man page has not helped. can anyone tell me the right incantation for achieving the sort described above?",
    "present_kp": [
      "sort"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to get cross-references to a struct type in ida by idapython and add comments to variables of the struct type. i'm writing an ida plugin using idapython in order to add comments (located in database) to variables of struct types. in order to do this, firstly, i need to get the list of cross-references to a given structure type (e.g. struct bitmapinfo) which can be found in structure subview in ida. i know ida provides this function from version 6.2 by right-button mouse clicking on the structure name and selecting list cross references to. a window like the following will be popped-up:each item of the list in above picture is either an address where a global variable of type %structure name% (here is bitmapinfo) is declared or a position where a local variable of type structure name is defined. the former is like(here is type guid, not bitmapinfo).the latter is likethis is the position where ida declare local variables based on its identified type.i wonder if there is a way to get these data by idapython.note: this is different from cross-references to a(ll) member(s) of a struct type, which can be got by right-button mouse clicking on the structure member name, shown as the followingbefore asking here, i do it like:#code 1ea = idc.locbyname(%structure name%) frm = [x.frm for x in idautils.xrefsto(ea)]i think i have got the whole list of cross-references to %structure name% using my above code. however, i found many eas in the list seem like ineffective such as '0xff0052c9' (maxea is 0x108f800). however, i guess my code has got the desired result because the length of returned list is equal to the number of items in the list shown as the 1st picture. but i can't explain the result especially the (seemingly) ineffective ones. also, when i add comments to the addresses in the list using the following code#code 2for ea in xrefs_list: # each cross-reference to the given struc type if repeatable: # add repeatable comment 'cmt' at address 'ea' idc.makerptcmt(ea, cmt) else: # add comment 'cmt' at address 'ea' idc.makecomm(ea, cmt)i found i only added comments to the effective addresses which are between idc.minea() and idc.maxea(), and these addresses are places where global instances of the queried struct type are declared, as shown in the 2nd picture.my questions are:is my above code (code 1) correct to get all cross-references to a struct type? if it is, how to explain those seemingly ineffective addresses (above 0xff000000)how to add comments to other cross-reference addresses other than the references to global instances of the struct type?",
    "present_kp": [
      "ida",
      "idapython"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "sort observablecollection after added new item. for a wpf application, i have to sort the items in observablecollection after added new item. public void updatesource(observablecollection<sometype> source, sometype newitem){ source.add(newitem); sortsource(source, newitem);}private void sortsource(observablecollection<sometype> source, sometype item){ var oldindex = source.indexof(item); var list = source.orderby(_=>_.someproperty).tolist(); var newindex = list.indexof(item); source.move(oldindex, newindex);}in order to keep the observable functionality for binding element, the observablecollection reference cannot be changed, so i used an assistance list to help me sort the souce.any improvement about this?",
    "present_kp": [
      "wpf"
    ],
    "absent_kp": [
      "c#"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "php e-mail form with recaptcha. i want to ask if you can review the code for a simple contact form and the php code which sends me an email once a visitor fills the form and is validated through recaptchaon my index.php file here are the form and php codehtml:<form id=contactform action=index.php#contact method=post class=form role=form> <div class=form-group> <input class=form-control required id=vname name=vname placeholder=your name type=text required /> </div> <div class=form-group> <input class=form-control required id=vemail name=vemail placeholder=your email type=email required /> </div> <div class=form-group> <textarea class=form-control required id=msg name=msg placeholder=your message rows=7 required></textarea> </div> <div class=form-group> <div class=g-recaptcha data-sitekey=google public key></div> </div> <div class=form-group> <input type=submit class=btn btn-success form-send value=send> </div> </form>php:<?php $captcha; if (isset($_post['g-recaptcha-response'])) { $captcha = $_post['g-recaptcha-response']; } // check for correct recaptcha $response = file_get_contents(<url> secret key&response= . $captcha . &remoteip= . $_server['remote_addr']); if (!$captcha || $response.success == false) { echo your captcha response was wrong; exit ; } else { // check for blank fields.. if ($_post[vname] == || $_post[vemail] == || $_post[msg] == ) { echo please fill all required fields;} else { // check if the sender's email input field is filled out $email = $_post['vemail']; // sanitize e-mail address $email = filter_var($email, filter_sanitize_email); // validate e-mail address $email = filter_var($email, filter_validate_email); if (!$email) { echo invalid sender's email; } else { $to = '<email>'; $subject = 'new form entry'; $message = new message was submitted from <br /> . <strong> . $_post['vname'] . </strong> . <br /><br />the message is:<br /> . <strong> . $_post['msg'] . </strong>; $headers = from: . $_post['vname'] . < . $email . >; $headers .= mime-version: 1.0 . ; $headers .= content-type:text/html;charset=utf-8 . ; // sender's email // message lines should not exceed 70 characters (php rule), so wrap it $message = wordwrap($message, 70, ); // send mail by php mail function if (mail($to, $subject, $message, $headers)) { echo your mail has been sent successfully!; } else { echo failed to send email, try again.; exit ; } }}}?>the only function of this is to send me an email and it works as i want it, but i am not aware if this form is secure? is it possible that someone can exploit this code and upload a shell, or do any other sort of attack against my site.",
    "present_kp": [
      "php",
      "form",
      "email",
      "captcha"
    ],
    "absent_kp": [
      "security"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "function to classify address as eu or non-eu. leaddetails class:namespace app\\services\\lead\\leaddetails;use app\\library\\localization\\currency;use app\\services\\lead\\leaddetails\\movingservices;use app\\services\\lead\\versionid;final class leaddetails{ /** * @var versionid */ private $id; /** * @var movingappointment */ private $origin; /** * @var movingappointment */ private $destination; /** * @var movingservices\\stopovers */ private $stopovers; /** * @var movingservices\\parkingban */ private $parkingban; /** * @var movingservices\\storage */ private $storage; /** * @var movingservices\\insurance */ private $insurance; /** * @var movingservices\\kitchen */ private $kitchen; /** * @var comments */ private $comments; /** * @var distance */ private $distance; /** * @var string */ private $accountmanager; /** * @var \\app\\library\\localization\\currency */ private $currency; /** * @var movingservices\\cleaning */ private $cleaning; /** * @param versionid $id * @param movingappointment $origin * @param movingappointment $destination * @param movingservices\\stopovers $stopovers * @param movingservices\\parkingban $parkingban * @param movingservices\\storage $storage * @param movingservices\\insurance $insurance * @param movingservices\\kitchen $kitchen * @param movingservices\\cleaning $cleaning * @param comments $comments * @param distance $distance * @param currency $currency * @param null $accountmanager */ public function __construct( versionid $id, movingappointment $origin, movingappointment $destination, movingservices\\stopovers $stopovers, movingservices\\parkingban $parkingban, movingservices\\storage $storage, movingservices\\insurance $insurance, movingservices\\kitchen $kitchen, movingservices\\cleaning $cleaning, comments $comments, distance $distance, currency $currency, $accountmanager = null ) { $this->id = $id; $this->origin = $origin; $this->destination = $destination; $this->stopovers = $stopovers; $this->storage = $storage; $this->parkingban = $parkingban; $this->insurance = $insurance; $this->kitchen = $kitchen; $this->cleaning = $cleaning; $this->comments = $comments; $this->distance = $distance; $this->accountmanager = $accountmanager; $this->currency = $currency; } /** * @return versionid */ public function id() { return $this->id; } /** * @return movingappointment */ public function origin() { return $this->origin; } /** * @return movingappointment */ public function destination() { return $this->destination; } /** * @return movingservices\\stopovers */ public function stopovers() { return $this->stopovers; } /** * @return movingservices\\parkingban */ public function parkingban() { return $this->parkingban; } /** * @return movingservices\\storage */ public function storage() { return $this->storage; } /** * @return movingservices\\insurance */ public function insurance() { return $this->insurance; } /** * @return movingservices\\kitchen */ public function kitchen() { return $this->kitchen; } /** * @return movingservices\\cleaning */ public function cleaning() { return $this->cleaning; } /** * @return comments */ public function comments() { return $this->comments; } /** * @return distance */ public function distance() { return $this->distance; } /** * @return null|string */ public function accountmanager() { return $this->accountmanager; } /** * @return currency */ public function currency() { return $this->currency; } /** * @return bool */ public function iscrossborder() { return $this->origin->countryid() != $this->destination->countryid(); }}movingappointment class:namespace app\\services\\lead\\leaddetails;use app\\library\\lead\\values\\appointmentdate;use app\\library\\lead\\values\\appointmenttime;use app\\library\\system\\arraydata;use app\\library\\values\\address;final class movingappointment{ /** * @var address */ private $address; /** * @var string */ private $countryid; /** * @var appointmentdate */ private $date; /** * @var arraydata */ private $appartment; /** * @var appointmenttime */ private $time; /** * @param address $address * @param string $countryid * @param appointmentdate $date * @param arraydata $appartment * @param appointmenttime|null $time */ public function __construct( address $address, $countryid, appointmentdate $date, arraydata $appartment, appointmenttime $time = null ) { $this->address = $address; $this->countryid = $countryid; $this->date = $date; $this->appartment = $appartment; $this->time = $time; } public function address() { return $this->address; } public function countryid() { return $this->countryid; } public function date() { return $this->date; } public function appartment() { return $this->appartment; } public function time() { return $this->time; }}address class:namespace app\\library\\values;use app\\library\\system\\arraydata;final class address{ /** * @var string */ private $street; /** * @var string */ private $city; /** * @var string */ private $postalcode; /** * @var string */ private $country; /** * @param string $street * @param string $city * @param string $postalcode * @param string $country */ public function __construct($street, $city, $postalcode, $country) { $this->street = $street; $this->city = $city; $this->postalcode = $postalcode; $this->country = $country; } /** * @return string */ public function street() { return $this->street; } /** * @return string */ public function city() { return $this->city; } /** * @return string */ public function postalcode() { return $this->postalcode; } /** * @return string */ public function country() { return $this->country; } /** * @return array */ public function toarray() { return [ 'street' => $this->street, 'zip_code' => $this->postalcode, 'city' => $this->city, 'country' => $this->country, ]; } /** * @param array $data * @return address */ public static function fromarray(array $data) { $data = new arraydata($data); return new self( $data->get('street'), $data->get('city'), $data->get('zip_code'), $data->get('country') ); }}countrytype class:namespace app\\library\\system;class iseucountry{ private $eucountries = [ 'at', 'be', 'bg', 'ch', 'cy', 'cz', 'de', 'dk', 'ee', 'el', 'es', 'fi', 'fr', 'gb', 'hu', 'ie', 'it', 'lt', 'lu', 'lv', 'mt', 'nl', 'pl', 'pt', 'ro', 'se', 'si', 'sk' ]; /** * @var countrycode */ private $countrycode; /** * @param countrycode $countrycode */ public function __construct(countrycode $countrycode) { $this->countrycode = $countrycode; } /** * @return bool */ public function applies() { return in_array(strtoupper($this->countrycode), $this->eucountries); }}i want to create a functions called:public function iseu(){ return (new iseucountry(**country_code**))->applies();}where in which level should i create this function, and why?shall i create it in the leaddetails class:public function iseu(){ return (new iseucountry($this->origin->address->country())->applies();}or shall i create it in the address class?public function iseu(){ return (new iseucountry($this->address->country())->applies();}",
    "present_kp": [],
    "absent_kp": [
      "php",
      "object oriented",
      "comparative review",
      "geospatial"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "javascript-based war card game - follow-up. original questioni have updated my code based on feedback from other users and am submitting this question for re-evaluation.html<!doctype html><html lang=en> <head> <title></title> <link type=text/css rel=stylesheet href=styles.css /> </head> <body> <div id=player1currentcard class=card> <div class=wardeck> <div class=warcardsholder></div> <div class=text> <p>war deck</p> </div> </div> <div class=cardholder></div> <div class=text> <p>player 1 card</p> </div> <div id=player1currentdeck class=currentdeck> <div class=currentcardsholder></div> <div class=text> <p>current deck</p> </div> </div> <div id=player1wondeck class=wondeck> <div class=woncardsholder></div> <div class=text> <p>won deck</p> </div> </div> </div> <div id=player2currentcard class=card> <div class=cardholder></div> <div class=wardeck> <div class=warcardsholder></div> <div class=text> <p>war deck</p> </div> </div> <div class=text> <p>player 2 card</p> </div> <div id=player2currentdeck class=currentdeck> <div class=currentcardsholder></div> <div class=text> <p>current deck</p> </div> </div> <div id=player2wondeck class=wondeck> <div class=woncardsholder></div> <div class=text> <p>won deck</p> </div> </div> </div> <button id=play>play</button> <button id=reshuffle>reshuffle</button> </body> <script src=classes.js></script> <script src=war.js></script></html>css.card{ position: relative; float: left; width: 350px; height: 500px; text-align: center;}.wondeck{ position: absolute; left: 100px; bottom: 0; width: 75px; height: 100px; text-align: center;}.currentdeck{ position: absolute; bottom: 0; width: 75px; height: 100px; text-align: center;}.card{ margin: 0 5%;}.card:first-of-type{ margin-left: 0;}.card:last-of-type{ margin-right: 0;}.card .text{ position: absolute; margin: 0 0 0 -25%; left: 35%; height: 30%; width: 50%; font-size: 26px; color: rgb(150, 150, 150);}.wondeck .text, .currentdeck .text{ position: absolute; margin: 0 0 0 -25%; left: 50%; height: 30%; width: 50%; font-size: 16px; color: rgb(150, 150, 150);}.wardeck .text{ position: absolute; margin: 15% 0 0 -25%; left: 50%; height: 30%; width: 50%; font-size: 16px; color: rgb(150, 150, 150);}.cardholder{ position: absolute; top: 0; left: 0; width: 75%; height: 350px; font-size: 26px; color: rgb(0, 0, 0); border: 1px dashed black; background-color: rgba(0, 0, 0, 0.5);}.woncardsholder, .warcardsholder, .currentcardsholder{ position: absolute; top: 0; left: 0; width: 100%; height: 100%; font-size: 16px; color: rgb(0, 0, 0); border: 1px dashed black; background-color: rgba(0, 0, 0, 0.5);}.wardeck{ position: relative; float: right; width: 75px; height: 100px;}war.jsvar play = document.getelementbyid(play);var reshuffle = document.getelementbyid(reshuffle);var cardholder = document.getelementsbyclassname(cardholder);var currentcardsholder = document.getelementsbyclassname(currentcardsholder);var woncardsholder = document.getelementsbyclassname(woncardsholder);var warcardsholder = document.getelementsbyclassname(warcardsholder);window.onload = function(){ player1 = new player(player 1, [], []); player2 = new player(player 2, [], []); deck.startgame(player1, player2);}play.onclick = function(){ playgame(player1, player2);}classes.jsfunction player(name, currentdeck, wondeck){ this.name = name; this.currentdeck = currentdeck; this.wondeck = wondeck;}function card(options){ this.suit = options.suit; this.facevalue = options.facevalue; this.cardtext = (function(){ switch(this.facevalue){ case 14: {return ace}; break; case 13: {return king}; break; case 12: {return queen}; break; case 11: {return jack}; break; default: {return string(this.facevalue);} break; } }).call(this);}player.prototype.getcurrentcard = function(){ this.currentcard = this.currentdeck.shift();}deck = { suits: [clubs, diamonds, hearts, spades], cards: [14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2], deck: [], shuffleddeck: [], builddeck: function(){ for(var suit = 0; suit < this.suits.length; suit++){ for(var card = 0; card < this.cards.length; card++){ this.deck.push(new card({suit: this.suits[suit], facevalue: this.cards[card]})); } } }, shuffledeck: function(unshuffleddeck, shuffleddeck){ while(unshuffleddeck.length){ var index = math.floor(math.random() * unshuffleddeck.length); shuffleddeck.push(unshuffleddeck.splice(index, 1)[0]); } unshuffleddeck = []; }, distributecards: function(player1deck, player2deck){ for(var i = 0; i < this.shuffleddeck.length / 2; i++){ player1deck.push(this.shuffleddeck[i]); player2deck.push(this.shuffleddeck[this.shuffleddeck.length - i - 1]); } }, dealwarcards: function(player, wardeck, num){ for(var i = 0; i < num; i++){ player.getcurrentcard(); wardeck.push(player.currentcard); } return wardeck; }, startgame: function(player1, player2){ this.builddeck(); this.shuffledeck(this.deck, this.shuffleddeck); this.distributecards(player1.currentdeck, player2.currentdeck); }}function playgame(player1, player2){ var player1wardeck = []; var player2wardeck = []; function gotowar(){ console.log(war); deck.dealwarcards(player1, player1wardeck, 2); deck.dealwarcards(player2, player2wardeck, 2); console.log(player1wardeck, player2wardeck); if(player1wardeck[player1wardeck.length - 1].facevalue === player2wardeck[player2wardeck.length - 1].facevalue){ console.log(tie); deck.dealwarcards(player1, player1wardeck, 2); deck.dealwarcards(player2, player2wardeck, 2); gotowar(); } if(player1wardeck[player1wardeck.length - 1].facevalue > player2wardeck[player2wardeck.length - 1].facevalue){ player1.wondeck = player1.wondeck.concat(player1wardeck, player2wardeck); console.log(player 1 wins); } else{ player2.wondeck = player2.wondeck.concat(player1wardeck, player2wardeck); console.log(player 2 wins); } warcardsholder[0].textcontent = player1wardeck[player1wardeck.length - 1].cardtext + of +player1wardeck[player1wardeck.length - 1].suit; warcardsholder[1].textcontent = player2wardeck[player2wardeck.length - 1].cardtext + of +player2wardeck[player2wardeck.length - 1].suit; cardholder[0].textcontent = player1wardeck[0].cardtext + of +player1wardeck[0].suit; cardholder[1].textcontent = player2wardeck[0].cardtext + of +player2wardeck[0].suit; } if(player1.currentdeck.length === 0){ reshuffledeck(player1); } else{ player1.getcurrentcard(); } if(player2.currentdeck.length === 0){ reshuffledeck(player2); } else{ player2.getcurrentcard(); } if(player1.currentcard.facevalue > player2.currentcard.facevalue){ player1.wondeck.push(player1.currentcard); player1.wondeck.push(player2.currentcard); } else{ player2.wondeck.push(player2.currentcard); player2.wondeck.push(player1.currentcard); } if(player1.currentcard.facevalue === player2.currentcard.facevalue){ player1wardeck.push(player1.currentcard); player2wardeck.push(player2.currentcard); gotowar(); } else{ cardholder[0].textcontent = player1.currentcard.cardtext + of +player1.currentcard.suit; cardholder[1].textcontent = player2.currentcard.cardtext + of +player2.currentcard.suit; currentcardsholder[0].textcontent = player1.currentdeck.length; currentcardsholder[1].textcontent = player2.currentdeck.length; woncardsholder[0].textcontent = player1.wondeck.length; woncardsholder[1].textcontent = player2.wondeck.length; warcardsholder[0].textcontent = ; warcardsholder[1].textcontent = ; } if(player1.currentdeck.length === 52){ gameover(player1); } if(player2.currentdeck.length === 52){ gameover(player2); }}function reshuffledeck(player){ deck.shuffledeck(player.wondeck, player.currentdeck);}function gameover(player){ console.log(player.name + wins!);}final submission",
    "present_kp": [
      "javascript"
    ],
    "absent_kp": [
      "object oriented",
      "playing cards"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is your vulnerability discovery process?. i myself am a static analysis guy; i mostly gave up dynamic reverse engineering ten years ago. so these days, my process is usually to locate where my input enters the module that i am interested in, and then perform heavy static analysis to determine how my input manipulates the state of the program. i have found some neat bugs such as information disclosure this way; however, i am undoubtedly a lot slower than my counterparts who employ a lot of dynamic analysis and dynamic input generation (e.g., randomized fuzzing). what steps do you usually take to discover vulnerabilities in closed-source programs?",
    "present_kp": [],
    "absent_kp": [
      "vulnerability analysis"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "requesting for new password in domaintools.com, when my account is not active. after some time, i'm back to domaintools.com. when i want to get a new password, and i enter my email address, i get the following message:the account associated with this email address is not activeok, what to do now? there is not a help or anyway to reactivate my account!",
    "present_kp": [],
    "absent_kp": [
      "password recovery"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "a command that gives username@hostname:pwd. i know that pwd gives the current working directory, hostname gives the current host and whoami gives the current user. is there a single unix command that will give me the output ofwhoami@hostname:pwdso that i can quickly paste the output into an scp command?",
    "present_kp": [
      "hostname",
      "pwd",
      "whoami"
    ],
    "absent_kp": [
      "shell",
      "shell builtin"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "logging and debugging for qemu virtual machines. i had a qemu virtual-machine which crashed several times because hdd in hypervisor had no space left. this made me wonder is there a possibility to to set up a logging/debugging for quemu virtual-machines. i tried to start virtual-machine with -d /tmp/qemu-debug-log command:qemu-system-i386 -d /tmp/qemu-debug-log -monitor pty -device e1000,netdev=tap0 -netdev tap,id=tap0 -m 512m -display vnc=:1 -drive file=freebsd10.2..but this did not even create a /tmp/qemu-debug-log file.in addition, qemu does not seem to write into messages or kernel ring buffer(dmesg). what are the best practices to enable logging for qemu virtual machines?",
    "present_kp": [
      "qemu"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "merge sort in clojure. i made my own version of merge sort: (defn rsort[a] (cond (<= (count a) 1) a :else (let [half (/ (count a) 2) [lh rh] (split-at half a)] (loop [res () slh (rsort lh) srh (rsort rh)] (cond (empty? slh) (into srh res) (empty? srh) (into slh res) :else (if (< (first slh) (first srh)) (recur (cons (first slh) res) (rest slh) srh) (recur (cons (first srh) res) slh (rest srh))))))))any suggestion how improve this code?",
    "present_kp": [
      "clojure"
    ],
    "absent_kp": [
      "algorithm",
      "sorting",
      "mergesort"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "sharing source code of my online game?. so one of my long-standing projects is an online fan-game that's been running for a good four years now, and is at least somewhat well-known among the related community. to give you an idea, alexa ranks my online game higher than the official website for the franchise my game is based on.one thing i've been wondering is, would it be a good idea to make the game's source code publicly viewable?the reasoning for the idea is that this will allow the more programmer-oriented people in the community discover how the game works internally, and research certain features to determine possible outcomes of events, and share this knowledge with other users. for instance, they would be able to discover the exact chances of various treasures appearing in chests, rather than just having a statistical analysis.major reasoning against it is people can copy it, but honestly there are already copycat sites out there and many of them fall flat on their face simply because they try to be my game and either just don't get any attention in the first place, or get called out on being a rip-off by other members of the community.these two points have been like a see-saw in my mind, tipping back and forth between i should totally do it and nowai, src iz myne! (not really like that :p) so i was hoping that i might be able to get some advice here, perhaps someone else has been through this and might have seasoned advice to offer?",
    "present_kp": [
      "source code"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "why doesn't the enter key send eol?. unix / linux eol is lf, linefeed, ascii 10, escape sequence .here's a python snippet to get exactly one keypress:import sys, tty, termiosfd = sys.stdin.fileno()old_settings = termios.tcgetattr(fd)try: tty.setraw(sys.stdin.fileno()) ch = sys.stdin.read(1)finally: termios.tcsetattr(fd, termios.tcsadrain, old_settings) return chwhen i press enter on my keyboard in response to this snippet, it gives , carriage return, ascii 13.on windows, enter sends cr lf == 13 10. *nix is not windows; why does enter give 13 rather than 10?it's known the python snippet is correct.",
    "present_kp": [
      "ascii"
    ],
    "absent_kp": [
      "command line",
      "terminal",
      "newlines"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "nomodeset doesn't fix black screen on startup. i'm trying to install linux mint on my 2010 21.5 in imac. i managed to install it after booting from the usb drive with the nomodeset option, but i haven't been able to boot from the actual install so far. the typical recommendation i found from googling is: on the boot selector screen (i'm using refind) hold shift and select grub options, then press e to edit the boot options and add nomodeset after where it says quiet splash. however, this does not work for me - i still get a black screen when it tries to boot up. help?",
    "present_kp": [
      "linux mint"
    ],
    "absent_kp": [
      "system installation"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "search in mutt by date and time. i've been using mutt for some days now, and i think i have the basics by now.i'm able to search messages by date (l, ~d date ...) but what i need is to also search by time.is it even possible?",
    "present_kp": [
      "mutt"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "what is a neural attention model and how can it be used for text summarization?. we are doing a research on multi-document (one document max 100 words) text summarization. we are looking into abstractive text summarization methods. i need following things to be clarified.please explain what exactly in neural attention model.how can the neural attention model be used for abstractive text summarization?",
    "present_kp": [],
    "absent_kp": [
      "neural networks",
      "natural language processing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is the meaning of 1- in the output of ps?. this is the output of the command ps -ef | grep java on one of our machines.jboss 1965 1935 1 2013 ? 04:41:05 java xxxxxxxxxxxxxxxxx jboss 2170 2141 0 2013 ? 00:42:57 java xxxxxxxxxxxxxxxxxjboss 2708 2679 7 2013 ? 1-04:38:51 java xxxxxxxxxxxxxxxxxtomcat 6845 1 1 2013 ? 03:28:03 xxxxxxxxxxxxxxxxxjboss 24651 24622 0 2013 ? 03:03:27 xxxxxxxxxxxxxxxxxtomcat 32533 1 0 2013 ? 00:12:08 xxxxxxxxxxxxxxxxx(xxxs added to hide the guilty!)what is the meaning of the 1- on the third line?",
    "present_kp": [
      "ps"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "is function with finite amount of $\\phi_x(x+1)$ occurrences turing computable?. i'm studying for my exam from logic and computability and we have to face following kind of examples:$$f(x) = egin{cases} \\ 1 & ext{if }\\phi_x(x+1)\\!\\downarrow ext{ and }x\\leq50\\ \\ 0 & ext{otherwise.}\\end{cases}$$unfortunatelly i'm not really clear on how to tackle those. $\\phi_x$ is the $x$-th computable function, thus it's also turing computable. this implies, there exists a tm (we have $n$ of them) which halts and returns the output.the task is to determine whether it's computable. in order to prove it we either provide an informal algorithm to show it's computable or a formal proof that it's not. can any of you solve this example? if $f(x)$ is computable, then $\\phi_x$ needs to halt for every $x \\in\\{1, \\dots, 51\\}$. however $\\phi_x$ could be undefined on one of the elements from $\\{1, \\dots, 51\\}$ and keep looping forever.at the other hand, there is an upper bound on $x$, which indicates its computability.please, can you show me the right direction?thanks!",
    "present_kp": [
      "computability"
    ],
    "absent_kp": [
      "turing machines",
      "undecidability"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "list querying capabilities in python 3.x. either out of boredom, or my odd fascination with linq and sql, this, thing, was created. essentially, it provides list querying capabilities. at the moment, you can only query lists based on conditions, and sort them. here's the source.py_listq.pyimport operator as operclass queryablelistcondition: a class used for conditions in queryablelist.where(*conditions). def __init__(self, _operator: str, _value): self._operator = _operator.lower() self._value = _value self._operators = { eq: oper.eq, ne: oper.ne, lt: oper.lt, gt: oper.gt, le: oper.le, ge: oper.ge } def compare(self, value): compares the values, with value as the first compared value. compari- sons look something like this: value operator self.value return self._operators[self._operator](value, self._value) def reverse_compare(self, value): compares the values with self.value as the second compared value. compari- sons look something like this: value operator self.value return self._operators[self._operator](self._value, value)class queryablelist: a special class which is like a default python list, except it has querying capabilities. def __init__(self, _items: list): self._items = _items def select(self): 'select' self._items and return it as a list. this is used in queryablelist. where as the first argument. return self._items def where(self, query_list: list, compare_type: str, *conditions: queryablelistcondition): select elements from a queryablelist based on conditions. result_list = [] for condition in conditions: for item in self._items: if compare_type.lower() == non-reverse: if condition.compare(item) and item not in result_list: result_list.append(item) if compare_type.lower() == reverse: if condition.reverse_compare(item) and item not in result_list: result_list.append(item) return result_list def order(self, query_list: list, reverse_results=false): returns a sorted version of the list. if reverse=true then the list is ordererd in reverse. return sorted(query_list, reverse=reverse_results)also, for reference, here's a simple example program:test.pyfrom py_listq import queryablelist, queryablelistconditiondata = queryablelist([6, 2, 7, 1223, 56, 14, 6, 2435, 234, 6, 2134, 124235, 243621, 74, 378, 3, 45])result = data.order( data.where( data.select(), non-reverse, queryablelistcondition(gt, 1000) ), reverse_results=true)print(result)i feel like i shouldn't have implemented this in an object-oriented way, and it feels, clunky, to use. so if you have any tips on those, they'd be very much appreciated.",
    "present_kp": [
      "python",
      "python 3.x"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "what is this form variable=$(...). what does the following mean:basedir=$(dirname $(echo $0 | sed -e 's,\\,/,g'))i'm particulalry interested in this part:varible=$(...)i know that parenthesis are used to execute a subprocess, but what if they are used alongside with $?",
    "present_kp": [],
    "absent_kp": [
      "shell"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "alternative turing machine proofs. i am asking this question again. i am aware of, and have read the other similar alternative proof tm questions, but unfortunately, they do help me.i am looking for a tm halting problem proof that does not have the following properties:uses diagonalization.uses recursion.uses self reference.relies on running the turing machine. specifically, there is a distinction between analyzing a turing machine to determine if it has a certain property and running / simulating a turing machine to see if it exhibits a certain property. it should go without saying that a halting function that determines whether or not a given turing machine halts by simulating it and waiting until it halts / returns is not the only way to determine whether or not a turing machine halts.uses proof by contradiction, although this is extremely context dependent. my main concern is with a proof by contradiction that is self referential such that it forms a key part of the proof that can not be separated from the proof without causing the result to collapse. i understand you may not understand or agree with my reasoning, but for my purposes a proof that did not use this technique would greatly simplify things.ideally, the proof would have the following properties:use technique such as exhaustively enumerative, pigeon hole, double count, etc.uses a different branch of mathematics to achieve the same result, i.e. graph theory, combinatorics, etc.still applies when reduced to the boolean domain (i.e., a turing machine built using just 2 input, 1 output nand gates (i.e., a computer) exhibits the same problem). specifically, the result can not contradict the fact that boolean algebras have been proven to be decidable, and every $n$-bit boolean $l$ can be shown to be decided by a bounded by a number of {and, or, not} gates (via shannons the synthesis of two-terminal switching circuits).i have spent quite a bit of time looking for alternate formulations of the halting problem that are not just simple permutations of the original proof given by turing.question: can you point me to a vetted proof of the halting problem that shares as absolutely as little in common with the one given by turing?please, instead of arguing with me as to whether or not my reasons are valid, or that i don't get it and should just accept the proof given by turing, it would be a great help to me and possibly someone else if you could simply help me locate an alternate proof. yes, i am looking for proofs with certain properties, properties that inconveniently cull a number of candidates. despite this, they are properties that i unfortunately need.",
    "present_kp": [
      "proofs",
      "halting problem"
    ],
    "absent_kp": [
      "computability",
      "turing machines"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to grep and pattern with pipe. i wrote a piece of script similar to something below:srvc_location='ps -ef| grep '${sandbox}.*pset' | head 1 | awk {print $9}'echo service loc : $srvc_locationbut strangely this $srvc_location is getting resolved most of the time, but sometimes its not getting resolved with this message:service loc : strrosdev_silo1_int.*psetsome additional info:the above piece of code runs in a loop so everytime $sandbox has a new value.i tried with grep -e ${sandbox}.*pset but it didn't solve the issue.i can solve the problem with having two grep, grep $sandbox |grep pset,but i wanted to use just one with a pattern.why is it behaving like this?",
    "present_kp": [
      "grep",
      "pipe",
      "ps"
    ],
    "absent_kp": [
      "shell script"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "segment tree in python3. i've implemented a segment tree in python3:import mathinf = int(2e9)class segmenttreenode: def __init__(self, l, r, v=inf): self.left = l self.right = r self.value = v def merge(self, left, right): if left is not none and right is not none: self.value = min(left.value, right.value) elif left is none and right is none: self.value = inf elif left is none: self.value = right.value else: self.value = left.valueclass segmenttree: def __init__(self, a): n = len(a) power = math.ceil(math.log(n, 2)) total = 2 ** (power + 1) self.__tree = [none] * int(total) self.__leaf_length = int(total/2)-1 self.__build(1, 0, self.__leaf_length, a) def __build(self, node, l, r, a): if l == r: self.__tree[node] = segmenttreenode(l, r) try: self.__tree[node].value = a[l] except indexerror: self.__tree[node].value = inf return leftchild = 2 * node rightchild = leftchild + 1 mid = (l + r) // 2 self.__build(leftchild, l, mid, a) self.__build(rightchild, mid + 1, r, a) self.__tree[node] = segmenttreenode(l, r) l = self.__tree[leftchild] r = self.__tree[rightchild] self.__tree[node].merge(l, r) def __query(self, node, l, r, i, j): if l >= i and r <= j: return self.__tree[node] elif j < l or i > r: return none else: leftchild = 2 * node rightchild = leftchild + 1 mid = (l + r) // 2 l = self.__query(leftchild, l, mid, i, j) r = self.__query(rightchild, mid + 1, r, i, j) if l is not none and r is not none: return segmenttreenode(-1, -1, min(l.value, r.value)) elif l is none and r is none: return segmenttreenode(-1, -1, inf) elif l is none: return segmenttreenode(-1, -1, r.value) else: return segmenttreenode(-1, -1, l.value) def query(self, i, j): return self.__query(1, 0, self.__leaf_length, i, j) def __update(self, node, l, r, i, v): if l == i and r == i: self.__tree[node].value = v elif i < l or i > r: return none else: leftchild = 2 * node rightchild = leftchild + 1 mid = (l + r) // 2 self.__update(leftchild, l, mid, i, v) self.__update(rightchild, mid + 1, r, i, v) l = self.__tree[leftchild] r = self.__tree[rightchild] self.__tree[node].merge(l, r) def update(self, i, value): self.__update(1, 0, self.__leaf_length, i, value)in_n, in_m = map(int, input().rsplit())in_array = list(map(int, input().rsplit()))segment_tree = segmenttree(in_array)for row in range(in_m): command = input().rsplit() x, y = map(int, command[1:]) if command[0] == 'min': print(segment_tree.query(x - 1, y - 1).value) else: segment_tree.update(x-1, y)i take n and m from stdin after i read an array[n]. and then i read m operations set i x or min l r.here is profiler log for relevant testcase: ncalls tottime percall cumtime percall filename:lineno(function) <phone>/49877 3.871 0.000 4.753 0.000 test.py:51(__query) <phone>/50123 1.792 0.000 2.685 0.000 test.py:74(__update) <phone> 0.829 0.000 0.829 0.000 test.py:8(__init__) 983162 0.732 0.000 1.081 0.000 test.py:13(merge) <phone> 0.576 0.000 0.576 0.000 {built-in method min} 100002 0.548 0.000 0.551 0.000 {built-in method input} 262143/1 0.545 0.000 0.907 0.907 test.py:33(__build) 1 0.429 0.429 9.550 9.550 test.py:1(<module>) 49877 0.076 0.000 0.076 0.000 {built-in method print} 100002 0.068 0.000 0.068 0.000 {method 'rsplit' of 'str' objects} 49877 0.045 0.000 4.799 0.000 test.py:71(query) 50123 0.034 0.000 2.719 0.000 test.py:89(update) 339 0.002 0.000 0.002 0.000 {built-in method utf_8_decode} 1 0.002 0.002 0.908 0.908 test.py:25(__init__) 339 0.001 0.000 0.003 0.000 codecs.py:310(decode) 2 0.000 0.000 0.000 0.000 {built-in method __build_class__} 1 0.000 0.000 0.000 0.000 {built-in method init_builtin}i guess the problem is in query function. how can it be improved? any ideas? here is a relevant test-case. i need to achieve the result at least 2 times faster.",
    "present_kp": [
      "python",
      "tree"
    ],
    "absent_kp": [
      "algorithm",
      "python 3.x"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "loop through column cells and compare value with a collection. i have a collection of 166 elements:dim mycollection as new collectionmycollection.add (%)mycollection.add (%o)mycollection.add (d)mycollection.add (cms)...i want to compare them to the value of the cells in a column. if the value of a cell is not in mycollection, i change the background color to red.for i = 2 to lastrow isvalid = false for each unit in mycollection if range(a & i).value = unit then isvalid = true end if next unit if not isvalid then range(a & i).interior.color = 192 end ifnext ihowever, it takes a long time to finish executing when there is a large number of row and i would like to improve this. maybe by using something else than a collection and some vba functions.",
    "present_kp": [
      "vba"
    ],
    "absent_kp": [
      "excel",
      "collections"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "split a sting field into an array in jq?. i have a json array returned from curl that looks like this:[ { title: some title, tags:taga tag-b tagc }, { title: some title 2, tags:taga tagc }, ...]i'd like to convert it to...[ { title: some title, tags:[taga, tag-b, tagc] }, { title: some title 2, tags:[taga, tagc] }, ...]so far i have:(map(select(.tags!=null)) | map(.tags | split( ))) as $tags | $tagsand that appears to give me something like: [ [ taga, tag-b, tagc ], [ taga, tagc ] ]but i don't seem to be able to weave that back into an output that would give me .tags as an array in the original objects with the original values...",
    "present_kp": [
      "json",
      "jq"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how do i force remove a package in arch with pacman?. how do i force remove a package in arch with pacman while other packages still depend upon it.pacman -r perl-libwww checking dependencies...error: failed to prepare transaction (could not satisfy dependencies):: perl-app-cpanminus: requires perl-libwww>=5.828:: perl-app-pmuninstall: requires perl-libwww:: perl-app-sd: requires perl-libwww:: perl-catalyst-action-rest: requires perl-libwww>=2.033 :: perl-catalyst-runtime: requires perl-libwww>=1.64:: perl-cpan: requires perl-libwww:: perl-cpan-mini: requires perl-libwww:: perl-cpan-uploader: requires perl-libwww:: perl-feed-find: requires perl-libwww:: perl-http-body: requires perl-libwww:: perl-http-request-ascgi: requires perl-libwww:: perl-module-cpants-analyse: requires perl-libwww:: perl-module-install: requires perl-libwww>=5.812:: perl-net-trac: requires perl-libwww:: perl-net-whois-raw: requires perl-libwww:: perl-prophet: requires perl-libwww:: perl-rt-client-rest: requires perl-libwww:: perl-uri-fetch: requires perl-libwww:: perl-www-mechanize: requires perl-libwww:: perl-xml-atom: requires perl-libwww:: perl-xml-feed: requires perl-libwwwbasically lwp 6 split a whole bunch of packages, and i need to remove it so i can reinstall it.",
    "present_kp": [
      "pacman"
    ],
    "absent_kp": [
      "arch linux",
      "package management"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "intuitively, what does it mean for a solution to be divergence free?. i have always been interested in generating discrete approximations that tend to mimic the properties of their continuum counterparts. usually, one approximation that is desired is that the discrete solution is divergence free. take maxwell equations in the time domain as an example:$$- rac{\\partial\\mathbf{b}(\\mathbf{r},t)}{\\partial t} = abla imes\\mathbf{e}(\\mathbf{r},t) \\\\mathbf{j}(\\mathbf{r},t) = abla imes\\mathbf{h}(\\mathbf{r},t) \\ abla\\cdot\\mathbf{b}(\\mathbf{r},t) = 0 \\ abla\\cdot\\mathbf{j}(\\mathbf{r},t) = 0$$the third equation implies that only two out of the three components of $\\mathbf{b}$ are independent of each other.however, i would like a more intuitive explanation of what it means to be divergent free. even if the context is other than em.",
    "present_kp": [
      "divergence free"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "examples of non-csls not created through diagonalization. hopcroft & ullman 1979, intro to automata theory, languages, & computation states (p. 224) that almost any language one can think of is csl; the only known proofs that certain languages are not csls are ultimately based on diagonalization. they give for example the universal turing machine. i presume this statement is still true today more than three decades later but wonder anyway. are there any non-csls known that are not ultimately based on diagonalization?i agree that this question is not necessarily strictly defined. i would also be willing to accept examples in the form where an algorithm was initially/originally built or constructed for some purpose and had unknown complexity and later the algorithm was proved to be non-csl.note non csls are also the exp-space hard problems. wikipedia mentions without reference an example of recursive language that is not context-sensitive is any recursive language whose decision is an expspace-hard problem, say, the set of pairs of equivalent regular expressions with exponentiation. i would like to know a reference for this if anyone has it.",
    "present_kp": [
      "automata theory"
    ],
    "absent_kp": [
      "reference request",
      "fl.formal languages",
      "big picture"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "an associated website won't host a link to my site because they are afraid it will increase their bounce rate. i made a site for a comedy club, they are associated with a fairly well known hotel (the shows will be there and the tickets are sold there), and we asked them to put a link to our website where all the info about the show is; they have a really good position in google. they refused and made us copy/paste all our info on to a blog post in their website arguing that putting a link to our website will increase their bounce rate. is there a good argument to make them change their minds?update: forgot to mention that i don't speak directly with hotel, my client does. so i'm looking for ideas that my client can explain to them and both parties can understand since they are not developers.",
    "present_kp": [
      "bounce rate"
    ],
    "absent_kp": [
      "seo"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "difference between heuristic and approximation algorithm?. i have a problem regarding the following situation. i have two arrays of numbers like this:index/pos 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 array 1(i): 1 2 3 4 7 5 4 3 7 6 5 1 2 3 4 2array 2(j): 4 4 8 10 10 7 7 10 10 11 7 4 7 7 4now suppose the second array is very hard to compute but i have noticed that if i add a[i] + a[i+1]in the array 1 i get the number very close to the number a[j] in the array 2. is my solution a heuristic or approximation?if i had a reason to believe that i will never overshoot the value of a[j] by +-x with this algorithm and can prove it, would then my solution be a heuristic or approximation?is there any literature that deals with heuristic vs. approximation questions for p class problems where the solution can be achieved in polynomial time but the input is just too big for a poly time algorithm to be practical.thank you",
    "present_kp": [
      "approximation"
    ],
    "absent_kp": [
      "heuristics"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "can python extract org-mode tables from org documents?. i'm trying to write some python code that reads specified tables in org mode documents. for example, i have a file ~/foo.org$ cat ~/foo.org#+title: example org document* section oneproin quam nisl, tincidunt et, mattis eget, convallis nec, purus.#+tblname: table1| i | want | python | to ||-------+---------+--------+--------|| read | this | table | only || 1 | 3 | 2 | 4 ||-------+---------+--------+--------|| i | want | the | dashed || lines | ignored | by | python |#+tblname: table2| i | don't | need | python | to ||------+-------+-------+--------+----|| read | this | table | 9 | 8 || 7 | 6 | 5 | 4 | 3 || 2 | 1 | 0 | 22 | 17 |if my file were less complex, say$ cat ~/bar.org| i | want | python | to || read | this | table | only || 1 | 3 | 2 | 4 || i | want | the | dashed || lines | ignored | by | python |then i could read the table into python with import csvcsv.dictreader(open('~/bar.org'), delimiter='|')is there any way to parse my desired table from the more complicated ~/foo.org?",
    "present_kp": [
      "python",
      "csv",
      "org mode"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "removing entry details text in email in cognito forms. is it possible to remove entry details text from sent emails? and if yes, how can i do this?",
    "present_kp": [
      "cognito forms"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "send enter key to python script from bash. i am trying to automate my ubuntu setup with a bash script and got the following problem:i want the script to automatically send a enter keystroke, when running umake ide eclipse (this installs eclipse ide from the terminal).this is the standard output, when running from the terminal without a script:$ umake ide eclipsechoose installation path: /home/gn4i/.local/share/umake/ide/eclipse<need to press enter>downloading and installing requirements normally i would do this with echo | umake ide eclipse, but i always get the following error $ echo | umake ide eclipsechoose installation path: choose installation path: error: unhandled exceptiontraceback (most recent call last): file /usr/lib/python3/dist-packages/umake/tools.py, line 158, in wrapper function(*args, **kwargs) file /usr/lib/python3/dist-packages/umake/ui/__init__.py, line 42, in display cls.currentui._display(contenttype) file /usr/lib/python3/dist-packages/umake/ui/cli/__init__.py, line 61, in _display contenttype.run_callback(result=rlinput(contenttype.content, contenttype.default_input)) file /usr/lib/python3/dist-packages/umake/ui/cli/__init__.py, line 41, in rlinput return input(prompt + )eoferror: eof when reading a linehow can i automate this installation?",
    "present_kp": [
      "bash",
      "python",
      "python3"
    ],
    "absent_kp": [
      "shell script",
      "shell"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how can i transition to a job using the microsoft stack?. i graduated from university with a computer science degree (bsc), having done some c, c++, java, python, and oracle along the way. i did some freelancing while i was at university and carried this into a full time job when i graduated, and i've been working full time for a year now.php isn't really what i want to do, and i think it's limiting my opportunities the longer i stay along this path. it's not a language i like to use on a daily basis, and i would like to work in a larger organization, where it doesn't seem php has much use.i want to stay in web development, and there seems to be plenty of jobs for the microsoft stack: c#, sql server, and asp.net mvc. but beyond a half semester writing common-line c++ in visual studio, i have no experience with microsoft technology.where do i start to make the transition to landing a job in a microsoft shop? are there any specific certifications i should be focusing on, or university courses i should be taking? what are employers or recruiters looking for?what can i do besides creating a pet project in my spare time?",
    "present_kp": [
      "c#",
      "microsoft"
    ],
    "absent_kp": [
      "career development"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why does 'awk' fail when i put it in a bash script. on the prompt:ls | awk '{printf(%s %s , $9, $3);}'succeeds in printing what it is supposed to be printing. if i put it in a file, i get four or five empty lines (about the number of lines in the output of ls):$ cat awk-file-owner-simple #!/bin/bashls | awk '{printf(%s %s , $9, $3);}'$ ./awk-file-owner-simple ____where _ stand for empty lines (that can't be displayed using the code formatter of stackexchange).",
    "present_kp": [
      "bash",
      "awk"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "hdmi switches - how are they supposed to work?. i have never used any hdmi switch before, so my knowledge about them is zero.now i have one and i am wondering if its behaviour is normal.i have a beamer for the output signal, a wii u as first input signal and a laptop as second input signal. the switch i bought has one output and two input plugs, so i put them in accordingly. the packaging says that the switch has an automatic switching function, pretending that the correct signal will be recognized.now what happens? the switch only let a signal through if both input devices are plugged in and switched on. then, i can use the manual switch button to switch to the signal i want. but whenever i turn one of the devices off (wii u or laptop) or unplug it (we only plug the laptop in if we want to wantch movies), no signal gets through to the beamer. and the manual switch button does not work either.is this behaviour normal, or have i just bought junk?i was expecting that it recognizes if just one input is switched on and lets this signal through accordingly. this is what i think of automatic switching.what are your experiences with hdmi switches? are there any models that do what i expect of them, or is the behaviour of my switch the normal one? mine is from vivanco, and it cost 45. it says full hdtv and 1080p, and supports deep color.any suggestions on other manufacturers, models and usual prices will be appreciated. i live in germany, so if anyone knows models i can get here (via amazon.de or just some media store), this will be especially appreciated.",
    "present_kp": [
      "hdmi",
      "switch"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how can i get a map or basic picture of discovered pages?. i'm working on a small business website at <url> and trying to see if there's evidence of anything blackhat. the website is conceptually small; the point is to get traffic interested in flooring work in chicago, explain the business to customers, and get a conversation going if there's a match.overall, it looks mostly clean; one site said it was mostly or completely free of things interpreted as spam by search engines. however, there are a couple of things i'm a bit puzzled by.first, according to <url> , there are 576 discovered pages. that is well more than i found viewing the site in a browser.second, i got a basically empty response from wget -r. a first try at httrack (which i just found out about) seemed to pull one page and stop; i am not completely sure how to identify the results (beyond that it seemed to run quickly enough not to be loading much more than one webpage), but it didn't seem to generate enough files to account for content available at a glance from the homepage.the homepage title may need changing, but beyond that, is there anything in the site as outlined that smells blackhat?",
    "present_kp": [
      "blackhat"
    ],
    "absent_kp": [
      "seo"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "replacement for facebook feature imported stories. what is a good replacement for the facebook feature imported stories that allowed you to import content from other sites like rss feeds, picasa, google reader etc. facebook recently disabled the feature with this message:the imported stories feature is no longer available. most of the sites supported by this feature now allow you to publish stories to facebook directly from the site.i used this feature to have my livejournal blog updates and picasa photos automatically post on my wall. how can i do that now? despite the advice from facebook, i can't find way to do this directly from either livejournal or picasa.",
    "present_kp": [
      "facebook",
      "rss"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "what can be concluded from a full application of resolution?. i know that resolution is refutation complete,but what can we conclude if a resolution procedure leads to a situation with no more chance to operate the resolution?given a propositional formula which we want to check its satisfiability,( a + b ) ( a + c ) after one resolution the formula become ( a + b ) ( a + c ) ( b + c )what can we conclude at this point?",
    "present_kp": [],
    "absent_kp": [
      "logic",
      "proof techniques",
      "first order logic"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "shortcut to remove orphaned parenthesis. while writing code it often happens that adding one parenthesis leaves another one orphaned somewhere. many syntax color scheme color the parenthesis differently to signal it. for example, while writing latex code$\\mathbb{e} something }$the second } will be marked in red in my vim. if i want to remove it i would have to dof}xassuming my cursor is located past the first }, otherwise i need to do some repositiong to reach it.since the syntax highlights picks up on it, is there a way to bind a command to something like 'go to the next unmatched parens and delete it'? bonus point if the cursor is left in its place and left in normal mode.",
    "present_kp": [],
    "absent_kp": [
      "key bindings",
      "syntax highlighting"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "duplicate meta descriptions after permalink change. i recently changed my permalink structure on wordpress to remove the year and month from the url.old structure - <url> structure - <url> have a canonical that has the new structure in the url but google webmaster tool is flagging up duplicate meta descriptions and title tags.currently it is telling me that i have 9000+ duplicates but obviously the pages are the same thing and there is only one instance of this. should i worry about this or is this something that will naturally correct itself over time?",
    "present_kp": [
      "wordpress"
    ],
    "absent_kp": [
      "google search console",
      "duplicate content"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "parameters in the server part or client part of url. i want to store the search history which a user has followed to find a product. it can arrive to the landing page of the product through numerous paths, so i want them stored somewhere. i have discarded a cookie approach, as i would like to allow the user to send the url to another user and still keep the data. so i thought of storing a database ticket which goes in the url. but now i am doutbful about storing in the server part of the url, for instance, <url> in the client part,<url> later approach obliges me to load the content via ajax. which is more convenient with respect to seo and other considerations?",
    "present_kp": [
      "seo",
      "url",
      "ajax"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "is there a greedy algorithm to solve the assignment problem?. the assignment problem is defined as:there are n people who need to be assigned to n jobs, one person per job. the cost that would accrue if the ith person is assigned to the jth job is a known quantity c[i,j] for each pair i, j = 1, 2, ..., n. the problem is to find an assignment with the minimum total cost.there is a question asking to design a greedy algorithm to solve the problem. it also asks if the greedy algorithm always yields an optimal solution and for the performance class of the algorithm. here is my attempt at designing an algorithm:am i correct in saying that my algorithm is of o(n^2)? am i also correct in saying that a greedy algorithm does not always yield an optimal solution? i used my algorithm on the following cost matrix and it is clearly not the optimal solution. did i do something wrong?",
    "present_kp": [
      "assignment problem"
    ],
    "absent_kp": [
      "algorithms",
      "greedy algorithms"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "-bash: android: command not found. i'm on a mac running mavericks trying to learn how to develop android apps (in the process i'm learning bash too).to run on the emulator they suggest the command android avd.i always get the error in the title of this post. i've searched all over and the issue seems to lie with changing the path.i've amended .bash_profile as such:export path=/usr/local/bin:$path export path=/usr/local/bin:$path export path=/development/dev_android/sdk/tools:$pathbut it's still not working.any thoughts?",
    "present_kp": [
      "bash",
      "path",
      "android"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "nautilus missing after kali linux update. so i updated kali linux and ever since nautilus is missing i cant find it its missing and when i try to install nautilus i 'root@kali:~# apt-get install nautilusreading package lists... donebuilding dependency tree reading state information... donesome packages could not be installed. this may mean that you haverequested an impossible situation or if you are using the unstabledistribution that some required packages have not yet been createdor been moved out of incoming.the following information may help to resolve the situation:the following packages have unmet dependencies: nautilus : depends: libgnome-desktop-3-2 (>= 3.2.0) but it is not going to be installede: unable to correct problems, you have held broken packages.'ive tried using aptitude and i tried running update and upgrade but to no avail'",
    "present_kp": [
      "kali linux",
      "nautilus"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "determine if cc system is realizable. in axioms and hulls, knuth defines the notion of a cc system, which is a generalization of euclidean points in general position. while all such arrangements of points in the plane give a cc system, not all cc systems give an arrangement of points, i.e. are not realizable. is there a polynomial-time procedure to determine if a given cc system is realizable?",
    "present_kp": [],
    "absent_kp": [
      "computational geometry"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how can i print out the delimiter character and allow user to edit line while reading standard input?. i'm trying to write a simple script that read from standard input, using ; character as delimiter to terminate the input line and that allows user to edit line.here is my test script:#!/bin/bashwhile true; do read -e -d ; -t 180 -p ><> srccommand if [ $? != 0 ]; then echo end; echo exit 0 fi case $srccommand in startapp) echo startapp command;; stopapp) echo stopapp command;; end) echo exit 0 ;; *) echo unknown command;; esacdonethis works but doesn't print the delimiter ';' char:# bash test.sh><> startappstartapp command><> stopappstopapp command><> endif i remove -e option it prints out ; but user can't correct his mistake using backspace character and echoed strings are just right after the delimiter:# bash test.sh><> startapp;startapp command><> stopapp;stopapp command><> end;how can i print out the delimiter character and allow user to edit line while reading standard input?this is the expected behavior:# bash test.sh><> startapp;startapp command><> stopapp;stopapp command><> end;thanks",
    "present_kp": [
      "bash",
      "read"
    ],
    "absent_kp": [
      "readline"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "create archive webpage for newsletters based on nested array. i am creating a webpage for my newsletter archives. the archives are divided by year (currently there is only 1, but there could be many), then by topic, before listing each newsletter. there will always be exactly five topics, but the number of newsletters in each topic will vary. i add the name of the newest newsletter in only one place - at the end of the array. i have a link to the most recent newsletter above the rest of the archives.this code works fine, but the way that i am retrieving the most recent newsletter seems somewhat hackythe newsletters are currently stored in my server as # segment year. $newsletter = array( year 1 => array( first topic => array( a,b,c,d,e,f,g,h,i, ), second topic => array( j,k,l,m,n,o,ab, ), third topic => array( cd,ef,fo,er,abd,cdf,eee, ), fourth topic => array( fff,wet,csg,qqq, ), fifth topic => array( ), ) );$archive_design = ;foreach($newsletter as $gilyear => $book){ $i=1; $archive_design .= <div id=$gilyear class=year> <!--<h3> year $gilyear </h3>--> <div class=year-content>; foreach($book as $book => $segment){ $archive_design .= <div id=$gilyear-$book class=column> <b><u>$book</u></b> <p></p> <ul> ; foreach($segment as $segment){ $link = $i .strtolower($segment). .$gilyear; $recent = $segment; $i++; $archive_design .= <li> <a href=\\<url> target='_blank'>$segment </a> <hr/> </li>; } $archive_design .= </ul></div><!--end $gilyear-$book-->; } $archive_design .= </div><!--end year-content --> </div><!--end $gilyear -->;}?><html lang=en ><body><div id=header> <a href=<url> echo $link; ?>.pdf target=_blank> current issue - <?php echo $recent;?></a></br> <h3>click on the segment to view, download and enjoy the archives!</h3></div><div id=archives><?php echo $archive_design;?></div><!--end archives-->",
    "present_kp": [
      "php",
      "html",
      "array"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to zip the particular type of file in a directory. i am having three kinds of files in a directory like .txt, .csv and files with no extensions.exampletestnestprodtest.csvprod.txtnest.csvi want to zip the files with no extension and move that zip to another directory. please help me to solve this",
    "present_kp": [],
    "absent_kp": [
      "shell script",
      "awk"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "on gentoo, how do i rebuild all packages depended on some other package?. i made mistake and changed perl non-threaded version to threaded by unmerge first, change use flags to include ithreads and emerge perl again. now most packages depending on perl are broken. how do i rebuild them?",
    "present_kp": [
      "gentoo",
      "emerge"
    ],
    "absent_kp": [
      "package management",
      "compiling"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "regression model for explained model(details inside). i am kind of a newbie on machine learning and i would like to ask some questions based on a problem i have .let's say i have x y z as variable and i have values of these variables as time progresses like :t0 = x0 y0 z0 t1 = x1 y1 z1 tn = xn yn zn now i want a model that when it's given 3 values of x , y , z i want a prediction of them like:input : x_test y_test z_test output : x_prediction y_prediction z_predictionthese values are float numbers. what is the best model for this kind of problem? thanks in advance for all the answers.more details:ok so let me give some more details about the problems so as to be more specific.i have run certain benchmarks and taken values of performance counters from the cores of a system per interval.the performance counters are the x , y , z in the above example.they are dependent to each other.simple example is x = ipc , y = cache misses , z = energy at core.so i got this dataset of all these performance counters per interval .what i want to do is create a model that after learning from the training dataset , it will be given a certain state of the core ( the performance counters) and predict the performance counters that the core will have in the next interval.",
    "present_kp": [
      "machine learning",
      "regression"
    ],
    "absent_kp": [
      "logistic regression",
      "predictive modeling"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "remove same column values. i have a very big file and i want to remove if the column value is 9.sample:my file value like this:1 5 8 3 5 9 5 7 6 92 5 7 4 2 9 7 6 3 15 9 7 4 1 9 5 7 9 1i want to delete any columns where the value, on all rows, is 9 ( my column size is very big then i can not check first column = 9 second column = 9 ... etc).i need a dynamic script.output should be like this:1 5 8 3 5 5 7 6 92 5 7 4 2 7 6 3 15 9 7 4 1 5 7 9 1i am new and i tried many things and did not do it.how can i do it?thanks for your help",
    "present_kp": [],
    "absent_kp": [
      "scripting"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "redirecting expired images to a category page got us a google penalty, but won't a 404 lose lots of seo juice?. we at a classified websites got banned from google images results. and we know why.you see, on our website, ads are subject to expiration. when ads expire we redirect the expired ad page with 301 to its closest listing page. google started indexing our images but as soon as their ads get expired, people who get to the site from the view image option lands on a redirection.google then, banned us on google results for images.we know what to do to fix this. we need to return a 404 instead of a redirection. but what about the seo juice that passes from the expired ads to our listing pages?",
    "present_kp": [
      "seo",
      "images"
    ],
    "absent_kp": [
      "redirects",
      "google image search"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to stop gold-plating and just be content to release working developments. the development team that i'm a member of has recently adapted to work according to agile practices. this has personally highlighted the fact that i can't stop myself gold-plating code (and documentation) and i consequently exceed original estimates, when i could've delivered solutions that meet the requirements much earlier.i think my ethic is bordering on the obsessive in that i become too attached to my code and am rarely content to release before i've refactored and perfected it to the nth degree. i am happy that i have realised this but how can i change my attitude/mentality to be content with my progress and release on-time instead?",
    "present_kp": [
      "agile"
    ],
    "absent_kp": [
      "programming practices",
      "productivity",
      "development process",
      "scrum"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "fix base64 data uri scripts function. so i noticed chrome has quirky behaviour when it encounters script tags whose src is a base64 value. i decided to write a quick jquery method that is supposed to work around it:jquery.extend({ /** * takes a script decodes the base64 src, puts it into the body of the script tag, * then puts it in whatever parent specified. * * @requires <url> * * @param {object} script the script tag that should be manipulated * @param {object} parent the parent element to append the final script tag to. * * @return {object} the script tag in question */ importbase64script : function ( script, parent ) { // check for base64 library if ( typeof $.base64 === 'undefined' ) throw 'no $.base64 found!'; // sanitize our script var // normalize our script object script = ( script instanceof jquery ? script : $(script) ); // check if it is a script tag if ( script[0].tagname !== script ) throw not a script tag; // set default parent value parent = parent || $('head'); // normalize our parent var parent = ( parent instanceof jquery ? parent : $(parent) ); // we're gonna extract the base64 value var re = /data:[a-z]+\\/[a-z]+;base64,([0-9a-za-z\\=\\+]+)/, base64content = script.prop('src').match(re)[1], scriptcontent = $.base64.decode( base64content ); // drop the decoded javascript into the contents of the script tag script.html( scriptcontent ); // clear src value script.prop('src',''); // append it to the parent parent.append(script); return script; }});i tested a few of the conditions on jsperf to see which is better performance wise. granted, i didn't do a full sweep on every browser.any suggestions that anybody could make?",
    "present_kp": [
      "javascript",
      "jquery"
    ],
    "absent_kp": [
      "dom"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "a simple hangman game in python. i'm a noobie in programming so i've made a simple hangman game in python and i'd like someone to tell me how i could improve the code.edit:how could i do it using classes?from sys import exitdef makegallow(gallow): for i in range(8): gallow.append([]) gallow[0].append(6 * '_') gallow[1].append(' | |') for i in range(2, 7): gallow[i].append(' |') gallow[7].append(11 * '-')gallow = list()secret_word = raw_input(choose a word: ).lower()secret_wordls = list()letters = len(secret_word)for c in secret_word: secret_wordls.append(c)blanks = list()for i in range(len(secret_word)): blanks.append('_')lettersused = list()tries = 0makegallow(gallow)while true: for i in gallow: print .join(i) print .join(i for i in blanks) used = , .join(i for i in lettersused) print letters used: + used letter = raw_input(type a letter: ).lower() if letter == 'quit': exit(1) elif letter in secret_word: letters -= 1 if letters == 0: print you won! exit(bye!) else: switch = secret_wordls.index(letter) blanks[switch] = letter elif letter not in secret_word: print wrong! tries += 1 lettersused.append(letter) if tries == 1: gallow[2] = ' o |' elif tries == 2: gallow[3] = ' | |' elif tries == 3: gallow[3] = '/| |' elif tries == 4: gallow[3] = '/|\\ |' elif tries == 5: gallow[4] = ' | |' elif tries == 6: gallow[5] = '/ |' elif tries == 7: gallow[5] = '/ \\ |' print sorry you lost :(. print the word was: + secret_word exit(bye!) elif letter in lettersused: print you have already typed that letter! print try another one.edit2: i found a bug where if the work had 2, 3... times the same letter it would count as one and you couldn't win. i change the while loop:while true: for i in gallow: print .join(i) print .join(i for i in blanks) used = , .join(i for i in lettersused) print letters used: + used letter = raw_input(type a letter: ).lower() if letter == 'quit': exit(bye) elif letter in secret_word: for i in range(secret_wordls.count(letter)): switch = secret_wordls.index(letter) blanks[switch] = letter secret_wordls[switch] = '-' letters -= 1 elif letter not in secret_word: print wrong! tries += 1 lettersused.append(letter) if tries == 1: gallow[2] = ' o |' elif tries == 2: gallow[3] = ' | |' elif tries == 3: gallow[3] = '/| |' elif tries == 4: gallow[3] = '/|\\ |' elif tries == 5: gallow[4] = ' | |' elif tries == 6: gallow[5] = '/ |' elif tries == 7: gallow[5] = '/ \\ |' print sorry you lost :(. print the word was: + secret_word exit(bye!) elif letter in lettersused: print you have already typed that letter! print try another one. if letters == 0: print you won! exit(bye!)",
    "present_kp": [
      "python",
      "hangman"
    ],
    "absent_kp": [
      "python 2.7"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "policy routing with load balanced ppp connections. this my setup using debian 6.0. 6. it's role is a proxy server and a load balancer using eight 3g modems.whenever a ppp connection dials it takes over the default gateway which is ok but it also cuts me out.the reason it's ok is that in ubuntu i noticed that it does not do that by default. by default it will leave the ethx gateway untouched. i could have added in the ppp options file replacedefaultroute but problems with iptables in ubuntu made me switch to debian.now i cannot ping other hosts on the lan but other hosts can ping me. i also lose connectivity from the outside and the only way i can get access is through another computer from inside the lan.also all the hosts in the lan can use the proxy.what i found out is that if i add a separate routing table called e1 and copy 192.168.2.0/24 dev eth0 proto kernel scope link src 192.168.2.126default via 192.168.2.3 dev eth0from the main table into table e1 and then do:ip rule add table e1it works, but this is not what i want since all the connections now flow through the eth0 interface.what can i try to restore the connections comming from outside, and to communicate with the local computers. however all traffic comming from the lan must still be made through the ppp links.root@proxy:~# iptables -l output -t mangle -vchain output (policy accept 433k packets, 217m bytes)pkts bytes target prot opt in out source destination433k 217m connmark all -- any any anywhere anywhere connmark restore 929 61011 mark all -- any any anywhere anywhere state new statistic mode nth every 8 mark set 0x1 929 61721 mark all -- any any anywhere anywhere state new statistic mode nth every 8 packet 1 mark set 0x2 929 61461 mark all -- any any anywhere anywhere state new statistic mode nth every 8 packet 2 mark set 0x3 929 61438 mark all -- any any anywhere anywhere state new statistic mode nth every 8 packet 3 mark set 0x4 929 61530 mark all -- any any anywhere anywhere state new statistic mode nth every 8 packet 4 mark set 0x5 929 61022 mark all -- any any anywhere anywhere state new statistic mode nth every 8 packet 5 mark set 0x6 929 61738 mark all -- any any anywhere anywhere state new statistic mode nth every 8 packet 6 mark set 0x7 928 61224 mark all -- any any anywhere anywhere state new statistic mode nth every 8 packet 7 mark set 0x8433k 217m connmark all -- any any anywhere anywhere connmark saveroot@proxy:~# iptables -l postrouting -t nat -vchain postrouting (policy accept 285 packets, 18881 bytes) pkts bytes target prot opt in out source destination 743 49005 masquerade all -- any ppp0 anywhere anywhere 705 47291 masquerade all -- any ppp1 anywhere anywhere 679 45581 masquerade all -- any ppp2 anywhere anywhere 679 45598 masquerade all -- any ppp3 anywhere anywhere 670 45177 masquerade all -- any ppp4 anywhere anywhere 638 42447 masquerade all -- any ppp5 anywhere anywhere 724 48671 masquerade all -- any ppp6 anywhere anywhere 679 45182 masquerade all -- any ppp7 anywhere anywhereroot@proxy:~#root@proxy:~# ip rule0: from all lookup local32758: from all fwmark 0x8 lookup d832759: from all fwmark 0x7 lookup d732760: from all fwmark 0x6 lookup d632761: from all fwmark 0x5 lookup d532762: from all fwmark 0x4 lookup d432763: from all fwmark 0x3 lookup d332764: from all fwmark 0x2 lookup d232765: from all fwmark 0x1 lookup d132766: from all lookup main32767: from all lookup defaultroot@proxy:~# ip ro sh t d1default via 10.64.64.64 dev ppp0root@proxy:~# ip ro sh t d2default via 10.64.64.65 dev ppp1root@proxy:~# ip ro sh t d3default via 10.64.64.66 dev ppp2root@proxy:~# ip ro sh t d4default via 10.64.64.67 dev ppp3root@proxy:~# ip ro sh t d5default via 10.64.64.68 dev ppp4root@proxy:~# ip ro sh t d6default via 10.64.64.69 dev ppp5root@proxy:~# ip ro sh t d7default via 10.64.64.70 dev ppp6root@proxy:~# ip ro sh t d8default via 10.64.64.71 dev ppp7root@proxy:~# ip ro10.64.64.67 dev ppp3 proto kernel scope link src 10.90.33.22110.64.64.66 dev ppp2 proto kernel scope link src 10.18.11.9010.64.64.65 dev ppp1 proto kernel scope link src 10.90.14.23510.64.64.64 dev ppp0 proto kernel scope link src 10.18.27.22610.64.64.71 dev ppp7 proto kernel scope link src 172.22.<phone>.64.64.70 dev ppp6 proto kernel scope link src 10.80.131.610.64.64.69 dev ppp5 proto kernel scope link src 172.20.17.18310.64.64.68 dev ppp4 proto kernel scope link src 10.80.61.34192.168.2.0/24 dev eth0 proto kernel scope link src 192.168.2.126default via 192.168.2.3 dev eth0root@proxy:~#if you can give me some ideas i would appreciate it..",
    "present_kp": [
      "iptables",
      "ip",
      "routing"
    ],
    "absent_kp": [
      "squid"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to change the path of shared libraries shown by ldd?. i am trying to make the postgis extension work on my system and it always spits out $libdir/postgis2.2 no such file or directory error. for my curiosity i executed ldd postgis-2.2.so and it spits out the following result : linux-vdso.so.1 => (0x00007ffff3bc8000) /usr/lib64/libjemalloc.so.1 (0x00002b3fe5ff4000) libgeos_c.so.1 => not found libproj.so.9 => not found libjson-c.so.2 => not found libxml2.so.2 => /usr/lib64/libxml2.so.2 (0x00002b3fe6237000) libm.so.6 => /lib64/libm.so.6 (0x00002b3fe659e000) libc.so.6 => /lib64/libc.so.6 (0x00002b3fe689c000) libpthread.so.0 => /lib64/libpthread.so.0 (0x00002b3fe6c41000) /lib64/ld-linux-x86-64.so.2 (0x00002b3fe5b2e000) libdl.so.2 => /lib64/libdl.so.2 (0x00002b3fe6e5d000) libz.so.1 => /lib64/libz.so.1 (0x00002b3fe7061000) liblzma.so.5 => /usr/lib64/liblzma.so.5 (0x00002b3fe7277000)and for a bunch of dependencies i see the path to their so is not present. this i guess is happening because i am not building postgis but just copying the required so's and libs to make it work manually. but i do know the path to those so files that are required by postgis. what should i do such that i can change the not found in dependencies to the required path by postgis?",
    "present_kp": [
      "libraries"
    ],
    "absent_kp": [
      "dynamic linking",
      "postgresql",
      "shared library"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is javascript, really?. all this started when i was looking for a way to test my webpage for javascript conformance like the w3c html validator. i have not found one yet. so let me know if you know of any...i looked for the official javascript page and find ecma script. these people have standardized a scripting language (i do not feel like calling it javascript anymore!) and called it ecma-262 (wikipedia). their latest work is edition 5.1javascript was developed by mozilla corporation and their last stable version is 1.8.5 (see this) which is based on the ecma's edition 5.1the wikipedia page linked mentions dialects. mozilla's javascript 1.8.5 is listed as a dialect along with jscript 9 (ie) and javascript (chrome's v8[wiki]) and a lot others. am i to understand that javascript 1.8.5 is a derivative of the ecma-262 and spidermonkey[wiki] is an engine that runs it? and chrome has its own dialect and v8 engine is the program that runs it?with all these dialects based off ecma-262, what i can no longer understand is what is javascript? are there any truly cross browser scripting languages? do the various implementers come together to agree on the dialect cross compatibility? is this effort ecma?",
    "present_kp": [
      "javascript"
    ],
    "absent_kp": [
      "history",
      "ecmascript"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "preventing multiple copies within function using static. i've a function to generate random characters from a set of alphabets. this function would be called multiple times and thus i'm trying to make it use same set of variables, i.e. have same seed so that strings don't repeat as long as possible.#include <iostream>#include <random>#include <string>std::string generaterandomchar(const unsigned int _len) { std::string result; result.reserve(_len); static constexpr char alphanum[] = <phone> abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvwxyz; std::random_device rd; std::mt19937 gen(rd()); std::uniform_int_distribution<> dis(0, 61); for (int i = 0; i < _len; result += (alphanum[dis(gen)]); } return result; }int main(){ for(int i = 0; i < 10; ++i){ std::cout << generaterandomchar(10) << std::endl; }}unfortunately i don't have any expertise with c++11 <random> functions and i was only using srand and friends earlier, so i might be making lots of mistakes here. currently it works and generates tons of strings without repeating, but i'm sure i could make either of:std::random_device rd;std::mt19937 gen(rd());std::uniform_int_distribution<> dis(0, 61);static variable too, so that it isn't calculated each time the function is called because that would be waste right? so which one should be static here? are there any mistakes/improvements that you see here?ideone",
    "present_kp": [
      "c++",
      "c++11",
      "random"
    ],
    "absent_kp": [
      "performance"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "trending it related search engine keywords. i tried google trends and insights but it doesn't specially give it (information technology) based trending keywords.",
    "present_kp": [
      "search engine"
    ],
    "absent_kp": [
      "statistics"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "suppress warning from ps -aux on linux. when i run the ps command i get a warning saying warning: bad syntax, perhaps a bogus '-'? see /usr/share/doc/procps-3.2.7/faq.how do i suppress this warning? is there some system setting that i must do for this. the command that i fire is:[root@localhost home]# ps -aux | grep curl -s -o test warning: bad syntax, perhaps a bogus '-'? see /usr/share/doc/procps-3.2.7/faqroot 4856 0.0 0.0 4044 672 pts/0 s+ 07:20 0:00 grep curl -s -o test[root@localhost home]# note that i have to fire the exact same command as above(i cannot change the -aux to aux, i cannot redirect stderr output). that is why i am looking for some system setting that will suppress the warning.",
    "present_kp": [
      "linux",
      "ps"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "can organizations be made temporarily invisible?. is it possible to make an organization invisible for periods of time and then make them visible at a later time when needed? we can make boards invisible without deleting them. is there a way to do the same with an organization?",
    "present_kp": [],
    "absent_kp": [
      "trello"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "capture errors in a sftp script. i have a sub sh file which puts receives an input of local directory and path to the remote directory and sends the file to the remote directory. if the remote path is invalid the error cannot be detected.sample content inside the sh script:sftp <email> <<==cd $remote_dir <= error happens herelcd $local_dir <= error did not happen so the return code resetsput a.tarbye==$result=$?return $resultsome application calls the sh file, then the return code is used to judge if the file was successfully sent or not. if yes, the local file is backed up somewhere then deleted.the problem is the shell returns zero even if the sending of the file failed. the file really was not copied, you can see that the error happens when you do the command manually, the path is not found is displayed in the screen, but when running as a job, it fails. i know another way to do this is to do two steps, first check if the path exists, then output to a log file and check if the log contains an error, if there are no error then you can call the sftp put shell script. i was wondering if there is another way.",
    "present_kp": [
      "sftp"
    ],
    "absent_kp": [
      "scripting",
      "error handling"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why is the generation of deep style images so slow and resource-hungry?. consider these neural style algorithms which produce some art work:neural doodleneural-stylewhy is generating such images so slow and why does it take huge amounts of memory? isn't there any method of optimizing the algorithm?what is the mechanism or technical limitation behind this? why we can't have a realtime processing?here are few user comments (how anyone can create deep style images):anything above 640x480 and we're talking days of heavy crunching and an insane amount of ram.i tried doing a 1024pixel image and it still crashed with 14gigs memory, and 26gigs swap. so most of the vm space is just the swapfile. plus it takes several hours potentially days cpu rendering this.i tried 1024x768 and with 16gig ram and 20+ gig swap it was still dying from lack of memory.having a memory issue, though. i'm using the g2.8xlarge instance type.",
    "present_kp": [
      "neural doodle"
    ],
    "absent_kp": [
      "performance",
      "deepdreaming"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why is a variable passed to the su command, but not an array from the same scope?. in the example below, why does $var get passed to the su in the start function, but not the array modules ?#!/bin/bashvar=catmodules=(onetwothreefour)start() { su gleventh -c for i in ${modules[@]}; do echo -- $i -- $var; done}$1echo out of functionfor i in ${modules[@]}; do echo -- $i -- $var; donethe script above returns:$ sudo ./test.sh start-- -- cat-- -- cat-- -- cat-- -- catout of function-- one -- cat-- two -- cat-- three -- cat-- four -- cat",
    "present_kp": [
      "bash"
    ],
    "absent_kp": [
      "shell",
      "quoting"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how can we create an installation disk from customized linux file system?. i have compiled a customized distro of linux from source code, using buildroot and i have new customized file system now. now i want to install new distro on any pc.what should i do to create a disk that it can boot and install new customized distro on pc?",
    "present_kp": [
      "linux",
      "buildroot"
    ],
    "absent_kp": [
      "system installation"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "surveys in other languages than english. where can i find surveys in languages than english ? if the question is found to be interesting (it could be usefull for graduated students who are not fluent in english for instance), it would be practical to propose one reference by answer, i.e. by subject and by language.",
    "present_kp": [],
    "absent_kp": [
      "cc.complexity theory",
      "reference request",
      "soft question",
      "big picture"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "stuck on laptop logo on statup. i had windows 10 on my laptop. then i have added kali-linux as dual boot.on start up i would get gnu grub. but after setting my bios password i am stuck on the splash screen that is the logo of my laptop upon booting up. any suggestion on how it can be fixed?",
    "present_kp": [
      "windows",
      "dual boot"
    ],
    "absent_kp": [
      "kali linux"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "convert free space lvm to ext. can i convert free space in a lvm partition to a ext3 partition?if i run pvs: pv vg fmt attr psize pfree /dev/sda3 ubuntu-vg lvm2 a-- 297,11g 30,02gso i have 30gb unused, i would like to take them out of the lvm partition to convert it in ext3 partition. is it possible? or is it better to just partition these 30 gb in a new logical volume?",
    "present_kp": [
      "partition",
      "lvm"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "gpgpu helper api. c# project to generate a 64-bit cekirdekler.dllc++ project to generate a64-bit kutuphanecl.dll (used in cekirdekler.dll)apart from error handling, does it have any catastrophic errors? i uploaded them from my computer using visual studio 2015 with a github add-on installed on it and synchronized repositories with two projects but i'm not sure if they have any other dependencies for other computers after they are compiled on them. if you can try its hello world, i'd appreciate it.here is the most frequently used class, which is used as a buffer wrapper to pass data to the opencl side from the c# side.using system;using system.collections;using system.collections.generic;using system.linq;using system.runtime.interopservices;using system.text;namespace cekirdekler{ /// <summary> /// parameter group to be crunched in kernel execution /// </summary> public interface icancompute { /// <summary> /// <para>blocking compute operation that does read + compute + write </para> /// </summary> /// <param name=cruncher></param> /// <param name=computeid></param> /// <param name=kernelnamesstring>string that contains all kernel names(to be executed) separated by space or by , or by ;</param> /// <param name=globalrange>total workitems to be distributed to devices</param> /// <param name=localrange>workitems per local workgroup. default value is 256</param> /// <param name=ofsetglobalrange>starting id for workitems.(for cluster add-on)</param> /// <param name=pipeline>true: pipeline is on</param> /// <param name=pipelinetype>cores.pipeline_event means event-driven 3-queued pipelined read+compute+write operation. </param> /// <param name=pipelineblobs>minimum 4, multiple of 4</param> void compute(clnumbercruncher cruncher, int computeid, string kernelnamesstring, int globalrange, int localrange = 256, int ofsetglobalrange = 0, bool pipeline = false, bool pipelinetype = cores.pipeline_event, int pipelineblobs = 4); } /// <summary> /// can optimize the buffer copies /// </summary> public interface ibufferoptimization { /// <summary> /// array to be used in computations. could be a c# array or fastarr{t} as a wrapper for c++ array /// </summary> object array { get; set; } /// <summary> /// read whole array before compute /// </summary> bool read { get; set; } /// <summary> /// <para>change the read into a partial type so it can be pipelined to hide latency to make compute() faster</para> /// </summary> bool partialread { get; set; } /// <summary> /// write results after kernel execution (possibly pipelined) /// </summary> bool write { get; set; } /// <summary> /// <para>number of array elements per workitem to be computed, to be buffer-copied. default=1</para> /// <para>also default=1 for c# arrays(without clarray nor fastarr )</para> /// <para>number of global workitems * elements per work item must not be greater than buffer size </para> /// <para>number of local workitems * elements per work item * number of pipeline blob must not be greater than buffer size </para> /// </summary> int numberofelementsperworkitem { get; set; } } /// <summary> /// holds clarray instances, c# arrays and fastarr instances to be computed later /// </summary> public class clparametergroup : icancompute, icanbind { /// <summary> /// not used, comes from interface /// </summary> public object array { get { throw new notimplementedexception(); } set { throw new notimplementedexception(); } } // for adding more parameters and to be turned into an array later internal linkedlist<object> arrays = new linkedlist<object>(); // read whole internal linkedlist<bool> reads = new linkedlist<bool>(); // read partial internal linkedlist<bool> partialreads = new linkedlist<bool>(); // write partial internal linkedlist<bool> writes = new linkedlist<bool>(); // elementsperworkitem internal linkedlist<int> arrayelementsperworkitem = new linkedlist<int>(); /// <summary> /// linked list to array conversion for all arrays /// </summary> /// <returns></returns> public object[] selectedarrays() { return arrays.toarray(); } /// <summary> /// <para>adds arrays as more parameters for kernel execution</para> /// <para>arrays order as parameters:</para> /// <para>this object - then - other params added to this object - then - parameters from .nextparam()</para> /// </summary> /// <param name=arrays_>c# arrays to add as kernel parameters</param> /// <returns>a clparametergroup that can crunch numbers or bind more parameters</returns> public clparametergroup nextparam(params array[] arrays_) { clparametergroup gr = new clparametergroup(); linkedlistnode<object> node = arrays.first; linkedlistnode<bool> node2 = reads.first; linkedlistnode<bool> node3 = partialreads.first; linkedlistnode<bool> node4 = writes.first; linkedlistnode<int> node5 = arrayelementsperworkitem.first; while(node!=null) { gr.arrays.addlast(node.value); gr.reads.addlast(node2.value); gr.partialreads.addlast(node3.value); gr.writes.addlast(node4.value); gr.arrayelementsperworkitem.addlast(node5.value); node = node.next; node2 = node2.next; node3 = node3.next; node4 = node4.next; node5 = node5.next; } for (int i = 0; i < arrays_.length; i++) { gr.arrays.addlast(arrays_[i]); // default values for c# arrays (they need to be wrapped in clarray to be modified) gr.reads.addlast(true); gr.partialreads.addlast(true); gr.writes.addlast(true); gr.arrayelementsperworkitem.addlast(1); } return gr; } /// <summary> /// <para>adds arrays as more parameters for kernel execution</para> /// <para>arrays order as parameters:</para> /// <para>this object - then - other params added to this object - then - parameters from .nextparam()</para> /// </summary> /// <param name=arrays_>fastarr type(clfloatarray,cluintarray,...) arrays to add as kernel parameters</param> /// <returns>a clparametergroup that can crunch numbers or bind more parameters</returns> public clparametergroup nextparam(params imemoryhandle[] arrays_) { clparametergroup gr = new clparametergroup(); linkedlistnode<object> node = arrays.first; linkedlistnode<bool> node2 = reads.first; linkedlistnode<bool> node3 = partialreads.first; linkedlistnode<bool> node4 = writes.first; linkedlistnode<int> node5 = arrayelementsperworkitem.first; while (node != null) { gr.arrays.addlast(node.value); gr.reads.addlast(node2.value); gr.partialreads.addlast(node3.value); gr.writes.addlast(node4.value); gr.arrayelementsperworkitem.addlast(node5.value); node = node.next; node2 = node2.next; node3 = node3.next; node4 = node4.next; node5 = node5.next; } for (int i = 0; i < arrays_.length; i++) { gr.arrays.addlast(arrays_[i]); // default values gr.reads.addlast(true); gr.partialreads.addlast(true); gr.writes.addlast(true); gr.arrayelementsperworkitem.addlast(1); } return gr; } /// <summary> /// <para>adds arrays as more parameters for kernel execution</para> /// <para>arrays order as parameters:</para> /// <para>this object - then - other params added to this object - then - parameters from .nextparam()</para> /// </summary> /// <param name=arrays_>clarray type arrays(which can wrap c# or c++ arrays) to add as kernel parameters</param> /// <returns>a clparametergroup that can crunch numbers or bind more parameters</returns> public clparametergroup nextparam(params ibufferoptimization[] arrays_) { clparametergroup gr = new clparametergroup(); linkedlistnode<object> node = arrays.first; linkedlistnode<bool> node2 = reads.first; linkedlistnode<bool> node3 = partialreads.first; linkedlistnode<bool> node4 = writes.first; linkedlistnode<int> node5 = arrayelementsperworkitem.first; while (node != null) { gr.arrays.addlast(node.value); gr.reads.addlast(node2.value); gr.partialreads.addlast(node3.value); gr.writes.addlast(node4.value); gr.arrayelementsperworkitem.addlast(node5.value); node = node.next; node2 = node2.next; node3 = node3.next; node4 = node4.next; node5 = node5.next; } for (int i = 0; i < arrays_.length; i++) { gr.arrays.addlast(arrays_[i]); gr.reads.addlast(arrays_[i].read); gr.partialreads.addlast(arrays_[i].partialread); gr.writes.addlast(arrays_[i].write); gr.arrayelementsperworkitem.addlast(arrays_[i].numberofelementsperworkitem); } return gr; } /// <summary> /// <para>adds arrays as more parameters for kernel execution</para> /// <para>arrays order as parameters:</para> /// <para>this object - then - other params added to this object - then - parameters from .nextparam()</para> /// </summary> /// <param name=parametergroups_>other parameter groups to add as kernel parameters</param> /// <returns>a clparametergroup that can crunch numbers or bind more parameters</returns> public clparametergroup nextparam(params clparametergroup[] parametergroups_) { clparametergroup gr = new clparametergroup(); linkedlistnode<object> node0 = arrays.first; linkedlistnode<bool> node02 = reads.first; linkedlistnode<bool> node03 = partialreads.first; linkedlistnode<bool> node04 = writes.first; linkedlistnode<int> node05 = arrayelementsperworkitem.first; while (node0 != null) { gr.arrays.addlast(node0.value); gr.reads.addlast(node02.value); gr.partialreads.addlast(node03.value); gr.writes.addlast(node04.value); gr.arrayelementsperworkitem.addlast(node05.value); node0 = node0.next; node02 = node02.next; node03 = node03.next; node04 = node04.next; node05 = node05.next; } for (int i = 0; i < parametergroups_.length; i++) { linkedlistnode<object> node = parametergroups_[i].arrays.first; linkedlistnode<bool> node2 = parametergroups_[i].reads.first; linkedlistnode<bool> node3 = parametergroups_[i].partialreads.first; linkedlistnode<bool> node4 = parametergroups_[i].writes.first; linkedlistnode<int> node5 = parametergroups_[i].arrayelementsperworkitem.first; while(node!=null) { if (node.value != null) { gr.arrays.addlast(node.value); gr.reads.addlast(node2.value); gr.partialreads.addlast(node3.value); gr.writes.addlast(node4.value); gr.arrayelementsperworkitem.addlast(node5.value); } node = node.next; node2 = node2.next; node3 = node3.next; node4 = node4.next; node5 = node5.next; } } return gr; } /// <summary> /// <para>blocking compute operation that does read + compute + write </para> /// </summary> /// <param name=cruncher></param> /// <param name=computeid></param> /// <param name=kernelnamesstring>string that contains all kernel names(to be executed) separated by space or by , or by ;</param> /// <param name=globalrange>total workitems to be distributed to devices</param> /// <param name=localrange>workitems per local workgroup. default value is 256</param> /// <param name=ofsetglobalrange>starting id for workitems.(for cluster add-on)</param> /// <param name=pipeline>true: pipeline is on</param> /// <param name=pipelinetype>cores.pipeline_event means event-driven 3-queued pipelined read+compute+write operation. </param> /// <param name=pipelineblobs>minimum 4, multiple of 4</param> public void compute(clnumbercruncher cruncher, int computeid, string kernelnamesstring, int globalrange, int localrange = 256, int ofsetglobalrange = 0, bool pipeline = false, bool pipelinetype = cores.pipeline_event, int pipelineblobs = 4) { string[] kernellertmp = kernelnamesstring.split(new string[] { ,,,;,-, }, stringsplitoptions.removeemptyentries); object[] arrs_ = arrays.toarray(); string[] reads_ = reads.select(x=> { return x? read :; }).toarray(); string[] partialreads_ = partialreads.select(x => { return x? partial :; }).toarray(); string[] writes_ = writes.select(x => { return x? write :; }).toarray(); string[] readwrites_ = new string[reads_.length]; for(int i=0;i<readwrites_.length;i++) { stringbuilder sb = new stringbuilder(partialreads_[i]); sb.append(reads_[i]); sb.append(writes_[i]); readwrites_[i] = sb.tostring(); } int[] elemperworkitem_ = arrayelementsperworkitem.toarray(); cruncher.numbercruncher.compute( kernellertmp, 0, , arrs_,readwrites_,elemperworkitem_, globalrange, computeid, ofsetglobalrange, pipeline, pipelineblobs, pipelinetype, localrange); if (cruncher.performancefeed) cruncher.numbercruncher.performancereport(computeid); } } /// <summary> /// binds new parameters to earlier ones to be used in kernel executions /// </summary> public interface icanbind { /// <summary> /// <para>adds arrays as more parameters for kernel execution</para> /// <para>arrays order as parameters:</para> /// <para>this object - then - other params added to this object - then - parameters from .nextparam()</para> /// </summary> /// <param name=parametergroups_>other parameter groups to add as kernel parameters</param> /// <returns>a clparametergroup that can crunch numbers or bind more parameters</returns> clparametergroup nextparam(params clparametergroup[] parametergroups_); /// <summary> /// <para>adds arrays as more parameters for kernel execution</para> /// <para>arrays order as parameters:</para> /// <para>this object - then - other params added to this object - then - parameters from .nextparam()</para> /// </summary> /// <param name=arrays_>clarray type arrays(which can wrap c# or c++ arrays) to add as kernel parameters</param> /// <returns>a clparametergroup that can crunch numbers or bind more parameters</returns> clparametergroup nextparam(params ibufferoptimization []arrays_); /// <summary> /// <para>adds arrays as more parameters for kernel execution</para> /// <para>arrays order as parameters:</para> /// <para>this object - then - other params added to this object - then - parameters from .nextparam()</para> /// </summary> /// <param name=arrays_>fastarr type(clfloatarray,cluintarray,...) arrays to add as kernel parameters</param> /// <returns>a clparametergroup that can crunch numbers or bind more parameters</returns> clparametergroup nextparam(params imemoryhandle[] arrays_); /// <summary> /// <para>adds arrays as more parameters for kernel execution</para> /// <para>arrays order as parameters:</para> /// <para>this object - then - other params added to this object - then - parameters from .nextparam()</para> /// </summary> /// <param name=arrays_>c# arrays to add as kernel parameters</param> /// <returns>a clparametergroup that can crunch numbers or bind more parameters</returns> clparametergroup nextparam(params array[] arrays_); /// <summary> /// c++ or c# array of current object /// </summary> object array { get; set; } } /// <summary> /// <para>float,byte,...</para> /// <para>double,long</para> /// <para>int,uint</para> /// </summary> /// <typeparam name=t></typeparam> public class clarray<t>: ilist<t>,icanbind,ibufferoptimization, icancompute { private int n; /// <summary> /// <para>number of elements of this array(c++ or c# side)</para> /// <para>when set to a different value, a new array is allocated(or created), old data is copied, remaining elements are defaulted</para> /// <para>if new value is smaller, elements at the end of array are lost</para> /// </summary> public int n { get { return n; } set { if(array==null || n==value) { // no array no change } else { if(iscsharparr) { t[] tmp__ = new t[n]; int il = math.min(n, value); var tmpt = array as t[]; array.constrainedcopy((t[])array, 0, tmp__, 0, il); if(il<value) { for (int i = il; i < value; i++) { tmp__[i] = default(t); } } array = tmp__; } else { object tmp__ = createfastarr(value); int il = math.min(n, value); var tmpt = array as fastarr<t>; var tmp___ = tmp__ as ilist<t>; for (int i = 0; i < il; i++) { tmp___[i] = tmpt[i]; } if (il < value) { for (int i = il; i < value; i++) { tmp___[i] = default(t); } } array = tmp__; } } n = value; } } /// <summary> /// <para>wrapper for c++ and c# arrays, can optimize their copies to devices(or devices' access to them)</para> /// </summary> /// <param name=n_>number of elements(not bytes), default=-1 (no array allocated)</param> /// <param name=a_>alignment value(in bytes), default=4096</param> public clarray(int n_=-1,int a_=4096) { n = n_; if(n_>0) { if(typeof(t) == typeof(float)) array = new clfloatarray(n_,a_); else if (typeof(t) == typeof(double)) array = new cldoublearray(n_, a_); else if (typeof(t) == typeof(int)) array = new clintarray(n_, a_); else if (typeof(t) == typeof(long)) array = new cllongarray(n_, a_); else if (typeof(t) == typeof(uint)) array = new cluintarray(n_, a_); else if (typeof(t) == typeof(char)) array = new clchararray(n_, a_); else if (typeof(t) == typeof(byte)) array = new clbytearray(n_, a_); } iscsharparr = false; // clarray,fastarr<t>,t[] default values read = true; // reads arrays as whole, not pipelined partialread = false; // no partial reads write = true; // partial writes numberofelementsperworkitem = 1; // 1 element per workitem } /// <summary> /// release c++ resources, callable multiple times /// </summary> public void dispose() { if (array != null) { if (!iscsharparr) { ((imemoryhandle)array).dispose(); } } } /// <summary> /// release c++ resources /// </summary> ~clarray() { if(array!=null) { if(!iscsharparr) { ((imemoryhandle)array).dispose(); } } } /// <summary> /// <para>if given true, allocates a c++ array(if c# array exists, copies from it)</para> /// <para>if given false, creates a c# array (if c++ array exists, copies from it)</para> /// <para>if array property is set from client code, this is changed accordingly with that array type</para> /// </summary> public bool fastarr { get { if (array == null) return false; return !iscsharparr; } set { // c++ array is requested if(value) { // if c# array exists, put c++ array in place of it and copy old values if(iscsharparr) { if (n > 0) { if (array != null) { object tmp_ = createfastarr(); ((imemoryoperations<t>)tmp_).copyfrom((t[])array, 0); array = tmp_; } else { array = createfastarr(); } } } // if c++ array already exists, no action, if no array, create new c++ array else if(!iscsharparr) { if(array==null) { if (n > 0) { array = createfastarr(); } } } } // c# array is requested else { // if c++ array exists, delete. put c# array in place of it and copy old values from it if (array != null && !iscsharparr) { if (n > 0) { t[] tmp_ = new t[n]; ((fastarr<t>)array).copyto(tmp_, 0); ((fastarr<t>)array).dispose(); array = tmp_; } } else if (array == null) { if(n>0) array = new t[n]; } } } } private object createfastarr(int n___=0) { if (n___ == 0) n___ = n; if (typeof(t) == typeof(float)) return new clfloatarray(n___); else if (typeof(t) == typeof(double)) return new cldoublearray(n___); else if (typeof(t) == typeof(int)) return new clintarray(n___); else if (typeof(t) == typeof(uint)) return new cluintarray(n___); else if (typeof(t) == typeof(long)) return new cllongarray(n___); else if (typeof(t) == typeof(char)) return new clchararray(n___); else if (typeof(t) == typeof(byte)) return new clbytearray(n___); else return null; } /// <summary> /// <para>float[], byte[], fastarr{t} ... </para> /// </summary> private object array_; private ilist<t> arrayasilist; /// <summary> /// c++(fastarr{t}) or c# (t[]) arrays /// </summary> public object array { get { return array_; } set { if (value.gettype() == typeof(t[])) iscsharparr = true; else iscsharparr = false; // when c++ array wrapper has no scope, destructor deletes it, can simmply set this: array_ = value; arrayasilist = array_ as ilist<t>; } } /// <summary> /// <para>initialize this wrapper from t[] or fastarr{t}</para> /// </summary> /// <param name=b></param> public static implicit operator clarray<t> (t[] b) { if (typeof(t) == typeof(int) || typeof(t) == typeof(uint) || typeof(t) == typeof(byte) || typeof(t) == typeof(char) || typeof(t) == typeof(double) || typeof(t) == typeof(long) || typeof(t) == typeof(float)) { clarray<t> clarray = new clarray<t>(); clarray.array = b; clarray.iscsharparr = true; return clarray; } else { return null; } } /// <summary> /// <para>initialize this wrapper from t[] or fastarr{t}</para> /// </summary> /// <param name=b></param> public static implicit operator clarray<t>(fastarr<t> b) { clarray<t> clarray = new clarray<t>(); clarray.array = b; clarray.iscsharparr = false; return clarray; } // for sub steps private t tmparr___ { get; set; } /// <summary> /// ilist{t} compatibility, gives number of elements of c# or c++ arrays /// </summary> public int count { get { return ((ilist<t>)array).count; } } /// <summary> /// ilist{t} compatibility, gives number of elements of c# or c++ arrays /// </summary> public int length { get { return ((ilist<t>)array).count; } } /// <summary> /// not implemented /// </summary> public bool isreadonly { get { throw new notimplementedexception(); } } private bool iscsharparr_; /// <summary> /// true = c# array /// false = c++ array (wrapper) /// </summary> internal bool iscsharparr { get { return iscsharparr_; } set { iscsharparr_ = value; } } /// <summary> /// not implemented /// </summary> /// <param name=item></param> /// <returns></returns> public int indexof(t item) { throw new notimplementedexception(); } /// <summary> /// not implemented /// </summary> /// <param name=index></param> /// <param name=item></param> public void insert(int index, t item) { throw new notimplementedexception(); } /// <summary> /// not implemented /// </summary> /// <param name=index></param> public void removeat(int index) { throw new notimplementedexception(); } /// <summary> /// not implemented /// </summary> /// <param name=item></param> public void add(t item) { throw new notimplementedexception(); } /// <summary> /// not implemented /// </summary> public void clear() { throw new notimplementedexception(); } /// <summary> /// not implemented /// </summary> /// <param name=item></param> /// <returns></returns> public bool contains(t item) { throw new notimplementedexception(); } /// <summary> /// copies to c# array (from its c++ or c# array) /// </summary> /// <param name=array>c# array</param> /// <param name=arrayindex>element index</param> public void copyto(t[] array, int arrayindex) { if (!iscsharparr) { // using ilist interface ((ilist<t>)this.array).copyto(array, arrayindex); } else { // using c# array ((ilist<t>)this.array).copyto(array, arrayindex); } } /// <summary> /// copies to other clarray objects' arrays /// </summary> /// <param name=array></param> /// <param name=arrayindex></param> public void copyto(clarray<t> array, int arrayindex) { if(!iscsharparr) { if(!array.iscsharparr) { // both c++ arrays ((imemoryoperations<t>)this.array).copyto_((fastarr<t>)array.array, arrayindex); } else { // uses ilist interface ((ilist<t>)this.array).copyto((t[])array.array, arrayindex); } } else { // this object has c# array if (!array.iscsharparr) { // other object has c++ array ((imemoryoperations<t>)array.array).copyfrom((t[])this.array, arrayindex); } else { // both c# arrays ((ilist<t>)this.array).copyto((t[])array.array, arrayindex); } } } /// <summary> /// copy to c++ array (fastarr) /// </summary> /// <param name=array></param> /// <param name=arrayindex></param> public void copyto(fastarr<t> array, int arrayindex) { if (!iscsharparr) { // both c++ arrays ((imemoryoperations<t>)this.array).copyto_(array, arrayindex); } else { // this is c# array, parameter is c++ array array.copyfrom((t[])this.array, arrayindex); } } /// <summary> /// copies from c# array(to c++ or c# array of itself) /// </summary> /// <param name=array></param> /// <param name=arrayindex></param> public void copyfrom(t[] array, int arrayindex) { if (!iscsharparr) { // this is c++ array, other is c# array ((imemoryoperations<t>)this.array).copyfrom((t[])array, arrayindex); } else { // both c# arrays ((ilist<t>)array).copyto((t[])this.array, arrayindex); } } /// <summary> /// copies from other clarray objects /// </summary> /// <param name=array></param> /// <param name=arrayindex></param> public void copyfrom(clarray<t> array, int arrayindex) { if (!iscsharparr) { if (!array.iscsharparr) { // both c++ arrays ((imemoryoperations<t>)this.array).copyfrom_((fastarr<t>)array.array, arrayindex); } else { // this is c++ array, other is c# array ((imemoryoperations<t>)this.array).copyfrom((t[])array.array, arrayindex); } } else { if (!array.iscsharparr) { // this is c# array, other is c++ array ((ilist<t>)array.array).copyto((t[])this.array, arrayindex); } else { // both c# arrays ((ilist<t>)array.array).copyto((t[])this.array, arrayindex); } } } /// <summary> /// copies from c++ array(to its own c++ or c# array) /// </summary> /// <param name=array></param> /// <param name=arrayindex></param> public void copyfrom(fastarr<t> array, int arrayindex) { if (!iscsharparr) { // both c++ arrays ((imemoryoperations<t>)this.array).copyfrom_(array, arrayindex); } else { // this is c# array, other is c++ array ((ilist<t>)array).copyto((t[])this.array, arrayindex); } } /// <summary> /// not implemented /// </summary> /// <param name=item></param> /// <returns></returns> public bool remove(t item) { throw new notimplementedexception(); } /// <summary> /// not implemented /// </summary> /// <returns></returns> public ienumerator<t> getenumerator() { throw new notimplementedexception(); } /// <summary> /// not implemented /// </summary> /// <returns></returns> ienumerator ienumerable.getenumerator() { throw new notimplementedexception(); } /// <summary> /// <para>adds arrays as more parameters for kernel execution</para> /// <para>arrays order as parameters:</para> /// <para>this object - then - other params added to this object - then - parameters from .nextparam()</para> /// </summary> /// <param name=arrays_>c# arrays to add as kernel parameters</param> /// <returns>a clparametergroup that can crunch numbers or bind more parameters</returns> public clparametergroup nextparam(params array[] arrays_) { clparametergroup bd = new clparametergroup(); bd.arrays.addlast(array); bd.reads.addlast(read); bd.partialreads.addlast(partialread); bd.writes.addlast(write); bd.arrayelementsperworkitem.addlast(numberofelementsperworkitem); for (int i = 0; i < arrays_.length; i++) { bd.arrays.addlast(arrays_[i]); // default bd.reads.addlast(true); bd.partialreads.addlast(false); bd.writes.addlast(true); bd.arrayelementsperworkitem.addlast(1); } return bd; } /// <summary> /// <para>adds arrays as more parameters for kernel execution</para> /// <para>arrays order as parameters:</para> /// <para>this object - then - other params added to this object - then - parameters from .nextparam()</para> /// </summary> /// <param name=arrays_>fastarr type(clfloatarray,cluintarray,...) arrays to add as kernel parameters</param> /// <returns>a clparametergroup that can crunch numbers or bind more parameters</returns> public clparametergroup nextparam(params imemoryhandle[] arrays_) { clparametergroup bd = new clparametergroup(); bd.arrays.addlast(array); bd.reads.addlast(read); bd.partialreads.addlast(partialread); bd.writes.addlast(write); bd.arrayelementsperworkitem.addlast(numberofelementsperworkitem); for (int i = 0; i < arrays_.length; i++) { bd.arrays.addlast(arrays_[i]); // default bd.reads.addlast(true); bd.partialreads.addlast(false); bd.writes.addlast(true); bd.arrayelementsperworkitem.addlast(1); } return bd; } /// <summary> /// <para>adds arrays as more parameters for kernel execution</para> /// <para>arrays order as parameters:</para> /// <para>this object - then - other params added to this object - then - parameters from .nextparam()</para> /// </summary> /// <param name=arrays_>clarray type arrays(which can wrap c# or c++ arrays) to add as kernel parameters</param> /// <returns>a clparametergroup that can crunch numbers or bind more parameters</returns> public clparametergroup nextparam(params ibufferoptimization[] arrays_) { clparametergroup bd = new clparametergroup(); bd.arrays.addlast(array); bd.reads.addlast(read); bd.partialreads.addlast(partialread); bd.writes.addlast(write); bd.arrayelementsperworkitem.addlast(numberofelementsperworkitem); for (int i = 0; i < arrays_.length; i++) { bd.arrays.addlast(arrays_[i]); // default bd.reads.addlast(arrays_[i].read); bd.partialreads.addlast(arrays_[i].partialread); bd.writes.addlast(arrays_[i].write); bd.arrayelementsperworkitem.addlast(arrays_[i].numberofelementsperworkitem); } return bd; } /// <summary> /// <para>adds arrays as more parameters for kernel execution</para> /// <para>arrays order as parameters:</para> /// <para>this object - then - other params added to this object - then - parameters from .nextparam()</para> /// </summary> /// <param name=parametergroups_>other parameter groups to add as kernel parameters</param> /// <returns>a clparametergroup that can crunch numbers or bind more parameters</returns> public clparametergroup nextparam(params clparametergroup[] parametergroups_) { clparametergroup bd = new clparametergroup(); bd.arrays.addlast(array); bd.reads.addlast(read); bd.partialreads.addlast(partialread); bd.writes.addlast(write); bd.arrayelementsperworkitem.addlast(numberofelementsperworkitem); for (int i = 0; i < parametergroups_.length; i++) { linkedlistnode<object> node = parametergroups_[i].arrays.first; linkedlistnode<bool> node2 = parametergroups_[i].reads.first; linkedlistnode<bool> node3 = parametergroups_[i].partialreads.first; linkedlistnode<bool> node4 = parametergroups_[i].writes.first; linkedlistnode<int> node5 = parametergroups_[i].arrayelementsperworkitem.first; while (node!=null) { if (node.value != null) { bd.arrays.addlast(node.value); bd.reads.addlast(node2.value); bd.partialreads.addlast(node3.value); bd.writes.addlast(node4.value); bd.arrayelementsperworkitem.addlast(node5.value); } node = node.next; node2 = node2.next; node3 = node3.next; node4 = node4.next; node5 = node5.next; } } return bd; } /// <summary> /// <para>blocking compute operation that does read + compute + write </para> /// </summary> /// <param name=cruncher></param> /// <param name=computeid></param> /// <param name=kernelnamesstring>string that contains all kernel names(to be executed) separated by space or by , or by ;</param> /// <param name=globalrange>total workitems to be distributed to devices</param> /// <param name=localrange>workitems per local workgroup. default value is 256</param> /// <param name=ofsetglobalrange>starting id for workitems.(for cluster add-on)</param> /// <param name=pipeline>true: pipeline is on</param> /// <param name=pipelinetype>cores.pipeline_event means event-driven 3-queued pipelined read+compute+write operation. </param> /// <param name=pipelineblobs>minimum 4, multiple of 4</param> public void compute(clnumbercruncher cruncher, int computeid, string kernelnamesstring, int globalrange, int localrange = 256, int ofsetglobalrange = 0, bool pipeline = false, bool pipelinetype = cores.pipeline_event, int pipelineblobs = 4) { string[] kernellertmp = kernelnamesstring.split(new string[] { , ,, ;, -, }, stringsplitoptions.removeemptyentries); object[] arrs_ = new object[] { array }; string[] reads_ = new string[] { read ? read : }; string[] partialreads_ = new string[]{ partialread ? partial : }; string[] writes_ = new string[] { write ? write : }; string[] readwrites_ = new string[reads_.length]; for (int i = 0; i < readwrites_.length; i++) { stringbuilder sb = new stringbuilder(partialreads_[i]); sb.append(reads_[i]); sb.append(writes_[i]); readwrites_[i] = sb.tostring(); } int[] elemsperworkitem_ = new int[]{ numberofelementsperworkitem }; cruncher.numbercruncher.compute( kernellertmp, 0, , arrs_, readwrites_, elemsperworkitem_, globalrange, computeid, ofsetglobalrange, pipeline, pipelineblobs, pipelinetype, localrange); if (cruncher.performancefeed) cruncher.numbercruncher.performancereport(computeid); } /// <summary> /// reads whole array before compute (no pipelining) /// </summary> public bool read { get; set; } /// <summary> /// <para>partial reads(possibly pipelined)</para> /// </summary> public bool partialread { get; set; } /// <summary> /// partial writes (possibly pipelined) /// </summary> public bool write { get; set; } /// <summary> /// <para>gnumber of array elements per workitem, default=1</para> /// <para>global range * this number must be smaller than or equal to array size </para> /// </summary> public int numberofelementsperworkitem { get; set; } /// <summary> /// <para>direct access to c++ array elements just like c# arrays</para> /// <para>don't cross boundaries, don't use after dispose()</para> /// </summary> /// <param name=i></param> /// <returns></returns> public t this[int i] { get { return arrayasilist[i]; } set { arrayasilist[i] = value; } } }}what about getting large at file size? does it look like a god object? i guess it is doing only its own job, buffer handling between c# and c++. is adding more interfaces in same file ok?",
    "present_kp": [
      "c#",
      "c++",
      "opencl"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "lynx is trying to unzip my downloads. i am having a problem with the lynx text browser. when i try to download a some tar.gz files, lynx is not downloading it. it is putting it in a tmp directory and then trying to unpack it with the command:/bin/tar tvf -it does not always do this. often it does just download the file, but if it is a link on a sourceforge site (one of those redirect your download will start shortly links), then it does the unwanted tvf behavior. obviously this is wrong because it is gzipped file, so lynx then hangs. the apparent reason it is doing this is that there is a line in /etc/mailcap that reads like this:application/x-tar; /bin/tar tfv -; print=/bin/tar tvf - [etc, more gobblediguck]this is doubly wrong because i don't want lynx to put it in a tmp directory and unpack the file. i want it to download it to the current directory.ideally, i do not want to monkey with my mailcap settings, having no experience with such things. how can i get lynx to just download tar.gz files when they are coming from sourceforge/auto download sites?",
    "present_kp": [
      "tar",
      "download",
      "lynx"
    ],
    "absent_kp": [
      "web",
      "mime types"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "does amazon change the specs of a given aws machine instance type over time?. we wish to reproduce results from a paper published in 2014 that used the m3.2xlargeinstance type. if we run that today would it have the same specs? if not is there a way to find documentation on what the differences would be? in particular:ram differencesnetworking speed differencescpu differences e.g. faster core i7 cpu's..changes to disk types (ssd vs sata) and access times",
    "present_kp": [],
    "absent_kp": [
      "amazon ec2",
      "amazon web services"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "exit vim more quickly. i use vim mainly for quick edits rather than long work sessions. in that sense, i find the keyboard sequence for quitting especially laborious: esc, shift + ;, w, q, enter.how to quit vim (possibly saving the document) with the least keystrokes? especially from insert mode.",
    "present_kp": [
      "vim"
    ],
    "absent_kp": [
      "keyboard shortcuts"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "simple property formatting. in a vb.net class, i have an method that replaces all square brackets [] in certain properties by normal brackets ().edit:the program allows users to write custom calculations, such as [3-(6-2)]. being able to use two kinds of brackets makes things easier for them, but when processing the calculation, it's easier to have the same brackets everywhere.the code is really redundant:public sub removebadbrackets() valuecalc = replace(replace(valuecalc, [, (), ], )) targetcalc = replace(replace(targetcalc, [, (), ], )) lllcalc = replace(replace(lllcalc, [, (), ], )) llcalc = replace(replace(llcalc, [, (), ], )) lcalc = replace(replace(lcalc, [, (), ], )) hcalc = replace(replace(hcalc, [, (), ], )) hhcalc = replace(replace(hhcalc, [, (), ], )) hhhcalc = replace(replace(hhhcalc, [, (), ], ))end sub i think this could be done in a more efficient way, and i'd like to learn how.side note:those properties are not the only one of the object.they are in the same region (although i am not sure regions can be used for anything else than my own convenience).thank you for your help improving this.",
    "present_kp": [
      ".net",
      "vb.net"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "using kde in debian squeeze. i just installed debian squeeze and it defaulted to the gnome desktop environment. i have installed kde via the package manager but do not know how to set it as the default environment. any help would be appreciated.",
    "present_kp": [
      "debian",
      "kde"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "switch to https - does backlink value to http drop?. when switching from http to https, how much of the backlink value is lost in the process?has it been documented anywhere what this instant switch does to how google views & evaluates the backlink profile?with nearly 100% external sites pointing to the http version of the site, how urgently do we need to gain links pointing to the https, in order to reverse any damage (if at all) done by the switch?",
    "present_kp": [
      "https"
    ],
    "absent_kp": [
      "seo",
      "backlinks"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how i may develop in php and html?. i am learning php from the internet on codeacademy.com. i have also learned html and css but the problem that i dont know how can i build a complete website then !i study everything on its side. but how to put all my knowledge together its very hard! like when should i use this php code? when should i use the html code? so that i can make a full website.",
    "present_kp": [
      "php",
      "html"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "cryptoanalysis of vigenere-ciphered text with kasiski test. i am creating a program that will analyse encrypted text using kasiski examination in order to find possible key length:#include <stdio.h>#include <stdlib.h>#include <conio.h>#include <string.h>#include <time.h>char *file_get_contents(char *filename, char *fmode) { file *f = fopen(filename, fmode); fseek(f, 0l, seek_end); long fsize = ftell(f); rewind(f); char *file_contents = malloc(fsize+1); fread(file_contents, fsize, 1, f); fclose(f); file_contents[fsize] = 0; return file_contents;}char *str_copy(char *string, int start, int length) { char *s = malloc(length); strncpy(s, string+start, length); s[length+1] = ''; return s;}void count_factors(int *array, int number) { int i; for (i=2; i<number; i++) if (number % i == 0) array[i]++;}int main() { clock_t start, end; double cpu_time_used; start = clock(); char *file_contents = file_get_contents(cipher.txt, rt); char *test_string, *compare_string, match, is_in_array; unsigned seq_min = 3, seq_max = 4; unsigned long i, j, k, f, number, str_index = 0; unsigned long iterations = 0; unsigned *factors_index = {0}; unsigned text_length = strlen(file_contents); unsigned long *factors_value = calloc(text_length, sizeof(unsigned long)); unsigned arr_str_count = text_length+1, arr_str_length = seq_max+1; char strings[arr_str_count][arr_str_length]; unsigned count_strings = 1; test_string = str_copy(file_contents, 0, seq_min); strcpy(strings[0], test_string); free(test_string); for (i=seq_min; i<=seq_max; i++) { count_strings = 1; for (j=0; j<text_length-i*2;) { match = 0; is_in_array = 0; test_string = str_copy(file_contents, j, i); for (f=0; f<arr_str_count; f++) { if(strcmp(strings[i], test_string) == 0) { is_in_array = 1; iterations++; break; } } //skipping couple hundreds of iterations in case we already processed current n-gram if (!is_in_array) { strcpy(strings[count_strings], test_string); count_strings++; str_index = j; for (k=j+i; k<text_length-i-1;) { compare_string = str_copy(file_contents, k, i); if (strcmp(test_string, compare_string) == 0) { number = (k-str_index); //distance between current and old n-gram for (f=2; f<number; f++) if (number % f == 0) { factors_value[f]++; iterations++; } str_index = k; match = 1; k += i; } else { k++; } free(compare_string); iterations++; } } match == 1 ? j+=i : j++; free(test_string); } } for (i=0; i<100; i++) { if (factors_value[i] > 1) printf(%d: %d , i, factors_value[i]); } printf(%llu iterations , iterations); free(file_contents); free(factors_value); end = clock(); cpu_time_used = ((double) (end-start)) / clocks_per_sec; printf(process finished: %4.3f secs. , cpu_time_used); getch(); return 0;}i've done all my best in order to make this process be as fast as i could make it, however, i consider this code pretty slow. using test text with length = 25663 characters i get following results:251,375,553 iterationsprocessing time: 17,387 seconds.text for this test (originally taken from dummytext-generator website) was encrypted with a 5 characters long key. as a final result i get following top of possible key values (value: number of occurrences): 2: 28227 3: 17769 4: 14605 5: <phone>: <phone>: <phone>: 13169the result is pretty precise in my opinion.so, my question is: how can i increase the performance of this code?",
    "present_kp": [
      "performance",
      "c"
    ],
    "absent_kp": [
      "cryptography",
      "vigenere cipher"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "mint 13 xfce: geforce fx 5500 not new enough to support nvidia-settings display configuration page. i have a geforce fx 5500 graphics card and so far, it has not worked properly in my linux mint 13 xfce installation.i have tried several solutions to get the graphics card working but nothing helped. i have searched the internet for solutions and tried them. (including the experimental 310 driver).after installing the experimental 310 driver (?) i saw the boot animation for the first time since i installed linux mint.i have tried every single driver that was available in the 'additional drivers' screen. nothing worked.i have tried to download and install the drivers directly from the nvidia website but i could not manage to get this install (had to do it as root, i tried, but everytime there was something wrong).i am running linux mint 13 xfce now (considering to install the latest mint lts release, or try elementary os (hoping that with elementary os the card works out of the box properly).but does someone have a solution for me?no matter what i try, everytime, in nvidia x server settings, the x display screen tells me: the nvidia driver is not new enough to... etc. see image below (not my screen, but same error):",
    "present_kp": [
      "linux mint",
      "drivers",
      "nvidia",
      "graphics",
      "x server"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "why are the first inode of the '/' mounted partition and inode of '/' different?. [~]$ stat -c %i /2as you can see in above, the inode for / is 2. but the first inode of /dev/sda2 is 11.[~]$ df -hfilesystem size used avail use% mounted on/dev/sda2 350g 67g 266g 21% /tmpfs 12g 44m 12g 1% /dev/shm[~]$ sudo tune2fs -l /dev/sda2 | grep 'first inode'first inode: 11can any one help me to understand this difference?",
    "present_kp": [
      "inode"
    ],
    "absent_kp": [
      "linux",
      "filesystems",
      "root filesystem",
      "vfs"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to delete a row in a google sheet based on date. i have a google sheet i'm using to geocode addresses, which shows where current road closures are located. in my table, i have a date field startdate and a number field duration. duration equals the number of days the road will remain closed after the startdate. i need a way to automatically delete the row when the duration of the closure has elapsed (i.e., startdate plus duration = new date when row will be automatically deleted). is there an easy way to accomplish this delete? i am not a programmer, so any help would be charitable.",
    "present_kp": [],
    "absent_kp": [
      "google spreadsheets"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "loading kernel module 'tp_smapi' fails for 'unknown symbol dev_printk (err -22)'. im running parabola gnu/linux-libre on a thinkpad x200, flashed with libreboot. my kernel version is 4.9.11-gnu-1.this is my problem: i cannot load the tp_smapi kernel module.# modprobe tp_smapimodprobe: error: could not insert 'tp_smapi': invalid argument# dmesg | tail -n 2[] tp_smapi: disagrees about version of symbol[ ] tp_smapi: unknown symbol dev_printk (err -22)theres a tp_smapi kernel module for managing some of the laptops hardware, see thinkpad wiki and arch wiki. i have installed the corresponding package tp_smapi from the parabola repositories via pacman. the kernel module in question is made for parabola and in particular for the linux-libre kernel:# pacman -qi tp_smapi | head -n 2name : tp_smapiversion : 0.42-10.parabola1.basekernel4.9searching for dev_printk led me to a linux cross reference page on it. it seems weird that such an essential symbol is claimed to be unknown.whats the issue here? what are good steps to do next in order to fix this? is it safe to force the insertion of the module via modprobe -f tp_smapi?",
    "present_kp": [],
    "absent_kp": [
      "kernel modules"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to check if 'wget' supports a certain feature programatically?. is there a way to programatically detect if the currently installed wget supports a given feature, for example, iri support?im asking because id like to add these lines to my .wgetrc:# use utf-8 as the default system encodinglocal_encoding = utf-8but this makes wget unusable if it doesnt support the local_encoding setting.so, id like to conditionally configure this setting, only if wget supports the feature.in vim (.vimrc files) you can do this using has() and exists():if has(autocmd) define autocommands hereendifhow can i do something similar for wget in .wgetrc?",
    "present_kp": [
      "wget"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "can search engines read title attributes that are modified using jscript?. i'm using a custom tooltip library, to view my titles and image alts. the way an image or link is rendered without this script is:<img src=url here title=title here height=123 width=321>as you see the title is there. but after rendering the page while having the script enabled, the above image will be rendered as:<img src=url here class=custom tooltip height=123 width=321>the alt is gone (or the title, if it's a link tag), and it's replaced by the custom class. when you hover it, you can see the proper title/alt. the html output is changed in the console, but not in the source output (obviously because jscript is client side). now the question is, can search engines still read the alt/title attributes?i've read that nowadays google renders your page with javascript enabled, so i'm not sure if it takes this into account.",
    "present_kp": [
      "title attribute"
    ],
    "absent_kp": [
      "seo",
      "images",
      "alt attribute"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "vim plugin to autocomplete method names of ruby packages?. i want to find a vim plugin that is able to autocomplete method names like in ide. i first import a ruby package such as prime and oauth. then i write prime::tab and i want to see all available commands.how can i get autocompletion for ruby methods of packages preferrably in dropdown list?",
    "present_kp": [],
    "absent_kp": [
      "plugin system",
      "filetype ruby"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "optimizing a 'while' loop. i have created a mini script to reboot my raspberry pi upon the push of a button. the script simply uses wiringpi (gpio command) to set pin 0 (pin 17 in raspberry pi standard numbering order) to input, and then reads the value until it's one (that's, when the button is pushed or held down).here's my script:gpio mode 0 inwhile (true)do if [ 'gpio read 0' -eq 1 ] then echo password | sudo -s reboot break fidone &the script works fine and everything. however, for those of you not familiar with the pi, it comes with very limited hardware resources (including 512 mb of memory) which can be easily consumed by a loop like the one i'm using.what i'm trying to achieve here is to find another way for bash to find out when the value has changed from 0 to 1 without having to dedicate a more like an unconditional loop for it. is this doable? please share your ideas.",
    "present_kp": [
      "bash",
      "raspberry pi"
    ],
    "absent_kp": [
      "shell script"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to create twitter vertical count button?. i saw vertical button by twitter in some pages and tutorials but i couldn't find out how to get the code for it. how can i make it look like the first widget on this image?",
    "present_kp": [
      "twitter"
    ],
    "absent_kp": [
      "social",
      "widgets"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "gvim doesn't remember cursor position within line if previous session ended on line 1. i noticed that whenever i quit gvim, it will remember the line and position within the line that the cursor was on, unless the cursor was on line 1. if the cursor was on line 1, then it will not remember the position within line 1.so if i have the cursor at the end of line 2 and i quit gvim, then the next time i open the same file in gvim, it will place the cursor at the end of line 2.but if i have the cursor at the end of line 1 and i quit gvim, then the next time i open the same file in gvim, it will place the cursor at the beginning of line 1.is this a bug in gvim?i'm running gvim 7.2 on windows.",
    "present_kp": [
      "gvim"
    ],
    "absent_kp": [
      "cursor movement",
      "microsoft windows"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "javascript string manipulation with certain logic in it. how would i make this code look less ugly?// we are looping over an array of images// width is a parameter passed to the current function //// the resulting string looks like this: 30px 0// (it's a tool that returns coordinates for a css sprite)// when iterating over the first item, we don't need // to add - and px to x coordinateresult.push( ( i === 0 ? 0 : - + i * ( ( is2pixelratio ? width / 2 : width ) + 10 ) + px ) + 0);",
    "present_kp": [
      "javascript"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "get current terminal name. how do i fetch the current terminal name?i mean to the name that ps shows in the tty column, e.g.:root@dor-desktop:/home/dor/documents/lamp_setup/webs_install/do/install# ps aux | egrep 'mysql|(^user)'user pid %cpu %mem vsz rss tty stat start time commanddor 2238 0.2 1.9 448052 79796 ? s 17:27 0:17 gedit /home/dor/documents/lamp_setup/webs_install/do/install/mysql.install /home/dor/documents/lamp_setup/webs_install/do/install/mysql.setuproot 4975 0.1 0.5 324984 22876 ? s 18:12 0:04 gedit /usr/local/mysql/bin/mysqld_saferoot 8160 0.0 0.0 4108 664 pts/2 s 19:08 0:00 /bin/sh /usr/local/mysql/bin/mysqld_safe --skip-networking --skip-grant-tables --user=mysql --basedir=/usr/local/mysql --ledir=/usr/local/mysql/libexecmysql 8279 0.0 0.4 146552 19032 pts/2 sl 19:08 0:00 /usr/local/mysql/libexec/mysqld --basedir=/usr/local/mysql --datadir=/usr/local/mysql/var --user=mysql --skip-networking --skip-grant-tables --log-error=/usr/local/mysql/var/dor-desktop.err --pid-file=/usr/local/mysql/var/dor-desktop.pid --socket=/usr/local/mysql/mysql.sock --port=3306root 8342 0.0 0.0 7632 1024 pts/2 r+ 19:14 0:00 egrep --color=auto mysql|(^user)in the above example, i need to fetch pts/2 which is probably the name for the current terminal that executed those commands.",
    "present_kp": [
      "terminal"
    ],
    "absent_kp": [
      "bash"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "rest api design with big (detailed) and small objects. we're facing the issue that we often have classes with lots of properties. but we don't need them for most api calls. so we end up having multiple versions for basically the same business object.e.g. public class smallcountry{ string code { get; set; } string name { get; set; }}andpublic class country{ string code { get; set; } string name { get; set; } string iso3166_2 { get; set; } string iso3166alpha2 { get; set; } string iso3166alpha3 { get; set; } long iso3166numeric { get; set; } }we discussed different options with interfaces, properties, special serializers, dynamic classes, etc. but nothing really convinced us. i'm sure we are not the only ones with this issue.the classes can be much bigger and sometimes we have lists with hundreds of them. so i think it will make a difference for rest api calls.we are using c# with an mvc web api.",
    "present_kp": [
      "c#",
      "api design"
    ],
    "absent_kp": [
      "class design"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "does google analytics track links that use onclick=parent.location='something.html' ?. i am using onclick=parent.location='#' to make a table into a link, but will google analytics see this as a link or will it cause mayhem in the reports?",
    "present_kp": [
      "html",
      "google analytics"
    ],
    "absent_kp": [
      "javascript"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "running darktable under xfce using specific color style/theme. i tried running darktable (version 1.0.4-1~bpo60+1 from debian squeeze-backports) under xfce, but i am using a fairly light style theme in xfce which darktable didn't work too well with. it seems to do some magic to set specific colors in gtk/gnome dialogs (the file open dialog for import being one example), and the two clash resulting in nearly unreadable file listings.when i switched to a darker theme (i tried xfce-dusk, but others worked similarly well), the colors aligned and usage was much smoother. however, i'd rather not switch to a dark color theme just for this one application, and i don't see any obvious way of switching darktable to a lighter color theme.i did find mention of the gtk2_rc_files environment variable (set it to the full path to a gtkrc), which seems to work for e.g. gedit (gtk2_rc_files=/usr/share/themes/highcontrastlargeprintinverse/gtk-2.0/gtkrc gedit works quite nicely and only affects that instance) but it does not seem to have any effect for darktable. since changing the global theme does work, there's obviously some way to make this work. so what other magic is needed?",
    "present_kp": [
      "xfce",
      "gtk",
      "theme"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "upload file over ssh and execute command on the remote machine. i am trying to find the simplest way to upload a file using ssh and after that run a command on the remote machine within the same ssh session for some post-processing, so that i don't need to login again. the upload should, if possible, show some progress indicator. so far i looked into scp and rsync, and both are not capable of running any hooks. (i could use the --rsync-path parameter to execute some script before rsync) but i want to do post-processing. is there any way to open a ssh session, upload, execute a command and close it again?",
    "present_kp": [
      "ssh"
    ],
    "absent_kp": [
      "file copy"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "determining if a digraph has any vertex-disjoint cycle cover. given a digraph, determine if the graph has any vertex-disjoint cycle cover.i understand that the permanent of the adjacency matrix will give me the number of cycle covers for the graph, which is 0 if there are no cycle covers. but is it possible to check the above condition directly?",
    "present_kp": [
      "adjacency matrix"
    ],
    "absent_kp": [
      "graphs",
      "graph theory"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is declarative ui?. i keep seeing this term being thrown around in blogs about frameworks. while i understand the difference between declarative and imperative programming, how does this apply specifically to ui? why does there appear to be a special term for it? are these different 'things'? if so, what is the alternative to declarative ui and why would i want to use it?",
    "present_kp": [
      "frameworks",
      "ui"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "error building and compiling gcc 5.2.0 from scratch on vortex86dx. in order to upgrade a vortexdx86 custom linux with a gcc 3.2.3 compiler, im trying to built the gcc 5.2.0 compiler to support the latest c++ 11 standard.i have downloaded its source code from gcc.gnu.org and did the standard linux package builder based on the this gnu link:$ mkdir ../gcc-build$ cd ../gcc-build$ ../gcc-5.2.0/configure --prefix=/usr --disable-multilib --with-system-zlib --enable-languages=c,c++the configuration runs fine. the i do:$ makeand im getting the following error: make[3]: entering directory '/home/ftp/pub/gcc-5.2.0/host-i586-pc-linux-gnu/gcc'g++ -c -g -din_gcc -fno-exceptions -fno-rtti -fasynchronous-unwind-tables -w -wall -wwrite-strings -wcast-qual -wno-format -wmissing-format-attribute -woverloaded-virtual -fno-common -dhave_config_h -dgenerator_file -i. -ibuild -i../.././gcc -i../.././gcc/build -i../.././gcc/../include -i../.././gcc/../libcpp/include \\ -o build/genmddeps.o ../.././gcc/genmddeps.ccc1plus: warning: -wmissing-format-attribute ignored without -wformatin file included from ../../gcc/genmddeps.c:19:../../gcc/system.h:201:19: string: no such file or directory../../gcc/system.h:218:22: algorithm: no such file or directory../../gcc/system.h:219:20: cstring: no such file or directory../../gcc/system.h:220:20: utility: no such file or directory../../gcc/system.h:249:19: cstdlib: no such file or directorymake[3]: *** [build/genmddeps.o] error 1make[3]: leaving directory '/home/ftp/pub/gcc-5.2.0/host-i586-pc-linux-gnu/gcc'make[2]: *** [all-stage1-gcc] error 2make[2]: leaving directory '/home/ftp/pub/gcc-5.2.0'make[1]: *** [stage1-bubble] error 2make[1]: leaving directory '/home/ftp/pub/gcc-5.2.0'make: *** [all] error 2after that the make procedure aborts. ive installed all the dependencies (tcl, expect, dejagnu, perl, m4, gmp, mpfr and mpc) and i dont know what is missing.as said, the original vortex linux has a gcc 3.2.3 compiler version already installed.i need to solve that but i dont know where to start from. it seens to have confusion with the own gcc libraries....help appreciated to solve that.",
    "present_kp": [
      "compiling",
      "gcc"
    ],
    "absent_kp": [
      "software installation"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "reduce number of parameters of the same type. i'm looking at a class diagram and it shows a method like this:saveprintoptions(bool,bool,bool,bool,bool,bool,bool,bool,bool);where each of the parameters match up with save options (checkboxes) in the ui. how could i refactor this signature to reduce the number of parameters?",
    "present_kp": [],
    "absent_kp": [
      "c#"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "reordering todo lists in trello cards. is there a way to reorder todo lists within a card? if not, i would like to see such a feature implemented.",
    "present_kp": [
      "trello cards"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how can i create a wifi hotspot and share my localhost across. i know how to make a wifi hotspot on my linux machine(using manjaro with i3 de), also i managed to connect my iphone to the wifi connection i made, but the goal is to start lampp and make my iphone and other devices, like another pc, a smart tv have access to my localhost server.is it possible to make this even without an internet connection? like without my wifi router on, or without any ethernet? i guess you could say that is somehow a fake hotspot without access to the outer world :dbasically i need something like a wifi lan, so if anybody could suggest some ideas, software or some tutorials/documentation on net, i would be grateful.",
    "present_kp": [
      "manjaro",
      "wifi hotspot"
    ],
    "absent_kp": [
      "wlan"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "sorting a data set with multiple 'tie breaks'. i'm currently working on a software project that requires several different types of sorting mechanisms. i have been searching, to no avail, to find an algorithm to do a sort with tie-breaking. in other words, say you have a data set that could be represented like so:[name of data]:[someone's id]-[same someone's score],[someone's id]-[same someone's score],[etc...] [name of data]:[someone's id]-[same someone's score],[someone's id]-[same someone's score],[etc...] [etc...]one line, then, may look like this: wins:7-1,8-1,9-1,2-1,10-1,3-0,4-0,5-0,1-0,6-0and another looks like this:speakerpoints:6-26,2-20,4-19,7-17,8-16,9-16,5-16,1-12,3-11,10-8i would like to be able to sort the ids in terms of 'winner', where the 'wins' criteria is 1st priority, and if there is a tie, the program would move to the 'speakerpoints' criteria, and so on...ordinarily i would use a platform specific mechanism (like linq for windows or nssortdescriptor for macos). however, the project is shared code that should compile on any platform necessary. if it matters, i'm using silver, an implementation of apple's swift that compiles for windows/macos/ios/android/etc... targets, so i am basically limited to using pure swift (pure swift code works perfectly well). how might i implement a sort to do this? i don't care about the algorithm being super efficient (though i don't want a simple sort to take hours either, i could do that myself :p), a modified bubble sort would be fine as long as it returned the correctly sorted id's.as a complication, the number of criteria is also a variable: this time it may be wins and speakerpoints, but next time may include opponentwins",
    "present_kp": [
      "sorting"
    ],
    "absent_kp": [
      "algorithms",
      "swift language"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "beginning c bst implementation. i'd really appreciate if i could get some feedback on the following code with regard to security, efficiency and possible uncaught errors. personally i feel the printandcleanstring and friendlylookup functions combined are very messy but i don't know how to fix them. in particular i would love feedack on how to more cleanly and efficiently return strings. #include <stdio.h>#include <malloc.h>#include <mem.h>struct bstnode { int val; struct bstnode *left; struct bstnode *right;};void insert(struct bstnode **head, int val);int lookup(struct bstnode **node, int val);void printdfs(struct bstnode *head);char *friendlylookup(struct bstnode **node, int val);void printandcleanstring(char *string);int main() { struct bstnode *bsttree = null; insert(&bsttree, 8); insert(&bsttree, 5); insert(&bsttree, 98); insert(&bsttree, 2); insert(&bsttree, 15); insert(&bsttree, 65); insert(&bsttree, 15); printdfs(bsttree); printf( ); printandcleanstring(friendlylookup(&bsttree, 1)); printandcleanstring(friendlylookup(&bsttree, 65));}void insert(struct bstnode **head, int val) { if (*head == null) { *head = malloc(sizeof(struct bstnode)); if (*head == null) { printf(malloc failed!); return; } (*head)->val = val; (*head)->left = null; (*head)->right = null; return; } if (val < (*head)->val) { return insert(&(*head)->left, val); } else { return insert(&(*head)->right, val); }}void printdfs(struct bstnode *head) { if (head->left != null) printdfs(head->left); printf(%d , head->val); if (head->right != null) printdfs(head->right);}int lookup(struct bstnode **node, int val) { if (*node == null) { return -1; } if ((*node)->val == val) { return val; } if (val < (*node)->val) { return lookup(&(*node)->left, val); } else { return lookup(&(*node)->right, val); }}char *friendlylookup(struct bstnode **node, int val) { int result = lookup(node, val); char resultstring[256]; char numberstring[256]; sprintf(numberstring, %d, val); if (result > -1) { sprintf(resultstring, is present in the bst ); } else { sprintf(resultstring, is not present in the bst ); } char *resultpointer = malloc(strlen(resultstring) + strlen(numberstring) + 1 * sizeof(char)); if (resultpointer == null) { printf(malloc error!); return null; } sprintf(resultpointer, %s%s, numberstring, resultstring); return resultpointer;}void printandcleanstring(char *string) { printf(string); free(string);}",
    "present_kp": [
      "c",
      "tree"
    ],
    "absent_kp": [
      "beginner"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "using wildcards to match a directory in bash. lets say the folder structure is like so:/home/--user1/asdf--user2/asdf1234--user3/asdf325234--cool/asdfhow could i change to asdf1234 without specifying the user? for example:cd /home/*/asdf1234how can i use not in bash? for example, lets say i want to go to /home/cool but use not capability:cd /home/!user*/asdf is this possible?are these bash tricks possible?",
    "present_kp": [
      "bash",
      "directory",
      "wildcards"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "auto execute a program that plays music. as mentionned above, i would like to execute a programme that plays music at certain hour automatically. i tried using crontab but no outcome. this is what i put in crontab: 8 15 30 3 4 python play_music.pywithout the scheduler, the python script is working. i wonder if using crontab is a solution to my problem as crontab might execute the program in its environment. does any one have an idea or other solutions? thanks in advance.",
    "present_kp": [
      "cron",
      "python"
    ],
    "absent_kp": [
      "programming"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "are proactive designs on new projects useful?. when starting a new project, are proactive designs useful? i mean, if you create a uml diagram/activity diagram/use case/class diagram/etc. for anything and everything can think of, then when you think you're done, you start coding. afterwards, you realize an important feature or method was left out. all your spiffy uml diagrams are nice stacks of worthless paper now.is it easier/better to design one class, work on it a bit, refine it, then work on it more?",
    "present_kp": [
      "design"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "is genetic programming relevant today?. my main concern is whether the genetic programming is an active field of research, with some promising applications in practice. it seems like in field of machine learning, the neural networks are the main buzzword, with mentions in mainstream news today, but i have never heard of similar genetic programming success story.",
    "present_kp": [
      "machine learning"
    ],
    "absent_kp": [
      "evolutionary computing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "the top reseller hosts with unlimited web space and bandwidth. hi i'm looking at finding myself a reseller hosting plan (preferably based in the uk) and i wondering if anyone knows of any good reseller hosts that provide unlimited disk space and bandwidth.the only uk host i seem able to find that matches this description is heartinternet.co.uk. are there any others you would recommend? p.s. hosts with cpanel get a +1 !!",
    "present_kp": [
      "reseller"
    ],
    "absent_kp": [
      "web hosting"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to manually detect deadlocks. i understand the concepts of deadlock well enough, but when i'm given a problem like the one below i'm not sure how to go about solving it. i can draw a resource allocation graph, but i'm not sure how to solve it from there.is there a better more formal way of solving this?consider a system with five processes, p1 through p5, and five resources, r1 through r5. resource ownership is as follows. p1 holds r1 and wants r3 p2 holds r2 and wants r1 p3 holds r3 and wants r5 p4 holds r5 and wants r2 p5 holds r4 and wants r2 is this system deadlocked? justify your answer. if the system is deadlocked, list the involved processes.",
    "present_kp": [],
    "absent_kp": [
      "algorithms"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "placing pdf files with keywords on many different wordpress sites. i do a daily search for my primary keyword phrases just to see what comes up. the past couple of months google results show 2-4 new items every day and each is a pdf file at a different wordpress.com site. for example, <url>. somejunk is just an example, of course, but it is always a meaningless name that very much appears to be computer generated. each pdf has several short snippets of text containing relevant content, including my primary keywords, but from several different websites, often from my website, but mostly from competitors' websites. there are no links in these pdf files. maybe this is being done by someone with an interest in my primary keywords, perhaps a competitor.is this some sort of seo strategy? how would it benefit anyone if it does not contain backlinks? also, does it represent a violation of my copyright when a short, partial sentence from my website is used?",
    "present_kp": [
      "seo",
      "wordpress",
      "keywords"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to find image base of an arbitrary boot-loader image?. i have picked up a rooted acer phone and dd-ed the lk partition to analyze it in ida. the goal is to pinpoint the routine processing fastboot commands. stripping first 512 bytes produces apparently meaningful disassembly. however, the cross reference of the string fastboot: processing commands doesn't look very meaningful. this has led me to believe that the image might need to be re-based to some address to get the references correct. any idea how i can do that?",
    "present_kp": [],
    "absent_kp": [
      "android"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "find the index of the element with maximum absolute deviation. i just completed a codility test. the question presented was pretty simple but i may have over thought it. question:find the element which has the highest absolute deviation from the mean. e.g. array = [1,1,-1,1].the average is 0.5, so the absolute deviation for each element in the array is [0.5,0.5,1.5,0.5] (note that the -1.5 goes to 1.5 after taking the absolute values of each deviation value) and thus the index with the max absolute deviation is 2 (the index starts from 0).my code was:def solution(a): if len(a) > 0: average = sum(a)*1.0/len(a) max_avg = 0 pos = 0 for i in range(len(a)): if abs(a[i] - average) > max_avg: max_avg = abs(a[i] - average) pos = i return pos else: return -1if there's a tie between the max deviation elements i.e. two or more indexes have the same absolute deviation then either of them can be given. the required time-complexity is \\$o(n)\\$ and space-complexity is \\$o(1)\\$. in theory, this problem should be really easy but this was my first technical test and i might have overlooked something. did i possibly miss anything?",
    "present_kp": [
      "complexity"
    ],
    "absent_kp": [
      "python",
      "programming challenge"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "baidu spider does not obey dns ttl. i moved my website from one vps to another on 16th april.the a record of the domain name has a ttl of 86400. i made the ip address change at 19:00 gmt 16th april.i do not want the site to face downtime so i am keeping the old vps alive.today is the third day and the access log on the old server shows hits from baidu spider.it has been more than 60 hours but why is this crawler still hitting the old ip address?i am using aws router53 for dns.",
    "present_kp": [
      "dns",
      "baidu"
    ],
    "absent_kp": [
      "web crawlers"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "single responsibility, api methods, error logging, and helper classes. i'm getting in a heated debate about the single responsibility principle and some code that i wrote. i feel strongly that i am right and would like some unbiased feedback.basically, we have decided to log a certain way in our api methods. since this code is exactly the same across all api methods i wrote some code to wrap it up in a class i'm calling methodsupport. i think i should note, eventually we plan to move to using an attribute that will add logging and error handling to our api methods.public class methodsupport{ public string classname { get; private set; } public string methodname { get; private set; } public dictionary<string, string> parameters { get; private set; } public methodsupport (string classname, string methodname, dictionary<string, string> parameters) { this.classname = classname; this.methodname = methodname; this.parameters = parameters; } public string getsignature() { var parameterscsv = string.join(, , this.parameters); var message = ${this.classname}.{this.methodname}({parameterscsv}); return message; } public void logbegin() { var message = this.getsignature() + : begin; logger.debug(message); } public void logend() { var message = this.getsignature() + : end; logger.debug(message); } public httpresponsemessage runlogic(httprequestmessage request, func<httresponsemessage> func) { this.logbegin(); httpresponsemessage response; try { response = func(); } catch (exception e) { response = exceptionhelper.handleexception(request, this, e); } this.logend(); return response; }}used like so...public class testcontroller : apicontroller{ public httpresponsemessage get(int id) { var methodsupport = new methodsupport(this.tostring(), nameof(get), new dictionary<string, string> { { nameof(id), id.gettype() } }); var result = methodsupport.runlogic(request, () => { //logic for the api method... }); return result; }}so, i'm open to a full discussion on the validity of this code. that being said, the specific response from my coworker is that methodsupport has two responsibilities. logging and running logic. my response is, logging is not a responsibility of this class because we could change how we log and it wouldn't effect this class (unless we change contracts/interface). what are your thoughts?i should add, he is our architect and wants me to delete the code leaving us with repeating everything that methodsupport does for all our api methods. in his defense, there is a plan to redo the application but we haven't even started on that.",
    "present_kp": [
      "single responsibility"
    ],
    "absent_kp": [
      "c#",
      "code quality",
      "code reviews"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "reason for security software to not scan directories with more than 10,000 resources in them?. we have a security software on linux which scans for exploits in files that exist in directories on the server (such as if websites had hack scripts uploaded to them). it defaults to a limit of 10,000 resources per directory or else it won't scan the files in that directory, but it permits setting it to unlimited if desired. i was thinking of increasing the limit but was wondering if there's something about linux and lots of files in a directory that causes some sort of non-linear slowdown or some other reason for limiting. like would scanning a directory with 50,000 files take exponentially longer than scanning 500 directories of 100 files each or some other reason?edit: here are some benchmarks in response to the comments### directory with 50,000 test files #### time for file in *; do echo test > $file; donereal 0m3.432s user 0m1.128s sys 0m2.303s # time for file in *; do a=$(head -1 $file); donereal 1m1.338suser 0m5.878ssys 0m22.885s### directory with 500 test files #### time for i in {1..100}; do ( for file in *; do echo test > $file; done ) ; done real 0m4.150s user 0m1.309s sys 0m2.801s# time for i in {1..100}; do ( for file in *; do a=$(head -1 $file); done ) ; donereal 0m58.278suser 0m3.558ssys 0m18.554s",
    "present_kp": [
      "linux",
      "files",
      "directory"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "should code comments have scope?. i am asking this because i have seen places where, whoever coded initially had provided proper comments, but later on modifications were made to the code but the comments were left untouched. i remember reading somewhere don't get suckered in by the comments, debug only code.so is it a good/ relevant/ practical idea that tells the scope of the comments so as to prompt the developer for editing the comment.your thoughts.",
    "present_kp": [
      "comments"
    ],
    "absent_kp": [
      "programming languages"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is a good way to sort two log files as they are created?. i have two scripts running, both of which output a log file. i'd like to make a third script that can sort these logs by time stamp and merge them into one file as they are created. what is a good way to do this, ideally without overwriting the file constantly?",
    "present_kp": [
      "logs",
      "sort"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "www server restart after upgrading bash regarding the shell shock bug?. if a webserver uses cgi with bash, does it calls bash every time when there is a request regarding cgi or does it caches (bash and related libraries) it when starting the webserver? q: so is a webserver restart needed after upgrading the bash packge regarding the shell shock bug?update: <url> that create such environment variables will need to be restarted to work with the new version of bash. this behavior is not used by any of the packages provided in any version of red hat enterprise linux.but later they write: do i need to reboot or restart services after installing this update?no, a reboot of your system or any of your services is not required.",
    "present_kp": [
      "bash",
      "cgi"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "turn off markdown highlighting for underscores in inline code?. markdown highlighting is great (when it works), but i've seen a problem on two different boxes with underscores in inline code and i'm wondering if there's an easy solution.when i put, for instance, file_names in inline code, the underscores in between the backticks are stripped of their special meaning as far as markdown rendering engines are concerned. however, vim still highlights them differently. on one box (my windows box using mobaxterm) the underscores themselves are highlighted in red, but none of the surrounding text is displayed differently (which isn't too bad). but on my mac, all the text from one underscore to the next is highlighted with a white background (on my black background terminal) which is extremely distracting.is there a way to get the markdown syntax highlighting code to properly ignore the underscores when they occur between backticks?",
    "present_kp": [
      "syntax highlighting"
    ],
    "absent_kp": [
      "filetype markdown"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to identify and report an ad causing an involuntary redirect?. an advertisement keeps popping up for me on multiple sites which redirects the user to a scam form after a delay of a few seconds. i was able to identify the culprit by this warning triggered in chrome when an iframe redirects the top-level page:frame with url '<url> attempted to navigate its top-level window with url 'https://...'. navigating the top-level window from a cross-origin iframe will soon require that the iframe has received a user gesture. see <url> url removed.)the ad redirects tohttps://prx2tst.global.ssl.fastly.net/in/ads_n00171528/?ads=wy4enxv9bs&sspid=pubmatic&referer=https%3a%2f%2fdsa.targetix.net%2fopenrtbdelivery%2fmarkup%3fr%3dszeuxjfdgs%26id%3d112%26ope%3d_y3sz3_x_ewux.mti8woom_gjymig9_gvj5dlitt1xdyutrn7i.bnmeg4cr.vkifhmxzqrlqaa.v1trt_djmqutyw0nrkvaolbmuxuub1p4bxfyrvhinfw_x6p888zuedtqu0yxp_.5spwifgqdrdc48xcmhnzvwqanna2xfklm8f7cue5obk1s0syf72jsjlyhhp6.gbkefjyer63ijaqmmjl0nghj6yp1y2q3xsqcd0dwxcz3xtt2mzpogm9hw70i2gvghsqasvkyajq_4k6cgmhvcuznk1b4knma93r0aofwso8if7ie1c1il0f0imbayaz3eeyygg1cmxn7dr7xbigscavsj8orgnxxvgmqdxz575ehdkadisvbaswtj6iesctljslc7jvyxfulybsbofx0zvnylapxmtg8jii69c4ja9gi9nsvk.il0ebefjkfwetihw.1csrykx.trcaqzfd03brdi90okqp2x71brnmafhcx93bxigjs1cte4b%26cturl%3dhttp%253a%252f%252fclicktrack.pubmatic.com%252fadserver%252faddisplaytrackerservlet%253fclickdata%253djnb1yklkpte1njmxnszzaxrlswq9mjazotawjmfkswq9mtewnziwnyzrywrzaxplawq9osz0bgrjzd0wjmnhbxbhawduswq9mty1mzamy3jlyxrpdmvjzd0wjmfku2vydmvyswq9mjqzjmltcglkptzgq0mxqtcwlte4nzatndgwnc04oeewlue1mte0rda5rtyymizwyxnzymfjaz0w_url%253d&p=ads_n00171528&click=%7bclick_url%7d&sitedomain=thenextweb.com&rand=null&pubid=pubmatic_156315&siteid=null&location=https%3a%2f%2fdsa.targetix.net%2fad%2fload%3fsrve%3d_tukeajopypaaq.vnzigi_jcjq7qqdube9r7zgrftr7rwxwjbikxumbmotdxybpanglt4vyb3fusxuhdyw8qo.c6m65c7kx8ptaa.0jhhxmjmdmkkhg13uy1wxr3afjjlxfahqc7lsuy7gaapx5bcogon.7kurpeew8ypbm9j66letzk5okubdktlkkoe2ufdddzuabgwmio1drscmzwp2aay0k7.w.2ukiqph8p4q_bldezglnnkctngfyqeax9athm3ph7nujlf2blbovaorctr8v7javkbwkshekkormjev18lm5ltss4w_qo8dixcohqluevpag39yhyprh6qfnet5u88wus1qaxnkxn8vc5kr71qvkie.tsi3tun.ou5kaycnqt.tg.ew8d.x8hryxv9tuhzcnu5.wotqdsstscolkftlwb0djpgjd.qdjp5i7w9gxfhw3sctehquasujclobt9pzuyw8x6dmhqzjps%26ctu%3dhttp%25253a%25252f%25252fclicktrack.pubmatic.com%25252fadserver%25252faddisplaytrackerservlet%25253fclickdata%25253djnb1yklkpte1njmxnszzaxrlswq9mjazotawjmfkswq9mtewnziwnyzrywrzaxplawq9osz0bgrjzd0wjmnhbxbhawduswq9mty1mzamy3jlyxrpdmvjzd0wjmfku2vydmvyswq9mjqzjmltcglkptzgq0mxqtcwlte4nzatndgwnc04oeewlue1mte0rda5rtyymizwyxnzymfjaz0w_url%25253d&dt1=0&dt2=0&dt4=2&dt5=0&dt6=0&dt7=0&dt8=17&dt9=na&dt10=no%3a25%7cblogger%7ccarbonmade%7camazon.com%7cbattle.net%7cairbnb%7cfacebook%7cexpedia%7cedx%7cfoursquare%7cgoogle_plus%7chackernews%7cmedium%7cmeetup%7cflickr%7ckhan_academy%7creddit%7cindeed%7cpaypal%7cskype%7cstack%7cvk%7ctumblr%7ctwitter%7csteam%7cpinterest%3bok%3a6%7cdropbox%7cdisqus%7cgmail%7cbitbucket%7cgithub%7cyoutube%3berr%3a4%7cacademia.edu%7cspotify%7csquare%7c500px&dt11=1and lands on the scam page after a few redirects, though these redirects could be different for different users. i was not able to reproduce the redirection when opening the ad frame in a new window, either. it seems that the ad is sending some parameters to a server first to check whether it should do the redirection or not. in fact, the ad first downloads a script from maguiredigital.com/html, which in turn fetches the following json string from trustedmarketings.com:{ location: https:\\/\\/prx2tst.global.ssl.fastly.net\\/in\\/uid-120ena-cid-90128p0079065\\/?ads=en6s9of8l5&sspid=pubmatic&referer=https%3a%2f%2fdsa.targetix.net%2fopenrtbdelivery%2fmarkup%3fr%3derfadtipho%26id%3d112%26ope%3d_vxmyzlez6t2.prlsbzjlae2uiu84try8s10d8edapsns4dxovugjkzpll8c4ywimseccxk_ihdahfnjmx.j5gsdgnpp0k_z39b150yrazmwkb19alnuwns3k8eajm8yex1e9nae1rk9dcrrsblir.jee8bqasn6hnv2iz5u.xcq40ej_pwf.phk9rf0pysb_v.ysqqz.hzn5gyft5eddhsgff.24vfkvznejf1r98ommtqy_1s.zlxlgi9a9jl9oeljibvps16u52_cnqqrfeonhr485ijxcxnftvl8zhrz5t3wglngoedmwhejrz.n7fnryph6gnnl8xya9w3yxy_jpetvna1kwinna.knm_nsiqzbhtm6npyblb3zqedskcdhi2wtthw9hsdokm5c1y8yh27xfeicjewza2ms4tpbfcasrkya_hdl4rwmf9cf8%26cturl%3dhttp%253a%252f%252fclicktrack.pubmatic.com%252fadserver%252faddisplaytrackerservlet%253fclickdata%253djnb1yklkpte1njixmizzaxrlswq9mtg2njyxjmfkswq9mta0ntc4nczrywrzaxplawq9nyz0bgrjzd0yntezmdg0nszjyw1wywlnbklkpte2ntmwjmnyzwf0axzlswq9mczhzfnlcnzlcklkpti0myzpbxbpzd0wotjbmuq5oc0xrdvcltqxnkitodiwos1bodhemuy5qurfqkemcgfzc2jhy2s9ma%253d%253d_url%253d&p=uid-120ena-cid-90128p0079065&click=%7bclick_url%7d&sitedomain=zerochan.net&rand=null&pubid=pubmatic_156212&siteid=null&location=https%3a%2f%2fdsa.targetix.net%2fad%2fload%3fsrve%3d_p__kwxxyz9cuhi3wc6tmyqfrmot2feo7b1zkiv8wnpcoioy09dekj4fjv.bj3p2azija4sh3yae60hm_6kzfu4hw0xnny0yy82zrehw0lhfwwoojwzewchglfkuzb.0f2jisdc64tcuuvj0dbh8usilu_8hejtixhjm3_.7kx2idyqp0l_mr_pzdpfhqtuyqfphcmzept62bppplntfrt86ntutpkxhclbd55k6efpuebqbz3nnjlxrzeulgzcba2yxkrqm2nxmlt03vnztchagmbkjsy3vwkwp43_ir1wnggjmv8pey87pe8seegigabljola96o_gpf29i0fhjjdoagzokh.ezuseugjket3c93gmatcbxz9h_p7p5bbyy_bitgi04pdgaynnq.1ki3mrkoxv_fnsn4ih9bctp.5zge_tyqd.jpvmbasyakd0y%26ctu%3dhttp%25253a%25252f%25252fclicktrack.pubmatic.com%25252fadserver%25252faddisplaytrackerservlet%25253fclickdata%25253djnb1yklkpte1njixmizzaxrlswq9mtg2njyxjmfkswq9mta0ntc4nczrywrzaxplawq9nyz0bgrjzd0yntezmdg0nszjyw1wywlnbklkpte2ntmwjmnyzwf0axzlswq9mczhzfnlcnzlcklkpti0myzpbxbpzd0wotjbmuq5oc0xrdvcltqxnkitodiwos1bodhemuy5qurfqkemcgfzc2jhy2s9ma%25253d%25253d_url%25253d&dt1=0&dt2=0&dt4=2&dt5=1&dt6=0&dt7=0&dt8=13&dt9=na&dt10=no%3a32%7cblogger%7ccarbonmade%7camazon.com%7cdisqus%7cbattle.net%7cairbnb%7cfacebook%7cgmail%7cbitbucket%7cedx%7cgoogle_plus%7cexpedia%7cfoursquare%7cgithub%7cflickr%7ckhan_academy%7cmedium%7cmeetup%7cindeed%7chackernews%7creddit%7cpaypal%7cstack%7cvk%7cpinterest%7cyoutube%7ctumblr%7ctwitter%7cskype%7csteam%7cspotify%7csquare%3bok%3a1%7cdropbox%3berr%3a2%7cacademia.edu%7c500px&dt11=1, time: 25000}through a quick search, i was able to find a few other users suffering from the same issue, and it seems like it's been going on for months.how can i figure out who is serving this ad, and where and how to report it? the visual presentation of the ad does not contain the adsense logo or any buttons or links to report it, so i assume it is from some other provider.",
    "present_kp": [
      "ads"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "validator class for php. i've made this validation class in php that would take inputs, on initialization sanitize them and validate them specific to the type. it works fine, but i would like to know if there's some bad practice involved and how i can improve it.i would also love to hear if there's anything that makes the code awful or hard to read. does it follow good coding standards? if not, how can i improve it?class validate { //take the user input into the data put it to a class. private $data; //function argument private $err; // boolean public $empty; private $errmsg; public function __construct($data) { //not sure about this one. have to check it out later and decide //perform some basic level validation like checking for emptiness. foreach ($data as $k =>$v) { $data[$k]= $v; if($this->checkempty($v)==1) { $this->empty=true; } } } private function checkempty($data) { if($data==|| empty($data) || !isset($data)) { return 1; } } private function basicsanitize($data) { //basic level input santization to be used by other functions only. $data=htmlspecialchars($data); $data=trim($data); $data=stripslashes($data); return $data; } public function sanitize($data) { // sanitization at a massive level to sanitize a lot of inputs at one go. foreach ($data as $k =>$v) { $this->data[$k]= $this->basicsanitize($v); } return $this->data; } public function name($data) { if(strlen($data)<3) { $this->errmsg = name too short.; } else if(strlen($data)>100) { $this->errmsg = name too big.; } if(isset($this->errmsg)) { return $this->errmsg; } else return true; } public function email($data) { if (!filter_var($email, filter_validate_email)) { $this->errmsg = invalid email format; } if(isset($this->errmsg)) { return $this->errmsg; } else return true; }}and here is calling the class: $name=j; $email=<email> $input= array ( 'name'=>$name, 'email'=>$email ); $val=new validate($input); if($val->empty==false) { $clean=$val->sanitize($input); $nameerr = $val->name($clean['name']); if($nameerr) { echo $nameerr; } else echo $clean['name'];should i be calling the error messages like that? what could be a more systematic and better way to do that?",
    "present_kp": [
      "php",
      "validation"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "np-complete problem with polynomially many certificates?. let's call a language $l \\in$ np sparsely certificated if and only if:there exists a polynomial $p : \\mathbb{n} ightarrow \\mathbb{n}$ such that for every input $x \\in \\sigma^*$ of size $n$, if $x \\in l$ then the set $u_x$ of certificates $u$ which verify that $x \\in l$ is polynomially sized, i.e. $|u_x| \\leq p(n)$.in shorter terms, every input $x$ has at a most polynomial amount of certificates which verify its inclusion in $l$.example: to illustrate, consider the $\\mathbf{clique}$ problem:$\\mathbf{clique} = \\{\\; (g,k) \\;\\mid\\; g ext{ has a clique of size } k \\;\\}$the language $\\mathbf{clique}$ is not sparsely certificated, as an input $x = (g,k)$ could easily have an exponential amount of $k$-cliques acting as certificates which prove that $x \\in \\mathbf{clique}$.end examplethe question, then, is: are there any known np-complete sparsely certificated languages? any insights are welcome, even if they don't answer the question!note: this definition is different from that of a sparse language!",
    "present_kp": [],
    "absent_kp": [
      "cc.complexity theory",
      "complexity classes"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "nested google maps listener. this nests google maps event listener for click or drag. how should i refactor the last part where it reuses the function getaddresscomponents()? if there are other parts to be refactored, please do offer suggestions. function getaddresscomponents() { geocoder.geocode({'latlng': marker.getposition()}, function(results, status) { if (status == google.maps.geocoderstatus.ok) { clearvalue(); if (results[0]) { // get address_components for (var i = 0; i < results[0].address_components.length; i++) { var addr = results[0].address_components[i]; if (addr.types[0] == 'street_address') $('#spot_street_address').val(addr.long_name); } } else { alert('no results found. please revise the address or adjust the map marker.'); } } }); } // add drag listener to marker for reverse geocoding google.maps.event.addlistener(marker, 'drag', function() { getaddresscomponents(); }); // add click listener to marker for reverse geocoding google.maps.event.addlistener(map, 'click', function(event) { marker.setmap(null); addmarker(event.latlng); getaddresscomponents(); google.maps.event.addlistener(marker, 'drag', function() { getaddresscomponents(); }); });the map will have existing marker, which is draggable and geocodable. but once i click on the map to create a marker (thus, removing the existing marker), that new marker as i drag it, can't be geocodable. that's why i have to nest the listener to get it working. what have i done wrong?",
    "present_kp": [
      "google maps"
    ],
    "absent_kp": [
      "javascript"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "handling data returned from fetch_assoc(). i connected to the db, pull the data using fetch_assoc() (fetch_all(numb) is not available on the machine we are working with else this would be less of an issue). i get the returned data and load it into an array. however, it ends up in as an numeric array embedded inside another assoc array. what is the best way to have get the returned data in one, single, simple assoc array?/*** @param $conn object [required], $tablename string [required]* @return $clmnames array*/public function getcolumnname($conn = null, $tablename = null ){ if ( !is_object( $conn ) || !is_string( $tablename ) ) { //params are not of correct type. this will eventually throw an error instead of printing to screen echo(error: getcolumnname returning false); return false; } //create sql query $stmt = select column_name from information_schema.columns where table_name = '. $tablename .' order by ordinal_position; //create query $query = $conn->query($stmt); //execute query $tmp = array(); while ($row = $query->fetch_assoc()) { //load all returned rows into an array $tmp[] = $row; } //clear memory $query->free();unset($query); //recreate array as a single assoc array instead of a double enbedded one. $clmnames = array(); foreach( $tmp as $tmpkey=>$tmpvalue ) { foreach( $tmpvalue as $tmpvaluekey=>$tmpvaluevalue ) { $clmnames[] = $tmpvaluevalue; } } return $clmnames;}the above returns the data correctly. however, i would like to be able to remove the entire foreach dealing with $tmp and $clmnames if possible.",
    "present_kp": [
      "array"
    ],
    "absent_kp": [
      "php",
      "mysqli"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "proving greedy algorithm is optimal for a scheduling problem. first, the problem discription:for a sequence of $4n$ tasks, $a_1a_2\\dots a_{4n}$, where $a_i\\in\\{0,1\\} orall i$, put them sequentially to the tail of one of the two initially empty queues of length $0$ such that in the end all the tasks are put in the queues and both queues has length $2n$. let $cost$ be 0 at first. for each task $a_i$ that has value $1$, we add its index in the queue to the $cost$.the objective is to minimize the $cost$.example, the sequence of tasks is $<phone>$, and we put the $1$th task in queue $1$, the $2$th task in queue $1$, the $3$th task in queue $2$, the $4$th task in queue $1$, the $5$th task in queue $2$, the $6$th task in queue $1$, $7$th task in queue $2$, the $8$th task in queue $2$, then queue $1$ will has task sequence $1000$ and queue $2$ will has task sequence $1110$, so the $cost$ is $(1)+(1+2+3)=7$.then we describe the greedy algorithm with parameter $0\\le k\\le 2n$it first puts the first $k$ tasks sequentially to the end of queue $1$. then for $i=k+1,\\dots$, if $a_i=1$ it puts the $i$th task into a queue with smaller length, else it puts the $i$th task into a queue with bigger length until one of the queues is full(it has length $2n$). finally it puts the rest of tasks sequentially to the end of the non-full queue.the question is proving the best greedy algorithm(over choices of $k$) gives the optimal cost for any $4n$ length sequence. i have many simulation results that support this statement and haven't found a proof yet.",
    "present_kp": [
      "scheduling"
    ],
    "absent_kp": [
      "ds.algorithms",
      "optimization",
      "lower bounds",
      "proofs"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "combinator logic and unification. summary: if we are trying to use combinator logic to solve first-order logic type problems, is the best method to feed in free variables and use the standard first-order unification algorithm?in standard first-order logic, consider the problem of deducing that the following is a contradictionp(x)~p(5)by the rule that we can substitute specific for general, we can derivep(5)~p(5)and q & ~q => false.the particular choice of the constant 5 to substitute for x is obtained using the standard unification algorithm, which in the case of first-order logic is computable (and indeed quite efficient).translating this into combinator logic givesp = k true~(p 5)substituting the value of p gives~(k true 5)evaluating gives~truewhich is false, qed.but it doesn't look so good when we try a more complex expressionp(f(f(x)))~p(f(f(5)))in standard first-order logic, unification works just as well here as for the simpler case. the translation to cl, however, iss (k p) (s (k f) f) = k true~(p (f (f 5)))this isn't a problem we can solve with simple substitution. one approach we could try is to feed both sides of an equation the same value; if we take the number 5 as a value, the left-hand side of the equation evaluates ass (k p) (s (k f) f) 5k p 5 (s (k f) f 5)p (s (k f) f 5)p (s (k f) f 5)p (k f 5 (f 5))p (f (f 5))so we havep (f (f 5)) = k true 5p (f (f 5)) = truesubstituting into the second expression from the original problem gives~trueso we have solved the problem, but we did it by guessing the correct value to feed both sides of the equation, by inspection. is there a general method that could be used?one that suggests itself is to feed in a variable, which would givep (f (f x)) = truefrom which the standard first-order unification algorithm could be used.have we just established that the only way to solve problems in cl is to convert them back into standard logic? perhaps it's not that bad; at least we have eliminated lambda and the other quantifiers, and their associated bound variables, leaving only free variables.still, is there a better method that i'm missing?(i'm omitting consideration of types here; because combinator logic is turing complete, presumably type checking will also be needed to avoid the paradoxes of the sort that make simple untyped lambda calculus unsound.)",
    "present_kp": [],
    "absent_kp": [
      "lo.logic"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "want to do some damage control. i applied for a job in a company. the initial conversation went well.they sent me an assignment and asked me to write a web application, and send it in a specified time. it seemed fairly simple. i completed and sent it to them. now my mind is completely revolving on the code that i wrote, and now i feel that, there was one small requirement which i did not fulfill. before they give the verdict shall i be proactive and admit it. definitely they will find it and i am certain of it. i can send them email saying that i forgot to add that.what shall i do? is there anything i do for damage control?",
    "present_kp": [],
    "absent_kp": [
      "interview"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "mariadb client has no prompt in emacs sql-mode. i just upgraded from an old mysql client to mariadb-clients-10.0.21-3 on arch linux. after the upgrade, i no longer see a prompt when using emacs's sql-mysql function.it seems mysql is buffering the prompt, because it shows up in the first line of output:reading table information for completion of table and column namesyou can turn off this feature to get a quicker startup with -awelcome to the mariadb monitor. commands end with ; or \\g.your mysql connection id is 19662server version: 4.1.11-standard-logcopyright (c) 2000, 2015, oracle, mariadb corporation ab and others.type 'help;' or '\\h' for help. type '\\c' to clear the current input statement.show tables;mysql [dbname]> +---------------------------------------------------------+| tables_in_dbname |+---------------------------------------------------------+...+---------------------------------------------------------+80 rows in set (0.02 sec)helpmysql [dbname]> general information about mariadb can be found athttp://mariadb.orglist of all mysql commands:...for server side help, type 'help contents'?mysql [dbname]> general information about mariadb can be found athttp://mariadb.orglist of all mysql commands:...for server side help, type 'help contents'exitmysql [dbname]> byein all cases, the line before mysql [dbname]> is what i typed. (... indicates output i omitted.)how can i get the prompt to display properly? i've tried the -n option of mysql; it had no effect. if i run mysql in a terminal, it works fine.",
    "present_kp": [
      "emacs",
      "mysql",
      "mariadb"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "what are the fundamental principles/algorithms on the process of equation solving?. i have seen a lot of solvers that are capable of, for example, getting an equation such as x ^ 2 + x = 12 and finding x = [3, -4]. i know some of them are implemented by hardcoding special cases. for example, when finding an equation on the quadratic form, they use a hardcoded bhaskara formula.what i'm looking for is something different, i want to learn more about the nature of solving equations. that is, i want simple algorithms or systems that generalize the process of solving an equation, in the same principle that lambda calculus encapsules the process of computation.what are the names i'm looking for?",
    "present_kp": [],
    "absent_kp": [
      "computability",
      "turing machines",
      "reductions",
      "logic",
      "proof techniques"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "using resttemplate with httpcomponentsclienthttprequestfactory in a multithreaded environment. i am working on a project in which i need to make an http url call to my server, running restful service, which returns the response as a json string. i am using resttemplate here along with httpcomponentsclienthttprequestfactory to execute a url.i have set up an http request timeout (read and connection time out) on my resttemplate by using httpcomponentsclienthttprequestfactory.here is my main code which is using the future and callables:public class timeoutthreadexample { private executorservice executor = executors.newfixedthreadpool(10); // does it have to be static final? private static final resttemplate resttemplate = createresttemplate(); // does this look right with the way i am creating 'resttemplate'? private static resttemplate createresttemplate(){ // is it ok to create a new instance of httpcomponentsclienthttprequestfactory everytime? httpcomponentsclienthttprequestfactory requestfactory = new httpcomponentsclienthttprequestfactory(); requestfactory.setreadtimeout(read_time_out); requestfactory.setconnecttimeout(connection_time_out); return new resttemplate(requestfactory); } public string getdata() { future<string> future = executor.submit(new task(resttemplate)); string response = null; try { response = future.get(500, timeunit.milliseconds); } catch (timeoutexception e) { e.printstacktrace(); } catch (interruptedexception e) { e.printstacktrace(); } catch (executionexception e) { e.printstacktrace(); } return response; }}here is my task class which implements the callable interface and uses the resttemplate:class task implements callable<string> { private resttemplate resttemplate; public task(resttemplate resttemplate) { this.resttemplate = resttemplate; } public string call() throws exception { string url = some_url; string response = resttemplate.getforobject(url, string.class); return response; }}now i have everything working in my above code. is the way i am creating resttemplate along with httpcomponentsclienthttprequestfactory thread-safe and efficient? since resttemplate is very heavy to be created so not sure whether i have it right or not. i'd like to improve anything in creation of resttemplate with httpcomponentsclienthttprequestfactory.",
    "present_kp": [
      "rest"
    ],
    "absent_kp": [
      "java",
      "performance",
      "multithreading"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "untar without top-level directory. i have a tar (websites.tgz) that contain a bunch of drupal websites that are tarred up starting with 'htdocs'i need to untar them into/local/htdocs/web1/local/htdocs/web2and so on. but i cannot place the websites.tgz file in /local and extract downward due to permissions. it is currently in my home directory. how can i untar the contents under /local/htdocs, without including the top-level htdocs directory?i am trying to avoid having:/local/htdocs/htdocs/web1/local/htdocs/htdocs/web2.thanks for any help.",
    "present_kp": [
      "tar"
    ],
    "absent_kp": [
      "command line"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to download lots of pictures sent via google drive in gmail. i got sent a few hundred pics from gmail to gmail, but on their end they were sent using google drive, so they are just links, and there's not a download all link. and i can't access the parent folder to get to them all that way either. what can i do? how can i download all pictures at once?",
    "present_kp": [
      "gmail",
      "google drive",
      "pictures"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "issues mounting a ntfs share on centos 6.3. we have a web server running on centos 6.3 and we intend to mount a shared windows drive (windows 2008) so that we can do our daily backups on to the shared drive. this is how i am mounting the drive at the moment.mount.cifs //172.27.0.190/moodle$ /srv/www/moodledata \\ -o username=lhu/moodledata,password=<password> \\ -o rw,dir_mode=0777,file_mode=0777,uid=root,gid=rootthis allows me to mount the drive and i am able to create new directories and files. however i am unable to delete anything, move or replace any files or folders. this is problematic since we use a script to automatically backup our database and we have a weekly rotation schedule which at the moment cannot delete files, so the shared drive is filling up with database copies since the script cannot delete obsolete copies. further more, we rsync the data from a folder on the server to a folder on the shared drive. this doesn't work either. e.g.rm: cannot remove 'weekly/mysql/weekly_mysql_2012-10-26_13h15m_43.sql.gz': permission deniedthe user info is correct and has full control on the folder in windows 2008.i would be very grateful for any advice to fix this issue.",
    "present_kp": [
      "centos",
      "mount",
      "windows"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "simple blackjack game in python 3.4. i made a simple blackjack game in python 3.4.3. i'm a beginner, so it would be awesome if you could rip up my code and tell me everything i did wrong. i know there's no insurance for when the dealer has an ace (yet), and there's some other blackjack rules that i'm going to add in the future, but i wanted to get a base code out there.import random as rimport itertools as isuit = 'scdh'rank = '23456789tjqka'deck = tuple(''.join(card) for card in i.product(rank, suit))val = ()for _ in range(9): val = val + (_+2, _+2, _+2, _+2) if _ == 8: for __ in range(3): val = val + (10, 10, 10, 10)val = val + (1, 1, 1, 1)deckval = dict(zip(deck, val))def deal(): global hand, dealer_hand, player_hand, counter hand = r.sample(deck, 52) counter = 0 dealer_hand = list(hand[counter:counter + 1]) counter += 2 player_hand = list(hand[counter:counter + 2]) counter += 2def sum_player_hand(): global hand, player_hand, counter, player_sum, opt_player_sum player_sum = 0 opt_player_sum = 0 for a in range(len(player_hand)): if int(deckval[player_hand[a]]) == 1 and opt_player_sum + int(deckval[player_hand[a]]) <= 21: opt_player_sum = player_sum + int(deckval[player_hand[a]]) + 10 player_sum += int(deckval[player_hand[a]]) elif opt_player_sum > 21: player_sum += int(deckval[player_hand[a]]) opt_player_sum = player_sum else: player_sum += int(deckval[player_hand[a]]) opt_player_sum += int(deckval[player_hand[a]])def dealer_init(): global hand, dealer_hand, counter, dealer_sum, opt_dealer_sum dealer_sum = 0 opt_dealer_sum = 0 if int(deckval[dealer_hand[0]]) == 1: dealer_sum += int(deckval[dealer_hand[0]]) opt_dealer_sum += dealer_sum + 10 else: dealer_sum = int(deckval[dealer_hand[0]]) opt_dealer_sum = int(deckval[dealer_hand[0]]) dealer_logic()def dealer_logic(): global hand, dealer_hand, counter, dealer_sum, opt_dealer_sum if dealer_sum >= 17 or opt_dealer_sum >= 17: pass else: while opt_dealer_sum <= 16: dealer_sum = 0 opt_dealer_sum = 0 dealer_hand = dealer_hand + list(hand[counter:counter + 1]) counter += 1 for _ in range(len(dealer_hand)): if int(deckval[dealer_hand[_]]) == 1 and (opt_dealer_sum + int(deckval[dealer_hand[_]])) <= 21: opt_dealer_sum += int(deckval[dealer_hand[_]]) dealer_sum += int(deckval[dealer_hand[_]]) else: dealer_sum += int(deckval[dealer_hand[_]]) opt_dealer_sum += int(deckval[dealer_hand[_]])def main(): global hand, dealer_hand, player_hand, counter, player_sum, dealer_sum, opt_player_sum, opt_dealer_sum sum_player_hand() print(' dealer has:', dealer_hand[0:2], '--') if player_sum <= 21: if opt_player_sum == player_sum or opt_player_sum > 21: print('your hand is:', player_hand, ' ', 'your sum is:', player_sum) else: print('your hand is:', player_hand, ' ', 'your sum is:', player_sum, 'or', opt_player_sum) choice = input('hit or stay? ').lower() if choice == 'hit': player_hand = player_hand + list(hand[counter:counter + 1]) counter += 1 main() elif choice == 'stay': print('') if opt_player_sum <= 21: print('final hand: ', player_hand, 'final sum:', opt_player_sum) dealer_init() if opt_dealer_sum <= 21: print('dealer has:', dealer_hand, 'sum:', opt_dealer_sum) if 21 >= opt_dealer_sum > opt_player_sum: print('dealer wins') else: print('you win') run() else: print('dealer has:', dealer_hand, 'sum:', dealer_sum) if 21 >= dealer_sum > opt_player_sum: print('dealer wins') else: print('you win') run() else: print('final hand: ', player_hand, ' ', 'final sum:', player_sum) dealer_init() if opt_dealer_sum <= 21: print('dealer has:', dealer_hand, 'sum:', opt_dealer_sum) if 21 >= opt_dealer_sum > player_sum: print('dealer wins') else: print('you win') run() else: print('dealer has:', dealer_hand, 'sum:', dealer_sum) if 21 >= dealer_sum > player_sum: print('dealer wins') else: print('you win') run() else: print('') print('***please enter hit or stay***') main() else: print('bust your hand was:', player_hand, ' your sum was:', player_sum, ' ') dealer_init() if opt_dealer_sum < 21: print('dealer has:', dealer_hand, 'sum:', opt_dealer_sum) if dealer_sum > 21: print('dealer busts') run() else: print('dealer has:', dealer_hand, 'sum:', dealer_sum) if dealer_sum > 21: print('dealer busts') run()def run(): play = input('******************** would you like to play again?').lower() if play == 'yes': deal() main() elif play == 'no': pass else: print('please enter yes or no') run()deal()main()run()",
    "present_kp": [
      "python",
      "beginner"
    ],
    "absent_kp": [
      "python 3.x",
      "playing cards"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "discrete math/logic problem used in computer science class: who robbed the national bank?. the following question is supposed to give you insight on how to maintain logic when making programs. please be open minded as this may not look like a computer science problem, but it definitely contains logic that you will use when making if statements and booleans, so please keep that in mind.who robbed the national bank?inspector malone knows that the culprit is one and only one of the following: alex, john, or sally. he interrogates them. each makes two statements.alex: it wasn't me. john did it.john: listen, alex did it.and sally did it.sally: i didn't do it. neither did alex.each has made one true statement and one false statement. who did it?",
    "present_kp": [],
    "absent_kp": [
      "discrete mathematics"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "web desktop environment. i have a mad idea, i want to make my own linux distro (maybe on *buntu system) with a de which i could develop with html5, css and js. to start, i just want to make my de a web page (i don't talk about a thin client).did you understand? if not, tell me, i really need help from professionals'.",
    "present_kp": [
      "linux",
      "desktop environment",
      "html",
      "web"
    ],
    "absent_kp": [
      "ubuntu"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how can i save a session in a directory owned by root?. i quite often open linux system configuration files with vim and sometimes have a session with lots of tabs, splits, quicklists etc that i want to save to a session and restore again later.the problem is i usually only open as a non-root user with read-only access and therefore my vim process doesn't have privileges to save the session.vim file in the current directory (i guess it may be able to save to home - but my preference is to save the session file to the site where i'm working).is it possible to elevate to sudo privileges inside vim simply for the :mksession command?",
    "present_kp": [],
    "absent_kp": [
      "sessions"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what's up with the '- ' in google search result. i recently did a google search of an article i wrote and this is what i see:if i am not wrong the string behind - should be the title of the original website, which is never v1.0.actually, nowhere in my website did i indicate that v1.0 is the title or anything significant. <title>0a explains: calculus (with pics and gifs)</title><meta name=viewport content=width=device-width, initial-scale=1.0><meta name=keywords content=0a, calculus, differentiation, integration><meta name=author content=archy wilhes><meta name=description content=calculus explained with pics and gifs. written in possibly (hopefully) the most comprehensible way for anyone who is new to limit, differentiation and integration. >it just happened to be the first string of characters in the website besides my logo which is a png.<div class=logo inline><img src=assets/img/0a.png><div class=version>v1.0</div></div>so is there a way to change it to something else? all i can think of is doing a bit of hack like putting a display-none string in front of v.10.",
    "present_kp": [
      "google",
      "google search",
      "title"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "unbind default key from inputrc. what i want to do is made c-h not send backwards-delete-charthere is nothing about it in my inputrc files but is shows up in bind -p",
    "present_kp": [
      "inputrc"
    ],
    "absent_kp": [
      "bash",
      "keyboard shortcuts",
      "readline"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to copy title and url in bookmarklet. this links tell me how to create a bookmarklet for copying a tabs title. javascript:var%20title=document.title;if(title){var%20re=/(\\|/|:|*|?|%22|<|>||)/gi;title=title.replace(re,'');void(prompt('page%20title',%20title));}i want to copy both the url and the title in the bookmarklethow can i do it? i don't want an add-on.",
    "present_kp": [
      "bookmarklet"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "setting (system-wide) cpu affinities for running processes on a linux platform. i am conducting a kind of research in that i schedule multiple parallel applications (e.g., openmp/pthreaded applications) and execute the applications on specific (partitioned) cores on linux-based multi-processor platforms.we can set cpu affinities for each application by using sched_setaffinity() system call. but, as you know, linux manages (all) running programs as well. so, the applications' executions that i scheduled are sometimes interrupted by other processes that linux scheduled.i want to set all processes and daemons (except for applications that i scheduled) to cpu 0. my first thought was to set cpu 0 manually by traversing all tasks from init task in a kernel module. but the result will be affected by linux load-balancing. we need another way to somehow turn off or manage linux cpu load balancing.is there any possible way or system configurations to do this? my target platform is amd opteron server (containing 64 cores) and linux version is 3.19.",
    "present_kp": [
      "linux",
      "kernel",
      "load balancing"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "linux [admin] how to test pivot_root?. i want to test the pivot_root command line, which moves the root file system of the current process to the directory put_old and makes new_root the new root file system. <url> i always get pivot_root: failed to change root from '.' to 'old-root/': invalid argumenti use fedora as base root, i have a archlinux in my home folder[root@localhost arch-root]# lsbin boot dev etc home lib lib64 mnt old-root opt proc root run sbin srv sys tmp usr var[root@localhost arch-root]# pivot_root . old-root/ pivot_root: failed to change root from '.' to 'old-root/': invalid argumenti also try to call linux function pivot_root(/chroot_test, /chroot_test/old-root);got same error.any idea about this ?update 1:i also try to test pivot_root in docker.i mount this arch-root in to docker container. but get the following error: operation not permittedroot@00d871ce892b:/# cd test_root/root@00d871ce892b:/test_root# lsbin boot dev etc home lib lib64 mnt old-root opt proc root run sbin srv sys test_pivot_root test_pivot_root.c tmp usr varroot@00d871ce892b:/test_root# pivot_root . tmp/pivot_root: operation not permittedi have found the solution:run docker with --privileged=true so we can test pivot_root in the docker container",
    "present_kp": [
      "c",
      "linux"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "information technology -> specialization of computer science?. my friend said to me it is a specialization of computer science. i said it isn't. is he correct?can someone clarify the difference between it and cs? i know they are related but according to me calling it a specialization of cs doesn't make sense.",
    "present_kp": [
      "computer science",
      "information"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "kill process instead of panic on hung_task. there are sysctl parameters for hung_task* that allow you to emit error messages or panic if a process is in the d state for some amount of time.is there anyway way to have the kernel kill the process rather than panic? i'd like the same detection mechanism (process in d state for too long) but i just want to kill the process rather than have the machine reboot.any ideas?thanks!",
    "present_kp": [
      "sysctl"
    ],
    "absent_kp": [
      "linux",
      "process management"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "creating sql queries for a mysql database. i am writing a php application that works with a mysql database. as i am fairly new to php i am looking for ways to improve my code. below are two functions, create_ct_query and create_ct_query2. create_ct_query2 is the result of my refactoring create_ct_query. i am fairly happy with it now, but am wondering how it looks to others.<?php// experiment with versions of create_ct_query$lvb_id_ml=30; // lvb id$lvb_name_ml=100; // competition name$lvb_competitions_table = array (table_name => lvb_competitions, table_cols => array (array ('id', $lvb_id_ml), array ('name', $lvb_name_ml), array ('age', 'int')), primary_key => array ('id', 'age'));function varchar ($len) { return varchar( . $len . );}function primary_key ($item) { return $item . primary key;}// version currently in scriptfunction create_ct_query ($tdesc) { $ret_val = create table . $tdesc['table_name'] . ( ; foreach ($tdesc[table_cols] as $row) { if (is_numeric ($row[1])) { $ret_val .= $row[0] . . varchar ($row[1]) . , ; } else { $ret_val .= $row[0] . . $row[1] . , ; } } // add primary key constraint $ret_val .= constraint pk_ . $tdesc[table_name] . primary key (; if (is_array ($tdesc['primary_key'])) { $ret_val .= implode (,, $tdesc['primary_key']); } elseif (is_string ($tdesc['primary_key'])) { $ret_val .= $tdesc[primary_key]; } else { die (primary key not of right type in tdesc for . $tdesc['table_name'] . ); } $ret_val .= )); return $ret_val;}function create_ct_query2 ($tdesc) { $ret_val = create table . $tdesc['table_name'] . (; $ret_val .= cols_desc ($tdesc['table_cols']) . , ; $ret_val .= primary_key_constraint ($tdesc['table_name'], $tdesc['primary_key']) . )); return $ret_val;}// create the item description from the item// example:// item_desc (array ('name', 10));// 'name varchar(10)'// item_desc (array ('name', int));// 'name int'function item_desc ($item) { return . $item[0] . . (is_numeric($item[1]) ? varchar($item[1]) : $item[1]);}// create the column description part of table creation query// example:// cols_desc (array (array ('id', 10), array ('id2', int)));// 'id varchar(10), id2 int'function cols_desc ($table_cols) { return implode (,, array_map ('item_desc', $table_cols));}// create the primary key constraint part of the table creation query// primary_key_constraint ('name', 'id');// constriant pk_name primary key (id)// primary_key_constraint ('name', array ('id', 'id2'));// constraint pk_name primary_key (id, id2)function primary_key_constraint ($table_name, $primary_key_desc) { return constraint pk_ . $table_name . primary key ( . (is_array($primary_key_desc) ? implode (,, $primary_key_desc) : $primary_key_desc);}echo create_ct_query ($lvb_competitions_table) . ;echo create_ct_query2 ($lvb_competitions_table) . ;",
    "present_kp": [
      "php",
      "sql",
      "mysql"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "symbolic link with option. is it possible to create a symbolic link to an executable that executes it with a certain option/argument?i know a workaround would be to create a bash script in one of the path directories but can i achieve it somehow with a symbolic link?edit: thanks for the answers, in my case an alias wouldn't do the job because i'm looking for a way to start matlab from dmenu and at least on arch matlab is only invokable from a terminal at first. since dmenu does not consider aliases it wouldn't work .. i should have made my problem more clear.",
    "present_kp": [],
    "absent_kp": [
      "symlink",
      "options",
      "ln"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "procmail: denying special privileges for /etc/procmailrcs/default.rc. i'm trying to setup postfix, dovecot and procmail to work together with virtual users. in the end i want to have virtual users and the possibility to add rules to sort incoming rules. for the last thing, i need procmail (right?).when i send an email to my server, i don't get it in my maildir, and see this in mail.log:jun 17 21:01:03 cs postfix/smtpd[24811]: connect from dub0-omc2-s13.dub0.hotmail.com[157.55.1.152]jun 17 21:01:03 cs postfix/smtpd[24811]: d8c9f44d88: client=dub0-omc2-s13.dub0.hotmail.com[157.55.1.152]jun 17 21:01:03 cs postfix/cleanup[24816]: d8c9f44d88: message-id=<<email> 17 21:01:04 cs postfix/qmgr[24806]: d8c9f44d88: from=<my-test-email>, size=1617, nrcpt=1 (queue active)jun 17 21:01:04 cs procmail[24818]: denying special privileges for /etc/procmailrcs/default.rcjun 17 21:01:04 cs postfix/smtpd[24811]: disconnect from dub0-omc2-s13.dub0.hotmail.com[157.55.1.152]jun 17 21:01:04 cs postfix/pipe[24817]: d8c9f44d88: to=<my-virtual-email>, relay=virtualprocmail, delay=0.18, delays=0.15/0/0/0.02, dsn=2.0.0, status=sent (delivered via virtualprocmail service)jun 17 21:01:04 cs postfix/qmgr[24806]: d8c9f44d88: removedhow can i fix the line denying special privileges procmail spits out?camilstaps@cs:/# ls -al /etc/procmailrcstotal 12drwxr-xr-x 2 root vmail 4096 jun 17 19:48 .drwxr-xr-x 97 root root 4096 jun 17 19:47 ..-rw------- 1 vmail postfix 44 jun 17 19:48 default.rchere's my /etc/postfix/master.cf:smtp inet n - - - - smtpdsubmission inet n - n - - smtpdpickup unix n - - 60 1 pickupcleanup unix n - - - 0 cleanupqmgr unix n - n 300 1 qmgrtlsmgr unix - - - 1000? 1 tlsmgrrewrite unix - - - - - trivial-rewritebounce unix - - - - 0 bouncedefer unix - - - - 0 bouncetrace unix - - - - 0 bounceverify unix - - - - 1 verifyflush unix n - - 1000? 0 flushproxymap unix - - n - - proxymapproxywrite unix - - n - 1 proxymapsmtp unix - - - - - smtprelay unix - - - - - smtpshowq unix n - - - - showqerror unix - - - - - errorretry unix - - - - - errordiscard unix - - - - - discardlocal unix - n n - - localvirtual unix - n n - - virtuallmtp unix - - - - - lmtpanvil unix - - - - 1 anvilscache unix - - - - 1 scachemaildrop unix - n n - - pipe flags=drhu user=vmail argv=/usr/bin/maildrop -d ${recipient}uucp unix - n n - - pipe flags=fqhu user=uucp argv=uux -r -n -z -a$sender - $nexthop!rmail ($recipient)ifmail unix - n n - - pipe flags=f user=ftn argv=/usr/lib/ifmail/ifmail -r $nexthop ($recipient)bsmtp unix - n n - - pipe flags=fq. user=bsmtp argv=/usr/lib/bsmtp/bsmtp -t$nexthop -f$sender $recipientscalemail-backend unix - n n - 2 pipe flags=r user=scalemail argv=/usr/lib/scalemail/bin/scalemail-store ${nexthop} ${user} ${extension}virtualprocmail unix - n n - - pipe flags=drxhuq user=vmail argv=/usr/bin/procmail -m e_sender=$sender e_recipient=$recipient er_user=$user er_domain=$domain er_detail=$extension nexthop=$nexthop /etc/procmailrcs/default.rcmailman unix - n n - - pipe flags=fr user=list argv=/usr/lib/mailman/bin/postfix-to-mailman.py ${nexthop} ${user}i'm on ubuntu server 13.04.",
    "present_kp": [
      "ubuntu",
      "email",
      "postfix",
      "procmail"
    ],
    "absent_kp": [
      "permissions"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "domain objects for reading vs saving. suppose an object that has like 100 properties, but to create a new object only a few pieces of information are needed.we are using wcf, and i see the following options:1) use the same domain object for both saving and reading the data. downside is that it may not be immediately intuitive what properties need to be populated in order to create a new object.2) create my wcf service call such that i just pass in each piece of information needed as a separate parameter. but what if there were 15 parameters required instead of just 3... or what if the object changes and then it's a hassle to have to update the wcf method signature.3) create a separate object with only the required properties for saving.i can see that maybe any of these 3 could be a suitable answer depending on the specific scenario. thoughts?",
    "present_kp": [],
    "absent_kp": [
      "domain driven design",
      "persistence"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to let users specify their own logic conditions?. i'm working on a jee project, here is the situation:i have an application that puts the maker/checker financial concept. the maker chooses an operation and can modify its values but the checker should approve these changes so they can be registered the problem is that some operation have certain conditions set that obliges to choose a certain checker these conditions can change depending on the maker. how can i take into consideration in my java code these conditions without knowing anything about them only that they're an input into the application?put simply how to code conditions made by an outsider user?example:operation: credit condition:if(card.equals(normal)) credit = 500 ;the maker may want to add another condition saying that the credit = 500 when we have a normal card and the user is under 20 years old. so the maker is changing the condition but he hasn't the access to the application however the application should take into consideration this change, does anyone have any idea how to implement this ?",
    "present_kp": [
      "java",
      "jee"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "fb hw conflict: nouveau vs vesa vga. i would like to start off by saying that i am aware that there are many similar questions, however none have an answer that has solved my problem. anyways, here's the problem: i have an hp compaq 8200 elite sff running windows 7 x64 with an inbuilt intel processor - which has its own graphics - and i have a separate nvidia graphics card. i am trying to install backtrack5 r3 via a live usb. however, when i boot into the backtrack live usb and attempt to open the default text option, commands are executed for a few seconds before i get the following error: fb: conflicting fb hw usage nouveau vs vesa vga - removing generic driverafter this, the backtrack live usb/installation is frozen. i cannot do anything, and to proceed i have to force-shutdown my computer and turn it back on. i have tried everything that i have seen on the internet, but to no avail. i have tried, along with their results:passing i915.modeset=1/i915.modeset=0 - i get attached scsi removable media, after which i can type, however what i enter has no effectpassing nomodeset - has no effectdisabling kms/passing nouveau.modeset=0 - has no effectuninstalling nouveau driver/blacklisting nouveau driver/startx - can't get to that pointanything else that involves terminal or commands - again, can't get to that pointreinstalling nvidia drivers from their website - installs the same as any other nvidia driver with no additional options, makes no differenceformatting the live usb and installing it again (with different builds) - makes no differencedisabling intel graphics through bios - bios is password-protected with a password i don't know (planning to reset bios/cmos password through motherboard soon)using a non-deprecated/still-supported os similar to backtrack - i don't want an os like backtrack, i want backtrack itself and nothing elsei am somewhat of a linux noob, and have no other linux operating system installed on my computer. i am otherwise fully prepared for a dual-boot. if any complicated linux procedures are required, please explain each step thoroughly yet simply. below are my system specs:motherboard: hp 611834-001cpu: intel i5-2400 @ 3.1 ghzram: 8gbgpu: geforce gtx 750 ti 2gb i would hate to have to remove/disable my external graphics card for backtrack to work, so any solutions that don't require that would be thoroughly appreciated. if i could remove this driver that's causing problems in windows, even if it means disabling inbuilt intel graphics, that would be optimal. i apologize for the wall of text, and thank you in advance for your help.",
    "present_kp": [
      "drivers",
      "nvidia",
      "backtrack"
    ],
    "absent_kp": [
      "grub2",
      "bootable"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "execute a process in the current pane, redirecting stdio to/from the pane. gnu screen has a command called exec that will launch a subprocess and optionally let you do things with its file descriptors, including tying them in your window. i frequently use :exec !! sx -b /tmp/file to send a file over a serial link using the xmodem protocol.i'm looking for the same functionality in tmux so i can move away from screen without having to use minicom's (or picocom's, etc,) function for sending files.",
    "present_kp": [
      "gnu screen",
      "tmux"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "applying a 1-qubit gate to entangled qubits. given two entangled qubits in the state ${|00+|11\\over oot \\of 2}$, i have trouble getting the way of applying a 1-qubit gate (let's say a hadamard) to 1 of the entangled qubits (let's say the first one). actually i have several questions:following this link, ran g. says that you can apply a quantum gate on any subset of the qubits.on the other, hand agent gotse says here that you can not compute the hadamard gate on only one qubit, as the state is entangled and cannot be factored into two independent qubits.do the answers contradict themself?i understand part of the principle of the entangled pair, it a superposition of bipartite system. but given for example an entangled pair $|00+|11 = |0 \\otimes |0 + |1 \\otimes |1$, and two unitary operators $a\\ b$ to appy to the system.why can't we juste do $a|0 \\otimes b|0 + a|1 \\otimes b|1$?i mean what is the mathematical meaning of that?finally, the superdense coding use an entangled pair, and in the procedure given in the link, alice apply some operator to her qubit without changing bob's one (because it's probably far far away...). quantum computing textbooks say that in the case of an entangled pair, alice has to apply $u \\otimes i$, with $u$ the chosing operator.but alice has only 1 qubit, and clearly $u \\otimes\\ i$ acts on 2 qubits (it's a $4*4$ matrix), how it's works physically?",
    "present_kp": [
      "quantum computing"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "make gtk or kde text fields be vi-like?. is there a way to make all gtk or kde text fields vi-like, akin to what set -o vi in ~/.bashrc does with the bash shell?",
    "present_kp": [
      "kde",
      "vi",
      "gtk"
    ],
    "absent_kp": [
      "vim",
      "vi mode"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "echoing back input in uppercase over a socket with hy, a python lisp dialect. i think i'd like to learn clojure eventually, but at the moment am having fun with hy, a dialect of lisp that's embedded in python. (import socket)(defn echo-upper (sock) (.send c (.upper (.recv c 10000))) (.close c))(def s socket.socket)(.bind s (tuple [ 8000]))(.listen s 5)(while true (echo-upper (.__getitem__ (s.accept) 0)))python generated from abs with astor:import socketdef echo_upper(sock): c.send(c.recv(10000).upper()) return c.close()s = socket.sockets.bind(tuple([u'', 8000]))s.listen(5)while true: echo_upper(s.accept().__getitem__(0))i'm not worried about not dealing with connections properly etc, i'd just like feedback on using lispy syntax. i'm tempted to do things like ((getattr s bind) (tuple [ 8000])) because it looks more like simple scheme, the only lisp i have a little experience with.any thoughts on syntax and style? any guidelines for dealing with python apis in a lispy way, something that apparently happens a lot in clojure with java?",
    "present_kp": [
      "python",
      "lisp",
      "socket",
      "hy"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "adjoint of the matlab $ t dwt3$ (3d wavelet transform) operator. how do i compute the adjoint of matlab's dwt3 operator?in other words, how do i compute the adjoint of the linear operator that takes a 3d complex array x as input and returns the quantity dwt3(x,'db4') as output.this adjoint computation is an important step in many iterative reconstruction algorithms.one might guess that idwt3 is the adjoint of dwt3, but the following code seems to show that this is not the case (unless my code contains an error):sz = [384,300,144];% randomly generate a 3d complex arrayx1 = randn(sz) + 1i*(randn(sz));% compute wavelet transform of x1wx1 = dwt3(x1,'db4');% randomly generate x2 which has the same size and shape as wx1 (which is a matlab structure)x2 = wx1;for i1 = 1:2 for i2 = 1:2 for i3 = 1:2 arrsz = size(x2.dec{i1,i2,i3}); x2.dec{i1,i2,i3} = randn(arrsz) + 1i*randn(arrsz); end endend% compute inverse wavelet transform of w2wtransx2 = idwt3(x2);% check to see if the adjoint property <x1,wtransx2> = <wx1,x2> is satisfiedprod1 = x1(:)'*conj(wtransx2(:));prod2 = 0;for i1 = 1:2 for i2 = 1:2 for i3 = 1:2 wx1_arr = wx1.dec{i1,i2,i3}; x2_arr = x2.dec{i1,i2,i3}; prod2 = prod2 + wx1_arr(:)'*conj(x2_arr(:)); end endendmax(abs(prod1 - prod2))",
    "present_kp": [
      "matlab",
      "wavelet"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how do i change from gnome to i3 debian 8. i have read all over the internet about changing text files or running commands to change my wm but none of them work. i have looked all around the gnome lock screen for a button allowing me to change my wm but i could find none. i have installed and uninstalled i3 multiple times but i cannot get it to run. is anyone able to help?",
    "present_kp": [
      "debian",
      "i3"
    ],
    "absent_kp": [
      "gnome3"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to have function input row/cell automatically update in response to new data?. i apply a function, such as sum(a10:a20) to a set of data.when i give a21 a value i would like sum(a10:a20) to automatically update to sum(a11:a21), following the last x number of cells in a row/column.example:| day 1 | day 2 | day 3 | day 4 || 1 | 1 | 1 | 1 || 2 | 2 | 2 | 2 || 3 | 3 | 3 | 3 || 4 | 4 | 4 | 4 || 5 | 5 | 5 | 5 || | 6 | 6 | 6 || | | 7 | 7 || | | | 8 || 15 | 21 | 27 | 33 |the four days represent the state of a single column during 4 days, as opposed to four separate columns.the last row, is the sum of the last 6 values in the row. so day one it, takes the 5 available rows: =sum(a2:a6), day 2 there's six rows, so all get included: =sum(b2:b7), day 3 there's seven rows, so the oldest one gets skipped: =sum(c3:c8), and the fourth day, there's eight rows, so the oldest two gets skipped: =sum(d4:d9)if i enter a new value on a daily basis, the above requires me to manually adjust the argument to sum daily. i would like to find a way to automate that process.here's a spreadsheet with the same info as above: example",
    "present_kp": [],
    "absent_kp": [
      "google spreadsheets"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "unable to install arch linux on sd card via virtual box. my aim is to create a bootable sd card with arch linux, for my raspberry pi.i have been trying to follow this guide without success.so far i have:a working arch linux virtual box, as my host is osxformatted the sdcard to fat before the processmounted my sdcard to and it is visible by the vm as storagei go to start following the guide and things start going wrong.unsure on how to 'get' at the device that is available i ran /dev/disk/by-label and saw my device @recovery which is the correct label. great.i then executed step 2 of the guide, as my sdcard is formatted. mkfs.vfat /dev/disk/recovery.. the only thing that happened is that my sdcard is no longer visible in /dev/disk/by-label ! my assumption is that i have in some way mounted the device but i am now out of my depth, even after googling for mounted devices/partitions i remain lost.what is my equivalent of /dev/sdx1 and how do i complete step 3 and 4 of the guide?cheers",
    "present_kp": [
      "linux",
      "raspberry pi"
    ],
    "absent_kp": [
      "virtualbox"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "decompressing zimage. i have a firmware that i've dumped from a phone. i found a uimage header and a zimage after running binwalk. the header says that the image is uncompressed, however i've read that zimages are usually compressed with lzma, xz, gzip, etc. does this mean the zimage is uncompressed and i can work with it directly, without decompressing it?352768 0x56200 uimage header, header size: 64 bytes, header crc: 0xfaad7908, created: 2015-09-01 19:12:03, image size: <phone> bytes, data address: 0xc8008000, entry point: 0xc8008000, data crc: 0xb0af08f8, os: linux, cpu: arm, image type: os kernel image, compression type: none, image name: linux-3.4.20-rt31-dvf-v1.2.6.1-r352832 0x56240 linux kernel arm boot executable zimage (little-endian)",
    "present_kp": [
      "firmware",
      "decompress"
    ],
    "absent_kp": [
      "ida"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "retrieve records from the database based on a possible id. this code will retrieve all records from the database if the id is null or return records by the available id. is there a way to combine the query instead of having two separate queries?// retrieve standard orderslet retrievestdorders id = if id = null then query{ for row in tool.db.order_order_standardorder do select { id = row.id; businesslocationid = row.businesslocationid; productoptionid = row.productoptionid; } } else query{ for row in tool.db.order_order_standardorder do where(row.id = id) select { id = row.id; businesslocationid = row.businesslocationid; productoptionid = row.productoptionid; } }",
    "present_kp": [],
    "absent_kp": [
      "f#"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "index-match on arrayformula. i'm making a database in google drive. in one column i have abbreviations of a number of departments someone is in, but people can be linked to multiple departments. the abbreviations are comma-seperated. i would like to have a formula that splits the abbreviations at [,], then matches the results to a range, and returns the full names of the departments.abb desired result--- --------------soc socratespla platodes descatesheg hegelheg,des hegel, descartesi've tried an arrayformula with index and match to no avail:=join(, ;arrayformula(index(b$2:$d$7;match(split(f2;,);d$2:d$7;0);2)))i've published a mwe here: <url> the mwe you can also find an attempt with filter, but it gives the 1xn or nx1 error, which makes perfect sense...)thanks so much for your help in advance!-gauwain",
    "present_kp": [],
    "absent_kp": [
      "google spreadsheets"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "usb tethering with linux and htc desire s other than rndis_host. i want to share the connection of my htc desire s with linux and i'm doing it but if it is possibile i'd like to avoid using the rndis_host module. the tethering can be configured as windows or mac osx and actually it is set to windows.can i set it to mac osx and use more standard protocol? which module should i enable in the kernel?",
    "present_kp": [
      "linux"
    ],
    "absent_kp": [
      "networking",
      "ip",
      "android"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "a chessboard model in javascript. i've created a chessboard model in javascript that will eventually iterate through some moves of a championship game.i've created a board object and a piece constructor. i've also created all the pieces with their correct position on a typical chessboard by rank and file. i don't think i've created my move method correctly in the piece constructor though, which would just move the pieces on the board. is this the best way to simulate moving pieces you think? i want the move method to push the moves onto my empty game array and eventually i want to be able to move back and forth on the array.this is going to be hooked up to the chessboard i've already made with the html and css code below if you care to look at it. (some of the boilerplate css has been omitted.)(function(window) { var board = { file: [a, b, c, d, e, f, g, h], rank: [1, 2, 3, 4, 5, 6, 7, 8] }; var piece = function (fileplace, rankplace) { this.file = board.file[fileplace - 1]; this.rank = board.rank[rankplace - 1]; this.move = function (a, b) { this.file = board.file[a - 1]; this.rank = board.rank[b - 1]; }; }; var whiterook1 = new piece(1, 1); var whiterook2 = new piece(8, 1); var whiteknight1 = new piece(2, 1); var whiteknight2 = new piece(7, 1); var whitebishop1 = new piece(3, 1); var whitebishop2 = new piece(6, 1); var whitequeen = new piece(4, 1); var whiteking = new piece(5, 1); var whitepawn1 = new piece(1, 2); var whitepawn2 = new piece(2, 2); var whitepawn3 = new piece(3, 2); var whitepawn4 = new piece(4, 2); var whitepawn5 = new piece(5, 2); var whitepawn6 = new piece(6, 2); var whitepawn7 = new piece(7, 2); var whitepawn8 = new piece(8, 2); var blackrook1 = new piece(1, 8); var blackrook2 = new piece(8, 8); var blackknight1 = new piece(2, 8); var blackknight2 = new piece(7, 8); var blackbishop1 = new piece(3, 8); var blackbishop2 = new piece(6, 8); var blackqueen = new piece(4, 8); var blackking = new piece(5, 8); var blackpawn1 = new piece(1, 7); var blackpawn2 = new piece(2, 7); var blackpawn3 = new piece(3, 7); var blackpawn4 = new piece(4, 7); var blackpawn5 = new piece(5, 7); var blackpawn6 = new piece(6, 7); var blackpawn7 = new piece(7, 7); var blackpawn8 = new piece(8, 7); var game = []; window.chess = { };})(window); body { background-color: darkgrey; } .container { width: 80%; margin: 3em auto 3em auto; min-width: 1in; max-width: 6in; } .chessboard .row { margin: 0; padding: 0; position: relative; clear: both; } .chessboard .row::before, .chessboard .row::after { font-size: 300%; position: absolute; } .chessboard .row::after { left: 103%; } .chessboard .row::before { right: 103%; } .chessboard .rank-8::before, .chessboard .rank-8::after { content: '8'; } .chessboard .rank-7::before, .chessboard .rank-7::after { content: '7'; } .chessboard .rank-6::before, .chessboard .rank-6::after { content: '6'; } .chessboard .rank-5::before, .chessboard .rank-5::after { content: '5'; } .chessboard .rank-4::before, .chessboard .rank-4::after { content: '4'; } .chessboard .rank-3::before, .chessboard .rank-3::after { content: '3'; } .chessboard .rank-2::before, .chessboard .rank-2::after { content: '2'; } .chessboard .rank-1::before, .chessboard .rank-1::after { content: '1'; } .chessboard .square { background-color: red; width: 12.5%; padding-bottom: 12.5%; display: inline-block; float: left; } .chessboard .row:nth-child(even) .square:nth-child(even) { background-color: red; } .chessboard .row:nth-child(even) .square:nth-child(odd) { background-color: lightgray; } .chessboard .square:nth-child(even) { background-color: lightgrey; } .chessboard .legend { display: inline-block; width: 12.5%; text-align: center; float: left; margin: 0; padding: 0; font-size: 300%; } nav { text-align: center; } nav button { font-size: 5ex; background-color: red; padding: 0 0.5em 0 0.5em; border-radius: 20%; } .chessboard .row .white::before, .chessboard .row .black::before { font-size: 300%; text-align: center; position: absolute; width: 12.5%; line-height: 1.2; } .chessboard .row .black.pawn::before, .white.pawn::before { content: 'f'; } .chessboard .row .black.knight::before, .white.knight::before { content: 'e'; } .chessboard .row .black.rook::before, .white.rook::before { content: 'c'; } .chessboard .row .black.queen::before, .white.queen::before { content: 'b'; } .chessboard .row .black.king::before, .white.king::before { content: 'a'; } .chessboard .row .black.bishop::before, .white.bishop::before { content: 'd'; } .chessboard .row .white { color: white; }<!doctype html><html class=no-js lang=><head> <meta charset=utf-8> <meta http-equiv=x-ua-compatible content=ie=edge> <title>tiy chessboard: kasparov v karpov (1984)</title> <meta name=description content=> <meta name=viewport content=width=device-width, initial-scale=1> <!-- place favicon.ico in the root directory --> <link rel=stylesheet href=css/normalize.css> <link rel=stylesheet href=css/main.css> <script src=js/vendor/modernizr-2.8.3.min.js></script></head><body> <!--[if lt ie 8]> <p class=browserupgrade>you are using an <strong>outdated</strong> browser. please <a href=<url> your browser</a> to improve your experience.</p> <![endif]--> <style> </style> <main class=container> <div class=chessboard> <section class=rowfilelegend> <p class=legend>a</p> <p class=legend>b</p> <p class=legend>c</p> <p class=legend>d</p> <p class=legend>e</p> <p class=legend>f</p> <p class=legend>g</p> <p class=legend>h</p> </section> <section class=row rank-8> <div class=square file-a black rook></div> <div class=square file-b black knight></div> <div class=square file-c black bishop></div> <div class=square file-d black queen></div> <div class=square file-e black king></div> <div class=square file-f black bishop></div> <div class=square file-g black knight></div> <div class=square file-h black rook></div> </section> <section class=row rank-7> <div class=square file-a black pawn></div> <div class=square file-b black pawn></div> <div class=square file-c black pawn></div> <div class=square file-d black pawn></div> <div class=square file-e black pawn></div> <div class=square file-f black pawn></div> <div class=square file-g black pawn></div> <div class=square file-h black pawn></div> </section> <section class=row rank-6> <div class=square file-a></div> <div class=square file-b></div> <div class=square file-c></div> <div class=square file-d></div> <div class=square file-e></div> <div class=square file-f></div> <div class=square file-g></div> <div class=square file-h></div> </section> <section class=row rank-5> <div class=square file-a></div> <div class=square file-b></div> <div class=square file-c></div> <div class=square file-d></div> <div class=square file-e></div> <div class=square file-f></div> <div class=square file-g></div> <div class=square file-h></div> </section> <section class=row rank-4> <div class=square file-a></div> <div class=square file-b></div> <div class=square file-c></div> <div class=square file-d></div> <div class=square file-e></div> <div class=square file-f></div> <div class=square file-g></div> <div class=square file-h></div> </section> <section class=row rank-3> <div class=square file-a></div> <div class=square file-b></div> <div class=square file-c></div> <div class=square file-d></div> <div class=square file-e></div> <div class=square file-f></div> <div class=square file-g></div> <div class=square file-h></div> </section> <section class=row rank-2> <div class=square file-a white pawn></div> <div class=square file-b white pawn></div> <div class=square file-c white pawn></div> <div class=square file-d white pawn></div> <div class=square file-e white pawn></div> <div class=square file-f white pawn></div> <div class=square file-g white pawn></div> <div class=square file-h white pawn></div> </section> <section class=row rank-1> <div class=square file-a white rook></div> <div class=square file-b white knight></div> <div class=square file-c white bishop></div> <div class=square file-d white queen></div> <div class=square file-e white king></div> <div class=square file-f white bishop></div> <div class=square file-g white knight></div> <div class=square file-h white rook></div> </section> <section class=rowfilelegend> <p class=legend>a</p> <p class=legend>b</p> <p class=legend>c</p> <p class=legend>d</p> <p class=legend>e</p> <p class=legend>f</p> <p class=legend>g</p> <p class=legend>h</p> </section> <nav> <button class=buttons>&#9654;</button> <button class=buttons>| <</button> <button class=buttons><</button> <button class=buttons>></button> <button class=buttons>> |</button> </nav> </div> </main></body></html>",
    "present_kp": [
      "javascript",
      "html",
      "css",
      "chess"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "3d solid 8 node fem matlab code. so this semester, i'm taking a finite element method course at my graduate school. we started out making codes for 1d bars and came all the way to 8 node solid elements. however, i seem to have run into a wall, as i have made my code and combed through it for the past week, making sure all the shape functions and mathematics were correct. in the end, everything looks correct. however, while i am supposed to get deflections in the z direction around the ballpark of 0.3 m, i keep getting deflections in the magnitude of e-04. i am very much at my wit's end. also, because i genuinely don't know which stackexchange community to post this question to, i have tried mathematics and overflow. i would like to emphasize that i'm not trying to just spam my question. if you kindly explain to me why my question doesn't belong here, i will delete it quickly. but i have done my research and i have made my own code, so i'm not trying to something out of nothing. any and all help is appreciated. i will detail the problem below:there is a bar that is fully clamped at one end. it is 1[m] long in the x-dir and 0.1[m] long in the y-dir andz-dir. a point force of 100[kn] is applied in an upward direction at thefree end of the bar. the number of elements in the y-dir and z-dir isto always be 1 element only, but the i'm supposed to increase thenumber of x-dir elements to show that the displacement valuesconverge. currently, not only do they not converge, but the valuesare much too small.once again, thank you for any and all help.https://drive.google.com/drive/folders/0bzaeyqa0epwvu2lltmzqsuzfltg?usp=sharing",
    "present_kp": [
      "finite element",
      "matlab"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "libandroid-shmem showing invalid elf header in chrooted linux. i have chrooted debian in android marshmallow (snapdragon 650 [64bit]).i installed iceweasel in chrooted debian. but it showed this error :: (firefox:16210): gdk-warning **: shmget failed: error 38 (function not implemented) segmentation faultso, i compiled libandroid-shmem.so from this repo using android-ndkand copied from armv8-a folder to /lib directory of chrooted debian. it then asked for liblog.so. iceweasel: error while loading shared libraries: liblog.so: cannot open shared object file: no such file or directoryso i copied liblog.so from android-ndk to chrooted debian /lib directory. now when i run env ld_preload=/lib/libandroid-shmem.so iceweasel . it displays this error :iceweasel: error while loading shared libraries: /usr/lib/aarch64-linux-gnu/libc.so: invalid elf headerhere are some details :: file /lib/libandroid-shmem.so/lib/libandroid-shmem.so: elf 64-bit lsb shared object, arm aarch64, version 1 (sysv), dynamically linked, buildid[sha1]=5ad4582c76effbe27a6688369ad979fea5dfac2a, strippedcat /usr/lib/aarch64-linux-gnu/libc.so/* gnu ld script use the shared library, but some functions are only in the static library, so try that secondarily. */output_format(elf64-littleaarch64)group ( /lib/aarch64-linux-gnu/libc.so.6 /usr/lib/aarch64-linux-gnu/libc_nonshared.a as_needed ( /lib/aarch64-linux-gnu/ld-linux-aarch64.so.1 ) )",
    "present_kp": [
      "firefox",
      "libraries",
      "chroot",
      "arm",
      "elf"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "is template metaprogramming in java a good idea?. there is a source file in a rather large project with several functions that are extremely performance-sensitive (called millions of times per second). in fact, the previous maintainer decided to write 12 copies of a function each differing very slightly, in order to save the time that would be spent checking the conditionals in a single function.unfortunately, this means the code is a pita to maintain. i would like to remove all the duplicate code and write just one template. however, the language, java, does not support templates, and i'm not sure that generics are suitable for this.my current plan is to write instead a file that generates the 12 copies of the function (a one-use-only template expander, practically). i would of course provide copious explanation for why the file must be generated programmatically.my concern is that this would lead to future maintainers' confusion, and perhaps introduce nasty bugs if they forget to regenerate the file after modifying it, or (even worse) if they modify instead the programmatically-generated file. unfortunately, short of rewriting the whole thing in c++, i see no way to fix this.do the benefits of this approach outweigh the disadvantages? should i instead:take the performance hit and use a single, maintainable function.add explanations for why the function must be duplicated 12 times, and graciously take the maintenance burden.attempt to use generics as templates (they probably don't work that way).yell at the old maintainer for making code so performance-dependent on a single function.other method to maintain performance and maintainability?p.s. due to the poor design of the project, profiling the function is rather tricky... however, the former maintainer has convinced me that the performance hit is unacceptable. i assume by this he means more than 5%, though that is a complete guess on my part.perhaps i should elaborate a bit. the 12 copies do a very similar task, but have minute differences. the differences are in various places throughout the function, so unfortunately there are many, many, conditional statements. there are effectively 6 modes of operation, and 2 paradigms of operation (words made up by myself). to use the function, one specifies the mode and paradigm of operation. this is never dynamic; each piece of code uses exactly one mode and paradigm. all 12 mode-paradigm pairs are used somewhere in the application. the functions are aptly named func1 to func12, with even numbers representing the second paradigm and odd numbers representing the first paradigm.i'm aware that this is just about the worst design ever if maintainability is the goal. but it seems to be fast enough, and this code hasn't needed any changes for a while... it's also worth noting that the original function has not been deleted (although it is dead code as far as i can tell), so refactoring would be simple.",
    "present_kp": [
      "java",
      "performance",
      "maintainability",
      "templates"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how can i get syntax highlighting to support multiline javascript strings?. currently i have javascript (.js) files that contain multiline strings. vim's syntax highlighting does not seem to support these, as seen in the image below:anything below the first line is not properly recognized as a string, and everything after the end quote is incorrectly considered a string. any way to fix this?",
    "present_kp": [
      "syntax highlighting"
    ],
    "absent_kp": [
      "line breaks",
      "filetype javascript"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "native access to facebook events and birthdays calendar. fbcal does a great job of exporting my facebook events and birthdays to an accessible calendar i can subscribe to in google calendar, ical, etc. however, it seems silly to require a third party app to do this.how can i directly access my event information on facebook as a calendar?",
    "present_kp": [
      "facebook"
    ],
    "absent_kp": [
      "data"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "huawei modem change auto answer settings. is there a way to change huawei modem ats0=1 command to be permanently set, so that there would not be need to send that command every time when modem is plugged in ?on each modem-plugin command is by default ats0=0, that means that modem auto call answer is disabled.",
    "present_kp": [
      "modem"
    ],
    "absent_kp": [
      "terminal"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "variations of the repository pattern. i've been using this variation of the repository pattern for over a year now: public interface ireadonlyrepository<t, in tid> where t : abstractentity<tid> { t get( tid id ); ienumerable<t> getall(); }/// <summary>/// defines a generic repository interface for/// classes solely in charge of getting and processing data from a data source/// </summary>/// <typeparam name=t></typeparam>/// <typeparam name=tid>the type of the id.</typeparam>public interface irepository<t, in tid> : ireadonlyrepository<t, tid> where t : abstractentity<tid>{ /// <summary> /// determines whether the specified entity has duplicates. /// </summary> /// <param name=entity>the entity.</param> /// <returns> /// <c>true</c> if the specified entity has duplicates; otherwise, <c>false</c>. /// </returns> bool hasduplicates(t entity); /// <summary> /// inserts the specified entity. /// </summary> /// <param name=entity>the entity.</param> void save( t entity ); /// <summary> /// inserts the entity or updates it if it already exists. /// </summary> /// <param name=entity>the entity.</param> t saveorupdate( t entity ); /// <summary> /// updates the specified entity. /// </summary> /// <param name=entity>the entity.</param> /// <returns></returns> t update(t entity); /// <summary> /// deletes the specified entity from the data source. /// </summary> /// <param name=entity>the entity.</param> void delete(t entity); /// <summary> /// deletes the entity with the specified id. /// </summary> /// <param name=id>the id.</param> void delete(tid id);}but recently, after rereading some books on design patterns, i've had this seemingly amazing idea to apply some patterns to my repositories.public interface irepository<t, in tid> : ireadonlyrepository<t, tid> where t : abstractentity<tid>{ void execute(irepositorycommand command); void execute(ibatchrepositorycommand command);}public interface irepositorycommand<t>{ void execute(t entity);}public interface ibatchrepositorycommand<t>{ void execute(ienumerable<t> entities);}public savecommand<t> : irepositorycommand<t> { public void execute(t entity) { // logic for saving goes here }}public batchsavecommand<t> : irepositorycommand<t>{ public void execute(ienumerable<t> entities) { // logic for batch saves go here }}which would then be called like this:_myrepository.execute(new savecommand());my reasoning is that placing logic for the common data access operations (e.g. saving, deleting) gets to be so repetitive that right now i'm relying on a t4 template to recreate those everytime i have a new entity enter the playing field. this way i just define the most commonly used data access operations and then have any of my callers execute whatever action they need to execute.can you critique my work? i do have the tendency to overthink and overengineer things.",
    "present_kp": [
      "design patterns"
    ],
    "absent_kp": [
      "c#"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to move and recreate a folder at the same time?. i have a folder called statistics in an ubuntu server in which data files are regularly stored. how can i rename statistics folder to backup-xx while re-creating statistics folder to be available for storing new files?the files in statistics folder is created by php file_put_contents.i prefer renaming the folder, as there are many files in the statistics folder.",
    "present_kp": [
      "files",
      "rename"
    ],
    "absent_kp": [
      "bash",
      "shell script"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "find subset of configurations which produces maximum result. given a set of $n$ configurations $c_1 , c_2 ... c_n$ and a function $f$ such that $f(c_i)$ produces a set of $m$ results $r_1, r_2 ... r_m$ where $r_j=\\{0,1\\}$we only care about the $\\sigma f(c_i)$ - i.e. $rj = 1$ if any $f(c_i)$ has $r_j = 1$what is an efficient algorithm to find a subset of $c$ of size $s$ such that a maximum number of $r_j = 1$ are found?a secondary problem is to have the subset sorted such that new $r_j = 1$ are found first. i.e $f(c_{i+1})$ will find $\\leq$ new $r_j = 1$ than $f(c_i)$",
    "present_kp": [],
    "absent_kp": [
      "ds.algorithms"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how is the number of unique user larger than users?. i am using google analytics campaign tracking with my android app,i don't understand how unique users (user who download the app just one time) is larger than users (new users and returning users) .here is a screen shot:",
    "present_kp": [
      "google analytics"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "copy certain files from specified subdirectories into a separate subdirectory. i want to copy all .fastq files from every subdirectory that ends in '_csf' into another already created subdirectory, dir1_csf.i understand how to copy certain files but not from certain directories.new to shell scripting, it would be great for some assistance.",
    "present_kp": [
      "shell script",
      "directory"
    ],
    "absent_kp": [
      "cp"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "splitting up method in base class to avoid errors if developer forgets to call parent: a good idea?. the create and update methods in my model base go through a few steps: pre_save, udpate_database, post_save. the pre-save and post-save are specifically designed to help with object-relational impedance mismatch. the idea is that any database operations that aren't a simple save value x to column y in the table are handled in pre_save or post_save. so for instance you could imagine something like (contrived example for clarity):$book_data = [ 'isbn' => '1-2', 'name' => 'a cool book', 'chapters' => [ [ 'id' => 1, 'a cool chapter' ], [ 'id' => 2, 'the coolest chapter' ] ]];$book = new book();$book->create( $book_data );in this example chapters is data for a child table in a one-to-many relationship. pre_save and post_save are responsible for making sure the information in chapters gets to the proper table, while update_database actually updates the books table with isbn and name. i have found this general organization to be very effective, but i've ran into an occasional problem with it.pre_save and post_save have their own tasks that are built into the model base and always have to be executed. they are also intended to be extended to add more functionality as needed. so in the above case my post-save might look something like:class book extends model_base { protected function post_save( ... ) { // necessary so the model base can do its thing parent::post_save( ... ); // then our custom code if ( isset( $input['chapters'] ) ) { $this->set_chapters( $input['chapters'] ); } }}the problem i have run into is that i occasionally forget to call the parent method, which causes subtle bugs. i usually figure that out pretty quickly. however, the other developers on my team also forget to call the parent, and they tend to figure that out less quickly. so i'm thinking about a small refactor so that there is no need to call a parent. my model base would effectively change to something like this:class model_base { public function create( $data ) { // ... normal create stuff here // now call $this->do_post_save instead of $this->post_save() $this->do_post_save( ... ); } protected function do_post_save( ... ) { // ... do the stuff that needs to always happen here // then call the old post_save $this->post_save( ... ); } protected function post_save( ... ) { // this does nothing by default }}i'm basically adding an additional method that acts as an intermediary between the create and post_save methods. this way there is no longer a need to call the parent method from inside post_save, so no more trouble with forgetting to do that. i'm leaving the current name, usage, and calling sequence of post_save unchanged, so no code should break (of course our tests will be the final measure of that). the only potential issue is that i'm making the model base a little less straight-forward, and anyone who sees these two similarly-named methods might be confused at first glance (i'll put in comments to help for anyone who looks, of course).architecturally though, is this additional layer of separation a step forward or backward? especially because this is the model base, i want to make sure any changes make sense.",
    "present_kp": [],
    "absent_kp": [
      "object oriented",
      "php",
      "object oriented design",
      "mvc"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how do we analyze algorithms with parallelism features. as i am comparing between two algorithms, i was wondering which is the best approach to use to compare between the both. big-o seems not the perfect metric for me, as the it's based on the worst-case, while most of cases in the area that i am working on is optimized. the second though is, i am using python-maps (and other parallelism features, but let's take just map no python as an example to simplify the question) to implement the first algorithm, while i don't use parallelism features in the second algorithm. what is the approach to compare, abstractly, the both algorithms ?. can you please give any hint ?",
    "present_kp": [
      "algorithms"
    ],
    "absent_kp": [
      "algorithm analysis",
      "parallel computing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why use pingler or pingomatic type of services when i could build it myself?. not sure if i'm asking in the right section, i usually hang around stackoverflow. i have a website that gets between 10-30 new articles a day written by our team. we like to automate as much as possible and have the building capacity to do so. one thing that stands out is people saying pinging pages using pingler or pingomatic is a great for ranking, but there's usage limits to those services, and it's manual..how does one ping a site or search engine ? the only ping i know of is command line ping.. is it the same thing?",
    "present_kp": [
      "ping"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "ending a 302 redirect and its effect on pagerank. please imagine the following scenario:abc.com 302 (temporary) redirect to def.comafter 6 months, we no longer use def.com, so discard the domain and we buy a new domain xyz.com.we then do a 302 redirect from abc.com to xyz.com.firstly, would abc.com show up back on google (temporarily, once we delete the 302 redirect)?and secondly, would this mean def.com would disappear and xyz.com would appear instead of it?",
    "present_kp": [
      "302 redirect"
    ],
    "absent_kp": [
      "seo",
      "domains",
      "google search"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "write to terminal and buffer. /** * execute an external program, display raw output in real-time, and buffer output pipes (stdout, stderr) for further processing. * * @param string|array $cmd command to execute (not escaped) * @param string[] $output_buffers buffers to capture * @return int exit status code */function tee($cmd, &...$output_buffers) { $proc = proc_open($cmd, array_fill(1, count($output_buffers), ['pipe', 'w']), $pipes); if($pipes) { $filepointers = []; foreach($pipes as $p => $pipe) { stream_set_blocking($pipe, 0); $filepointers[$p] = fopen(php://fd/$p, 'w'); } foreach(array_keys($output_buffers) as $b) { $output_buffers[$b] = ''; } $sleep = 10*1000; while($output_buffers) { $readsuccess = false; foreach(array_keys($output_buffers) as $b) { $p = $b+1; if(feof($pipes[$p])) { fclose($pipes[$p]); unset($pipes[$p]); unset($output_buffers[$b]); } else { $line = fgets($pipes[$b + 1]); if(strlen($line)) { $readsuccess = true; fwrite($filepointers[$p], $line); $output_buffers[$b] .= $line; } } } if($readsuccess) { $sleep = 10*1000; } else { usleep($sleep); if($sleep < 150*1000) { $sleep *= 2; } } } foreach($filepointers as $fp) { fclose($fp); } } return proc_close($proc);}i wrote this function because i write a lot of php-cli scripts that interface with other shell programs. often passthru works well for this, but sometimes you also want to capture stderr in the event that the status code doesn't give enough information. this function lets you do that.this commenter says that we have to put the pipes into non-blocking mode or the process can hang while it's waiting for you to read the other pipes, so that's what i've done.example usage:tee(echo 'hello'; (>&2 echo \\error\\); echo 'world', $stdout, $stderr);dump($stdout);dump($stderr);output:helloerrorworldhello world error",
    "present_kp": [
      "php",
      "io"
    ],
    "absent_kp": [
      "console"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "best practice in application design / sql authentication. i am currently involved in the implementation/design of an existing application for a large scale customer. the application has a similar model as e.g. vcenter server whereby a number of components store information in and extract information and workflows form a central database. the environment is managed from a console application which uses a windows service to connect to the database.due to security considerations sql authentication is not an option, but the application does support windows authentication. however when using windows authentication it passes the credentials of the user who is using the management console application on to the database in stead of using the service account under which the service runs.this results in the situation where the security roles as they are defined within the application (ad users/groups mapped to certain permissions) requires these ad objects to also have a sql server login and certain permissions on the database (db-writer for certain tables, depending on the specific permissions in the app).apart from the additional administration, this also creates a security risk imo because the whole point of the application security roles in this specific application is to allow users access to only a part of a certain table (e.g. a subset of the objects). but since they now have a sql login and db_writer on that entire table they could bypass the app and with a direct connection to the database they could read/alter records they would not be allowed to view/manipulate through the application.this risk could then (partly) be mitigated by restricting access to the database on a network level, or possibly use triggers to check the calling hostname and program name, but as far as i know that is not completely safe.the only benefit as is presented to me is there is now a full audit trail available in sql server on exactly who did what. but this is already implemented in the application as well so it seems kind of moot to me.i have been frantically searching for any info on this and what would be the best practice. i can only find asp / web based stuff but nothing regarding this type of application.is it really the best way to go with passing through authentication to sql server or should the app handle authorization and send all database requests using the service account?p.s.: this is my first question on stackexchange, if i posted on the wrong sub-site or you find my question inappropriate, please just let me know.",
    "present_kp": [
      "security",
      "sql",
      "roles"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "errormanager management. recently i created a class that would manager errors, log them, and write details about them to files when the function logerror was called. it would be done by having an error code and error message and then displaying the error to the console and writing it to the file if the user wanted to.i just posted this here to see if there was any way i can improve the class.using log4net;using system;using system.collections.generic;namespace kiwi.application.base.error{ sealed class errormanager { private readonly dictionary<string, string> errorcodes; private readonly ilog mylogger; public errormanager() { errorcodes = new dictionary<string, string>(); mylogger = logmanager.getlogger(typeof(errormanager)); // add some errors and their messages to the dictionary errorcodes.add(bn1x, error when beginning to listen on server socket.); errorcodes.add(dm9e, unable to locate the error log file.); } public void logerror(string errorcode, bool writeerrortofile = false) { string errormessage; if (errorcodes.trygetvalue(errorcode, out errormessage)) { mylogger.warn(errormessage); if (writeerrortofile) logtofile(errorcode); } else { logunhandelderror([error code + errorcode + ] + errorcode); } } private void logunhandelderror(string errorcode) { mylogger.error(unhandeld error + errorcode); } private void logtofile(string errorcode = , string filename = logs/error.log) { try { using (system.io.streamwriter file = new system.io.streamwriter(filename, true)) { file.writeline(error + errorcode + logged at + geterrortimestamp()); file.writeline(message: + trygeterrormessage(errorcode)); } } catch (system.io.filenotfoundexception) { logerror(dm9e); } } private string geterrortimestamp() { return datetime.now.tolongdatestring(); } private string trygeterrormessage(string errorcode) { string errormessage; if (errorcodes.trygetvalue(errorcode, out errormessage)) return errormessage; else return ; } }}",
    "present_kp": [],
    "absent_kp": [
      "c#",
      "error handling"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is emulation and/or virtualization faster when the host and guest systems are more similar?. is emulation and/ or virtualization performance faster when the guest and host os are similar? if yes, how big, relatively speaking, is the difference? and whether the answer is yes or no, why is it so?for example:same architecturewill a x86 windows host emulate and/or virtualize an x86 android guest faster than it will an arm android guest?similar osin the following two examples, please ignore the fact that debian is arguably more resource efficient than windows, and simply focus on relative emulation/virtualization performance.will a windows 10 host emulate/virtualize a windows 7 guest faster than it will a debian guest? (all are x86.)remotely similar oswill an os x host emulate/virtualize a debian guest faster than it will a windows guest? (all are x86.) (since os x and debian are both unix or unix-like.)i would appreciate if any answers used many layman terms and analogies to explain in a way that people who are not computer scientists could understand it.",
    "present_kp": [],
    "absent_kp": [
      "computer architecture",
      "operating systems"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "multitail: scroll up. i use multitail to view 3 log files at the same time.let's say i notice something strange in the second log file.how to scroll up the second log file, to look back at what had happened earlier? preferably without hiding the other two log files, because i want to keep an eye on them while investigating.",
    "present_kp": [
      "multitail"
    ],
    "absent_kp": [
      "scrolling"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "i changed my phone number and can't verify my gmail account. i changed my phone number and can't verify my gmail account. how do i fix this?i know my password. i've tried accessing my alternative email. nothing is working. i can't regain access to my old number.how do i access my account?",
    "present_kp": [
      "gmail"
    ],
    "absent_kp": [
      "sms",
      "verification"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why doesn't gnu screen split show up for remote user?. i have a screen session running on my local machine:~/<email>$screen -s pairthen, turn on multiuser:ctrl-a:multiuser onfinally, allow the user pair to connect to the session:ctrl-a:acladd pairso far so good. a remote user can log into the machine as pair and connect to the session:~/$ssh <email>~/$screen -x test/pairbut, if i open a split:ctrl-a:|the user pair cannot see the split. is it possible to have multiple splits open in a shared session?",
    "present_kp": [
      "gnu screen",
      "remote",
      "split"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "backup 'empty' parts of disk drive. i have a usb drive which has around 7mb of 'empty' space at its beginning. the first partition in mbr starts at 8225kb, according to parted. how can i back up this space using something like dd? i've already backed up the mbr table using dd if=/dev/sdx bs=512 count=1 of=device.mbr. how can i give dd a byte range in the disk to grab and then write that locally to a file on disk?",
    "present_kp": [
      "dd"
    ],
    "absent_kp": [
      "hard disk"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "google places review - owner or business name?. i currently manage a google places business, where we have received several reviews. i am wanting to respond to these reviews but i want to respond as the company, not as myself. i am the 'owner' of the page, it is through my personal gmail address.like facebook, when replying to comments and posts by other users on the business' wall, you reply as the actual business, not as your personal name.is this the case with google places? if not, how can i get it so the reviews are posted as the company name, not my personal name?",
    "present_kp": [
      "google places"
    ],
    "absent_kp": [
      "website review"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is this a sufficient demonstration of the effects of exception handling. we have some developers in house that believe it is best practice to use exception handing as flow control, as well as, thinking that catching and re-throwing exceptions is effective error handling. in an effort to educated, i attempted to come up with a simple sample to demonstrate to them the negative effects of doing so. the code sample below is what i came up with. it consistently demonstrates that there is a penalty for gratuitous usage of try/catch blocks; but the effect is much greater than i had anticipated. i can't help but think i've done something very wrong in this sample. i would appreciate any suggestions of improvement.in this sample, i am comparing the cost of catching and re-throwing an exception with just allowing the exception to bubble up. i needed to execute the methods in parallel to avoid waiting days for the code to execute on high values. static void main(string[] args) { timespan ts1 = new timespan(); timespan ts2 = new timespan(); list<task> tasks = new list<task>(); for (int i = 0; i < 100; ++i) { task a = new task(() => { stopwatch watch1 = new stopwatch(); watch1.start(); function1(function2); watch1.stop(); ts1 += watch1.elapsed; }); a.start(); tasks.add(a); task b = new task(() => { stopwatch watch2 = new stopwatch(); watch2.start(); function1(function3); watch2.stop(); ts2 += watch2.elapsed; }); b.start(); tasks.add(b); } while (!tasks.all(t => t.iscompleted)) ; console.writeline(watch1: + ts1); console.writeline(watch2: + ts2); console.readline(); } static void function1(action action) { try { action(); } catch { } } static void function2() { try { int.parse(null); } catch { throw; } } static void function3() { int.parse(null); }",
    "present_kp": [
      "exception handling"
    ],
    "absent_kp": [
      "c#"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "will the numerical solving of the differential equation be wrong if i take the step too small?. if i take the step too large i will get error, while if i take the step too small i also get an error. in my case, instead of seeing the function decreasing, i have it increasing if i take the step too small.",
    "present_kp": [],
    "absent_kp": [
      "numerical analysis",
      "ode"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how do i suppress error messages from cp?. i am currently looking ways to suppress error command in linux, in particular, the command cp. i do: root@ubuntu:~$ cp /srv/ftp/201*/wha*/*.jj ~/.cp: cannot stat '/srv/ftp/201*/wha*/*.jj': no such file or directoryhow do i suppress the error message that gets printed on the screen? i.e., i don't want to see this error message in my monitor.",
    "present_kp": [
      "cp"
    ],
    "absent_kp": [
      "shell",
      "wildcards"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "adding two different group of objects to a list of groups. i have two lists of strings and two lists of objects. i need to copy the list of strings to the correspondence list of objects with specific values.public list<name> retrievecitiesandcountries(){ list<string> list1 = new arraylist<string>(); //list of cities list<string> list2 = new arraylist<string>(); //list of countries list1 = retrievelistofcities(); list2 = retrievelistofcountries(); list<name> nameslist = new arraylist<names>(); return nameslist;}name classclass name{ string name; string attrib; ..}i need to add both lists to nameslist and add the respective attribs. for example, all cities should have city as their attribs and all countries should have country as their attribs. for(int i=0;i<list1.size();i++){ name name = new name(); name.setname(list1.get(i)); name.setattrib(city); nameslist.add(name); } for(int i=0;i<list2.size();i++){ name name = new name(); name.setname(list2.get(i)); name.setattrib(country); nameslist.add(name); }i am wondering if there is any better way to do it.i use nameslist to return in json format.@requestmapping(value = /names)public @responsebody list<name> retrievecitiesandcountries(){ list<name> nameslist = retrievalserv.retrievecitiesandcountries(); return nameslist;}",
    "present_kp": [],
    "absent_kp": [
      "java",
      "spring"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "canonicalising to less linked urls. i have a question about how to use canonical urls on an ecommerce site with a complex category structure.i have many products and they each sit in multiple categories. each product can be accessed on multiple urls like:/first-category-name/product-name & /second-category-name/product-namethey can all also be accessed via standard path like /products/product-namethe reason for the category versions is we vary the recommendations for alternative products and colours depending on which category the user is browsing. for example, a single unisex product may have many colour alternatives but we would show different subsets of these on the two following urls:/a-manly-category-with-dark-colours/product-name & /a-girly-category-with-eight-shades-of-pink/product-nameon /products/product-name all the alternatives are shown.the obvious solution (to me) to this was to add a canonical tag pointing all product pages to their /products/product-name version.however, it worries me that most of the links around the site will point to the duplicate versions and not the canonical target which could be sending google mixed messages. is this an issue i need to worry about or will it all be fine as long as i have the canonicals set up right?",
    "present_kp": [
      "canonical url"
    ],
    "absent_kp": [
      "seo",
      "best practices"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is there a better way of writing this javascript, or is it idiomatic?. my background is with c and python, and javascript is fairly new to me, so i'm wondering if this is reasonable code.in particular, is it appropriate to use the higher order function makecb in this context, or is there a better/shorter/clearer way of writing it?$(function(){ var controllers = { #showhelp: { name: help, id: #help }, #showlog: { name: log, id: #log } }; function hidemodals(){ for(cter in controllers){ $(controllers[cter].id).css(display, none); $(cter).text(show +controllers[cter].name); } } function showmodal(cter){ hidemodals(); $(controllers[cter].id).css(display,block); $(cter).text(hide +controllers[cter].name); } function makecb(controller){ return function(){ if($(controllers[controller].id).css(display)=='none'){ showmodal(controller); }else{ hidemodals(); } } } for(cter in controllers){ $(cter).click(makecb(cter)); }});the idea is that the only thing that has to be done to add a new modal dialogue is to add the description. the appropriate html is as follows:<div id=bottombar> <a id=showlog href=#>show log</a> <a id=showhelp href=#>show help</a> </div><div id=log style=display:none></div><div id=help style=display:none></div>ps: if you have the privileges needed to add the tags data-driven-programming or idiom and you feel it would be appropriate, i'd appreciate it.",
    "present_kp": [
      "javascript"
    ],
    "absent_kp": [
      "jquery"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "optimizing binary search tree. how could i optimize this binary search tree?#include<stdio.h>#include<stdlib.h>struct node{ int info; struct node *right; struct node *left; struct node *parent;};typedef struct node node;node *createnode(node *temp,int number){ temp=(node*)malloc(sizeof(node)); temp->info=number; temp->right=null; temp->left=null; temp->parent=null;return temp;}node *insert(node *head,int number){ node *temp; temp=createnode(temp,number); node *traverse=head; if(head==null) { head=temp; temp->parent=null; return head; } else { while(traverse!=null) { if(number < traverse->info) { if(traverse->left==null) { traverse->left=temp; temp->parent=traverse; break; } traverse=traverse->left; } else if(number>traverse->info) { if(traverse->right==null) { traverse->right=temp; temp->parent=traverse; break; } traverse=traverse->right; } else break; } }return head;}node *find(node *head,int number) // finding the position of the element which we want to delete{ node *traverse=head; if(traverse==null) return null; else { while(traverse!=null) { if(number<traverse->info) { if(traverse->left==null) { printf(number not found ); return null; } traverse=traverse->left; } else if(number>traverse->info) { if(traverse->right==null) { printf(number not found ); return null; } traverse=traverse->right; } else break; } }return traverse;}int find_children(node *temp) // finding the number of children of the the node that we want to delete{ int count=0; if(temp->left!=null) count+=1; if(temp->right!=null) count+=1;return count;}node *delete(node *head,int number) //deleting the node{ int children; node *temp=head; temp=find(temp,number); if(temp==null) return head; else if(head->left==null && head->right==null) return null; else { children=find_children(temp); if(children==0) { if(temp->parent->left==temp) { temp->parent->left=null; } else if(temp->parent->right==temp) { temp->parent->right=null; } } if(children==1) { if(temp->parent==null) { if(temp->right!=null) { temp->right->parent=null; head=temp->right; } else if(temp->left!=null) { temp->left->parent=null; head=temp->left; } } else { if(temp->parent->left==temp) { if(temp->right!=null) { temp->right->parent=temp->parent; temp->parent->left=temp->right; } else if(temp->left!=null) { temp->left->parent=temp->parent; temp->parent->left=temp->left; } } if(temp->parent->right==temp) { if(temp->right!=null) { temp->right->parent=temp->parent; temp->parent->right=temp->right; } else if(temp->left!=null) { temp->left->parent=temp->parent; temp->parent->right=temp->left; } } } } if(children==2) { node *temp1; node *temp2; temp1=temp; temp2=temp->right; while(temp2->left!=null) { temp1=temp2; temp2=temp2->left; } temp->info=temp2->info; if(temp1->left==temp2) { temp1->left=temp2->left; } else { temp1->right=temp2->right; } } }return head;}void printing(node *head){ if(head==null) return ; printing(head->left); printf(%d ,head->info); printing(head->right);}int main(){ node *head; int number,choice; printf(choose one of the option ); while(1) { printf( 1.enter a element 2.delete an element 3.print tree 4.exit ); scanf(%d,&choice); switch(choice) { case 1: { scanf(%d,&number); head=insert(head,number); break; } case 2: { scanf(%d,&number); head=delete(head,number); break; } case 3: { printing(head); break; } case 4: { exit(0); break; } } } return 0;}",
    "present_kp": [
      "c",
      "tree"
    ],
    "absent_kp": [
      "performance"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "follow-the-cursor eye animation. i have code for googly eyes that is too long. basically i have variables for each element pupils (one for right -other for left), eyesockets (left, right). i wish i could compress my code so i can call a function animateeye (eye). function animateeye(eye) { console.log(eye); var pupille = eye.find(div);for now this is what i have and i have no idea how to continue.this is the actual code:var pupillelinks = $('#pupillelinks');var pupillerechts = $('#pupillerechts');// var abstandh = pupille.offset().left- e.pagex + pupille.height()/2;$(document).on('mousemove', function(e){ var abstandv1 = pupillelinks.offset().top- e.pagey +pupillelinks.height()/2; var pupillel1 = pupillelinks.position().top - abstandv1 ; // 10 = breite pupille / 2 var augelinks = $(#augelinks) var distancex=augelinks.width()-pupillelinks.width(); var distancey=augelinks.height()-pupillelinks.height(); if(pupillel1 > distancey) { (pupillel1 = distancey); } if(pupillel1 < 0) { (pupillel1 = 0); } var abstandh1 = pupillelinks.offset().left- e.pagex + pupillelinks.width()/2; var pupillel2 = pupillelinks.position().left - abstandh1 ; if (pupillel2 < 0) { (pupillel2 = 0); } if(pupillel2 > distancex) { (pupillel2 =distancex); } pupillelinks.css({ left: pupillel2+px, top: pupillel1+px }); var abstandh2 = pupillerechts.offset().left- e.pagex + pupillerechts.width()/2; var pupiller1 = pupillerechts.position().left - abstandh2; var augerechts = $(#augerechts) var distancex2=augerechts.width()-pupillerechts.width(); var distancey2=augerechts.height()-pupillerechts.height(); if(pupiller1 < 0) { (pupiller1 = 0); } if(pupiller1 > distancex2) { (pupiller1 = distancex2); } var abstandv2 = pupillerechts.offset().top- e.pagey + pupillerechts.width()/2; var pupiller2 = pupillerechts.position().top - abstandv2; if(pupiller2 < 0) { (pupiller2 = 0); } if(pupiller2 > distancey2) { (pupiller2 =distancey2); } pupillerechts.css({ left: pupiller1+px, top: pupiller2+px }); });.auge { position: relative; display: inline-block; width: 150px; height: 150px; border: 10px solid black;}.pupille { position: absolute; top: 50%; left: 50%; width: 30px; height: 30px; background-color: blue; border-radius: 15px;}<script src=<url> id=augelinks class=auge> <div id=pupillelinks class=pupille></div></div><div id=augerechts class=auge> <div id=pupillerechts class=pupille></div></div>",
    "present_kp": [
      "animation"
    ],
    "absent_kp": [
      "javascript",
      "jquery"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to make sense of an iptables chain configuration in openwrt. this is about how to make sense of the chains found in the iptables default configuration on a typical home router running openwrt (a stripped down linux for router devices), but which ultimately may not be specific to that particular system.let's focus on the input main chain here, and disregard forward and output from the same table, as well as prerouting and postrouting from the nat table.doing an iptables -l -t filter shows a large number of rules. i have rearranged the output below to make it less intimidating, and in an attempt to pinpoint the parts that hamper my understanding.there are three built-in chains in the filter table, which appear at the top of the output. (i specified -v because i find it less confusing.)chain input (policy accept 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 1260 133k accept all -- any any anywhere anywhere ctstate related,established 8 544 accept all -- lo any anywhere anywhere 787 41632 syn_flood tcp -- any any anywhere anywhere tcp flags:fin,syn,rst,ack/syn13012 1249k input_rule all -- any any anywhere anywhere13012 1249k input all -- any any anywhere anywherechain forward # not considering this chain herechain output # not considering eitheras you can see, i snipped the chains referenced from forward and output in order to focus on input. (i could have chosen any of the other two as they are built up in a similiar manner.)input has a policy of accept, and it specifies five rules. the first three ones are clear to me. first, accept stuff that is established or related. (for example, accept the response from an http or dns request i made.) seconds, accept everything going to the loopback device (127.0.0.1). (this may only come from localhost itself, and i do want that to work. wouldn't make sense otherwise.) third, have a synflood protection. (which protects against a certain kind of attack.)chain syn_flood (1 references) pkts bytes target prot opt in out source destination 787 41632 return tcp -- any any anywhere anywhere tcp flags:fin,syn,rst,ack/syn limit: avg 25/sec burst 50 0 0 drop all -- any any anywhere anywherebut then, there are two rules branching into two chains called input and input_rule, and the question is, why are there two of them, and which one are you supposed to use for what?let's drill down the jump stack of those rules.chain input_rule (1 references) pkts bytes target prot opt in out source destinationthere's nothing in here yet. it is meant for me to add rules. but what kind of rules?chain input (1 references) pkts bytes target prot opt in out source destination 6315 482k zone_lan all -- br-lan any anywhere anywhere 6697 767k zone_wan all -- pppoe-wan any anywhere anywhereokay, this one does have stuff, jumping further down into lan and wan, which makes sense for a home router.chain zone_lan (1 references) pkts bytes target prot opt in out source destination 6315 482k input_lan all -- any any anywhere anywhere 6315 482k zone_lan_accept all -- any any anywhere anywherechain zone_wan (1 references) pkts bytes target prot opt in out source destination 0 0 accept udp -- any any anywhere anywhere udp dpt:bootpc 0 0 accept icmp -- any any anywhere anywhere icmp echo-request 6697 767k input_wan all -- any any anywhere anywhere 6697 767k zone_wan_reject all -- any any anywhere anywhereas you can see, each one of those rules jumps further down the stack to more user-defined rules.chain input_lan (1 references) pkts bytes target prot opt in out source destinationchain zone_lan_accept (2 references) pkts bytes target prot opt in out source destination 4 1322 accept all -- any br-lan anywhere anywhere 6315 482k accept all -- br-lan any anywhere anywherewhat is the purpose of input_lan? the other one is probably to accept packets, but it makes me wonder the policy for input is accept, so why repeat accept here?now, input from wan. if you scroll up you can see that some udp and icmp stuff is accepted. this is for dhcp and, basically, ping. that much is clear. what is less clear, again, is the partially empty stuff following those rules:chain input_wan (1 references) pkts bytes target prot opt in out source destinationsame question as for input_lan.chain zone_wan_reject (2 references) pkts bytes target prot opt in out source destination 0 0 reject all -- any pppoe-wan anywhere anywhere 6697 767k reject all -- pppoe-wan any anywhere anywhereokay, that is input from wan (not established or related), and yes, we probably want to reject it, and now there are two kinds of rejection here, one closing the socket (tcp-reset) for tcp connection attempts, and another one via icmp reply (icmp-port-unreachable) for icmp messages (think ping).chain reject (5 references) pkts bytes target prot opt in out source destination 595 31817 reject tcp -- any any anywhere anywhere reject-with tcp-reset 4858 582k reject all -- any any anywhere anywhere reject-with icmp-port-unreachablethis last one is a catch-all. so nothing will get accepted here.finally, here's a list of other chains found in the filter table that aren't referenced from the built-in input chain in the net table. just for completeness, and to see that they seem to have analogous constructs.# other chains, not reached from the input chain, so truncated and moved herechain forward (1 references)chain forwarding_lan (1 references)chain forwarding_rule (1 references)chain forwarding_wan (1 references)chain nat_reflection_fwd (1 references)chain output (1 references)chain output_rule (1 references)chain reject (5 references)chain zone_lan_drop (0 references)chain zone_lan_reject (1 references)chain zone_lan_forward (1 references)chain zone_wan_accept (2 references)chain zone_wan_drop (0 references)chain zone_wan_forward (1 references)so, well. sorry for this long post. there were a couple questions along the way. i don't know how to put this in an easier or shorter way. this iptables configuration is not exactly easy to grasp because there are unclear details spread about here and there. hope you can clarify this and explain the underlying rationale. thanks for your attention.",
    "present_kp": [
      "iptables",
      "openwrt",
      "router"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "an algorithm to find highest lines in at least one point. suppose we have $n$ lines $l_1, l_2, \\dots, l_n$, where $l_i$ has the equation $y = a_i x + b_i$. we call $l_i$ the highest line at $x_0$ if for each $j \\in \\{1, 2, \\dots, n\\}$$$a_i x_0 + b_i \\ge a_j x_0 + b_j.$$goal: find an algorithm in $o(n \\log n)$ time to return a set of lines that are the highest lines in at least one point.hint: use the divide and conquer technique.since we should use divide and conquer technique, i think i should split the problem into two subproblems with size of $n/2$ and try to solve these two first. i don't know what to do next. thanks in advance.",
    "present_kp": [
      "divide and conquer"
    ],
    "absent_kp": [
      "algorithms",
      "computational geometry"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "spacesort - a new sorting algorithm. a sorting algorithm i have written.pros- gives each element a position once.- few comparisons needed.cons- for certain sets of data, large amount of extra storage is needed.- only works with integers. the main reason i post the algorithm here is to learn if it already exists or not. from the research i have done i can't find anything similar. though i am sceptical it would be very fun to know if it is new or not. what i mostly want to know however is if the algorithm could be useful and if my implementation of it is reasonable. algorithm:basic idea: when you divide the number you want to sort with the largest number of the set you get a percentage. this percentage gives a rough position where in the sorted list that number should be. because it is difficult to know how the data is distributed the mean/average is used. by dividing the mean-value with the amount of unique values in the set, you can decide how much of the data set is above and below the mean-value.then by using the above idea on all the elements, each element can get a position and be put in a new sorted list.some lists with a high amount of clustering or small number of elements with large values, can have the same position calculated from different elements. therefore all the elements are first put in a larger array to lessen the amount of collisions and if a collision occur the element is moved sligthly up/down. bacause the new array is larger, elements can be moved up and down without causing problems with insertion of new ones.duplicates are handled by counting the collisions of each element. when all elements have been placed in the larger array it is then collapsed into an array of the correct size. exampel:[4, 20, 6, 4, 4]min = 4, max = 20, mean = 8 calculating the possible amout of unique elements:(max - min + 1) = (20 - 4 + 1) = 17calculating a percentage for the mean:(1 - (mean-min)/unique) = (1 - (8-4)/17) = 0.76this tells us roughly how large portion of the elements are below the mean. this makes sense because we have 4 out of 5 elements lower than 8. if we for this example create a 20 times larger array (100 elements) we know that the 80 first elements are for the 4 elements under 8. thus each element has 20 places to use for collisions.first element:4 => ((element - min)*places for each element) = (4-4)*20 = 06 => (6-4)*20 = 40here 4 gets position 0 and 6 gets position 40. just as 6 is between 4 and 8,40 is between 0 and 80. after doing this for all elements and then removing all the empty space in the large array, an array have been sorted. testing:i have timed a python vesion of this code by hard-coding different scenarios andcomparing time with quicksort and countingsort. out of a lack of experience ican't draw any real conclusions but these are my results: with a set of data larger than roughly 500 elements spacesort took half the time quicksort did.generally, countingsort is faster but certain sets of data such as a completely homogeneous set can be done faster with spacesort after a few code changes. and of course sets with a large range of data. good to know:the large empty array created is called space in the code.except for a data-array the function needs an argument size, this konstant multiplied by the length of the data-array gives the length of the space-array. e.g the example above used size = 20.how large the empty array created needs to be depends alot on the set of data. the smaller the size the faster the algorith and less memory usage. a completely homogeneous set needs only the same size as the original data set. the less homogeneous the larger it needs to be. a good starting point might be 10 times larger.struct spaceelement { public int data; public bool set; // true if a value have been set to this element.} /* takes two arguments, data = set of data to be sorted and size then returns the sorted list.*/ static int[] spacesort (int[]data, int size) { int min, max, mean; minmaxmean(data, out min, out max, out mean); spaceelement[] space = insertelements(data, size, min, max, mean); return collapsespace(space, data.length); } // arguments: list of data, size constant for space, min-value, max-value, mean-value. // returns the space array with sorted elements. static spaceelement[] insertelements(int[] data, int size, int min, int max, int mean){ int lowmax, highmin; lowmaxhighmin(data, min, max, mean, out lowmax, out highmin); int numdiffelements = max - min + 1; // largets possible amount of different elements. e.g 4,20,6,4,4 has 20-4+1 = 17 elements. double meanpercent = 1 - (mean - min) / (double)numdiffelements; // determines what percentage of elements are below the mean. e.g 4,5,6,7,20 => 1 - (7 - 4)/17 = 0.82. // creates a large array where the sorted elements can be inserted according to their size. int spacelength = data.length * size; spaceelement[] space = new spaceelement[spacelength + size * 2 + 1]; // determine how much space each element gets if it lies below or above the mean. double lowmult = meanpercent * data.length * size / (lowmax - min + 1); double highmult = (1 - meanpercent) * data.length * size / (max - highmin + 1); foreach ( int x in data) { bool done = false; int index = 0; int laststep = size; // determine the initial position of the element in the space array. if ( x <= mean) { index = (int)math.round( (x - min) * lowmult + size/2 ); laststep = (int)math.round(lowmult); } else { index = (int)math.round( (x - highmin) * highmult + size + meanpercent*spacelength ); laststep = (int)math.round(highmult); } /* first checks if the position is empty, if it is put the element there. else if value is the same as the already stored value increment count on that element. if the value is not equal to the already stored value jump up/down half of the lowmult/highmult. */ while (!done) { if (!space[index].set){ space[index].data = x; space[index].set = true; done = true; } else if (space[index].data == x) { space[index].count += 1; done = true; } else if ( space[index].data > x ) { laststep = (int)math.round(laststep / 2.0d); index -= laststep; } else { laststep = (int)math.round(laststep / 2.0d); index += laststep; } } } return space; } /* arguments: list of data. out: min-value, max-value and mean/average-value */ static void minmaxmean ( int[] data, out int min, out int max, out int mean ) { min = data[0]; max = data[0]; long sum = 0; foreach ( int x in data ) { sum += x; if ( x < min ) min = x; if ( x > max ) max = x; } mean = (int)math.round(sum / (double)data.length); } /* arguments : list of data, min-value, max-value, mean/average-value. returns the highest value less than or equal to the mean-value and the lowest value greater than the mean-value. */ static void lowmaxhighmin( int[] data, int min, int max, int mean, out int lowmax, out int highmin) { lowmax = min; highmin = max; foreach ( int x in data ) { if ( x > lowmax && x <= mean ) lowmax = x; if ( x < highmin && x > mean ) highmin = x; } } // creates an array of the same size as the original data array and populates it with the sorted values. static int[] collapsespace( spaceelement[] space, int length ) { int[] newarray = new int[length]; int count = 0; foreach ( spaceelement x in space) { if ( x.set) { for (int y = 0; y <= x.count; y++) { newarray[count] = x.data; count++; } } } return newarray; }",
    "present_kp": [
      "algorithm",
      "sorting"
    ],
    "absent_kp": [
      "c#"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "rsync: --fuzzy doesn't detect name changes with link-dest. so these are my test files:$ ls -li live/total <phone> -rwxrwx--- 1 ariel ariel <phone> feb 22 00:45 live/file1804975 -rwxrwx--- 1 ariel ariel <phone> feb 22 00:47 live/file2and a first backup set:$ rsync -avix live backups/1/sending incremental file listcreated directory backups/1cd+++++++++ live/>f+++++++++ live/file1>f+++++++++ live/file2$ ls -il backups/*/live/*804981 -rwxrwx--- 1 ariel ariel <phone> feb 22 00:45 backups/1/live/file1804982 -rwxrwx--- 1 ariel ariel <phone> feb 22 00:47 backups/1/live/file2i rename file1 and see if rsync can spot the renamed file with --link-dest -fuzzy --fuzzy:$ mv live/file1 live/file1live/file1 -> live/file1$ rsync -avhii --debug=hlink3,fuzzy2 --link-dest=../1 --fuzzy --fuzzy live backups/2sending incremental file listcreated directory backups/2fuzzy size/modtime match for ../1/live/file1fuzzy basis selected for live/file1: ../1/live/file1cd..t...... live/>f+++++++++ live/file1hf live/file2sent 1,048,997 bytes received 188 bytes 2,098,370.00 bytes/sectotal size is 2,097,152 speedup is 2.00$ ls -il backups/*/live/*804981 -rwxrwx--- 1 ariel ariel <phone> feb 22 00:45 backups/1/live/file1804982 -rwxrwx--- 2 ariel ariel <phone> feb 22 00:47 backups/1/live/file2805110 -rwxrwx--- 1 ariel ariel <phone> feb 22 00:45 backups/2/live/file1804982 -rwxrwx--- 2 ariel ariel <phone> feb 22 00:47 backups/2/live/file2rsync linked file2 so the path given to --link-dest is correct, and --fuzzy --fuzzy did match file1 to file1, but it was still copied, not linked. why?",
    "present_kp": [
      "rsync"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "login with apache user. i'm following this tutorial step by step,forhere we clone the repo into a chmodded /var/www/[site_dir] folder. note that we switch to the www-data user before running the git clone command. this is an important step because the deploy key we generated is owned by the www-data user and it will only work for that user, even if you are on the root.i need to switch to www-data user, but when i try sudo su - www-data ( or sudo su www-data ) i faced with this account is currently not available.i try ps aux and i see that the www-data user exists, now i want to know how can i achieve that ? ( login with www-data user)",
    "present_kp": [
      "sudo",
      "su"
    ],
    "absent_kp": [
      "users",
      "webserver"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "i am using kali linux, but however, i want to know how to switch my normal user account into root admin?. i want to know how to switch my normal user account into root admin?for example, when i log into my account i cannot modify and save sources.list from /etc/apt/folder nor can i get an update for apt-get update or even apt-get upgrade.is there anyway possible i can convert my normal user account to admin (root)?",
    "present_kp": [
      "kali linux",
      "root"
    ],
    "absent_kp": [
      "terminal"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "terminal setup when running make from vim. what exactly is vim doing when executing the :make command?i was expecting that it just executes the content of the makeprg variable just like running :!make and additionally collects the output, but apparently the behavior is different. for example the terminal size is set up differently.for example create a makefile with the following contentcheck: echo asdf > foo.txt aspell -c foo.txtthen compare the behavior of :make and :!make.is it different for you, too, or is this something strange with my setup?",
    "present_kp": [
      "makeprg"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "white boards -- who uses them?. so as both a full-time programmer and hobbyist as well (developing my own things for personal use and maybe to sell one day), i feel that me purchasing a big white board to hang in my room at home or something would be very useful.does anyone here have one as well, to use for high level designs (uml, architecture, etc.) and things like very early ui mockups, etc.?if you guys do have them, which ones have you bought? i can't seem to pin one that would be good for home use and i'm not sure of the pricing/other things.thanks!",
    "present_kp": [
      "design"
    ],
    "absent_kp": [
      "development environment"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "stat symbolic link : won't show original file modification time. stat symbolic linkstat fileit only shows data on when the symbolic link was created.i was hoping to get data on when the origin had been modified.",
    "present_kp": [
      "stat"
    ],
    "absent_kp": [
      "symlink"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "changing to nvidia driver broke cinnamon. when changing to the nvidia 3.75 driver, through the manager and restarting, i'd received a nice little message about cinnamon crashing and an update of mode. i tried to restart cinnamon through the dialogue box but it just loops and doesn't fix the problem.what can i do about this? google isn't turning over anything up to date for me, on this problem.i'm running linux mint 18.1if any more information is needed, please ask me. thanks!",
    "present_kp": [
      "linux mint",
      "nvidia",
      "cinnamon"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "why do some linux distributions still have /dev/ttys0, ttys1, etc., even though newer computers don't have such a serial port?. many new laptop and desktop computers do not have 9-pin/25-pin serial ports. why do many linux distributions still contain /dev/ttys0, dev/ttys1 device files?since udev can create the device files dynamically, why are /dev/ttys0, /dev/ttys1 still created statically? each time i boot up, /dev/ttys0 and /dev/ttys1 are in there.by the way: i am using debian 7.0.",
    "present_kp": [
      "linux",
      "udev",
      "serial port"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "representing the opening and closing time for a business. i have an openclose class which just represents the hours of operation of a business by the opening and closing time. it takes the opening and closing times as arguments to its constructor, and each is a datetime object.the data is coming from an external source in a string, formatted like hh:mm (am|pm)-hh:mm (am|pm)i have the following function to turn this into an openclose object:def __get_times(self, hours): return openclose(*map(lambda x: datetime.strptime(x, %i:%m %p), hours.split(-)))what do you think of this? is it too much for one line? too confusing? should i split it up into multiple lines?it sort of bugged me when doing it with multiple lines, but i can certainly see how this would be more readable, despite my hatred of explicitly doing the datetime calculation twice:format = %i:%m %popen_time, close_time = hours.split(-)open_time = datetime.strptime(format)close_time = datetime.strptime(format)return openclose(open_time, close_time)an alternative would be to use a combination of these approaches:format = %i:%m %phours = hours.split(-)open_time, close_time = map(lambda x: datetime.strptime(x, format), hours)return openclose(open_time, close_time)which of these is best?",
    "present_kp": [
      "datetime"
    ],
    "absent_kp": [
      "python",
      "comparative review",
      "interval"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is the reason behind changing the name of well-known apps?. mate has forked a number of applications originating as the gnome core applications, and developers have written several other applications from scratch. the forked applications have new names - mostly in spanish:caja (box) file manager (from nautilus)pluma (quill) text editor (from gedit)eye of mate image viewer (from eye of gnome)atril (lectern) document viewer (from evince)engrampa (staple) archive manager (from file roller)mate terminal terminal emulator (from gnome terminal)marco (frame) window manager (from metacity)mozo (waiter) menu item editor (from alacarte)what it use different name for well-known programs? it make user confused (e.g. gedit foo.txt breaks and i have to run pluma foo.txt)they are almost the same as the original ones. wasn't it better they send their modification to main branch?",
    "present_kp": [
      "mate"
    ],
    "absent_kp": [
      "gnome2"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is paypal the best solution for payment gateway for a website?. i have a realty website that needs a payment gateway for their property reservation. the reservation fee range from $500-$600 and about 5-6 people per month. i was wondering if paypal is the best solution for accepting payment. what will be the pros and cons using paypal.paypal was my first choice because it's easy to integrate on my existing website and i wouldn't be minding so much on the security. p.s.it's not a part of the question, but if you can site some realty website that accept payment and would be a good inspiration. it would be highly appreciated.thanks!",
    "present_kp": [
      "paypal"
    ],
    "absent_kp": [
      "web development",
      "website design"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to remap ctrl-backspace in normal and insert modes?. how do i remap backspace combinations? i can remap backspace itself in normal mode, but not in insert mode, and i cannot map combinations: this works:nmap <bs> dd this doesn't work:imap <bs> <esc>ddnmap <c-bs> ddimap <c-bs> <esc>ddi'm using vim 7.4 inside terminator.",
    "present_kp": [],
    "absent_kp": [
      "key bindings"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "web crawler (a tour of go #71). so i was trying to do this exercise from tour of go. i managed to get it working. i am not at all sure that it is correctly concurrent or idiomatic (i started learning go, like, 4 hours ago).i would really appreciate feedback about my solution. here's the relevant meat of the code. full code available below.type urltofetch struct { url string depth int}// group stuff together, so that it's easier to pass around.type fetchcontext struct { queue chan urltofetch history map[string]int quit chan int fetcher fetcher}func crawl(url string, depth int, fetcher fetcher) { context := fetchcontext{ queue: make(chan urltofetch, 500), history: make(map[string]int), quit: make(chan int, 50), fetcher: fetcher, } go fetchone(&context) context.queue <- urltofetch{url, depth} <-context.quit}func fetchone(ctx *fetchcontext) { timeout := time.after(2000 * time.millisecond) select { case utf := <-ctx.queue: if utf.depth > 0 { body, urls, err := ctx.fetcher.fetch(utf.url) ctx.history[utf.url] = 1 if err != nil { fmt.println(err) return } fmt.printf(found: %s %q , utf.url, body) for _, u := range urls { go fetchone(ctx) if _, ok := ctx.history[u]; !ok { ctx.queue <- urltofetch{u, utf.depth - 1} } else { } } } case <-timeout: ctx.quit <- 1 }}the full source is at: <url>",
    "present_kp": [
      "go"
    ],
    "absent_kp": [
      "concurrency",
      "http"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "do .net functions have function prologs?. if i'm looking at a binary compiled with vc++ in a hex editor and i want to identify the start of functions - i can look for the hex 55 8b - which is a common function prolog.is there something equivalent with .net cil? i.e. is there a hex pattern i can look for to identify the start of functions raw?the application here is to look for shared code between malware samples.",
    "present_kp": [
      "hex",
      ".net"
    ],
    "absent_kp": [
      "assembly"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is it advisable to create a single api to access the database?. got an organization that its core work is acquiring various sources of data, validating it and integrating it to existing mostly financial data sets and then availing that data to many different types of customers through various web, mobile, ussd and api applications it has developed internally and others which have been outsourced.some of the challenges they are having and will continue to have is enforcing application authentication to access the database, managing db sessions and importantly, ensuring that the various developers from different countries and organizations return the same result as other applications.we want to enhance security by encrypting the db -postgresql db- and want to be able to manage access to it. there is also more development work for new applications but all new applications will now be fully outsourced. since data is the core asset of this company, is it advisable that the company provides one api that all other applications call when they want data from the company? will this be a bottleneck to manage and maintain or its actually the best way to go about in such an environment? by such an environment i mean the core job is data aggregation, securing the data and then giving access to business partners who are building web apps, ussd applications, sms applications etc making hundreds of thousands of calls a day.",
    "present_kp": [
      "database",
      "postgres"
    ],
    "absent_kp": [
      "api design",
      "enterprise architecture"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "need free / open source software licences for specific requirements. before publishing my reusable software source code to some online repository, i have some license requirements as follows. i need to know whether any existing free or open software source license agreement can be used or i need to create a custom license agreement?here are my requirements,people should be able to download, modify, reuse my code in their application / program with / without any modifications for both commercial & non-commercial purpose , with following restrictions applied (this clause is for use of my code in their application e.g. google using open-source webkit in chrome, however chrome is not open-source. or os x using darwin open-source kernel, however its os is not entirely open source):a) they should strictly use my code within their application in private domain e.g. restricted to a specific group of people in a project /company. (let me clarify it more, i think, i have written this in wrong way. what i meant by this is, if they just want to use my code within their application & release / sell the application as whole to entire public (e.g. google releasing chrome with webkit) it's use is not restricted to group of people. on other side, if they have modified my original code (e.g webkit modified / forked), however they do not want to publish it to general public, then they should keep the modification / bug fixes with themselves or within their organisation known to specific group of people.)b) it should include, apply & obey my original license agreement. (what i meant is to include this license, when packaging my original code / modified or forked code. e.g. chrome includes license of webkit / blink (forked webkit) in their credits section.)c) the free of charge code doesn't apply in private domain. (let me explain what i mean by this with an e.g. suppose if google is working on gmail server project & they use free & open-source reusable component in gmail project. now, they employ / pay a person who modifies the node.js to well work well with gmail, who finally privately gives / in-directly sells unnamed forked node.js to google for their gmail server product. thus in this case google is not re-distributing node.js to public, its just used within their organisation & internally within their product & they may have pay someone to develop this forked node.js)however, if they want to re-publish my code, either in compiled or un-compiled form (as if it's their own code) on some other public repository / website or branch within my repository itself for re-use, then following restrictions should apply (e.g google releasing forked webkit i.e. blink for public use):a) it should include, apply & obey my original license agreement. (what i meant is to include this license, when packaging my original code / modified or forked code. e.g. chrome includes license of webkit / blink (forked webkit) in their credits section.)b) they must publish the modified source code, if the code (either in compiled or un-compiled form) is intended for re-usable or generic component. in this case the code should be free of any charge. (e.g. google releasing forked webkit i.e. blink for re-use in public; & should be free of any charge.)c) the modified source is not necessary to publish, only when the code is in compiled form and is hosted as part of application or program, that is not intended for reusable software components. in case as code itself is not sold, rather it is part of application; the free of charge code doesn't apply here. (it is same thing i meant in clause 1c. please suggest if clause 1c is also obvious or does needs to be specifically mentioned in the license?)d) the code must have some modifications done apart from just naming conventions / comments / format changes. (reason, i wanted to include this as one the clauses is, because someone might take my code, change some variable names, re-distribute & take unnecessary credit. instead, i would like, if people report any bugs or also necessary modifications, may it be minor variable change or major enhancement at my code repository)e) people cannot publish the modified code, with same title / heading as original. e.g. if original code is published with name / title / heading, say 'abc common components' they should give either entirely different name or prefix with some words, say 'abc common components extensions'. the new title must not contain version as prefix say, abc common components v2' or version 2 or just 2. (ok, i see from responses, that i need to use a trademark to protect my software name being reused. then if that's the solution, i can remove this clause too)f) people should not force me to incorporate their modification to my original published code trunk / branch, however they can suggest / request modifications and report bugs to my original published code. (after, all your responses, it seems that, i don't need to worry about it as it's something obvious, thus we can safely remove this clause.)",
    "present_kp": [],
    "absent_kp": [
      "license recommendation"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "reference wrapper for functions overloadings and universal references. i have design a little class to allow to overload a function with universal references for a known type, but i'm not sure if that class will work as expected in any contexts.the indended purpose is to allow overload a ''universal reference'' templated function, for specific types:template<class arg_t>void fun(arg_t&& obj); // obj will be just forwarded// overload for std::string but unaware of qualifiers, for// forwarding purposes.template<class arg_t = std::string?>void fun(arg_t&& str);c++ currently doesn't offer any feature to allow that (templated qualifiers). so, my class is like a std::reference_wrapper, but transporting in a second template parameter the decayed type, to write things like:// overloads the generic version for a std::string qualified family.template<class qualified_t>void fun(uref<qualified_t, std::string> uref){ fun2(uref.get()); } // no need to do std::forwardthe get() return type will be std::string&, std::string const& or std::string&&.that is the best i was able to achieve:template<class ref_type_t, class type_t = std::decay_t<ref_type_t> >class uref{ ref_type_t o;public: using type = type_t; using ref_type = ref_type_t; static_assert(std::is_reference<ref_type>::value, ref_type must be a reference); explicit uref(ref_type t) : o(std::forward<ref_type>(t)) {} ref_type get() { return std::forward<ref_type>(o); } type const& get() const { return o; } template<class t, class = std::enable_if_t<std::is_convertible_v<ref_type, t> and !std::is_same_v<std::decay_t<t>, type>> > operator t() const { return get(); } template<class t, class = std::enable_if_t<std::is_convertible_v<type const&, t> and !std::is_same_v<std::decay_t<t>, type>> > operator t() { return get(); } operator type const&() const & { return o; } operator type&() & { return o; } operator ref_type() && { return std::forward<ref_type>(o); }};template<class type>uref<type&&> make_uref(type&& t){ return uref<type&&>(std::forward<type>(t)); }the way of using it is:void client(float) { std::cout << floating << std::endl; }void client(std::string const&){ std::cout << from const char* to std::string << std::endl; }template<class param_t>auto service(param_t&& arg){ return client(make_uref(std::forward<param_t>(arg))); }template<class ref_t>void client(uref<ref_t, std::string>){ std::cout << specific << std::endl; }int main(){ service(3); service(std::string(hi!)); service(hi!); service(3.f); return 0;}since uref saves the original reference type on the first template parameter t, and with get() you get the reference as-is, and you can have different universal reference overloads for different ''decayed'' types. with the conversion operators, the client can receive the type without begin wrapped, the conversion will free the reference bound on the service parameter. you can still use the reference wrapper to construct other objects to any convertible-to type.the question is, is desirable to have such an object? can be seen as an anti-pattern? for example, the two overloads, receiving a std::string and uref<t, std::string>, can confused the reader, specifically, when each one of them will be called.any downside of this design?",
    "present_kp": [
      "c++",
      "template",
      "wrapper",
      "overloading",
      "reference"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how can i list subdirectories recursively?. the obviousls -drdoes not work.i am currently usingfind /path/ -type d -lsbut the output is not what i need (plain listing of sub-folders)is there a way out?",
    "present_kp": [
      "find",
      "ls"
    ],
    "absent_kp": [
      "directory"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "copy file from web to google drive. there is a document on the web that i want to store in my google drive. the only way i know to do this is saving the document on my computer and then upload it again to my google drive.is there a more straightforward way to do this?",
    "present_kp": [
      "google drive"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "why does the unnamed register only contain the last deleted text when appending deleted text to a register?. the vim documentation says the following under quote_quote about the unnamed register():vim fills this register with text deleted with the d, c, s,x commands or copied with the yanky command, regardless of whether or not a specific register was used (e.g. xdd). this is like the unnamed register is pointing to the last used register. thus when appending using an uppercase register name, the unnamed register contains the same text as the named register.however, the part with uppercase register names does not work for me when deleting. i only get the last deleted text into the unnamed register. for example, when i enter add and then add, registera contains both deleted lines, but the unnamed register only contains the last one, although it should contain the same text as registera. however, things work as expected with the yank(y) command. why is this?",
    "present_kp": [
      "register"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "what does big o mean as a term of an approximation ratio?. i'm trying to understand the approximation ratio for the kenyon-remila algorithm for the 2d cutting stock problem.the ratio in question is $(1 + arepsilon) ext{opt}(l) + o(1/arepsilon^2)$.the first term is clear, but the second doesn't mean anything to me and i can't seem to figure it out.",
    "present_kp": [
      "approximation"
    ],
    "absent_kp": [
      "algorithms",
      "asymptotics"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "macbook pro retina 2013 with full disk encryption boot issue. i have installed ubuntu 15.10 on macbook pro retina late 2013. it was working well until i break the boot by reinstalling grub.now i am not able to repair it.i have three partitions:/dev/sda1 - /boot - ext2 /dev/sda2 - / - root, ext4, enrypted with luks /dev/sda3 - swap, encrypted with luksi have no logical volumes there.i have tried to reinstall grub with initramfs to configure to boot in bios mode and was getting either blinking cursor and nothing happening or macbook into reboot loop.my configuration is:/etc/crypttab (unchanged as i had it from beginning)root uuid=***'<uuid of encrypted /dev/sda2>'*** none luks,discardswap uuid=***'<uuid of encrypted /dev/sda3>'*** none luks,discard,swap/etc/initramfs-tools/conf.d/cryptroot:target=root,source=uuid=***'<uuid of encrypted /dev/sda2>'***,key=none,discard/etc/initramfs-tools/conf.d/resume:resume=uuid=***'<uuid of encrypted /dev/sda3>'***/etc/default/grub (this configuration gives me blank screen with blinking cursor):grub_enable_cryptodisk=ygrub_default=0grub_hidden_timeout=0grub_hidden_timeout_quiet=truegrub_timeout=10grub_distributor='lsb_release -i -s 2> /dev/null || echo debian'grub_cmdline_linux_default=quiet splash nomodesetgrub_cmdline_linux=cryptdevice=/dev/sda2:root root=/dev/mapper/rootgrub_preload_modules=lvm luks cryptodiskgrub_init_tune=480 440 1when i change to grub_cmdline_linux=cryptdevice=uuid=<uuid of encrypted /dev/sda2>:root root=/dev/mapper/root - i get into continuous reboot loopsteps i take:sudo sucryptdisk luksopen /dev/sda2 rootmount /dev/mapper/root /mntmount /dev/sda1 /mnt/bootmount -o bind /dev /mnt/devmount -o bind /dev/pts /mnt/dev/ptsmount -o bind /sys/mnt/sysmount -o bind /proc /mnt/procmount -o bind /run /mnt/runchroot /mnt /bin/bashupdate-initramfs -k all -cgrub-install /dev/sdaupdate-grubexitrebootlsinitramfs /boot/initrd* | grep cryptsetup gives:sbin/cryptsetuplib/cryptsetuplib/cryptsetup/askpasslib/x86_64-linux-gnu/libcryptsetup.so.4any ideas what i could be missing or doing wrong?",
    "present_kp": [
      "grub",
      "encryption",
      "luks"
    ],
    "absent_kp": [
      "macintosh"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is it possible to use both align and size parameters for images in moinmoin?. i'm getting a 500 internal server error if i try something like {{attachment:image.png|alt text|width=100 align=left}}, both in the wiki i am actually trying to edit (<url>) and in the moinmoin wiki sandbox (<url>).the examples at moinmo.in/helponlinking show how to use both parameters, but never combining them in the same image. it definitely seems to be possible to use more than one parameter (e.g. width and height), but the combination of width and align apparently brings the wiki engine down. is there something i'm overlooking?how can i add a thumbnail of an image to a page in a moinmoin wiki without using the default alignment (inline with text)?",
    "present_kp": [
      "images",
      "wiki"
    ],
    "absent_kp": [
      "embed"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "bash split a list of files. i have 200 files in a folder like: test_1_cfg.dat, test_2_cfg.dat,.... and so on. i need to include in a bash script the first 40 files as input and run some process, another script to read the next 40 files and run another process. so i was thinking of a way to have a list of the names of the files and then just split that list but i'm not sure how to do it in bash.any idea?",
    "present_kp": [
      "bash",
      "split"
    ],
    "absent_kp": [
      "shell script"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "example of level set method. i am looking for easily to understand example of level set method used to track phases interfaces. i would like to solve it using fem because my solution is based on the fem solution of second fick law. initialization step is straight forward - it just distance of each node from the interface. but i have problem with second step - solution of the interface velocity equation:$$ rac{\\partial \\phi }{\\partial t}+f\\left| abla \\phi ight|=0 $$if i understand correctly i need a weak formulation of that differential equation, where f is linear velocity of the interface. solution will give me a new position of the interface. is it correct?",
    "present_kp": [],
    "absent_kp": [
      "finite element"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to search and list files and folders with specific pattern?. imagine i only know a part of the name of the folder(s) or file(s), but dont know their location(s) in the computer. i even dont know whether they are hidden or not. how can i search and list them all?(say, for example i want to list all the folders and files which are having mysql as the part of their name)",
    "present_kp": [],
    "absent_kp": [
      "file search"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "dmp 3300mx-a embedded pc, debian 6.0 squeeze, kernel 3.2, bios watchdog and harwdare reset. i have been studying how to properly set up a watchdog on my embedded system, have read plenty of posts, but i am stuck with no solution.i've read:how to correctly configure debian watchdog daemon for bios watch dog?what is a watchdog reset?how do i cause a watchdog reset of my embedded linux devicehow enable watchdog?i even gave a deep look athttp://www.sat.dundee.ac.uk/~psc/watchdog/linux-watchdog.htmlbut no way out at this moment.so i will start describing my hardware/software and explaining what i am trying to achieve, then i will explain what i did and what was the result. hoping that someone of you can point me out to the right direction.harwdare:a dmp embedded with vortex86dx cpu (a9111 @ 933mhz), see some documentation at the following addresses:<url> has harwdare support for 2 watchdog timers (16c550 serial port 1, gpio port 4) with ali (acer labs) m6117 watchdog hardware, a 32.768khz clock with a 24 bits counter. the timer ranges is from 30.5u sec to 512 sec with resolution 30.5u sec. when the timer times out; a system reset, nmi or irq may happen. bios:american megatrends amibios (62-0100-000001-<phone>-a9100-1adsv000-y2kc), bios date 08/03/2011os:debian 6.0 squeeze with linux kernel (# uname -r) 3.2.0-0.bpo.4-486driver:ali m6117 watchdog v0.2 given to me directly by the manufacturer that states that it can be used with my kernel, and which is based on federico bareilles original driver for linux 2.4.x (<url>)my goal:since this machine is installed far from where i live, and sometimes it freezes, i am forced to take my car and go there to do an hardware reset.i would like to set it up so that when the system freezes, the machine is able to reset itself.what i've done:i've compiled the above driver (make clean, make) and i've installed it (make install). the module is properly found in the extra folder under the kernel path.i've edited /etc/watchdog.conf file and uncommented the line /dev/watchdog to enable the watchdog (and nothing else, no other lines uncommented or added)i've moved to the /lib/modules/3.2.0-0.bpo.4-486/extra folder and loaded the module with insmod alim6117_wdt.kochecked that the module was loaded withlsmod and i gotmodule alim6117_wdt, size 12565, used by 0 (this becomes 1 when i enable the watchdog in the bios)rebooted the machine, entered the bios and enabled watchdog 0, watchdog 0 timer 512sec, watchdog 0 signal select resetwhat i've got:the system boots up properly,i see the messages about the watchdog being enabled/loaded, but after a bit it reboots with the messagealim6117_wdt unexpected close, not stopping watchdogif i run dmesg | watchdog i get [0.017827] nmi watchdog disabled (cpu0): hardware events not enabled, but i think this is not important since all i need is an hardware reset.i've made a further test, without succeeding, by adding to the /etc/watchdog.conf file the following lines:ping = 127.00.0.1interface = lan0 lan0 is the proper name of the ethernet interface, obtained with ifconfig command. or should i use lo as interface since i am trying to ping the loopback interface?just in case of any help, the last messages i see when the system completes its boot are:stopping watchdog keepalive daemon....stargint watchdog daemon....startpar: service(s) returnet failure: rc.local ... failed!in the boot sequence, at a certain point, the watchdog keepalive daemon gets started, then it gets stopped right after starting the watchdog daemon. is this an odd thing? or is it normal? should the wd_keepalive stay always on or it is supposed to go down when the watchdog daemon goes up? (i've investigated this a bit and, if i understand, it seems to be normal, so i am a bit confused)this is becoming a thorn in my side.what am i doing wrong? could someone point me to the right direction?following is the source code of the driver /** * ali m6117 watchdog timer driver. * * (c) copyright 2003 federico bareilles <<email>, * instituto argentino de radio astronomia (iar). * * this program is free software; you can redistribute it and/or * modify it under the terms of the gnu general public license as * published by the free software foundation; either version 2 of * the license, or (at your option) any later version. * * the author does not admit liability nor provide warranty for any * of this software. this material is provided as-is in the hope * that it may be useful for others. * * based on alim1535_wdt.c by alan cox and other wdt by several * authors... * * ali (acer labs) m6117 is an i386 that has the watchdog timer * built in. watchdog uses a 32.768khz clock with a 24 bits * counter. the timer ranges is from 30.5u sec to 512 sec with * resolution 30.5u sec. when the timer times out; a system reset, * nmi or irq may happen. this can be decided by the user's * programming. **/#define ali_wdt_version 0.2.0#include <linux/module.h>#include <linux/miscdevice.h>#include <linux/watchdog.h>#include <asm/io.h>#include <asm/uaccess.h>#include <linux/reboot.h>#include <linux/init.h>#include <linux/proc_fs.h>#define our_name alim6117_wdt/* port definitions: */#define m6117_port_index 0x22#define m6117_port_data 0x23/* yes, the two unused ports of 8259: * 0020-003f : pic1 * * the 8259 interrup controller uses four port addresses (0x20 through * 0x23). although ibm documentation indicates that these four port * addresses are reserved for the 8259, only the two lower ports (0x20 * and 0x21) ar documented as usable by programers. the two ports * (0x22 and 0x23) are used only when reprogramming the 8259 for * special dedicated systems that operate in modes which are not * compatible with normal ibm pc operation (this case). **//* index for ali m6117: */#define ali_lock_register 0x13#define ali_wdt 0x37#define ali_wdt_select 0x38#define ali_wdt_data0 0x39#define ali_wdt_data1 0x3a#define ali_wdt_data2 0x3b#define ali_wdt_ctrl 0x3c/* time out generates signal select: */#define wdt_signal_irq3 0x10#define wdt_signal_irq4 0x20#define wdt_signal_irq5 0x30#define wdt_signal_irq6 0x40#define wdt_signal_irq7 0x50#define wdt_signal_irq9 0x60#define wdt_signal_irq10 0x70#define wdt_signal_irq11 0x80#define wdt_signal_irq12 0x90#define wdt_signal_irq14 0xa0#define wdt_signal_irq15 0xb0#define wdt_signal_nmi 0xc0#define wdt_signal_srset 0xd0/* set signal to use: */#define wdt_signal wdt_signal_srset/* ali_wd_time_factor is <phone>/30.5 */#define ali_wd_time_factor 32787 /* (from seconds to ali counter) */static unsigned long wdt_is_open;static char ali_expect_close;static int wdt_run = 0;static bool nowayout = watchdog_nowayout;module_param(nowayout, bool, 0);module_parm_desc(nowayout, watchdog cannot be stopped once started (default= __module_string(watchdog_nowayout) ));static unsigned wdt_timeout = 60;module_param(wdt_timeout, int, 0);module_parm_desc(wdt_timeout, initial watchdog timeout (in seconds));static int alim6117_read(int index){ outb(index, m6117_port_index); return inb(m6117_port_data);}static void alim6117_write(int index, int data){ outb(index, m6117_port_index); outb(data, m6117_port_data);}static void alim6117_ulock_conf_register(void){ alim6117_write(ali_lock_register, 0xc5);}static void alim6117_lock_conf_register(void){ alim6117_write(ali_lock_register, 0x00);}static void alim6117_set_timeout(int time){ u32 timeout_bits; timeout_bits = time * ali_wd_time_factor; alim6117_write(ali_wdt_data0, timeout_bits & 0xff); alim6117_write(ali_wdt_data1, (timeout_bits & 0xff00) >> 8); alim6117_write(ali_wdt_data2, (timeout_bits & 0xff0000) >> 16); return;}static void alim6117_wdt_disable(void){ int val = alim6117_read(ali_wdt); val &= 0xbf; /* 1011|1111 */ alim6117_write(ali_wdt, val);}static void alim6117_wdt_enable(void){ int val = alim6117_read(ali_wdt); val |= 0x40; /* 0100|0000 */ alim6117_write(ali_wdt, val);}static void alim6117_wdt_signal_select(int signal){ int val = alim6117_read(ali_wdt_select); val &= 0xf0; val |= signal; alim6117_write(ali_wdt_select, val);}static void ali_wdt_ping(void){ int val; /* if no run, no ping; wdt start when ping it. */ if (wdt_run) { alim6117_ulock_conf_register(); val = alim6117_read(ali_wdt); val &= ~0x40; /* 0100|0000 */ alim6117_write(ali_wdt, val); val |= 0x40; /* 0100|0000 */ alim6117_write(ali_wdt, val); alim6117_lock_conf_register(); /* printk(kern_info our_name : wdt ping... ); */ } else { printk(kern_warning our_name : wdt is stopped ); }}static void ali_wdt_start(void){ alim6117_ulock_conf_register(); alim6117_wdt_disable(); alim6117_set_timeout(wdt_timeout); alim6117_wdt_signal_select(wdt_signal); alim6117_wdt_enable(); alim6117_lock_conf_register(); wdt_run = 1;}static void ali_wdt_stop(void){ int val; if ( wdt_run ) { alim6117_ulock_conf_register(); val = alim6117_read(ali_wdt); val &= ~0x40; /* 0100|0000 */ alim6117_write(ali_wdt, val); alim6117_lock_conf_register(); wdt_run = 0; /* printk(kern_info our_name : wdt stop... ); */ }}/** * ali_wdt_notify_sys: * @this: our notifier block * @code: the event being reported * @unused: unused * * our notifier is called on system shutdowns. we want to turn the timer * off at reboot otherwise the machine will reboot again during memory * test or worse yet during the following fsck. * */static int ali_wdt_notify_sys(struct notifier_block *this, unsigned long code, void *unused){ if (code == sys_down || code == sys_halt) { /* turn the timer off */ ali_wdt_stop(); } return notify_done;}/** * ali_write - writes to ali watchdog * @file: file handle to the watchdog * @data: user address of data * @len: length of data * @ppos: pointer to the file offset * * handle a write to the ali watchdog. writing to the file pings * the watchdog and resets it. writing the magic 'v' sequence allows * the next close to turn off the watchdog. */static ssize_t ali_write(struct file *file, const char *data, size_t len, loff_t * ppos){ /* can't seek (pwrite) on this device */ if (ppos != &file->f_pos) return -espipe; /* check if we've got the magic character 'v' and reload the timer */ if (len) { size_t i; ali_expect_close = 0; /* scan to see wether or not we got the magic character */ for (i = 0; i != len; i++) { u8 c; if (get_user(c, data + i)) return -efault; if (c == 'v') ali_expect_close = 42; } ali_wdt_ping(); return 1; } return 0;}/** * ali_ioctl - handle watchdog ioctls * @inode: inode of the device * @file: file handle to the device * @cmd: watchdog command * @arg: argument pointer * * handle the watchdog ioctls supported by the ali driver. */static long ali_ioctl(struct file *file, unsigned int cmd, unsigned long arg){ int options; static struct watchdog_info ident = { .options = wdiof_keepaliveping | wdiof_settimeout, .firmware_version = 0, .identity = ali m6117 wdt, }; switch (cmd) { case wdioc_keepalive: ali_wdt_ping(); return 0; case wdioc_settimeout: if (get_user(options, (int *) arg)) return -efault; if (options < 1 || options > 512) return -efault; wdt_timeout = options; ali_wdt_start(); case wdioc_gettimeout: return put_user(wdt_timeout, (int *) arg); case wdioc_getsupport: if (copy_to_user ((struct watchdog_info *) arg, &ident, sizeof(ident))) return -efault; return 0; case wdioc_getstatus: case wdioc_getbootstatus: return put_user(0, (int *) arg); case wdioc_setoptions: if (get_user(options, (int *) arg)) return -efault; if (options & wdios_disablecard) { ali_wdt_stop(); return 0; } if (options & wdios_enablecard) { ali_wdt_start(); return 0; } return -einval; default: return -enotty; }}/** * ali_open - handle open of ali watchdog * @inode: inode of device * @file: file handle to device * * open the ali watchdog device. ensure only one person opens it * at a time. also start the watchdog running. */static int ali_open(struct inode *inode, struct file *file){ if(test_and_set_bit(0, &wdt_is_open)) return -ebusy; ali_wdt_start(); return 0;}/** * ali_release - close an ali watchdog * @inode: inode from vfs * @file: file from vfs * * close the ali watchdog device. actual shutdown of the timer * only occurs if the magic sequence has been set or nowayout is * disabled. */static int ali_release(struct inode *inode, struct file *file){ if (ali_expect_close == 42 && !nowayout) { ali_wdt_stop(); } else { printk(kern_crit our_name : unexpected close, not stopping watchdog! ); } ali_expect_close = 0; clear_bit(0, &wdt_is_open); return 0;}static struct file_operations ali_fops = { .owner = this_module, .write = ali_write, .unlocked_ioctl = ali_ioctl, .open = ali_open, .release = ali_release,};static struct miscdevice ali_miscdev = { .minor = watchdog_minor, .name = watchdog, .fops = &ali_fops,};/* * the wdt needs to learn about soft shutdowns in order to turn the * timebomb registers off. */static struct notifier_block ali_notifier = { .notifier_call = ali_wdt_notify_sys, .next = null, .priority = 0};static int __init alim6117_init(void){ if (wdt_timeout < 1 || wdt_timeout > 512){ printk(kern_err our_name : timeout out of range (0 < wdt_timeout <= 512) ); return -eio; } if (misc_register(&ali_miscdev) != 0) { printk(kern_err our_name : cannot register watchdog device node. ); return -eio; } register_reboot_notifier(&ali_notifier); printk(kern_info wdt driver for ali m6117 v( ali_wdt_version ) initialising. ); return 0;}static void __exit alim6117_exit(void){ misc_deregister(&ali_miscdev); unregister_reboot_notifier(&ali_notifier); ali_wdt_stop(); /* stop the timer */}module_init(alim6117_init);module_exit(alim6117_exit);module_author(federico bareilles <<email>);module_description(driver for watchdog timer in ali m6117 chip.);module_license(gpl);module_supported_device(watchdog);",
    "present_kp": [
      "debian",
      "embedded",
      "bios",
      "watchdog"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "what happens during nic master failover?. during nic master failover, will the mac address of bond0 change?",
    "present_kp": [],
    "absent_kp": [
      "linux",
      "networking",
      "bonding"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "password length and complexity. i am using centos and i want to set the minimum password length, minimum password expiry , maximum password expiry and password complexity in my system.i have looked over some tutorials to change the system-auth file, but it didnt work.hoping to get some answers here.thanks in advance :d",
    "present_kp": [
      "centos",
      "password"
    ],
    "absent_kp": [
      "linux",
      "authentication"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "flux boundary condition in solute transport. i have a pretty naive question, though imporant to me. usually when solving the following pde in solute transport:$ rac{{\\partial c}}{\\partial t } = abla. (d abla c -vc )=0$one can be ask to use a third-type boundary condition (called flux in software like comsol or phreeqc):$(d abla c -vc )=vco$where the left side is assume to be the center of the cell and the right the inflow or outflow flux (am i right about this last sentence?)in some software (comsol) you do not have to specify co, therefore i assume that in such cases the boundary condition is reduced to:$d abla c = 0$is that true? can such boundary condition be considered constant flux boundary conditon? thanks.",
    "present_kp": [
      "comsol"
    ],
    "absent_kp": [
      "boundary conditions"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "google form with an alert. i created a registration form with google forms. now i would like to have an alert displayed before the submitter starts filling out data.this is my code:function onopen() { formapp.getui() .createmenu('custom menu') .additem('show alert', 'showalert') .addtoui();}function showalert() { var ui = formapp.getui(); var result = ui.alert( 'lorem ipsum', 'dolor sit amet, consectetur adipiscing elit', ui.buttonset.ok);}the triggers were set up. the script works fine but only while a form is in in the editing mode, not in a view mode. actually i'm the only one who can see the message box. it's not what i meant to ;-). how to show this alert to everyone who is going to fill out my form?",
    "present_kp": [
      "google forms"
    ],
    "absent_kp": [
      "google apps script"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to execute a command on window focus/unfocus?. i'd like to execute xinput disable bcm5974 when gnome terminal (and maybe some other application) gets focused, and xinput enable bcm5974 when it loses focus.this is because libinput and my macbook's touchpad are not friends, libinput's palm rejection barely works, it's really driving me nuts when editing code in vim and it scrolls by accident, or when typing a command at the terminal.libinput 1.1.4-1xf86-input-libinput 0.16.0-1archlinux",
    "present_kp": [
      "gnome",
      "xinput",
      "libinput"
    ],
    "absent_kp": [
      "scripting",
      "x11"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "systemd service not executing the python script. i was making a battery notifier script for my raspberry pi3. the script is executing when i do/usr/bin/python /home/pi/documents/shutdown.pyand showing the popup notifications. however the service is not executing it or not showing the notification. i can see the python process if i do sudo systemctl status battery-notifier.service.battery-notifer.service[unit]description=battery notifier[service]type=simpleworkingdirectory=/home/pi/documentsexecstart=/usr/bin/python /home/pi/documents/shutdown.py[install]wantedby=multi-user.targetshutdown.pyimport raspiupshatimport statisticsimport subprocessfrom time import sleepraspiupshat.init()while(true): voltageslist = [] sleep(0.5) currentvoltage = raspiupshat.getv() voltageslist.append(currentvoltage) medianvoltage = statistics.median(voltageslist) if(medianvoltage>4): subprocess.popen([notify-send,battery full!])this is the status of the service when i do sudo systemctl status battery-notifier.service: battery-notifier.service - battery notifier loaded: loaded (/lib/systemd/system/battery-notifier.service; enabled) active: active (running) since sat 2017-07-15 04:05:18 utc; 48min ago main pid: 28384 (python) cgroup: /system.slice/battery-notifier.service 28384 /usr/bin/python /home/pi/documents/shutdown.pyjul 15 04:05:18 raspberrypi systemd[1]: started battery notifier.",
    "present_kp": [
      "systemd",
      "python",
      "raspberry pi"
    ],
    "absent_kp": [
      "libnotify",
      "notify send"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "show partially hidden wrapped lines. i have turned on line wrapping with set wrap linebreak nolist in my .vimrc. however, this has the annoying behavior that lines not completely visible on the screen are hidden outright and replaced with @ signs:how can i make these lines show up even though they are partially hidden?",
    "present_kp": [
      "wrapping"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "knuth's algorithm m that produces mixed-radix numbers. this is the c++ code of my implementation of knuth's algorithm m that produces mixed-radix numbers:#include visit.hvoid algorithmm(vector<int>& m){ m.insert(m.begin(),2); const int n=m.size(); vector<int> a(n,0); m2: visit(false,a); int j=n-1; m4: if (a[j]==m[j]-1) {a[j]=0;--j;goto m4;} if (j==0) return; else {a[j]++;goto m2;} }int main(){ vector<int> m; int i; while(std::cin>>i) {if(i<0) continue; m.push_back(i); }algorithmm(m);return 0;}this is the code of visit.h:#include <iostream>#include <vector>using std::vector;using std::cout;template<class t> void visit(bool first,vector<t>& l){ size_t dt=first?0:1; for(typename vector<t>::iterator i=l.begin()+dt;i!=l.end();++i)cout<<*i; cout<<' ';}the c++ code is very close to the knuth's pseudocode.and now this is an imperative haskell implementation using mutable arrays:import data.array.ioimport control.monad.stateimport data.iorefdata countlist = countlist {intlist::[int],count::int}lenarr arr = do b<-getbounds arr return (snd b)takeinput :: state (string,int) [int]takeinput = do (s,count)<-get let g=reads s if g==[] then return [] else do put (snd(head g),count+1) l<-takeinput return $ (fst(head g)):ltakeinput2 :: string->countlisttakeinput2 s = let (l,ss)=runstate (takeinput) (s,0) in countlist l (snd ss)fillarray :: countlist->io((ioarray int int),(ioarray int int))fillarray l = do arr<-newarray (0,(count l)) 0 x<-nowfill 1 (intlist l) arr y<-newarray (0,(count l)) 0 writearray x 0 2 return (x,y) where nowfill i l arr = do if l==[] then return arr else do writearray arr i (head l) nowfill (i+1) (tail l) arrvisit ::(ioarray int int)->int->io ()visit x i = do c<-lenarr x if i>c then putstrln else do a<-readarray x i putstr (show a) visit x (i+1)maj :: (ioarray int int)->(ioarray int int)->int->io((ioarray int int),int)maj m a j = do valaj <- readarray a j valmj <- readarray m j if valaj==valmj-1 then do writearray a j 0 maj m a (j-1) else return (a,j)m5 :: (ioarray int int)->int->io((ioarray int int),int)m5 a j = if j==0 then return (a,j) else do valaj<-readarray a j writearray a j (valaj+1) return (a,j)algorithmm0 m a = do visit a 1 n<-lenarr m (a',j)<-maj m a n (a'',j')<-m5 a' j if j'==0 then return () else algorithmm0 m a''algorithmm = do l<-getline let mycountlist = takeinput2 l (m,a)<-fillarray mycountlist algorithmm0 m amain :: io ()main = algorithmmi also have and a more functional approach using lists in haskell which is smaller but i don't want to enlarge the post.can you please give me some advice on how to shrink the haskell code?i think that the main reason of using a high-level language like haskell is to write less code but i don't think this happens here so i suppose that i am doing something wrong.",
    "present_kp": [
      "algorithm",
      "haskell"
    ],
    "absent_kp": [
      "optimization"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "wd advanced format confusion. one of my hitachi deskstar 7k160 hard disks died today and the other one is not safe (has bad sectors) so i've ordered a new 1 tb wd blue wd10ezex. apparently nowadays new hard disks come with this advanced format technology. i don't know much about hard disk partitioning, cylinders, blocks, aligning &c. but as i read, wrong type of configuration can cause problems such as slower r/w speeds, loss of capacity &c. this is how i would like to partition the new drive:win7(ntfs) | storage1 (ntfs) | swap | debian jessie (ext4) | storage2 (ext4) *should i just use debian's partition manager (on netinstall, i guess it's gparted based?) -as i always do- then install win7 (64-bit) and then install debian and i should not encounter any problems? if not, should i worry about:what partition manager should i employ? this wiki suggest that i use gpt instead of mbr. but i have an old motherboard with bios, no uefi. can i use gpt at all?is having windows on the same hard disk with debian gonna screw things up? just what is the correct procedure to partition this new hard disk?! *: the order of the partitions can be different if this order is gonna cause problems.",
    "present_kp": [
      "debian",
      "partition"
    ],
    "absent_kp": [
      "grub2"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to replace multiple blank lines with a specific non-blank line?. the sample input is123456789the expected output is123---45---678---9",
    "present_kp": [],
    "absent_kp": [
      "sed",
      "awk"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "recursive spinlock for c using asm, revision #1. precursor for this revision here in code-review:recursive spinlock for c using asmimprovements have been made thanks to forsvarir.now i am trying to optimize this lock so it becomes more practical.the code in question.#define gate_gate volatile int#define gate_pass volatile intextern inline voidgate_enter(gate_gate *gate, gate_pass *pass){ asm volatile ( jmp gate_enter_check gate_enter_wait: pause gate_enter_check: cmp %[lock], %[pass] // skip if pass >= lock jge gate_enter_skip mov %[lock], %%eax // spinlock start cmp %[gate], %%eax // check if locked without lock cmd je gate_enter_wait // if locked, wait. lock xchg %%eax, %[gate] // lock seems to be free, now we attempt to take it. test %%eax, %%eax jnz gate_enter_wait // spinlock end gate_enter_skip: add %[lock], %[pass] // checkin pass : [gate] =m (*gate), [pass] =m (*pass) : [lock] r (1) : eax );}extern inline intgate_leave(gate_gate *gate, gate_pass *pass){ if (*pass <= 0) { // signal underflow for debugging. return -1; } asm volatile ( // cleaner solution thanks to forsvarir/stackoverflow add %[checkout], %[pass] jnz gate_leave_skip mov %[unlock], %[gate] gate_leave_skip: : [gate] =m (*gate), [pass] =m (*pass) : [unlock] r (0), [checkout] r (-1) : eax ); return 0;}// example use. like forsvarir pointed out, we have to be careful and keep pass on the stack.void exampleuse(int count, gate_pass pass){ int underflow; printf(pass=%d, gate=%d , pass, gate); gate_enter(&gate, &pass); if (count == 3) { // pass = 0; // force deadlock. } if (count < 5) { printf(count = %d , count); exampleuse(++count, pass); } underflow = gate_leave(&gate, &pass); if (!underflow) { printf(pass=%d, gate=%d , pass, gate); } else { printf(underflow detected ); }}gate_pass pass = 0;exampleuse(0, pass);the questionthe intent of this code is a lock that allows recursion and does so efficiently.is there a point for this optimization? am i right to assume that lock operator is very expensive and this approach is justified and right?is my attempt to early? locking is hard to test. for this reason i strongly believe that spotting an error is a valid candidate for an answer.finally if all is well, there is always something to improve. this is a code review! feel free to review the code as you see fit.",
    "present_kp": [
      "c",
      "locking"
    ],
    "absent_kp": [
      "reinventing the wheel",
      "assembly"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "localized wrapping in vim. i have wrapping turned on in vim, but is it possible to disable it for certain regions in the file i'm editing? to be specific, i'm editing latex files, i would like the contents of some environments (tabular for example) not to be wrapped, since this would make the table contents much easier to read.a solution based on automatically detecting such environments based on their opening/closing tags would be the best of course, but i'm not against a solution needing some explicit tags or commands in a comment in the file.",
    "present_kp": [
      "vim"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "analysing ip camera's udp packages. i have an industrial ip camera and i want to read its images with a micro-controller, but the camera is only operable from a computer running its drivers/firmware. i used wireshark to access the udp packages, but the messages seem to be encrypted (or at least i cannot make any sense of them) and i don't really know how to proceed on deciphering the control requests. i would really appreciate any ideas, hints or general comments on how to go on.here are a couple of frames sent by the computer to the camera during the initialization and capture. (working on getting the frames from the cam, but wireshark and windows have this problem with lans).here's one of the frames sent by the camera to a not supported device. the two data bytes between ** change sequentially, the rest (of the data) stays the same. 0000 ff ff ff ff ff ff 00 1b a2 12 c6 b4 08 00 45 00 ..............e.0010 01 1c 00 01 00 00 80 11 8f d1 a9 fe 00 01 ff ff ................0020 ff ff c3 52 c3 52 01 08 5e 5e 71 00 e0 00*01*00 ...r.r..^^q.....0030 f8 00 34 30 30 32 39 32 32 30 35 31 00 00 84 01 ..<phone>....0040 13 00 20 00 00 00 0a 00 04 01 00 00 00 00 05 21 .. ............!0050 00 00 02 0a 00 00*cc*01 e8 03 00 1b a2 12 c6 b4 ................0060 ff ff a9 fe 00 01 ff ff 00 00 00 00 00 00 a9 fe ................0070 00 01 ff ff 00 00 00 00 00 00 00 00 00 00 00 00 ................0080 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................0090 00 00 00 00 00 00 00 00 00 12 c6 b4 00 00 19 01 ................00a0 01 01 00 00 00 00 00 00 00 00 11 00 03 01 01 00 ................00b0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................00c0 00 00 00 00 00 00 00 00 00 00 e8 03 05 00 00 00 ................00d0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................00e0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................00f0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................0100 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................0110 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................0120 00 00 00 00 00 00 00 00 00 00 ..........edit: i spent these last hours doing my homework and thought, since i only want to read its images, deciphering might be a bit of an overkill. is it probable that a replay attack would also do the trick?",
    "present_kp": [
      "wireshark"
    ],
    "absent_kp": [
      "sniffing",
      "interoperability"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how do i get from a highly manual process of development and deploy to continuous integration?. we have a development process which is completely manual. no unit tests, interface tests are manual, and merging and integration are as well. how could we go from this state to implementing continuous integration with full (or at least close to full) automation of build and test? we have a pretty intense development cycle, and are not currently using agile, so switching to agile with ci in one move would be a very complicated and expensive investment. how can we take it slowly, and still moving constantly towards a ci environment?",
    "present_kp": [
      "continuous integration"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "comparing data to model and returning a chi squared value. this is quite basic but useful to test various (9) different models using one set of data. i have tried to make it clear and use the pep8 formatting.i am currently creating a version that can read csv files to make it even faster. i'd like any suggestions on how i could improve my code or any hints as to what i could add next.import numpyimport pylabimport matplotlib.pyplotimport scipy.optimizefrom scipy.optimize import curve_fit''' a program that determines the reduced chi squared value for various theoretical models.''''''the best fit parameters are derived using levenberg-marquardt algorithm which solves the non-linear least squares problem.'''''' test = 1 assumes a linear model '''''' test = 2 assumes a quadratic model '''''' test = 3 assumes a cubic model '''''' test = 4 assumes a quartic model '''''' test = 5 assumes a quintic model '''''' test = 6 assumes a logarithmic model '''''' test = 7 assumes a power law model '''''' test = 8 assumes a exponential model '''''' test = 9 assumes a exponential offset model '''#recorded observed dataxdata = numpy.array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])ydata = numpy.array([-10, -7, 1, 5, 7, 7, 6, 7, 10, 15])xerror = numpy.array([0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3])yerror = numpy.array([0.1, 0.1, 0.4, 0.3, 0.2, 0.1, 0.1, 0.2, 0.1, 0.1])#user prompted input valuestest=int(raw_input(enter a number assigned theoretical model to test: ))#theoretical models to be testedif test == 1: print 'testing linear model', ' ' #scipy optimise cuve_fit model produces expected values using a linear model def func(x, a, b): return a*x + b #constants of theoretical model popt, pcov = curve_fit(func, xdata, ydata) print 'constant ax is',(%.2f %popt[0]) print 'constant b is',(%.2f %popt[1]), ' 'if test == 2: print 'testing quadratic model', ' ' #scipy optimise cuve_fit model produces expected values using a quadratic model polynomial model def func(x, a, b, c): return a*x**2 + b*x + c #constants of theoretical model popt, pcov = curve_fit(func, xdata, ydata) print 'constant ax^2 is',(%.2f %popt[0]) print 'constant bx is',(%.2f %popt[1]) print 'constant c is',(%.2f %popt[2]), ' ' if test == 3: print 'testing cubic model', ' ' #scipy optimise cuve_fit model produces expected values using a cubic model polynomial model def func(x, a, b, c, d): return a*x**3 + b*x**2 + c*x + d #constants of theoretical model popt, pcov = curve_fit(func, xdata, ydata) print 'constant ax^3 is',(%.2f %popt[0]) print 'constant bx^2 is',(%.2f %popt[1]) print 'constant cx is',(%.2f %popt[2]) print 'constant d is',(%.2f %popt[3]), ' 'if test == 4: print 'testing quartic model', ' ' #scipy optimise cuve_fit model produces expected values using a quartic polynomial model def func(x, a, b, c, d, e): return a*x**4 + b*x**3 + c*x**2 + d*x + e #constants of theoretical model popt, pcov = curve_fit(func, xdata, ydata) print 'constant ax^4 is',(%.2f %popt[0]) print 'constant bx^3 is',(%.2f %popt[1]) print 'constant cx^2 is',(%.2f %popt[2]) print 'constant dx is',(%.2f %popt[3]) print 'constant e is',(%.2f %popt[4]), ' 'if test == 5: print 'testing quintic model', ' ' #scipy optimise cuve_fit model produces expected values using a quintic polynomial model def func(x, a, b, c, d, e, f): return a*x**5 + b*x**4 + c*x**3 + d*x**2 + e*x + f #constants of theoretical model popt, pcov = curve_fit(func, xdata, ydata) print 'constant ax^5 is',(%.2f %popt[0]) print 'constant bx^4 is',(%.2f %popt[1]) print 'constant cx^3 is',(%.2f %popt[2]) print 'constant dx^2 is',(%.2f %popt[3]) print 'constant ex is',(%.2f %popt[4]) print 'constant f is',(%.2f %popt[5]), ' 'if test == 6: print 'testing logarithmic model', ' ' #scipy optimise cuve_fit model produces expected values using a logarithmic model def func(x, a, b): return a*numpy.log(x)+ b #constants of least squared theoretical model popt, pcov = curve_fit(func, xdata, ydata) print 'constant a is',(%.2f %popt[0]) print 'constant bx is',(%.2f %popt[1]), ' 'if test == 7: print 'testing power law model', ' ' #scipy optimise cuve_fit model produces expected values using a power law model def func(x, a, b): return a*(x**b) #constants of least squared theoretical model popt, pcov = curve_fit(func, xdata, ydata) print 'constant a is',(%.2f %popt[0]) print 'constant b is',(%.2f %popt[1]), ' 'if test == 8: print 'testing exponential model', ' ' #scipy optimise cuve_fit model produces expected values using a exponential model def func(x, a, b, c): return a*numpy.exp(b*x + c) #constants of least squared theoretical model popt, pcov = curve_fit(func, xdata, ydata) print 'constant a is',(%.2f %popt[0]) print 'constant bx is',(%.2f %popt[1]) print 'constant c is',(%.2f %popt[2]), ' 'if test == 9: print 'testing exponetial offset model', ' ' #scipy optimise cuve_fit model produces expected values using a exponential offset model def func(x, a, b, c, d): return a*numpy.exp(b*x + c) + d #constants of least squared theoretical model popt, pcov = curve_fit(func, xdata, ydata) print 'constant a is',(%.2f %popt[0]) print 'constant bx is',(%.2f %popt[1]) print 'constant c is',(%.2f %popt[2]) print 'constant d is',(%.2f %popt[3]), ' '#derived chi squared value for this modelchi_squared = numpy.sum(((func(xdata, *popt)-ydata)/xerror)**2)reduced_chi_squared = (chi_squared)/(len(xdata)-len(popt))print 'the degrees of freedom for this test is', len(xdata)-len(popt)print 'the chi squared value is: ',(%.2f %chi_squared)print 'the reduced chi squared value is: ',(%.2f %reduced_chi_squared)#observed values are plotted with expected valuesmatplotlib.pyplot.figure()matplotlib.pyplot.scatter(xdata, ydata, s=0)matplotlib.pyplot.plot(xdata,func(xdata, *popt), label='theoretical model')matplotlib.pyplot.errorbar(xdata, ydata, xerr=xerror, yerr=yerror, linestyle=, color='red')matplotlib.pyplot.xlabel(' observed values for x ')matplotlib.pyplot.ylabel(' f(x)')matplotlib.pyplot.legend(loc='lower right')matplotlib.pyplot.show()",
    "present_kp": [],
    "absent_kp": [
      "python",
      "graph",
      "statistics",
      "physics"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "systemd apt pinned to -1 and installed in upgrade from debian 8 to debian 9. while in the process of migrating some servers from my testing network, a few of them installed systemd; i was a bit surprised as i am using sysv, and have systemd pinned to -1.what happened?",
    "present_kp": [
      "debian",
      "apt"
    ],
    "absent_kp": [
      "dist upgrade"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "filter function implementations using copy_if algorithm vs. for loop. i have the following method of some class, which also defines the method isallowed:auto filter(const auto& in){ auto ret = decltype(in) {}; for(auto x : in) if(isallowed(x)) ret.insert(x); return ret;}this is a clear case where copy_if could be used instead. i see two alternative versions:auto filter(const auto& in){ auto ret = decltype(in) {}; copy_if(begin(in), end(in), inserter(ret, end(ret)), [this](auto i) {return this->isallowed(i);}); return ret;}orauto filter(const auto& in){ auto ret = decltype(in) {}; copy_if(begin(in), end(in), inserter(ret, end(ret)), bind(&a::isallowed, this, placeholders::_1)); return ret;}both seem much less obvious than the original, so i am inclined to keep the for loop. is there a strong argument not to? (and is there a better way?)otherwise, i feel itchy because cases like this point to the limited usefulness of the algorithm tools, despite what best practices advise.there are probably good arguments for choosing copy_if that are not being considered properly (performance?). this question is mainly after those, as the others seem more intuitive and easier to get.",
    "present_kp": [
      "algorithm"
    ],
    "absent_kp": [
      "c++",
      "comparative review",
      "c++14",
      "c++17"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "/etc/hosts in debian resets itself on reboot. i've been having a problem where when our vps provider decides to restart the server (running debian 5.0.8), the server fails to remember changes to /etc/hosts. all i need is an database alias that is used for the web applications on the server which points to 127.0.0.1 (localhost).i want it to look like this:# the following lines are desirable for ipv6 capable hosts# (added automatically by netbase upgrade)::1 ip6-localhost ip6-loopbackfe00::0 ip6-localnetff00::0 ip6-mcastprefixff02::1 ip6-allnodesff02::2 ip6-allroutersff02::3 ip6-allhosts127.0.0.1 localhost.localdomain localhost webservice database# auto-generated hostname. please do not remove this comment.xxx.xx.xxx.xx xxxxxx.net.au xxxxxx <url> xxxxxxxhowever whenever there is a reboot it resets itself to:# the following lines are desirable for ipv6 capable hosts# (added automatically by netbase upgrade)::1 ip6-localhost ip6-loopbackfe00::0 ip6-localnetff00::0 ip6-mcastprefixff02::1 ip6-allnodesff02::2 ip6-allroutersff02::3 ip6-allhosts127.0.0.1 localhost.localdomain localhost webservice# auto-generated hostname. please do not remove this comment.xxx.xx.xxx.xx xxxxxx.net.au xxxxxx <url> xxxxxxxwithout the database, and i have to manually change the file to get things to work. this has been happening for awhile and has become a nuisance, but i can't seem to find a way to get changes to stick. anyone know what to do?",
    "present_kp": [
      "debian",
      "alias",
      "hosts"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "what does analytics '% of total' mean?. i feel this may be a stupid question but i don't understand this:under total visits i have around 6,000 which is % of total 75.58% (8,000). what does this mean and why doesn't it just list 100%?",
    "present_kp": [],
    "absent_kp": [
      "google analytics"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "google's python exercise about lists very different from the given solution. disclaimer: although i do believe that this is somewhat a code review, i'm not sure if this kind of question applies to the site; if not, please let me know and i'll delete it promptly.i found some python exercises that were made by google in their python classes and decided to spend some time with them.given the following description:e. given two lists sorted in increasing order, create and return a merged ist of all the elements in sorted order. you may modify the passed in lists. deally, the solution should work in linear time, making a single pass of both lists.so, knowing that comparing two characters is \\$o(1)\\$, python's sorted() function in this situation is \\$o(n \\log{n})\\$ and thinking that merging two lists into a new one is \\$o(k)\\$ where \\$k\\$ the number of elements in list1 + list2, which is quite linear in the sense of what was asked, i did...def linear_merge(list1, list2): return sorted(list1 + list2)however, when looking at the problem's solution, i found it to be somewhat different:def linear_merge(list1, list2): result = [] # look at the two lists so long as both are non-empty. # take whichever element [0] is smaller. while len(list1) and len(list2): if list1[0] < list2[0]: result.append(list1.pop(0)) else: result.append(list2.pop(0)) # now tack on what's left result.extend(list1) result.extend(list2) return resultwhich is followed by the following comment:note: the solution above is kind of cute, but unforunately list.pop(0) is not constant time with the standard python list implementation, so the above is not strictly linear time. an alternate approach uses pop(-1) to remove the endmost elements from each list, building a solution list which is backwards. then use reversed() to put the result back in the correct order. that solution works in linear time, but is more ugly.this confused me a little bit, since my solution looks... better in general (code and complexity, given the last comment paragraph). are any of my assumptions about my version of the code wrong? keep in mind that i wrote this version using python3, when instead google's python classes uses python2. i'm not really sure, but this may have something to do with it.here is the rest of the related source to give a full example:def test(got, expected): if got == expected: prefix = ' ok ' else: prefix = ' x ' print('{} got: {} expected: {}'.format(prefix, repr(got), repr(expected)))# calls the above functions with interesting inputs.def main(): print('linear_merge') test(linear_merge(['aa', 'xx', 'zz'], ['bb', 'cc']), ['aa', 'bb', 'cc', 'xx', 'zz']) test(linear_merge(['aa', 'xx'], ['bb', 'cc', 'zz']), ['aa', 'bb', 'cc', 'xx', 'zz']) test(linear_merge(['aa', 'aa'], ['aa', 'bb', 'bb']), ['aa', 'aa', 'aa', 'bb', 'bb'])if __name__ == '__main__': main()",
    "present_kp": [
      "python",
      "complexity"
    ],
    "absent_kp": [
      "python 3.x",
      "python 2.7",
      "comparative review"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "association count in single query for activerecord. this is a small gem that gives the possibility to include counts of reflections when fetching active record objects. what do you think of the code? any improvements or other (similar) use cases where it might be useful?require 'active_record'activerecord::base.extend associationcountmodule associationcount def association_count(counted_model, distinct) table_name = self.table_name counted_table = counted_model.table_name counted_name = counted_table.singularize distinct_sql = distinct ? 'distinct' : '' joins(counted_table.to_sym) .select(#{table_name}.*, count(#{distinct_sql} #{counted_table}.id) as #{counted_name}_count_raw) .group(#{table_name}.id) end def can_count(model_name, opts = {}) model_name = model_name.to_s reflection = reflections[model_name] fail no such reflection: '#{model_name}' unless reflection options = { distinct: true }.merge!(opts) singular_name = model_name.singularize define_association_count_method(model_name, singular_name) define_count_scope(singular_name, reflection, options[:distinct]) end def define_association_count_method(model_name, singular_name) define_method #{singular_name}_count do raw_count_name = #{singular_name}_count_raw return send(raw_count_name) if self.respond_to?(raw_count_name) send(model_name).count end end def define_count_scope(singular_name, reflection, distinct) scope_name = include_#{singular_name}_count class_eval do scope scope_name, -> { association_count(reflection.klass, distinct) } end endend",
    "present_kp": [
      "sql",
      "active record"
    ],
    "absent_kp": [
      "ruby",
      "ruby on rails"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to deal with ad-hoc mindsets?. i joined a dev team of six two month ago. people are nice, all is good. but more and more i observe an ad-hoc mindset. stuff gets quick fixed, at the cost of future usability, there is little testing and two people happily admitted, that they like to carry the knowledge around in their head, rather than to write it down. how to deal with this? i'd like to lead by example, but time is limited - i like architecting and actually implementing the stuff. but i'm afraid the ad-hoc mindset infects me and rather than striving for clearness and simplicity in design and code - which isn't simple to establish - i get pulled down the drain of an endless spiral of hacks on hacks - which no outsider can uncouple - just for schedule's and management's sake.",
    "present_kp": [
      "management"
    ],
    "absent_kp": [
      "architecture",
      "coding style",
      "technical debt"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "sending html using mailx with postfix 2.6.6 and centos 6.4. i'm trying to send an html email from our centos server to users. in the long run i'll be building a cobol program that runs the command to send reports to the user. here are a few details. centos 6.4mau : mailx mta : postfix 2.6.6postfix is running a relay through an exchange server. as for what commands i've tried running. the one i've seen the most today has been the following.$ mailx -a 'content-type: text/html' -s command line test <email> < ./bodytext.htmlafter running the shown command, i get an error saying content-type: text/html: no such file or directory. i'm pretty sure that after a certain update they stopped allowing -a as a flag for content-type designation. i've also tried adding the 'content-type: text/html' to the actual bodytext.html file as the very first line. i'm kind of just at a loss for what i can do to send the html email. some of the sources i've found say that mailx and postfix can't properly send html emails. hopefully that's not the case, but if it is than i'd like to know what your take on other mau and mta technologies?",
    "present_kp": [
      "centos",
      "email",
      "postfix",
      "mailx"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "optimal linux distro to install to sd card to use as usb-bootloader for pc?. (copied from andoid stackexchange)many people install linux-based distros on usb drives to use for, largely, basic pc troubleshooting. has anyone tried installing such a thing on their android phone's sd card? that way instead of carrying about both a usb drive and a phone, one could simply plug their phone in, boot the pc from usb, and set about performing whatever actions are necessary. does anyone have any recommendations for particular flavors of linux which are best suited to this task?",
    "present_kp": [
      "linux",
      "usb"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "algorithms for text clustering. i have a problem of clustering huge amount of sentences into groups by their meanings. this is similar to a problem when you have lots of sentences and want to group them by their meanings.what algorithms are suggested to do this? i don't know number of clusters in advance (and as more data is coming clusters can change as well), what features are normally used to represent each sentence?i'm trying now the simplest features with just list of words and distance between sentences defined as:(a and b are corresponding sets of words in sentence a and b)does it make sense at all? i'm trying to apply mean-shift algorithm from scikit library to this distance, as it does not require number of clusters in advance.if anyone will advise better methods/approaches for the problem - it will be very much appreciated as i'm still new to the topic.",
    "present_kp": [
      "clustering",
      "algorithms"
    ],
    "absent_kp": [
      "text mining",
      "scikit learn"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "selectively copy from a collection of remote directories. i have a remote machine with a large number of numbered directories, like so:dir1 dir2 dir3 ... dir40each of which contain several numbered files:file1 file2 file3 ... file2530i want to copy only a selected range of the files in each directory. since the files' names are identical in each directory, i want to re-create the directory hierarchy on my local machine. but since i don't want every file, i can't just use scp -r to copy every file in the directory.i can't set up an automated connection with ssh keys on the remote machine, so i would prefer a method that doesn't involve repeated calls to a remote copy command. the files are also big, so i don't want to just copy the whole thing over and delete the ones i don't want with rm and brace expansion.how can i copy a set of files from a remote machine, along with those files' parent directories, while preserving the directory structure and without copying every file in those directories?",
    "present_kp": [
      "scp",
      "remote"
    ],
    "absent_kp": [
      "rsync",
      "file copy"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how bad is cloaking for my site's search engine rankings?. my website is very javascript-heavy. all the content of the site is loaded asynchronously using javascript.so my developers implemented cloaking - server sends a static version of the webpage if it's requested by googlebot. i recently found out that is called cloaking and considered a black hat seo technique.i'm wondering what are the consequences? is there any way to measure what is the effect of using using this technique to site's search rankings?",
    "present_kp": [
      "seo",
      "cloaking"
    ],
    "absent_kp": [
      "search engines"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "mdadm raid - restart during grow (shrink) reshape - not working (stuck). i have a linux software raid6 array (mdadm). i grew it from 6x4tb disks (16tb usable) to 7x4tb (20tb usable). the reshape went fine, but when i did resize2fs, i got the fairly well known issue of the ext4 16tb filesystem limit. i checked and the filesystem does not have the 64 bit flag. so in an effort to reclaim the extra drive i had just added to the array, i did this:johnny@debian:~$ sudo resize2fs /dev/md0 16000gjohnny@debian:~$ sudo mdadm --grow /dev/md0 --array-size=16000gjohnny@debian:~$ sudo mdadm --grow /dev/md0 --raid-devices=6 --backup-file=/tmp/backupnotice the backup-file location. that's going to be important in a minute, because i'm on debian.so things were going fine, slow but working. the progress got to 3.7% and it had slowed to a crawl. i had assumed this was because i was reshaping a few other arrays during this same time. when those other jobs finished and this one didn't speed up, i got really worried. since it said it would take years to finish, i decided i should restart and see if it would speed up, so i restarted the system.this is when bad things start happening...i'm on debian, and it is my understanding that the /tmp folder is wiped out when the system comes up, so my backup-file from the reshape was lost. also, because my /etc/fstab file was trying to mount md0, which wasn't assembling now, the system failed to come up a few times. i started from a live cd and fixed the fstab file and got the system to come back up.once i sorted that out, the system was up, and that was the first time i saw that md0 had not simply assembled itself and continued reshaping. panic set in...i don't have the output of the following commands, but i managed to find the commands i typed in. brief explanation of what happened to follow...johnny@debian:~$ sudo mdadm --assemble /dev/md0johnny@debian:~$ sudo mdadm --assemble --force /dev/md0johnny@debian:~$ sudo mdadm --assemble --force /dev/md0 --backup-file=/tmp/backupthe first command failed, so i tried the --force option, which also failed, but the error message told me the failure was because it needed the --backup-file option, so i ran the third command. i expected the backup file to still exist, but it didn't because it was in the /tmp folder and had been deleted. this didn't seem to cause any problems though, because the array assembled.here is what md0 looks like now. notice the disk marked removed. i suspect this is the disc that was being removed, sdj1.johnny@debian:~$ sudo mdadm --detail /dev/md0/dev/md0: version : 1.2 creation time : fri jan 11 09:59:42 2013 raid level : raid6 array size : <phone> (14903.59 gib 16002.61 gb) used dev size : <phone> (3725.90 gib 4000.65 gb) raid devices : 6 total devices : 6 persistence : superblock is persistent intent bitmap : internal update time : sat mar 5 20:45:56 2016 state : clean, degraded, reshaping active devices : 6working devices : 6 failed devices : 0 spare devices : 0 layout : left-symmetric chunk size : 512k reshape status : 3% complete delta devices : -1, (7->6) name : bigraid6 uuid : 45747bdc:ba5a85fe:ead35e14:24c2c7b2 events : <phone> number major minor raiddevice state 11 8 224 0 active sync /dev/sdo 2 0 0 2 removed 6 8 80 2 active sync /dev/sdf 7 8 176 3 active sync /dev/sdl 12 8 16 4 active sync /dev/sdb 8 8 32 5 active sync /dev/sdc 9 8 128 6 active sync /dev/sdiand here is the current progress of the reshape. notice it's completely stuck at 0k/sec.johnny@debian:~$ cat /proc/mdstatpersonalities : [raid1] [raid6] [raid5] [raid4] md0 : active raid6 sdo[11] sdi[9] sdc[8] sdb[12] sdl[7] sdf[6] <phone> blocks super 1.2 level 6, 512k chunk, algorithm 2 [6/5] [u_uuuu] [>....................] reshape = 3.7% (1455<phone>8) finish=284022328345.0min speed=0k/sec bitmap: 5/30 pages [20kb], 65536kb chunkunused devices: <none>here are the individual discs still in the array.johnny@debian:~$ sudo mdadm --examine /dev/sd[oflbci]/dev/sdb: magic : a92b4efc version : 1.2 feature map : 0x5 array uuid : 45747bdc:ba5a85fe:ead35e14:24c2c7b2 name : bigraid6 creation time : fri jan 11 09:59:42 2013 raid level : raid6 raid devices : 6 avail dev size : <phone> (3725.90 gib 4000.65 gb) array size : <phone> (14903.59 gib 16002.61 gb) used dev size : <phone> (3725.90 gib 4000.65 gb) data offset : 262144 sectors super offset : 8 sectors unused space : before=262064 sectors, after=688 sectors state : clean device uuid : 99b0fbcc:46d619bb:9ae96eaf:840e21a4internal bitmap : 8 sectors from superblock reshape pos'n : <phone> (14348.28 gib 15406.34 gb) delta devices : -1 (7->6) update time : sat mar 5 20:45:56 2016 checksum : fca445bd - correct events : <phone> layout : left-symmetric chunk size : 512k device role : active device 4 array state : a.aaaaa ('a' == active, '.' == missing, 'r' == replacing)/dev/sdc: magic : a92b4efc version : 1.2 feature map : 0x5 array uuid : 45747bdc:ba5a85fe:ead35e14:24c2c7b2 name : bigraid6 creation time : fri jan 11 09:59:42 2013 raid level : raid6 raid devices : 6 avail dev size : <phone> (3725.90 gib 4000.65 gb) array size : <phone> (14903.59 gib 16002.61 gb) used dev size : <phone> (3725.90 gib 4000.65 gb) data offset : 262144 sectors super offset : 8 sectors unused space : before=262064 sectors, after=688 sectors state : clean device uuid : b8d49170:06614f82:ad9a38a4:e9e06da5internal bitmap : 8 sectors from superblock reshape pos'n : <phone> (14348.28 gib 15406.34 gb) delta devices : -1 (7->6) update time : sat mar 5 20:45:56 2016 checksum : 5d867810 - correct events : <phone> layout : left-symmetric chunk size : 512k device role : active device 5 array state : a.aaaaa ('a' == active, '.' == missing, 'r' == replacing)/dev/sdf: magic : a92b4efc version : 1.2 feature map : 0x5 array uuid : 45747bdc:ba5a85fe:ead35e14:24c2c7b2 name : bigraid6 creation time : fri jan 11 09:59:42 2013 raid level : raid6 raid devices : 6 avail dev size : <phone> (3725.90 gib 4000.65 gb) array size : <phone> (14903.59 gib 16002.61 gb) used dev size : <phone> (3725.90 gib 4000.65 gb) data offset : 262144 sectors super offset : 8 sectors unused space : before=262064 sectors, after=688 sectors state : clean device uuid : dd56062c:4b55bf16:6a468024:3ca6bfd0internal bitmap : 8 sectors from superblock reshape pos'n : <phone> (14348.28 gib 15406.34 gb) delta devices : -1 (7->6) update time : sat mar 5 20:45:56 2016 checksum : 59045f87 - correct events : <phone> layout : left-symmetric chunk size : 512k device role : active device 2 array state : a.aaaaa ('a' == active, '.' == missing, 'r' == replacing)/dev/sdi: magic : a92b4efc version : 1.2 feature map : 0x5 array uuid : 45747bdc:ba5a85fe:ead35e14:24c2c7b2 name : bigraid6 creation time : fri jan 11 09:59:42 2013 raid level : raid6 raid devices : 6 avail dev size : <phone> (3725.90 gib 4000.65 gb) array size : <phone> (14903.59 gib 16002.61 gb) used dev size : <phone> (3725.90 gib 4000.65 gb) data offset : 262144 sectors super offset : 8 sectors unused space : before=262064 sectors, after=688 sectors state : clean device uuid : 92831abe:86de117c:710c368e:8badcef3internal bitmap : 8 sectors from superblock reshape pos'n : <phone> (14348.28 gib 15406.34 gb) delta devices : -1 (7->6) update time : sat mar 5 20:45:56 2016 checksum : dd2fe2d1 - correct events : <phone> layout : left-symmetric chunk size : 512k device role : active device 6 array state : a.aaaaa ('a' == active, '.' == missing, 'r' == replacing)/dev/sdl: magic : a92b4efc version : 1.2 feature map : 0x5 array uuid : 45747bdc:ba5a85fe:ead35e14:24c2c7b2 name : bigraid6 creation time : fri jan 11 09:59:42 2013 raid level : raid6 raid devices : 6 avail dev size : <phone> (3725.90 gib 4000.65 gb) array size : <phone> (14903.59 gib 16002.61 gb) used dev size : <phone> (3725.90 gib 4000.65 gb) data offset : 262144 sectors super offset : 8 sectors unused space : before=262064 sectors, after=688 sectors state : clean device uuid : 8404647a:b1922fed:acf71f64:18dfd448internal bitmap : 8 sectors from superblock reshape pos'n : <phone> (14348.28 gib 15406.34 gb) delta devices : -1 (7->6) update time : sat mar 5 20:45:56 2016 checksum : 358734b4 - correct events : <phone> layout : left-symmetric chunk size : 512k device role : active device 3 array state : a.aaaaa ('a' == active, '.' == missing, 'r' == replacing)/dev/sdo: magic : a92b4efc version : 1.2 feature map : 0x5 array uuid : 45747bdc:ba5a85fe:ead35e14:24c2c7b2 name : bigraid6 creation time : fri jan 11 09:59:42 2013 raid level : raid6 raid devices : 6 avail dev size : <phone> (3725.90 gib 4000.65 gb) array size : <phone> (14903.59 gib 16002.61 gb) used dev size : <phone> (3725.90 gib 4000.65 gb) data offset : 262144 sectors super offset : 8 sectors unused space : before=262064 sectors, after=688 sectors state : clean device uuid : d7e84765:86fb751a:466ab0de:c26afc43internal bitmap : 8 sectors from superblock reshape pos'n : <phone> (14348.28 gib 15406.34 gb) delta devices : -1 (7->6) update time : sat mar 5 20:45:56 2016 checksum : c3698023 - correct events : <phone> layout : left-symmetric chunk size : 512k device role : active device 0 array state : a.aaaaa ('a' == active, '.' == missing, 'r' == replacinghere is /dev/sdj1, which used to be the only member of the array that was not a whole disk member. this was the one being removed from the array during the reshape. i suspect it is still needed to finish the reshape although it is not currently a member of the array because it has the data on it from before the reshape.johnny@debian:~$ sudo mdadm --examine /dev/sdj1mdadm: no md superblock detected on /dev/sdj1.so here are my problems...1. i can't get the reshape to finish.2. i can't mount the array. when i try, i get this.johnny@debian:~$ sudo mount /dev/md0 /media/bigraid6mount: wrong fs type, bad option, bad superblock on /dev/md0, missing codepage or helper program, or other error in some cases useful info is found in syslog - try dmesg | tail or so.johnny@debian:~$ sudo dmesg | tail[<phone>] sd 15:0:0:0: [sdk] [<phone>] add. sense: unrecovered read error - auto reallocate failed[<phone>] sd 15:0:0:0: [sdk] cdb: [<phone>] read(10): 28 00 89 10 bb 00 00 04 00 00[<phone>] end_request: i/o error, dev sdk, sector <phone>[<phone>] ata16: eh complete[<phone>] md: md1: data-check done.[<phone>] ext4-fs (md0): bad geometry: block count <phone> exceeds size of device <phone> blocks)[<phone>] ext4-fs (md0): bad geometry: block count <phone> exceeds size of device <phone> blocks)[<phone>] ext4-fs (md0): bad geometry: block count <phone> exceeds size of device <phone> blocks)i'm sure mounting would succeed if the reshape was finished, so that's probably what is most important. just fyi, the data on this array is too big to be backed up, so if i lose it, the data is gone. please help!edit 1:i could shell out $1000 (or more) and get enough disks to copy everything over, but i'd need to be able to mount the array for that to work.also, i just noticed that the bad geometry error message i get when trying to mount the array has some interested info in it.[1<phone>] ext4-fs (md0): bad geometry: block count <phone> exceeds size of device <phone> blocks)the size of device, <phone>, is exactly 1/4 of the array size of md0, <phone>. from mdadm --detail /dev/md0array size : <phone> (14903.59 gib 16002.61 gb)i don't know where the <phone> number is coming from... but doesn't that mean the array is the right size to fit on these disks? or do these sizes not account for the mdadm metadata? is <phone> including the metadata perhaps?i could swear i tried a few times to get the sizes right before the reshape would even start, so i thought everything was good to go. maybe i was wrong.",
    "present_kp": [
      "debian",
      "raid",
      "mdadm",
      "software raid"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "is there a protocol for identifying a users's device?. a user connects to my web application,i grab some sort of device id through w3c standard protocols,the user logs in,i can then identify this specific unique device owned by the user (not just ipad - i'm looking for ipad3_2c38ty43058 or even just john's ipad).this would let me design my web application to treat logins from this device in a specific way, know which of john's 5 devices he's logged in from, etc. is there such a protocol? is there any method of identify a specific device as described?",
    "present_kp": [],
    "absent_kp": [
      "web applications"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to show search results for all open buffers. one thing i often do (but rely on the command line to do) is searching/grepping in multiple files.is there a way to display search results for all the open buffers?ideally, i'd like to have a new split buffer containing result location and snippet much like grep does. for example (/statistics):models/statistics.php: /*! ile statistics.phpcontrollers/stats.php: $this->load->model('statistics');controllers/stats.php: // query statistics...controllers/stats.php: $num_exams = $this->statistics->countexams();as a bonus, i'd like to be able to search for the term under the cursor, pretty much like gd does.",
    "present_kp": [
      "search",
      "multiple files"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "why are machine learning models called black boxes?. i was reading this blog post titled: the financial world wants to open ais black boxes, where the author repeatedly refer to ml models as black boxes. a similar terminology has been used at several places when referring to ml models. why is it so?it is not like the ml engineers don't know what goes on inside a neural net. every layer is selected by the ml engineer knowing what activation function to use, what that type of layer does, how the error is back propagated, etc.",
    "present_kp": [
      "machine learning",
      "terminology"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "[sdc] assuming drive cache: write through need to hide this message!. when you plug an external drive into my ubuntu server this message pops up [sdc] assuming drive cache: write through...i would really like it so it stops sending it to the screen. any ideas on how i can shut it up? :)",
    "present_kp": [
      "ubuntu"
    ],
    "absent_kp": [
      "linux",
      "shell",
      "kernel"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "filter post and route them on a tab in a facebook page. i use dlvr.it to route feeds from many sites on my facebook page. the problem is that they all appear in my main page, hiding the post i create.so i was wondering, is there a way to redirect all those post on a separate tab? maybe some kind of filtering based on the content of the post?thanks a lot",
    "present_kp": [
      "facebook",
      "feeds"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "project euler 28 - number spiral diagonals. project euler problem 28starting with the number 1 and moving to the right in a clockwise direction a 5 by 5 spiral is formed as follows:$$egin{array}{rrrrr}\\color{red}{21} &22 &23 &24 &\\color{red}{25}\\20 & \\color{red}{7} & 8 & \\color{red}{9} &10\\19 & 6 & \\color{red}{1} & 2 &11\\18 & \\color{red}{5} & 4 & \\color{red}{3} &12\\\\color{red}{17} &16 &15 &14 &\\color{red}{13}\\\\end{array}$$it can be verified that the sum of the numbers on the diagonals is 101.what is the sum of the numbers on the diagonals in a 1001 by 1001 spiral formed in the same way?i realized that the spiral was essentially an arithmetic sequence, so i based my code off of that.from timeit import default_timer as timerstart = timer()def spiral_diag_sum(n): if n < 1: return none elif n == 1: return 1 elif n % 2 == 0: return none else: numbers = [1] while len(numbers) < (2*n - 1): increment = int(len(numbers) * 0.5 + 1.5) for p in range(4): numbers.append(numbers[-1] + increment) return sum(numbers)ans = spiral_diag_sum(1001)elapsed_time = (timer() - start) * 1000 # s --> msprint found %d in %r ms. % (ans, elapsed_time)",
    "present_kp": [],
    "absent_kp": [
      "python",
      "programming challenge"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to represent complex permissions system in single hash or set of characters?. eve online, a game i used to play, had an interesting permissions system for account interactions with other apps.the permissions encompassed dozens of individual permissions, each under a category (which may or may not have been relevant to the resulting hash)the interesting thing is that the entire permission state of the account could be saved as one resulting string, or hash, which translated in turn, back to a detailed set of individual permissions. what algorithm or system is responsible for such an ingenious and compressed way of signifying permissions?",
    "present_kp": [
      "permissions"
    ],
    "absent_kp": [
      "algorithms"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how should i name a function which shows or hides an element?. i have a function to display or hide some elements. what i want to do is to manage the display of those elements regarding a value.how should i replace the term toggle? toggling for me means 'show the element if it's hidden, or hide it if it shown.' (yes, i mean toggling visibility)function togglepassengers() { var nbpassengers = $(#nbpassengersfortravel).val(), $passengers = $('#d_passenger').find('fieldset'); $passengers.each( togglepassenger ); function togglepassenger(i){ $passenger = $(this); if( ++i <= nbpassengers ) showpassenger($passenger); else hidepassenger($passenger); } function showpassenger($passenger){ $passenger .removeclass(hidden) .find(select.age option[value='-1']).remove(); } function hidepassenger($passenger){ $passenger.addclass(hidden); var $passengerage = $passenger.find(select.age); if ( ! $passengerage.find(option[value='-1']:first-child).length ) $passengerage.prepend(<option value='-1'>---------</option>).val('-1'); }}",
    "present_kp": [],
    "absent_kp": [
      "javascript",
      "jquery"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is blocking java applets a necessary security measure?. chrome 11 is now asking user permission to run both signed and unsigned applets (yes, for signed applets the user is asked twice). chromium team decided that this measure is needed even when the user is using an up-to-date jre.here's my bug report (which reflects solely my opinion: <url>).my question is, how do you guys see it? is java sandbox dated and unsafe? do browsers need to impose a second layer of protection by default?update: i'm also curious about how many of you guys have a clean record experience with java against how many every hit a piece of malicious software? as a java power user for more than 10 years, the only time my antivirus ever complained about something related to java was a false positive (i was downloading some libraries from maven central repository).",
    "present_kp": [
      "java",
      "security",
      "chrome"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "howto configure autofs to work with usbsticks formatted with differend filesystems. i have some usb-sticks using fat32 as filesystem and others using ext4. now i want to configure autofs to mount the sticks automatically. to make sure that the user can write to fat32 sticks i use the umask option in my /etc/auto.misc file. however (according to the mount man page), this option is not available for ext4 file systems and when i try to automount an ext4 stick this way i get an error message (via dmesg | tail) which tells me that umask is an unrecognized mount option and the mount operation fails. if i omit the umask option, the ext4 stick works as expected but also as expected the fat32 stick is only writable by root. is there any way to make autofs work for both types of sticks? is it possible to use somehow an if condition to decide whether to mount it with umask or not?i studied the mount man page to solve the problem and discovered the mount parameter -s (sloppy), but i don't know how to apply this for autofs and it did not even work with normal mount command and umask (mount -s -o umask=000 /dev/sdb1 /media/mymountpoint).as expected: same problem with utf8 mount option.",
    "present_kp": [
      "mount",
      "ext4",
      "autofs",
      "fat"
    ],
    "absent_kp": [
      "automounting"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is mch_sskpd warning in dmesg?. while i was reading dmesg log just to check that everything is fine, i met [ 18.956187] [drm] wrong mch_sskpd value: 0x16040307[ 18.956190] [drm] this can cause pipe underruns and display issues.[ 18.956192] [drm] please upgrade your bios to fix this.looks that it does not cause problems on my laptop, but what does this message stands for? what can it cause? where can i read more about mch_sskpd?",
    "present_kp": [
      "dmesg",
      "drm"
    ],
    "absent_kp": [
      "kernel"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "improving performance for depth-first search algorithm. i am using the following depth-first search algorithm to compute value of a property called rotation_absolute based on previous values of rotation of parent elements recursively.the result of the script is correct (open console to see the result) but i would like your code review regarding performance, considering that app.data could contained thousands of objects... how could i further improve my code?notes:i am targeting latest versions of chrome and firefox. var app = { getbyid: function (id) { var result; this.data.some(function (item) { if (item.id === id) { result = item; return true; } else { return false; } }); return result; }, findchildren: function (id) { var result = this.data.filter(function (item) { if (item.parent === id) { return true; } else { return false; } }); return result; }, data: [ { id: 'root', parent: '', rotation: 0, rotation_absolute: 0 }, { id: 'a', parent: 'root', rotation: 10 }, { id: 'a-a', parent: 'a', rotation: 10 }, { id: 'a-b', parent: 'a', rotation: 10 }, { id: 'a-b-a', parent: 'a-b', rotation: 10 }, { id: 'a-b-a-a', parent: 'a-b-a', rotation: 10 }, { id: 'a-b-a-a-a', parent: 'a-b-a-a', rotation: 10 }, { id: 'a-b-a-b', parent: 'a-b-a', rotation: 10 }, { id: 'b', parent: 'root', rotation: 0 }, { id: 'b-a', parent: 'b', rotation: 10 }, { id: 'b-a-a', parent: 'b-a', rotation: 10 }, { id: 'b-a-a-a', parent: 'b-a-a', rotation: 10 }, { id: 'b-b', parent: 'b', rotation: 10 }, ], calculate: function () { var recursion = function (id) { var children = this.findchildren(id); var parent = this.getbyid(id); children.foreach(function (item) { if ('rotation_absolute' in item === false) { item.rotation_absolute = 0; } item.rotation_absolute += item.rotation + parent.rotation_absolute; recursion.call(this, item.id) }, this); }; recursion.call(this, 'root') }, }; app.calculate(); console.clear(); console.log(json.stringify(app.data));",
    "present_kp": [
      "algorithm"
    ],
    "absent_kp": [
      "javascript"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "list all the files in ending with several file extensions?. if i want files ending with .fas i know i can do the following with ls command :ls -l /users/jenna/bio/*.fasand this lists out all the files ending with .fas. but how can i get files ending with either .fas or .pep ?ls -l /users/jenna/bio/(*.fas | *.pep) doesn't seem to work ? i know there might be other commands that can do this, but is there a way to make it work with ls ? thanks for any help !",
    "present_kp": [
      "files",
      "ls"
    ],
    "absent_kp": [
      "directory"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how is username used in evernote?. i'm trying to decide what kind of username to create in evernote. how is the username actually used throughout the application? is it ever shown publicly? if so, in what contexts?",
    "present_kp": [
      "evernote",
      "username"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "when can i use a temporary ifs for field splitting?. in bash, say you have var=a.b.c., then:$ ifs=. printf %s $vara.b.chowever, such a usage of ifs does take effect while creating an array:$ ifs=. arr=($var)$ printf %s ${arr[@]}abcthis is very convenient, sure, but where is this documented? a quick reading of the sections on arrays or word splitting in the bash documentation does not give any indication either way. a search for ifs through the single-page documentation doesn't provide any hints about this effect either.i'm not sure when i can reliably do:ifs=x do somethingand expect that ifs will affect field splitting.",
    "present_kp": [
      "bash"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "computational power of cellular neural networks. a cellular neural network is a kind of recurrent neural network that could be thought of as a hybrid between neural nets and cellular automata.as i understand it, the classic chua-yang cnn is limited to a fairly small set of functions, but addition of a second layer or a different choice of nonlinearity expands the function set to all computable. this sounds very similar to traditional neural networks, where kolmogorov's 1957 theorem says that a two-layer net can represent any continuous function arbitrariliy close.however, there is also the significant caveat that traditional shallow neural nets are very inefficient at representing complicated functions. do universal cnns have a similar weakness? do they require excessively large network sizes or take too long to converge when used for complicated function computation?",
    "present_kp": [],
    "absent_kp": [
      "ai.artificial intel",
      "ne.neural evol"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "capturing keyboard events for a limited time. i'm trying to code a kind of simple video game where there are two kind of players:human players: they enter an keyboard inputcpu players: a random input is calculatedfor human players there is a t round time and a t' input entering time. let's say, if t=20 and t'=5, a player has 20 seconds to start typing, but only 5 seconds of typing time. so, if the player starts typing at t=10, will only be able to type those t'=5 seconds until t=15.just so you know, i'm using typescript, and i had something like this in mind:while (!gameover) { // do some stuff like knowing who's player turn is it var enteredinput; if (player.ishuman()) { enteredinput = capturekeyboardinputsfor(seconds); } else { enteredinput = calculaterandomcpuplayerinput(); } // process input & do actions}what i would like to achieve here is a kind of synchronous typing process, i have the game logic well defined in mind, but i'm trying to figure out how can i do this keeping my loop game synchronous (as the players play in turns), using typescript.i would really appreciate any kind of guidance, alternatives or ideas of possible ways of solving my concerns.",
    "present_kp": [
      "typescript"
    ],
    "absent_kp": [
      "javascript",
      "programming practices",
      "event programming",
      "asynchronous programming"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "refactoring multiple if statements where switch cannot be used. i want to refactor below code.do() method below gets the position number from other component _calcomponent.getposition() and compares the value with the pf control positions (pfcontrolpositions).as you can see there are many if conditions that i had to use here. i did not like this approach. i am not able to write the switch case also because the position value comes dynamically and hence switch case throws compile time error. i have a constraint that i can only modify dosomething class.can you please suggest?public interface icalcomponent{ int getposition();}public class dosomething{ private readonly icalcomponent _calcomponent; public dosomething(icalcomponent calcomponent) { _calcomponent = calcomponent; } public void do() { var functionality = new dictionary<controlaction, controlfunctionality>(); int number = _calcomponent.getposition(); if (number == pfcontrolpositions.pf1positionoff) { functionality[controlaction.medium] = controlfunctionality.doaction1; } if (number == pfcontrolpositions.pf1positionon) { functionality[controlaction.medium] = controlfunctionality.doaction2; } if (number == pfcontrolpositions.pf1positionon) { functionality[controlaction.medium] = controlfunctionality.doaction3; } if (number == pfcontrolpositions.pf2positionoff) { functionality[controlaction.medium] = controlfunctionality.doaction4; } if (number == pfcontrolpositions.pf1positionon) { functionality[controlaction.medium] = controlfunctionality.doaction5; } if (number == pfcontrolpositions.pf1positionoff) { functionality[controlaction.medium] = controlfunctionality.doaction6; } if (number == pfcontrolpositions.pf2positionon) { functionality[controlaction.medium] = controlfunctionality.doaction7; } if (number == pfcontrolpositions.pf2positionoff) { functionality[controlaction.medium] = controlfunctionality.doaction8; } if (number == pfcontrolpositions.pf3positionon) { functionality[controlaction.medium] = controlfunctionality.doaction9; } if (number == pfcontrolpositions.pf3positionoff) { functionality[controlaction.medium] = controlfunctionality.doaction10; } }}public class pfcontrolpositions{ public static controlposition pf1positionoff = new controlposition(0); public static controlposition pf1positionon = new controlposition(1); public static controlposition pf2positionon = new controlposition(2); public static controlposition pf2positionoff = new controlposition(3); public static controlposition pf3positionon = new controlposition(4); public static controlposition pf3positionoff = new controlposition(5); public static controlposition pf4positionon = new controlposition(6); public static controlposition pf4positionoff = new controlposition(7); public static controlposition pf5positionon = new controlposition(8); public static controlposition pf5positionoff = new controlposition(9);}public class controlposition { private readonly uint32 _physicalposition; public controlposition(uint32 physicalstepposition) { _physicalposition = physicalstepposition; } public static implicit operator int(controlposition position) { return (int)position._physicalposition; }}public enum controlfunctionality{ doaction1, doaction2, doaction3, doaction4, doaction5, doaction6, doaction7, doaction8, doaction9, doaction10,}public enum controlaction{ dial = 0, short = 1, medium = 2}thank you very much !!",
    "present_kp": [],
    "absent_kp": [
      "c#"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to create tar archive in a different directory?. i want to create a tar archive in a different directory rather than the current directory.i tried this command:tar czf file.tar.gz file1 -c /var/www/but it creates the archive in the current directory. why?",
    "present_kp": [
      "tar"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "reformatting and version control. code formatting matters. even indentation matters. and consistency is more important than minor improvements. but projects usually don't have a clear, complete, verifiable and enforced style guide from day 1, and major improvements may arrive any day. maybe you find thatselect id, name, addressfrom persons join addresses on persons.id = addresses.person_id;could be better written as / is better written thanselect persons.id, persons.name, addresses.address from persons join addresses on persons.id = addresses.person_id;while working on adding more columns to the query. maybe this is the most complex of all four queries in your code, or a trivial query among thousands. no matter how difficult the transition, you decide it's worth it. but how do you track code changes across major formatting changes? you could just give up and say this is the point where we start again, or you could reformat all queries in the entire repository history.if you're using a distributed version control system like git you can revert to the first commit ever, and reformat your way from there to the current state. but it's a lot of work, and everyone else would have to pause work (or be prepared for the mother of all merges) while it's going on. is there a better way to change history which gives the best of all results:same style in all commitsminimal merge work?to clarify, this is not about best practices when starting the project, but rather what should be done when a large refactoring has been deemed a good thing\u2122\ufe0f but you still want a traceable history? never rewriting history is great if it's the only way to ensure that your versions always work the same, but what about the developer benefits of a clean rewrite? especially if you have ways (tests, syntax definitions or an identical binary after compilation) to ensure that the rewritten version works exactly the same way as the original?",
    "present_kp": [
      "version control",
      "indentation"
    ],
    "absent_kp": [
      "coding style"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "duplicate content inside iframe on other domain - is it bad?. my website has reviews pages for cars.some other website wants to show the reviews in his website too.the implementation is an iframe on his website loading the an url in my website with the review. under the iframe (not inside it), there is a link to my website for the full page with the reviews.is it ok with google? are there better implementations?",
    "present_kp": [
      "iframe"
    ],
    "absent_kp": [
      "seo"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "a more efficient way to make jql (jira query language). so my code here takes some parameters and then creates an sqlesque type of string (jql). function makejql(status, project, priority, description, summary){ var arr = new array(); // could put a null check here if all parametres are undefined if(status.length > 0){ jql = status=+''+status+''; arr.push(jql) } if(project.length > 0){ jql = project=+''+project+''; arr.push(jql) } if(priority.length > 0){ jql = priority=+''+priority+''; arr.push(jql) } if(description.length > 0){ jql = description=+''+description+''; arr.push(jql) } if(summary.length > 0){ jql = summary=+''+ summary +''; arr.push(jql) } var jql = ; for(var i = 0; i < arr.length; i++){ jql = jql + arr[i]+ ' and ' } jql = jql.substring(0, jql.length - 5); return jql}the only thing is, is that there is a lot of code repetition, but i don't know how to overcome this really. i also haven't put a null check in because if the function is called, there has to be parameters passed.",
    "present_kp": [],
    "absent_kp": [
      "javascript"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "documentation for transfrag class codes (cuffcompare). is there any extensive documentation or description for each class of transfrag class codes as reported by the cuffcompare tool in the cufflinks package?official doc might not be the best. e.g. what does contained mean (class code: c)? or, what is a generic exonic overlap (class code: o)?",
    "present_kp": [],
    "absent_kp": [
      "transcriptome"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "merge 2 files in one with a customized line in between. i have 2 files a and b. i want to merge them in file c with a customized line in between.file a:hellofile b:hithe merged file should contain:records in file a is hellorecords in file b is hi",
    "present_kp": [],
    "absent_kp": [
      "bash",
      "text processing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "redistribution of linux distribution?. if a linux distribution is under gpl, and i would redistribute it, is it correct that i only need to be able to give the sources? for example, say i build a 4-in-1 multi distribution dvd (i got sick of burning 4 separate discs so i made it multi-bootable). could i distribute it for others to use without facing a lawsuit?",
    "present_kp": [
      "gpl"
    ],
    "absent_kp": [
      "distributions"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "language and automata textbook, free or low cost?. i'll be teaching a standard undergraduate class on languages and automata next semester, and would prefer to use a legitimate free or low-cost text. any suggestions?i love the sipser text but the latest edition costs $196, which is hard to say with a straight face in the age of free courses.",
    "present_kp": [
      "teaching"
    ],
    "absent_kp": [
      "soft question",
      "books"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "can i scrape a website for font stylings?. i am trying to scrape websites for valuable text for example the title of an article, the author's name, and other distinguished text. i cannot always guarantee that this sort of text will have informative tags, and but this needs to be done as quickly as possible. as a possible short cut i think that i could just try to pull the text with unique styling. the title is normally bigger than the body text, and the by-line is normally smaller. is there a way to quickly pull all of the font stylings of a page and then rank them by size and how often it is used?",
    "present_kp": [],
    "absent_kp": [
      "html",
      "css",
      "web scraping",
      "fonts"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what features will be restricted for dropbox free users after march 15, 2017?. actual news is here: <url> didn't understand the news.i contacted them but they are asking me to submit ticket. i submitted ticket 10 days back, still no reply.after march 15, will i be able to use the share option which is located beside a file?please help me. i am dependent on dropbox.i daily share files in dropbox. i feel dropbox is more comfortable than google drive or other cloud services. i am a free user of dropbox.",
    "present_kp": [
      "dropbox"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "setup google analytics's to track image or page views on another site. scenariosite (a) which i control and havegoogle analytics setup on.site (b) which is controlled by a 3rdparty but we are allowed to customise html including images/javascript (e.g. something like a directory page)is it possible to use google analytics to track visitor usage on site (b) alongside site (a) ? - e.g. we can already see how many people visited from b but we can't see how many viewed so can't work out ctr.is this possible with google analytics?if not, is this possible with any other hosted analytics package or is it back to images and log file analysis?",
    "present_kp": [
      "google analytics"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "running programs backwards. i never used a debugger that can run a program backwards but i would like to. now i wonder if there is much evidence and theory about backwards runnable programming, when and why a program can be run backwards or when and why not?intuitively i find that the computer should be able to run the program backwards, if it once has run the program forward knowing that the program will run backwards hence saving states of the program that otherwise might get lost and deallocated. debugging is just a practical example. i'm also interested in it for the sake of the theory of reversibility, if the program can be run backwards, then what the program does is reversible, isn't it?",
    "present_kp": [],
    "absent_kp": [
      "automata",
      "programming languages"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to handle fine grained field-based acl permissions in a restful service?. i've been trying to design a restful api and have had most of my questions answered, but there is one aspect of permissions that i'm struggling with.different roles may have different permissions and different representations of a resource. for example, an admin or the user himself may see more fields in his own user representation vs another less-privileged user. this is achieved simply by changing the representation on the backend, ie: deciding whether or not to include those fields.additionally, some actions may be taken on a resource by some users and not by others. this is achieved by deciding whether or not to include those action items as links, eg: edit and delete links. a user who does not have edit permissions will not have an edit link.that covers nearly all of my permission use cases, but there is one that i've not quite figured out. there are some scenarios whereby for a given representation of an object, all fields are visible for two or more roles, but only a subset of those roles my edit certain fields.an example:{ person: { id: 1, name: bob, age: 25, occupation: software developer, phone: <phone>, description: could use some sunlight.. }}given 3 users: an admin, a regular user, and bob himself (also a regular user), i need to be able to convey to the front end that:admins may edit all fields, bob himself may edit all fields, but a regular user, while they can view all fields, can only edit the description field. i certainly don't want the client to have to make the determination (or even, for that matter, to have any notion of the roles involved) but i do need a way for the backend to convey to the client which fields are editable.i can't simply use a combination of representation (the fields returned for viewing) and links (whether or not an edit link is availble) in this scenario since it's more finely grained.has anyone solved this elegantly without adding the logic directly to the client?",
    "present_kp": [
      "rest"
    ],
    "absent_kp": [
      "api design"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "computing the line equations of two crossing tangents in a point set separated by a vertical line?. i have provided a picture as an example. we have two point sets, p and q. p is to the left of this vertical line (named x = x0), and q is to the right of it. the goal is to compute the line equations for both l1 and l2, as listed in the picture. my understanding is that this is a linear programming problem. the algorithm i'm looking for would run in o(n+m) time. i have only been able to find examples of o(n log n) time, and i'm not quite sure where else to look for o(n+m) time specifically.",
    "present_kp": [
      "linear programming"
    ],
    "absent_kp": [
      "computational geometry"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "cover letter generator: guides writing a template and autofills information. this is my first public web-application. i'm a beginner to javascript and this is my first time using jquery. it's not complete, but i'd love some feedback on the programming and the functionality. as i said, i'm just learning and i'm not sensitive, so feel free to be thorough with your criticism/feedback.you can view source/see in action here, and here's the current code:// for bootstrap popup tips$(#blob).popover();$('#company_name').popover();$('#company_address').popover();$('#reason').popover();$('#opening_paragraph').popover();$('#closing_paragraph').popover();// this resets the company information fields and // scrolls to the section$('#reset_company').click(start_new_letter);$('#restart_button').click(start_new_letter);function start_new_letter(e) { $('#company_address').val(); $('#company_name').val(); $('#position_title').val(); $('#position_code').val(); $('#reason').val(i want to work here because ); $(div.preview_div).html(); $('html,body').animate({ scrolltop: $('#company_div').offset().top }, 'slow');};// this opens an iframe when the editor button is clicked$('#editor').click(function () { document.getelementbyid(hemingway_editor).innerhtml = <iframe src='<url> height='700' width='90%'></iframe>;});// this googles the company name + company when clicked// or gives an alert if no company name has been entered$('#google_company').click(function () { if ($('#company_name').val() == ) { $('#company_name_td').append('<div class=alert alert-danger alert-dismissible role=alert><button type=button class=close data-dismiss=alert><span aria-hidden=true>&times;</span><span class=sr-only>close</span></button> you need to enter a company name for me to search. </div>'); } else { window.open(<url> + $('#company_name').val() + company); }});// this does the same but for the company address$('#google_address').click(function () { if ($('#company_name').val() == ) { $('#company_address_td').append('<div class=alert alert-danger alert-dismissible role=alert><button type=button class=close data-dismiss=alert><span aria-hidden=true>&times;</span><span class=sr-only>close</span></button> you need to enter a company name for me to search. </div>'); } else { window.open(<url> + $('#company_name').val() + address); }});// this adds and deletes rows from the skill bank when the // add row / delete row buttons are pressed$(document).ready(function () { var i = 1; $(#add_row).click(function () { $('#addr' + i).html('<td> <div class=input-group> <span class=input-group-addon> <input type=checkbox checked></span> <textarea class=form-control placeholder='describe a skill. use \\company\\ and \\position\\ as placeholders (quotes included), but do not overdo it. a few mentions make the letter look tailored but too many look weird.' rows=3 style=width:100%></textarea></div><!-- /input-group --> </td>'); $('#tab_logic').append('<tr id=addr' + (i + 1) + '></tr>'); i++; }); $(#delete_row).click(function () { if (i > 1) { $(#addr + (i - 1)).html(''); i--; } });})// this repaces javascript newlines with html breaks // when called, i don't think i'm using it anymore// but i have plans that might entail using itfunction htmlfortextwithembeddednewlines(text) { var htmls = []; var lines = text.split(/ /); alert(it is used); // the temporary <div/> is to perform html entity encoding reliably. // // document.createelement() is *much* faster than jquery('<div></div>') // <url> // // you don't need jquery but then you need to struggle with browser // differences in innertext/textcontent yourself var tmpdiv = jquery(document.createelement('div')); for (var i = 0 ; i < lines.length ; i++) { htmls.push(tmpdiv.text(lines[i]).html()); } return htmls.join(<br>);}// this copies the text in the output to the user's // clipboard when the copy button is clicked$(#copy_button).on('click', function copy_content(e) { e.preventdefault();}).zclip({ path: '<url> copy: function () { var html_text = $('#preview_div').html(); html_text = html_text.replace(/<br> /g, ' '); html_text = html_text.replace(/<br>/g, ' '); html_text = html_text.replace(/<b>/g, ''); html_text = html_text.replace(/<\\/b>/g, ''); return html_text; }});// this allows the user to go through the different input areas// using the enter button instead of the tab button $('body').on('keydown', 'input, select, textarea', function(e) { var self = $(this) , form = self.parents('form:eq(0)') , focusable , next ; if (e.keycode == 13) { focusable = form.find('input,a,select,textarea').filter(':visible'); next = focusable.eq(focusable.index(this)+1); if (next.length) { next.focus(); } else { form.submit(); } return false; }});// this stops the form from being submitted when the user presses// the enter button$(form).submit(function(e) { e.preventdefault();});$('#salutation2').hide();// this just allows the same generate function to be // run from two different buttons with different ids$('#preview1').click(get_data);$('#preview2').click(get_data);// this organizes the header info into the right format// and returns it as a stringfunction get_header(company_name, company_address, position_title, position_code, salutation){ var d = new date(); var monthnames = [january, february, march, april, may, june, july, august, september, october, november, december]; var date = monthnames[d.getmonth()] + + d.getdate() + , + d.getfullyear() + <br><br>; var header = date; company_name + <br>; header += company_address + <br>; header += <br> re: + position_title + position + position_code + <br><br>; header += salutation; header += <br><br>; return header;};// this gets the skill data and formats it into a paragraph// and returns it as a stringfunction get_skills(e){ var skill_list = []; var check_list = []; // get the skills $(#tab_logic).find('textarea').each(function (i, el) { var skill = $(this).val(); skill_list.push(skill); }); if( salutation === other) { salutation = $('#salutation2').val(); } if (skill_list[0] == ) { skill_list[0] = 'i have a strong programming foundation and am motivated to learn more, so i would be an asset for your position position.'; } if (skill_list[1] == ) { skill_list[1] = 'as well as technical skills, i have good interpersonal and communication skills and would fit in well with your workplace culture.'; } $(#tab_logic).find('input[type=checkbox]').each(function (i, el) { var check = $(this).prop(checked); check_list.push(check); }); var skill_string = ; for (var i = 0; i < skill_list.length; i++) { if (check_list[i] == true) { skill_string += + skill_list[i]; } } return skill_string;}// this gets all the data from all the fields,// replaces it with placeholders if they're undefined// organizes the header, output, skills, and paragraphs// into the long string, then outputs itfunction get_data(e) { $('html,body').animate({ scrolltop: $('#preview_div').offset().top }, 'slow'); var company_name = $('#company_name').val(); var company_address = $('#company_address').val(); var position_title = $('#position_title').val(); var position_code = $('#position_code').val(); var reason = $('#reason').val(); reason = reason.replace(i want to work here because , ); var opening_paragraph = $('#opening_paragraph').val(); var closing_paragraph = $('#closing_paragraph').val(); var name = $('#name').val(); var email = $('#email').val(); var phone = $('#phone').val(); var salutation = $('#salutation').val(); var signature = $('#signature').val(); if (company_name == ) { company_name = $(#company_name).attr('placeholder'); } if (company_address == ) { company_address = $(#company_address).attr('placeholder'); } if (position_title == ) { position_title = $(#position_title).attr('placeholder'); } if (position_code !== ) { position_code = # + position_code; } if (reason == ) { reason = '<b>reason</b>'; } if (email == ) { email = '<b>email</b>'; } if (phone == ) { phone = '<b>phone</b>'; } if (name == ) { name = '<b>name</b>'; } if (opening_paragraph == ) { opening_paragraph = '<b>opening paragraph</b>'; } if (closing_paragraph == ) { closing_paragraph = '<b>closing paragraph</b>'; } var header = get_header(company_name, company_address, position_title, position_code, salutation); var skill_string = get_skills(); var paragraphs = opening_paragraph + <br><br> paragraphs += skill_string + <br><br>; paragraphs += closing_paragraph; var closing = <br><br>thank you for your time and consideration.; closing += '<br><br>' + signature + '<br><br>name'; var output_text = header + paragraphs + closing; output_text = output_text.replace(/reason/g, reason); output_text = output_text.replace(/company/g, company_name); output_text = output_text.replace(/phone/g, phone); output_text = output_text.replace(/email/g, email); output_text = output_text.replace(/position/g, position_title); output_text = output_text.replace(/name/g, name); output_text = output_text.replace(.., .); $(div.preview_div).html(output_text);}<!doctype html><html lang=en> <head> <meta charset=utf-8> <meta http-equiv=x-ua-compatible content=ie=edge> <meta name=viewport content=width=device-width, initial-scale=1> <title>cover letter tutor</title> <!-- bootstrap core css --> <link href=css/bootstrap.min.css rel=stylesheet> <!-- custom styles for this template --> <link href=custom_style.css rel=stylesheet> <script src=<url> <script type=text/javascript src=js/jquery.zclip.min.js></script> <script type=text/javascript src=js/bootstrap.min.js></script> <script type=text/javascript src=<url> <link rel=icon href=favicon.ico type=image/x-icon /> </head> <body> <!-- the header and navigation tabs --> <div class=container> <div class=header> <ul class=nav nav-pills pull-right> <li class=><a id=reset_company href=#>start new letter</a> </li> <li class=><a id=about href=about.html>about</a> </li> <li class=dropdown> <a class=dropdown-toggle data-toggle=dropdown href=#> download <span class=caret></span> </a> <ul class=dropdown-menu role=menu> <li class=disabled><a href=#>as word doc (upcoming)</a> </li> <li class=disabled><a href=#>as pdf (upcoming)</a> </li> <li class=divider></li> <li class=><a id= href=javascript:void(0); onclick=copy_content()>copy to clipboard</a> </li> </ul> </li> </ul> <h1 class=text-muted>cover letter tutor</h1> </div> </div> <!-- grayed area --> <div style=text-align:center; class=preview_button> <button type=button id=preview1 class=btn btn-success btn-md> <span class=glyphicon glyphicon-file></span> preview example</button> </div> <br> <div class=container> <div class=jumbotron style=text-align:left; padding-top:15px; id=company_div> <!-- form name --> <legend>company information</legend> <form id=company_form class=form-horizontal> <table> <tr> <td style=padding-top:15px; vertical-align:top;>company name</td> <td id=company_name_td> <div style=width:100% class=input-group> <input id=company_name style=width:100% class=form-control type=text data-toggle=popover data-trigger=select data-placement=top placeholder=pied piper data-content=don't be too formal with the name, 'pied piper corporation ltd.' doesn't sound natural. /> <span style=width:35% class=input-group-btn> <button id=google_company class=btn btn-primary type=button style=width:100% >google company</button> </span> </div> </td> </tr> <tr> <td style=padding-top:15px; vertical-align:top;>company address</td> <td id=company_address_td> <div style=width:100% class=input-group> <input style=width:100% type=text id=company_address class=form-control placeholder=200 bachman rd <br> silicon valley, ca 95101 type=text data-toggle=popover data-trigger=select data-placement=top data-content=for multi addresses, write <br> where you want to start a new line. /> <span style=width:35% class=input-group-btn> <button style=width:100% id=google_address class=btn btn-primary type=button>google address</button> </span> </div> </td> </tr> <tr> <td>position title/code</td> <td id=company_address_td> <div class=input-group> <input type=text id=position_title class=form-control placeholder=software developer style=width:65%; class=form-control type=text /> <span class=> <input type=text id=position_code style=width:35%; class=form-control placeholder=123 (if applicable) type=text /> </span> </div> </td> </tr> <tr> <td>complete the sentence</td> <td> <textarea style=width:100% id=reason class=form-control data-toggle=popover data-trigger=select data-placement=top data-content=this is the most important part of the letter. it's worth reading up on the company a bit to make this stand out. and be genuine! >i want to work here because </textarea> </td> </tr> <tr> <td></td> <td> <button type=button id=blob class=btn btn-md btn-danger pull-right data-toggle=popover data-placement=left title=google the company and find any of the following data-content=mention a specific project &bullet; mention something about the workplace culture &bullet; mention a certain practice the company has &bullet; mention the location and why you'd want to move there>click here for ideas</button> </td> </tr> </table> </form> </div> <div class=jumbotron style=text-align:left; padding-top:15px;> <legend>describe your skills</legend>this is your skill bank. describe a few technical and personal skills, such as programming, microsoft office, communication, or teamwork. read through the job description and customize each cover letter by checking and unchecking which skills they're looking for. <br> <br> <div class=row clearfix> <div class=col-md-12 column> <table class=table table-hover id=tab_logic style=width:100%;> <tbody> <tr id='addr0'> <td> <div class=input-group> <span class=input-group-addon> <input type=checkbox checked> </span> <textarea style=width:100% class=form-control rows=2 style=width:100% placeholder='i have a strong programming foundation and am motivated to learn more, so i would be an asset for your position position.'></textarea> </div> <!-- /input-group --> <div class=col-lg-6> </td> </tr> <tr id='addr0'> <td> <div class=input-group> <span class=input-group-addon> <input type=checkbox checked> </span> <textarea class=form-control rows=2 style=width:100% placeholder='as well as technical skills, i have good interpersonal and communication skills and would fit in well with your workplace culture.'></textarea> </div> <!-- /input-group --> <div class=col-lg-6> </td> </tr> <tr id='addr1'></tr> </tbody> </table> </div> </div> <a id=add_row class=btn btn-primary pull-left btn-sm>add skill</a><a id='delete_row' class=pull-right btn btn-primary btn-sm>remove skill</a> <br> </div> <div class=jumbotron style=text-align:left; padding-top:15px;> <!-- form name --> <legend>letter content</legend>this is where most of the content for your cover letter is. edit these paragraphs to suit your style, using position and company where relevant. the opening paragraph is the only place you should include reason. apart from those placeholders, these templates will be the same for all your cover letters. <br> <br> <form class=form-horizontal> <table> <tr> <td>opening <br>paragraph template</td> <td> <textarea id=opening_paragraph rows=4 cols=50 class=form-control data-trigger=select data-placement=top data-content='express enthusiasm to work for the company, tell them why you are a good fit for the position, and give a specific reason you want to work there.'>i believe i am a good candidate for the position position you have available. the reason i want to work specifically at company is because reason.</textarea> </td> </tr> <tr> <td>closing <br>paragraph template</td> <td> <textarea id=closing_paragraph rows=4 cols=50 class=form-control data-toggle=popover data-trigger=select data-placement=top data-content='tell them how you could contribute to the company, invite them to give you the chance to, and give them your contact information.' >company is a company that i think i could contribute to and i would appreciate the opportunity to prove it. to arrange an interview please contact me at email or phone.</textarea> </td> </tr> <tr> <td>salutation</td> <td> <select class=form-control id='salutation' name = 'color' onchange = if ($('#salutation').val() == 'other') { $('#salutation2').show(); } else { $('#salutation2').hide(); }> <option value=dear hiring manager,>dear hiring manager</option> <option value=dear sir/madam,>dear sir/madam</option> <option value=to whom it may concern,>to whom it may concern</option> <option value=other>other (specific names are a good idea)</option> </select><input class=form-control type='text' id='salutation2' placeholder=dear ms. smith,/> <tr> <td>signature</td> <td> <select class=form-control id=signature> <option value=sincerely,>sincerely</option> <option value=yours truly,>yours truly</option> <option value=thankfully,>thankfully</option> <option value=regards,>regards</option> <option value=cheers,>cheers</option> <option value=all the best,>all the best</option> </select> </table> </form> </div> </div> </div> <div class=container> <div class=jumbotron style=text-align:left; padding-top:15px;> <legend>personal information</legend>you can leave this part blank if you prefer, but rest assured that this website doesn't store or do anything with the entered content. i don't even know how to do that. <br> <br> <form class=form-horizontal> <table> <tr> <td>your name</td> <td> <input type=text size=40 class=form-control id=name placeholder=crclayton style=width:100%> </td> </tr> <tr> <td>your email</td> <td> <input type=text class=form-control id=email placeholder=<email> style=width:100%> </td> </tr> <tr> <td>your number</td> <td> <input type=text class=form-control id=phone placeholder=<phone> style=width:100%> </td> </tr> </form> </table> <div></div> </div> <div style=text-align:center> <button type=button id=preview2 class=btn btn-success btn-md> <span class=glyphicon glyphicon-file></span> generate</button> <button id='copy_button' class=btn btn-success btn-md><span class=glyphicon glyphicon-arrow-down></span> copy</button> <button id='restart_button' class=btn btn-success btn-md><span class=glyphicon glyphicon-repeat></span> restart </button> <button id='editor' class=btn btn-success btn-md><span class=glyphicon glyphicon-edit></span> editor </button> </div></div> <br> <br> <div class=preview_div id=preview_div style=font-family:times new roman; font-size:12; padding-right:20%; padding-left:20%;></div> <br> <div id=hemingway_editor style=text-align:center;> </div> <div class=footer container> <p>&copy; crclayton 2014</p> </div> </div></div>",
    "present_kp": [
      "javascript",
      "jquery",
      "html"
    ],
    "absent_kp": [
      "twitter bootstrap"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "strong extractors with reusable seeds. i have convinced myself of the following:for every $(k,\\epsilon^2\\hspace{.005 in})$-strong extractor ext, for every distribution $x$,if $\\;\\; k\\leq$ $\\:h_{\\infty}$$(\\hspace{.01 in}x\\hspace{.015 in}) \\;\\;$ then the probability over the seed $u$ that the distribution of $\\: ext{ext}\\hspace{.01 in}(\\hspace{.01 in}x,u\\hspace{.015 in}) \\:$ is not $\\epsilon$-close to uniformis at most $\\epsilon$.are there any known explicit strong extractors that are known to handle seed reuse better than can be shown by using the above on a standard strong extractor?what if each instance of extractor in the rest of this post was replaced with blender$\\hspace{.01 in}$?",
    "present_kp": [
      "extractors"
    ],
    "absent_kp": [
      "cr.crypto security",
      "randomness"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "very small eigenvalues using shooting method on sturm-liouville problem. i am trying to implement a shooting method for a sturm-liouville problem on the interval [0,1] given by$$ rac{d}{dx}(p(x,\\lambda^2) rac{dy}{dx}) - q(x,\\lambda^2)y = 0$$with$$p(x,\\lambda^2) = (1+sx)\\lambda^2, \\q(x,\\lambda^2) = k^2((1+sx)\\lambda^2 + sg),$$and parameters $k^2, s, g$. after providing a guess for the eigenvalue $\\lambda^2$ the code needs to find the optimal $\\lambda^2$ given the knowledge of sturm's oscillation theorem and the shooting method. by chance i figured out a pretty good set of parameters. when i evaluate the ode with my runge-kutta solver using the parameters and the boundary values $y_1(0) = 0$, $y_1'(0) = y_2(0)/p(0,\\lambda^2) = 1$ it gives a good trajectory, but not the best one. the user is then asked to provide another guess for the eigenvalue and the system is again evaluated. when still not hit correctly in a user defined tolerance, a new eigenvalue is calculated using the secant method. this is the part where everything seems to go wrong. the optimal eigenvalue according to the secant method is of the order 1e-15, which is way to small, and occurs immediately after one iteration. as a consequence when i plot the corresponding eigenvector it is also small. it should be similar to the plot generated by the runge-kutta procedure, but with a better hit to $y(1) = 0$. i am aware that the secant method can converge pretty fast but why to such small values? i have tried to spot a possible error in the past 2 days without success. i would be happy if someone could explain to me why this happens and moreover take his/her time to look into the, rather long, code. import sysimport timeimport numpy as npimport matplotlib.pyplot as pltdef f(u, x, k, s, g, lam): define the ode of the problem u = (y1, y2) f = f(y2/p, q*y1) ----------------------- du0/dx = f0 = u1/p(x,ww) du1/dx = f1 = q(x,ww)*u0 with u0(0) = 0.0 and u0'(0) = 1 = u1(0)/p(0,ww) y1, y2 = u return np.array([y2/p(p0,x,s,lam), q(p0,x,k,s,g,lam) * y1])# ode coefficientsdef p0(x, s): '''linear density profile ''' return 1 + s*xdef p(p0, x, s, lam): '''defines the function p(x,w^2) in the ode system''' p0 = p0(x, s) return p0*lamdef q(p0, x, k, s, g, lam): '''defines the function q(x,w^2) in the ode system, note that it contains the first derivative of the density profile ''' p0 = p0(x, s) return k*(p0*lam + s*g)my runge-kutta solver:def rungekutta4(f, u0, n, tbegin, tend, **kwargs): general function to solve generic first order or first order coupled odes of the form u' = f(u, t) with runge-kutta's 4th order method input: f - function containing the first order ode --> np.array when coupled odes u0 - intial condition(s) for the ode given as list n - # points to be generated on domain of interest = # iterations tbegin - begin point of integration domain tend - end point of integration domain **kwargs - (optional) dictionary containing constants needed in ode function(s) note: kwargs is just convention name, only aesterisc ** needed followed by variable name output: u - 1d or 2d numpy array containing the derivative(s) corresponding to the values in the time array t - 1d array containing the time points dt = tend/n #step size u = np.zeros((n+1, len(u0))) t = np.linspace(tbegin, tend, len(u)) u[0] = u0 for i in range(n): k1 = dt*f(u[i], t[i], **kwargs) k2 = dt*f(u[i] + 0.5*k1, t[i] + 0.5*dt, **kwargs) k3 = dt*f(u[i] + 0.5*k2, t[i] + 0.5*dt, **kwargs) k4 = dt*f(u[i] + k3, t[i] + dt, **kwargs) u[i+1] = u[i] + (k1 + 2.0*(k2+k3) + k4)/6.0 return u, tmy shooting method:def shootingmethod(f, bvs, fbvs, iv, err, **kwargs): function to solve boundary value problems by using the shooting method on a first order (coupled) ode of the form u' = f(u, t) making use of the secant method it possible that the algorithm not converges, in order to overcome this we impose a maximum number of iterations input: f - function containing the first order ode --> np.array when coupled odes bvs - tuple containing the two boundaries a & b in which we integrate such that u(a) = x, u(b) = y fbvs - tuple containing the function values x & y at the boundaries iv - first guess for u'(0) at begin boundary such that u'(a) = iv err - accuracy we desire to get at the end boundary for accepting a solution **kwargs - (optional) dictionary containing constants needed in ode function(s) output: u - 1d or 2d numpy array containing the derivative(s) corresponding to the values in the time array which satisfy boundary value problem t - 1d array containing the time points # maximum number of iterations before termination nmax = 20 # unpack the tuples of bvs and fbvs (bv1, bv2) = bvs (fbv1, fbv2) = fbvs iv1 = iv # integrate the ode to the other boundary using the first guess u, t = rungekutta4(f, u0=[fbv1,iv1], n=1000, tbegin=bv1, tend=bv2, **kwargs) # in order to look if the boundary is hit or missed we need last value u1last = u[-1,0] # determine the zero of function at boundary, which also gives error between # numerical and exact solution at boundary fiv1 = u1last-fbv2 print(' trial %d: iv1 = %10.8f, error = %10.8f' % (0, iv1, abs(fiv1))) time.sleep(0.5) # the probability that the first guess gives the solution within pre-chosen # accuracy is small, so we do at least a second iteration with new guess # we start with the second guess and follow the same procedure as earlier, # if the second guess is not within the pre-chosen accuracy then we apply the # secant method to refine our guess and start over again iv2 = float(input('please provide a new value for the guess: ')) print() # it is forbidden that the second guess has same value as first and program # terminates if iv2==iv1: raise valueerror('second guess value cannot be the same as the first!') niter = 1 while niter<nmax: u, t = rungekutta4(f, u0=[fbv1,iv2], n=1000, tbegin=bv1, tend=bv2, **kwargs) u2last = u[-1,0] fiv2 = u2last-fbv2 print('trial %d: iv%d = %10.8f, error = %10.8f' % (niter, niter+1, iv2, abs(fiv2))) time.sleep(0.5) if abs(fiv2)<err: print(' solution found!') print('exiting shooting algorithm to go plotting...') time.sleep(3.0) break else: # apply the secant method given our two failed guesses iv1, iv2 for # searching a better root inbetween them # our new point uv3 will be determined by the straight line through # the points (iv1, f(uv1)) and (iv2, f(iv2)) with f(iv1) = fiv1 & # f(iv2) = fiv2 # the calculation will be done now with iv2 and iv3 as initial values # in order to accomplish this, iv1 will be overwritten by iv2 and iv2 # will be overwritten with our newly found root iv3 # of course f(iv1) needs also to be overwritten by f(iv2) iv3 = iv2 - fiv2 * (iv2 - iv1)/(fiv2 - fiv1) iv1 = iv2 iv2 = iv3 fiv1 = fiv2 niter += 1 if niter==nmax: print(' no convergence at the boundary after %d iterations' %nmax) print('error is still: %.8f' %(abs(fiv2))) print('aborting program...') time.sleep(3.0) sys.exit() return u, tthe main program:#k = k^2 and lam = lam^2k, s, g, lam = 2.0, 1.0, -9.8, 2.0# put parameters in dictionary to feed into functionsvars = ['k', 's', 'g', 'lam']values = [k, s, g, lam]params = dict(zip(vars, values)) # define the boundaries, function values at boundary, initial value and # accuracy where the solution has to be inbvs = (0.0, 1.0)fbvs = (0.0, 0.0)iv = p(p0, 0, s, lam)err = 1e-4# integrate with rk4, only for checking behaviouru, x = rungekutta4(f, u0=[0.0,iv], n=1000, tbegin=0.0, tend=1.0, **params)# unpack the derivativesy1 = u[:,0]y2 = u[:,1]# plotting from runge-kutta plt.plot(x, y1)#plt.plot(x, y2)plt.show() # apply shooting algorithm to do real problemu, x = shootingmethod(f, bvs, fbvs, iv, err, **params)# unpack the derivativesy1shoot = u[:,0]y2shoot = u[:,1]# plotting from shootingplt.plot(x, y1shoot) #plt.plot(x, y2shoot)plt.show()",
    "present_kp": [],
    "absent_kp": [
      "numerical analysis",
      "python",
      "computational physics"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "date comparison script. the university library where i work is in the process of redesigned our home page. one of the features we wish to include is a widget to display the current hours. unfortunately for a variety of reasons this must be done using client side scripting instead of a php script which pulls from google calendar. after much wailing and gnashing of teeth i have created such a script. it uses functions to see check if key dates fall within certain ranges and then assigns hours based on those results. after error checked it by manually setting the date assigned to the variable current everything seems fine. but its given me enough trouble that i want more experienced eyes to look at it and see if there any lingering issues i missed.the script works as follows. after getting the current date and day of the week, the library's hours are assigned to a set of arrays. the set of variables which follows are the dates at which our schedule changes. making each one a date object (as opposed to a number) seems to be the only way to compare apples to apples and not get an erroneous result. also it makes it easier for non-programmers to update the script. in the functions which follows the current date is compared against these an if it falls within that range, the function returns true. the exception is the short_hours function which must exist because there are a handful of days out of the year when we are open from 7:30 am to 5pm. lastly the functions are called using a set of if statements and the one which is true sets the current hours from one of the arrays.flame away!<url> written by michael paulmenovar current = new date(); //creates new date objectvar weekday = current.getday(); //gets day of week (0-6 in javascript)var hours = document.getelementbyid(hours); //stores code to simplify if statements//the variables below set the library's operating hours var hours_regular = [sunday's hours: 2:00pm to 10:00pm, monday's hours: 7:30am to 10:00pm, tuesday's hours: 7:30am to 10:00pm, wednesday's hours: 7:30am to 10:00pm,thursday's hours: 7:30am to 10:00pm, friday's hours: 7:30am to 4:00pm, closed today]; var hours_exam = [sunday's hours: 2:00pm to 12:00pm, monday's hours: 7:30am to 12:00pm, tuesday's hours: 7:30am to 12:00pm, wednesday's hours: 7:30am to 12:00pm, thursday's hours: 7:30am to 12:00pm, friday's hours: 7:30am to 4:00pm, closed today];var hours_intercession = [closed today, monday's hours: 8:00am to 4:30pm, tuesday's hours: 8:00am to 4:30pm, wednesday's hours: 8:00am to 4:30pm, thursday's hours: 8:00am to 4:30pm, friday's hours: 8:00am to 4:00pm, closed today, today's hours: 8am - 5pm];var hours_august_intercession = [closed today, monday's hours: 8:00am to 5:00pm, tuesday's hours: 8:00am to 5:00pm, wednesday's hours: 8:00am to 5:00pm, thursday's hours: 8:00am to 5:00pm, friday's hours: 8:00am to 4:00pm, closed today];var hours_shortened = today's hours: 7:30am to 5:00pm;//these are the 'important dates' for the year. at each of these dates our operating hours change. the format is year + month + date. //change the values in parenthesis every year to reflect the important dates for the next academic year.//note that in javascript counting begins at 0, so january is month 0 and december is month 11.//fall datesvar fall_semester_begin = new date (2014,7,18);var fall_break_begin = new date (2014,9,15);var fall_break_end = new date (2014,9,20);var fall_exams = new date (2014,11,8);var thanksgiving_break = new date (2014,10,24);var thanksgiving_break_end = new date (2014,11,1);var winter_intercession = new date (2014,11,14);var winter_holidays = new date (2015,11,23);//spring datesvar winter_intercession2 = new date (2015,0,1);var spring_semester_begin = new date (2015,0,12);var spring_break_begin = new date (2015,2,9);var spring_break_end = new date (2015,2,16);var spring_exams = new date (2015,4,4);//summer datesvar may_intercession = new date (2014,4,11);var summer_hours_regular = new date(2014,5,2);var august_intercession = new date (2014,6,31);//do not touch anything below this line.//the functions below compare the current date as formatted above to the important dates to determine at what point in the year the current date falls in.function regular_hours_spring (today) { if (today >= spring_break_end && today < spring_exams || today >= spring_semester_begin && today < spring_break_begin ) { return true; } else { return false; }}function regular_hours_summer (today){ if (today >= summer_hours_regular && today < august_intercession) { return true;} else {return false;}}function regular_hours_fall (today) { if (today >= thanksgiving_break_end && today < fall_exams || today >= fall_break_end && today < thanksgiving_break || today >= fall_semester_begin && today < fall_break_begin) { return true; } else { return false; }}function exam_hours (today) { if (today >= fall_exams && today <= winter_intercession || today >= spring_exams && today <= may_intercession) { return true; } else { return false; }}function intercession_hours (today) { if (today >= winter_intercession && today < winter_holidays || today >= thanksgiving_break && today < thanksgiving_break_end || today > fall_break_begin && today < fall_break_end || today >= august_intercession && today < fall_semester_begin || today >= may_intercession && today < summer_hours_regular || today > spring_break_begin && today < spring_break_end || today > winter_intercession2 && today < spring_semester_begin) { return true; } else { return false; }}function short_hours (today) { if (fall_break_begin && today > fall_semester_begin) { return true; } else { return false; }}function closed (today) { if (today >= winter_holidays) { return true; } else { return false; }}//the value returned by the functions above is logged to the error console for troubleshootingconsole.log (regular_hours_spring(current));console.log (regular_hours_summer(current));console.log (regular_hours_fall(current));console.log (exam_hours(current));console.log (intercession_hours(current));console.log (short_hours(current));console.log (closed(current));//the if statements below call the functions above and determine whether the hours displayed should be regular, exam, or intercession.if (closed (current) && current >= winter_holidays) { hours.innerhtml = closed today;} else if(intercession_hours(current) && current >= august_intercession && current < fall_semester_begin) { hours.innerhtml = hours_august_intercession[weekday];}else if(intercession_hours(current)) { hours.innerhtml = hours_intercession[weekday];}else if (exam_hours (current)) { hours.innerhtml = hours_exam[weekday];} else if(regular_hours_spring(current)){ hours.innerhtml = hours_intercession[weekday]; } else if (regular_hours_summer (current)){ hours.innerhtml = hours_regular[weekday];} else if (regular_hours_fall (current)){ hours.innerhtml = hours_regular[weekday];} else if (short_hours ()) { hours.innerhtml = hours_shortened;} else {hours.innerhtml = <a href='<url> target='_blank'>click here for library hours</a>};",
    "present_kp": [
      "javascript"
    ],
    "absent_kp": [
      "datetime"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "does google search exclude contents of from summary?. html5 has a new tag, <nav>.do popular search-engines ignore contents within such tag when they generate page summaries?",
    "present_kp": [
      "google search",
      "html5"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to know if x and y have coauthored?. is there any tool where one can figure out if two people have coauthored or not? like the tool where one can figure out somebody's erdos _number_ .",
    "present_kp": [],
    "absent_kp": [
      "soft question"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "should i try to separate state from implementation?. right now i'm working with some code that combines state and operations.it looks something like this (note: doesn't actually deal with cars/trucks. i'm abstracting the business logic here, and apologize in advance that the analogy doesn't entirely make sense)truckimpl { // fields/properties int mileage bool missionaccomplished // ... and many, many more // constructor truckimpl truckimpl(irentalagreement agreement) // methods void drive() { // sets a whole bunch of fields/properties which are subsequently used to make decisions in park() getcarfromdealer() haul() } void getcarfromdealer() void haul() // return to rental company if done, otherwise park at home void park()}carimpl { // fields int mileage bool missionaccomplished // constructor carimpl carimpl(irentalagreement agreement) // methods // sets a whole bunch of fields/properties which are subsequently used to make decisions in park() void drive() { // sets a whole bunch of fields/properties which are subsequently used to make decisions in park() getcarfromdealer() race() } void getcarfromdealer() void race() // return to rental company if done, otherwise park at home void park()}it's used in some code that looks similar to the following pseudocode:rentalvehiclehandler { ivehicleimplfactory factory; handle(irentalagreement agreement) { impl = factory.create(agreement) impl.drive() impl.park() }}main() { while(true) { var agreements = getagreements(); foreach(var agreement in agreements) { handler.handle(agreement) }}in the above code, we're getting batches of rentalagreements, but still processing each of them individually. now, i want to handle cars and trucks slightly separately. for trucks, i will still drive() then park() each individually. for cars, i want to getcarfromdealer() individually, but then race() the entire batch of cars at once before parking them. i might want to do something similar for trucks in the future, but maybe not.i thought that it would be helpful to separate my state from my implementation like below: (to start, no batching-related stuff added yet):// one per rental agreementrentalvehicle { #properties rentalagreement agreement int mileage bool missionaccomplished //(...all of the truckimpl propertiies?)}// can handle any number of rentalagreements, maybe multiplecardriver { rentalvehicle drive(rentalvehicle vehicle) rentalvehicle park(rentalvehicle vehicle)}truckdriver { rentalvehicle drive(rentalvehicle vehicle) rentalvehicle park(rentalvehicle vehicle)}is this a helpful/reasonable thing to do? or is it just a bunch of extra work that will gain me no advantage? i think it will help me both with the problem at hand and help me with further refactoring, but i've also tried a few other things that were dead-ends and i don't want to spend a bunch of time refactoring this for no reason.",
    "present_kp": [
      "refactoring",
      "state"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "what is a good way to represent (programatically) graphs/networks?. so i have made this program dealing with cities and the roads linking them, and i am wondering if anyone could point me towards a way to represent this network graphically (in 3 or 2ds).could be either a specific programming friendly file format (which i could easily generate an instance of programatically), or a specific language's api or library for such representation.my current work is in java, but if there's a solution in an other language it's not a problem.",
    "present_kp": [
      "java"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "are google shortened urls (goo.gl) publicized, sequential, or secret?. google has a shortening service at <url> which let's you convert a long url to a short one. it says the following about the urls it generates:all goo.gl urls and click analytics are public and can be accessed by anyone.i understand that the urls are not private. however, the above does not specify the difference between a publicized, sequential, or secret url. below i will define what i mean by each of these terms. notice how their definitions make them mutually exclusive. which of these, then, best describes what goo.gl uses?say i use goo.gl and get the url <url> google has a page such as <url> which has all of the urls that have been generated, then i would consider that to be publicizing the url. or if there were a page that listed urls by category, or by number of visits, etc. that would also be considered publicizing the url. if google has an api that lets people access this information, then i would also consider that publicizing the url.sequential:if google distributes urls in a sequential manner, then it is really easy to get a list such as the publicized version above. for example, if someone got the url <url> they could enter in the previous url, <url> and easily see the link i just generated. if it is both sequential and publicized, publicized would trump this definition, since it's much easier to use the publicized page than randomly looking through a sequence.secret:the urls google generates are random enough to not be sequential (i.e. so it's not easy to guess them), and they are not publicized anywhere. therefore, while it is possible that someone could access the link i just generated, it's not probable since it would involve pure chance. in other words, i can assume that when i generate a goo.gl url no one will know about it unless the url is specifically shared with them.private:these would be links that can only be accessed by people you intend to share the url with. (e.g. it may require a login and password). google's service is obviously not a private one, since it states that above.notes:there is a page that lists all of your generated urls. if you are the only one that can view this page, that would not necessarily mean that the service is publicized. if there is any way for someone else to view these details (e.g. via an api), that means that the service is publicized; unless it is very difficult to view this page (e.g. they need a secret id associated with your account which they can't easily get unless you give it to them).",
    "present_kp": [
      "goo.gl"
    ],
    "absent_kp": [
      "privacy",
      "url shortening"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "tracking users behaviour - with or without google analytics. if i understand correctly the following (point & from ga tos):privacy . you will not (and will not allow any third party to) use the service to track or collect personally identifiable information of internet users, nor will you (or will you allow any third party to) associate any data gathered from your website(s) (or such third parties' website(s)) with any personally identifying information from any source as part of your use (or such third parties' use) of the service. you will have and abide by an appropriate privacy policy and will comply with all applicable laws relating to the collection of information from visitors to your websites. you must post a privacy policy and that policy must provide notice of your use of a cookie that collects anonymous traffic data.you are not allowed to use custom variables that will identify the visitor(for example website username, e-mail, id etc.)so the question is how can i track a specific user behaviour(for example the actions that every single logged in user do).",
    "present_kp": [
      "google analytics",
      "tracking",
      "visitors"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "what are the alternatives to resx file. i am developing a windows application and i want to store all the texts for labels, radio buttons, buttons, check boxes, column headers of grid at one place. i have tried class file, xml file, database table and resource file.i found that class file is the best way to store all these texts, but, i want to know is their any alternate way? if i am using class file then, i will have to recompile project if any text changes.please suggest me if you have know any a way other than resx, class file, xml file and database.i came across dictionary. i found that it is easier. if i use dictionary will it take my ram space greater than using properties in class file?",
    "present_kp": [],
    "absent_kp": [
      "c#"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is possible to install mediawiki deb package without needing apache?. i would like to install the mediawiki package on debian wheezy. mediawiki has dependencies on (apache2 or httpd) and mysql, but i want to use nginx-extras (which provides httpd) and sqlite (i.e. not apache and not mysql).i first installed nginx-extras, which automatically installed httpd. but when i tried to install the mediawiki withapt-get install mediawikiit tried to install various apache modules.how can i remind apt-get that it shouldn't install apache because the package httpd has already been provided?i have also installed php5-sqlite, and mediawiki needs one of php5-sqlite, php5-pgsql or php5-mysql. however, apt-get also wanted to install the modules associated with the mysql server.i triedapt-get install mediawikion an ubuntu system and it seemed to respect the fact that httpd was already installed.have i uncovered a debian bug, or am i missing some configuration option in debian?",
    "present_kp": [
      "apt"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "checking dates against a .properties file. i am writing a script that will check the set values against a .properties file and i am just wondering if there is any nicer way to write what i have here.#!/bin/bash # @summary check values # --------------------------------------------------------------------------------------------------function _runchecks{successfuldiffrun=true timestamp() { date +%f; }todaysdate=$(timestamp)#read savedstate.properties filewhile read line; do eval $line; done < savedstate.properties#compare the valuesecho hotpatch : if [ $todaysdate = $wd_managegold_datetimestamp ]; then echo dates are a match : todaysdate:$todaysdate = hotpatchrundate:$wd_managegold_datetimestamp if [ $successfuldiffrun = $wd_managegold_success ]; then echo successfuldiffrun : $wd_managegold_success echo hotpatch run was successful else echo successfuldiffrun : $wd_managegold_success echo exiting exit 0 fielse echo dates not a match: todaysdate:$todaysdate = hotpatchrundate:$wd_managegold_datetimestamp exit 0 fiecho echo rc prod : if [ $todaysdate = $wd_managerc_datetimestamp ]; then echo dates are a match : todaysdate:$todaysdate = rcrundate:$wd_managerc_datetimestamp if [ $successfuldiffrun = $wd_managerc_success ]; then echo successfuldiffrun : $wd_managerc_success echo rc run was successful else echo successfuldiffrun : $wd_managerc_success echo exiting exit 0 fielse echo dates not a match: todaysdate:$todaysdate = hotpatchrundate:$wd_managegold_datetimestamp exit 0 fi}# --------------------------------------------------------------------------------------------------# @summary start_runchecks",
    "present_kp": [
      "datetime",
      "bash"
    ],
    "absent_kp": [
      "performance",
      "shell"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what determines if a key stroke is waited indefinitely or if it times out?. i have a very customized vim configuration, but i don't think it has anything to do with this, because nothing has changed the key binding system.when i press g i can leave the window there and make a sandwich and come back and it's still shown in the part of my statusbar that shows pending keystrokes (i forget what that is called).but when i press \\ (<leader>) it times out and beeps after one second. when i check maps, i see that neither g nor \\ have primary mappings to just that key (which would indeed cause them to trigger their mapping upon timeout or receiving another non-matching key).so what then is the difference between these two cases? why doesn't \\ leave me more time, or, why does g not time out?",
    "present_kp": [],
    "absent_kp": [
      "key bindings"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "hooks class for basic plugin functionality. i've built a small hooks class for adding some capabilities of basic plug-in functionality.is there room for improvement? i am just beginning to learn oop. i have two ways to manually add execution points and delete them:autoloading method for plugins in a specific foldermethod for executiondefined('base_path') || (header(http/1.1 403 forbidden) & die('403 - acces direct interzis.'));class hooks{private static $hooks = [];private static $instance; /** * @param null $plugin_dir * @return hooks * @desc ne asiguram ca avem o singura instanta globala */ public static function init($plugin_dir = null) { if(self::$instance == null) self::$instance = new self($plugin_dir); return self::$instance; } /** * @param $plugin_dir * @desc instantierea efectiva, nu cred ca e nevoie de descriere si rulam load_action */ private function __construct($plugin_dir) { self::load_action($plugin_dir); } /** * @desc ne aisguram ca obiectul nu poate fi clonat */ private function __clone(){} /** * @param $plugin_dir * @return bool * @desc incarcarea dinamica de pluginuri (se efectueaza doar daca clasa este instantiata) * cautam directorul pt pluginuri le includem si le inregistram cu self::add_action */ private static function load_action($plugin_dir) { if($plugin_dir == null) return null; $hooks = new globiterator($plugin_dir.directory_separator.'*.php', filesystemiterator::key_as_filename); foreach($hooks as $hook => $file) { require_once $file; (empty($priority)) ? self::add_action($where, $callback) : self::add_action($where, $callback, $priority); } return null; } /** * @param $where punctul de executie * @param $callback metoda chemata poate fi functie sau metoda * @ex add_action('punct executie',[someclass, 'somestaticmethod']); * @param int $priority ordinea in care sant executate callbackurile pt fiecare pct de executie * @desc inregistram pluginuri */ public static function add_action($where, $callback, $priority = 50) { if(!isset(self::$hooks[$where])) self::$hooks[$where] = []; self::$hooks[$where][$callback] = $priority; } /** * @param $where * @param $callback * @desc stergem un callback pt un pct de executie */ public static function remove_action($where, $callback) { if(isset(self::$hooks[$where][$callback])) unset(self::$hooks[$where][$callback]); } /** * @param $where * @param array $args * @desc executam callbackurile pt un pct de executie si ai dam parametri necesari (optionali) */ public static function execute_action($where, $args = []) { if(isset(self::$hooks[$where]) && is_array(self::$hooks[$where])) { arsort(self::$hooks[$where]); foreach(self::$hooks[$where] as $callback => &$priority) call_user_func_array($callback, $args); } }}index.phpdefine('base_path', realpath(__dir__).directory_separator);require_once base_path.'hooks.php';hooks::init(base_path.'plugin'.directory_separator);$array_pt_test = [ 'home' => 'home.php', 'top' => 'top.php', 'hello' => 'hello.php'];function render_nav($array =[]){ $output = ''; foreach($array as $key => $value) { $output .= '<li>'; $output .= '<a href='.$value.'>'.$key.'</a>'; $output .= '</li>'; } echo $output;}hooks::add_action('nav', 'render_nav');<nav id=nav> <ul> <?php hooks::execute_action('nav', [$array_pt_test]); ?> </ul> <br class=clear /> </nav>this is a simple example of displaying a navigation menu. we register a default function (action) to process the array. in overwrite.php, the plugin directory, we register another function and give the overwrite_nav function a higher priority to be processed, and then call the hooks::remove_action method to unregister the default one. just play with the $priority value in overwrite.php to see the effects. you can have multiple actions tied to the same execution point.overwrite.php defined('base_path') || (header(http/1.1 403 forbidden) & die('403 - acces direct interzis.'));$where = 'nav';$callback = 'overwrite_nav';$priority = 57 ;function overwrite_nav($array = []){ hooks::remove_action('nav','render_nav'); $output = ''; foreach($array as $key => $value) { $output .= '<li>'; $output .= '<a href='.$value.'>'.strtolower($key).'</a>'; $output .= '</li>'; } echo $output;}",
    "present_kp": [
      "php"
    ],
    "absent_kp": [
      "beginner",
      "object oriented"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to configure virtual host to consider domain path in apache. the following virtual host definition is working for an arbitrary url starting with <url> *:80> servername <url> documentroot /home/user/www/application/current/public <directory /home/user/www/application/current/public> order allow,deny allow from all </directory></virtualhost>how to rewrite this configuration so that it is applied only for url's starting with <url> tried serverpath /user, but it doesn't work...any ideas?",
    "present_kp": [
      "apache",
      "virtualhost"
    ],
    "absent_kp": [
      "httpd.conf"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "when is a non-unitary quantum system only theoretical?. suppose we construct a non-unitary quantum system in hilbert space. it entails that this system would have no direct parallel in quantum circuitry as it is a requirement that all quantum gates are unitary and therefore the function constructed from them is also unitary. but does this mean that we cannot practically implement ?a potential parallel is found in the simulation of classical computations on a quantum computer, achieved by simulating an information-destroying nand gate using an information-preserving toffoli gate. if a non-unitary quantum system was such because it implemented a 'quantum nand gate', it would be possible to redesign it as a unitary system using a toffoli gate. it is not clear to me (and i cannot immediately find literature on this) if there are clearly defined bounds on what non-unitary operations can be simulated using unitary equivalents. is there a clearly defined subset of non-unitary quantum systems where is practically implementable? is it, perhaps, universal?",
    "present_kp": [],
    "absent_kp": [
      "quantum computing",
      "quantum information"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "non-blocking unix domain socket. i've developed quickly two kinds of socket use: the first with blocking mode and the second with non-blocking mode. the sockets are unix domain sockets. my problem is that the kernel consume a huge amount of cpu (approx: 85 %). my goal is to minimize the kernel cpu usage and to increase the throughput.i use taskset command to affect each process to a particular cpu core.the blocking mode unix socket shows performances of approx 1.3 gb/s. the non-blocking mode unix socket shows performances of approx 170 mb/s.the blocking version is faster than the non-blocking (+ epoll) version by approximately 8.blocking version:client.c#include <stdio.h>#include <stdlib.h>#include <errno.h>#include <string.h>#include <unistd.h>#include <time.h>#include <stdint.h>#include <sys/types.h>#include <sys/socket.h>#include <sys/un.h>#define sock_path echo_sockettypedef struct proto_t { uint32_t len; uint8_t *data;} proto_t;int main(void){ int s; int t; int len; struct sockaddr_un remote; char buffer[1400]; proto_t *frame = (proto_t *)buffer; if ((s = socket(af_unix, sock_stream, 0)) == -1) { perror(socket); exit(1); } printf(trying to connect... ); remote.sun_family = af_unix; strcpy(remote.sun_path, sock_path); len = strlen(remote.sun_path) + sizeof(remote.sun_family); if (connect(s, (struct sockaddr *)&remote, len) == -1) { perror(connect); exit(1); } printf(connected. ); srand(time(null)); for (;;) { len = (rand() % (sizeof(buffer) - sizeof(uint32_t))) + sizeof(uint32_t); frame->len = htobe32(len - sizeof(uint32_t)); if (send(s, frame, len, 0) == -1) { perror(send); close(s); exit(1); } } close(s); return 0;}server.c#include <stdio.h>#include <stdlib.h>#include <errno.h>#include <string.h>#include <unistd.h>#include <stdint.h>#include <time.h>#include <sys/types.h>#include <sys/socket.h>#include <sys/un.h>#define sock_path echo_socketint main(void){ int s, s2, t, len; struct sockaddr_un local, remote; char str[100]; int stat = 0; int last_stat = 0; if ((s = socket(af_unix, sock_stream, 0)) == -1) { perror(socket); exit(1); } local.sun_family = af_unix; strcpy(local.sun_path, sock_path); unlink(local.sun_path); len = strlen(local.sun_path) + sizeof(local.sun_family); if (bind(s, (struct sockaddr *)&local, len) == -1) { perror(bind); close(s); exit(1); } if (listen(s, 5) == -1) { perror(listen); close(s); exit(1); } for(;;) { int n; printf(waiting for a connection... ); t = sizeof(remote); if ((s2 = accept(s, (struct sockaddr *)&remote, &t)) == -1) { perror(accept); close(s); exit(1); } printf(connected. ); do { uint8_t buffer[1400]; uint32_t frame_len; int now = time(null); n = recv(s2, &frame_len, sizeof(frame_len), 0); frame_len = be32toh(frame_len); if (n < sizeof(uint32_t)) { break; } n = recv(s2, buffer, frame_len, 0); if (frame_len > 0) { if (n < frame_len) { close(s); perror(recv); break; } } stat += frame_len + sizeof(uint32_t); if (now - last_stat > 1) { last_stat = now; printf(received %f mb. , ((float)stat / 1024 / 1024)); stat = 0; } } while (1); close(s2); } return 0;}non-blocking version:client.c#include <stdio.h>#include <sys/types.h>#include <sys/socket.h>#include <string.h>#include <errno.h>#include <netinet/in.h>#include <stdlib.h>#include <sys/epoll.h>#include <sys/un.h>#include <unistd.h>#include <fcntl.h>#include <time.h>int epollfd;#define serverport 8080#define maxconn 5#define maxevents 100#define maxlen 1400#define sock_path echo_sockettypedef struct event_t { int fd; uint32_t event; char data[maxlen]; int length; int offset;} event_t;typedef struct proto_t { uint32_t len; uint8_t *data;} proto_t;static void event_set(int epollfd, int op, int fd, uint32_t events, void* data){ struct epoll_event server_ev; server_ev.events = events; server_ev.data.ptr = data; if(-1 == epoll_ctl(epollfd, op, fd, &server_ev)) { printf(failed to add an event for socket%d error:%s , fd, strerror(errno)); exit(1); }}static void event_handle(void* ptr){ event_t *ev = ptr; if(epollin == ev->event) { return; } else if(epollout == ev->event) { int ret; proto_t *frame = (proto_t *)ev->data; if (ev->length == 0) { /* init send */ ev->length = (rand() % (sizeof(ev->data) - sizeof(uint32_t)) + sizeof(uint32_t)); frame->len = htobe32(ev->length - sizeof(uint32_t)); } ret = write(ev->fd, (ev->data) + (ev->offset), ev->length); if( (ret < 0 && eintr == errno) || ret <= ev->length) { /* * we either got eintr or write only sent partial data. * add an write event. we still need to write data. */ if(ret > 0) { /* * the previous write wrote only partial data to the socket. */ ev->length = ev->length - ret; ev->offset = ev->offset + ret; } if (ev->length == 0) { /* write complete */ ev->offset = 0; ev->length = 0; } } else if(ret < 0) { /* * some other error occured. */ printf(error: ret < 0); close(ev->fd); free(ev); return; } if (ret == 0) { printf(------------ ); } event_set(epollfd, epoll_ctl_add, ev->fd, epollout, ev); }}static void socket_set_non_blocking(int fd){ int flags; flags = fcntl(fd, f_getfl, null); if (flags < 0) { printf(fcntl f_getfl failed.%s, strerror(errno)); exit(1); } flags |= o_nonblock; if (fcntl(fd, f_setfl, flags) < 0) { printf(fcntl f_setfl failed.%s, strerror(errno)); exit(1); }}int main(int argc, char** argv){ int clientfd; int len = 0; struct sockaddr_un remote; struct epoll_event *events = null; event_t ev; /* * create server socket. specify the nonblocking socket option. * */ clientfd = socket(af_unix, sock_stream | sock_nonblock, 0); if(clientfd < 0) { printf(failed to create socket.%s , strerror(errno)); exit(1); } bzero(&remote, sizeof(remote)); remote.sun_family = af_unix; strcpy(remote.sun_path, sock_path); len = strlen(remote.sun_path) + sizeof(remote.sun_family); /* * connect to the server * */ if (connect(clientfd, (struct sockaddr *)&remote, len) == -1) { perror(connect); exit(1); } printf(connected. ); /* * create epoll context. */ epollfd = epoll_create(1); if(epollfd < 0) { printf(failed to create epoll context.%s , strerror(errno)); exit(1); } /* * main loop that listens for event. */ events = calloc(maxevents, sizeof(struct epoll_event)); bzero(&ev, sizeof(ev)); ev.fd = clientfd; event_set(epollfd, epoll_ctl_add, clientfd, epollout, &ev); while(1) { int n = epoll_wait(epollfd, events, maxevents, -1); if(n < 0) { printf(failed to wait.%s , strerror(errno)); exit(1); } for(int i = 0; i < n; i++) { event_t *event = (event_t *)events[i].data.ptr; if(events[i].events & epollhup || events[i].events & epollerr) { printf( closing connection socket ); close(event->fd); } else if(epollin == events[i].events) { event->event = epollout; event_set(epollfd, epoll_ctl_del, event->fd, 0, 0); printf(error, cannot receive data ); //event_handle(ev); } else if(epollout == events[i].events) { event->event = epollout; /* * delete the write event. */ event_set(epollfd, epoll_ctl_del, event->fd, 0, 0); event_handle(event); } } } free(events); exit(0);}server.c#include <stdio.h>#include <sys/types.h>#include <sys/socket.h>#include <string.h>#include <errno.h>#include <netinet/in.h>#include <stdlib.h>#include <sys/epoll.h>#include <sys/un.h>#include <unistd.h>#include <fcntl.h>#include <time.h>int epollfd;#define serverport 8080#define maxconn 5#define maxevents 100#define maxlen 1400#define sock_path echo_sockettypedef struct event_t { int fd; uint32_t event; char data[maxlen]; int length; int offset;} event_t;static void event_set(int epollfd, int op, int fd, uint32_t events, void* data){ struct epoll_event server_ev; server_ev.events = events; server_ev.data.ptr = data; if(-1 == epoll_ctl(epollfd, op, fd, &server_ev)) { printf(failed to add an event for socket%d error:%s , fd, strerror(errno)); exit(1); }}static int stat;static int last_stat;static void event_handle(void* ptr){ event_t *ev = ptr; uint32_t *len = (uint32_t *)ev->data; if(epollin == ev->event) { uint32_t buffer[1400]; int n = 0; uint32_t blen; if (ev->length < sizeof(uint32_t)) { n = read(ev->fd, ev->data + ev->length, sizeof(uint32_t)); } else { uint32_t blen = be32toh(*len); if (blen > 0) { n = read(ev->fd, ev->data + ev->length, blen - (ev->length - sizeof(uint32_t))); } else { ev->length = 0; return; } } if(n == 0) { /* * client closed connection. */ printf( client closed connection. ); close(ev->fd); free(ev); } else if(n < 0) { perror(read from socket); close(ev->fd); free(ev); } else { int now = time(null); blen = be32toh(*len); ev->length += n; if (ev->length >= sizeof(uint32_t) && blen == ev->length - sizeof(uint32_t)) { /* data complete */ stat += ev->length; ev->length = 0; } if (now - last_stat >= 1) { printf(received %f mb. , ((float)stat / 1024 / 1024)); last_stat = now; stat = 0; } /* * we have read the data. add an write event so that we can * write data whenever the socket is ready to be written. */ /* printf( adding write event. ); event_set(epollfd, epoll_ctl_add, ev->fd, epollout, ev); */ event_set(epollfd, epoll_ctl_add, ev->fd, epollin, ev); } } else if(epollout == ev->event) { int ret; printf(epollout not handled yet ); return; ret = write(ev->fd, (ev->data) + (ev->offset), ev->length); if( (ret < 0 && eintr == errno) || ret < ev->length) { /* * we either got eintr or write only sent partial data. * add an write event. we still need to write data. */ event_set(epollfd, epoll_ctl_add, ev->fd, epollout, ev); if(-1 != ret) { /* * the previous write wrote only partial data to the socket. */ ev->length = ev->length - ret; ev->offset = ev->offset + ret; } } else if(ret < 0) { /* * some other error occured. */ close(ev->fd); free(ev); return; } else { /* * the entire data was written. add an read event, * to read more data from the socket. */ printf( adding read event. ); event_set(epollfd, epoll_ctl_add, ev->fd, epollin, ev); } }}static void socket_set_non_blocking(int fd){ int flags; flags = fcntl(fd, f_getfl, null); if (flags < 0) { printf(fcntl f_getfl failed.%s, strerror(errno)); exit(1); } flags |= o_nonblock; if (fcntl(fd, f_setfl, flags) < 0) { printf(fcntl f_setfl failed.%s, strerror(errno)); exit(1); }}int main(int argc, char** argv){ int serverfd; int len = 0; struct sockaddr_un local; struct sockaddr_un remote; struct epoll_event *events = null; /* * create server socket. specify the nonblocking socket option. * */ serverfd = socket(af_unix, sock_stream | sock_nonblock, 0); if(serverfd < 0) { printf(failed to create socket.%s , strerror(errno)); exit(1); } bzero(&local, sizeof(local)); local.sun_family = af_unix; strcpy(local.sun_path, sock_path); unlink(local.sun_path); len = strlen(local.sun_path) + sizeof(local.sun_family); /* * bind the server socket to the required ip-address and port. * */ if(bind(serverfd, (struct sockaddr*)&local, len) < 0) { printf(failed to bind.%s , strerror(errno)); close(serverfd); exit(1); } /* * mark the server socket has a socket that will be used to . * accept incoming connections. */ if(listen(serverfd, maxconn) < 0) { printf(failed to listen.%s, strerror(errno)); exit(1); } /* * create epoll context. */ epollfd = epoll_create(maxconn); if(epollfd < 0) { printf(failed to create epoll context.%s , strerror(errno)); exit(1); } /* * create read event for server socket. */ event_set(epollfd, epoll_ctl_add, serverfd, epollin, &serverfd); /* * main loop that listens for event. */ events = calloc(maxevents, sizeof(struct epoll_event)); while(1) { int n = epoll_wait(epollfd, events, maxevents, -1); if(n < 0) { printf(failed to wait.%s , strerror(errno)); exit(1); } for(int i = 0; i < n; i++) { if(events[i].data.ptr == &serverfd) { int connfd; if(events[i].events & epollhup || events[i].events & epollerr) { /* * epollhup and epollerr are always monitored. */ close(serverfd); exit(1); } /* * new client connection is available. call accept. * make connection socket non blocking. * add read event for the connection socket. */ len = sizeof(remote); connfd = accept(serverfd, (struct sockaddr*)&remote, &len); if(-1 == connfd) { printf(accept failed.%s , strerror(errno)); close(serverfd); exit(1); } else { event_t *ev = null; socket_set_non_blocking(connfd); printf(adding a read event ); ev = calloc(1, sizeof(event_t)); ev->fd = connfd; /* * add a read event. */ event_set(epollfd, epoll_ctl_add, ev->fd, epollin, ev); } } else { /* *a event has happend for one of the connection sockets. *remove the connection socket from the epoll context. * when the event is handled by event_handle() function, *it will add the required event to listen for this *connection socket again to epoll *context */ if(events[i].events & epollhup || events[i].events & epollerr) { event_t *ev = (event_t *) events[i].data.ptr; printf( closing connection socket ); close(ev->fd); free(ev); } else if(epollin == events[i].events) { event_t* ev = (event_t *) events[i].data.ptr; ev->event = epollin; /* * delete the read event. */ event_set(epollfd, epoll_ctl_del, ev->fd, 0, 0); event_handle(ev); } else if(epollout == events[i].events) { event_t* ev = (event_t*) events[i].data.ptr; ev->event = epollout; /* * delete the write event. */ event_set(epollfd, epoll_ctl_del, ev->fd, 0, 0); event_handle(ev); } } } } free(events); exit(0);}according to what i've read on the internet, non-blocking mode should be faster than blocking mode. why am i observing the reverse performance?is there a way to increase the throughput more than 1.3 gb/s?is there a way to minimize the kernel cpu usage?programs were compiled using:gcc -std=gnu99 -o3 {file}.c {bin-name}i use htop + perf to measure performance.",
    "present_kp": [
      "performance",
      "c",
      "io",
      "socket",
      "unix"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "renaming files in sftp session. this is part of a sftp file transfer. here are the steps:check files with .csv extension and get them to the local directory.after that, move them to the another folder in the remote connection.tried using the rename command but it throws an error failuretried using -b batch-file option with sftp but looks like rename command needs a specific file-name instead of a set of files.so how do i achieve this ?",
    "present_kp": [
      "rename",
      "sftp"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "i3 dmenu does not browse $path. i want to run a shell script ~/.local/bin/test.sh via dmenu. if i run dmenu via $mod+d and browse for the entry test.sh i couldn't find it.the path ~/.local/bin is already set to my $path variable in ~/.profile$ echo $path/home/ubuntu/bin:/home/ubuntu/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/gamesi also removed ~/.cache/dmenu_run and restart i3. what can i do to launch the test script via dmenu?",
    "present_kp": [
      "i3",
      "dmenu"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to access the local ftp yum repo from client machine. i setup the ftp server on machine a 192.168.1.105 and created the yum repo. and placed following contents in it.#vi rhel-localftp.repo[rhel-localftp.rep]name=bla bla blabaseurl=file:///var/ftp/pub/server(rpms are under server folder)enabled=1gpgcheck=0then installed httpd everything worked fine on machine a.now turn of machine b 192.168.1.106i created the repository to access the rpms from machine a and contents are#vi remoteftp.repo[rhel-remoteftp.rep]name=bla bla blabaseurl=<url> then i tried to install package and it gave me this errornote: there is no firewall in between themdo i have to install ftp server also on machine b",
    "present_kp": [
      "rhel",
      "yum",
      "ftp"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "virtualbox-5.0 : depends: libvpx2 (>= 1.4.0) but it is not installable. i'm trying to install virtualbox 5.0 in my ubuntu machine linux federico 4.4.0-24-generic #43-ubuntu smp wed jun 8 19:27:37 utc 2016 x86_64 x86_64 x86_64 gnu/linuxwhen i run apt- install i receive this error:root@federico:~# apt-get install virtualbox-5.0reading package lists... donebuilding dependency tree reading state information... donesome packages could not be installed. this may mean that you haverequested an impossible situation or if you are using the unstabledistribution that some required packages have not yet been createdor been moved out of incoming.the following information may help to resolve the situation:the following packages have unmet dependencies. virtualbox-5.0 : depends: libvpx2 (>= 1.4.0) but it is not installablee: unable to correct problems, you have held broken packages.i followed this link without success. virtualbox-5.0 won't install on ubuntu-server 14.04.4any suggestion?",
    "present_kp": [
      "ubuntu",
      "virtualbox",
      "dependencies"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "fuji xerox docuprint cm305df not detected in sane for scanning. is anyone able to help a techno tragic install a fuji xerox docuprint cm305df please in linux mint 17 (quiana) cinnamon. i have tried to work out the thing with sane but cannot get any thing to detect the scanner (gscan2pdf, simplescan, xsane & gimp) printer works fine as a network printer no problem. i have used all the tips & tweaks i can find, even though it was a little nervy using the terminal in case i stuff things up. i have run lsusb it shows fuji xerox inc on a usb port & if i switch usb ports lsub picks it up on whatever port i plug it into. whether this is just picking it up as a usb printer i cant say. all the effort over the last couple of weeks points to this in sane, but i don't know how to install it. if someone could please take me through it step by step i would be very grateful. thank you in advance linux\\ ubuntu society description the sane-xerox_mfp library implements a sane (scanner access now easy) backend that provides access to the following usb and network multifunction-peripheral: phaser 3200mfp dell mfp laser printer 1815dn xerox phaser 6110mfp samsung clx-3170fn & clx-3175fw samsung scx-4200 samsung scx-4300 samsung scx-4500 samsung scx-4500w samsung scx4725-fn xerox workcentre 3119 series",
    "present_kp": [],
    "absent_kp": [
      "debian"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "cups: connection timed out. per title i'm showing connection timed out in the /var/log/cups/error_log. d [29/oct/2015:17:43:32 -0400] cupsdsetbusystate: printing jobsd [29/oct/2015:17:44:04 -0400] [job 11441] connection error: connection timed outi find that if i change the default output policy to accept, cups processes my print jobs without a hitch. here is my iptables config (showing policy drop)chain output (policy drop 4 packets, 262 bytes) pkts bytes target prot opt in out source destination 61345 14m accept all -- * * 0.0.0.0/0 0.0.0.0/0 state related,established 60 3600 accept tcp -- * * 0.0.0.0/0 0.0.0.0/0 multiport dports 80,443,636 state new,established 0 0 accept icmp -- * * 0.0.0.0/0 0.0.0.0/0 state new,related,established 1 76 accept udp -- * * 0.0.0.0/0 0.0.0.0/0 udp dpt:123 state new,established 10 600 accept tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:22 state new,established 4 240 accept tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:25 state new,established 265 15900 accept tcp -- * * 0.0.0.0/0 0.0.0.0/0 multiport dports 1433 state new,established 26 1662 accept tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:445 state new,established 0 0 accept tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:631 state new,established 0 0 accept udp -- * * 0.0.0.0/0 0.0.0.0/0 udp dpt:631 state new,established 0 0 accept tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:139 state new,established 6143 2445k accept udp -- * * 0.0.0.0/0 0.0.0.0/0 udp dpt:514 state new,established 0 0 accept tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:4443 state new,established i'm allowing outbound traffic on port 631 (upd and tcp). if i relax output to accept, the print queue processes normally, but i am not showing any outbound traffic on port 631.here's the relevant section from /etc/cups/cupsd.conf: # only listen for connections from the local machine.listen localhost:631listen /var/run/cups/cups.sockthis configuration worked until yesterday when i moved the printer to a new vlan (from 2.2.2.2 to 3.3.3.3). i can ping the printer ok with the firewall up. what could be going on with my iptables config to prevent print jobs from being sent to the printer?",
    "present_kp": [
      "iptables",
      "cups",
      "output"
    ],
    "absent_kp": [
      "centos"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is a zombie process or thread?. what is a zombie process or thread, and what creates them? do i just kill them, or can i do something to get diagnostics about how they died?",
    "present_kp": [],
    "absent_kp": [
      "multithreading",
      "debugging",
      "linux development"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is problem pt07x on spoj can be solved using bottom-up approach dp?. you are given an unweighted, undirected tree. write a program to find a vertex set of minimum size in this tree such that each edge has as least one of its end-points in that set.inputthe first line of the input file contains one integer n --- number of nodes in the tree (0 < n <= 100000). next n-1 lines contain n-1 edges of that tree --- each line contains a pair (u, v) means there is an edge between node u and node v (1 <= u,v <= n).outputprint number of nodes in the satisfied vertex set on one line.example 1input:31 21 3output:1explanation:the set can be {1}example 2input:31 22 3output:1explanation:the set can be {2}i solved it using recursion(top-down) apporach can it be solved using bottom-up manner?",
    "present_kp": [],
    "absent_kp": [
      "dynamic programming"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to understand pipes. when i just used pipe in bash, i didn't think more about this. but when i read some c code example using system call pipe() together with fork(), i wonder how to understand pipes, including both anonymous pipes and named pipes.it is often heard that everything in linux/unix is a file. i wonder if a pipe is actually a file so that one part it connects writes to the pipe file, and the other part reads from the pipe file? if yes, where is the pipe file for an anonymous pipe created? in /tmp, /dev, or ...? however, from examples of named pipes, i also learned that using pipes has space and time performance advantage over explicitly using temporary files, probably because there are no files involved in implementation of pipes. also pipes seem not store data as files do. so i doubt a pipe is actually a file.",
    "present_kp": [
      "pipe"
    ],
    "absent_kp": [
      "shell",
      "system calls",
      "architecture"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is there a way to demux a h264 video stream from an flv (h264, aac)?. using ffmpeg to demux an h264 stream from a .flv video always gives me a stream which i cannot remux into a .mp4 container (using either mp4box from package *gpac), or ffmpeg). here is the demuxing command which produces the invalid stream.. (unknown format) ffmpeg -i (h264-aac).flv # get stream info # stream #0.0: video: h264, yuv420p, 320x240 [par 1:1 dar 4:3], 259 kb/s, 29.97 tbr, 1k tbn, 59.94 tbcffmpeg -y -map 0.0 -vcodec copy -an -i (h264-aac).flv \\ (h264-aac).flv.h264 i've also tried it with: -f h264, but get the same error. the above command works when run against a video in a .mp4 container, so maybe this is something specific to flv(?).the closest i've come to isolating the h264 video stream is with the following command; but it wraps the stream in an flv container which has lost its aspect ratio.... ffmpeg -y -map 0.0 -vcodec copy -an -i (h264-aac).flv \\ (h264-aac).flv.new.flv is there some way to grab this video stream? ps. i've treid with several different input .flv vids, ...same error.",
    "present_kp": [
      "video",
      "ffmpeg"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "android phone with removeable battery, sd card, usb host/otg for ~100 - $150. i'm looking for a new or gently used phone for the first time in a while. they all seem to look alike, and the features i care about are not well advertised. i have an at&t note 4, which is not able to get custom roms loaded to it due to a locked bootloader, though temp root has been achieved.i want to use the t-mobile $30 walmart plan (100 minutes, unlimited texts, 5gb high speed data) with this new-to-me phone. t-mobile is gsm and therefore requires a sim card. i'm looking for a phone for use in the usa with the following criteria:removable batterymicrosd card slotthe phone has 2gb or more of ramusb host or usb otg supportthe phone has been rootedthe phone is reasonably popular and has roms available somewhere, preferably across all of its carrierscosts $100 - $150 ishwish list (probably not gonna happen)hardware keyboardi thought by now there might be midrange or budget phones that meet this spec, but i can't seem to find any. i think my best bet is going to be ebay.here are some phones i have found that meet these criteria:samsung note 2 (ebay)samsung galaxy s3 (ebay)these meet the criteria except for price:samsung galaxy s4samsung note 3samsung note 4i have found the site gsmarena and while it has all the information i am looking for, i cannot search by removable battery nor usg host/otg. there are somewhere between 70 and 200 phones for me to search through.what phones do you recommend?",
    "present_kp": [
      "android"
    ],
    "absent_kp": [
      "smartphones"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "can i filter or query an importrange in a single formula without importing the range multiple times or using a 2nd worksheet?. title sums it up, i want to be able to filter an importrange in a single formula, can this be done without importing the range multiple timer in the filter? i want to do this without using a 2nd worksheet to hold the importrange.ie. filter(myimportrange, importrangepart = mycondition) since this has to import the range twice.i also cannot seem to query the importrange since this results in blank cells as seen here:",
    "present_kp": [],
    "absent_kp": [
      "google spreadsheets"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what makes (or why is) a language server-side?. since swift 3 is boasting its new capabilities, it occurs to me that i don't actually know why swift is server-side. i've been hunting full stack and the listings have mean, they have c#/asp, some do mongodb, hadoop, rest/sql, but my main point is the interface language.i was thinking, could it be network requests? all languages do remote procedure calls. then i was like what about http requests?, but they all do that also. i found a similar question on differences between, (posted in comments). but in nothing more than an epiphany, i realized all of them are interpreted on some level.are they server-side because they run on the spot?, yet swift is compiled and java is a gray area (pardon my ignorance with bytecode if i'm wrong here).server-side javascript can take a post request and process it now, but so can wordpress with an .htaccess modification (well, technically apache's server, but still, the idea of a dynamic request stays the same).so... why is java with grails a thing, but you never hear of c++ running a database request as full stack dev?based on my level of ignorance, a simple they are interpreted if that is it, would be fine. or it can grab from a database or it processes queries fast enough to not hit the tcp/ip timeout limit of 60sec. i just, don't know why php is server-side and objective c/c can't be.i understand practicality on shouldn't be, because too complex, but the solid, defining line on can/can't be server-side by taking requests and returning a database call, that is puzzling.edit: before writing this, i kept the idea that there's only one type of server. but apache would serve text and while that text is processing it would have data to process while swift/express.js/node would be the server and feed the data directly. i suppose my question pertained to html and has script vs the is the server returning the text, but the answers are still the same. apache i guess is more boxed into a package where i need to use a language that apache runs. but otherwise can make a daemon in any language to serve pages using 8080.thank you.",
    "present_kp": [
      "rest",
      "http request"
    ],
    "absent_kp": [
      "client server",
      "dynamic languages"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "systemd: running a script on shutdown after filesystems are mounted read-only. is there a way to run a script on shutdown, after the file system is remounted as read-only?i've a raspberry pi connected to a wireless socket, which i can control via a sender and a script. i want to power off that socket (powering off the raspberry) on shutdown, after the file system is mounted read-only.i've tried this:[unit]description=testdefaultdependencies=norequires=shutdown.target umount.target final.targetafter=shutdown.target umount.target final.target[service]type=oneshotexecstart=/testkillmode=none[install]wantedby=halt.targetthe script /test does output the current mounts. when it's run on shutdown, it states read/write for the root file system and not read-only as expected.edit:content of /test:#!/bin/bashecho -n 'debug-mount: ' > /dev/tty1cat /proc/mounts | grep /dev/sda > /dev/tty1screen output on shutdown:",
    "present_kp": [
      "systemd",
      "shutdown"
    ],
    "absent_kp": [
      "init script",
      "readonly",
      "unmounting"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is there a name for complement of recursively enumerable set?. if there is a recursive function$$f(x) = egin{cases}1 & ext{if $x otin a$}\\ ext{undefined} & ext{if $x \\in a$}\\\\end{cases}$$is there a special name for the class of sets like $a$?",
    "present_kp": [],
    "absent_kp": [
      "terminology",
      "computability",
      "undecidability"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "member restricted game with api. my website is poorly coded.the structure is pretty simple:a rewriterule redirects /.... to /index.php?page=$1mypages/page.php contains the page content and actions (model controller and view glued together...)index.php wrap the demanded page into a theme (basically include header and footer around it)the site is a complex mmorpg site (~40 differents pages). i need to convert each page to a view in my iphone/android app (probably made in js+html with phonegap). so i guess i'll have to create an api and separate the model/controller/view on each page.i don't know how to manage the connection, but i'm sure i'm doing it in a bad way (see code below).my controller looks like this:<?php// database connection, get $_get(page) ...if (file_exists($page)) { if(ismemberrestricted($page)) { $loaduserinf = true; if(!isvalid($_session['usern'], $_session['passw'], $db)) { if(!isvalid($_cookie['cookie_password'] , $_cookie['cookie_password'], $db)) { if(!isvalid($_post['usern'], $_post['passw'], $db)) { if(isvalid($_post['usern'], $_post['passw'], $dbt)) { $page = controller_path . /desarchivage.php; } else { $page = controller_path . /notconnected.php; } $loaduserinf = false; } } } else { // not yet connected if($loaduserinf) // but connexion accepted connexion (); } } if($loaduserinf) { require ('userinf.php'); // loads informations about the connected user pageroutine(); // stuff to do on each member restricted page }} else { $page = controller_path . /404.php;}require ($header);require ($page);require ($footer);?>i really don't know where to start. i've read about mvc/api. i understood the theory, but i don't know where to begin with an existing project.",
    "present_kp": [
      "php",
      "mvc",
      "api"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "recommendation for a cheap and small computer for usb webcam microscope. i require a cheap and small computer to act as the monitor of a webcam-microscope. the computer must be able to display the output of the usb webcam microscope with as low a time delay as possible (<50ms delay) and display the output on a monitor with a wired connection (hdmi, dvi or vga). the computer, usb microscope and monitor are intended to be placed inside a biosafety hood much like this one. it will allow the user to be able to view and manipulate small objects under the microscope without needing to lift the glass safety sash, as one will need to do when using a normal binocular microscope. i have considered the raspberry pi for this purpose, but my question on the rbp se seemed to suggest that it may not be fast enough for this purpose. my requirements are as follows:1: the computer must introduce less than 50ms of delay when displaying live images from a usb webcam with a resolution of 720p. 2: under us$400 not including the monitor and microscope3: have at least one form of hardware video output and have at least one usb port. 4: preferably fanless, as fans may disrupt the airflow within the biosafety cabinet. 5: preferably has integrated wifi and bluetooth for remote access to save data without needing to place usb drives into the biosafety cabinet. one method to quantify the webcam time delay would be to point your webcam directly at the computer monitor, and observe the nesting of images. the rate at which the images nest into each other allows the approximate time delay to be measured.",
    "present_kp": [
      "usb"
    ],
    "absent_kp": [
      "scientific instruments"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "x-mst3k in http response header, what it is and how?. on a website of my college i noticed something strange.here:what is it, and how they put it there?i searched for mst3k and this i found:<url>",
    "present_kp": [],
    "absent_kp": [
      "http headers"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what should a test method test?. suppose we're unit testing a class with xunit-like framework - so a test class is created for class-under-test. now, how much should be tested by individual test methods?should there be one-to-one mapping between methods-under-test and test methods:class testmath { testsqrt() { assertequals(math.sqrt(4), 2); assertexception(math.sqrt(-2)); }}or is it better to have one test method per scenario:class testmath { testsqrtofpositivenum() { assertequals(math.sqrt(4), 2); } testsqrtofnegativenum() { assertexception(math.sqrt(-2)); }}which is better and what are pros and cons of both methods?",
    "present_kp": [
      "unit testing"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "installing virtualbox in fedora 21 using rpm fusion is giving me problems. i've recently installed fedora 21 workstation and i want to install the virtualbox package from rpm fusion. but it is giving me problems.$ uname -alinux a10-5800k 3.18.5-201.fc21.x86_64 #1 smp mon feb 2 21:00:58 utc 2015 x86_64 x86_64 x86_64 gnu/linux$ sudo yum install virtualbox kmod-virtualboxloaded plugins: langpacksresolving dependencies--> running transaction check---> package virtualbox.x86_64 0:4.3.20-3.fc21 will be installed--> processing dependency: libsdl-1.2.so.0()(64bit) for package: virtualbox-4.3.20-3.fc21.x86_64---> package kmod-virtualbox.x86_64 0:4.3.20-4.fc21.3 will be installed--> processing dependency: kmod-virtualbox-3.18.6-200.fc21.x86_64 >= 4.3.20-4.fc21.3 for package: kmod-virtualbox-4.3.20-4.fc21.3.x86_64--> running transaction check---> package sdl.x86_64 0:1.2.15-17.fc21 will be installed---> package kmod-virtualbox-3.18.6-200.fc21.x86_64.x86_64 0:4.3.20-4.fc21.3 will be installed--> processing dependency: kernel-uname-r = 3.18.6-200.fc21.x86_64 for package: kmod-virtualbox-3.18.6-200.fc21.x86_64-4.3.20-4.fc21.3.x86_64--> finished dependency resolutionerror: package: kmod-virtualbox-3.18.6-200.fc21.x86_64-4.3.20-4.fc21.3.x86_64 (rpmfusion-free-updates) requires: kernel-uname-r = 3.18.6-200.fc21.x86_64 installed: kernel-core-3.17.4-301.fc21.x86_64 (@koji-override-0/$releasever) kernel-uname-r = 3.17.4-301.fc21.x86_64 installed: kernel-core-3.18.5-201.fc21.x86_64 (@updates) kernel-uname-r = 3.18.5-201.fc21.x86_64 available: kernel-debug-core-3.17.4-301.fc21.x86_64 (fedora) kernel-uname-r = 3.17.4-301.fc21.x86_64+debug available: kernel-debug-core-3.18.5-201.fc21.x86_64 (updates) kernel-uname-r = 3.18.5-201.fc21.x86_64+debug you could try using --skip-broken to work around the problem you could try running: rpm -va --nofiles --nodigestthe problem is that the rpm fusion kmod-virtualbox.x86_64 points to kmod-virtualbox-3.18.6-200.fc21.x86_64. there is an available kmod-virtualbox for my current kernel and i try to install that like so.$ sudo yum install virtualbox kmod-virtualbox-3.18.5-201.fc21.x86_64.x86_64but i still get the same dependency check error as above. i guess my question is how do i install virtualbox so that it uses the kmod with the same kernel fedora 21 is using?i have tried updating as of this post but fedora 21 still doesn't have the 3.18.6-200.fc21 kernel available.",
    "present_kp": [
      "fedora",
      "virtualbox"
    ],
    "absent_kp": [
      "software installation"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "error app domains: domain.com is not a valid domain. i am having issues configuring facebook app. in app domains: field i have entered: domain.com and in website with facebook login site url i have entered <url> but i still get an error that sayserrorapp domains: domain.com is not a valid domain.site url is not a valid url.this is very weird as the domain is working fine and never had such issues before.",
    "present_kp": [
      "facebook"
    ],
    "absent_kp": [
      "facebook apps"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how can i build vim with just python 3 support?. i'm on arch, and recently the vim-python3 and gvim-python3 packages were removed, so i'm working on building vim from source such that the :python command will act as python3. specifically, i'm doing this so that the vim-jedi package will work properly. i tried changing the --enable-pythoninterp=dynamicflag in my pkgbuild to--disable-pythoninterpbut when i did vim --version after building and installing, it still showed+python/dynalso, doing:python import sys;print(sys.version)showed python 2 still. what do i need to change to have only python 3?",
    "present_kp": [
      "installing"
    ],
    "absent_kp": [
      "linux",
      "vimscript python",
      "linux arch"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "sed remove characters from new line. after some processing, i have my input file like below. file1.txt12345|john|student43321|jack|professor78965|alex|lectureri need to process the above file further and so i need a line breaker at the end of the line too. currently, i can accomplish it as below. sed 's/$/|/' file1.txtthe above command results in the output as,12345|john|student|43321|jack|professor||78965|alex|lecturer|as we can see, the | is appended to blank lines too. when i tried to remove the | character again from blank lines using below command,sed 's/|//g' file1.txtthe | character is getting deleted everywhere. how can i delete only the | in blank lines? i need to keep the blank line also.",
    "present_kp": [
      "sed"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "can't push docker image into private repository. i'm trying to push my image to my friend's private repository but i keep getting this error:index response didn't contain any endpoints i have added --insecure-repository to my launch config because it didn't wanted to start at all. i'm using the newest manjaro.[jhajto@dev-station so_backend_old]$ docker infocontainers: 11 running: 0 paused: 0 stopped: 11images: 15server version: 1.11.2storage driver: aufs root dir: /var/lib/docker/aufs backing filesystem: extfs dirs: 44 dirperm1 supported: truelogging driver: json-filecgroup driver: cgroupfsplugins: volume: local network: bridge null hostkernel version: 4.4.13-1-manjarooperating system: manjaro linuxostype: linuxarchitecture: x86_64cpus: 4total memory: 7.785 gibname: dev-stationid: cxrt:oqvn:f7o5:dnmu:fv2c:gzne:kx6d:oinq:sxlb:23hz:ngbu:5wbcdocker root dir: /var/lib/dockerdebug mode (client): falsedebug mode (server): falseregistry: <url>",
    "present_kp": [
      "docker",
      "manjaro"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "writing a 3d array from petsc. i am trying to do something fairly simple somehow i have made it hard.is there an example of how to send a 3d array to a binary file?",
    "present_kp": [
      "petsc"
    ],
    "absent_kp": [
      "matrices"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "'fast interrupts' in linux. as far as i know, linux has 'fast interrupts', those that were requested with sa_interrupt flag; fast interrupts are executed with all other interrupts disabled on the current cpu. but how does it differ from the normal interrupt handler behavior (where)?",
    "present_kp": [
      "linux",
      "interrupt"
    ],
    "absent_kp": [
      "kernel"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "print the system clipboard content in bash. there is a way to put some command output to system clipboard via xclip.some-command | xclip -selection clipboardi'd like to perform a reverse task - print system clipboard to terminal. how it can be done?",
    "present_kp": [
      "bash",
      "clipboard"
    ],
    "absent_kp": [
      "xorg"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "term for expected randomized given advice string?. problems which run in deterministic polynomial time given polynomial sized advice are in class p/poly. problems which run in probabilistic polynomial time given polynomial sized advice are still in class p/poly. what about problems which have an expected randomized polynomial time given polynomial sized advice under uniform distribution of inputs where expected here signifies average in the sense that a very small portion of inputs are allowed to run in exponential time using the algorithm?",
    "present_kp": [],
    "absent_kp": [
      "complexity theory",
      "randomness"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "complexity of intersection of regular languages as context-free grammars. given regular expressions $r_1, \\dots, r_n$, are there any non-trivial bounds on the size of the smallest context-free grammar for $r_1 \\cap \\cdots \\cap r_n$?",
    "present_kp": [
      "regular language"
    ],
    "absent_kp": [
      "fl.formal languages",
      "context free"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "get nth item from end of list. i am starting out in haskell and thought i would try to make a function that got the index (nth) from the end of the list.so getnthfromend 5 [1..10] would equal 5the code i have so far isgetnthfromend :: int -> [a] -> agetnthfromend _ [] = error empty list givengetnthfromend nth list | listlength < nth = error $ list length must be greater than ++ show nth | listlength == nth = head list | listlength >= nth = list !! (listlength - nth - 1) where listlength = length listgetnthfromend _ _ = error unknown exception occurredthe code works as intended as far as i can tell, but seeing as though i am just getting to know haskell i thought your suggestions on how this code could be cleaned up or refactored to make better use of fp concepts would be invaluable to my learning of haskell and the fp paradigm overall.",
    "present_kp": [
      "haskell"
    ],
    "absent_kp": [
      "beginner"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "dmar: failed to find handle for acpi object. i'm using a lenovo b50-50 with this specs:intel core i3-5005u cpu @ 2.00ghz 4intel hd graphics 5500 (broadwell gt2)hard disk 500 gbram 4 gbi'm currently running debian(linux debian 3.16.0-4-amd64 #1 smp debian 3.16.39-1+deb8u2 (2017-03-07) x86_64 gnu/linux)the problem is that when i boot up the system (i am currently booting in quiet splash mode) the following messages are displayed:[ 0.835186] dmar: failed to find handle for acpi object _sb.pci0.sdma[ 0.835243] dmar: failed to find handle for acpi object _sb.pci0.sdhceverything works fine after the boot.what can, or should, i do about these messages?",
    "present_kp": [
      "debian",
      "boot",
      "acpi"
    ],
    "absent_kp": [
      "linux kernel",
      "drivers"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what makes a good scrum master?. how do i identify a good scrum master?here are some possibilities:the person is agile (doesn't just do agile). indicators: blog, volunteer activitiesthe person connects well with others at the level of emotions and needs (not just technical stuff).the person is relentless and fearless when removing impediments",
    "present_kp": [
      "scrum",
      "scrum master"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "where to put details about the acceptance criteria of a user story?. in this blog post about acceptance criteria the author explains that good acceptance criteria should:state an intent not a solution (e.g. the user can choose an accountrather than the user can select the account from a drop-down)are independent of implementation (ideally the phrasing would be thesame regardless whether this feature/story would be implemented one.g. web, mobile or a voice activated system)are relatively high level (not every detail needs to be in writing)and further details such as:the column heading is balancethe rolling balance format is 99,999,999,999.9 d/crwe should use a dropdown rather than checkboxesshould be moved to either a team internal documentation or automated acceptance testshowever,i often hear people frowning about using cucumber or similar frameworks for doing gui tests.moreover, using an internal documentation could generate lots of problems due to failure to update the documentation regularly.i'm still struggling to find an effective way to capture such details during the conversation with the customer.",
    "present_kp": [
      "user story"
    ],
    "absent_kp": [
      "agile",
      "development process",
      "scrum",
      "requirements"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is this class too over the top?. in response to my previous question, i've refactored my class quite a bit. i've truncated my class a bit, because i have several methods that are very similar, with the exception of the query for the database. the class itself is at the 930 line number. so, a little background. i'm not only the sole programmer on this project, but also in the company. and this is the first time i've ever attempted anything like this (only out of school 3 years). the purpose of the program is to query a local database, and generate a daily report. i've broken it up into two projects: wpf, and report class library (what you see below). i already knew i didn't want the ui project to actually do the calculations. so i broke it off into the class library. i also knew i didn't want to just query the whole database and store all it's data in memory. so, i have each calculation calling a query method. i had looked into stored procedures, but this is a sqlite database, and stored procedures aren't supported.now, with all that in mind, i'm not really sure how best to improve this any further. so here i am, asking you guys: is this too over the top, or is it fine?using system;using system.collections.generic;using system.data.sqlite;using system.linq;using mysql.data.mysqlclient;using nlog;namespace dailyreport{ public class report { #region global variables #region enums public enum timeperiod { daily, total, none } #endregion enums private static readonly logger logger = logmanager.getlogger(reportlogger); public string connectionstringfile { get; private set; } #region query parameters public string starttime { get; set; } public string endtime { get; set; } public string date { get; set; } #endregion query parameters #endregion global variables #region constructors public report(string stringfilename, datetime date = default(datetime), string start = 0, string end = 0) { logger.info(creating a new report...); starttime = start; endtime = end; date = date.tostring(yyyy-mm-dd); setstartendtimes(); connectionstringfile = string.format(@data source=c:\\path\\{0}.hdd;version=3;new=false;compress=true;, stringfilename); logger.info(report created); } public report() { } #endregion constructors #region calculations #region hours /// <summary> /// drilling hours /// </summary> /// <param name=period>enum: period of time</param> /// <param name=timeframe>bool: if the calculation is for a specific time frame</param> /// <returns>double: drilling hours</returns> public double drillinghours(timeperiod period = timeperiod.daily, bool timeframe = false) { logger.info(calculating drilling hours...); const string query = select timestamp from mydatabase where measured_dist = bit_loc and rop > 0; var templist = getdatabaseresults(timeframe, query, period); var drillinghours = calculatehours(templist); logger.info(period + drilling hours calculated.); return drillinghours; } /// <summary> /// circulating hours /// </summary> /// <param name=period>enum: period of time</param> /// <param name=timeframe>bool: if the calculation is for a specific time frame</param> /// <returns>double: circulating hours</returns> public double circulatinghours(timeperiod period = timeperiod.daily, bool timeframe = false) { logger.info(calculating circulating hours...); const string query = select timestamp from mydatabase where pump_press > 100; var templist = getdatabaseresults(timeframe, query, period); var circulatinghours = calculatehours(templist); logger.info(period + circulating hours calculated.); return circulatinghours; } /// <summary> /// nondrillingactivity hours /// </summary> /// <param name=period>enum: period of time</param> /// <param name=timeframe>bool: if the calculation is for a specific time frame</param> /// <returns>double: nondrillingactivity hours</returns> public double nondrillingactivityhours(timeperiod period = timeperiod.daily, bool timeframe = false) { logger.info(calculating nondrillingactivity hours...); const string query = select timestamp from mydatabase where pump_press <= 100 and rop = 0; var templist = getdatabaseresults(timeframe, query, period); var nondrillingactivityhours = calculatehours(templist); logger.info(period + nondrillingactivity hours calculated.); return nondrillingactivityhours; } #endregion hours /// <summary> /// footage /// </summary> /// <param name=period>enum: period of time</param> /// <param name=timeframe>bool: if the calculation is for a specific time frame</param> /// <returns>double: footage</returns> public double footage(timeperiod period = timeperiod.daily, bool timeframe = false) { logger.info(calculating footage...); var query = period == timeperiod.daily ? select measured_dist from mydatabase where date = @date : select measured_dist from mydatabase; var templist = getdatabaseresults(timeframe, query); var dailydepths = templist.select(convert.todouble).tolist(); dailydepths.sort(); var footage = math.round((dailydepths.last() - dailydepths.first()), 2, midpointrounding.awayfromzero); logger.info(period + footage calculated.); return footage; } /// <summary> /// rop average /// </summary> /// <param name=footage>double: footage drilled for the day</param> /// <param name=drillinghours>double: drilling hours for the day</param> /// <param name=period>enum: period of time</param> /// <returns>double: rop average</returns> public double ropavg(double footage, double drillinghours, timeperiod period = timeperiod.daily) { logger.info(calculating rop avg...); double ropavg = 0; if(footage > 0 && drillinghours > 0) { ropavg = math.round(footage / (drillinghours*60), 2, midpointrounding.awayfromzero); } logger.info(period + rop avg calculated.); return ropavg; } /// <summary> /// pump strokes /// </summary> /// <param name=pump>int: pump being evaluated</param> /// <param name=drillinghours>double: drilling hours for the day</param> /// <param name=period>enum: period of time</param> /// <param name=timeframe>bool: if the calculation is for a specific time frame</param> /// <returns>double: pump strokes</returns> public double pumpstrokes(int pump, double drillinghours, timeperiod period = timeperiod.daily, bool timeframe = false) { logger.info(calculating pump strokes...); string query; switch(pump) { case 1: query = period == timeperiod.daily ? select pump_1_strokes_pm from mydatabase where date = @date : select pump_1_strokes_pm from mydatabase; break; case 2: query = period == timeperiod.daily ? select pump_2_strokes_pm from mydatabase where date = @date : select pump_2_strokes_pm from mydatabase; break; default: throw new nodataexception(there is no pump number + pump); } ienumerable<string> templist; switch (period) { case timeperiod.daily: templist = getdatabaseresults(timeframe, query); break; case timeperiod.total: templist = getdatabaseresults(timeframe, query, period); break; default: throw new nodataexception(this should never happen.); } var pumpstrokes = templist.select(convert.todouble).tolist(); var totalstrokes = math.round((pumpstrokes.sum() / pumpstrokes.count) * (drillinghours * 60), 2, midpointrounding.awayfromzero); logger.info(period + pump + pump strokes calculated.); return totalstrokes; } /// <summary> /// psi max /// </summary> /// <param name=period>enum: period of time</param> /// <param name=timeframe>bool: if the calculation is for a specific time frame</param> /// <returns>double: psi max</returns> public double psimax(timeperiod period = timeperiod.daily, bool timeframe = false) { logger.info(calculating max psi...); var query = period == timeperiod.daily ? select pump_press from mydatabase where date = @date : select pump_press from mydatabase; var templist = getdatabaseresults(timeframe, query); var psilist = templist.select(convert.todouble).tolist(); var psimax = math.round(psilist.max(), 2, midpointrounding.awayfromzero); logger.info(period + max psi calculated.); return psimax; } /// <summary> /// torque avg /// </summary> /// <param name=drillinghours>double: drilling hours for the day</param> /// <param name=period>enum: period of time</param> /// <param name=timeframe>bool: if the calculation is for a specific time frame</param> /// <returns>double: torque avg</returns> public double torqueavg(double drillinghours, timeperiod period = timeperiod.daily, bool timeframe = false) { logger.info(calculating avg torque...); var query = period == timeperiod.daily ? select torque from mydatabase where date = @date : select torque from mydatabase; var templist = getdatabaseresults(timeframe, query); var torquelist = templist.select(convert.todouble).tolist(); if(torquelist.sum() > 0 && drillinghours > 0) { var torqueavg = math.round((torquelist.sum() / torquelist.count), 2, midpointrounding.awayfromzero); logger.info(period + avg torque calculated.); return torqueavg; } logger.info(torque avg = 0 because torque sum = + torquelist.sum() + or drillinghours = + drillinghours); return 0; } /// <summary> /// spm avg /// </summary> /// <param name=pumpstrokes1></param> /// <param name=pumpstrokes2></param> /// <param name=drillinghours>double: drilling hours for the day</param> /// <param name=period>enum: period of time</param> /// <param name=timeframe>bool: if the calculation is for a specific time frame</param> /// <returns>double: spm avg</returns> public double spmavg(double pumpstrokes1, double pumpstrokes2, double drillinghours, timeperiod period = timeperiod.daily, bool timeframe = false) { logger.info(calculating avg spm...); var totalpumpstrokes = pumpstrokes1 + pumpstrokes2; if (totalpumpstrokes > 0 && drillinghours > 0) { var spmavg = math.round(totalpumpstrokes/(drillinghours*60), 2, midpointrounding.awayfromzero); logger.info(period + avg spm calculated.); return spmavg; } logger.info(spm avg = 0 because spm sum = + totalpumpstrokes + or drillinghours = + drillinghours); return 0; } /// <summary> /// psi off bottom /// </summary> /// <param name=period>enum: period of time</param> /// <param name=timeframe>bool: if the calculation is for a specific time frame</param> /// <returns>double: psi off bottom</returns> public double psioffbottom(timeperiod period = timeperiod.daily, bool timeframe = false) { logger.info(calculating psi off bottom...); const string query = select pump_press from mydatabase where measured_dist != bit_loc; var templist = getdatabaseresults(timeframe, query, period); var psilist = templist.select(convert.todouble).tolist(); var psioffbottom = math.round(psilist.max(), 2, midpointrounding.awayfromzero); logger.info(period + psi off bottom calculated.); return psioffbottom; } #endregion calculations #region everything else private static double calculatehours(ienumerable<string> projecthours) { return convert.todouble(math.round(timecalculations(convertstringlisttodatetimelist(projecthours)).totalhours, 2, midpointrounding.awayfromzero)); } private ienumerable<string> getdatabaseresults(bool timeframe, string query, timeperiod period = timeperiod.none) { var daily = period == timeperiod.daily; query = buildquery(query, daily, timeframe); var templist = executequery(query); if (templist.any()) { return templist; } throw new nodataexception(there was no data for the selected time frame. please select another.); } private static timespan timecalculations(ilist<datetime> timestamps) { var interval = new timespan(0, 0, 10); var totaltime = new timespan(); for (var j = 0; j < timestamps.count - 1; j++) { if (timestamps[j + 1].subtract(timestamps[j]) > interval) continue; var timedifference = timestamps[j + 1].subtract(timestamps[j]); totaltime = totaltime.add(timedifference); } return totaltime; } private list<string> executequery(string query) { logger.info(executing query...); var templist = new list<string>(); try { using (var connection = new sqliteconnection(connectionstringfile)) { using (var command = new sqlitecommand(query, connection)) { connection.open(); command.parameters.add(new sqliteparameter(@date, date)); command.parameters.add(new sqliteparameter(@starttime, starttime)); command.parameters.add(new sqliteparameter(@endtime, endtime)); using (var reader = command.executereader()) { while (reader.read()) { for (var i = 0; i < reader.fieldcount; i++) { templist.add(reader.getvalue(i).tostring()); } // for } // while } // using reader } // using command } // using connection } // try catch (exception ex) { logger.error(ex.message); } logger.info(query complete); return templist; } private static string buildquery(string query, bool daily, bool timeframe) { logger.info(building query...); const string dailyadd = and date = @date; const string timeframeadd = and timestamp between @starttime and @endtime; // stack overflow would hate this so much if (daily) { query += dailyadd; } if (timeframe) { query += timeframeadd; } logger.info(query built); return query; } // the list of strings should be datetimes so that we can calculate hours private static list<datetime> convertstringlisttodatetimelist(ienumerable<string> stringlist) { var datetimelist = stringlist.select(convert.todatetime).tolist(); return datetimelist; } /// <summary> /// get list of unique dates. these are dates that have information in the database. /// </summary> /// <returns>list of unique dates</returns> public list<string> getuniquesdates() { var dates = new list<string>(); const string query = select date from mydatabase; try { using (var connection = new sqliteconnection(connectionstringfile)) { using (var command = new sqlitecommand(query, connection)) { connection.open(); using (var reader = command.executereader()) { while (reader.read()) { for (var i = 0; i < reader.fieldcount; i++) { dates.add(reader.getvalue(i).tostring()); } } } } } } catch (exception ex) { logger.error(ex.message); } dates.sort(); return dates.distinct().tolist(); } /// <summary> /// sets the global variables for starttime and endtime /// </summary> /// starttime and endtime won't change at all throughout the live of the report object private void setstartendtimes() { // because the timestamp retrieved will be a string, and will be in yyyy-mm-dd hh:mm:ss format if (starttime.equals(0)) return; starttime = date + string.format(starttime, hh:mm:ss); endtime = date + string.format(endtime, hh:mm:ss); } #endregion everything else }}",
    "present_kp": [],
    "absent_kp": [
      "c#"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how can psychometry measure the very high iq's in adults?. from wikipedia's page on mental age, we learn that:originally, the differences between mental age and chronological age were used to compute the intelligence quotient, or iq. this was computed using the ratio method, with the following formula: mental age/chronological age * 100 = iq. no matter what the child's chronological age, if the mental age is the same as the chronological age, then the iq will equal 100.[4] an iq of 100 thus indicates a child of average intellectual development. for a gifted child, the mental age is above the chronological age, and the iq is higher than 140; for a mentally retarded child, the mental age is below the chronological age, and the iq is below 70.in this way, precocious children can get exceptionally high scores (e.g. scores above 160, s.d. 15), even if they answer correctly only a fraction of the total number of questions, provided the test is sufficiently hard in comparison with their actual chronological age. however, i am not aware of any tests for adults that discriminate accurately above the threshold of iq $\\sim$ 160 - their accuracy is even less than 4 s.d., taking into account a ceiling effect. how can psychometry assess an iq of 170 or 180 in adults when the rarity of such scores is 1 in several millions?can the iq scores of exceptionally and profoundly gifted children be extrapolated into adulthood?is it true that that a profoundly gifted child with an iq of 170 would score better than 99.9998% of the general population?it seems that two different definitions of iqs are used here: is there a correlation between them?",
    "present_kp": [
      "intelligence"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "python gui for cropping and saving images quickly. i wrote a simple gui applications to help me select 'positive' regions of a bunch of photos for the purpose of training an object detectir using opencv haar cascades. for training purposes, you need a series of 'positive' images which show the kind of item you want, closely cropped, and then you need a bunch of negative/background images that don't have the item at all. this script helps you generate both at once by paging through photos and quickly generating the rectangle you want to use to crop the image to select the 'positive' region. i'd like feedback on everything, of course, but particularly on whether the division of labor is sensible and also on formatting. i have the feeling i could make this code a lot more readable. imageeditor.pyimport tkinterimport image, imagetkfrom tkinter import tk, bothfrom ttk import frame, button, styleimport imagefeedimport imagefilefeedimage_directory = /home/whales/imgs/positive_directory = /home/whales/tkinter/positive/negative_directory = /home/whales/tkinter/negative/image_resize_factor = .2class imageeditor(frame): class imageeditor provides functionality to page through photos so that users can select a portion of each photo to be saved separately as a 'positive' image and the rest to be saved separately as four 'negative' images def __init__(self, parent): initializes the window with access to an imagefeed class that supplies from and saves images to the appropriate locations frame.__init__(self, parent) self.parent = parent self.corners = [] self.image_feed = imagefeed.imagefeed(imagefilefeed.filefeed(image_directory, positive_directory, negative_directory), image_resize_factor) self.image = self.image_feed.returntkimage() self.canvas = none self.initui() self.resetcanvas() def initui(self): adds a tkinter canvas element that tracks mouse clicks to select image region for saving self.style = style() self.style.theme_use(default) self.pack(fill=both, expand=1) self.canvas = tkinter.canvas(self, width = self.image.width(), height = self.image.height()) self.canvas.bind(<button-1>, self.onmousedown) self.canvas.pack() nextbutton = button(self, text=next, command=self.next) nextbutton.place(x=0, y=0) resetbutton = button(self, text=reset, command=self.reset) resetbutton.place(x=0, y=22) def next(self): saves current edits and advances to the next image if len(self.corners) == 2: self.image_feed.writeimages(self.corners) self.image_feed.nextimage() self.reset() def resetcanvas(self): resets all canvas elements without advancing forward self.image = self.image_feed.returntkimage() self.canvas.create_image(0, 0, image=self.image, anchor=nw) self.canvas.configure(height = self.image.height(), width = self.image.width()) self.canvas.place(x = 0, y = 0, height = self.image.height(), width = self.image.width()) def reset(self): removes all drawings on the canvas so user can start over on same image self.corners = [] self.canvas.delete(all) self.resetcanvas() def onmousedown(self, event): records location of user clicks to establish cropping region self.corners.append([event.x, event.y]) if len(self.corners) == 2: self.canvas.create_rectangle(self.corners[0][0], self.corners[0][1], self.corners[1][0], self.corners[1][1], outline ='cyan', width = 2)def main(): root = tk() root.geometry(250x150+300+300) app = imageeditor(root) root.mainloop() if __name__ == '__main__': main() imagefeed.pyimport cv2import image, imagetkclass imagefeed: the imagefeed class manages all operations related to loading, saving, and formatting images for presentation. the imagefeed has a member file manager that determines what files it loads. the imagefeed supplies a tkinterimage to requester objects. def __init__(self, file_feed, rescale_factor): self.file_feed = file_feed self.rescale_factor = rescale_factor self.image = none self.cv_img = none self.nextimage() def returntkimage(self): return self.image def nextimage(self): calls the file feed's method to advance in the file list and then loads and formats the next image file. img = cv2.imread(self.file_feed.next_file()) self.cv_img = img img_small = cv2.resize(img, (0,0), fx = self.rescale_factor, fy = self.rescale_factor) b, g, r = cv2.split(img_small) img_small = cv2.merge((r,g,b)) im = image.fromarray(img_small) self.image = imagetk.photoimage(image=im) def writeimages(self, corners): writes the single 'positive' image to the positive directory and the four 'negative' images to the negative directory. the 'negative' images are the four rectangles around the positive image that do not contain the positive image. the parameter corners supplies two diagonal points of the rectangle enclosing the 'positive' region of the image. new_img = self.cv_img[corners[0][1]/self.rescale_factor:corners[1][1]/self.rescale_factor, corners[0][0]/self.rescale_factor:corners[1][0]/self.rescale_factor] cv2.imwrite(.join(self.file_feed.get_positive_file()), new_img) low_x = min(corners[0][0], corners[1][0])/self.rescale_factor high_x = max(corners[0][0], corners[1][0])/self.rescale_factor low_y = min(corners[0][1], corners[1][1])/self.rescale_factor high_y = max(corners[0][1], corners[1][1])/self.rescale_factor neg_file_name = self.file_feed.get_negative_file(); new_img = self.cv_img[ :low_y, :] cv2.imwrite({}{}{}.format(neg_file_name[0], ly, neg_file_name[1]), new_img) new_img = self.cv_img[ high_y: , :] cv2.imwrite({}{}{}.format(neg_file_name[0], hy, neg_file_name[1]), new_img) new_img = self.cv_img[ :, :low_x ] cv2.imwrite({}{}{}.format(neg_file_name[0], lx, neg_file_name[1]), new_img) new_img = self.cv_img[:, high_x: ] cv2.imwrite({}{}{}.format(neg_file_name[0], hx, neg_file_name[1]), new_img)imagefilefeed.pyimport osclass filefeed(): the filefeed class determines appropriate file paths to use for retrieving images and for saving 'positive' and 'negative' images while avoiding duplicating work in the event that user completes classification in multiple sessions. to avoid duplication the same positive_directory and negative_directory should always be used for one batch def __init__(self, existing_directory, positive_directory, negative_directory): saves file locations to instance variables and determines the appropriate files for editing, based on removing any that have already been edited self.existing_directory = existing_directory self.positive_directory = positive_directory self.negative_directory = negative_directory self.index = 0 # retrieves complete list of files to be edited list_of_files = [] file_names = [] walker = iter(os.walk(self.existing_directory)) next(walker) for dir, _, _ in walker: files = [dir + / + file for file in os.listdir(dir)] list_of_files.extend(files) file_names.extend(os.listdir(dir)) # determines which files have already been edited list_of_processed_files = [] processed_file_names = [] walker = iter(os.walk(self.positive_directory)) next(walker) for dir, _, _ in walker: files = [dir + / + file for file in os.listdir(dir)] list_of_processed_files.extend(files) processed_file_names.extend(os.listdir(dir)) # list of files to edit does not include those that have already been edited good_names = set(file_names) - set(processed_file_names) self.list_of_files = [f for i, f in enumerate(list_of_files) if file_names[i] in good_names] def next_file(self): self.index += 1 return self.list_of_files[self.index - 1] def get_negative_file(self): returns a tuple containing the absolute directory (0) and filename (1) where a negative file should be saved for the corresponding image/index. this preserves folder and file identity from the original directory, but now in the 'negative' directory. files = self.list_of_files[self.index].split(/) try: os.stat(self.negative_directory+files[-2]) except: os.mkdir(self.negative_directory+files[-2]) return ({}{}/.format(self.negative_directory, files[-2]), files[-1]) def get_positive_file(self): returns same items as get_negative_file except for the positive directory files = self.list_of_files[self.index].split(/) try: os.stat(self.positive_directory+files[-2]) except: os.mkdir(self.positive_directory+files[-2]) return ({}{}/.format(self.positive_directory, files[-2]), files[-1])",
    "present_kp": [
      "python",
      "tkinter",
      "opencv"
    ],
    "absent_kp": [
      "object oriented"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "find the appropriate transformer for an abstract value. i want to get rid of the following code duplication within the myfacadebean. consider the following situation:public class facadebean implements facade { @ejb private crudservice crudservice; @inject private firstassembler firstassembler; @inject private secondassembler secondassembler; @override public void save(firstvalue value) { firstentity entity = this.firstassembler.transformtoentity(value); this.crudservice.persist(entity); } @override public void save(secondvalue value) { secondentity entity = this.secondassembler.transformtoentity(value); this.crudservice.persist(entity); }}public interface myfacade { void save(firstvalue value); void save(secondvalue value);}with the crudservice:public interface crudservice { void persist(object entity);}@stateless@local(crudservice.class)@transactionattribute(transactionattributetype.mandatory)public class crudservicebean implements crudservice { public static final string persistence_unit_name = my_persistence_unit; private entitymanager entitymanager; @persistencecontext(unitname = persistence_unit_name) public void setentitymanager(entitymanager entitymanager) { this.entitymanager = entitymanager; } @override public void persist(object entity) { this.entitymanager.persist(entity); }}with the following assemblers:public class firstassembler extends abstractassembler<firstentity> { public firstentity transformtoentity(firstvalue value) { if (value == null) return null; firstentity entity = new firstentity(); transformabstractvaluetoabstractobject(value, entity); entity.setfixedrate(value.getfixedrate()); entity.setstartdate(value.getstartdate()); return entity; }}public class secondassembler extends abstractassembler<secondentity> { public secondentity transformtoentity(secondvalue value) { if (value == null) return null; secondentity entity = new secondentity(); transformabstractvaluetoabstractobject(value, entity); entity.settransactiontype(value.gettransactiontype()); entity.setvaluedate(value.getvaluedate()); return entity; }}public abstract class abstractassembler<t extends abstractentity> { protected void transformabstractvaluetoabstractobject(abstractvalue value, t object) { object.setuniqueid(value.getuniqueid()); object.setnominalamountvalue(value.getnominalamountvalue()); }}with the following entities:@entitypublic class firstentity extends abstractentity { private static final long serialversionuid = 1l; @id @column(name = id) private long id; @column(name = start_date) @temporal(temporaltype.date) private date startdate; @column(name = fixed_rate) @digits(integer = 1, fraction = 10) private bigdecimal fixedrate; public long getid() { return id; } public void setid(long id) { this.id = id; } public date getstartdate() { return startdate; } public void setstartdate(date startdate) { this.startdate = startdate; } public bigdecimal getfixedrate() { return fixedrate; } public void setfixedrate(bigdecimal fixedrate) { this.fixedrate = fixedrate; }}@entitypublic class secondentity extends abstractentity { private static final long serialversionuid = 1l; @id @column(name = id) private long id; @column(name = value_date) @temporal(temporaltype.date) private date valuedate; @column(name = transaction_type) @enumerated(enumtype.string) private transactiontype transactiontype; public long getid() { return id; } public void setid(long id) { this.id = id; } public date getvaluedate() { return valuedate; } public void setvaluedate(date valuedate) { this.valuedate = valuedate; } public transactiontype gettransactiontype() { return transactiontype; } public void settransactiontype(transactiontype transactiontype) { this.transactiontype = transactiontype; }}@mappedsuperclasspublic abstract class abstractentity implements serializable { private static final long serialversionuid = 1l; @column(name = transaction_nom_amount_value) @digits(integer = 18, fraction = 5) @min(0) private bigdecimal nominalamountvalue; public bigdecimal getnominalamountvalue() { return nominalamountvalue; } public void setnominalamountvalue(bigdecimal nominalamountvalue) { this.nominalamountvalue = nominalamountvalue; }}i tried the following approach:public class facadebean implements facade { @inject private assembler assembler; @inject private assemblerfactory assemblerfactory; @override public <t extends abstractvalue> void save(t value) { assembler assembler = assemblerfactory.createassembler(value); abstractentity entity = assembler.transformtoentity(value); this.crudservice.persist(entity); }}problems are the assemblerfactoryimpl and the assemblerimpl in which i have to do instanceof checks and castings...another idea would be to let the value know which transformer to use (or how to transform). but i want the value to be dumb.",
    "present_kp": [],
    "absent_kp": [
      "java",
      "design patterns",
      "polymorphism"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "disable mounting to '$home/.gvfs'. i am using debian wheezy with xfce and thunar. thunar (and other similer managers) enables e.g. connecting to ssh server using sftp address like sftp://mysvr/ and browsing it like a local folder. i normally access my remote box via command line, but occasionally i appreciate this gui sugar.however, in default configuration, this has a pretty annoying downside: file systems accessed this way are automatically mounted under folder $home/.gvfs, which creates quite a nightmare for tasks that involve traversing through home folder (typically dotfile searching, or analyzing disk space usage issues).gnome 2.2 rns say:gvfs also offers a fuse mountpoint in ~/.gvfs/ so that gvfs mounts can be exposed to legacy applications using standard posix io.so i guess it's possible to turn this off (i assume by legacy they don't mean nautilus and the likes).however, i can't seem to find any information on how to do it. any pointers?",
    "present_kp": [
      "debian",
      "gvfs"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how do you deal with freebsd's periodic reports?. out of the box, every freebsd machine sends two e-mails per day to the administrator (root -- usually aliased to someone):daily cron output (with status of disks, network interfaces, etc.)daily security output (with penetration-attempts found in logs, etc.)there are also weekly and monthly report-pairs, with outputs of the jobs deemed heavier (like locate-updates), which do not require daily runs.most of the time these e-mails are tedious to the extreme and i often find myself deleting them without reading. but i feel guilty doing it -- because one day i'm liable to miss something important and because, if i'm not reading it, i may as well redirect them into /dev/ to begin with.with 5-7 freebsd boxes (of my own and immediate family) under my care, this is getting bothersome -- is there, perhaps, some sort of software, that can alert me to potentially interesting reports while quietly stashing the rest away? something trainable -- like a bayesian spam-filter?ideally, it would integrate with seamonkey/thunderbird, but can also be command-line based (i'll run it from inside ~/.procmailrc)...",
    "present_kp": [
      "freebsd",
      "filter"
    ],
    "absent_kp": [
      "email"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "alternative to using sleep() to avoid a race condition in pyqt. i have a situation where i would like to use a single qthread to run two (or more) separate methods at different times. for example, i would like the qthreadto run play() sometimes, and when i am done playing, i want to disconnect the qthread.started() signal from this method so that i may connect it somewhere else. in essence i would like the qthread to act as a container for anything i would like to run in parallel with the main process.i have run into the problem where starting the qthread and then immediately disconnecting the started() signal it causes strange behavior at runtime. before i discovered what 'race condition' meant (or really understanding much about multithreading), i had the sneaking suspicion that the thread wasn't fully started before being disconnected. to overcome this, i added a 5 ms sleep in between the start() and disconnect() calls and it works like a charm. it works like a charm but it isn't the right way. how can i implement this functionality with one qthread without making the call to sleep()?def play(self): self.statelabel.settext(status: playback initated ...) self.mythread.started.connect(self.mouserecorder.play) self.mythread.start() time.sleep(.005) #this is the line i'd like to eliminate self.mythread.started.disconnect()def record(self): self.statelabel.settext(status: recording ...) self.mythread.started.connect(self.mouserecorder.record) self.mythread.start() time.sleep(.005) #this is the line i'd like to eliminate self.mythread.started.disconnect()",
    "present_kp": [
      "multithreading",
      "pyqt"
    ],
    "absent_kp": [
      "python"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "three cards trick - assigning random number to the cards. at first i have three cards like this: ___ ___ ___| | | | | | | a | | a | | q ||___| |___| |___|and then i shuffle them to this: ___ ___ ___| | | | | || 1 | | 2 | | 3 ||___| |___| |___|so now i want to write a function that generates a random number between 1 and 3 and assign it to be the value of the q card. for example if i generate number 2, the cards when turned around will have this order: ___ ___ ___| | | | | || a | | q | | a ||___| |___| |___|so i came up with a code like thisdef createcard(card): print (' _____') print ('| |') print ('| ',card,' |',sep='') print ('|_____|')def shuffle(): q = random.randint(1,4) if q == 1: cards = ['q','a','a'] elif q == 2: cards = ['a','q','a'] else: cards = ['a','a','q'] return cardsfor card in shuffle(): createcard(card)i guess what i'm asking is this the optimal way since i don't think i'm allowed to use any other function of random module besides randint or randrange.",
    "present_kp": [
      "shuffle"
    ],
    "absent_kp": [
      "python"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "separate all leaves of a weighted tree with minimum weight cuts. this is part of a larger problem, which i believe i have reduced to this. given a tree $t$ having positive edge weights, and $k$ leaves (nodes which have exactly one connected node), i need to delete some edges in the tree so that no two leaves in the original tree are connected (by a path) in the newly formed forest (of trees). the total sum of the weights of the deleted edges needs to be minimized.my understanding is that atleast $k-1$ edges need to be deleted to separate out all the $k$ leaves. any more deletions will unnecessarily increase the total cost. thus, we need to perform exactly $k-1$ deletions.my hypothesis:for every pair of leaf nodes $l_i$ and $l_j$, find the edge with the minimum weight in the (unique) path from $l_i$ to $l_j$. the $k-1$ least weight edges from this set of edges need to be deleted. this will minimize the sum of weights of the edges to be deleted in order to disconnect all leaves from each other.i am unable to prove or disprove this hypothesis. can someone please prove the correctness of this hypothesis, or give a counter-example along with the correct algorithm to solve this problem? if this is indeed correct, is there a faster way (asymptotic complexity wise) to solve this problem? this approach will take $\\theta({k^2})$ time. thanks in advance!",
    "present_kp": [
      "trees"
    ],
    "absent_kp": [
      "algorithms",
      "graph theory"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "crud code for a finance webform. i have a finance web-application in asp.net webforms using telerik rad controls. two event handlers wound up being very similar in implementation.what steps could i take to improve the organization of and/or the efficiency or dry-ness of the code? what other issues / code-smells are present?the codeupdate event protected void gv_financepositionlist_updatecommand(object sender, gridcommandeventargs e) { var item = (grideditformitem) e.item; int financelistid = int.parse(item.getdatakeyvalue(keyname: financelistid).tostring()); int companyid = int.parse(((radcombobox) item.findcontrol(id: ddl_company)).selectedvalue); int year = ((radmonthyearpicker) item.findcontrol(id: pick_year)).selecteddate.value.year; int periodtypeid = int.parse(((radiobuttonlist) item.findcontrol(id: rbtn_periodtypename)).selectedvalue); int partition = 1; if (((raddropdownlist) item.findcontrol(id: ddl_periodtype)).visible) { partition = int.parse(((raddropdownlist) item.findcontrol(id: ddl_periodtype)).selectedvalue); } string cashandequivalents = ((radnumerictextbox) item.findcontrol(id: txt_cashandequivalents)).text; decimal? cashandequivalentsval = null; if (!string.isnullorempty(cashandequivalents)) { cashandequivalentsval = decimal.parse(cashandequivalents); } string shortterminvestments = ((radnumerictextbox) item.findcontrol(id: txt_shortterminvestments)).text; decimal? shortterminvestmentsval = null; if (!string.isnullorempty(shortterminvestments)) { shortterminvestmentsval = decimal.parse(shortterminvestments); } using (var ctx = new model()) { financepositionlist financepositionlist = ctx.financepositionlists.find(financelistid); financepositionlist.companyid = companyid; financepositionlist.cashandequivalents = cashandequivalentsval;financepositionlist.shortterminvestments = shortterminvestmentsval; ctx.savechanges(); }}insert eventprotected void gv_financepositionlist_insertcommand(object sender, gridcommandeventargs e) { var item = (grideditformitem) e.item; int companyid = int.parse(((radcombobox) item.findcontrol(id: ddl_company)).selectedvalue); int year = ((radmonthyearpicker) item.findcontrol(id: pick_year)).selecteddate.value.year; int periodtypeid = int.parse(((radiobuttonlist) item.findcontrol(id: rbtn_periodtypename)).selectedvalue); int partition = 1; if (((raddropdownlist) item.findcontrol(id: ddl_periodtype)).visible) { partition = int.parse(((raddropdownlist) item.findcontrol(id: ddl_periodtype)).selectedvalue); } string cashandequivalents = ((radnumerictextbox) item.findcontrol(id: txt_cashandequivalents)).text; decimal? cashandequivalentsval = null; if (!string.isnullorempty(cashandequivalents)) { cashandequivalentsval = decimal.parse(cashandequivalents); } string shortterminvestments = ((radnumerictextbox) item.findcontrol(id: txt_shortterminvestments)).text; decimal? shortterminvestmentsval = null; if (!string.isnullorempty(shortterminvestments)) { shortterminvestmentsval = decimal.parse(shortterminvestments); var financepositionlist = new financepositionlist { companyid = companyid, cashandequivalents = cashandequivalentsval, shortterminvestments = shortterminvestmentsval } using (var ctx = new model()) { ctx.financepositionlists.add(financepositionlist); ctx.savechanges(); } }",
    "present_kp": [
      "asp.net",
      "finance"
    ],
    "absent_kp": [
      "c#",
      "database",
      "event handling"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "proper way to overwrite debian kernel modules. i just downloaded and compiled tg3.ko kernel module. where should i put it on a debian system? there is one in /lib/modules/2.6.32-5-xen-amd64/kernel/drivers/net/tg3.ko already.ideally, i would like to leave the original one where it is, and bump the priority for mine. so if mine doesn't get loaded or disappears, the original is still there as a fallback.the only way i know to do it is dpkg-divert, but i feel a slight shiver in my stomach when i use it. it is especially scary to do it on a server, with the network module. :)",
    "present_kp": [
      "debian",
      "kernel modules"
    ],
    "absent_kp": [
      "package management"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "decidability of equality of radical expressions. consider terms built from elements of $\\mathbb q$ and the operations $+, imes,-,/$, and $\\sqrt[n]{\\,\\cdot\\,}$ for each natural number $n$. given the promise that two terms are well-formed -- that is, there is no division by zero, and no even roots of negative numbers -- is there an algorithm which decides when the two terms are equal?a related question was posted here, but it is more general (as it allows arbitrary exponentiation, rather than just by rational numbers).",
    "present_kp": [
      "equality"
    ],
    "absent_kp": [
      "computability",
      "undecidability",
      "number theory"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "can someone find my gravatar profile if they have my email address, or gravatar url associated with that email address?. gravatar now offers profiles that are publicly visible. let's say i have entered some info in my profile. let's also say i have two email addresses, email1 and email2, registered with gravatar but with different avatar images. is there any way someone (other than gravatar/wordpress itself) who knows my email1 address or the corresponding gravatar url find my public profile, email2 address, or the corresponding gravatar url, or otherwise associate email1 with any of those things?at first glance, it seems like this would not be possible without access to my gravatar account. (this is, of course, assuming that i've not done anything obvious like put that email address on my public profile, or the email is [mygravatarusername]@[somedomain].com or something like that.) however, i wanted to get some other opinions in case i've overlooked something.",
    "present_kp": [
      "gravatar"
    ],
    "absent_kp": [
      "privacy",
      "anonymous"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "problem installing jdt in eclipse on linux. i installed eclipse on fedora core 14 using yum install eclipse-platform. i ran eclipse and tried to install the java development tools, but then this error came up:cannot complete the install because of a conflicting dependency. software being installed: eclipse java development tools 3.6.2.r362_v20101117-0800-7z8xfw6flflmjjcvz03jyefbls_f (org.eclipse.jdt.feature.group 3.6.2.r362_v20101117-0800-7z8xfw6flflmjjcvz03jyefbls_f) software currently installed: eclipse platform 3.6.1 (eclipse platform 3.6.1) only one of the following can be installed at once: eclipse ide ui 3.6.1.m20100825-0800 (org.eclipse.ui.ide 3.6.1.m20100825-0800) eclipse ide ui 3.6.0.i20100601-0800 (org.eclipse.ui.ide 3.6.0.i20100601-0800) eclipse ide ui 3.6.2.m20101201-0800 (org.eclipse.ui.ide 3.6.2.m20101201-0800) cannot satisfy dependency: from: eclipse platform 3.6.1 (eclipse platform 3.6.1) to: org.eclipse.ui.ide [3.6.1.m20100825-0800] cannot satisfy dependency: from: eclipse java development tools 3.6.2.r362_v20101117-0800-7z8xfw6flflmjjcvz03jyefbls_f (org.eclipse.jdt.feature.group 3.6.2.r362_v20101117-0800-7z8xfw6flflmjjcvz03jyefbls_f) to: org.eclipse.platform.feature.group 3.6.2 cannot satisfy dependency: from: eclipse platform 3.6.2.r362_v20110210-9gf78gs1frignhdhwkecopon8amxezflgdgkqi (org.eclipse.platform.feature.group 3.6.2.r362_v20110210-9gf78gs1frignhdhwkecopon8amxezflgdgkqi) to: org.eclipse.ui.ide [3.6.2.m20101201-0800]what should i do?",
    "present_kp": [
      "linux",
      "fedora",
      "yum",
      "java",
      "eclipse"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "enum constants for convert units. i have a few constants in a game that i'm doing for hobby. i need to store constants for the total size of the a physical world and the size of the screen. i can position the world's objects to the screen or vice versa.to position the images on the screen for example i get the position that the body occupies in the physical world and transform to the screen by doing:(full screen size / total size of the world) * position of the object in the world.i came up with an ingenious way to keep these constant while i do conversion operations using the enum below:import com.badlogic.gdx.math.vector2;import static br.com.games.test.sizes.*;public class test { public static enum sizes { world(new vector2(25.806f, 15.48f)), screen(new vector2(800f, 480f)), transform( new vector2(-1, -1)); private vector2 value; private sizes(vector2 value) { this.value = value; } private vector2 inputvalue; private vector2 sourcevalue; public sizes value(vector2 value) { inputvalue = value; return this; } public sizes from(sizes size) { sourcevalue = size.value; return this; } public vector2 to(sizes size) { float x = size.value.x / sourcevalue.x * inputvalue.x; float y = size.value.y / sourcevalue.y * inputvalue.y; return vector2.zero.set(x, y); } public vector2 getvalue() { return value; } }}i can use it this way: public static void main(string[] args) { transform.value(new vector2(10, 10)).from(world) .to(screen); }vector2 class is here.what do you think of this way of using constants and operations? do you see any problems? could you point out some improvement?",
    "present_kp": [
      "enum"
    ],
    "absent_kp": [
      "java",
      "object oriented",
      "coordinate system"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "song database query. i am relatively inexperienced with mysql and have a query, which to my eyes appears relatively complex: select sql_calc_found_rows songsid, song_name, artist_band_name, author, song_artwork, song_file, genre, song_description, uploaded_time, emotion, tempo, user, happiness, instruments, similar_artists, play_count, projects_count, rating, ratings_count, waveform, datasize, display_name, user_url, genre_id, if(user_ratings_count, 'user voted', 'not voted') as voted from ( select sp.songsid, projects_count, avg(rating) as rating, count(rating) as ratings_count, count(if(userid=$userid, 1, null)) as user_ratings_count from ( select songsid, count(*) as projects_count from $stable s left join $stable2 p on s.songsid = p.songs_id group by songsid) as sp left join $stable3 r on sp.songsid = r.songid group by sp.songsid) as sprjoin $stable s using (songsid)left join $stable5 q on s.user = q.id left join ( select g.song_id, group_concat(g.genre_id separator ',') as genre_id from $stable6 g join $stable h on h.songsid = g.song_id group by h.songsid) as gs on s.songsid = gs.song_id essentially, this query collects data from several different tables about a list of songs:the song table itself is $stable with the other tables containing various related information such as ratings, projects, uploaded user information etc.the final part of the query collects a comma-separated list of genre_ids from $stable6.the where clause is dynamically generated depending on what the user is filtering upon.i am specifically concerned about the fact that i am currently dynamically generating the where clause when a user wants to search by genre_id, by looping through a string of comma-separated genre_ids and building the where clause like so:where genre_id like '%6%' or genre_id like '%3%' or genre_id like '%8%'this strikes me as inefficient but given the dynamic nature of this specific app i have been unable to come up with a different solution that compares directly using '='.therefore, this is actually two questions in one:is there any way to improve the performance of this query overall (any comments on on index schemes or a way of simplifying the query itself are most welcome)?is there a better way of performing the dynamic where clause so the database doesn't have to use like searching through comma separated strings?the table in question, $stable6, is simply a link table for a many-to-many relationship:$stable (songs) $stable6 (genres_link) $stable7 (genres)songsid* genre_id** genre_id**column song_id* genre_namecolumn icon_url etc....columncolumn$stable 7 is not used in the above query at all and does not need to be.i get the following returned via mysql explain when using a simple 2 genre_id where clause (with only 9 rows of songs in total, i will be testing large datasets soon).id select_type table type possible_keys key key_len ref rows extra1 primary <derived4> all null null null null 2 using where1 primary <derived2> all null null null null 9 using where; using join buffer1 primary s eq_ref primary primary 4 gs.song_id 1 1 primary q eq_ref primary primary 8 songbanc_cms.s.user 1 4 derived g index primary primary 8 null 6 using index; using temporary; using filesort4 derived h eq_ref primary primary 4 songbanc_cms.g.song_id 1 using index2 derived <derived3> all null null null null 9 using temporary; using filesort2 derived r ref primary primary 4 sp.songsid 2 using index3 derived s index null primary 4 null 2 using index3 derived p ref songs_id songs_id 4 songbanc_cms.s.songsid 4 using index",
    "present_kp": [
      "performance",
      "sql",
      "mysql"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "online examination portal for learning. i am new to asp.net and i just developed a simple online examination portal for learning. i used ado.net, mysql and developed in vs 2010.i have a login page in which the user can login and register for a new user. after successful login, the user is redirected to the question page and i fetch the first question from the database. i populated the question in the label and options in the radio button list. the user can select one option and click the next button. in the click event of the next button i calculate the marks. i store all values in the session only. when the user click next of last question that is 4 user is redirected to the results page and prints the marks. public partial class questions : system.web.ui.page{ protected void page_load(object sender, eventargs e) { response.cache.setcacheability(httpcacheability.nocache); response.cache.setexpires(datetime.now.addseconds(-1)); response.cache.setnostore(); if (!ispostback) { renderquestions(1); session[buttonindex] = 1; session[marks] = 0; } } public void renderquestions(int index) { mysqlconnection con = null; string constring = configurationmanager.connectionstrings[constring].connectionstring; string qry = select * from questions where question_id=+index+; try { using (con = new mysqlconnection(constring)) { con.open(); using (mysqlcommand cmd = new mysqlcommand(qry, con)) { using (mysqldataadapter ada = new mysqldataadapter(cmd)) { datatable dt = new datatable(); ada.fill(dt); if (dt.rows.count > 0) { clsquestion ques = new clsquestion(); ques.questionid = convert.toint32(dt.rows[0][0]); ques.question = convert.tostring(dt.rows[0][1]); ques.option1 = convert.tostring(dt.rows[0][2]); ques.option2 = convert.tostring(dt.rows[0][3]); ques.option3 = convert.tostring(dt.rows[0][4]); ques.option4 = convert.tostring(dt.rows[0][5]); ques.answer = convert.toint32(dt.rows[0][6]); renderquesandanswers(ques); } } } } } catch (exception ex) { throw ex; } finally { con.close(); } } public void renderquesandanswers(clsquestion quest) { lblquestion.text = quest.question; radiobuttonlist1.items.clear(); radiobuttonlist1.items.add(quest.option1); radiobuttonlist1.items.add(quest.option2); radiobuttonlist1.items.add(quest.option3); radiobuttonlist1.items.add(quest.option4); session[questionnumber] = quest.questionid ; session[answer] = quest.answer; } public class clsquestion { private int questionid; private string question; private string option1; private string option2; private string option3; private string option4; private int answer; public int questionid { get { return questionid; } set { questionid = value; } } public string question { get { return question; } set { question = value; } } public string option1 { get { return option1; } set { option1 = value; } } public string option2 { get { return option2; } set { option2 = value; } } public string option3 { get { return option3; } set { option3 = value; } } public string option4 { get { return option4; } set { option4 = value; } } public int answer { get { return answer; } set { answer = value; } } } protected void option1_checkedchanged(object sender, eventargs e) { if (convert.toint32 (session[answer]) == 1) { int marks=convert.toint32 (session[marks]); marks++; session[marks] = marks; } } protected void option2_checkedchanged(object sender, eventargs e) { if (convert.toint32(session[answer]) == 2) { int marks = convert.toint32(session[marks]); marks++; session[marks] = marks; } } protected void option3_checkedchanged(object sender, eventargs e) { if (convert.toint32(session[answer]) == 3) { int marks = convert.toint32(session[marks]); marks++; session[marks] = marks; } } protected void option4_checkedchanged(object sender, eventargs e) { if (convert.toint32(session[answer]) == 4) { } } protected void btnnext_click(object sender, eventargs e) { } protected void btnnext_click1(object sender, eventargs e) { int buton = convert.toint32(session[buttonindex]); if (buton < 5) { if (radiobuttonlist1.selectedindex + 1 == convert.toint32(session[answer])) { int marks = convert.toint32(session[marks]); marks++; session[marks] = marks; } session[buttonindex] = convert.toint32(session[buttonindex]) + 1; renderquestions(convert.toint32(session[buttonindex])); if (buton == 4) { server.transfer(results.aspx); session.removeall(); } } }}}html<form id=form1 runat=server><div><h3>please choose the right answer</h3></div><table class=style1> <tr> <td class=style3> <asp:panel id=panel1 runat=server> <asp:label id=lblquestion runat=server text=></asp:label> </asp:panel> &nbsp;</td> <td class=style4> </td> </tr> <tr> <td class=style2> answers:</td> <td> &nbsp;</td> </tr> <tr> <td class=style2> <asp:panel id=panel2 runat=server> <asp:radiobuttonlist id=radiobuttonlist1 runat=server> </asp:radiobuttonlist> <asp:radiobutton id=option1 runat=server checked=false autopostback=true groupname=option oncheckedchanged=option1_checkedchanged /> <asp:radiobutton id=option2 runat=server checked=false autopostback=true groupname=option oncheckedchanged=option2_checkedchanged /> <asp:radiobutton id=option3 runat=server checked=false autopostback=true groupname=option oncheckedchanged=option3_checkedchanged /> <asp:radiobutton id=option4 runat=server checked=false autopostback=true groupname=option oncheckedchanged=option4_checkedchanged /> </asp:panel> </td> <td> &nbsp;</td> </tr> <tr> <td class=style2> &nbsp;</td> <td> &nbsp;</td> </tr> <tr> <td class=style2> <asp:button id=btnnext runat=server onclick=btnnext_click1 text=next /> </td> <td> &nbsp;</td> </tr></table></form>i got the result perfectly and no issues yet. but i want to make sure i am doing it the best way, if my code meets the standards, and if there are any security issues.",
    "present_kp": [
      "mysql",
      "asp.net"
    ],
    "absent_kp": [
      "c#",
      "beginner"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "put a section of an article into a category on wikipedia. i'd like to place sections of articles into categories on wikipedia. (for example, placing pollution in india#water pollution into the category water pollution in india) is there any way to do this besides creating a redirect page for the article and then putting the redirect page in the category?",
    "present_kp": [
      "wikipedia"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to allow gmail to receive zip files. i want to send a zipped file to someone with a gmail account, but it doesn't arrive. i get a delivery status notification (failure).so how can you configure gmail to receive zip files?ps the file doesn't contain a virus or other malware.",
    "present_kp": [
      "gmail"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "awk a value from irregular output by column name. i'm writing a bash script that uses helm history (kubernetes tool) command to get the last revision number of my release. the output for the command is like this:helm history release revision updated status chart description 1 mon feb 27 12:46:10 2017 superseded chart-solution-0.1.0-beta1 install complete 2 fri mar 3 11:40:55 2017 superseded chart-solution-0.1.0-beta1 upgrade complete 3 fri mar 3 11:41:02 2017 deployed chart-solution-0.1.0-beta1 upgrade completethe expected result is to return only the number 3 (last value) from the revision column. i can do that for now using:helm history release | awk -v col=revision 'nr==1{for(i=1;i<=nf;i++){if($i==col){c=i;break}} print $c} nr>1{print $c}' | tail -n 13however, the column updated is irregular (space separated items), and the above command only works because the revision column comes before updated. if in future versions the helm history command change the order of columns, things can get messed.a simple example is if i a try to get the last value using the column status, which comes after updated:helm history release | awk -v col=status 'nr==1{for(i=1;i<=nf;i++){if($i==col){c=i;break}} print $c} nr>1{print $c}' | tail -n 1marthere is a way to get the correct last revision value even if the column order changes?",
    "present_kp": [
      "awk",
      "helm"
    ],
    "absent_kp": [
      "text processing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how does the galil's rule work on boyer-moore algorithm?. i would like to know how boyer-moore text searching algorithm with galil's rule works,. i tried to search for but i couldn't understand the information i found, for example this wikipedia page.and why with this rule we go to a linear time complexity?",
    "present_kp": [],
    "absent_kp": [
      "algorithms",
      "algorithm analysis",
      "search algorithms",
      "strings",
      "exact string matching"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "sed to delete between delimiters but keep first delimter. i need to delete everything between the second = in a string and the first / in a string, but keep the = in place. i've tried many, many things, the most recent of which is sed -i 's/=[^/]*//",
    "present_kp": [
      "sed"
    ],
    "absent_kp": [
      "shell script",
      "text processing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "dispatcher for calling laravel rest api. my api.js contains following, i am using laravel 5 , i need to handle csrf tokenmismatchexception as well for which i am using exception/handler.phpam i doing this right? i am replacing $.ajax in each of my js files to api.callhow would you handle/create something like following? actually code is working, but what about existing error callbacks i am having in my other js files, because i am replacing it with error specified in api.call method (look at api.call method below). is there any existing library available for this kind of thing? function api() {}api.defaultoptions = { url: '', type: '', data: {}, headers: { 'x-csrf-token': $('meta[name=csrf-token]').attr('content') }, datatype: 'json', success: function (res) { }, error: function (res) { }}api.get = function (setup) { setup.type = 'get'; api.call(setup);}api.post = function (setup) { setup.type = 'post'; api.call(setup);}api.put = function (setup) { setup.type = 'put'; api.call(setup);}api.patch = function (setup) { setup.type = 'patch'; api.call(setup);}api.delete = function (setup) { setup.type = 'delete'; api.call(setup);}api.call = function (setup) { var request = $.extend(api.defaultoptions, setup); request.error = function (res) { if (res.status == 400 && res.responsejson.msg) { toastr.showerror(res.responsejson.msg); } } $.ajax(request);}",
    "present_kp": [
      "ajax",
      "rest"
    ],
    "absent_kp": [
      "javascript",
      "jquery"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why are most mutex implementations unfair?. my understanding is that most popular implementations of a mutex (e.g. std::mutex in c++) do not guarantee fairness -- that is, they do not guarantee that in instances of contention, the lock will be acquired by threads in the order that they called lock(). in fact, it is even possible (although hopefully uncommon) that in cases of high contention, some of the threads waiting to acquire the mutex might never acquire it.this seems like an unhelpful behavior to me -- it seems to me that a fair mutex would yield behavior more in line with what a programmer would want/expect.the reason given for why mutexes are typically not implemented to be fair is performance, but i'd like to understand better what that means -- in particular, how does relaxing the mutex's fairness requirement improve performance? it seems like a fair mutex would be trivial to implement -- just have lock() append the calling thread to the tail of the mutex's linked list before putting the thread to sleep, and then have unlock() pop the next thread from the head of that same list and wake it up.what mutex-implementation insight am i missing here, that would explain why it was considered worthwhile to sacrifice fairness for better performance?",
    "present_kp": [
      "threads"
    ],
    "absent_kp": [
      "concurrency",
      "synchronization"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "find files that end with number. im trying to make some backup script as the log files get bigger and bigger. what i have is coping the current file, (for example secure file in /var/log/) and remove the content from that file. but there are some files with the name like: secure.1, secure.2 and all this i like to count them, and if the number is bigger then 2 to archive them all. i can't find the method to find this files or count them. the first think that come up to me was:find /var/log/ -name *.1 | wc -land this will always print 1 as there is one file secure.1. how can i count like in for loop where i can specified a range of numbers like {1..5} or similar. is there a way to separate this files and make them as one and them backup or delete or what ever ... or first of all how can i find all this numbers that ends up with number.",
    "present_kp": [
      "find"
    ],
    "absent_kp": [
      "filenames",
      "wildcards",
      "patterns"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "unhide comment on my post will not work. i have made posts where some of my friend's comments are hidden, and i don't know why they've been hidden. when i go to my post and unhide the hidden comment, the action does not stick. when i go back to the post again, the comment has been hidden again. i have closed/reopened my browser, and the comments are still hidden, even though i have gone in and clicked unhide. how can i permanently unhide these comments from my posts??",
    "present_kp": [],
    "absent_kp": [
      "facebook comments"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what linux app will show me how much internet bandwidth each process is using?. linuxmint 13 cinnamon 32 bit with ver. 16 backportsethernet (eth0) + pppoe (ppp0) + ftth (fiber optic to the home)no wifiwhat program(s) can i use to tell me how much up/down internet traffic each running process is using? pid, process name and user name displayed with rx (download data rate) and tx (upload data rate) with sort options would be great.tia.",
    "present_kp": [
      "linux",
      "ethernet"
    ],
    "absent_kp": [
      "networking"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how do i query for unknown in google voice?. google voice lists all callers without phone numbers -- those that *67 as unknown. how do i query for all calls from unknown and delete them? is this possible?",
    "present_kp": [
      "google voice"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "business logic: client-side vs. server side. let's say 3-5 years ago (more or less) n-tier application on the server side - and some javascript/html/css for the ui was a basic approach for web development.nowadays we can see that traditional web development paradigm changes a lot. each day i saw more and more application who do not have server side in traditional way. they just consume some services (data-service, auth-service, etc.) but the business logic placed on client side. also already a lot of javascript frameworks creates for simplify development according such model (angular, backbone, etc.)what are the main benefits and disadvantages of new model versus traditional approach?",
    "present_kp": [
      "web development"
    ],
    "absent_kp": [
      "design",
      "architecture"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "retrieve and stores multiple data to database. i am currently working on a project that involves me creating a worksheet for storing student test scores and grades. i wrote a script that retrieves users information and presents it in form of a worksheet. if changes are made to this document it saved the program stores or updates the database.i need reviews and optimization since i am certain my coding habits are crude and needs some polishing thanks<?php//getrecord.php scriptsession_start();$_session['data']['tid'] = $term = $_post['term'];$_session['data']['sid'] = $subject = $_post['subject'];$_session['data']['sess'] = $session = $_post['session'];$_session['data']['class'] = $class = $_post['class'];#get assessment records for that class and session$q = sprintf(select grades.grade_id, concat(fname,' ', lname) as name, test1, test2, test3, exam from grades, pupils where pupils.pupil_id = grades.pupil_id and class_id = %d and session = '%s' and term_id = %d and subject_id = %d, $class, $session, $term, $subject); $r = $dbc->query($q);?> <form method=post action= > <table class=worksheet> <tr> <th width=2% class=heading></th> <th width=38%>student full name</th> <th width=15%>test 1 score</th> <th width=15%>test 2 score</th> <th width=15%>test 3 score</th> <th width=15%>exam score</th> </tr><?php if ($r->num_rows > 0){ $i = 1; #if it does exist #get the student info for that paarticular subject while (list($gradeid, $stdname, $t1, $t2, $t3, $e) = $r->fetch_array()) { ?> <tr> <td class=heading><?php echo $i++; ?></td> <td><?php echo $stdname; ?></td> <td><input type=text name=test1[<?php echo $gradeid; ?>] value=<?php if(isset($t1)){ echo $t1; }else{ echo 0; } ?>/></td> <td><input type=text name=test2[<?php echo $gradeid; ?>] value=<?php if(isset($t2)){ echo $t2; }else{ echo 0; } ?> /></td> <td><input type=text name=test3[<?php echo $gradeid; ?>] value=<?php if(isset($t3)){ echo $t3; }else{ echo 0; } ?> /></td> <td><input type=text name=exam[<?php echo $gradeid; ?>] value=<?php if(isset($e)){ echo $e; }else{ echo 0; } ?> /></td> </tr> <tr> <td colspan=6><input type=hidden name=action value=update /></td></tr> <?php } }else{ //get assessment records for that class and session $q = sprintf(select students.pupil_id, concat(fname,' ', lname) as name from students, pupils where pupils.pupil_id= students.pupil_id and class_id = %d and session = '%s', $class ,$session); $r = $dbc->query($q); if ($r->num_rows >= 1 ) { $i=1; //if it does exist //get the student info for that paarticular subject while (list($stdid, $stdname) = $r->fetch_array()){ ?> <tr> <td class=heading><?php echo $i++; ?></td> <td><?php echo $stdname; ?></td> <td><input type=text name=test1[<?php echo $stdid; ?>] value=0/></td> <td><input type=text name=test2[<?php echo $stdid; ?>] value=0 /></td> <td><input type=text name=test3[<?php echo $stdid; ?>] value=0 /></td> <td><input type=text name=exam[<?php echo $stdid; ?>] value=0 /></td> </tr> <?php } echo ' <tr> <td colspan=6><input type=hidden name=action value=addnew /></td> </tr>'; } } ?> </table> <input type=submit value=submit record name=btnsubmit class=btn-center/> </form>the setrecord script<?phprequire ('../includes/connect.php');if (isset($_post['btnget'])){ require('sub-modules/getrecord.php');}if (isset($_post['btnsubmit'])){//i am using sample data for now but normally //the data are received from session$term = $_session['data']['tid'];$subject = $_session['data']['sid'];$session = $_session['data']['sess'];$cid = $_session['data']['class'];$test1 = $_post['test1'];$test2 = $_post['test2'];$test3 = $_post['test3'];$exam = $_post['exam'];$query = '';$insert_data = array();$success = $failure = 0; $total_insertions = 0;foreach ($test1 as $k => $v){ $test1 = $test1[$k]; $test2= $test2[$k]; $test3=$test3[$k]; $exam = $exam[$k]; if (isset($_post['action']) && $_post['action'] == 'addnew') { $query = insert into 'grades'('pupil_id','subject_id','class_id' ,'term_id', 'test1', 'test2', 'test3', 'exam', 'session') values ($k,$subject,$cid,$term,$test1,$test2,$test3,$exam, '$session'); }else{ $query = update 'grades' set 'test1' = $test1, 'test2' = $test2, 'test3' = $test3, 'exam' = $exam where grade_id = $k; }//i did this for debugging purpose so ican be able to see if the insertions //were complete and also see those ids that were not inserted $result= $dbc->query($query) or trigger_error($dbc->error); if ($dbc->affected_rows > 0){ $success += 1; }else{ $failure += 1; $insert_data[] = $k; } $total_insertions += 1;}if ($success == $total_insertions){ echo 'insertion was complete with no errors';}elseif ($failure == $total_insertions){ echo 'insertion was complete with errors';}else{ echo '<p>some insertion complete with some errors</p>'; echo <p>ids:; foreach ($insert_data as $n){ echo $n.', '; } echo '<br>'.$dbc->error;}}if (isset($_post['action'])){ $action = $_post['action']; if ($action == 'update'){ } if ($action == 'addnew'){ }}?><form method=post action=> <div> <label>class: </label> <select name=class id=class> <option value=3>primary 1</option> <option value=4>primary 2</option> <option value=5>primary 3</option> <option value=8>primary 4</option> <option value=9>primary 5</option> <option value=10>primary 6</option> </select> </div> <div> <label>subject: </label> <select name=subject id=subject> <option value=1>english language</option> <option value=2>mathematics</option> <option value=3>religious knowledge</option> <option value=4>civic education</option> </select></div><div> <label>session: </label> <select name=session> <option value=2015/2016>2015/2016</option> </select> </div> <div> <label>term:</label> <select name=term> <option value=1>first term</option> <option value=2>second term</option> <option value=3>third term</option> </select></div><input type=submit name=btnget value=get record/></form>database create table if not exists 'grades' ('grade_id' int(11) not null auto_increment,'pupil_id' int(11) not null,'subject_id' int(11) not null,'class_id' int(11) not null,'term_id' int(11) not null,'test1' int(11) not null,'test2' int(11) not null,'test3' int(11) not null,'exam' int(11) not null,'subposition' tinyint(3) default null,'session' char(9) not null,'finalize' tinyint(1) not null default '0',primary key ('grade_id'),unique key 'score_id_unique' ('grade_id'),key 'fk_score_terms1_idx' ('term_id'),key 'fk_score_subjects1_idx' ('subject_id'),key 'fk_grades_pupils1_idx' ('pupil_id'),key 'fk_grades_classes1_idx' ('class_id')) engine=innodb default charset=utf8 auto_increment=47 ;---- dumping data for table 'grades'--insert into 'grades' ('grade_id', 'pupil_id', 'subject_id', 'class_id', 'term_id', 'test1', 'test2', 'test3', 'exam', 'subposition', 'session', 'finalize') values(27, 7, 1, 3, 1, 10, 10, 0, 70, null, '2015/2016', 1),(28, 3, 1, 3, 1, 0, 0, 0, 0, null, '2015/2016', 0),(29, 8, 1, 3, 1, 0, 0, 0, 0, null, '2015/2016', 0),(30, 9, 1, 3, 1, 0, 0, 0, 0, null, '2015/2016', 0),(31, 10, 1, 3, 1, 0, 5, 0, 0, null, '2015/2016', 0),(32, 7, 2, 3, 1, 5, 7, 10, 0, null, '2015/2016', 1),(33, 3, 2, 3, 1, 6, 9, 10, 0, null, '2015/2016', 1),(34, 8, 2, 3, 1, 5, 7, 5, 0, null, '2015/2016', 1),(35, 9, 2, 3, 1, 7, 7, 7, 0, null, '2015/2016', 1),(36, 10, 2, 3, 1, 9, 9, 8, 0, null, '2015/2016', 1),(42, 11, 2, 4, 1, 5, 10, 0, 0, null, '2015/2016', 1),(46, 11, 1, 4, 1, 4, 10, 7, 0, null, '2015/2016', 0);",
    "present_kp": [
      "php"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "daily calculation and record insertion in google spreadsheet. i have a google spreadsheet that i am using to checkout last prices or levels of certain stock indices's. i have two worksheets in the workbook, one contains the indices's and a calculation i am keeping track of and the other sheet is empty.i want, on the empty sheet to check the calculation on the other sheet daily at 5pm eastern standard time and take that value and create a new record on the empty sheet going from a2:ahow would i go about this?sheet one looks like this:index, price, no. of components^dji, 12,456, 30etc.calculation=mycustomformulaworksheet two should be=day 1 at 5pm est, value of mycustomcalculationi would also like the value in spreadsheet to, to recalculate work sheet 1 every day at 5pm est in order to get the value i am looking for.",
    "present_kp": [],
    "absent_kp": [
      "google spreadsheets",
      "google apps script"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "programming-oriented word swap using transwrd.vim. the plugin transwrd.vim allows to to easily swap two words by pressing <a-t> while having cursor on the latter word.for example: two words becomes words two.i think the pattern which designates a word is contained in the g:transwrd_wordpattern variable and is (as default) the following: \\k\\+is it possible to define an advanced pattern which allows to be more programming language savvy (while still working for normal words)? eg.param[0], param[1] --> param[1], param[0]int call1() --> call1() intthis way it could also be used to swap function arguments (related to this other question).",
    "present_kp": [],
    "absent_kp": [
      "regular expression"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is it possible to download the demographic data day by day in google analytics?. when checking demographics data in ga, i can see each day's detailed data by moving the cursor over the graph. but how can i download a full version of that data? i tried export data but it only downloads the overall data.for example, this is what i need:| date | male | female ||---------|--------|---------|| apr 11 | 209 | 248 || apr 12 | 311 | 325 || apr 12 | 340 | 467 |thanks for any kind of tips.",
    "present_kp": [
      "google analytics"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "process descendants. i'm trying to build a process container. the container will trigger other programs. for example - a bash script that launches running background tasks with '&' usage.the important feature i'm after is this: when i kill the container, everything that has been spawned under it should be killed. not just direct children, but their descendants too.when i started this project, i mistakenly believed that when you killed a process its children were automatically killed too. i've sought advice from people who had the same incorrect idea. while it's possible to catch a signal and pass the kill on to children, that's not what i'm looking for here.i believe what i want to be achievable, because when you close an xterm, anything that was running within it is killed unless it was nohup'd. this includes orphaned processes. that's what i'm looking to recreate.i have an idea that what i'm loooking for involves unix sessions.if there was a reliable way to identify all the descendants of a process, it would be useful to be able to send them arbitrary signals, too. e.g. sigusr1.",
    "present_kp": [
      "process",
      "signals",
      "container"
    ],
    "absent_kp": [
      "fork"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "use finite difference discretization to find approximate solution to the poisson's equation. i've just been introduced to the poisson's equation. i've never had the need to dealt with pde, so i'm a bit lost.apparently we can compute an approximate solution of the poisson's equation$$ rac{\\partial^2}{\\partial x^2} + rac{\\partial^2}{\\partial y^2} u(x, y) = f(x, y)$$by discretizing the 2d poisson's equation using finite differences. here's a picture of that discretization taken from the website i've just linked you to:according to that website, we have:the above linear equation relating $u(i,j)$ and the value at its neighbors (indicated by the blue stencil) must hold for $1 <= i,j <= n$, giving us $n=n^2$ equations in $n$ unknowns. where by above linear equation i guess they are referring to$$4u_{i,j}+u_{i+1,j}+u_{i1,j}+u_{i,j+1}+u_{i,j1} = b_{i,j}$$$$1 \\leq i, j \\leq n$$when $(i,j)$ is adjacent to a boundary ($i=1$ or $j=1$ or $i=n$ or $j=n$), one or more of the $u(i+-1, j+-1)$ values is on the boundary and therefore $0$. $$b(i,j) = -f(i*h,j*h)*h^2$$ the scaled value of the right-hand-side function $f(x,y)$ at the corresponding grid point $(i,j)$.questionswhere does the $-4$ in front of $u_{i,j}$ in the equation above comes from?what's $b(i, j)$? why is it equal to $4u_{i,j}+u_{i+1,j}+u_{i1,j}+u_{i,j+1}+u_{i,j1}$? i mean, i don't understand where does it come from. usually $b$ refers to a right-hand side, but...why do we have $b(i,j) = -f(i*h,j*h)*h^2$?",
    "present_kp": [
      "finite difference",
      "discretization",
      "poisson"
    ],
    "absent_kp": [
      "numerical analysis"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "url percent encoded (hex) characters in .htaccess. there is an old page with a space in the filename, and this is no longer found on the website. so i need to redirect this page to another page using a 301 redirect in .htaccess.if i place the filename directly into .htaccess (eg. bouquets%20%26%20loose.html), the redirect does not work. if i escape the % sign(eg. bouquets\\%20\\%26\\%20loose.html), the redirect still does not work.how do i get this redirect to work in .htaccess?",
    "present_kp": [
      "htaccess"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "seo issues with categories which are frequently empty. we have a medium traffic site which sells used boats, the site does fairly well for popular search phrases, often ranking on first page. a common way for people to search is by boat manufacturer, for example sunseeker for sale or sunseeker 33 for sale. to service those searches, we have search results page with url's like: /used-boats-for-sale/sunseeker and /used-boats-for-sale/sunseeker/33 (i.e. make and model). this is fine for common makes but we have a lot of makes where we might have just one which, when sold, then leaves the page with no boats to show. it could then be just weeks till we get another one or sometimes years. once a manufacturer has no boats for sale, we automatically remove the link to that page from the site and from the sitemap. these pages are now being flagged as soft 404s in webmaster tools. currently these pages still work and just show a no results found message. i am unsure of how to deal with these pages. options as i see them:add a no-index, follow tag to the pages and continue to remove them from the sitemap. my concern is that when we do get a new boat for sale, the page will not rank again or take a long time to be re-indexed. add value to the 'no results found' page - for example, show listings for similar boats. if i do this (which makes sense from a usability perspective), would it be acceptable to leave these pages with an index tag?",
    "present_kp": [
      "seo"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to write to raid 1 disks on raspberry pi. i'm using arch linux on my raspberry pi, though i'm not sure that's relevant. i followed this tutorial to set up a raid 1 array with 2 usb hds. i then mounted the array (md0) in /media. what i can't find is how to write to this array. do i write to /media/md0? or do i write to one of the disks and mdadm then copies it to the second? md0 has only root write privileges. when i wrote a test file to it, it creates the file like it would for a normal file system, but i don't see that file when i cd to the disks. shouldn't i be able to see the file on the disks?below is the output of mdstat, which is how the tutorial suggests to monitor the disks:$awk '/^md/ {printf %s: , $1}; /blocks/ {print $nf}' </proc/mdstatmd0: [_u]",
    "present_kp": [
      "raid"
    ],
    "absent_kp": [
      "filesystems"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "bash variable substitution problem. i have a problem with variable substitution in bash. i'm doing like as explained in below code, but cd $d is not going into original directory i.e. after expanding $varexport var=/a/b/cfor d \\'cat file.f'; docd $ddonefile.f:$var/aa/bb/cc",
    "present_kp": [
      "bash"
    ],
    "absent_kp": [
      "shell script"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "algorithm for group reassignment of points. i have a list of clustered points but they are not in the cluster which has the center closest to them. the objective is to reassign them to minimize the total distance of each point to its cluster center.all the points can not be reassigned individually since there's a constraint on cluster size, and moving any of these points individually would cause a violation.so it's only possible to reassign them in pairs or groups. i want to build a potential assignment graph g=(v, e) with all the points.there could be one or more closer clusters than the point's current cluster, so besides a pair swap between two clusters there could be a group reassignment across three or more clusters.if i can get a proper represented graph, i could identify possible assignment by searching strongly-connected-components. but i'm not sure how to construct the graph to find the points that is guaranteed to decrease the objective. any thoughts are appreciated! thanksi'm not sure if there's enough information for you to answer. there's a distance matrix for all the points to all cluster centers. or if anything else needed, please let me know.",
    "present_kp": [
      "graph",
      "cluster"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to store keys for an encrypted storage in .net in-house service. in the company i'm working for we have a certain service where users can upload documents (text, sometimes document scans), it's nothing confidential but if those documents will be leaked, i'll get a lot of headache explaining why all of them are publicly available. recently we had a breach of out network and while this time no one managed to get anything it raised some security concerns. it's a vast legacy system, replacing it right now is out of the question. it exists within our corporate network and users can't access this data storage directly due to specific ad and network security policies. as practice shows those policies can be overridden because of our it dept negligence. so what i want is to add an additional layer of security by encrypting everything within the storage.the storage acts as an archive. people dump old documents into it and those documents are rarely if ever accessed. placing all of the incoming files into some encrypted containers is not an issue, easy to implement. the question is - how do i store keys for those containers? if i'll put them inside of the general service database, anyone who'd like to access those documents in an unauthorized manner will be able to do so by simply gaining access to the database (sql server, used by many apps, possibly compromised due to badly designed legacy services in the company). i can theoretically store keys in some other database (dedicated server with a proper setup of sql server\\ postgresql), physically disconnected from the main one and send keys from there only when requested and prevent attempts to dump all of the keys, any 'unusual' behavior is very easy to detect because people rarely request anything. and i don't care how long it'll take to restore and decrypt those containers, 1ms or 1 hour don't make any difference when taking in account the overall system processes. however, this approach feels a bit convoluted.are there any sort of 'best practices' for solving such problems?",
    "present_kp": [
      "database",
      ".net",
      "security"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "why do some packages use 'make install' while other use 'sudo make install'. i recently installed lame encoder on my server to convert wav to mp3 and that worked with './configure, make, make install;i then tried to install ogg encoder and that requires './configure, make, sudo make install' - which i cannot do, since i don't have a vps (and i'd rather not pay $15 a month for a vpn just to install a codec).i'm new to all this, and i understand sudo gives you permissions - but i have been trying to find out why exactly some 'make installs' require sudo access and others dont. i haven't yet found the answer. again, very knew to this so i'm just trying to learn what's going on.",
    "present_kp": [
      "sudo",
      "make"
    ],
    "absent_kp": [
      "software installation"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what should be the word count of an article for better ranking in 2014?. i know that serps depends on many factors like backlinks, page authority, keywords placement etc. but i noticed that google also look for the word count of the article. in 2010,2011 articles with 250+ word count got ranked in google. butin 2012,2013 the trend was different, if the post is 500+ words, itis likely to get ranked top in google compared to 250 word countarticles. in late 2013 we can see that all seo bloggers(who watch the seotrends) started to publish articles with 1500+ word count and theyranked better that the 500+ word count articles.now i was googling for making a review about external hdds and i found this post <url> its word count is 12k and got ranked top for the keyword portable hdd review in just 5 days. the ranking will depend on many factors but i am a bit confused about the word count. moz, searchengineland and other seo specialists suggest in 2014 to keep the word count near to 1.5k q: what should be the word count of an article for better ranking in 2014?nb: please don't mark this as a duplicate question. these questions are asked two years before. and i am asking about the trend in 2014. the image attached is from a post about word count published in march-2014. what should a page's minimum word count be in order to be effectively indexed?ideal word count per web page?",
    "present_kp": [
      "seo",
      "google",
      "serps",
      "post"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to mount a google drive directory locally?. i want to store backups to my google drive account.i want to mount google drive so that i can copy folders in there and it gets saved in my google drive.i have read this, but since i don't have any gui installed, is not possible for me to use the grive interphase.",
    "present_kp": [
      "mount",
      "google drive"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "track form fills using google analytics/ google tag manager. i am trying to track my form fills. i want to know which pages did the user went to before filling up the form. how can i track this in google analytics or google tag manager? looking for a step by step solution.",
    "present_kp": [
      "google analytics",
      "google tag manager"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "can multiple google accounts for google adwords effect campaign performance. i'm managing multiple google adwords accounts with different google accounts for each one and each of these separate google accounts has the exact same personal info such as name, sex, date of birth and mobile phone. could this somehow be a hinderance to each campaign's performance, because when run them all at the same time the results are flat verses when i run each one by itself? could somehow google see this as the same advertiser and effect the outcome?",
    "present_kp": [
      "google",
      "google adwords"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "i'm having some trouble moving mx records to the registrars dns. i run google apps email and have traditionally hosted my own dns records through my plesk setup. it has been recommended to us that we'd more reliably serve emails during downtime if we kept the mx and cname listings required by google apps hosted at enom instead of handling it internally.i have essentially copied the settings from plesk over to enom. the domain itself forwards with no problem, but the mx records, while they seem to be input into enom without error, never seem to reroute my mail. if i try to send to the email address i'm trying to route, i get the following error bounced back:google tried to deliver your message, but it was rejected by the recipient domain. we recommend contacting the other email provider for further information about the cause of this error. the error that the other server returned was: 553 553 sorry, that domain isn't in my list of allowed rcpthosts (#5.7.1) (state 14).this sort of error gets referenced a lot with regard to propogation, but it's been two days and enom updates everything else almost instantly. ideas?",
    "present_kp": [
      "email",
      "google apps"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "grub2 command line loaded instead ubuntu os. i have a strange problem. after installing ubuntu on usb stick (at already installed ubuntu computer), after removing usb stick with installed ubuntu from computer i get the trouble: after booting to computer i get grub 2 command line. how can i revert it to select ubuntu os by arrow keys (or load ubuntu strictly without grub, as just earlier)? now, i need to put these commands in grub to automatic load ubuntu:set root=(hd0,gpt2)linux /boot/vmlinuz-3.13.0-52-generic root=/dev/sda2initrd /boot/initrd.img-3.13.0-52-genericbootoutputs of some commands:$ sudo parted -l /dev/sdamodel: ata wdc wd20earx-00p (scsi)disk /dev/sda: 2000gbsector size (logical/physical): 512b/4096bpartition table: gptnumber start end size file system name flags 1 1049kb 538mb 537mb fat32 boot 2 538mb 1993gb 1992gb ext4 3 1993gb 2000gb 7446mb linux-swap(v1)my /etc/fstab:# <file system> <mount point> <type> <options> <dump> <pass># / was on /dev/sda2 during installationuuid=9310bdc0-b3f5-4258-ad7e-9d1de9dfb161 / ext4 errors=remount-ro 0 1# /boot/efi was on /dev/sda1 during installationuuid=c4d0-4779 /boot/efi vfat defaults 0 1# swap was on /dev/sda3 during installationuuid=d835e59a-4205-4951-9bad-e0d586f52c52 none swap sw 0 0",
    "present_kp": [
      "boot",
      "grub2"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "tar/ntfs/linux permissions. i want to backup my windows 7 partition on a dual boot system. i'd like to do the backup from the linux side so the windows files can't change during backup. i have a backup drive formatted in ntfs for this purpose. can anyone tell me if cd windows_partition_root;tar cfp - . | (cd backup_ntfs_partition; tar xvfp -)will preserve my windows ntfs permissions?",
    "present_kp": [
      "permissions",
      "tar",
      "ntfs"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "does java development typically involve more subclassing than c#/.net?. i've recently started looking at android development. this has brought me back into the world of java software development. the last time i worked with java, i'll admit, i didn't understand oop nearly as much as (i think) i do now.having mainly used c# in my career, i'm noticing a startling difference in how inheritance is used java and c#.in c# it seemed like inheritance could be avoided in most situations. the task at hand could usually be accomplished by using concrete classes of the .net framework.in java, from what i'm gathering from code samples, it seems like the java framework supplies many interfaces or abstract classes that are then meant to be implemented/extended by the developer.this seems to be too big a difference to just boil down to style. what is the reasoning behind this? i feel like i won't be writing clean java code until i understand this.also, is this limited to just the android sdk or is this a java-wide approach to oop?or put in another way,what is it about the design of these two languages that (seems to encourage) more or less inheritance use than the other?if the languages treat inheritance identically, and assuming my observation is valid, then it means this is related to the design of the frameworks/libraries and not the languages. what would the motivation be for this kind of design?",
    "present_kp": [
      "java",
      "c#",
      "inheritance"
    ],
    "absent_kp": [
      "object oriented",
      "language design"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "algorithm to determine if (union of cartesian products of subsets) equals (cartesian product of full sets). i have already asked this question in stackoverflow (open bounty closing on aug 25th).let's say i have some finite sets: a, b, ..., ki also have a1, a2, ... an, which are subsets of a; b1, b2, ... bn, which are subsets of b, etc.let's say s is the cartesian product a x b x ... x kand sn is the cartesian product of an x bn x ... x knis there an algorithm to efficiently determine if the union of all sn is equivalent to s?some pointers to literature where i can study this problem are greatly appreciated.",
    "present_kp": [],
    "absent_kp": [
      "ds.algorithms"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why does mkvmerge not create two equal files after issuing the same command?. i have a .mkv file which i want to delete one audio track from; let's call it source.mkv. the id of the audio track i want to remove is 1. so i issued this command:mkvmerge -o out.mkv -a 2 source.mkvafter this i got curious and decided to repeat the same process to see if i got the same output file. so i issued the command:mkvmerge -o out2.mkv -a 2 source.mkvafter doing this i calculated the sha256 sum of both files and, to my surprise, they were different.so my question is: why are the output files of the same command different?my first thought is that maybe the video or audio track of the output files is loosing quality in the process, but i don't know why.",
    "present_kp": [
      "audio",
      "video"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "can i merge two partitions from dfferent /dev/sda mount points into a single partition?. history:my laptop came with a windows 7 os installed, with a single partition (c:). i made a secondary partition out of that c: drive and installed linux mint in the second drive. i installed linux mint using bootable usb and selected something else. i made a separate /, swap, and /home out of the second partition. therefore, making a dual-boot.now, i wanted to go linux full time. i deleted/removed the windows partition using gparted. then after that, i clicked the unallocated drive and made a new partition in which i renamed data mounted at /mnt/data (highlighted in the screenshot). please see attached screenshot from gparted.now, my question is: is it possible to merge /dev/sda2 to my /home partition (which is /dev/sda7 under /dev/sda3)? i wanted to make that data partition included in the /home so that i will just have a single partition. data partition does not contain any data at since i just created that last night.",
    "present_kp": [
      "linux mint"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how do i attribute authors if i don't know their real names?. i want to write an r package and release on cran. but i have concerns about legal issues: licensing and authorship/ownership.code of some functions in the package will be derived from or based on posts in websites like these:a. this answer and this code for function 1;b. this comment for several other functions.what authors/contributors should i include in my r package as i only know nicknames of people who wrote the posts? a related question about what license this code comes under is here.",
    "present_kp": [
      "ownership"
    ],
    "absent_kp": [
      "attribution"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to change cflags option -march from native to corei7?. is it possible to change the cflags -march=native to -march=corei7? would i have to recompile the whole system once this change is made?i would like to setup distcc and i read that march cannot be set to native.",
    "present_kp": [],
    "absent_kp": [
      "gentoo",
      "gcc"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "set default options in cups. the cups docs say to change the printer defaults by ....lpoptions -p printer/instance -o name=valuethe option i want to change isfxcolormode/output color: *color blackso i have tried ..lpoptions -p test_printer/fxcolormode/output -o color=black but i get the errorunable to add printer or instance: undefined error: 0where am i going wrong?",
    "present_kp": [
      "cups"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "fedora gui package installer takes very long (>15min) to start. i tried to use linux gui for fedora but i did not find it intuitive than that of windows. whenever i tried installing any software using rpm file by double clicking file, many times it stuck and when i reinvoke that from terminal it says yum is already locked and some other installation process is already running and i always have to force quit that process. from os point of view this seems to be scheduling problem. (how) can we replace scheduling for linux x window? is there any better scheduling available for fedora linux x window?update: sorry about late response: => when i double click rpm package , it does not show up anything even after waiting for two minutes. => when i try to invoke that package from command line thinking something wrong with fedora gui it throws me a message saying some other process has the lock. => when i kill that process using kill command and try to install using command line it starts with no problems. => many times when i don't kill the process after invoking rpm installation package, i start doing some other work, after more than 15 minutes the installation window comes from nowhere asking for root password, after entering which i get window for downloading dependencies ?can somebody clarify me how this problem is not related to scheduling ?",
    "present_kp": [
      "linux",
      "fedora",
      "gui"
    ],
    "absent_kp": [
      "package management"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "google account - how to set my alternate (non-gmail) address as the account primary?. i'm a big fan of google services in general, and use quite a few of them. i don't happen to use gmail though. i do have a gmail address as part of my account, but i don't ever check it. i have added my regular email account as an alternate in my account settings, but i can't seem to figure out how to make the alternate address the primary one. because of this i never get any notices from my account because they all go to gmail, which i never look at.this page: <url> a note about setting the alternate address as the primary by first deleting it, but gives no further information on what to do next. ok, i've deleted it about a dozen times now but i still can't set it as the primary address.anyone have any suggestions? thanks.",
    "present_kp": [
      "google",
      "google account"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "proprietary software leveraging gfdl data?. i am building a website that will allow users to enter preferences for certain things (we'll call them cars), and get suggestions for other cars that they may be interested in. finding similarities between the cars requires a very large data set on the characteristics and features of car models.i've collected a significant amount of this data, much of which is public domain. i intend to use it in a proprietary, commercial project, which is fine so far.however, one of the data sets i'd like to use is released under the gnu free documentation license. i've read through the license, and i'm unsure how its terms about modification and the like apply to this situation. i don't really want to modify the original document (the data), i just want to read it and process it as part of a larger project. i'd certainly be happy to re-distribute the data freely as part of my work.is this allowable under the gfdl terms? would my new project have to be gfdl? would that even make sense since it's a software project, not a documentation project?i'm hazy on the difference between modifying the data, redistributing the data, and writing an algorithm that leverages the data to do other things...",
    "present_kp": [
      "gfdl"
    ],
    "absent_kp": [
      "license compatibility"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "is mentioning my blog on my resume helpful or hurtful to a job search?. i have a blog that i use mostly to record solutions to problems i have had, that i had some trouble finding an answer to. mostly problems where the online doc i googled provided too much info, and i found the answer to my question on the fifth page of my third google hit. (or if i asked the question here, i either didn't get an answer or i got slammed for asking a question that the answer could be easily googled.) i frequently look up stuff on this blog to remind myself of how i solved a problem, and it gets a decent amount of hits from others as well. anyway, i was wondering if mentioning this blog on my resume would help or hurt me in a job search? the topics are all over the map. what i would hope it shows is thati am a person who finds solutions to problemsi have used many different technologies in my worki am not afraid to tackle a challenge what i am concerned it shows is thatthis person had trouble with something that simple?why is this person bothering to blog this stuff?",
    "present_kp": [],
    "absent_kp": [
      "communication",
      "skills"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "long uids create issues with some unix commands. we have users with long uids from active directory. i just noticed today that a 'ps aux' shows the uid and not the user name. likewise, 'who' does not see the users at all. on the other hand, an ls -l works fine.an example of a long uid: 200148259 any ideas, suggestions, pointers?the os is centos 6.7the shell is bash 4.1.2",
    "present_kp": [
      "uid"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "handle decimal in mongodb. i'm trying to find a way to handle decimals in mongodb. is there any good way and/or should i handle it with a class like this:using system;public class mongodecimal{ public long value { get; set; } public long subvalue { get; set; } public long subvaluetovalue { get; set; } public decimal todecimal() { return convert.todecimal(value) + (convert.todecimal(subvalue) / convert.todecimal(subvaluetovalue)); } public override string tostring() { return todecimal().tostring(); } public string tostring(iformatprovider provider) { return todecimal().tostring(provider); } public string tostring(string format) { return todecimal().tostring(format); } public string tostring(string format, iformatprovider provider) { return todecimal().tostring(format, provider); } public static mongodecimal tomongodecimal(decimal value) { const long subvaluetovalue = 100000000; return tomongodecimal(value, subvaluetovalue); } public static mongodecimal tomongodecimal(decimal value, long subvaluetovalue) { var major = math.truncate(value); var minor = math.truncate((value - major) * convert.todecimal(subvaluetovalue)); return new mongodecimal { value = convert.toint64(major), subvalue = convert.toint64(minor), subvaluetovalue = subvaluetovalue }; } public static mongodecimal operator +(mongodecimal value, mongodecimal value2) { var right = safedecimal(value); var left = safedecimal(value2); return tomongodecimal(left.todecimal() + right.todecimal(), left.subvaluetovalue >= right.subvaluetovalue ? left.subvaluetovalue : right.subvaluetovalue); } public static mongodecimal operator -(mongodecimal value, mongodecimal value2) { var right = safedecimal(value); var left = safedecimal(value2); return tomongodecimal(left.todecimal() - right.todecimal(), left.subvaluetovalue >= right.subvaluetovalue ? left.subvaluetovalue : right.subvaluetovalue); } public static mongodecimal operator *(mongodecimal value, mongodecimal value2) { var right = safedecimal(value); var left = safedecimal(value2); return tomongodecimal(left.todecimal() * right.todecimal(), left.subvaluetovalue >= right.subvaluetovalue ? left.subvaluetovalue : right.subvaluetovalue); } public static mongodecimal operator /(mongodecimal value, mongodecimal value2) { var right = safedecimal(value); var left = safedecimal(value2); if (right.value == 0 && right.subvalue == 0) throw new dividebyzeroexception(); return tomongodecimal(left.todecimal() / right.todecimal(), left.subvaluetovalue >= right.subvaluetovalue ? left.subvaluetovalue : right.subvaluetovalue); } public static bool operator ==(mongodecimal value, mongodecimal value2) { if (value == null && value2 == null) return true; if ((value == null && value2 != null) || (value != null && value2 == null)) return false; return value.todecimal() == value2.todecimal(); } public static bool operator !=(mongodecimal value, mongodecimal value2) { return !(value == value2); } public override bool equals(object obj) { return this == (obj as mongodecimal); } public override int gethashcode() { unchecked { int hash = (int)2166136261; hash = (hash * <phone>) ^ value.gethashcode(); hash = (hash * <phone>) ^ subvalue.gethashcode(); hash = (hash * <phone>) ^ subvaluetovalue.gethashcode(); return hash; } } public static mongodecimal safedecimal(mongodecimal d) { if (d != null) return d; return new mongodecimal { subvalue = 0, value = 0, subvaluetovalue = 1 }; }}",
    "present_kp": [
      "mongodb"
    ],
    "absent_kp": [
      "c#",
      "fixed point"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "classifying test data into several classes. the following code is to classify the test data into several classes:function pdfmx = pdfparzen(train, test, winwidth)% computes probability density for all classes% using parzen window approximation% train - train set; the first column contains label% used to compute mean and variation for all classes% test - test set (without labels)% winwidth - width of the parzen window % pdfmx - matrix of probability density for all classes% class with label idx is stored in pdfmx(:,idx)classnb = rows(unique(train(:,1)));pdfmx = ones(rows(test), classnb);for samp=1:rows(test)for cl=1:classnbclidx = train(:,1) == cl;indiv = zeros(sum(train(:,1) == cl), columns(test));for feat=1:columns(test)indiv(:,feat) = normpdf(test(samp,feat), train(clidx, feat + 1), winwidth);endpdfmx(samp,cl) = mean(prod(indiv,2));endendmatlab command line usage:ercf_parzen = bayescls(train, test, @pdfparzen, 0.25 * ones(1,4), 0.1);parzen classification error:0.35581how can i reduce the error coefficient of this code?training data: train.txttest data: test.txtdriver programfunction errcf = bayescls(train, test, hpdf, apriori, winwidth)% bayes classifier% train - training set; the first column contains label% test - test set; the first column contains label% hpdf - handle to function used to compute probability density% apriori - row vector of a priori probabilities for all classes% winwidth - window width (just for parzen window hpdf function) clpdf = hpdf(train, test(:,2:end), winwidth); clpr = clpdf .* repmat(apriori, rows(test), 1); [val lab] = max(clpr, [], 2); errcf = mean(test(:,1) ~= lab);",
    "present_kp": [
      "ai",
      "matlab"
    ],
    "absent_kp": [
      "machine learning"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to create a template that links from one mediawiki to another?. i am trying to create a linkage between two media wiki. specifically, a template that links to an specific article on the other wiki.i would like to writesee {{otherwiki|some thing}}the template otherwiki should convert the above to[<url> some thing]the template i wrote works pretty well for single-word terms, and it works if i write some_thing, but i can't get it to work for phrases that contain whitespace.[<url>}}} {{{1}}}]how do i fix the above template to make it support whitespace?",
    "present_kp": [
      "mediawiki"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "cp /etc/dir_colors ~/.dir_colors not responding. i'm using centos 6.5 and putty.my problem is that directory file names are shown in dark blue color which is hard to read. i google searched and found this link; basically it's copying the dir_colors file from /etc to the home directory so changes will only affect the user instead of everyone. the real problem is that whenever i run this command cp /etc/dir_colors ~/.dir_colors, no .dir_colors file is created. and there is no error message too. i ran it using sudo too but also no file is created. when i named the file dir_colors(without the dot) then the file will be created, but when i changed the color from 1;34 to 1;33 in dir # directory, the dark blue color doesn't change to the new color. i'm guessing it is because the dot is missing before the file name. any ideas why no file is created when i used .dir_colors?",
    "present_kp": [
      "centos",
      "directory",
      "colors"
    ],
    "absent_kp": [
      "filenames",
      "file copy"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "ssl issues on my website. i'm running a xenforo website and i am now getting ssl security issues. i'm not so experienced in ssl, so i'm hoping you guys can provide some clarity on them. i was thinking that they probably had something to do with chmod, but don't know enough about it to help.here is a screenshot indicating the issues:",
    "present_kp": [],
    "absent_kp": [
      "https",
      "security certificate"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "index names of all the files in a plain text file. i've got this directory full of images, and i can do this:echo *.jpgimage1.jpg image2.jpg image3.jpg # and so onhow can i get the output in a plain text file in this format?image1.jpgimage2.jpgimage3.jpg",
    "present_kp": [
      "files"
    ],
    "absent_kp": [
      "bash",
      "command line"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "why would i need 'equals' if i have already 'hashcode'?. i got a question about why we need equals if we have hashcode.my first attempt was the answer because collision. but we corrected starting point with the assumption that we have not many objects so there is no collision at all. my second attempt was the answer because of the speed. but i also got the reply that there is something conceptual difference between hashcode and equals.so i read a lot of posts, the java doc and can not find the answer. do i miss something?",
    "present_kp": [
      "java"
    ],
    "absent_kp": [
      "hashing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "benefits of java bytearrayinputstream vs byte array?. i have a scenario where my java process will be receiving a stream of binary data:public class datahandler { public void handledata(data data) { // todo: do something with data }}i am trying to figure out the proper typing for data. i could use a byte[]:public class datahandler { public void handledata(byte[] data) { // todo: do something with data }}or a bytearrayinputstream:public class datahandler { public void handledata(bytearrayinputstream datastream) { // todo: do something with datastream }}to me, bytearrayinputstream implies a continuous stream of data that is constantly being modified. based on that, it feels like bytearrayinputstream is better suited for my use case as it probably adds a bunch of bells and whistles on top of a plain 'ole byte[]. but then i see that its constructor takes byte[] and doesn't allow you to add more bytes to it post-construction, which might not necessarily do anything for me in this particular case.so my questions:in my use case, are there any benefits to bytearrayinputstream over byte[]? why/why not? what are the factors that drive this decision (in general)?is there a better way to represent a true stream of flowing data (which ultimately comes in as bytes) besides the methods i've recommended here?",
    "present_kp": [
      "java",
      "data",
      "binary",
      "io",
      "byte"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "java compiler and vm compatibility. a co-worker and i recently had a discussion about java versions and the jvm. i use java 7 but we use java 6 for our client (while he says that some are still on 5). my immediate thought was, why can't we target those vms too?the java vm is somewhat different than a real machine in that it has a bunch of runtime features. type checking, exception handling, garbage collection, etc. but it's still a virtual machine which has a bytecode. (which is why we can have things like c to jvm compilers.) so why can't we target older vms with newer version of java? why does the language and the runtime have to be tied together? besides the obvious performance penalties, it seems like it should be completely possible to compile java 7 code to the java 6 jvm. (and considering how little changed from java 6 to 7, i can't imagine the compiler changes being that extensive.)",
    "present_kp": [
      "java",
      "jvm"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "javascript/php file uploader. i'm writing a javascript/php file uploader and i want to start this project off on the right track. i've started with this, a javascript uploader object which will handle ajax calls, display progress, show error messages, etc. i'm just hoping to get some feedback on the way i'm setting up my scripts.function uploader(method) {if(arguments.callee._singleton) return arguments.callee._singleton;var _this = arguments.callee._singleton = this;_this.opts = { form:#uploader, filepath:public/uploads/, errorcodes:{ 1:no file/folder selected, 2:invalid file/folder type, 3:file/folder exceeds upload size } }_this.methods = { init: function(options) { for(var o in options) { _this.opts[o] = options[o]; } _this.methods.setup(); }, setup: function() { }} if(_this.methods[method]) return _this.methods[method].apply(this, array.prototype.slice.call(arguments, 1));else if(typeof method === object || !method) return methods.init.apply(this, arguments);}",
    "present_kp": [
      "javascript",
      "ajax",
      "file"
    ],
    "absent_kp": [
      "object oriented"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what do you do to improve your logical programming skills?. do you think that only the programming pratice will help you to improve your logical programming skill or do you train your brain with puzzle games, trying imagine how universe works, playing instruments and so on?devoting more time with programming, will do you get logical programming skills more fast?",
    "present_kp": [
      "skills"
    ],
    "absent_kp": [
      "self improvement",
      "programming logic"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "structure of inheritance between animal classes and methods. i'm trying to do some basic inheritance between the 3 classes and use some simple methods, and i'm looking to learn what should be the correct structure between them and what can be improved.class animal: def __init__(self, kind, place): self.kind = kind self.place = place#this class inherits kind and place arguments from animal class (which can work for any animal)#initiates a dog object with name, gender and breed parameters.class dog(animal): def __init__(self, name, gender, breed): self.name = name self.gender = gender self.breed = breed animal.__init__(self, 'dog', 'ground') #woof method, just prints an action for the dog object with his name. def woof(self, *args): print(%s just did a %s woof % (self.name, *args)) #getallinfo method, get's all the parametrs of both classes. def getallinfo(self): print(%s is a %s %s %s, sitting on the %s % (self.name, self.gender, self.breed, self.kind, self.place))#cat class inherits the paramets of use for a cat (similar things) like name, gender and breed, which they both share, also the getallinfo method and initiate them. class cat(dog): def __init__(self, name, gender, breed): dog.__init__(self, name, gender, breed) animal.__init__(self, 'cat', 'ground')#speak method, returns a print for a cat meow with him name. def speak(self, *args): print(%s just did a %s meow % (self.name, *args))#here i create 3 objects, 2 dogs and 1 cat with selected arguments.#and check for some methods on the objects.mickey = dog('mickey', 'male', 'bulldog')flora = dog('flora','female','pug')tina = cat('tina','female','persian')tina.getallinfo()tina.speak('soft')dog.getallinfo(flora)dog.woof(mickey, 'loud')",
    "present_kp": [
      "inheritance"
    ],
    "absent_kp": [
      "python",
      "object oriented",
      "python 3.x"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "are more pages in one site better for seo?. i developed a site nearly 2 years (php, java), but i am a new seoer. my database has almost 1m articles. i have some questions i need help with: is it as a good idea to create static pages from all the articles? are more pages in the one site better for seo? if i created each article into a static page, that could be many gbs. how do i let the search engine crawler collect my database?",
    "present_kp": [
      "seo"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to ssh into a remote box, run a command and keep it running after i disconnect. i've seen people saying use screen.but when i typed screen in the terminal, i got: please set a terminal type.any idea how i can fix this?",
    "present_kp": [],
    "absent_kp": [
      "shell script",
      "gnu screen"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "can i bulk/schedule download several of my own youtube videos?. i know youtube doesn't allow normal users to direct download video files, but i'm talking about my own videos. they have explicit functionality for that so i'm not talking about violating youtube's terms of service or requesting some illicit program.youtube has a download mp4 button when using video manager, but it seems to only let me download 3 videos at a time, based on some weird timer; after the 3 downloads are over, it doesn't seem to let me download another.i have a few hundred uploaded videos that aren't on my hdd due to a lost disk (yes, i backup...but i only kept those videos on the backup disk, which died). i would really like to get local copies of those videos, but downloading them 3 at a time, especially with an unknown waiting period between batches, just doesn't scale. is there some way i can make this easier? i'm not seeing any other download options on youtube.",
    "present_kp": [
      "youtube"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "autofs : dynamic mounting rule for s3 bucket. i successfully implement autofs to automatically mount s3 bucket to the server which running ubuntu server 14.04.5 by following this tutorials. but the number of bucket (that needs to automatically mounted) is dynamic, means its can be increase or decrease. so far i need to add/remove rule in autofs config whenever the bucket number changed. the option command for mount those bucket is same. only path and bucket name that have difference. here is my configuration :on /etc/auto.master+auto.master/- /etc/auto.s3bucket --timeout=30on /etc/auto.s3bucket[mount-point-bucket1] -fstype=fuse,uid,gid,etc,etc :[tool-mounting]#bucket1[mount-point-bucket2] -fstype=fuse,uid,gid,etc,etc :[tool-mounting]#bucket2.....[mount-point-bucketx] -fstype=fuse,uid,gid,etc,etc :[tool-mounting]#bucketxmy question : is there a built-in script or function in autofs to dynamically add or remove rule in the file configuration?. so i don't need to re-config whenever the bucket is decrease or increase.",
    "present_kp": [
      "fuse",
      "autofs"
    ],
    "absent_kp": [
      "file server",
      "amazon s3"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "weekly flex calculator. this is one of the first rspec tests i've written, and i'm not really sure what the best practices are etc. my question basically is: what would you do differently? what is good? what is bad?the complete code can be found here.here is the class to be tested:class weeklyflexcalculator attr_reader :params def initialize(params) @params = params end def calculate group_efforts( (params.start_date..params.end_date).map do |date| daily_effort(date) end.compact ) end def group_efforts(result) weekly = result.group_by { |e| get_week_key(e[:date]) } weekly.map do |key,w| { year: get_year_from_week_key(key), week: get_week_from_week_key(key), weektarget: get_target_sum(w), weekeffort: get_effort_sum(w), efforts: w } end.sort { |a, b| b.efforts[0].date <=> a.efforts[0].date } end def daily_effort(date) target = get_target(date) effort = get_effort(date) return if target == 0 && effort == 0 { date: date, effort: effort, target: target, diff: effort - target } end def get_target_sum(efforts) efforts.inject(0){|sum,e| sum + e[:target]} end def get_effort_sum(efforts) efforts.inject(0){|sum,e| sum + e[:effort]} end def get_week_key(date) date.cwyear.to_s + | + date.cweek.to_s end def get_year_from_week_key(key) key.split('|')[0] end def get_week_from_week_key(key) key.split('|')[1] end def get_target(date) day_off?(date) ? 0 : params.user.hours_per_day end def get_effort(date) ts = get_timesheet(date) ts.nil? ? 0.0 : ts.timeinhours end def day_off?(date) date.wday == 0 or date.wday == 6 or params.holidays.include? date.to_s end def get_timesheet(date) params.timesheets.select {|ts| ts.date == date.to_s}.first endendand here is the test: require './weeklyflexcalculator'describe weeklyflexcalculator, during christmas week do subject(:calculation) { weeklyflexcalculator.new(params).calculate } let(:params) do messages = { :start_date => date.new(2013,12,23), :end_date => date.new(2013,12,29), :holidays => [2013-12-24, 2013-12-25, 2013-12-26], :timesheets => [], :user => user } double(:params,messages) end let(:user) do messages = { :hours_per_day => 7.5 } double(:user, messages) end let(:timesheet) do messages = { :date => date.new(2013,12,23).to_s, :timeinhours => 5.0 } double(:timesheet, messages) end context with no work performed do it { should have(1).item } context the week calculated do subject(:workweek) {calculation[0]} its([:year]) { should eq 2013 } its([:week]) { should eq 52 } its([:weektarget]) { should eq 15.0 } its([:weekeffort]) { should eq 0.0 } context the work efforts do subject(:efforts) {workweek[:efforts]} it { should have(2).items } context the first work effort do subject(:effort) {efforts[0]} its([:target]) {should eq 7.5} its([:diff]) {should eq -7.5} its([:effort]) {should eq 0.0} end end end end context with work effort on normal day do before do params.stub(:timesheets => [timesheet]) end it { should have(1).item } context the week calculated do subject(:workweek) {calculation[0]} its([:year]) { should eq 2013 } its([:week]) { should eq 52 } its([:weektarget]) { should eq 15.0 } its([:weekeffort]) { should eq 5.0 } context the work efforts do subject(:efforts) {workweek[:efforts]} it { should have(2).items } context the first work effort do subject(:effort) {efforts[0]} its ([:effort]) { should eq 5.0 } its ([:diff]) { should eq -2.5 } end end end end context with work effort on a holiday do before do timesheet.stub(:date => date.new(2013,12,24).to_s) params.stub(:timesheets => [timesheet]) end it { should have(1).item } context the week calculated do subject(:workweek) {calculation[0]} its([:year]) { should eq 2013 } its([:week]) { should eq 52 } its([:weektarget]) { should eq 15.0 } its([:weekeffort]) { should eq 5.0 } context the work efforts do subject(:efforts) {workweek[:efforts]} it { should have(3).items } context the first work effort do subject(:effort) {efforts[0]} its ([:effort]) { should eq 0.0 } its ([:diff]) { should eq -7.5 } end context the second work effort do subject(:effort) {efforts[1]} its ([:effort]) {should eq 5.0} its ([:diff]) { should eq 5.0} end end end endend",
    "present_kp": [
      "rspec"
    ],
    "absent_kp": [
      "ruby"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how can i make google crawl content behind an ajax load more button. i have a feed that has 10 questions and answers loaded when you first open the page. and then i have a load more button that gets you 10 new questions and answers everytime you click on it.how can i make googlebot crawl those new questions and answers as if they were paginated, i.e.: page 1 ==> 0-9 questions and answers page 2 ==> 10-11 questions and answers and so on.or can you provide a better alternative?",
    "present_kp": [
      "googlebot",
      "ajax"
    ],
    "absent_kp": [
      "seo"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "manipulate xml files in c#. i just finished my working code, but still want to improve it.i want to transform this input:<item>asdf</item><item>asdf</item><item>asdf</item>to this output:<string name=x1>asdf</string><string name=x2>asdf</string><string name=x3>asdf</string>my current code: static void main(string[] args) { string content; using (streamreader reader = new streamreader(arrays2.xml)) { content = reader.readtoend(); } int counter = 0; int startindex = 0; while ((startindex = content.indexof(<item>, startindex)) != -1) { counter++; content = content.substring(0, startindex) + <string name= ule + counter + \\> + content.substring(startindex + 6); } content = content.replace(</item>, </string>); using (streamwriter writer = new streamwriter(arrays2_formatted.xml)) { writer.write(content); } console.writeline(content); }i think the bottleneck is in the content assignment within the loop, as a lot of string instance are created and thrown away.however, is there a completely different way to do this efficiently?",
    "present_kp": [
      "c#",
      "xml"
    ],
    "absent_kp": [
      "strings"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "exclude files in tar.gz. i want to exclude two files from a tar file.. but it doesn't seem to workcd /var/www/public/api_testtar -zcvf api_v2.x.tar.gz api --exclude ./api/distributor_test.php --exclude ./api/library/distributor_api.php",
    "present_kp": [
      "tar"
    ],
    "absent_kp": [
      "linux",
      "debian",
      "gzip"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "alternative for digest -a sha256 command from solaris in linux. i have file from solaris which have digest -a sha256. but this command is not working in linux. i want the alternative for this command.",
    "present_kp": [
      "linux"
    ],
    "absent_kp": [
      "utilities",
      "checksum"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how do i find out where my server is?. i have this image hosted through dropbox and it loads slowly. i need to determine if the location of the server is too far away from my main audience so i can determine if that is the cause of the performance issue.",
    "present_kp": [
      "server"
    ],
    "absent_kp": [
      "web hosting",
      "images",
      "cdn",
      "geolocation"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "dsl design for ios autolayout. i am building a small dsl for ios' autolayout feature. obviously i want to have a typesafe way to define autolayouts instead of doing vsl. i came up with the following://in reality this will be a xamarin/ios viewtype view = | viewtype constraint = | max of int | min of int | eq of int | both of int * inttype element = | gap of constraint | part of view * constraintlet horizontal superview : element list = []let max v = max vlet min v = min vlet eq v = eq vlet minmax min max = both(min, max)let gap c es = gap c :: eslet view v c es = part (v, c) :: es//transform needs to implement the real autolayouting.//not yet donelet transform es = list.rev eslet super_view, left_view, middle_view, right_view = view, view, view, viewhorizontal super_view |> gap (min 20) |> view left_view (min 20) |> gap (max 30) |> view middle_view (eq 45) |> gap (eq 50) |> view right_view (minmax 20 100) |> gap (max 0) |> transformi am sort of content with the design so far; for example, it has a pretty fluid touch to it. however i am asking myself if there are better ways to do it. one particular issue is the simple fact that every definition is a intersection of gaps and views starting with a gap and ending with a gap. is there a possibility to encode such a design into types instead of doing it via runtime exceptions (which i would do now).",
    "present_kp": [
      "dsl"
    ],
    "absent_kp": [
      "f#"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "studying for the graduate entrance exam at csula. i'm an undergraduate student, but not a computer science major. i'm thinking of switching to cs after i graduate. i found a program that seems to allow students to take a test and gain entry into their school without having majored specifically in computer science first. here's the link: <url> online resources (youtube videos, online courses, etc.) would enable someone without workplace or coding experience to fully address each of the prompts on this entrance exam?",
    "present_kp": [],
    "absent_kp": [
      "education"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what to do when you have a speed-accuracy trade-off in your data?. many studies report that a speed-accuracy trade-off (sato) did not occur in the data since there is a positive correlation between rts and error rates. in other words, people took longer to respond for the trials that were more difficult, i.e. the same that elicited more errors on average, rather than trading speed for accuracy or vice-versa, a participant behaviour seen as undesirable. the absence of a sato is then strongly indicative of a participant having given their best during the task, i.e. correctly following the usual instructions to give fast but accurate responses. a certain level of skill is then said to characterise a particular sato curve, and this parameter of the curve can change as people become better at the task.however, rarely if ever have i seen papers that do report a sato, i.e. a negative correlation betewen rts and error rates, or, in other words, participants that remained at a relatively constant level of skill but simply moved up and down their sato curve. clearly this is not a sign of good data, but still, what is the correct analysis to perform in such situations, assuming that you can't just proceed to do analyses on rts and error rates the way you normally do (in the absence of a sato)?also, the above two paragraphs refer to the cases where a rt-errors correlation exists and is positive (1st para) or negative (2nd para). but what if there isn't a significant correlation at all?a good paper discussing this phenomenon is pachella's 1974 unclassified us air force paper the interpretation of reaction time in information processing research. however, i don't believe it addresses my questions above.referencespachella, r. g. (1974). the interpretation of reaction time in information-processing research. university of michigan human performance center technical report no.45 [pdf online]available at: defence technical information centre",
    "present_kp": [
      "reaction time"
    ],
    "absent_kp": [
      "cognitive psychology",
      "methodology",
      "experimental psychology",
      "statistics"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to export a subset of a 'less' output. every so often i need to take a subset of a log file and save it into a file for later use.i use less a lot to view and search within the logs and for exporting the interesting part, i currently do the following:in less show the line numbers and note down the line range i needback on the cli, using sed i extract the range i need and save this into a fileis it possible to do this from within less, i.e. without the sed part?",
    "present_kp": [
      "logs",
      "less"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "redhat acquires ansible - why?. recently, redhat acquired ansible, the developer of the famous configuration management software ansible, apparently for a price of about $100m.ansible, the software, afaik is licensed fully under the gpl. ansible, the company, does however also develop an extension to the software called tower that, as i understand it, is basically just a web gui to the core software.considering that redhat could use ansible for free anyway, and would, i guess, be very well capable of creating their own sort of gui if that's what they needed, and that even if they bought the company they still have to adhere to the gpl for the core, why on earth would they spend such a huge amount of money on this acquisition?i'd really appreciate if someone could share some wisdom as this is not the first time that such a transaction has left me puzzled.",
    "present_kp": [
      "gpl"
    ],
    "absent_kp": [
      "commercial"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is wrong with my init.d script [segmentation fault]. i currently filter out data from a mqtt live stream. i then dump this datum into a csv file named mqtt.csv and the dumping process is done by executor_start_up_job.txt which is written in bash. i want to make the dumping of the filter daemon, i.e. in the background, and also want to make it in so that every time the system is rebooted it is done automatically. so i've concluded to write an init.d script, and contents of my code is:#!/bin/bashset -xrequested_command=$1start() { /home/ed/start_up_job/executor_start_up_job.txt &}# restart the dumpstop() { killproc executor_start_up_job.txt echo}### main logic ###case $requested_command in start) start ;; stop) stop ;; status) last_line='cat mqtt.csv | tail -1' echo last print line: $last_line ;; restart) stop start ;; *) echo usage: $0 {start|stop|restart|status} exit 1 ;;esacexit 0once i have inserted set -x i get the following output:+ ./mysql_table_update_daemon.sh start+ requested_command=start+ start+ exit 0q: it's my first time writing an init.d and i have written my code based on this example it doesn't work, as i get the error segmentation fault",
    "present_kp": [
      "init.d"
    ],
    "absent_kp": [
      "shell script",
      "init script"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "collective service to build packages for linux. is there a service that guides how to wrap your software for all flavors of linux, and provides a build farm that cross-compiles them?",
    "present_kp": [
      "linux"
    ],
    "absent_kp": [
      "packaging"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "seo optimization for every subdomain blog.exampledomain.com vs. <url>. we are hosting our blog under blog.exampledomain.com and a seo consultant suggested to move it towww.exampledomain.com/blog because seo has to be done for every subdomain.background info:<url> ... landingpagealpha.exampledomain.com ... webapplicationblog.exampledomain.com ... blogis this true?what happens we also make the content available at the <url> (as far i know duplicate content is really bad)what steps are necessary to move it there? (if necessary)any other points that we should consider from seo perspective?",
    "present_kp": [
      "seo",
      "subdomain"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "is following sat case in $\\mathsf{p}$?. f[1](2, 3, 20) : 10011010f[2](4, 20, 21) : 10101001f[3](4, 20, 22) : 10010110f[4](2, 4, 23) : 10011010f[5](5, 23, 24) : 10101001f[6](5, 21, 23) : 01101001f[7](2, 5, 26) : 10011010f[8](2, 3, 33) : 10101001f[9](3, 22, 33) : 10000110f[10](3, 21, 40) : 10100110f[11](3, 21, 41) : 01101001f[12](21, 41, 42) : 10101001f[13](40, 42, 44) : 10010110f[14](3, 4, 45) : 10101001f[15](26, 45, 46) : 10101001f[16](26, 45, 47) : 10010110f[17](24, 46, 47) : 11001001f[18](24, 44, 47) : 01101001each f[i](a, b, c) : truth-table means some boolean formula of 3 variables $x_a, x_b, x_c$. the main formula is conjunction of them all (i just converted cnf for 53 factorization circuit to such form and managed to reduce it to given instance).$$\\phi = f_1 \\land f_2 \\land\\ ...\\ \\land f_n$$what i think is that now i need only logarithmic (or even constant) amount of all possible assignments to see if it's satisfiable.for example, i can take $x_2 = 1$ and it'll automatically result in $x_{20} = 0, x_{21} = 0, x_4 eq x_{22}, x_{23} = 0,$ etc. it seems that half of all variables will automatically be assigned, if i'll just put one. so, is it in $\\mathsf{p}$?",
    "present_kp": [],
    "absent_kp": [
      "complexity theory",
      "satisfiability",
      "3 sat"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "force/suggest the client to use one specific eap method. i would like to suggest to the client which one eap method and inner authentication to use.i'm using hostapd to create the ap.i've tried by removing some methods in the hostapd.eap_user but i simply managed to make those methods don't work anymore in case the client choose one of them.i've also tried to add a realm, with a specific eap method, by adding a 'nai_realm' line in hostapd.conf but when i launch hostapd it doesn't recognize 'nai_realm'.thank you.",
    "present_kp": [
      "hostapd"
    ],
    "absent_kp": [
      "configuration",
      "wpa supplicant",
      "802.1x",
      "wpa2 eap"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "google not returning an international version of the page. i run a website that duplicates some content across international editions. these are differentiated by the country codes e.g. /uk/folder/article1//au/folder/article1/the uk version is considered the origin of the content. recently the uk version of a specific article is being indexed by google as i am able to access via keyword search, however when i try to search for it via: site:domain.com/uk/folder/article1/ then it is not displaying, however the au version is. identical articles in the same folder are not having this issue. there are no errors within webmaster tools and i have recently refetched the specific url.the main reason why this is problematic is because the article is now no longer appearing on the uk edition of the site for internal site search. how can i find out why google is not getting a result when the url is entered but it is coming up when doing a specific keyword search?",
    "present_kp": [],
    "absent_kp": [
      "google search console",
      "web crawlers"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "advantages of ann classifiers over the adaboost. so what are the advantages of ann classifiers over the adaboost or boosting algorithm?",
    "present_kp": [],
    "absent_kp": [
      "machine learning",
      "lg.learning",
      "cv.computer vision"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to count dpi of an image (challenge)?. a) i have used an online service where you can upload an image.b) afterwards, you can select a print size and it gives you information on what is actual dpi based on that print size.c) when it gives you information if that particular dpi (dots per inch) for that print size is enough quality to print, so basically it outputs bad, average or good.i want to built this functionality on my own site, but i lack logic's behind how they count dpi.this is my two tests where i upload slightly different photos. as site outputs dpi it is easy to try to get how it counts it, but as for different photos it does differently. i just can not grasp the pattern on why it does so.here is my two different photos uploaded:product: postersize: 1000x1200i will just explain the first value - 8x10 -> 1200 / 10 - you get dpi by subtracting photo height with print size height.8x10 -> 1200 / 10 ->>> why this dpi is counted differently from other image? 10x10 -> 1000 / 10 12x12 -> 1000 / 12 12x16 -> 1200 / 16 ->>> why this dpi is counted differently from other image? 14x14 -> 1000 / 14 16x16 -> 1000 / 16 12x18 -> 1200 / 18 18x18 -> 1000 / 18 16x20 -> 1200 / 20 ->>> why this dpi is counted differently from other image? 18x24 -> 1200 / 24 ->>> why this dpi is counted differently from other image? 24x36 -> 1200 / 36 product: postersize: 1000x13338x10 -> 1000 / 8 ->>> comparing to this value 10x10 -> 1000 / 1012x12 -> 1000 / 1212x16 -> 1000 / 12 ->>> comparing to this value 14x14 -> 1000 / 1416x16 -> 1000 / 1612x18 -> 1333 / 18 18x18 -> 1000 / 1816x20 -> 1000 / 20 ->>> comparing to this value 18x24 -> 1000 / 18 ->>> comparing to this value 24x36 -> 1333 / 36 just do not get it why for a very similar image, counting of the same print sizes are different for particular values?any ideas?",
    "present_kp": [],
    "absent_kp": [
      "image processing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "can context-free grammar generates $a^{2^n}$. context-free grammar can generate the string $a^{2^n}$ for $n \\geq 0$.the production rule p is $s ightarrow ss | a$.the derivations is, for example:1) $s \\rightarrow a$ (this is when n = 0)2) $s \\rightarrow ss \\rightarrow aa$ (this is when n = 1)3) $s \\rightarrow ss \\rightarrow ss ss \\rightarrow aaaa$ (this is when n = 2)4) $s \\rightarrow ss \\rightarrow ss ss \\rightarrow ss ss ss ss \\rightarrow aaaa aaaa$ (this is when n = 3).am i right?",
    "present_kp": [],
    "absent_kp": [
      "context free"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "not able to execute system files. i was trying to install ruby with :sudo apt-get install ruby1.8output:w: not using locking for read only lock file /var/lib/dpkg/locke: unable to write to /var/cache/apt/e: the package lists or status file could not be parsed or opened.output of sudo dpkg --configure -adpkg: unable to access dpkg status area: read-only file system output of mount/dev/sda3 on / type ext4 (rw,errors=remount-ro)tmpfs on /lib/init/rw type tmpfs (rw,nosuid,mode=0755)proc on /proc type proc (rw,noexec,nosuid,nodev)sysfs on /sys type sysfs (rw,noexec,nosuid,nodev)udev on /dev type tmpfs (rw,mode=0755)tmpfs on /dev/shm type tmpfs (rw,nosuid,nodev)devpts on /dev/pts type devpts (rw,noexec,nosuid,gid=5,mode=620)/dev/sda1 on /boot type ext4 (rw)/dev/sdb1 on /home type ext4 (rw)mount: warning: /etc/mtab is not writable (e.g. read-only filesystem). it's possible that information reported by mount(8) is not up to date. for actual information about system mount points check the /proc/mounts file.output ofcat /proc/mountsrootfs / rootfs rw 0 0none /sys sysfs rw,nosuid,nodev,noexec,relatime 0 0none /proc proc rw,nosuid,nodev,noexec,relatime 0 0none /dev devtmpfs rw,relatime,size=1553128k,nr_inodes=216450,mode=755 0 0none /dev/pts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000 0 0/dev/disk/by-uuid/cf4fb4ae-6d12-407b-bf43-3b0daaaaaf74 / ext4 ro,relatime,errors=remount-ro,barrier=1,data=ordered 0 0tmpfs /lib/init/rw tmpfs rw,nosuid,relatime,mode=755 0 0tmpfs /dev/shm tmpfs rw,nosuid,nodev,relatime 0 0/dev/sda1 /boot ext4 rw,relatime,barrier=1,data=ordered 0 0/dev/sdb1 /home ext4 rw,relatime,barrier=1,data=ordered 0 0output of dmesg (some last part)[<phone>.237601] jbd2: detected io errors while flushing file data on sdb1-8[<phone>.229102] jbd2: detected io errors while flushing file data on sdb1-8[<phone>.799409] ipv6 addrconf: prefix with wrong length 56[<phone>.325125] ipv6 addrconf: prefix with wrong length 56[<phone>.801848] ipv6 addrconf: prefix with wrong length 56[<phone>.245363] jbd2: detected io errors while flushing file data on sdb1-8[<phone>.698223] ipv6 addrconf: prefix with wrong length 56[<phone>.105506] jbd2: detected io errors while flushing file data on sdb1-8[<phone>.119764] jbd2: detected io errors while flushing file data on sdb1-8[<phone>.205686] ipv6 addrconf: prefix with wrong length 56[<phone>.713179] ipv6 addrconf: prefix with wrong length 56[<phone>.241633] jbd2: detected io errors while flushing file data on sdb1-8[<phone>.220758] ipv6 addrconf: prefix with wrong length 56[<phone>.462909] jbd2: detected io errors while flushing file data on sdb1-8[<phone>.231049] jbd2: detected io errors while flushing file data on sdb1-8[<phone>.728348] ipv6 addrconf: prefix with wrong length 56[<phone>.247944] ipv6 addrconf: prefix with wrong length 56[<phone>.321558] jbd2: detected io errors while flushing file data on sdb1-8[<phone>.105491] jbd2: detected io errors while flushing file data on sdb1-8. /var/log/syslog do not have updated logwhat should i do in order to have my file system read write without loosing all my data ?is it appropriate to do sudo mount / -o remount,rw ? what other option do i have ?note: system is debian vm running in esxi server.",
    "present_kp": [
      "mount"
    ],
    "absent_kp": [
      "filesystems"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "jdbc connection and retrieval of info. if you are wondering why i ask, it is because i have numerous different methods all with very similar code (the only difference in the code in each method is what is between the generate resultset comments). i have tried any amount of times to try and shorten this but i am out of ideas. what i have been trying to get is to have the repeating code in the main method (so therefore it won't be repeating), and then to be able to call the resultset methods from here./** * this is the main method. * * @param args */public static void main(string[] args) {// call method resultset1resultset1();}/** * this method shows resultset1. */public static void resultset1() {// declare my variables and assign values to themstring url = jdbc:mysql://lamp.eeecs.qub.ac.uk/mmeaklim01;connection connection;// declare my prepared statement and assign a value to itpreparedstatement updatesalaries;string updatesalariesstring = update staff_members set salary = ? where idstaff = ?;// declare two arrays and put values in themint[] staffsalaries = { 45000, 40000, 35000, 42000, 29000, 29500, 47000, 34000, 30500, 42000, 44500, 41000, 29500, 29500, 30000, 34000 };string[] staffids = { m001, m002, m003, m004, m005, m006, m007, m008, m009, m010, m011, m012, m013, m014, m015, m016 };// try catch block to handle exceptionstry { // connect to the database connection = drivermanager.getconnection(url, mmeaklim01, xxx); // start the update updatesalaries = connection.preparestatement(updatesalariesstring); // declare my variable and assign a value to it int length = staffids.length; // for loop to take the values from my two arrays and use them to // update the staff_members table for (int loop = 0; loop < length; loop++) { updatesalaries.setint(1, staffsalaries[loop]); updatesalaries.setstring(2, staffids[loop]); updatesalaries.executeupdate(); } // generate resultset1 resultset resultset1 = updatesalaries .executequery(select name, position, salary from staff_members); // while loop system.out.println(resultset 1 ); system.out.println(name position salary); system.out.println(---------- ---------- ----------); while (resultset1.next()) { string name = resultset1.getstring(name); string position = resultset1.getstring(position); int salary = resultset1.getint(salary); // print out resultset1 system.out.println(name + + position + + salary + ); } // end of resultset1 // close the update updatesalaries.close(); // close the connection connection.close(); // add a linebreak system.out.println(); // output a message to the user to indicate that the program has // executed correctly system.out.println(the method has executed correctly.);} catch (sqlexception sqlexception) { // catch block system.out.println(error: + sqlexception.getmessage());}}and all i want the resultset methods to have, ideally, would be: // generate resultset1 resultset resultset1 = updatesalaries .executequery(select name, position, salary from staff_members); // while loop system.out.println(resultset 1 ); system.out.println(name position salary); system.out.println(---------- ---------- ----------); while (resultset1.next()) { string name = resultset1.getstring(name); string position = resultset1.getstring(position); int salary = resultset1.getint(salary); // print out resultset1 system.out.println(name + + position + + salary + ); } // end of resultset1i'm sure there is probably an easy way around this but i can't seem to figure it out.",
    "present_kp": [
      "mysql",
      "jdbc"
    ],
    "absent_kp": [
      "java",
      "beginner"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "integrate an external rest api with my web application. i have a service running on aws exposed via a rest api. the service processes files.clients of the service post a file to the api and are returned a job id.clients can then request the status of the job, if it is complete the job result is returned as json.the api essentially consists of a simple gateway service that accepts the job requests and puts them on an sqs queue. then there are a pool of ec2 workers that process jobs and update a database in rds with their results.the actual time taken to process a file can be between 20 seconds and 2 minutes depending on the file size, however depending on the queue size and size of my scaling group it can take several minutes from when a client submits a job and to when it is complete.i am now building a separate web application that allows humans submit jobs to this service. the user will upload a file to the web application, and will then see some sort of 'processing...' screen and then when the job is complete will see the results.i am unsure as to the best way to handle how the web application knows when a job is complete.at the moment this is my approach:1) when a user submits a file, my web application posts the file to the processing service and is returned a job id. that job id is stored in my local db with a state of 'processing'. that job id is added to a local 'jobs in progress' queue in my web application2) the web application returns the job id to the browser and the browser polls the web application checking the state of the job id in the database3) a separate thread pulls items from the local 'jobs in progress' queue and for each one polls the api service checking if the job is complete. this thread pulls say 50 items from the queue and a time, and checks the status of each job against the api every 10 seconds. if the job is complete its status is updated in the local database along with the job results.i know this will 'work' but it seems cumbersome and i wonder how it will scale, and if i am deploying this in a load balanced environment (say on aws) how will it cope with the potential failure of one of my web app nodes.it also seems to me that this is a common use case, and perhaps there are some better patterns. the web application is being built using spring.",
    "present_kp": [
      "aws"
    ],
    "absent_kp": [
      "web applications",
      "web services"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "keyboard for programmers?. i'd like to invest in a new keyboard - i've been typing on a macbook pro keyboard forever and, while i love it, it's time for something new and slightly more ergonomic. requirements:must play nice with my 2012 macbook promust not slide around too easily, that's my pet peeve with keyboardsmust be affordable (ideally < $200, but much less would be better)format should be us/qwerty. bonus points if i can rip the keycaps off and turn it onto dvorak, if i ever get the ambition up to do so.ideally:bluetooth, i don't have very many spare usb portshave an option to have an os x-style 'command' key symbol instead of the nasty windows key symbol.not sound like hail hitting a tin roof, that makes me cringe when i type. not needed:numeric pad: i hate those things and never use them anywayportability: i don't have any issues using my macbook keyboard when travelling.i've heard good things about the code keyboards - does anyone have any experience with them? are there other keyboards that might work well for my needs?",
    "present_kp": [
      "keyboards"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "simple async/await progress bar. in researching the new .net async and await keywords in .net 4.5, i created a simple example showing how to update a progress bar and a label while an async function is running. i also included how to cancel the process while it is taking place.to use the code below, create a windows form and include two buttons:button1 = start buttonbutton2 = stop buttonalso add a progress bar called progressbar1 and a label called label1.a few things to note:the functions use async and await all the way down from the initial button click to the final vb function that supports asynchronous support;note the use of the cancellationtokensource to signal that we want to cancel things;the final asynchronous vb function we use is webclient.downloadstringtaskasync(). this is arbitrary, and is used for example purposes only.please let me know if you have any questions or problems with this code.' simple progress bar example with cancellation and label updating' using the async and await keywords available through .net 4.5' helpful sources include:' - <url> - <url> system.netimports system.threadingpublic class form1 dim test as webclient = new webclient ' this is specific to the type of async function we're doing. yours may be different. dim mytoken as cancellationtokensource ' this is used to notify the async function that we want to cancel it. dim myprogress as progress(of integer) ' we use this as the vehicle for reporting progress back to our gui thread dim counter as integer = 0 ' this is a variable that we use to track our progress (specific to this example) private async sub button1_click(sender as object, e as eventargs) handles button1.click progressbar1.value = 0 counter = 0 mytoken = new cancellationtokensource await startprogressbar(mytoken) ' note that the signature of our startprogressbar method includes the cancellationtokensource end sub private sub button2_click(sender as object, e as eventargs) handles button2.click if not mytoken is nothing then mytoken.cancel() ' we tell our token to cancel end if end sub public async function startprogressbar(ct as cancellationtokensource) as task dim progresstarget as action(of integer) ' we have to create an event handler for the function we use to report progress progresstarget = addressof reportprogress ' we assign the handler to our specific function (see below) myprogress = new progress(of integer)(progresstarget) ' when we initialize our progress reporter, we pass our handler to it for i = 0 to 9 if ct.iscancellationrequested = true then exit for ' we then break out of the loop end if dim mystring as string = await retrievestring(myprogress) ' we don't do anything with this string, just so we can run our async function next end function public async function retrievestring(byval myprogress as iprogress(of integer)) as task(of string) dim mystring as string = if not myprogress is nothing then 'if we have a valid progress object, report our progress using it counter = counter + 10 myprogress.report(counter) end if mystring = await test.downloadstringtaskasync(<url>) ' this is a throwaway function - just something that uses async return mystring ' we are really not doing anything with the results of this function, just returning it end function private sub reportprogress(byval myint as integer) label1.text = step number: & myint.tostring progressbar1.value = myint end subend class",
    "present_kp": [
      ".net"
    ],
    "absent_kp": [
      "vb.net",
      "async await"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "which fonts do designers use?. let me inform you that i am not a designer by trade and i don't have any design sense, either. but by luck i have to design forms in my company. but i do not get the professional look that you see on websites. i want to know which fonts designers use for:descriptiontitle (say, the title of a blog post)also what should the sizes be?",
    "present_kp": [
      "fonts",
      "design"
    ],
    "absent_kp": [
      "html"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "zfs on linux and amazon ec2. i have a situation where i need dynamically expandable storage space on a linux machine running in amazon ec2, and zfs came to mind. if i remember properly, zfs supports a way to dynamically expand a non-redundant jbod-like array. i have a few requirements for a project i'm working on:i should be able to move the array to another ec2 instance if need be.i should be able to dynamically expand the zfs array, adding new drives to the zfs volume without any downtime.is this possible? if zfs provides the ability to dynamically expand the array, i should be able to run a script which would dynamically create new ebs volumes, attach them to the ec2 instance, and have zfs add them to its pool, all dynamically without downtime.",
    "present_kp": [
      "amazon ec2",
      "zfs"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "laptop sleeps by itself - in logs sleep requested - requested by what?. i left my linux laptop on with the lid open while it was on battery power for several minutes, and it went to sleep (according to the systemd journal) - i assume this means the same as hibernation?but my laptop, for whatever reason, is unable to resume from sleep/hibernate, so it just booted up from scratch and i would have lost whatever unsaved work i had (fortunately, i didn't have any unsaved work).i went into kde system settings to try to configure this, but the only time that it is configured (in kde) to hibernate is when the battery charge level is critical, which in turn is configured to mean 5% or below. but the battery level was nowhere near as low as 5%.if i runjournalctl -x --since=2016-06-19 08:00:00 --until=2016-06-19 08:15:00to see what was happening around that time, there is not much to see:jun 19 08:04:26 localhost.localdomain audit[1]: service_stop pid=1 uid=0 auid=<phone> ses=<phone> msg='unit=dnf-makecache comm=systemd exe=/usr/lib/systemd/systemd hostname=? addrjun 19 08:11:04 localhost.localdomain networkmanager[1113]: <info> sleep requested (sleeping: no enabled: yes)jun 19 08:11:04 localhost.localdomain networkmanager[1113]: <info> sleeping...jun 19 08:11:04 localhost.localdomain audit: netfilter_cfg table=nat family=10 entries=52jun 19 08:11:04 localhost.localdomain networkmanager[1113]: <info> (wlp2s0): device state change: activated -> unmanaged (reason 'sleeping') [100 10 37]jun 19 08:11:04 localhost.localdomain audit: netfilter_cfg table=mangle family=10 entries=40jun 19 08:11:04 localhost.localdomain audit: netfilter_cfg table=filter family=10 entries=83jun 19 08:11:04 localhost.localdomain audit: netfilter_cfg table=nat family=2 entries=58jun 19 08:11:04 localhost.localdomain audit: netfilter_cfg table=mangle family=2 entries=40jun 19 08:11:04 localhost.localdomain audit: netfilter_cfg table=filter family=2 entries=92jun 19 08:11:04 localhost.localdomain networkmanager[1113]: <info> (wlp2s0): canceled dhcp transaction, dhcp client pid 2065jun 19 08:11:04 localhost.localdomain networkmanager[1113]: <info> (wlp2s0): dhcpv4 state changed bound -> donejun 19 08:11:04 localhost.localdomain avahi-daemon[918]: withdrawing address record for 192.168.1.101 on wlp2s0.jun 19 08:11:04 localhost.localdomain avahi-daemon[918]: leaving mdns multicast group on interface wlp2s0.ipv4 with address 192.168.1.101.jun 19 08:11:04 localhost.localdomain kernel: brcmfmac: brcmf_inetaddr_changed: fail to get arp ip table err:-23jun 19 08:11:04 localhost.localdomain avahi-daemon[918]: interface wlp2s0.ipv4 no longer relevant for mdns.jun 19 08:11:04 localhost.localdomain kernel: brcmfmac: brcmf_cfg80211_reg_notifier: not a iso3166 codejun 19 08:11:04 localhost.localdomain avahi-daemon[918]: withdrawing address record for fe80::290:4cff:fe0d:f43e on wlp2s0.jun 19 08:11:04 localhost.localdomain networkmanager[1113]: <info> networkmanager state is now asleepjun 19 08:11:05 localhost.localdomain networkmanager[1113]: <warn> failed to gdbus.error:fi.w1.wpa_supplicant1.notconnected: this interface is not connected: disconnect.jun 19 08:11:05 localhost.localdomain dbus[943]: [system] activating via systemd: service name='org.freedesktop.nm_dispatcher' unit='dbus-org.freedesktop.nm-dispatcher.service'jun 19 08:11:05 localhost.localdomain systemd[1]: reached target sleep.-- subject: unit sleep.target has finished start-up-- defined-by: systemd-- support: <url> -- unit sleep.target has finished starting up.-- -- the start-up result is done.jun 19 08:11:05 localhost.localdomain systemd[1]: starting sleep.-- subject: unit sleep.target has begun start-up-- defined-by: systemd-- support: <url> -- unit sleep.target has begun starting up.jun 19 08:11:05 localhost.localdomain systemd[1]: starting suspend...-- subject: unit systemd-suspend.service has begun start-up-- defined-by: systemd-- support: <url> -- unit systemd-suspend.service has begun starting up.jun 19 08:11:05 localhost.localdomain systemd[1]: starting network manager script dispatcher service...-- subject: unit networkmanager-dispatcher.service has begun start-up-- defined-by: systemd-- support: <url> -- unit networkmanager-dispatcher.service has begun starting up.jun 19 08:11:05 localhost.localdomain systemd-sleep[19112]: suspending system...-- subject: system sleep state suspend entered-- defined-by: systemd-- support: <url> -- the system has now entered the suspend sleep state.how can i find out what caused my laptop to go to sleep?",
    "present_kp": [
      "systemd",
      "kde",
      "laptop",
      "hibernate"
    ],
    "absent_kp": [
      "fedora"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "version hash to solve event sourcing problems. the basic examples i have seen about event sourcing do not deal with out of order events, clock offsets in different systems and late events from system partitions.i am wondering if more polished event sourcing implementations rely on a version stamp of modified objects?for example, assuming that the system is rendering the entity client with version id abcd1234. if the user modifies the entity, the system will create an event with the modified fields and the version id reference to which version it applies. later the event responder would detect out of order events and merge them.",
    "present_kp": [
      "event sourcing"
    ],
    "absent_kp": [
      "cqrs",
      "distributed system"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "implementing 'zip4' in haskell. learn you a haskell explains zip4:--ghci> zip4 [2,3,3] [2,2,2] [5,5,3] [2,2,2] -- [(2,2,5,2),(3,2,5,2),(3,2,3,2)] please critique my implementation:zip4 :: [a] -> [a] -> [a] -> [a] -> [(a,a,a,a)]zip4 xs ys zs as = zip4' xs ys zs as where zip4' [] _ _ _ = [] zip4' _ [] _ _ = [] zip4' _ _ [] _ = [] zip4' _ _ _ [] = [] zip4' (b:bs) (c:cs) (d:ds) (e:es) = (b,c,d,e) : zip4' bs cs ds estests:*main> zip4 [2,3,3] [2,2,2] [5,5,3] [][]*main> zip4 [2,3,3] [2,2,2] [5,5,3] [1][(2,2,5,1)]*main> zip4 [2,3,3] [2,2,2] [5,5,3] [3][(2,2,5,3)]*main> zip4 [2,3,3] [2,2,2] [5,5,3] [3..100][(2,2,5,3),(3,2,5,4),(3,2,3,5)]*main> zip4 [1,1,1] [2,2,2] [3,3,3] (replicate 100 100)[(1,2,3,100),(1,2,3,100),(1,2,3,100)]",
    "present_kp": [
      "haskell"
    ],
    "absent_kp": [
      "reinventing the wheel"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to access environment variables from unix services dynamically?. i have a command-line program that is periodically run by services (either as a cronjob or by systemd) and it heavily depends on environment variables which are not static (that is, they might change) which makes it impossible to create a file and source it in the service script (as stated in this question).is there a way to access environment variables from a unix service (crontab or systemd) dynamically?",
    "present_kp": [
      "cron",
      "systemd",
      "environment variables",
      "services"
    ],
    "absent_kp": [
      "daemon"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "data mapping object with lazy loaded properties. let's imagine we have an object user with a property of type profile that can be accessed by the method getprofile and since we don't need the user's profile for every operation we decided to lazy load the user's profile. but now, what happens when we decide to update the user object in the database using it's datamapper? using the most simple approach would make the user object load the profile and then save it to database.but it is easy to see that loading an object from the database just to save it again it's useless. what would be a good design choice for this situation?",
    "present_kp": [
      "design"
    ],
    "absent_kp": [
      "design patterns",
      "architecture",
      "software"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "regarding pfaffian methods in counting and combinatorics. recently, i was going over an introduction to holographic algorithms. i came across some combinatorial objects called pfaffians. i do not really know much about those at the moment and came across some surprising uses they can be put to.for instance, i came to know that they can be used to efficiently count the number of perfect matchings in planar graphs. also, they can be used to count the number of possible tilings of a chessboard using 2*1 tiles. the tiling connection seemed very curious to me and i tried searching for more relevant materials on the web but in most places i merely found just one statement or two about the connection and nothing else.i just meant to ask if someone could suggest some reference to relevant literature as that would be really great and i am looking forward to study some related materials.",
    "present_kp": [],
    "absent_kp": [
      "reference request",
      "co.combinatorics",
      "counting complexity"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how to change temporarily font size in text console in opensuse?. i found several answers already but all of them are focused on changing size for good, i.e. at lilo/grub level. i don't want this though.so, key issues:changing size on-fly, i would like to increase size, and 5 minutes later, decrease it backit is question about text console, not terminal running in x11opensuse 11.4 if this matters. thank you in advance.i am only interested in size of the font, not in typeface of it.",
    "present_kp": [
      "console"
    ],
    "absent_kp": [
      "linux",
      "fonts"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "latexmk, from makefile, from bash script, from cron - latexmk not being executed. i have several git repositories with latex files that i want to have typeset automatically. the idea is to have a central bash script (run by a cronjob) that executes a bash script in every repository, which (1) pulls new commits and (2) executes make all, which should call latexmk on the changed latex files.the central bash script simply contains lines like:bash ./repos/repo-xyx/cron.shthen in repos/repo-xyz/cron.sh is something like:cd $(dirname $0)git pullmake allcd -and in the makefile in the same directory:all: $(subst .tex,.pdf,$(wildcard *.tex))%.pdf: %.tex latexmk -pdf -pdflatex=pdflatex -shell-escape $< </dev/nullin my user's crontab, i have * * * * * bash .../cron.sh 2>&1 > .../cron.log and shell=/bin/bash. when the cronjob is executed, i read the following in the log:already up-to-date.latexmk -pdf -pdflatex=pdflatex -shell-escape myfile.tex </dev/null.../ (this comes from the line cd -)as you can see, latexmk is invocated but doesn't do anything. myfile.pdf is not generated.when i run bash cron.sh (as the same user) from the highest directory, this does work.what could cause the makefile to not execute commands when run from a bash script that is run by a cron job (at least, i think it's make not executing this command)? this is gnu make 3.81 on linux ubuntu 3.13.0-51-generic #84-ubuntu smp wed apr 15 12:08:34 utc 2015 x86_64 x86_64 x86_64 gnu/linux.",
    "present_kp": [
      "bash",
      "ubuntu",
      "cron",
      "latex",
      "gnu make"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "java sequential bfs. i am currently working on optimizing my code. in order to do that, i have to make sure that my sequential bfs works perfectly. then, i should i apply threads or executor service to run it in parallel. will my bfs break if the graph is too big?public void bfs(int rootnode, isvisited visitor){// bfs uses queue data structurequeue<integer> q = new linkedlist<integer>();int currentlevel = 0;q.add(rootnode);visitor.visit(rootnode, currentlevel);while (!q.isempty()){ currentlevel++; set<integer> nextnodes = new linkedhashset<integer>(); q.foreach(currnode ->nextnodes.addall(outgoingneighbors(currnode))); q.clear(); for (int child : nextnodes) { visitor.visit(child, currentlevel); q.add(child); }} }",
    "present_kp": [
      "java",
      "graph"
    ],
    "absent_kp": [
      "breadth first search",
      "visitor pattern"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "stat file system sizes. with stat 8.13 on a debian based linux - among many others - the following format directives (--format=) are offered:in combination with --file-system (-f):%s block size (for faster transfers) %s fundamental block size (for block counts) question(s): what exactly is meant?my best guess is that %s, %s equals %b (display in blocks) and %b (display block's size), where the latter are for files and the first two are for file systems. is that correct?",
    "present_kp": [
      "linux",
      "stat"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how to limit administrative privileges to various config files?. i want one administrator to be able to edit the /etc/fstab file but one administrator to not be able to edit the fstab file.both administrators need access to all other administrative tasks.is this possible?i am assuming i would need to somehow setup what files sudo gives them access to. or possible just make non administrative accounts and then create a group called fstab-access and add one of the users to that and setup privileges for that group... ? am i on the right track?",
    "present_kp": [
      "sudo",
      "privileges"
    ],
    "absent_kp": [
      "administration"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what is the state-of-the-art on using computers to clean-up images?. two part question - we've all seen the movies/tv shows where the police/feds/spies use computer software to take a grainy photo and do a clean-up to see a better picture and more details. i assume the concept is executed by some sort of uber-power pixel-smoothing or anti-aliasing type of algorithms to fill in the blanks based on deductive processing.part 1: how real is this technology in the public/commercial software world? i'm not asking about any speculation on alleged secret gov software or such, i just want to know where we actually are with this concept today? how much is fully automated vs human-assisted. part 2: assuming there actually is some reality with this technology for photographs the second part of this question is how (if at all) has this been applied to videos? again the issue of fully automated vs human assisted is of interest here.at the heart of this post is the ultimate question of how viable is today's software for being able to take an old vhs or dvd recording and process the frames to create a new hd-resolution remaster. considering that doing this would mean cleaning up tens-of-thousands of frames for even a simple wedding video i am not expecting this technology to be fast of course.note: per in-topic and meta discussions i went ahead and cross-posted this in two other suggested se forums to acquire their viewpoints and expertise on this matter. so far (it is still a little early in the day so to speak) i have received some pretty interesting information in 2 of the 3.when this is all done (when good answers have been selected) i would appreciate a way to merge these for the benefit of all three se communities:computer graphics: what is the state-of-the-art on using computers to clean-up images?signal processing: <url> production: <url>",
    "present_kp": [
      "video"
    ],
    "absent_kp": [
      "image processing",
      "photo realistic"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "java classes and database queries. can someone please explain the best way to solve this problem.suppose i have three classespersonvenuevehiclei have a dao method that needs to return some or all of these attributes from each of the classes after doing a query.please note, by requirements i am using one dao for all three classes and no frameworks.only my own mvc implementationhow do i accomplish this? it seems very wrong to make a class personvenuevehicle and return that as an object to get the instance field, values.i was taught that the database entities must be reflected by classes, if this is case how is it implemented in such a situation?",
    "present_kp": [
      "java",
      "database",
      "dao"
    ],
    "absent_kp": [
      "object oriented",
      "java ee"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "on ssh connection the auth.log shows as source ip the external ip of the server instead of the ip of the client. i have a very odd situation with my ssh connection. every time i connect to my server, whatever is my external ip, the connection have as source the server's external ip.example server s.o.: pclinuxos client ip: [c.c.c.c] server ip: [s.s.s.s]after ssh successful connection, i execute the last command and i get this: username pts/0 [s.s.s.s] wed jan 29 19:29 still logged inthis is the result of netstat searching for port 22:$ netstat -atn | egrep '(:22)'tcp 0 96 192.168.1.34:22 [s.s.s.s]:31685 establishedi can't find the client ip [c.c.c.c] anywhere.i wouldn't have any particular issue on this, but now comes the best part.i found an attack on my server inside the auth.log:jan 27 12:55:42 localhost sshd[2295]: invalid user a from [s.s.s.s]jan 27 12:55:42 localhost sshd[2295]: input_userauth_request: invalid user a [preauth]jan 27 12:55:42 localhost sshd[2299]: invalid user a from [s.s.s.s]jan 27 12:55:42 localhost sshd[2299]: input_userauth_request: invalid user a [preauth]jan 27 13:49:58 localhost sshd[17917]: invalid user jack from [s.s.s.s]jan 27 13:49:58 localhost sshd[17917]: input_userauth_request: invalid user jack [preauth]jan 27 13:50:07 localhost sshd[17923]: invalid user ibsadmin from [s.s.s.s]jan 27 13:50:07 localhost sshd[17923]: input_userauth_request: invalid user ibsadmin [preauth]after this attack the result is that my fail2ban daemon will insert the server ip [s.s.s.s] on the banned list and i can't connect to my server anymore.the server is behind a router with a nat port forwarding on ssh port. the issue is not present when i connect to the server from inside my lan, the correct ip is showed in this case.this is my sshd_config file:acceptenv lang lc_ctype lc_numeric lc_time lc_collate lc_monetary lc_messagesacceptenv lc_identification lc_allacceptenv lc_paper lc_name lc_address lc_telephone lc_measurementauthorizedkeysfile .ssh/authorized_keyschallengeresponseauthentication noclientaliveinterval 60hostkey /etc/ssh/ssh_host_dsa_keyhostkey /etc/ssh/ssh_host_keyhostkey /etc/ssh/ssh_host_rsa_keyignorerhosts yeskeyregenerationinterval 1hlogingracetime 30maxauthtries 2maxsessions 4maxstartups 10:30:60passwordauthentication nopermitemptypasswords nopermitrootlogin noport 22pubkeyauthentication yesrsaauthentication yesstrictmodes yessubsystem sftp /usr/lib64/ssh/sftp-servertcpkeepalive yesusedns nousepam nouseprivilegeseparation yesx11forwarding yesi know for sure that i didn't had this issue one month ago, but i can't figure out what is changed.edit:this is the result of iptables -l command:$ iptables -lchain input (policy accept)target prot opt source destinationfail2ban-ssh tcp -- anywhere anywhere tcp dpt:sshchain forward (policy accept)target prot opt source destinationchain output (policy accept)target prot opt source destinationchain fail2ban-ssh (1 references)target prot opt source destinationreturn all -- anywhere anywhere",
    "present_kp": [
      "linux",
      "ssh",
      "ip"
    ],
    "absent_kp": [
      "networking",
      "routing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "touchpad was disabled after plugging a bluetooth mouse in and back out. i have installed arch linux on my computer (asus k53sv) and everything works well. but after i plug in my bluetooth mouse, which is a logitech m215, then the touchpad is disabled, except for the left and right mouse buttons. the mouse cursor doesn't move though i move my hand on touchpad. so, what is the problem and how can i fix it?",
    "present_kp": [
      "mouse",
      "bluetooth",
      "touchpad"
    ],
    "absent_kp": [
      "xorg",
      "xinput"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what are some good converters to rip audio from youtube videos?. i have some videos that i want to listen to, but don't really want to watch. what are some good sites to download just the audio in formats like mp3?",
    "present_kp": [
      "youtube",
      "video",
      "audio",
      "mp3"
    ],
    "absent_kp": [
      "webapp rec"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "xclip image binary contents pasted into text fields. say i pick some png file and run the following command:xclip -selection clip -t image/png image.pngi now have that image in my clipboard. if i paste it somewhere that accepts an image, it is pasted as expected and all is good.now instead consider that i press ctrl+v while being in the firefox address bar or in the text field i'm typing this in. the binary contents of the file are pasted verbatim into the text field, in some cases causing the browser to hang for a while.of course i know it doesn't make sense to paste an image there, but i sometimes do it accidentally, and then it causes problems.when i instead paste an image i copied using firefox's copy image button, it doesn't get pasted when i try to paste it, so it must be possible to store it in the clipboard to allow for this behaviour.how can i place an image in the clipboard without making the image get pasted verbatim as binary data into text fields? if it's possible to somehow place both an image and a text string (such as the path to the image or something) in the clipboard and have it pick the appropriate one when pasting, that would be awesome.",
    "present_kp": [
      "clipboard",
      "xclip"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "balancing old cases with agile practices. my team is just getting started integrating agile practices (we've chosen kanban) into our dev, test and design teams, but we have a lot of bug cases and feature cases not written in user stories left over from the old system and we're trying to find a good way to take care of the bugs but still keep the devs that need to stay heads down focused. for now we have a newer dev assigned to bug fixes so the rest can work on new features but we are looking for the right way to structure the team. we have 7 devs, 2 testers and 2 designers.thanks in advance for the help :)",
    "present_kp": [
      "agile",
      "kanban"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "create better unit test. question: is there anything you could do in order to improve the test? i am using spring framework 4, hibernate 4, junit 4. all dao unit tests inherit from testdaosetup class:@contextconfiguration(locations={ classpath:/config/test/spring.xml, classpath:/config/test/hibernate.xml})@transactionconfiguration@transactionalpublic class testdaosetup {}one of unit tests:@runwith(springjunit4classrunner.class)public class citydaoimpltest extends testdaosetup { @rule public expectedexception exception = expectedexception.none(); @autowired private citydao citydao; @autowired private countrydao countrydao; @test public void create_created(){ countryentity countryentity = countrydao.find(1l); cityentity cityentity = new cityentity(); cityentity.setname(koice); cityentity.setcountryentity(countryentity); citydao.create(cityentity); assert.assertnotnull(expected not null value.,citydao.find(cityentity.getid())); } @test public void create_nocountryassociation_exceptionthrown(){ exception.expect(constraintviolationexception.class); cityentity cityentity = new cityentity(); cityentity.setname(koice); citydao.create(cityentity); citydao.flush(); } @test public void create_existingnamesamecountry_exceptionthrown() { exception.expect(constraintviolationexception.class); countryentity countryentity = countrydao.find(1l); cityentity cityentity = new cityentity(); cityentity.setname(bratislava); cityentity.setcountryentity(countryentity); citydao.create(cityentity); citydao.flush(); } @test public void create_existingnamedifferentcountry_created(){ countryentity countryentity = countrydao.find(2l); cityentity cityentity = new cityentity(); cityentity.setname(bratislava); cityentity.setcountryentity(countryentity); citydao.create(cityentity); assert.assertnotnull(expected not null value., citydao.find(cityentity.getid())); } @test public void update_existingnamesamecountry_exceptionthrown(){ exception.expect(constraintviolationexception.class); cityentity cityentity = citydao.find(2l); cityentity.setname(bratislava); citydao.update(cityentity); citydao.flush(); } @test public void update_existingnamedifferentcountry_updated(){ cityentity cityentity = citydao.find(3l); cityentity.setname(bratislava); citydao.update(cityentity); assert.assertequals(expected different value., bratislava, citydao.find(3l).getname()); } @test public void delete_notused_deleted() { cityentity cityentity = citydao.find(4l); citydao.delete(cityentity); assert.assertnull(expected null value., citydao.find(cityentity.getid())); } @test public void delete_used_exceptionthrown(){ exception.expect(constraintviolationexception.class); cityentity cityentity = citydao.find(1l); citydao.delete(cityentity); citydao.flush(); }}question: is there anything you could improve in the test? edit:to make test conditions more clear:there are separated configurations, one is specifically for testing purposes.there is database script which prepopulates the test database with test data before tests actually start.data are the same for each test, because there is a rollback after each test.test data are not prepopulated in each test class, but they are prepopulated in separate sql file, which is executed before tests actually start.",
    "present_kp": [
      "junit"
    ],
    "absent_kp": [
      "java",
      "unit testing"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "usb boot recently stopped working. not sure where to post this, so if i should move it somewhere else, please let me know.i recently got a new laptop, dell inspiron 7559 i7 64 bit cpu.i stripped out the dell stuff by reformatting the drive and installing a fresh copy of windows 10 i had available. windows works fine on the 1tb hybrid drive that came with the laptop (typing this on the windows side now). i also installed a second drive in the m.2 slot, a 275gb ssd.the next step in the plan was to install linux on the extra drive as a dual boot as i do plenty of work in linux as well. i used rufus to make a usb boot drive about a week or so ago. ubuntu 16.04 if it helps.i used the boot drive today and everything loaded fine, but i was having some trouble getting the partitions to work correctly and install linux, so i rebooted the computer. now, whenever i try to load from the usb stick it hangs on the ubuntu splash screen. it does this even after redownloading the iso of 16.04, and 14.04 trusty distributions using rufus on the usb boot drive.i hit esc to look at the boot loader, and it appears to get stuck in a cycle of soft-lockup cpu#1 lockup for 22s! just prior to that, it says it has an error and it can't access a memory region. i infer that it was trying to load the os into ram, and it hit a block of memory it couldn't access and hung. i have slight linux knowledge, but since i can't actually get into ubuntu to run any commands i don't know what to do at this point. i need linux and at this point i'm down to using a mint vm, which seems ridiculous given that i should be able to dual boot.thanks!",
    "present_kp": [
      "boot loader",
      "laptop"
    ],
    "absent_kp": [
      "live usb"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "can i read and write to alternate hfs+ file forks or ntfs data streams from linux?. it's possible to mount both apple hfs+ and microsoft ntfs filesystems under linux.both filesystems support multiple content streams per file, though this is not widely used.apple uses the term fork.microsoft uses the term alternate data stream.are there (semi) standard ways to access these filesystem features from linux? if so are the methods uniform between the two filesystems or are only arcane ad-hoc methods available? (perhaps something with ioctls?)",
    "present_kp": [
      "filesystems",
      "ntfs"
    ],
    "absent_kp": [
      "api"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "team work and agile development. i think this question related not only to agile but to teamwork in general. when we are working in a team and each member is working a user story to complete how to avoid creation of duplicate classes and conflict?i mean if my user story require the creation of class a and also my team member need the same class and created the same (he may create it with slightly different name), how would we plan so that we move smoothly?",
    "present_kp": [
      "agile",
      "teamwork"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "name of a distro. recently i've been trying to remember the name of the first distro i ever used, and no searching has turned up anything yet. i even found a distro 'family tree' that covered the period but i didn't see anything that rang any bells. i suspect it starts with a 'c' or a 'p' but i'm likely wrong, though i'm pretty sure it ended with an 'x'.the mascot/logo on the front was of a green circular character, dressed up like a roman centurion, and i believe the cd case cover, one of the double-width jewel case styles, was a dark red colour.this was a distro that came on a 2-3 cds somewhere around '96-'98... it was definitely pre 2.4, and possibly pre 2.2.i know it was release before beos r5 pro (released 2000).",
    "present_kp": [],
    "absent_kp": [
      "linux",
      "history",
      "distributions"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "can i use a list of blog ping services for a portal?. i'm setting up a list of ping services for a portal.it has a blog, forum, articles, restaurants, hotels and many other information, so it is far beyond a blog.i have a list of standard ping services for wp blogs - but i do not know if this should be literally only for blogs.my questions are:is it recommended to ping blog services from a portal, such as <url> there any penalties for sites that are not recognized as blogs?is there some list of ping services for regular websites and not only blogs?thanks!",
    "present_kp": [
      "ping"
    ],
    "absent_kp": [
      "seo"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "should i use module pattern for this notification controller?. i have the following code, i use this pattern for the entire javascript code. i'm trying to re-factor my code again. should i use the module pattern? revealing module pattern to be specific?what's happening is, somewhere in my code i say dnianas.notification.init().first i bind the events, and i will handle them one by one.is there a better way to do this?also i notice that i don't use the var keyword. because they depend on each other. dnianas.notification = { init: function () { this.bindevents(); this.cache(); }, bindevents: function () { $(document).on('click', '#opennotifii', this.markasread); }, cache: function () { $notification = $('#opennotifii'); }, countnotifications: function () { var $notifications = $('.boxnotificationsusers').children().find('#boxsendnotifi'); ids = []; // add unread notifications to the ids array. $notifications.each(function () { if ($(this).data('read') === 0) { ids.push($(this).data('id')); } }); return ids; }, markasread: function () { self = dnianas.notification; ids = self.countnotifications(); if (ids.length > 0) { var request = $.ajax({ url: '/notifications/read', data: { notifications: ids, _token: token, } }); request.done(function (data) { self.rendernotificationcount(data); }); } }, rendernotificationcount: function (data) { if (data.seen) { $notification.find('.not_nu1').fadeout(200); } }};",
    "present_kp": [
      "javascript",
      "revealing module pattern"
    ],
    "absent_kp": [
      "jquery"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "processing database data into a multidimensional array. the purpose of this code is to get data from the database and output it into a multidimensional array in json for my web app. but my gut instinct is telling me this code is way too messy and just generally badly written.// structure of user_medication <url> function getusermedication($data, $success, $fail){ permission::required(1, function() use($data, $success, $fail){ $user = $data[user]; if(!$this->isuseruidvalid($user)){ $fail([detail=>user does not exist!]); return; } $medicationdates = $this->database->rowquery(select * from (select *, id as date_id from user_medication_dates where user_uid = ? order by date desc limit 2) as umd left join user_medication as um on umd.date = um.date order by umd.date desc, array($data[user])); $processedmedications = []; foreach ($medicationdates as $key => $value) { if(!isset($processedmedications[$value[date_id]])){ $processedmedications[$value[date_id]][date] = $value[date]; } elseif(!isset($processedmedications[$value[date_id]][$value[time]])){ $processedmedications[$value[date_id]][$value[time]] = []; } $processedmedications[$value[date_id]][$value[time]][] = $value; } $success($processedmedications); });}/* the output in json;{ status:success, data:{ 96:{ date:2015-05-30, am:[ { id:90, date:2015-05-30, user_uid:553669eea746d, date_id:96, medication:test, dose:1, quantity:2, time:am } ], pm:[ { id:91, date:2015-05-30, user_uid:553669eea746d, date_id:96, medication:another, dose:1, quantity:2, time:pm } ] } }}*/",
    "present_kp": [],
    "absent_kp": [
      "php",
      "mysql"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "configurations needed to be done post gcc compilation and installation. i have compiled the latest gcc 4.7.0, on my linux mint 12, using gcc 4.6.1 (default).compiler compiled successfully, steps followed./configuremakemake installbinaries of gcc-4.7.0 are in /usr/local/bin/ after install step.now the goal is to make the new compiler the default while also keeping the old one around(no uninstall) i.e. i want to be able to do gcc a.c or g++ a.cpp directly in the terminal window. i have been unable to find a proper series of steps that would help me accomplish this. also, doing /usr/local/bin/g++ hello-world.cpp generates iostream file/directory not found.any help is appreciated.",
    "present_kp": [
      "linux mint",
      "gcc"
    ],
    "absent_kp": [
      "software installation"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "car controlled by steering wheel. i'm a young coder who is looking to improve in my coding practices because i was told by my supervisor to look into a more healthier method to code.what i would like to ask is how can i improve my coding structures? for example, is using a whole lot of bool to check for conditions considered acceptable? // other than commenting it.ps : i'm not going to present an updated copy of the code to my supervisor, i just want to improve myself for next time.pss : i study coding, but for coding structures, i was only taught the theory. there were no coding assignments.psss : this code here is for a car in unity that can drive back and front using a logitech steering wheel gt force.i don't really know where else to ask since i'm put to internship and i won't be meeting my lecturers for a long time.here is an example of a code i've recently worked on:using system.collections;using system.collections.generic;using unityengine;// debug : make sure when alternating between pedals, the momentum is still therepublic class vehiclemovements : monobehaviour { private rigidbody vehiclerigidbody; private transform vehicletransformer; public float momentum; private float turning; [serializefield] float movespeed; [serializefield] float reversespeed; [serializefield] float steeringwheel; [serializefield] float turningspeed; // 1 [serializefield] float pedal; [serializefield] float momentummultiplier; // 1.05 [serializefield] float startoffspeed; // 0.05 // can change to private when done testing [serializefield] bool ispedal; [serializefield] bool isbrake; [serializefield] bool hasbrakereset; [serializefield] bool stillbacking; // use this for initialization void start () { vehiclerigidbody = getcomponent<rigidbody>(); vehicletransformer = getcomponent<transform>(); reversespeed -= (reversespeed + reversespeed); } // update is called once per frame void fixedupdate () { steeringwheel = input.getaxis(steeringwheel); pedal = input.getaxis(pedal); if (pedal > 0.1) { hasbrakereset = true; isbrake = false; stillbacking = false; if (!ispedal) { momentum = startoffspeed; ispedal = true; } if (momentum <= pedal) momentum *= momentummultiplier; else momentum /= momentummultiplier; movevehicle(movespeed); } else if (pedal >= 0 && pedal <= 0.1) { if (momentum > pedal) momentum /= momentummultiplier; else if (momentum < 0.1) { momentum = 0; } ispedal = false; isbrake = false; if (!stillbacking) movevehicle(movespeed); else movevehicle(-movespeed); } else { stillbacking = true; ispedal = false; hasbrakereset = false; if (!isbrake) { isbrake = true; momentum = startoffspeed; } if (momentum <= -pedal) momentum *= momentummultiplier; else momentum /= momentummultiplier; movevehicle(reversespeed); } if(pedal >= 0 && hasbrakereset) { if (momentum != 0) { turning += ((steeringwheel * momentum) * turningspeed); vehicletransformer.rotate(transform.eulerangles = new vector3(0, turning, 0)); } } else { turning += -((steeringwheel * momentum) * turningspeed); vehicletransformer.rotate(transform.eulerangles = new vector3(0, turning, 0)); } } void movevehicle(float speed) { var v3 = transform.forward * (momentum * speed); v3.y = vehiclerigidbody.velocity.y; vehiclerigidbody.velocity = v3; }}",
    "present_kp": [],
    "absent_kp": [
      "c#",
      "unity3d"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "exogenous variables in time series models. i found that in the forecast package in r that i can easily incorporate an exogenous variable y in my arima model meant to forecast x. while i have a general understanding of the kind of process i need to go through in order to determine the right arima model on x, i'm not sure what i need to understand about y in order to incorporate it as an exogenous variable.i was wondering if anybody could help me understand the implications of having this variable in my model and whether there are any tests/checks i can perform to determine whether it's a good idea to have it in my model (other than my intuition and subject matter understanding). any reading material on this would be helpful. thanks!",
    "present_kp": [
      "r",
      "time series",
      "forecast"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "as a young student aspiring to have a career as a programmer, how should i feel about open source software?. every once in a while on some technology websites a headline like this will pop up:<url> initial thought about government and organizations moving to open source software is that tons of programmers would lose their jobs and the industry would shrink. at the same time the proliferation and use of open source software seems to be greatly encouraged in many programming communities. is my thinking that the full embrace of open source software everywhere will hurt the software industry a misconception? if it is not, then why do so many programmers love open source software?",
    "present_kp": [
      "open source"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "how can i see the whole file and also wait for more data to be added to that file?. i want to read whole file and make it waiting for input, just like tail -f but with the complete file displayed.the length of this file will always change, because this is a .log file. how can i do it, if i don't know length of the file?",
    "present_kp": [
      "tail"
    ],
    "absent_kp": [
      "command line"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "what's the difference between a flag, an option, and an argument?. ls -a (i consider -a an option)sudo -u username (-u = option, username = arg)chmod 664 my-dir (664 = option, my-dir = arg)i can't think of an example that might say this is a flag except perhaps when looking at dir listing:-r--------. 1 david david 3344 may 19 17:48 611056.pdfthis has the read flag set for the owner, but that's all. what's to stop me from calling that a read option?i write and edit technical documentation, primarily in docbook xml, and i'm looking for an explanation of the difference, which is consistent and accurate as possible. however, i'm already seeing a pattern forming:flags tend to be booleans. e.g., setenforce 0options help define how a command should behave. some may be optional.arguments tell commands what object to operate on.i could see myself combining flags and options (some options may have a dozen possible values, but booleans only have two). arguments appear sufficiently different to maintain them as such.",
    "present_kp": [
      "arguments",
      "options"
    ],
    "absent_kp": [
      "linux"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "embedding video in facebook without youtube. if i post a link to a youtube video on facebook, it will imbed the video on the page. what are some other sites besides youtube that will also embed video if i post a link on facebook?i don't want to upload directly to facebook because the number of views isn't counted.",
    "present_kp": [
      "facebook",
      "youtube",
      "video"
    ],
    "absent_kp": [
      "facebook integration",
      "video streaming"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "unique min-cut means unique max flow?. this is not a home work question. is that correct to say in a flow network, if min cut is unique then max flow must also be unique? assume we have two max flow in a network. then for each max flow, we can get a min cut and they are surely different.",
    "present_kp": [],
    "absent_kp": [
      "graph theory",
      "network flow"
    ],
    "domain": "stackexchange"
  },
  {
    "text": "how does someone find an email account that i delegated to them in gmail?. when i delegate my email account to someone else, i know they should be able to see it by clicking their profile photo in the upper right of their gmail account. but, the person i delegated to could not find my account.i made sure, that she really did except my delegation request. so the account should be there. is there another way to find a delegated account? is there a direct link to it?",
    "present_kp": [
      "gmail"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  },
  {
    "text": "static ip changes to dynamic automatically after time. i have found only one discussion about my problem here, but not a solution.i have access to one debian 7.8.0 amd64 virtual machine (under vmware) and its ip address is gained through dhcp, e.g., 192.168.0.90 (gateway 192.168.0.121).to obtain a static ip i configure /etc/network/interfaces like this:auto eth0iface eth0 inet static address 192.168.0.10 netmask 255.255.255.0 gateway 192.168.0.121then i run ifdown eth0 && ifup eth0 and everything works for hour or two (from ifconfig i see 192.168.0.10). after that time i see with ifconfig that my ip has changed automatically to 192.168.0.90 (the old one obtained from dhcp).why is this happening?i tried to execute /etc/init.d/networking restart, but it says that it is deprecated because it may not enable some interfaces... and my eth0 goes away and does not show up in ifconfig anymore (i need to run ifup eth0 to bring it up again).noone in the network is using x.x.x.10.",
    "present_kp": [
      "debian",
      "networking",
      "ip",
      "dhcp"
    ],
    "absent_kp": [],
    "domain": "stackexchange"
  }
]