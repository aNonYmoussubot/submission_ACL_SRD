[
    {
        "text": "create partition on centos server?. i have a headless centos server that i'd like to create a new partition on so i have a separate boot and data partition. i have an existing server with the setup that i want to achieve and running fdisk -l looks like this:disk /dev/cciss/c0d0: 299.9 gb, 299966445568 bytes255 heads, 63 sectors/track, 36468 cylindersunits = cylinders of 16065 * 512 = <phone> bytes device boot start end blocks id system/dev/cciss/c0d0p1 * 1 13 104391 83 linux/dev/cciss/c0d0p2 14 36468 292824787+ 8e linux lvmdisk /dev/cciss/c0d1: 899.8 gb, 899898718208 bytes255 heads, 63 sectors/track, 109406 cylindersunits = cylinders of 16065 * 512 = <phone> bytes device boot start end blocks id system/dev/cciss/c0d1p1 * 1 109406 878803663+ 83 linuxhere's the same command on my server:disk /dev/cciss/c0d0: 587.1 gb, 587128266752 bytes255 heads, 63 sectors/track, 71380 cylindersunits = cylinders of 16065 * 512 = <phone> bytes device boot start end blocks id system/dev/cciss/c0d0p1 * 1 13 104391 83 linux/dev/cciss/c0d0p2 14 71380 573255427+ 8e linux lvmi've been told it's not as straightforward as creating a partition and setting mount-points to it because of the additional logical volume layer on centos. how can i create a partition without causing problems to the data structure?",
        "present_kp": [
            "centos",
            "partition"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "centos server",
                "partition",
                "logical volume"
            ],
            "absent_kp": [
                "fdisk",
                "boot partition",
                "data partition"
            ]
        }
    },
    {
        "text": "governance models for multi-institution open source projects. i'm working on an open source project that has full time professional developers from several universities, plus a couple of other organisations. the product has something like a dozen deployments, various variations, plugins, related components etc. generally development so far has been driven by institutions scratching their own itch, but with an effort to merge improvements back to a central code base.as it's starting to mature, i'm interested in possible models of open source governance to follow. (so this question isn't what are some good things to do, it's specifically what existing, tested models are worth looking at and possibly following)specific aspects that such models might cover:how decisions about big-impact changes are made (and what happens if someone makes big changes without discussing them first)who manages the product's public image (product marketing, for want of a better term)who represents the product in any comparisons with competing productswhether enhancements become core, plugins, related products etcwhether and how roadmaps are created and publishedhow variations on the product are handled (in this case, versions for different academic disciplines)expectations and obligations of participants in the projectexpectations and obligations of the institutions for which those developers workwe'll be looking for something as lightweight and informal as practical.",
        "present_kp": [
            "open source"
        ],
        "absent_kp": [
            "management",
            "community",
            "distributed development"
        ],
        "predict": {
            "present_kp": [
                "open source project",
                "governance models",
                "multi-institution"
            ],
            "absent_kp": [
                "software development",
                "collaboration",
                "project management"
            ]
        }
    },
    {
        "text": "how to find the min of a column in every nth intervals of a file, using sed, sort, tail?. i want to find the minimum of the 5th column of a file in every 12th interval of that and save the associated line into a new file. to find the minimum of the last 12th line i can usetail -n 12 $filename | sort -g -k 5,5 | head -1| awk '{print}'> tmp.outbut how can i perform such a process iteratively?i have triedwhile read $filenamedo ....donewhich was not successful.the file looks like4.7 0.17 0.529 0 4.48464.7 0.17 0.529 1 4.54374744.7 0.17 0.529 2 5.632297394.7 0.17 0.529 3 4.<phone>.7 0.17 0.529 4 4.<phone>.7 0.17 0.529 5 4.<phone>.7 0.17 0.529 6 4.<phone>.7 0.17 0.529 7 3.699997174.7 0.17 0.529 8 4.69999984.7 0.17 0.529 9 2.74.7 0.17 0.59 10 3.99999984.7 0.17 0.59 11 4.699999999985 1 0.59 0 4.495698465 1 0.59 1 4.543305745 1 0.59 2 4.637396535 1 0.59 3 3.672339575 1 0.59 4 4.<phone> 1 0.59 5 4.<phone> 1 0.59 6 4.69993295 1 0.59 7 4.699999997175 1 0.59 8 4.699999985 1 0.59 9 3.25475 1 0.529 10 4.699999999985 1 0.529 11 4.69999999998with almost 2000 lines.",
        "present_kp": [
            "awk",
            "sed"
        ],
        "absent_kp": [
            "text processing"
        ],
        "predict": {
            "present_kp": [
                "sed",
                "sort",
                "tail"
            ],
            "absent_kp": [
                "awk",
                "file processing"
            ]
        }
    },
    {
        "text": "are copyleft and share-alike synonyms?. copyleft and share-alike are two very similar types of licences. here is how wikipedia defines them:copyleft (a play of the word copyright) is the practice of offering people the right to freely distribute copies and modified versions of a work with the stipulation that the same rights be preserved in derivative works down the line. (source)share-alike is a copyright licensing term, originally used by the creative commons project, to describe works or licences that require copies or adaptations of the work to be released under the same or similar licence as the original. copyleft licences are free content or free software licences with a share-alike condition. (source)are there any meaningful distinctions between these concepts, or is the only real difference their heritage and most common uses (software vs media)?",
        "present_kp": [
            "copyleft"
        ],
        "absent_kp": [
            "terminology"
        ],
        "predict": {
            "present_kp": [
                "copyleft",
                "share-alike"
            ],
            "absent_kp": [
                "copyright",
                "licensing"
            ]
        }
    },
    {
        "text": "creating a function when user clicks on element to toggle another element. i use a lot of when user clicks on element then another element will be displayed as visible/hidden... aka toggle. so i'm trying to make a general function to abide the dry principal.before jumping into the code, i just wanted to state that there are still bugs. i just want to know before continuing if writing a function like the one below is necessary or if i should just ignore the dry in this case and have lots of separate toggle functions. or if it's necessary, would there be a more efficient method? jsfiddle*brief description of code -there are 4 parametersthe first two are mandatory, the last two are optionalthe index parameter is a string of numbers ex. 1.0,1the reason i need the last two parameters is because i have elements that can't be easilytargeted. for example, they don't have an #id. so to grab those generic elements, i use getelementsbytagnamethen i'll use the index parameter to get the element in the array.index parameter: the number before '.' is the index of the clickelementand the number after '.' is the index of the toggleelement. clickelement = 1. toggleelement = 0. function toggleclick(clickelement, toggleelement, condition, index) { var toggleindex; var clickindex; if(condition === false){ if(index.indexof('.') != -1){ //indexof returns -1 if '.' is not found. // checks to see if two index is entered indexarray = index.split('.'); clickindex = indexarray[0]; toggleindex = indexarray[1]; clickelement = document.getelementsbytagname(clickelement)[clickindex]; toggleelement = document.getelementsbytagname(toggleelement)[toggleindex]; }else{ //if there is no '.', that means only one index was entered. // by function requirement, it should be the index of clickelement clickelement = document.getelementsbytagname(clickelement)[index]; } } $(clickelement).click(function(){ $(toggleelement).toggle(); }); } this is one of my first functions, and so i need some guidance on whether i'm writing bad code. yes, this might be broad because there may be many different ways to write this function. but i just need to know if there's a more efficient way or if my way is okay.",
        "present_kp": [],
        "absent_kp": [
            "javascript",
            "jquery",
            "event handling"
        ],
        "predict": {
            "present_kp": [
                "function",
                "element",
                "toggle"
            ],
            "absent_kp": [
                "javascript",
                "dry principle"
            ]
        }
    },
    {
        "text": "how can i find out when i subscribed to a youtube channel?. i am trying to find out when i subscribed to a certain youtube channel. i don't think i did. i have searched and cannot find any way. is there any way to learn when or how long i have been subscribed to a youtube channel?",
        "present_kp": [
            "youtube"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "youtube channel",
                "subscribed"
            ],
            "absent_kp": [
                "subscription date",
                "youtube history"
            ]
        }
    },
    {
        "text": "how do i create and automatically sync a merged directory from multiple other directories without duplicating files?. if i have some number of source directories, e.g. dir1, dir2... dir5, how can i create an automatically syncing merged destination directory that has all the files and directories of the source directories but doesn't involve duplicating files?for example, i have the following source directory structure...dir1/ - a - bdir2/ - b <-- note the duplicate name, this one is more recent than dir1 - c - dir2.1/ <-- subdirectories present too. - zdir3/ - d... which when merged would look like this:merge/ - dir2.1/ - z - a - b <-- which one to show is based on modified time; most recent first. - c - d(assume there could be dozens of directories with thousands of files and subdirectories.)all the files and directories in the source list should remain in place and unchanged and the merged destination directory should take up no addition storage; i.e. it's possibly just all symlinks managed by some daemon using inotify. the source directories are also having files and subdirectories added and removed frequently and this needs to be reflected, as soon as possible, in the merged directory too.some usage examples:i create a new file, dir3/e, and it automatically and immediately (or within a few seconds) appears in the merge directory.i remove the file dir1/a and it automatically and immediately (or within a few seconds) disappears from the merge directory.i edit dir3/d by opening merge/di call touch on dir1/b so it it has a newer modified date than dir2/b and so merge/b automatically updates to point to dir1/b as it is the most recent.i remove dir1/b and now merge/b will point to the older file dir2/b.i attempt to create a file in merge but get an error because doing so makes no sense!",
        "present_kp": [
            "directory"
        ],
        "absent_kp": [
            "linux",
            "debian"
        ],
        "predict": {
            "present_kp": [
                "merged directory",
                "source directories",
                "symlinks",
                "inotify"
            ],
            "absent_kp": [
                "file synchronization",
                "directory management",
                "daemon"
            ]
        }
    },
    {
        "text": "writing an effective method that reflects good oo design and fixes this broke method. i'm working on my personal project and have recently had a bit of trouble with a method i wrote during the week.this method compares user input from the same jcombobox. a before and after, if you will. it then makes an appropriate adjustment to a 2nd combobox depending on the user's input into the first box. it took me a bit of effort to figure this out. when i finally did, i was pretty happy with myself. until i realized that the method uses a class variable and ruins any concept of good object-oriented design. /**** missilebalancer compares the users input before and after selection and adjusts the contents of the oh missile count box as necessary.** @param jcombobox combobox is the combobox concerning the op counts, jcombobox combobox1 is the combobox concerning the oh counts, prevoh is the masteroh count.** @return** @throws**/public void missilebalancer(jcombobox combobox, jcombobox combobox1, int prevop){ int difference = 0; int newoh = 0; int newop = (int) combobox.getselecteditem(); //current op count int oldoh = (int) combobox1.getselecteditem();//current oh count --adjusting this by the appropriate amount is the goal of this method int oldop = prevop; //oldop is the op that we are testing against it needs to be set by the program into a var that we can use in our test. if(newop < oldop){ oldpac3 = newop;//we need to keep track of our count outside of the method. }else if(newop > oldop){ difference = newop - oldop; newoh = oldoh - difference; combobox1.setselecteditem(newoh); if(newoh < 0){//you cannot set selected item to a jcombobox with a negative number so in all cases where newoh is less than zero we simply set the selected item to 0. combobox1.setselecteditem(0); } oldpac3 = newop; }}//end missile balancerthe problem is with the user of the oldpac3 variable. the method cannot always use this variable. i have variables for oldgemt and oldgemc, that this method needs to modify as appropriate. i can think of a couple ways to make this work, but i don't think any of them are very clean. i want the code for this method to be concise and elegant. any help or some guidance in the right direction would be great.this is the rewritten method. it does what needs doing, but i think it is ugly. i'm posting it because i don't want to give the impression that i'm looking for other people to solve my problems for me. i really just want to write better code. /**** missilebalancer compares the users input before and after selection and adjusts the contents of the oh missile count box as necessary.** @param jcombobox combobox is the combobox concerning the op counts, jcombobox combobox1 is the combobox concerning the oh counts, prevoh is the masteroh count.** @return** @throws**/public void missilebalancer(jcombobox combobox, jcombobox combobox1, int prevop){ int difference = 0; int newoh = 0; int newop = (int) combobox.getselecteditem(); //current op count int oldoh = (int) combobox1.getselecteditem();//current oh count --adjusting this by the appropriate amount is the goal of this method int oldop = prevop; //oldop is the op that we are testing against it needs to be set by the program into a var that we can use in our test. if(newop < oldop){ if(combobox.equals(pac_3opcount)){ oldpac3 = newop;//we need to keep track of our count outside of the method. } else if(combobox.equals(gemcopcount)){ oldgemc = newop; } else if(combobox.equals(gemtopcount)){ oldgemt = newop; } }else if(newop > oldop){ difference = newop - oldop; newoh = oldoh - difference; combobox1.setselecteditem(newoh); if(newoh < 0){//you cannot set selected item to a jcombobox with a negative number so in all cases where newoh is less than zero we simply set the selected item to 0. combobox1.setselecteditem(0); } if(combobox.equals(pac_3opcount)){ oldpac3 = newop;//we need to keep track of our count outside of the method. } else if(combobox.equals(gemcopcount)){ oldgemc = newop; } else if(combobox.equals(gemtopcount)){ oldgemt = newop; } } }//end missile balancer",
        "present_kp": [],
        "absent_kp": [
            "java",
            "object oriented"
        ],
        "predict": {
            "present_kp": [
                "method",
                "object-oriented design",
                "jcombobox"
            ],
            "absent_kp": [
                "java",
                "user input",
                "code refactoring"
            ]
        }
    },
    {
        "text": "remote syslog command line client. i use logger pretty regularly, but is there a foss command line tool for remote submission of syslog messages over the network? right now, i have to configure a facility/priority pair to submit to a remote server on the rsyslogd side. i'd like to cut out the middleman and not have to modify my local syslog daemon's configuration. does such a tool exist?",
        "present_kp": [
            "syslog",
            "logger"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "remote syslog",
                "command line client",
                "logger",
                "syslog messages",
                "rsyslogd"
            ],
            "absent_kp": [
                "foss",
                "remote server",
                "syslog daemon"
            ]
        }
    },
    {
        "text": "feedburner does not get links right. feedburner doesn't get my feed links right. i tried resyncing and pinging and everything at my disposal but it doesn't seem to work. then i emailed my problem to <email> and i got no response. i just can't stand to see my feed getting wrong links.e.g: <url> is the link in the actual feed (<url>) while feedburner links go to <url>",
        "present_kp": [
            "feedburner"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "feedburner",
                "links"
            ],
            "absent_kp": [
                "feed management",
                "rss feed"
            ]
        }
    },
    {
        "text": "what was the first mechanical turing-complete machine ever constructed?. we know that charles babbage designed the first turing-complete mechanical machine - the analytical engine - in the 1800s, but it was never actually built (not yet anyway).in recent history, at least one mechanical turing machine has been built. (this example is in fact a universal turing machine, albeit with finite storage space.) but was this the first one, or are there earlier examples?what was the first mechanical turing-complete machine constructed, who built it and when?edit: by mechanical, i mean no electronics are used.",
        "present_kp": [
            "history"
        ],
        "absent_kp": [
            "computer architecture",
            "turing completeness"
        ],
        "predict": {
            "present_kp": [
                "mechanical turing-complete machine",
                "analytical engine",
                "universal turing machine"
            ],
            "absent_kp": [
                "charles babbage",
                "finite storage space",
                "electronics"
            ]
        }
    },
    {
        "text": "where could distributed version control systems currently be in gartner's hype cycle?. edit: given the recent downvoting (+8/-6 at this point) it was made clear to me that gartner's lifecycle is a biased metric from a programmer's perspective. this is something that is part of a paper i'm going to present to management, and management types are part of gartner's audience. giving dvcs exposure & enthusiasm (that could be deemed as hype, or at least attacked as such), think about the following question when reading this one: how could i use gartner's hype cycle to convince management that dvcss are ready (or ready-enough) for us, and that it is not just hypejust asking if dvcss is hype wouldn't be constructive, gartner's hype cycle is a more objective instrument than just asking that (even if this instrument is regarded as biased). if you know any other instrument please, by all means, mention it.edit #2: i agree that gartner's life cycle is not for every technology, but i consider it may have generated enough buzz to be considered hype by some, so it maybe deserves to be at least evaluated/pondered as such by using this instrument in order to prove/disprove it to whatever degree. i'm an advocate of dvcs, btw.edit #3: thanks for your answers. bounty goes to caleb for answering my question with detail and practical advise. accepted answer goes to philosodad for providing another useful instrument and answering beyond my question.i'm doing research for a whitepaper i'm writing in favor of dvcs adoption at company and i stumbled upon the concept of social proof. i want to prove that the social proof of dvcs adoption is not necessarily cargo cult and doing further research i now stumbled upon gartner's hype cycle which describes technology maturity in 5 phases.my question is: what could be an indicator of the current location of distributed version control systems (i mean git, mercurial, bazaar, etc. in general) at a particular phase in the hype cycle?... in other (less convoluted) words, would you say that currently expectations of dvcss are a) starting, b)inflated, c)decreasing (disillusionment), d)increasing (enlightenment) or e)stabilizing (mature) and (more importantly) why?i know it is a hard question and there is subjectivity involved, but i'll grant the answer (and the traditional cookie) to the clearest argument/evidence for a particular phase.",
        "present_kp": [
            "research",
            "dvcs"
        ],
        "absent_kp": [
            "cvcs"
        ],
        "predict": {
            "present_kp": [
                "distributed version control systems",
                "gartner's hype cycle",
                "social proof"
            ],
            "absent_kp": [
                "git",
                "mercurial",
                "bazaar",
                "technology maturity"
            ]
        }
    },
    {
        "text": "detailed description of malware content. i have just started my journey into the vast and intersting field of malware analysis. i would like to know if there is any website/book or another resource that explains what a particular block of assembly code does. a detailed description of the code would be well appreciated. i know assembly language to some extent and is familiar with all the concepts,function call procedure etc. but i have very little knowledge on how all these applies when it come to windows. like what happens when a dll is used,etc... i would be very glad if someone could tell me where to find a resource that provides a step by step detailed analysis of any malware or any program for that matter.i have already a considerable experience in reverse engineering windows applications, most of the knowledge been taken from the 'legend of r4ndom' and woodman websites. i need something that clearly explains how a particular assembly code interacts with windows dlls, resources like menu bars text boxes, etc..",
        "present_kp": [
            "malware"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "malware analysis",
                "assembly code",
                "reverse engineering",
                "windows dlls"
            ],
            "absent_kp": [
                "malware detection",
                "linux",
                "debugging tools"
            ]
        }
    },
    {
        "text": "kickstart: one network card with dhcp, one with static ip?. i am using kickstart to automate a centos install. i wanted one of the interfaces to have static ip so i wrote -network --onboot yes --bootproto static \\ -ip=intended ip -netmask=intended netmask \\ -gateway=intented gateway --device eth0*i also wanted the other ethernet to have dynamic ip so i also configured a dhcp server for my network and for the other interface card wrotenetwork --onboot yes --device eth1 --bootproto dhcpbut to my surprise the ethernet to be configured static was also provided with dynamic ip. what is wrong here?",
        "present_kp": [
            "centos",
            "ip",
            "kickstart"
        ],
        "absent_kp": [
            "networking"
        ],
        "predict": {
            "present_kp": [
                "kickstart",
                "network card",
                "dhcp",
                "static ip",
                "centos"
            ],
            "absent_kp": [
                "linux",
                "network configuration"
            ]
        }
    },
    {
        "text": "extract line number of a file which is having a non zero value before a specified string. i'm having a file which contains following data1. verification: 10 passed 0 failed2. verification: 10 passed 0 failed3. verification: 10 passed 1 failed4. verification: 10 passed 3 failed5. verification: 10 passed 0 failedi want to know the line numbers of 3 and 4.",
        "present_kp": [
            "string"
        ],
        "absent_kp": [
            "grep",
            "numbering"
        ],
        "predict": {
            "present_kp": [
                "line number",
                "file",
                "non zero value",
                "specified string"
            ],
            "absent_kp": [
                "data verification",
                "line extraction"
            ]
        }
    },
    {
        "text": "given a csl formula, how can we generate an automaton that accepts the formula?. the problem is same as the title, given a continous stochastic logic(csl) formula how can we create a machine which accepts the formula? any intuitive ideas or references will be appreciated.",
        "present_kp": [],
        "absent_kp": [
            "lo.logic",
            "model checking"
        ],
        "predict": {
            "present_kp": [
                "csl formula",
                "automaton"
            ],
            "absent_kp": [
                "continuous stochastic logic",
                "machine learning"
            ]
        }
    },
    {
        "text": "cohen-sutherland clipping. specify individually the translation and scaling matrices required to transform a 2dwindow of [xmin=-234, ymin=156] and [xmax=66, ymax=456] to a display viewportof [umin=45, vmin=35] and [umax=245, vmax=185].ignore the question above since i solved the matrix, the information is just relevant for the question i'm stuck oni was asked to compute the view-port positions (u1,v1) and (u2,v2) for two points a (-100,300)and b (30,-40) in a 2d window and determine if these two points are inside the view-port.based on the transformation matrix i found (u1,v1) to be (403/3, 407) and (u2,v2) to be (221, -103). it turns out that both these points are outside our view-port but part of the line between them is inside.now i'm confused about this part below:apply a 2d clipping method to the line segment between the two points a and b asgiven in above.my attemptdelta x = 221-(403/3) = 260/3delta y = -103 - 406 = -510m = delta y / delta x = -5.88i started with u1,v1 since it is above our viewport:y = 185x = 403/3 + (185-407)*(delta x/ delta y)x = 279.48point 1 - (172, 185) is this correct? since the point is now within the view-port. do i then do the same for the second point?",
        "present_kp": [
            "clipping"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "cohen-sutherland clipping",
                "2d clipping method",
                "view-port",
                "transformation matrix"
            ],
            "absent_kp": [
                "graphics",
                "line clipping",
                "viewport transformation"
            ]
        }
    },
    {
        "text": "do search engines follow the additionaltype url in microdata?. for example <div itemscope itemtype=<url> <meta itemprop=additionaltype content=<url> </div>if so, which vocabulary(?) is more descriptive?productontology.orgdbpedia.orgwikipedia.org ?",
        "present_kp": [
            "microdata"
        ],
        "absent_kp": [
            "seo",
            "html5",
            "schema.org"
        ],
        "predict": {
            "present_kp": [
                "search engines",
                "microdata",
                "additionaltype url"
            ],
            "absent_kp": [
                "schema.org",
                "structured data",
                "vocabulary"
            ]
        }
    },
    {
        "text": "command to skip process if file exists. i have been using a for loop to run a pipeline for multiple files but unfortunately the terminal froze halfway. i would like to run the pipeline again but because of time i would like to skip the directories that already has the output files created. basically nest a if statement - if file output file exists, ignore if not run pipeline. is this possible?for f in /volumes/my\\ passport/documents/projects/untitled\\ folder\\ 2/untitled\\ folder\\ 3/untitled\\ folder\\ 2/untitled\\ folder/*/*_1.fastq; dosubdir=${f%/*}pushd $subdir &>/dev/nullfile1=${f##*/}file2=${file1%_1.fastq}_2.fastqadapter=/volumes/my\\ passport/documents/adapters.fareference=/volumes/my\\ passport/documents/ucsc_hg19/ucsc.hg19.fastadbsnp=/volumes/my\\ passport/documents/ucsc_hg19/dbsnp_138.hg19cosmic=/volumes/my\\ passport/documents/ucsc_hg19/cosmiccodingmuts.vcfinterval=/volumes/my\\ passport/documents/plist.bedsjdb=/volumes/my\\ passport/documents/ucsc_hg19/ucsc.hg19.gtffile3=${file1%_1.fastq}_1_trimmed.fastqfile4=${file2%_2.fastq}_2_trimmed.fastq#preqc (cutadapt -o subtracted, prinseq -min_qual_score 4 -ns_max_p 2 subtracted)~/desktop/utsw/applications/bbmap/bbduk.sh -xmx120g in1=${file1} in2=${file2} out1=${file1%_1.fastq}_1_trimmed.fastq out2=${file2%_2.fastq}_2_trimmed.fastq ref=${adapter} trimq=10paste - - - - < ${file3} | sort -k1,1 -t | tr > ${file3%_1_trimmed.fastq}_trimmed_sorted_1.fastqpaste - - - - < ${file4} | sort -k1,1 -t | tr > ${file4%_2_trimmed.fastq}_trimmed_sorted_2.fastqparallel -j $parallel_tasks perl ~/utsw/applications/prinseq-lite-0.20.4/prinseq-lite.pl -fastq ${file3%_1_trimmed.fastq}_trimmed_sorted_1.fastq -fastq2 ${file4%_2_trimmed.fastq}_trimmed_sorted_2.fastq -no_qual_header -trim_right 1 -custom_params a 75%;t 75%;g 75%;c 75% min_qual_mean 25 -min_len 40 -out_format 3 -out_good ${f%.*}_qc -out_bad null -logdone",
        "present_kp": [],
        "absent_kp": [
            "bash",
            "scripting",
            "macintosh"
        ],
        "predict": {
            "present_kp": [
                "file exists",
                "pipeline",
                "if statement"
            ],
            "absent_kp": [
                "bash",
                "shell script",
                "loop"
            ]
        }
    },
    {
        "text": "numbering of computable functions. is there a numbering (not gdel numbering) of all computable functions $u(p, x)$, such that the set of numbers of functions defined in zero is exactly the set of even numbers. more formally: $i = \\{p,\\ |\\ u(p, 0)\\ \\mathrm{defined}\\} = 2\\mathbb{n}$.my guess that it's true. but i'm not sure how to prove it.ideas:we can construct a numbering of all computable functions, defined in zero using the function $f(p, x, t)$ which is equal to $0$ if $u(p, x)$ hasn't finished work in $t$ steps and $1$ in other case. we can do it because set of pairs $(p, t)$ is enumerable.then, having this function $v(p, x)$ and some other numbering $u(p, x)$ we can construct numbering$$u'(p, x) = egin{cases}v( rac{p}{2}, x)\\ \\ if\\ p dots 2\\u( rac{p + 1}{2}, x)\\ \\ if\\ p otdots 2\\end{cases}$$",
        "present_kp": [],
        "absent_kp": [
            "computability",
            "discrete mathematics"
        ],
        "predict": {
            "present_kp": [
                "computable functions",
                "numbering"
            ],
            "absent_kp": [
                "godel numbering",
                "even numbers"
            ]
        }
    },
    {
        "text": "what is copyleft?. straight up: i've never heard of the term 'copyleft' before. the fact that i get that little red squiggle under it tells me it's not really a word. concisely, my question is, what does it mean to you? i saw it tagged on a few questions so i read the tag description:for questions about the copyleft concept (also known as share-alike), a concept that promotes or enforces the use of the same or compatible license for derived works.and i read the wikipedia page on it. my understanding is that copyleft has attributes of share-alike but the author must disclose any other terms alongside it. to me, it seems like a bit of a useless term. why describe it as copyleft if you need to add a licence to it anyway? would you not be better off describing it as copyright with already-agreed-upon licenses? otherwise it's only another word for share-alike.",
        "present_kp": [
            "copyleft"
        ],
        "absent_kp": [
            "terminology"
        ],
        "predict": {
            "present_kp": [
                "copyleft"
            ],
            "absent_kp": [
                "copyright",
                "share-alike"
            ]
        }
    },
    {
        "text": "composed architecture. when refactoring and old class in php, i noticed that it contained a lot of business logic and thus was not allowing code reuse. so i thought about composition in order to achieve horizontal code reuse (since inheritance permits only vertical reuse) and the possibility to do dependency injection eventually. di gives then the possibility to use mock services in order to test the classes.basically, that class permits sending sms using the api of a third-party and we have hundreds of apis we need to deal with, so this is why the idea of horizontal code reuse would be useful.when we send an sms, we need to:send an http request to some url provided by the carrier with the right body contents (phone number, sms title, sms content and other information, depending on the carrier) and in the right format (xml, json, soap, depending on the carrier)log the sent sms*in the database (phone number, title, contents, delivery_status). the table where it is logged is used for tracking all sent smsses. also, we attach a delivery_status code (1 for delivered, -1 for not delivered) because some carriers send us then a delivery report (by requesting a script on our end) if the phone acknowledges the delivery.log the sent sms specifically for that carrier in another dedicated table that contains more fields than (phone_number, title, message, delivery_status). those fields could be used in the future for special purposes.i decided to separate the logic in mainly three classes:loggerservice: class that logs the sent sms informations in the database (also logs the carrier-specific sent sms informations)requestmaker: trait that simply makes http requests and returns the result without any further processingserializerservice: service that serializes the body of the request (converts from a dictionary of parameters to a specific format required by the carrier like xml or json, etc.). xmlbodyserializer is one implementation of that service. jsonbodyserializer is another.smssender: the core class that is using the requestmaker trait and the two other services in order to perform an api call to the specific carrier's api. if a carrier named mycarrier exists, we would have a mycarriersender class inheriting smssender. the main task of that class would be to build the right request body and to pass it to the serializer, then to the request maker which will call the api. finally, the class should fill the sent sms dto that will the be logged using the loggerservice currently in use.do you think it is a good idea to adopt this design? in other words, does it meet the goals of:decoupling business logiccode reusingpermitting testabilityhere is an implementation of the architecture:interface smssender { public function initservices( iloggerservice $resultloggerservice, // that logs information about sent sms in the databse ibodyserializer $serializerservice, // request body serializer ); public function sendtext( $msisdn, $title=, $text=, $options=array());}trait requestmaker { public function makeget($url, $params=array()); public function makepost($url, $params=array(), $body); public function buildurl($url, $params=array()); public function request($url, $method=null, $params=array(), $body=array());}interface iresultloggerservice { // logs the sms sent to the database public function logresult($sms2dto); // logs sent sms specific informations for that carrier public function loginmt($specificmtdto);}/*** request body serializer (xmlbodyserializer, jsonbodyserializer implements it) that serializes the body of the request*/interface ibodyserializer { public function serialize($data); public function parse($response);}/** * example of an sms sender for a specific carrier*/class mycarriersmssender implements smssender { use requestmaker; public function __construct() { $this->initservices( new mycarrierloggerservice(), new xmlbodyserializer() // will use xml for communication with the carrier's api ); } protected function presend($msisdn, $title, $text, $options) { /** * performs some verifications, prepares some variables * .... */ // example : $sentsms = new sms2dto(); $sentsms->setmsisdn($msisdn); // etc. $this->setsms2(); // set the sent sms dto } public function sendtext($msisdn, $title, $body, $options = array()) { $this->presend($msisdn, $title, $body, $options); $data = { // .... prepare request data } // serialize data using the injected service $raw_data = $this->getserializerservice()->serialize($data); // call request() from the requestmaker trait $raw_result = $this->request($url, array(), $raw_data, post); // deserialize api's response $result= $this->getserializerservice()->parse($raw_result); // interprets api's response and returns a success or fail status code $status = $this->interpretresult($result); $this->postsend($status); return $status; } protected function postsend($status) { if($status == success) { // log sent sms $this->getloggerservice()->logresult($this->getsms2dto()); // log specific sent sms by this carrier (contains more useful and specific info to that carrier) $this->getloggerservice()->loginmt($this->getmtdto()); } }}there are also dtos: sms2dto and {carrier}mtdto for every carrier.before that design, everything here was done in one class. there was a class for each carrier so a lot of code was repeated.",
        "present_kp": [
            "php"
        ],
        "absent_kp": [
            "design patterns"
        ],
        "predict": {
            "present_kp": [
                "copyleft"
            ],
            "absent_kp": [
                "copyright",
                "share-alike"
            ]
        }
    },
    {
        "text": "diff between a string and a file. basically i want to check the difference of the same file before and after a sedtried to run:diff /opt/posttrades.sh <<< $(sed 's/1\\ min/10\\ min/g' /opt/posttrades.sh)anddiff <<< $(sed 's/1\\ min/10\\ min/g' /opt/posttrades.sh) < /opt/posttrades.sh anddiff <<< (sed 's/1\\ min/10\\ min/g' /opt/posttrades.sh) < /opt/posttrades.shalways getting:diff: missing operand after '/opt/posttrades.sh'diff: try 'diff --help' for more information.what's the correct way to do it?thanks.",
        "present_kp": [
            "diff"
        ],
        "absent_kp": [
            "here document",
            "here string"
        ],
        "predict": {
            "present_kp": [
                "diff",
                "file"
            ],
            "absent_kp": [
                "string",
                "sed"
            ]
        }
    },
    {
        "text": "mobile ios application to server interaction design?. i've built a set of what will be server-side programs in python. essentially they crawl the net and index news articles.the ios mobile application will be what the user interacts with. so it needs to query the server somehow to get the results back. i.e. if the user types in 'google' or 'goog' it will return news articles concerning google.i'm assuming the server needs to be sent the query, the server produces the results by going over the hosted data and then sends it back, but how does it all work?i'm looking for a set of basic design principles for this sort of thing. although if you would like to give a breakdown that would be even better.",
        "present_kp": [
            "server",
            "mobile"
        ],
        "absent_kp": [
            "search engine"
        ],
        "predict": {
            "present_kp": [
                "mobile ios application",
                "server interaction design",
                "python",
                "news articles"
            ],
            "absent_kp": [
                "client-server architecture",
                "api design",
                "data querying"
            ]
        }
    },
    {
        "text": "can't get user $home over su on solaris and aix. i'm trying to get user $home variable over su. solaris# su oracle$ echo $home/rootaix# su oracle$ echo $home/linux# su oracle$ echo $home/home/oraclecan someone explain why on solaris and aix when i try to get $home variable it gets the root's $home directory?updateusing login with with - or -l works, but i can't use su - on my script. any thoughts on how to overcome this?i was trying to not use a solution like the above but i'm getting out of optionscat /etc/passwd | grep oracle | cut -d: -f 6or as thomas suggested:cat /etc/passwd | awk -f: '$1 ~ /^'oracle'$/ {print $6;}'",
        "present_kp": [
            "solaris",
            "aix",
            "su"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "solaris",
                "aix",
                "home variable"
            ],
            "absent_kp": [
                "linux",
                "script",
                "user directory"
            ]
        }
    },
    {
        "text": "how can i see in google analytics where my users came from?. i am using google analitics and struggling to find where my users came from. i can see demographics (country / city) information, but struggling to find what site have they came from.i found this answer:from your dashboard on the left side clicktraffic sourcesources all trafficbut can not see this on my left side. it hasdashboardsshortcutsintelligence eventsreal-timeaudienceacquisitionbehaviorconversionscan someone help me?",
        "present_kp": [
            "google analytics"
        ],
        "absent_kp": [
            "referrer"
        ],
        "predict": {
            "present_kp": [
                "google analytics",
                "users",
                "traffic source"
            ],
            "absent_kp": [
                "demographics",
                "dashboard",
                "acquisition"
            ]
        }
    },
    {
        "text": "merge two csvfiles by column header. i have two csv files:success.csvid,legacy id,field1, field21,1111,google,news2,2222,yahoo,newserror.csvlegacy id,field1,field2,message3333,aol,news,failed to upload data 4444,cbs,news,alredy existshow can i merge these two files and create a new file as shown below? i cannot use indexes as the size and order of fields will keep changing.results.csvid,legacy id,message1,1111,2,2222,,3333,failed to upload data,4444,alredy existsthe requirement is to create the results.csv file with only three columns from both success.csv and error.csv.if the row is successfully loaded then we get id in the success file as the first column with no message columnif it's failed we get an error in the message field which is always the last field in the file. in this case the id will be empty.read the values from success.csv as follows:awk '{print $1, $2;}' success.csvread the values from error.csv as follows:awk '{print $1, nf;}' error.csvi am not able to figure out a way to combine both the statements and write the result to a file.",
        "present_kp": [
            "awk",
            "csv"
        ],
        "absent_kp": [
            "command line",
            "text processing",
            "scripting"
        ],
        "predict": {
            "present_kp": [
                "csv",
                "merge"
            ],
            "absent_kp": [
                "python",
                "data processing"
            ]
        }
    },
    {
        "text": "what is volume in terms of databases?. i was reading a text. it said,information presentation must be compatible with the response time needs of systems. the response time should be short enough that the information does not lose its freshness and value but it should be long enough to reduce volume (and costs) and reveal important trends that signal the need for actionwhat does volume mean in it? please help out. thanks",
        "present_kp": [
            "database"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "volume",
                "databases"
            ],
            "absent_kp": [
                "response time",
                "information presentation"
            ]
        }
    },
    {
        "text": "printing problems with ddst (aka pcl6) printer ricoh aficio sp c240dn. i'm having big problems printing on a ricoh aficio sp c240dn (a color laser printer). cups/openprinting doesn't have a driver for exactly this printer. there are similar numbers but not 240dn. it also seems there is no ppd file for this printer, as it doesn't accept postscript at all. there are only so-called ddst and icm drivers for windows and mac, but no ppd as opposed to slightly older models from ricoh (320dn for example). the technician at the company where i bought the printer said that ddst is a stripped-down version of pcl6.the connection to the printer works, i can access the web interface for management, i can print test pages via buttons on the printer or via the web interface. if i try any other driver (320dn for example), the printer shows that it is receiving data, the spool on the computer is processing, and then it just thinks the job is finished and done but the printer does nothing.i'm planning to return it, but was wondering if somebody could explain a bit about drivers (what is ddst?), and if somebody knows if there is any chance that such driver for linux will be out in any time soon.i'm actually ready to donate a small amount to somebody who would hack an open-source driver (for cups).",
        "present_kp": [
            "printing",
            "cups",
            "postscript"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "ddst",
                "pcl6",
                "ricoh aficio sp c240dn",
                "printer",
                "cups",
                "openprinting",
                "driver"
            ],
            "absent_kp": [
                "linux",
                "open-source driver",
                "postscript",
                "ppd file"
            ]
        }
    },
    {
        "text": "is this code structure beneficial in any way?. i was recently thrown into a java web application project, and ive come across a number of classes that follow this type of format:public class mythingy { private final int p1; private final string p2; public mythingy (int p1, string p2, ) { this.p1 = p1; this.p2 = p2; } public static void dosomething(int p1, string p2, ) throws throwable { final mythingy mythingy = new mythingy(p1, p2, ); mythingy.execute(); } private void execute() throws throwable { //do stuff }}it seems like this could be accomplished with the following code, which to me seems way easier to read.public class mythingy { public static void dosomething (int p1, string p2, ) throws throwable { //do stuff }}the only possible benefit i can see from doing it the first way, is that if you had to break up execute() into smaller pieces, they could all share the initial parameters without having to explicitly pass them around. but this maybe only benefits the lazy coder, as it becomes difficult for the reader to tell which methods need which parameters and when the values might be changed (akin to global variables.)is there something i'm missing? threading, performance?edit:i should have mentioned, although the constructor is public, it is not called. the only usage is like this:mythingy.dosomething(p1, p2...);aside from this in itself being problematic for testing, i can't see any reason not to put the logic of execute() directly into dosomething(). even if we were to get rid of the static function, the constructor still doesn't make sense to me; i think the parameters should be passed directly to the method that will use them.",
        "present_kp": [
            "java"
        ],
        "absent_kp": [
            "coding style"
        ],
        "predict": {
            "present_kp": [
                "java",
                "web application",
                "code structure"
            ],
            "absent_kp": [
                "threading",
                "performance",
                "testing"
            ]
        }
    },
    {
        "text": "recommended method for shared data store writeable by all users. i have a few users sharing a computer and accessing content (pictures, audio, video recordings etc) that other users save. i need a shared drive for this content. file permissions, access control or overwriting is not an issue - just need a simple method to share files, in fact, without worrying about ownership and read-write privileges.i used to have a hard-drive partition that was formatted ntfs which so far served this purpose, but i am getting tired to having to run chkdsk every now and then so am looking for a solution that is more native to linux.unlike this question, this is not about the path of the directory, but about the way to implement the store - e.g., filesystem, share configuration etc.,.this setting is required only for this shared folder- otherwise, these users and their (other) data needs to be kept private like in any standard linux installation.",
        "present_kp": [
            "linux"
        ],
        "absent_kp": [
            "filesystems"
        ],
        "predict": {
            "present_kp": [
                "shared data store",
                "linux"
            ],
            "absent_kp": [
                "file permissions",
                "access control",
                "ntfs"
            ]
        }
    },
    {
        "text": "is there any specific reason to use else if clause?. we all use if ..else if.. else.but still i'm confused as to why we use else if. where if does the same thing as else if.so why are we using else if?any specific reasons behind this?is there any algorithm where it's mandatory to use else if?",
        "present_kp": [],
        "absent_kp": [
            "programming languages",
            "algorithms"
        ],
        "predict": {
            "present_kp": [
                "else if clause",
                "if",
                "else"
            ],
            "absent_kp": [
                "algorithm",
                "conditional statements"
            ]
        }
    },
    {
        "text": "validator class in php. i am trying to create a validation class that can be used to validate many forms of my webpage in general. it makes use of the database as well to verify records. the problem with it is that it looks bad and somewhat inefficient. i couldn't find any alternatives and i have used too many if-else if statements. and also dob makes use of an arbitrary variable. overall the code seems bad, i know. but i couldn't find an alternative. any help is extremely appreciated. i would also be happy if you could comment on the design and efficiency. i am willing to rewrite the whole code needed. this works, but it's all messed up. thanks! class validate { //take the user input into the data put it to a class. protected $data; //function argument private $err; // boolean protected $empty; private $error; private $clean; private $query; public function __construct() { $this->query= new dbhandler(); } private function basicsanitize($data) { //basic level input santization to be used by other functions only. $data=htmlspecialchars($data, ent_quotes); $data=trim($data); return $data; } public function sanitize($data) { // sanitization at a massive level to sanitize a lot of inputs at one go. foreach ($data as $k =>$v) { $data[$k]= $this->basicsanitize($v); } return $this->clean=$data; } public function showerror($type,$data) { return $this->error[$type.$data]; } private function basicemptycheck($data) { if($data==|| empty($data) || !isset($data) || $data==-1) { return true; } } public function isempty($data) { foreach ($data as $k =>$v) { if($this->basicemptycheck($v)){ $this->error['empty']=there is an empty field; return true; } } } private function checklength($data,$min,$max) { if(strlen($data)>=$min && strlen($data)<=$max) { return true; } } private function lengtherr($data,$min,$max) { if(strlen($data)>$max) { return too long; } else if (strlen($data)<$min) { return too short; } } public function name($data) { if(!$this->checklength($data,3,100)) { $this->error['name'.$data]=name .$this->lengtherr($data,3,100); } else if(!preg_match(/^[a-za-z ]+$/,$data)) { $this->error['name'.$data] = please don't include special characters in names; } else return true; } private function isuniqueemail($data) { $sql= select stuff1 from tablename where stuff2=?; if(!$this->query->checkifexists($sql, s, 'jells')) { return true; //the value set here jells is for testing purposes only. this will be replaced by $data } } public function email($data) { if (!$this->checklength($data,8,254)) { $this->error['email'.$data]= enter a valid email; } else if (!filter_var($data, filter_validate_email)) { $this->error['email'.$data] = enter a valid email; } else if (!$this->isuniqueemail($data)) { $this->error['email'.$data]= this email already exists; } else return true; } public function password($data) { if (!$this->checklength($data,8,128)) { $this->error['pass'.$data]='password '.$this->lengtherr($data, 8, 128); } else if (count(array_keys($this->clean,$data))>1) { $this->error['pass'.$data]='password is too obvious'; } else return true; } private function agecheck($month,$day,$year,$limit) { # object oriented $date=$year.-.$month.-.$day; $from = new datetime($date); $to = new datetime('today'); if($some=$from->diff($to)->y>=$limit) { return 1; } } public function age($data,$month, $day, $year,$limit) { if(!is_numeric($day) || !is_numeric($month) || !is_numeric($year)) { $this->error['age'.$data] = please enter a valid date; } else if(!checkdate($month, $day, $year)){ $this->error['age'.$data] = please enter a valid date; } else if(!$this->agecheck($month,$day, $year,$limit)) { $this->error['age'.$data] = you must have a minimum of .$limit. of age to sign up; } else return true; } public function sex($data) { if($data=='m' || $data=='f') { return true; } else { $this->error['sex'.$data]='add a valid sex'; } } public function errorexists() { if(isset($this->error)) { return true; } } }",
        "present_kp": [
            "php",
            "validation"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "validator class",
                "php",
                "validation",
                "database",
                "if-else statements",
                "design",
                "efficiency"
            ],
            "absent_kp": [
                "javascript",
                "form validation",
                "object-oriented programming",
                "code refactoring",
                "unit testing"
            ]
        }
    },
    {
        "text": "problems implementing the remez algorithm. so first off:*** this code is not being used in production software.it is a personal project of mine, trying to understand approximation theory and advanced curve fitting. in other words, i'm trying to understand how it works, not trying to get a currently existing solution.so i have been trying to implement the remez algorithm for polynomial approximation.i sort of/maybe/kind of have it working (not really).my current solution generate ok polynomials, but a) the coefficients are not converging&b) while monitoring the coefficients at each stage, i've noticed that the x-values seem to slip past each other.i'll give some examples to show what i mean.my base function i'm trying to model is the square root function with a 4 degree polynomial on the domain [0.25, 1]round 1x[0] = 0.25x[1] = 0.4x[2] = 0.55x[3] = 0.7x[4] = 0.85x[5] = 1round 2x[0] = 0.25x[1] = 0.595076928220583x[2] = 0.493988453622788x[3] = 0.714333640596557x[4] = 1.14135676154991x[5] = 1round 3x[0] = 0.25x[1] = 0.638393337463021x[2] = 0.63752199068821x[3] = 0.538600997945798x[4] = 1.07101841739164x[5] = 1round 4x[0] = 0.25x[1] = 0.559423143598625x[2] = 0.560673538304964x[3] = 0.580378820375143x[4] = 1.04454592077508x[5] = 1so here's a look at my actual code.template <typename func_t>type_t minimax(size_t degree, const type_t& lowerlimit, const type_t& upperlimit, unsigned char iterations, func_t f0, func_t f1, func_t f2){ // (c) jacob wells 2015 // this code is licensed under the bsd 3-clause license // <url> if((degree < 1) || (iterations < 1)) { return (type_t)nan; } const type_t one(1), neg1(-1), zero(0); matrix_t<type_t> m(degree + 2, degree + 3); vector<type_t> xval; type_t delta, sign, pow, err; type_t d1, d2; size_t i, j; delta = (upperlimit - lowerlimit) / (degree + 1); xval.resize(degree + 2); coef.resize(degree + 1); sign = one; for(i = 0; i < (degree + 2); i++) // generate our initial x-values { xval[i] = (i * delta) + lowerlimit; } do { sign = neg1; for(i = 0; i < (degree + 2); i++) { pow = one; m[i][degree + 1] = sign; // enters the alternating error sign m[i][degree + 2] = f0(xval[i]); // enters the f(x) value sign *= neg1; for(j = 0; j <= degree; j++) // evaluates the polynomial for each power { m[i][j] = pow; pow *= xval[i]; } } rref(degree + 2, m); // use row reduction echelon form to find the polynomial coefficients err = m[degree + 1][degree + 2]; for(i = 0; i <= degree; i++) // copy the coefficients into the polynomial class' array { coef[i] = m[i][degree + 2]; } if(iterations > 1) { for(i = 1; i <= degree; i++) // use newton's method to find our new x-values { d1 = nth_deriv(xval[i], 1) - f1(xval[i]); d2 = nth_deriv(xval[i], 2) - f2(xval[i]); if((d1 != zero) && (d2 != zero)) { xval[i] -= (d1 / d2); } } } }while(--iterations != 0); return err;} a quick little guide to some of my code: f0, f1, & f2 are, respectively, the square root functions, it's first derivative, and it's second derivative.nth_deriv calculate the nth derivative of the current polynomial.rref reduces the matrix to row reduction echelon formmatrix_t is a bare bones matrix class i came up with.now i have done a lot of testing on these functions, and i haven't found a single error with them, so i feel very confident that the problem is in the minimax function.edit: my barebones matrix classtemplate <typename type_t>class matrix_t{ public: ~matrix_t() {} matrix_t(size_t coll, size_t rowl) { arr.resize(coll * rowl); rowlen = rowl; } type_t* operator [] (size_t i) { return &arr[i * rowlen]; } const type_t* operator [] (size_t i) const { return &arr[i * rowlen]; } type_t at(size_t i) { return arr[i]; } private: size_t rowlen; vector<type_t> arr; matrix_t(); matrix_t(const matrix_t& m); void operator = (const matrix_t& m);};",
        "present_kp": [
            "polynomials",
            "curve fitting"
        ],
        "absent_kp": [
            "regression",
            "approximation algorithms"
        ],
        "predict": {
            "present_kp": [
                "remez algorithm",
                "polynomial approximation",
                "square root function"
            ],
            "absent_kp": [
                "curve fitting",
                "matrix class",
                "newton's method"
            ]
        }
    },
    {
        "text": "what is the state of art in geometric lod in games?. how do modern games do geometry level-of-detail for object meshes like characters, terrain, and foliage? there are two parts to my question:what does the asset pipeline look like? do artists make a high-poly model which is later decimated? if so, what decimation algorithms are most popular? are lod meshes sometimes done by hand?how do engines transition between different object lods at run time? are there any smooth or progressive transitions?the answer might be different studios use different techniques. if so, please identify some of the most common practices. it would also be great if you could point me to whitepapers/slides that cover specific examples.",
        "present_kp": [
            "geometry"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "geometric lod",
                "games",
                "geometry level-of-detail",
                "object meshes",
                "asset pipeline",
                "high-poly model",
                "decimation algorithms",
                "lod meshes",
                "engines",
                "smooth transitions",
                "progressive transitions"
            ],
            "absent_kp": [
                "characters",
                "terrain",
                "foliage",
                "studios",
                "whitepapers",
                "slides"
            ]
        }
    },
    {
        "text": "getting re error: repetition-operator operand invalid on osx sed. i'm copy a sed script form ubuntu debian to osx but gettingre error: repetition-operator operand invalidwhat is wrong?$ . sed_shorter_version_user_extensions_to_ruby.shsed: 22: ### delete whole lines ...: re error: repetition-operator operand invalidinspecting 1 file......the script is:(i left the line numbers in in case the 22 means line 22). 1 sed ' 2 ### delete whole lines 3 /\\/\\//d 4 /^$/d 5 ### change large chunks 6 s/^storedvars\\[/ def / 7 s/sad/sad/ 8 s/happy/happy/ 9 s/\\][[:space:]]*=[[:space:]]*/\\ 10 / 11 s/;/\\ 12 end\\ 13 / 14 ### change small chunks 15 s/css=// 16 s/link=// 17 s/label=// 18 ### change specific lines 19 ### scoped corrections for clarity 20 /def insurance_expiration/ { 21 /expiration_month/ { 22 s/value=.*\\+1)/(date.new + 1.month).strftime(%b)/ 23 } 24 /expiration_year/ { 25 s/value=.*fullyear())/(date.new + 1.month).strftime(%y)/ 26 } 27 } 28 ### unable to combine these for the %b and %y despite several tries mdd 9/13/2015 29 /date.*new.*month/ { 30 s///g 31 s/%b/%b/ 32 s/%y/%y/ 33 } 34 /choose_submodel_text/ { 35 s/ \\] =/ / 36 } 37 /email.*albert.*random/ { 38 s/(albert.*gmail\\.com)/faker::internet.email/ 39 } 40 ' variables/user-extensions.js | awk ' 41 ### add header and footer 42 begin { print # page object methods; print module pageobject # variable values } 43 { print } 44 end { print end } '> rspec_conversions/new_page_object_methods.rb 45 rubocop -a rspec_conversions/new_page_object_methods.rb",
        "present_kp": [
            "debian",
            "sed",
            "osx"
        ],
        "absent_kp": [
            "shell"
        ],
        "predict": {
            "present_kp": [
                "sed",
                "osx",
                "re error"
            ],
            "absent_kp": [
                "ubuntu",
                "debian",
                "script"
            ]
        }
    },
    {
        "text": "is meta name=verify-v1 essentially google webmaters verification meta?. if a website uses meta name=verify-v1 within the head, does this mean they are using google webmaster tools? and is it associated with any other services?",
        "present_kp": [],
        "absent_kp": [
            "google search console",
            "meta tags"
        ],
        "predict": {
            "present_kp": [
                "meta name=verify-v1",
                "google webmaster tools"
            ],
            "absent_kp": [
                "website verification",
                "meta tags"
            ]
        }
    },
    {
        "text": "asymptotic approximation of a recurrence relation (akra-bazzi doesn't seem to apply). suppose an algorithm has a runtime recurrence relation:$ t(n) = \\left\\{ egin{array}{lr} g(n)+t(n-1) + t(\\lfloor\\delta n floor ) & : n \\ge n_0\\ f(n) & : n < n_0 \\end{array} ight.$ for some constant $0 < \\delta < 1$. assume that $g$ is polynomial in $n$, perhaps quadratic. most likely, $f$ will be exponential in $n$.how would one go about analyzing the runtime ($\\theta$ would be excellent)? the master theorem and the more general akra-bazzi method do not seem to apply.",
        "present_kp": [
            "recurrence relation"
        ],
        "absent_kp": [
            "asymptotics"
        ],
        "predict": {
            "present_kp": [
                "recurrence relation",
                "asymptotic approximation"
            ],
            "absent_kp": [
                "algorithm analysis",
                "master theorem"
            ]
        }
    },
    {
        "text": "does verification that owner of domain undergo on google ever expire?. recently i've logged into my webmaster tools and noticed that i still got verifications on some of the sites that i do not own anymore, for like several years already. it suddenly brought multitude of questions to my mind, for example there were sites that i did together with some ex-partners, that i do not communicate with anymore, because of personal reasons. does this mean that they still own some rights to my sites from googles point of view? shouldn't verification auto expire at some point in time?",
        "present_kp": [
            "domain"
        ],
        "absent_kp": [
            "google apps"
        ],
        "predict": {
            "present_kp": [
                "verification",
                "google",
                "webmaster tools"
            ],
            "absent_kp": [
                "domain ownership",
                "expiration"
            ]
        }
    },
    {
        "text": "can cognito form work offline and be embedded in ipad app?. can cognito form work offline? i need a form that works both online and offline. when offline, user can still fill in the form. data is stored in the app and be sent to database once the ipad is connected to internet again.can cognito form be embedded in ipad app?",
        "present_kp": [],
        "absent_kp": [
            "cognito forms"
        ],
        "predict": {
            "present_kp": [
                "cognito form",
                "ipad app"
            ],
            "absent_kp": [
                "offline form",
                "data synchronization"
            ]
        }
    },
    {
        "text": "how to edit rpm's header and name-version-release. is there's a way or a tool that can modify the header and the name-version-release for an existing rpm without installing/rebuilding it?",
        "present_kp": [
            "rpm"
        ],
        "absent_kp": [
            "package management"
        ],
        "predict": {
            "present_kp": [
                "rpm",
                "header",
                "name-version-release"
            ],
            "absent_kp": [
                "package management",
                "linux"
            ]
        }
    },
    {
        "text": "why was my site not aproved for adsense?. our new site <url> has failed to get approval for adsense.even though site look reasonably better we cant figure out what is the real issue for disapproval can somebody help regarding this?email we got : we did not approve your application for the reasons listed below.issues:site does not comply with google policiesfurther detail:site does not comply with google policies: we're unable to approve your adsense application at this time because we feel that your site does not comply with google adsense policies or webmaster quality guidelines. it's our goal to provide our advertisers sites that offer rich and meaningful content, receive organic traffic, and allow us to serve well-targeted ads to users. we believe that currently your site does not fulfill this criteria.please help us find out the real factor which caused disapproval.",
        "present_kp": [
            "google adsense"
        ],
        "absent_kp": [
            "google search console"
        ],
        "predict": {
            "present_kp": [
                "adsense",
                "google policies"
            ],
            "absent_kp": [
                "website approval",
                "google adsense guidelines"
            ]
        }
    },
    {
        "text": "which principle is it to fetch only needed data?. it often makes sense to fetch only what you need for example if i should display only 10 rows of data then i should not fetch the entire data set because it would waste resources for a large data set. a practical example is the sql limit keyword. select * from users order by added limit 10i wonder if we can connect that to a software principle. there is the keep-it-simple principle but maybe it is a case of rule of the least power but for data instead of programs?",
        "present_kp": [],
        "absent_kp": [
            "principles",
            "workload estimation"
        ],
        "predict": {
            "present_kp": [
                "sql limit keyword",
                "fetch only needed data"
            ],
            "absent_kp": [
                "software principle",
                "keep-it-simple principle",
                "rule of the least power"
            ]
        }
    },
    {
        "text": "remote text mode terminal shell screen. i know it is possible to have remote vnc screen on server and connect to it and see, what is happening there.is it possible to have the same in text-mode shell?i would connect to remote machine with ssh, then connect to such remote screen and see, what my program does there?the general task is following:i have python script, that is scrapping the web. this script just prints what it does to stdout. currently i am running the script in ssh terminal. from time to time i am switching to ssh window and see, how my script feels: either it is still working, or it is crashed with error report.but in this situation, if i reboot my machine or network disconnect, my ssh session will quit and all containin programs will stop.how to avoid this? how to have something like text-mode vnc?",
        "present_kp": [
            "ssh"
        ],
        "absent_kp": [
            "background process"
        ],
        "predict": {
            "present_kp": [
                "remote screen",
                "text-mode shell",
                "ssh",
                "python script"
            ],
            "absent_kp": [
                "vnc",
                "web scraping",
                "terminal emulator"
            ]
        }
    },
    {
        "text": "pci passthrough device memory access incorrect. i have a wrlinux yocto system running on an intel haswell processor. here we pass through a pcie device (broadcom switch) to the vm, launched using qemu/kvm. the guest is able to detect the device via config space and is allocating memory space based on bar, but the device memory access seems incorrect. perhaps the guest physical address mapping went wrong? access doesn't fail, it returns invalid data. how to debug whether the dma remapping to guest is properly setup or not?",
        "present_kp": [
            "memory",
            "kvm",
            "pci passthrough"
        ],
        "absent_kp": [
            "virtual machine"
        ],
        "predict": {
            "present_kp": [
                "pci passthrough",
                "device memory access",
                "dma remapping"
            ],
            "absent_kp": [
                "intel haswell",
                "qemu",
                "kvm"
            ]
        }
    },
    {
        "text": "unable to use yum repos whether with baseurl or mirrorlist in centos 6.5. since a couple of days, i'm unable to retrieve repositories data with yum, in my centos 6.5 server.i did yum clean all a dozen of times, it emptied all but didn't solved the problem.i tried to retrieve things by restricting to the single base repo :[base]name=centos-$releasever - basemirrorlist=<url> --verbose update results in :loading fastestmirror pluginloading priorities pluginconfig time: 0.010yum version: 3.2.29setting up package sacksdetermining fastest mirrorscould not retrieve mirrorlist <url> error was14: pycurl error 22 - the requested url returned error: 403 forbiddenerror: cannot find a valid baseurl for repo: baseand when i configure a baseurl instead of a mirrorlist, i get this : loading fastestmirror pluginloading priorities pluginconfig time: 0.010yum version: 3.2.29setting up package sacksdetermining fastest mirrorshttp://mirror.centos.org/centos/6/os/x86_64/repodata/repomd.xml: [errno 14] pycurl error 22 - the requested url returned error: 404 not foundtrying other mirror.error: cannot retrieve repository metadata (repomd.xml) for repository: base. please verify its path and try againthe server pings mirror.centos.org and i can reach without any problem the mirrorlist and the repomd.xml in a web browser. disabling the plugins changed nothing.the mirrorlist gives a 403 error and a baseurl 404... rpm -q --verify -f /etc/yum.repos.d/* results in : s.5....t. c /etc/issue.......t. c /etc/yum.repos.d/centos-base.repo.......t. c /etc/yum.repos.d/centos-debuginfo.repo.......t. c /etc/yum.repos.d/centos-media.repo.......t. c /etc/yum.repos.d/centos-vault.repos.5....t. c /etc/issue.......t. c /etc/yum.repos.d/centos-base.repo.......t. c /etc/yum.repos.d/centos-debuginfo.repo.......t. c /etc/yum.repos.d/centos-media.repo.......t. c /etc/yum.repos.d/centos-vault.repos.5....t. c /etc/issue.......t. c /etc/yum.repos.d/centos-base.repo.......t. c /etc/yum.repos.d/centos-debuginfo.repo.......t. c /etc/yum.repos.d/centos-media.repo.......t. c /etc/yum.repos.d/centos-vault.repos.5....t. c /etc/issue.......t. c /etc/yum.repos.d/centos-base.repo.......t. c /etc/yum.repos.d/centos-debuginfo.repo.......t. c /etc/yum.repos.d/centos-media.repo.......t. c /etc/yum.repos.d/centos-vault.repo.......t. c /etc/yum.repos.d/epel-testing.repo.......t. c /etc/yum.repos.d/epel.repo.......t. c /etc/yum.repos.d/epel-testing.repo.......t. c /etc/yum.repos.d/epel.repole fichier /etc/yum.repos.d/isv:owncloud:community.repo n'appartient aucun paquetage.......t. c /etc/yum.repos.d/jpackage.repole fichier /etc/yum.repos.d/jpackage.repo.rpmsave n'appartient aucun paquetagen'appartient aucun paquetage means doesn't belong to any package, in french...those problems occur with whatever repo i enable/disable.",
        "present_kp": [
            "centos",
            "yum",
            "rpm"
        ],
        "absent_kp": [
            "linux"
        ],
        "predict": {
            "present_kp": [
                "yum",
                "centos",
                "repository"
            ],
            "absent_kp": [
                "linux",
                "package manager"
            ]
        }
    },
    {
        "text": "when is {a,b,c} expanded in bash, when is it not?. a bash script that containsfor i in {a,b}-{1,2}; do echo $i;doneprintsa-1a-2b-1b-2when executed. this is what i expected - as the {a,b} construct is expanded.however, when (another) script containsv={a,b}-{1,2}echo $vit prints{a,b}-{1,2}which is not what i expected. i expected it to print a-1 a-2 b-1 b-2. obviously, the {a,b} construct is not expanded.i can make it expand like sov=$(echo {a,b}-{1,2})based on these observations i have two questions: 1) when is the {a,b} construct expanded? 2) is $(echo {a,b}-{1,2}) the preferred way to trigger an expansion when required?",
        "present_kp": [
            "bash"
        ],
        "absent_kp": [
            "brace expansion"
        ],
        "predict": {
            "present_kp": [
                "bash",
                "expansion"
            ],
            "absent_kp": [
                "shell script",
                "command line"
            ]
        }
    },
    {
        "text": "how to make 'xargs' ignore child's exit and keep processing further. i sometimes run long xargs jobs overnight and it is really annoying to discover in the morning that xargs died somewhere in the middle, for example because of a segmentation fault in one single special case, as happened this night.if even one xargs child is killed, it does not process any more input:console 1:[09:35:48] % seq 40 | xargs -i --max-procs=4 bash -c 'sleep 10; date +%h:%m:%s {};'xargs: bash: terminated by signal 1509:35:58 309:35:58 409:35:58 2<exit with code 125>console 2:[09:35:54] kill 5601can i somehow prevent xargs from stopping to process any more input once a child process dies and instead continue processing?",
        "present_kp": [
            "kill",
            "xargs"
        ],
        "absent_kp": [
            "signals"
        ],
        "predict": {
            "present_kp": [
                "xargs"
            ],
            "absent_kp": [
                "bash",
                "process management"
            ]
        }
    },
    {
        "text": "change library location. i've got different versions of the libnet library installed in different locations on the same system:whereis libnetlibnet: /usr/lib/libnet.la /usr/lib/libnet.a /usr/lib/libnet.so /usr/local/lib/libnet.la /usr/local/lib/libnet.a /usr/local/lib/libnet.so /usr/include/libnet.h /usr/include/libnet /usr/man/man3/libnet.3i have some problems with compiling a program that depends on these libnet libraries so i want to remove the usr/local/....-ones. can you tell me how to do that, i.e, when i whereis libnet on the command line i want to not see the /usr/local/... references ?!thank you!",
        "present_kp": [],
        "absent_kp": [
            "linux"
        ],
        "predict": {
            "present_kp": [
                "libnet",
                "library"
            ],
            "absent_kp": [
                "compiling",
                "command line"
            ]
        }
    },
    {
        "text": "wikipath stack in java - part ii/iv - the implicit wikipedia article graph. this question is the continuation of the wikipath stack series: the two classes that - given a wikipedia article \\$a\\$ - return the lists of neighbour articles. the forward node expander return the list of articles to which \\$a\\$ links, and the backward node expander return the list of articles that link to \\$a\\$. since both are available, we can perform a bidirectional search for a shortest path. also, note that whenever given two terminal nodes (the source article \\$s\\$ and the target article \\$t\\$), and both the expander classes, we can construct implicitly the wikipedia article graph. by implicit we mean that only needed nodes (articles) are actually generated.as an overview of the entire software stack, i repost the diagram:the part in this question is the wikipediagraphnodeexpanders.below is my code:abstractwikipediagraphnodeexpander.java:package net.coderodde.wikipedia.graph.expansion;import com.google.gson.jsonarray;import com.google.gson.jsonobject;import com.google.gson.jsonparser;import java.io.ioexception;import java.io.unsupportedencodingexception;import java.net.url;import java.net.urlencoder;import java.nio.charset.charset;import java.util.arraylist;import java.util.hashmap;import java.util.list;import java.util.map;import java.util.objects;import java.util.regex.pattern;import net.coderodde.graph.pathfinding.uniform.delayed.abstractnodeexpander;import org.apache.commons.io.ioutils;/** * this abstract class specifies the facilities shared by both forward and * backward node expanders. * * @author rodion rodde efremov * @version 1.6 (aug 6, 2016) */public abstract class abstractwikipediagraphnodeexpanderextends abstractnodeexpander<string> { protected static final map<character, string> encoding_map = new hashmap<>(); static { encoding_map.put(' ', _); encoding_map.put('', %22); encoding_map.put(';', %3b); encoding_map.put('<', %3c); encoding_map.put('>', %3e); encoding_map.put('?', %3f); encoding_map.put('[', %5b); encoding_map.put(']', %5d); encoding_map.put('{', %7b); encoding_map.put('|', %7c); encoding_map.put('}', %7d); encoding_map.put('?', %3f); } /** * the script url template for expanding forward. */ private static final string forward_request_api_url_suffix = ?action=query + &titles=%s + &prop=links + &pllimit=max + &format=json; /** * the script url template for expanding backwards. */ private static final string backward_request_api_url_suffix = ?action=query + &list=backlinks + &bltitle=%s + &bllimit=max + &format=json; /** * the pattern for wikipedia urls. */ private static final pattern wikipedia_url_pattern = pattern.compile(^(https://|http://)?..+\\.wikipedia.org/wiki/.+$); /** * the https protocol prefix. */ private static final string secure_http_protocol_prefix = https://; /** * the http protocol prefix. */ private static final string http_protocol_prefix = http://; /** * the <tt>wiki</tt> directory token. */ private static final string wiki_dir_token = /wiki/; /** * the api script path. */ private static final string api_script_name = /w/api.php; /** * caches the basic wikipedia article url. for example, the basic url of * <tt><url> is * <tt>en.wikipedia.org</tt>. */ protected final string basicurl; /** * caches the textual representation of the url pointing to the * <a href=<url> api</a>. */ private final string apiurl; /** * constructs a graph node expander for the language subgraph specified in * the input url. * * @param wikipediaurl the entire wikipedia article url. */ protected abstractwikipediagraphnodeexpander(string wikipediaurl) { final string originalwikipediaurl = wikipediaurl; objects.requirenonnull(wikipediaurl, the input wikipedia article is null.); if (!wikipedia_url_pattern.matcher(wikipediaurl).matches()) { throw new illegalargumentexception( [input error] the input url is not a valid wikipedia + article url: \\ + originalwikipediaurl + \\.); } wikipediaurl = removeprotocolprefix(wikipediaurl); if (wikipediaurl.contains(://)) { throw new illegalargumentexception( [input error] the input url specifies unknown protocol: + \\ + originalwikipediaurl + \\.); } this.apiurl = constructapiurl(wikipediaurl); this.basicurl = wikipediaurl. substring(0, wikipediaurl.indexof(wiki_dir_token)); } /** * returns the raw wikipedia url. for example, for * <tt><url> this method will * return <tt>en.wikipedia.org</tt>. this is used for making sure that a * particular language (<tt>en</tt> in the example above) is selected. * * @return the basic wikipedia url. */ public string getbasicurl() { return basicurl; } /** * {@inheritdoc } */ @override public boolean isvalidnode(final string node) { return !expand(node).isempty(); } /** * constructs a wikipedia api url from the raw {@code wikipediaurl}. the * input {@code wikipediaurl} is of the form <tt>en.wikipedia.org</tt>. the * idea here is that the search may be applied to article subgraphs with * different languages. * * @param wikipediaurl the wikipedia url. * @return full url to wikipedia api. */ private string constructapiurl(final string wikipediaurl) { return secure_http_protocol_prefix + wikipediaurl.substring(0, wikipediaurl.indexof(wiki_dir_token)) + api_script_name; } /** * if the input string {@code url} has a prefix http:// or https://, * removes it from the url and returns the url. * * @param url the url to process. * @return the url without the protocol selector. */ private string removeprotocolprefix(final string url) { if (url.startswith(secure_http_protocol_prefix)) { return url.substring(secure_http_protocol_prefix.length()); } if (url.startswith(http_protocol_prefix)) { return url.substring(http_protocol_prefix.length()); } return url; } /** * the actual implementation of the method producing the neighbors of a * graph node. * * @param node the node to expand. * @param forward specifies the direction of the node expansion operation. * if {@code forward} is {@code true}, generates the child * nodes of {@code node}. otherwise, generates the parent * nodes of {@code node}. * @return */ protected list<string> basegetneighbors(final string node, final boolean forward) { string jsondataurl; try { jsondataurl = apiurl + string.format(forward ? forward_request_api_url_suffix : backward_request_api_url_suffix, urlencoder.encode(node, utf-8)); } catch (final unsupportedencodingexception ex) { throw new illegalstateexception(ex.getmessage(), ex); } string jsontext; try { jsontext = ioutils.tostring(new url(jsondataurl), charset.forname(utf-8)); } catch (final ioexception ex) { throw new illegalstateexception( [i/o error] failed loading the json data from the + wikipedia api: + ex.getmessage(), ex); } return forward ? extractforwardlinktitles(jsontext) : extractbackwardlinktitles(jsontext); } /** * returns all the wikipedia article titles that the current article links * to. * * @param jsontext the data in json format. * @return a list of wikipedia article titles parsed from {@code jsontext}. */ private static list<string> extractforwardlinktitles(string jsontext) { list<string> linknamelist = new arraylist<>(); jsonarray linknamearray; try { jsonobject root = new jsonparser().parse(jsontext).getasjsonobject(); jsonobject queryobject = root.get(query).getasjsonobject(); jsonobject pagesobject = queryobject.get(pages).getasjsonobject(); jsonobject mainobject = pagesobject.entryset() .iterator() .next() .getvalue() .getasjsonobject(); linknamearray = mainobject.get(links).getasjsonarray(); } catch (nullpointerexception ex) { return linknamelist; } linknamearray.foreach((element) -> { int namespace = element.getasjsonobject().get(ns).getasint(); if (namespace == 0) { string title = element.getasjsonobject() .get(title) .getasstring(); linknamelist.add(encodewikipediastyle(title)); } }); return linknamelist; } /** * returns all the wikipedia article titles that link to the current * article. * * @param jsontext the data in json format. * @return a list of wikipedia article titles parsed from {@code jsontext}. */ private static list<string> extractbackwardlinktitles(string jsontext) { list<string> linknamelist = new arraylist<>(); jsonarray backlinkarray; try { jsonobject root = new jsonparser().parse(jsontext).getasjsonobject(); jsonobject queryobject = root.get(query).getasjsonobject(); backlinkarray = queryobject.get(backlinks).getasjsonarray(); } catch (nullpointerexception ex) { return linknamelist; } backlinkarray.foreach((element) -> { int namespace = element.getasjsonobject() .get(ns) .getasint(); if (namespace == 0) { string title = element.getasjsonobject() .get(title) .getasstring(); linknamelist.add(encodewikipediastyle(title)); } }); return linknamelist; } /** * encodes some special characters using percent encoding. * * @param s the string to encode. * @return the encoded version of {@code s}. */ private static string encodewikipediastyle(final string s) { final stringbuilder sb = new stringbuilder(); for (final char c : s.tochararray()) { string encoder = encoding_map.get(c); if (encoder != null) { sb.append(encoder); } else { sb.append(c); } } return sb.tostring(); }}forwardwikipediagraphnodeexpander.java:package net.coderodde.wikipedia.graph.expansion;import java.util.list;/** * this class implements a forward node expander in the wikipedia article graph. * if article <tt>a</tt> has a link to <tt>b</tt>, this expander will generate * <tt>b</tt> whenever asked to process <tt>a</tt>. we can say that this * expander traverses each directed arc from tail to head. * * @author rodion rodde efremov * @version 1.6 (aug 6, 2016) */public class forwardwikipediagraphnodeexpander extends abstractwikipediagraphnodeexpander { public forwardwikipediagraphnodeexpander(final string wikipediaurl) { super(wikipediaurl); } @override public list<string> expand(string node) { return basegetneighbors(node, true); }}backwardwikipediagraphnodeexpander.java:package net.coderodde.wikipedia.graph.expansion;import java.util.list;/** * this class implements a backward node expander in the wikipedia article * graph. if article <tt>a</tt> has a link to <tt>b</tt>, this expander will generate * <tt>a</tt> whenever asked to process <tt>b</tt>. we can say that this * expander traverses each directed arc from head to tail. * * @author rodion rodde efremov * @version 1.6 (aug 6, 2016) */public class backwardwikipediagraphnodeexpander extends abstractwikipediagraphnodeexpander { public backwardwikipediagraphnodeexpander(final string wikipediaurl) { super(wikipediaurl); } @override public list<string> expand(final string node) { return basegetneighbors(node, false); }}demonstrationyou can see the expanders in action at <url> requesti want to improve anything there is to improve, yet i don't see myself any opportunity for that, so tell my anything that comes to mind.component list so farwikipediagraphnodeexpandersdelayedgraphsearchlibrary",
        "present_kp": [
            "java",
            "json",
            "graph",
            "pathfinding"
        ],
        "absent_kp": [
            "web scraping"
        ],
        "predict": {
            "present_kp": [
                "java",
                "wikipedia",
                "graph",
                "node expander",
                "bidirectional search"
            ],
            "absent_kp": [
                "pathfinding",
                "json",
                "api",
                "encoding",
                "url"
            ]
        }
    },
    {
        "text": "find all combinations of a number sequence which is first increasing then decreasing. given an integer n(natural number), a program/algorithm to find the remainder of arrangements that can be obtained by rearranging the numbers 1, 2, ...., n. input format: one line containing the integer n output format: an integer m, giving the remainder of the number of arrangements that could be obtained from 1, 2, ...., n is divide by mod constraints:mod = 10^9+7n 10^9 example 1 input3 output2 explanation: consider the first three natural numbers 1, 2, 3. these can be arranged in the following ways: 2, 3, 1 and 1, 3, 2. in both of these arrangements, the numbers increase to a certain point and then decrease. there are two such arrangements: 2, 3, 1 and 1, 3, 2. example 2input4 output6 explanation: the six arrangements are (1, 2, 4, 3), (1,3,4,2), (1,4,3,2), (2,3,4,1), (2,4,3,1), (3,4,2,1).#include<stdio.h>#include<stdlib.h>#define m 1000000007unsigned long long int power(unsigned long long int x, unsigned long long int n){ unsigned long long int res = 1; while(n > 0){ if(n & 1){ res = res * x; res = res%m; } x = x * x; x= x%m; n >>= 1; } return res;}int main(){ unsigned long long int n,res=0,temp=1,i; scanf(%llu, &n); if(n==1 || n==0){ printf(0 ); return 0; } temp = power(2, n-1); temp--; temp--; printf(%llu , temp); return 0;}can anyone solve this with better time complexity?",
        "present_kp": [
            "algorithm",
            "c"
        ],
        "absent_kp": [
            "performance",
            "combinatorics"
        ],
        "predict": {
            "present_kp": [
                "number sequence",
                "increasing then decreasing",
                "arrangements",
                "natural number",
                "remainder",
                "mod"
            ],
            "absent_kp": [
                "algorithm",
                "time complexity",
                "c programming"
            ]
        }
    },
    {
        "text": "why would a c executable be smaller when compared to c++ executable. i'm trying to understand why the output file sizes are significantly different when using a c and a c++ compiler.i was writing a small hello world program in c and c++, i noticed that in c version, the size of the executable was 93.7kb and in c++, the size of the same hello world program was 1.33mb. i am not sure why that is. i think it may be because c++ has more libraries and namespaces to use so i removed the using namespace std line and simply used std::cout and still the result was the same. c#include <stdio.h>int main(){ printf(hello world); return 0;}// size 93.7kbc++#include <iostream>int main(){ std::cout<<hello world; return 0;}// size 1.33mbthere doesn't seem to be much difference in the code above. is there some sort of compiler difference that creates the differing file sizes?",
        "present_kp": [
            "c++",
            "c"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "c",
                "c++",
                "executable"
            ],
            "absent_kp": [
                "compiler",
                "file size"
            ]
        }
    },
    {
        "text": "what is the relationship between the number of states in quantum finite automata and the number of non-regular languages they can recognize?. it is has been shown that quantum finite automata can recognize at least some non-regular languages. what is the relationship between the number of states in a qfa and the number of non-regular languages it can recognize? is there a relationship at all or has it not yet been established?i have been unable to find a paper that addresses this in an overtly and understandable way. more specifically, for any one specific type (ex/ 1-way mm-qfa), has any such relationship been established for the class it recognizes? i know different types of qfas have different closure properties and other characteristics that can make them vastly different and bounds have been established for some, but i'm wondering if anyone knows of, has seen, or knows who to contact regarding the relationship between teh number of states in a qfa and its recognition properties? or, if this question is totally mute and i'm looking at it wrong, tell me why. i have read into some of freivalds' research where he touches on this, but can't find any theorems, proofs, lemmas, ect. that hammer it out, if anyone has done so at all. there is a possibility that this question is open and that's why i havent found an answer. any help you can give would be appreciated.",
        "present_kp": [],
        "absent_kp": [
            "fl.formal languages",
            "automata theory",
            "probabilistic automata"
        ],
        "predict": {
            "present_kp": [
                "quantum finite automata",
                "non-regular languages",
                "number of states"
            ],
            "absent_kp": [
                "regular languages",
                "finite automata",
                "closure properties"
            ]
        }
    },
    {
        "text": "updating freebsd 8.0 to 8.1 (methods and policy). i have 8.0-release-p4 + few ports installed. i wonder wheather i should update to 8.1.how long is 8.0 supported?how to update the system? i couldn't find anything about it in handbook.solution (based on gvkv answer): i take the liberty of describing all steps i've done at the end: # step 1: revert to generic kernelcd /tmp wget -r <url> pub/freebsd/releases/i386/8.0-release/kernels/sha256 generic.* install.sh | diff - checksum.sha256./install.sh genericnextboot -k generic# step 2: upgrade - part 1freebsd-update upgrade -r 8.1-release # ignore kernel warning. fix configurationfreebsd-update installshutdown -r now# step 3: upgrade - part 2nextboot -k genericfreebsd-update installshutdown -r now# step 4: upgrade - part 3rm -rfv /usr/objportmaster -raf # rebuilds all packages. if you don't use portmaster use other tool or do it manuallycd /usr/srcmake buildkernel kernconf=custom # rebuild kernelmake installkernel kernconf=custom # install kernelshutdown -r now",
        "present_kp": [
            "freebsd",
            "upgrade"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "freebsd",
                "update"
            ],
            "absent_kp": [
                "linux",
                "upgrade process"
            ]
        }
    },
    {
        "text": "is collection.stream().filter().foreach() inefficient compared to a standard for each loop?. intellij idea recommended to me just now to replace the following for-each loop with a java 8 foreach call: for (object o : objects) { if (o instanceof someobject) { dosomething(); } }the recommended call would like like this:objects.stream().filter(o -> o instanceof someobject).foreach(o -> dosomething());unless i'm misunderstanding how the underlying functionality of stream works, it seems to me like using stream is an o(2n) operation as opposed to an o(n) operation for the standard for-each loop.",
        "present_kp": [
            "java"
        ],
        "absent_kp": [
            "performance",
            "java8"
        ],
        "predict": {
            "present_kp": [
                "stream",
                "foreach",
                "filter"
            ],
            "absent_kp": [
                "java",
                "loop"
            ]
        }
    },
    {
        "text": "computing the circumference of two circles. this is a program which computes the circumference of two circles and gives some information such as the difference between circumferences. i want to know whether the method declarations and method calling in my code is correct. i didn't face any errors.import java.text.*;import javax.swing.*;class q5main{ public static void main(string args[]){ double smallcircum; double largecircum; inputhandler input = new inputhandler(); question5 myq = new question5(); double a = input.getdouble(radius for the smaller circle:); smallcircum = (myq.start(a));//calculate the small circle circumference a = input.getdouble(radius for the larger circle: ); largecircum = (myq.start(a));//calculate the large circle circumference myq.showdetails(smallcircum, largecircum); }}class question5{ decimalformat df = new decimalformat(0.000); private double circumference;//to store circumference //carry out the full process public double start(double value){ return circum(value); } //compute the circumference private double circum(double radius){ circumference = 2 * math.pi * radius; return circumference; } public void showdetails(double smallcircum, double largecircum){ system.out.println(circumference of smaller circle: +df.format(smallcircum)); system.out.println( ); system.out.println(circumference of larger circle: +df.format(largecircum)); system.out.println( ); system.out.println(difference: +df.format((largecircum-smallcircum))); }}class inputhandler{ private static final string double_default_prompt = enter integer: ; public double getdouble(){ return getdouble(double_default_prompt); } public static double getdouble(string prompt){ string instr; instr = joptionpane.showinputdialog(null,prompt); return double.parsedouble(instr); }}",
        "present_kp": [
            "java"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "circumference",
                "circle"
            ],
            "absent_kp": [
                "java",
                "programming"
            ]
        }
    },
    {
        "text": "what other personality traits are proved to correlate with self-discipline?. are there any studies that prove other personality traits being correlated with the big5 facet c5 - self-discipline? (i'm looking for links to scientific reports)",
        "present_kp": [
            "personality"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "personality traits",
                "self-discipline",
                "big5 facet c5"
            ],
            "absent_kp": [
                "scientific reports",
                "studies"
            ]
        }
    },
    {
        "text": "moving tcp flows between interfaces and recovering traffic. lets say, i have two machines a and b.both have a 2 port 10g nic. let their interfaces names be eth0 and eth1 respectively.let eth0 of machine a be connected to eth0 of machine b.and eth1 of machine a be connnected to eth1 of machine b.i have the following questions.i have a tcp application (some web application) that starts sending some data using eth0 port (machine a), so it will be received at eth0 port (machine).after a few seconds, i want to move the tcp application from eth0 port to eth1 port (in machine a). how can i do this?as soon as i move the data to eth1 port (in machine a), i want to capture the data in eth1 port (in machine b). same mechanism as above.now, part of the data (packets) was passed through eth0 port and part of the data would have passed through eth1 port. i presume this will break the tcp connection. how can i recover the data at machine b (ie, unify and recover packets that were obtained in eth0 and eth1 interfaces).any pointers or help is greately appreciated.",
        "present_kp": [
            "tcp"
        ],
        "absent_kp": [
            "linux",
            "networking"
        ],
        "predict": {
            "present_kp": [
                "tcp",
                "interfaces",
                "traffic"
            ],
            "absent_kp": [
                "network",
                "data recovery"
            ]
        }
    },
    {
        "text": "svn best practices - different code logic in branches. while working with one project i was assigned to, i noticed a small problem with our svn strategy. a few months ago, someone created a new feature branch from our trunk. in this branch he implemented a second implementation of an already implemented feature (but in a more efficient way). he changed actually only one class.after this action some of the classes methods are common for both places - trunk and feature branch - but some methods are specific for each implementation. it was decided that we have to support both version so sometimes we create a deploy from branch, sometimes from trunk (some business assumptions). now i have some problems with merging - normally i want to merge only these code parts which are common for trunk and branch. it makes each merge more complicated because each time i have conflicts to solve (and i have many of them) i have to pay extra attention to what action should i take for each conflicted line. my question is: am i right in claiming that the decision of creating a new version which should be supported as a feature branch was a bad idea? what are the best practices in such case?",
        "present_kp": [
            "svn"
        ],
        "absent_kp": [
            "branching"
        ],
        "predict": {
            "present_kp": [
                "svn",
                "branches",
                "merge"
            ],
            "absent_kp": [
                "version control",
                "conflict resolution",
                "best practices"
            ]
        }
    },
    {
        "text": "are there any known android phones planned which will work on project fi besides nexus 5x and nexus 6p?. i know project fi recently rolled out and currently requires a nexus 5x or nexus 6p. at the time i'm writing this, no other phones currently support project fi, but are there any others in the pipeline that will work, which will be released (or have been released later)?",
        "present_kp": [
            "android",
            "project fi"
        ],
        "absent_kp": [
            "smartphones"
        ],
        "predict": {
            "present_kp": [
                "android phones",
                "project fi",
                "nexus 5x",
                "nexus 6p"
            ],
            "absent_kp": [
                "smartphones",
                "mobile network"
            ]
        }
    },
    {
        "text": "when writing tests for a wordpress plugin, should i run them inside wordpress or in a normal browser?. i have started using bdd for a wordpress plugin i'm working on and i'm rewriting the js codebase to do tests. i've encountered a few problems but i'm going steady now, i was wondering if i had the right approach, because i'm writing test that should pass in a normal browser environment and not inside wordpress.i choose to do this because i want my plugin to be totally indipendent from the wordpress environment, i'm using requirejs in a way that i don't expose any globals and i'm loading my version of jquery that doesn't override the one that ships with wordpress. in this way my plugin would work the same on every wordpress version and my code would not break if they cheange the jquery version or someone use my plugin on an old wordpress version.i wonder if this is the right approach or if i should always test inside the environment i'm working in. since wordpress implies some globals i had to write some function purely for testing purpose, likeget_ajax_url: function() { if( typeof window.ajaxurl === undefined ) { return <url> } else { return window.ajaxurl; }}, but apart from that i got everything working right. what do you think?",
        "present_kp": [
            "bdd",
            "wordpress"
        ],
        "absent_kp": [
            "javascript",
            "unit testing",
            "plugins"
        ],
        "predict": {
            "present_kp": [
                "wordpress",
                "plugin",
                "tests",
                "browser"
            ],
            "absent_kp": [
                "javascript",
                "requirejs",
                "jquery",
                "bdd"
            ]
        }
    },
    {
        "text": "vim c++ clang_complete doesn't work for std::cin.get(). i'm using vim to do c++ projects on mac os x.to auto complete, i use this plugin: clang_completefor most of cases, such as members of user-defined classes, members of namespaces, it works very well.for example, when i type std::, many things such as cout, cin will popup.however, when i type std::cin., or std::string str; str. nothing popups and i got an error:pattern not foundit seems that user-defined classes' members can popup automatically whereas c++ library stuff's members can't. i don't know why.here is my .vimrc file:",
        "present_kp": [
            "vimrc"
        ],
        "absent_kp": [
            "autocompletion",
            "filetype c++",
            "plugin clang complete"
        ],
        "predict": {
            "present_kp": [
                "vim",
                "c++",
                "clang_complete"
            ],
            "absent_kp": [
                "auto complete",
                "mac os x"
            ]
        }
    },
    {
        "text": "what's the difference between 'mkdir -p' and 'install -d'?. what, precisely, is the difference in what is being performed by mkdir -p and install -d, in terms of what changes the two commands are doing to the system?",
        "present_kp": [
            "c"
        ],
        "absent_kp": [
            "coreutils"
        ],
        "predict": {
            "present_kp": [
                "mkdir -p",
                "install -d"
            ],
            "absent_kp": [
                "linux commands",
                "directory creation"
            ]
        }
    },
    {
        "text": "wild and changing business rules implemented with functional programming. i attempted the business rules kata. here's a video overview.however, i am not confident that going functional is a good strategy for the following objective:how can you tame these wild business rules? how can you build a system that will be flexible enough to handle both the complexity and the need for change? and how can you do it without condemning yourself to years and years of mindless support?is there an alternative fp approach that satisfies the objective stated above?module paymentsystem(*types*)type productid = productid of stringtype memberid = memberid of stringtype email = email of stringtype agent = agenttype royaltydepartment = royaltydepartmenttype packingslip = { memberid:memberid productid:productid}type physicalproducts = | book | video | othertype membershiptype = | membership of memberid | upgrade of memberidtype paymentfor = | physicalproduct of physicalproducts * packingslip | membership of membershiptypetype packingslipoptions = | packingslip of packingslip | duplicateslips of packingslip | withfirstaidvideo of packingsliptype paymentresponse = | packingslip of packingslipoptions | activatemembership of memberid | upgrademembership of memberid | emailowner of membershiptype | commissionpayment of agent(*functions*)let publish payload = () // stublet getagent productid = agent // stublet respondto (payment:paymentfor) = match payment with | physicalproduct (kind , packingslip) -> publish (commissionpayment (getagent packingslip.productid)) match kind with | book -> publish (duplicateslips packingslip) | video -> publish (withfirstaidvideo packingslip) | other -> publish packingslip | membership kind -> publish(emailowner kind) match kind with | membershiptype.membership memberid -> publish(activatemembership memberid) | membershiptype.upgrade memberid -> publish(upgrademembership memberid)",
        "present_kp": [],
        "absent_kp": [
            "f#"
        ],
        "predict": {
            "present_kp": [
                "functional programming",
                "business rules"
            ],
            "absent_kp": [
                "software development",
                "system design"
            ]
        }
    },
    {
        "text": "if i like a post that is shared with friends but the original post was public, who can see that i liked the shared post?. if i like a post that is shared by a friend with their friends but the original post was public, who can see that i liked the shared post?in other words, if friend a (let's call her anna) shares a post from a general page b (let's call this fb page bananas) - and the original bananas post was public but anna is only sharing that bananas post with her friends, who can see my 'like' to this shared post?",
        "present_kp": [],
        "absent_kp": [
            "facebook like",
            "facebook privacy"
        ],
        "predict": {
            "present_kp": [
                "post",
                "friends",
                "public"
            ],
            "absent_kp": [
                "privacy settings",
                "social media"
            ]
        }
    },
    {
        "text": "implications of faster randomized $circuit sat$ algorithm. in here on page $13$ proposition $1$ it says 'if $circuit$ $sat$ on $n$ inputs and $m$ gates is in $2^{n^{o(1)}}poly(m)$ time, then $exp ot\\subseteq p/poly$'.can we have randomized $2^{n^{o(1)}}poly(m)$ time in above statement?is there a similar result that would give $e ot\\subseteq size(2^{\\delta n})$ (this would give $p=bpp$)?how large can $o(1)$ be in the statement above and in 1. if applicable (can it be as large as $1/\\log\\log n$)?",
        "present_kp": [],
        "absent_kp": [
            "big picture",
            "derandomization"
        ],
        "predict": {
            "present_kp": [
                "circuit sat",
                "randomized algorithm"
            ],
            "absent_kp": [
                "complexity theory",
                "computational complexity"
            ]
        }
    },
    {
        "text": "find files with path of a directory exclude subdirectories. how can i use find to traverse a directory, but not recurse into its subdirectories?i tried -prune and it does not work. and there is no -maxdepth option.find /opt/projectname/bin -type f /opt/projectname/bin -prune -o -printfind: missing conjunction/opt/projectname/bin/file_1_is_printed/opt/projectname/bin/file_2_is_printed/opt/projectname/bin/directory_within_bin/some_file_should_not_be_printed/opt/projectname/bin/directory_2_within_bin/some_file_2_should_not_be_printed/usr/bin/find: find.c $date: 2011/08/12 15:04:36 $revision: r11.31/4 patch_11.31 (phco_42158) libcpio.c $date: 2008/05/27 16:08:10 $revision: r11.31/2 patch_11.31 (phco_36666) $revision: @(#) find r11.31_bl2011_0923_2 patch_11.31 phco_42158this question is not a repeat question. there is no gnu here. although the question may be a repeat question, the answers posted on those questions ask installations of gnu tools. hence if the answers here help solve, this is a unique thread.",
        "present_kp": [
            "find"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "find",
                "directory",
                "subdirectories"
            ],
            "absent_kp": [
                "gnu",
                "file search"
            ]
        }
    },
    {
        "text": "how to tell apt to use the latest package by default?. i put jessie-backports in my /etc/apt/sources.list, but it seems that apt will not automatically use the packages from backports but older packages.however, if i use apt-cache show to check the version it shows the latest, and i am able to use apt install xxx=<latest-version> to install it.how to tell apt always use the latest package by default?",
        "present_kp": [
            "apt"
        ],
        "absent_kp": [
            "debian"
        ],
        "predict": {
            "present_kp": [
                "apt",
                "latest package",
                "backports"
            ],
            "absent_kp": [
                "debian",
                "package management"
            ]
        }
    },
    {
        "text": "spotify is not working at all after cancelling premium. everytime i double-click a song in my library (saved, not local) i get an error that the current song cannot be played. i cancelled premium and i've played a few songs in my app, but the desktop software and web player will not play anything. help appreciated!",
        "present_kp": [
            "spotify"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "spotify",
                "premium"
            ],
            "absent_kp": [
                "music streaming",
                "desktop software",
                "web player"
            ]
        }
    },
    {
        "text": "collisions in independent hashing. let $h$ be a $s$-wise independent family of hash functions from $\\{1,\\ldots,m\\}$ to $\\{1,\\ldots,n\\}$. it is easy to bound one collision, but are there good bounds for muliple collision ?",
        "present_kp": [
            "hash"
        ],
        "absent_kp": [
            "computability",
            "data structures",
            "probability theory",
            "hash tables"
        ],
        "predict": {
            "present_kp": [
                "collisions",
                "independent hashing",
                "hash functions"
            ],
            "absent_kp": [
                "probability",
                "hash table"
            ]
        }
    },
    {
        "text": "check accuracy of model provided by consultant. my company has recently engaged a consultant firm to develop a predictive model to detect defective works.i understand that there are many ways to validate the model, for example, using k-fold cross-validation and i believe that the consultant firm will carry out the validation before submitting the model to us.however, at the employer's side, how can i check the accuracy of the model developed by the consultant firm ?? someone suggested that i can give 2000-2015 data to the consultant firm and keep 2016 data for our own checking. however, a model with good accuracy on 2016 data does not imply that it will have good predictive power in the future. in my view, keeping 2016 data for checking seems like adding one more test set for validation, which in my view, is unnecessary since i already hv k-fold cross validation.could someone advise what the employer can do to check the consultant's model?",
        "present_kp": [
            "cross validation",
            "accuracy"
        ],
        "absent_kp": [
            "machine learning",
            "predictive modeling"
        ],
        "predict": {
            "present_kp": [
                "predictive model",
                "k-fold cross-validation",
                "model accuracy"
            ],
            "absent_kp": [
                "consultant firm",
                "data validation",
                "predictive power"
            ]
        }
    },
    {
        "text": "update .profile in /etc in unix. how can i append the following text in .profile in folder /etc/security of unix os?ps1='hostname -s':$logname'[$pwd]'i tried:print 'export ps1='hostname -s':$logname'[$pwd]' ' >> profilemy output gives:export ps1='hostname -s':$logname[/etc]with [/etc].",
        "present_kp": [],
        "absent_kp": [
            "shell",
            "quoting",
            "prompt"
        ],
        "predict": {
            "present_kp": [
                "unix",
                "profile"
            ],
            "absent_kp": [
                "shell script",
                "environment variables"
            ]
        }
    },
    {
        "text": "proper use or convenience, or both?. in my current project, i am working with a lot of json objects that contain arrays, er.. lists so i setup a decorator for convienece when the list is one item or multiple. even though this is convienent in the current project, is it reusable code?def collapse(function): @functools.wraps(function) def func(*args, **kwargs): call = function(*args, **kwargs) if isinstance(call, (list, tuple)) and (len(call) == 1): return call[0] return call return func@collapsedef get_results(query, wrapper=none): # get json object. result = result.json() # using the requests library. if wrapper: return result[wrapper] return resultso, get_results() has the potential of returning either a list or a dict. in most cases, the code knows when what type is returned, so using the @collapse decorator changes [result_dict] to just result_dictis this a good practice, or should i write a decorator that takes a limit parameter@limit(1) # def limit(items=none): ... return data[0:items] if items else datadef get_results(query, wrapper=none):just wrote out the limit decorator...def limit(items=none, start=0, collapse=false): if items and (start > 0): items += start def wrapper(function): @functools.wraps(function) def func(*args, **kwargs): call = function(*args, **kwargs) if isinstance(call, (list, tuple)): results = call[start:items] if items else call if collapse and (len(results) == 1): return results[0] else: return results return call return func return wrapper",
        "present_kp": [],
        "absent_kp": [
            "python"
        ],
        "predict": {
            "present_kp": [
                "json",
                "decorator",
                "list"
            ],
            "absent_kp": [
                "python",
                "function"
            ]
        }
    },
    {
        "text": "count occurrence of a word in google spreadsheet. i've got a ton of cells (say 6x20) that have various names in them. i'd like to total the number of times a name in another field matches any of the other cells.alice bob clairedoug alice chrisbob claire bobit seems like there should be a way to look at the example 3x3 above (or my actually much larger group of names) and extract how many bobs or alices or whatever occur. my spreadsheet-fu is weak and i haven't been able to find an answer via google (probably because i just don't know the right term for what i want to do).here's a sample of what i'm trying for: count occurenceit uses if statements, but since each cell needs it's own if statement this is not really a viable way to make the spreadsheet, especially as more people can be added in at later points.",
        "present_kp": [],
        "absent_kp": [
            "google spreadsheets"
        ],
        "predict": {
            "present_kp": [
                "google spreadsheet",
                "count occurrence"
            ],
            "absent_kp": [
                "excel",
                "data analysis"
            ]
        }
    },
    {
        "text": "git and sshfs: status is inaccurate. i am mounting a git repository on a virtual machine over sshfs onto my host machine. so far so good. however, git claims that there are many uncommitted changes when viewing the repository over sshfs. why is that?the relevant bit from /etc/fstab, if that helps:sshfs#usr@virtual:/home/user/repos /home/user/repos/ fuse noauto,user,uid=1000,gid=1000,umask=0,workaround=rename 0 0note: cloning, push, and pull operations could done instead but the code needs to compile on the virtual machine os and not on the host machine -- long story, it has to be this way. i do not fancy doing a commit per compile, that's just silly.",
        "present_kp": [
            "git",
            "sshfs"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "git",
                "sshfs"
            ],
            "absent_kp": [
                "virtual machine",
                "file system"
            ]
        }
    },
    {
        "text": "disable search option for resolv.conf. we are running a script that tries to resolve thousands of domains for a research project. the issue we are having is that a lot of domains are not resolvable for example foo.com. if we ping foo.com the system will do a lookup for foo.com. if that does not resolve, it will do a lookup for foo.com.ourdomain.com. it /etc/resolv.conf we had:# generated by networkmanagersearch ourdomain.comnameserver 8.8.8.8nameserver 4.2.2.2the above result is expected since we had the search setting set. if we changed it to say:# generated by networkmanagersearch ourdomain.netnameserver 8.8.8.8nameserver 4.2.2.2then as expected any non-resolvable query will be queried again with ourdomain.net at the end.the issue that we have is if we remove the search line completely from resolv.conf then system goes back to using ourdomain.com as the search. what we want is if a dns lookup does not resolve to not then look it up with the domain that was set in resolv.conf (which is also the domain of the box).",
        "present_kp": [
            "resolv.conf"
        ],
        "absent_kp": [
            "centos"
        ],
        "predict": {
            "present_kp": [
                "resolv.conf",
                "dns lookup"
            ],
            "absent_kp": [
                "network configuration",
                "domain resolution"
            ]
        }
    },
    {
        "text": "crontab order run with differents schedule. i have a crontab with different time to execute some task, for example every minute, every 10 min, 1 hour, daily... and i have a question, when some of this cron coincide in the same time for example, when 10 minutes execute, also execute 1 min cron and this cron execute in parallel... but i want to execute in sequence, for example all jobs in 1 minute, and the all jobs in 10 min... how can i do this??",
        "present_kp": [
            "cron"
        ],
        "absent_kp": [
            "executable"
        ],
        "predict": {
            "present_kp": [
                "crontab",
                "schedule"
            ],
            "absent_kp": [
                "task scheduling",
                "cron jobs"
            ]
        }
    },
    {
        "text": "how to convert a colored file to mail readable in bash?. i have a file which was created by a script which will be colored when i open using cat. but when i tried to send that file as attachment, it is not showing properly. like below it is showing.^[[33m================================================================================^[[m ^[[34m172.29.0.110^[[m^[[33m================================================================================^[[mfilesystem size used avail use% mounted on/dev/mapper/centos 109g 13g 91g 13% //dev/mapper/mpatha 1.6t 1.3t 277g 83% /var/lib/sql^[[33m================================================================================^[[m ^[[34m172.29.8.110^[[m^[[33m================================================================================^[[mfilesystem size used avail use% mounted on/dev/mapper/centos 117g 9.1g 102g 9% //dev/mapper/mpatha 1.6t 1.4t 109g 93% /var/lib/sql^[[33m================================================================================^[[m ^[[34m172.29.16.110^[[m^[[33m================================================================================^[[mfilesystem size used avail use% mounted on/dev/mapper/centos 117g 18g 94g 17% //dev/mapper/vg01-lv 1.5t 812g 590g 58% /var/lib/sql^[[33m================================================================================^[[m ^[[34m172.29.26.110^[[m^[[33m================================================================================^[[mfilesystem size used avail use% mounted on/dev/mapper/logvol02 117g 22g 90g 20% //dev/mapper/mpathm 1.6t 1.1t 435g 71% /var/lib/sqlthe script(part of the script) that creates the file is ::for ip in $(cat file.txt); do (echo -e \\e[33m$line\\e[m echo -e \\e[34m$ip\\e[m echo -e \\e[33m$line\\e[m echo -e filesystem size used avail use% mounted on ssh $ssh_arg -q user@${ip} df -ph | egrep -iv 'filesystem|boot|tmpfs') >> /disk_${date}_log echo -e \\e[33m$line\\e[m echo successful for $ipdonecat disk_${date}_log | mail -s disk space <email> can remove those echo's that creating colors, but i want to know is there any way i can send this file properly by mail or can i do something in mail command to solve this?",
        "present_kp": [
            "bash",
            "files"
        ],
        "absent_kp": [
            "linux",
            "sendmail"
        ],
        "predict": {
            "present_kp": [
                "bash",
                "mail"
            ],
            "absent_kp": [
                "script",
                "email attachment"
            ]
        }
    },
    {
        "text": "any way to download from scribd?. is there any way i can download from scribd? i need to download a few documents for my research.",
        "present_kp": [
            "scribd"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "scribd"
            ],
            "absent_kp": [
                "document download",
                "research"
            ]
        }
    },
    {
        "text": "is it possible to select tabs as tabs with mouse in urxvt?. i am running urxvt on arch linux. i can select the output with mouse for copy / paste. the problem occurs when output contains tabulators. all tabulators are selected and copied as spaces. that makes it really difficult to preserve the structure of some outputs when copying them.is there any way to fix this behaviour?edit: i am using zsh if that has any effect on the issue.",
        "present_kp": [
            "mouse",
            "rxvt"
        ],
        "absent_kp": [
            "terminal",
            "clipboard"
        ],
        "predict": {
            "present_kp": [
                "urxvt",
                "tabs",
                "mouse",
                "arch linux",
                "zsh"
            ],
            "absent_kp": [
                "terminal emulator",
                "copy paste",
                "tabulators",
                "output structure"
            ]
        }
    },
    {
        "text": "theta estimation of two functions. i'm in a data structures class, and am working on an assignment right now that asks me to find the theta complexity of certain loops. i missed class the day we were introduced to the topic, and everything i can find online expects more prior knowledge than i feel i have. can someone just explain this in beginners terms for me? i don't know what big-o or theta even refer to in this notation. i think i have a loose idea of what complexity refers to (a measure of how efficient code is, depending on how long a function will take in different circumstances?)the problem in question:demonstrate the $\\theta$-complexity of the functions below. in order to demonstrate that $f(n) = \\theta(g(n))$ you must find two constants $c_1$ and $c_2$ such that$$ c_1g(n) f(n) c_2g(n). $$a.) $f(n) = n^3 - 3n^2 + 5$.b.) $f(n) = 2\\log_2 n - 4$.",
        "present_kp": [],
        "absent_kp": [
            "asymptotics",
            "landau notation"
        ],
        "predict": {
            "present_kp": [
                "theta complexity",
                "data structures",
                "big-o",
                "complexity"
            ],
            "absent_kp": [
                "algorithm",
                "time complexity"
            ]
        }
    },
    {
        "text": "how to check the file usage summary in google drive. after sharing the folder/file in google drive to some other mail address, how can i monitor the log activity, like when the respective user has accessed the file, what are the modifications done etc.?is this possible in google drive?",
        "present_kp": [
            "google drive"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "google drive",
                "file usage summary",
                "log activity"
            ],
            "absent_kp": [
                "file sharing",
                "access monitoring",
                "modification tracking"
            ]
        }
    },
    {
        "text": "reverse engineering a whole website. how do i reverse engineer every single aspect and functionality of a website so that i get an exact fully working copy of it?. all interactions including javascript, cascade style sheets, php to make a perfect clone of it?",
        "present_kp": [],
        "absent_kp": [
            "websites"
        ],
        "predict": {
            "present_kp": [
                "reverse engineering",
                "website",
                "javascript",
                "cascade style sheets",
                "php"
            ],
            "absent_kp": [
                "web development",
                "clone"
            ]
        }
    },
    {
        "text": "bash operations on a for loop object. i'm attempting to run a bash command on a forloop object but it's trying to look for a file instead of use the forloop object.example:the input file contains lines in the format of user:passwordfor item in $(cat myitems);do user = cat $item | cut -d : -f1 pass = cat $item | cut -d : -f2donethe result of this is that it says file $item isn't foundi also triedfor item in $(cat myitems);do user = $(cat $item | cut -d : -f1) pass = $(cat $item | cut -d : -f2)done",
        "present_kp": [],
        "absent_kp": [
            "shell script"
        ],
        "predict": {
            "present_kp": [
                "bash",
                "for loop"
            ],
            "absent_kp": [
                "shell script",
                "file handling"
            ]
        }
    },
    {
        "text": "can't install libcairo / x11-common. i was trying to install libcairo which required x11-common. however, upon installation it sayssetting up x11-common (1:7.7+7) ...update-rc.d: warning: start and stop actions are no longer supported; falling back to defaultsinsserv: warning: script 'node-influenza.sh' missing lsb tags and overridesinsserv: there is a loop between service monit and node-influenza.sh if stoppedinsserv: loop involving service node-influenza.sh at depth 2insserv: loop involving service monit at depth 1insserv: stopping node-influenza.sh depends on monit and therefore on system facility '$all' which can not be true!insserv: exiting now without changing boot order!update-rc.d: error: insserv rejected the script headerdpkg: error processing package x11-common (--configure): subprocess installed post-installation script returned error exit status 1processing triggers for systemd (215-17+deb8u5) ...errors were encountered while processing: x11-commone: sub-process /usr/bin/dpkg returned an error code (1)i suspect it's a problem with the post install - however i have no idea on how to fix it. i tried pretty much installing and uninstalling everything by hand, to no avail. strangely, on my debian 8 vm it works fine.i uploaded the x11-common.postinst if it is needed.",
        "present_kp": [
            "debian",
            "dpkg"
        ],
        "absent_kp": [
            "software installation"
        ],
        "predict": {
            "present_kp": [
                "libcairo",
                "x11-common"
            ],
            "absent_kp": [
                "debian",
                "installation error"
            ]
        }
    },
    {
        "text": "does reinforcement learning require the help of other learning algorithms?. can't reinforcement learning be used without the help of other learning algorithms like svm and mlp back propagation? i consulted two papers:paper 1paper 2both have used other machine learning methods in the inner loop.",
        "present_kp": [
            "machine learning",
            "reinforcement learning"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "reinforcement learning"
            ],
            "absent_kp": [
                "svm",
                "mlp back propagation"
            ]
        }
    },
    {
        "text": "how big is the variance of the treewidth of a random graph in g(n,p)?. i am trying to find how close $tw(g)$ and $e[tw(g)]$ really are, when $g \\in g(n,p=c/n)$and $c>1$ is a constant not depending on n (so $e[tw(g)] = \\theta(n)$). my estimate is that $tw(g) \\leq e[tw(g)] + o(n)$ w.h.p, but i haven't been able to prove it.",
        "present_kp": [
            "treewidth"
        ],
        "absent_kp": [
            "graph theory",
            "co.combinatorics",
            "random graphs"
        ],
        "predict": {
            "present_kp": [
                "treewidth",
                "random graph"
            ],
            "absent_kp": [
                "graph theory",
                "variance"
            ]
        }
    },
    {
        "text": "is traversing an unconnected graph possible?. i have been assigned a fun project: design and implement a program that maintains the data of a simple social network. each person in the network should have a profile that contains his name, current status, and a friends list.i think it is clear that the project calls for the use of the adt graph. each vertex represents a person in the network and an edge between vertices a friendship. now, the graph may not be connected because some members do not have any friends in the network. with that in mind, consider this feature that must be implemented:the network must have a feature that computes the emergency phone chain, make sure that each member in the network is contacted, and only by one person. any of the people in the network may initiate the first call. utilize a depth-first graph traversal algorithm.now, what i think my professor is suggesting is merely a full traversal of the graph. how is that possible for an unconnected graph? any suggestions?(btw, the wording above is somewhat unclear - does the professor mean that everyone in the network is contact by the same one person? thoughts? i would ask her, but she is unavailable until next week.)",
        "present_kp": [],
        "absent_kp": [
            "graphs",
            "weighted graphs"
        ],
        "predict": {
            "present_kp": [
                "graph",
                "social network",
                "depth-first graph traversal algorithm"
            ],
            "absent_kp": [
                "unconnected graph",
                "emergency phone chain"
            ]
        }
    },
    {
        "text": "fetch elapsed time value from a file. i need to fetch elapsed time output from file. i need the value just before elapsed that is 2:10:42.file content :312.90user 15.57system 2:10:42elapsed 4%cpu (0avgtext+0avgdata 0maxresident)k0inputs+0outputs (1major+152440minor)pagefaults 0swaps",
        "present_kp": [],
        "absent_kp": [
            "shell script",
            "text processing"
        ],
        "predict": {
            "present_kp": [
                "elapsed time",
                "file"
            ],
            "absent_kp": [
                "cpu usage",
                "system performance"
            ]
        }
    },
    {
        "text": "how to compare files from solaris and linux. i want to compare the contents of same file present there in both solaris and linux for our testing purpose.i there is any tool available for that.if i want to develop new tool, haw can i achieve that?we are migrating project from solaris to linux.we want to verify the final output with is there in binary format by comparing them.both the files are there in different systems.please suggest me how we will do that comparison.",
        "present_kp": [
            "linux",
            "solaris"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "solaris",
                "linux",
                "compare files"
            ],
            "absent_kp": [
                "file comparison tool",
                "binary format",
                "migration"
            ]
        }
    },
    {
        "text": "given a truth table, force a contradiction. suppose i have a formula, and a lying witness is attempting to make it evaluate to false.given a truth table $c(f_1,, f_n)$, how could you force a lying witness to contradict herself?a contradiction is simply when the witness's statements are logically impossible; i.e. that $x_1,x_2$ are each true, but $x_1 \\space and\\space x_2$ is false.how can i characterize the set of all formula for which i force the witness to contradict herself?what complexity class does this problem fall in?",
        "present_kp": [],
        "absent_kp": [
            "complexity theory",
            "computability",
            "np complete",
            "closure properties",
            "decision problem"
        ],
        "predict": {
            "present_kp": [
                "truth table",
                "contradiction",
                "formula",
                "witness"
            ],
            "absent_kp": [
                "logic",
                "complexity class"
            ]
        }
    },
    {
        "text": "epiphany shuts down as it is loading. on my raspberry pi, epiphany starts up and then after loading 'most visited'... it closes. i reinstalled epiphany after doing a full update, and the same thing still is happening. other programs are running ok, including the dillo browser.",
        "present_kp": [],
        "absent_kp": [
            "debian"
        ],
        "predict": {
            "present_kp": [
                "epiphany",
                "raspberry pi"
            ],
            "absent_kp": [
                "browser",
                "dillo"
            ]
        }
    },
    {
        "text": "can pre-populating html and ajax replacing it when scrolled be a good lazy loading strategy for seo?. so i've read several posts here regarding seo and lazy loading as well as the google page for lazy loading your site's content. since setting up html snapshots for an ajax website is a large amount of work after the learning curve, i propose the following alternative to serving static content to a crawler.i have prepopulated my content divs with seo optimized, bare html content. my asynchronous content is loaded in replacing the original static content (per scrolling trigger of course).i cannot think of a reason why this isn't a smooth way to serve up the static html for crawlers. i mean if they really can't index asynchronous content then they shouldn't even recognize that the above process is happening after seeing the original content.am i missing something here?",
        "present_kp": [
            "seo",
            "ajax"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "html",
                "ajax",
                "seo",
                "lazy loading"
            ],
            "absent_kp": [
                "crawler",
                "static content"
            ]
        }
    },
    {
        "text": "wifi stopped working, unknown symbol wireless_nlevent_flush. as of this afternoon the wifi on my dell xps 13 stopped working (running debian sid with kernel 4.4.0-1-amd64). lspci detects my wifi card, but ifconfig -a shows only the loopback interface.i tried plugging in a usb wifi dongle which i have used recently with this laptop, and this also does not get recognized as a network interface. both during startup and immediately after plugging in a usb wifi dongle, the following message appears in dmesg:cfg80211: unknown symbol wireless_nlevent_flush (err 0)cfg80211 sounds like it would be used for configuring 802.11 (i.e. wifi) so i suspect this kernel module isn't getting loaded correctly. indeed, if i try to modprobe cfg80211 i receive the following error:modprobe: error: could not insert 'cfg80211':? unknown symbol in module, or unknown parameter (see dmesg)and checking dmesg i see the same message as above.googling for unknown symbol wireless_nlevent_flush yields zero results, although googling for just wireless_nlevent_flush seems to imply that it relates to wext somehow. one page suggested that rfkill must be loaded before cfg80211, but rfkill is already loaded. i would be very grateful for any advice.",
        "present_kp": [
            "wifi",
            "dmesg"
        ],
        "absent_kp": [
            "kernel modules"
        ],
        "predict": {
            "present_kp": [
                "wifi",
                "kernel module",
                "cfg80211"
            ],
            "absent_kp": [
                "linux",
                "network interface"
            ]
        }
    },
    {
        "text": "is polynomial time reducibility reversible?. if a language $a$ is reducible to some language $b$, does it follow that $b$ is reducible to $a$?my guess is no, it having something to do with the function $f$ in the definition of $a$ reducing to $b$ needing to be invertible.",
        "present_kp": [
            "polynomial time"
        ],
        "absent_kp": [
            "complexity theory",
            "reductions"
        ],
        "predict": {
            "present_kp": [
                "polynomial time reducibility"
            ],
            "absent_kp": [
                "computational complexity",
                "invertible function"
            ]
        }
    },
    {
        "text": "how to build the reduction from hamiltonian cycle problem to subgraph isomorphism?. i'm trying to prove that the subgraph isomorphism problem is npc using the hamiltonian cycle problem.unfortunately i feel (or don't understand) that the solution is empty and doesn't explain the hamiltonian cycle - subgraph isomorphic connection, @luke mathieson says that hamiltonian cycle to subgraph isomorphism is really just rephrasing what it means for a graph to have a hamiltonian cycle - but i don't get it.how does one transforms from hamiltonian cycle to subgraph isomorphism?i read reducing from hamiltonian cycle to subgraph isomorphism and <url> and couldn't understand how should a proper reduction look like, how to build one that proves the subgraph problem?your help in simplifying the problem(s) will be very appreciated.",
        "present_kp": [
            "graph isomorphism"
        ],
        "absent_kp": [
            "complexity theory",
            "np complete",
            "reductions",
            "hamiltonian path"
        ],
        "predict": {
            "present_kp": [
                "hamiltonian cycle problem",
                "subgraph isomorphism"
            ],
            "absent_kp": [
                "np-complete",
                "graph theory"
            ]
        }
    },
    {
        "text": "completeness and first order logic with least fixed point operator (lfp). is there any result about the extension of first order logic with least fixed point operator, being complete (as logic in general on infinite structures too) or not? in other words does the goedel completeness theorem of first order logic extent to such fo-lfp logic ?",
        "present_kp": [
            "first order logic"
        ],
        "absent_kp": [
            "descriptive complexity"
        ],
        "predict": {
            "present_kp": [
                "first order logic",
                "least fixed point operator",
                "lfp"
            ],
            "absent_kp": [
                "completeness theorem",
                "infinite structures"
            ]
        }
    },
    {
        "text": "install app on tiny core. how do you install mysql on tiny core? also how do you install clamav? can you install apt-get or yum on tiny core?all should be done in command line / terminal",
        "present_kp": [
            "mysql"
        ],
        "absent_kp": [
            "tinycore"
        ],
        "predict": {
            "present_kp": [
                "tiny core",
                "mysql",
                "clamav",
                "command line",
                "terminal"
            ],
            "absent_kp": [
                "apt-get",
                "yum"
            ]
        }
    },
    {
        "text": "sharing zsh and vim configuration with root user. i have modified my .zshrc and .vimrc to my likings. both files source other files containing more configuration and plugins (vundle, antibody). now i would like to use these configurations when i change to the root user as well.currently my .zshrc looks like this# .zshrc# sources the files in .shell directoryexport shell_conf_dir='/home/myuser/.shell'source $shell_conf_dir/initshell.shinitshell.sh does the following# initshell.sh# sources further scripts. $shell_conf_dir/antibody.sh. $shell_conf_dir/compinstall.sh. $shell_conf_dir/configuration.sh. $shell_conf_dir/options.sh. $shell_conf_dir/aliases.sh. $shell_conf_dir/variables.sh. $shell_conf_dir/virtualenv.sh. $shell_conf_dir/functions.sh. $shell_conf_dir/keybindings.sh. $shell_conf_dir/xorg.shi want to manage my .vimrc in a similar fashion.now the root users .zshrc is just a symlink to my normal users .zshrc. this works pretty well but i think, concerning security, this might not be an ideal solution. so where should i actually put all this configuration and how should i handle it so the root user can use it, too? or is this actually the wrong approach? i know that i can preserve environment variables with sudo -e but that doesn't work when doing sudo -i.",
        "present_kp": [
            "sudo",
            "environment variables",
            "root",
            "vimrc"
        ],
        "absent_kp": [
            "bashrc"
        ],
        "predict": {
            "present_kp": [
                "zsh",
                "vim",
                "root user",
                "configuration"
            ],
            "absent_kp": [
                "shell script",
                "environment variables"
            ]
        }
    },
    {
        "text": "is it safe to just use mysqli?. i have developed an open source php application and currently it uses both the mysqli or mysql extension for backwards compatibility. i'm wondering about switching it over to only be compatible with mysqli since php now no longer supports the mysql. another reason i'd like to get away from supporting both mysql and mysqli is that i'm basically using mysqli the same way i do as with the old mysql extension. can anyone explain to me if this is a good or bad idea?",
        "present_kp": [
            "php",
            "mysql",
            "compatibility"
        ],
        "absent_kp": [
            "backward compatibility"
        ],
        "predict": {
            "present_kp": [
                "mysqli",
                "php"
            ],
            "absent_kp": [
                "mysql",
                "database"
            ]
        }
    },
    {
        "text": "how can i decouple query and context in a query object scenario?. i'm using a query object pattern (similar to this) to manage disparate queries while avoiding bloaded facades/repositories.a query object takes a number of constructor parameters, representing query arguments. the query is then passed to an iqueryhandler by the caller, into which is injected the idatacontext. the idatacontext is then passed into an execute method of the iquery.the thing i don't like is this:public interface iquery<tresult>{ public tresult execute(idatacontext context);}because the idatacontext is passed into the method and thus explicitly declared in the interface, there is no option to have a query that fetches things via a different mechanism - iquery is coupled to whatever defines the idatacontext interface. suppose, for example, i want to switch from an sql db store to a document db.i've tried a couple of alternatives, but neither quite gets me where i want to be.the first is to inject the data context into the query's constructor and use a factory to generate the query objects. this successfully decouples the interfaces, but now working with the queries is more cumbersome. instead of using the constructor/object initializer to set up the query, callers have to do something like this:var query = _queryfactory.create<personquery, person>(); // second type argument can't be inferred from first due to limitations in generic type inference.query.name = bob;query.age = 32;var result = query.execute();the second is to abstract the execution of the query - and thus the dependency on idatacontext - to a handler class, then have another class that resolves a handler for each query, which again abstracts the dependency away from any interfaces:public class isomequeryhandler : iqueryhandler<somequery, somequeryresult>{ private readonly idatacontext _context = ...; public somequeryresult execute(somequery query) { ... }}i don't like this option because it means every new query involves writing two classes, which, again, is cumbersome and increases the potential for a query to exist with no way of handling it (it also involves an empty marker interface for queries since the execute method is moved to the handler, which always feels off). it also involves some rather funky convention-based dependency resolution to get the right handler for each query.creating an abstraction over idatacontext is not really feasible due to its complexity.is there a happy medium i haven't seen?",
        "present_kp": [],
        "absent_kp": [
            "c#",
            "design patterns"
        ],
        "predict": {
            "present_kp": [
                "query object",
                "idatacontext"
            ],
            "absent_kp": [
                "decoupling",
                "sql",
                "document db"
            ]
        }
    },
    {
        "text": "is there any research on the notion of weak isolation?. (first of all, sorry for the long article which makes you want to skip through, but since the background and motivations are important to this question or it would be nonsense to the main problem, forgive me if this makes you sleep. i owe you a cup of coffee.)backgroundisolation lemma, one of the best tool in complexity theory invented by k. mulmuley, u. vazirani and v. vazirani in matching is as easy as matrix inversion, have been used to prove many amazing results like an rnc-algorithm for matching in the above paper; valiantvazirani theorem which is a randomized reduction from np problems to usat; nl/poly=ul/poly; and many others. surveys and introduction in blogs are also all over the web. in the center of the isolation lemma, we use randomization to assign weights on the base set $u$, such that for a family $\\mathcal{f}$ of subsets of $u$, there is a unique minimum weighted subset in $f$ with high probability. formally,lemma. let $u$ be a set of size $n$, and $\\mathcal{f}$ be a non-empty family containing elements which are the subsets of $u$. uniform-randomly assign integral weights in $[2n]$ on the set $u$, then there exist a unique minimum weighted subset in $\\mathcal{f}$ with probability at least $1/2$.a stronger form which covers a collection of $\\mathcal{f_1}, \\ldots, \\mathcal{f_m}$ that is used in practical situations can be found in the survey. there are also some variants that may reduced the use of random bits, or derandomized the lemma to get a deterministic weight assigning algorithm when the size of $\\mathcal{f}$ is not too large.problemafter a search of literature, it seems that all the research on this topic were all focus on isolating a unique subset of $u$. i want to know whether there is any obvious reason that we don't have a weak isolation lemma, in the sense that the number of minimum weighted subsets is bounded by a particular bound, and the requirement for the weak lemma is less than the original one. for example, do we have a lemma that only isolates $o(\\log n)$ minimum weighted subsets? how about linear or polynomial? since the size of $\\mathcal{f}$ is at most $2^n$, when the bound is set to $2^n$, the result becomes trivial since no isolation is needed.problem. is there any research on the notion of weak isolation, that only limits the number of minimum weighted subsets instead of a unique one? if so, is there any references? if not, what is the most obvious obstacle toward such a result?i've been trying to prove such a lemma with some approaches, but despite of using the exactly same techniques (thus the same requirements) which is clearly no better than the original one (since with the same requirements you can indeed isolate a unique subset), there is no success to reduce any conditions we need, even in the case that we only need a loose bound on the number of minimum subsets, say polynomial. (please read on if you need motivations to the problem! the part below contains some technical details to the usage of the lemma, and some applications to log-space computations.)motivationi am working on the problems in log-space computations, precisely the relation between classes $\\mathsf{nl}$, $\\mathsf{ul}$, and many other classes below and in between.consider the above lemma, which requires $o(n \\log n)$ random bits if we assign weights to $u$ accordingly. a random-bit-saving version of the lemma is presented in this paper by chari et al., which states:lemma. let $u$ be a set of size $n$, and $\\mathcal{f}$ be a non-empty family containing elements which are the subsets of $u$. there is a way of assigning weights in $[n^7]$ on $u$ with $o(\\log|f|+\\log n)$ random bits, such that there exist a unique minimum weighted subset in $\\mathcal{f}$ with probability at least $1/4$.that gives us an $\\mathsf{uspace}[o(\\log|f|+\\log n)]$ algorithm for reachability. of course this is a pity bound, since by savitch's theorem we have $\\mathsf{nl} \\subseteq \\mathsf{l}^2 \\subseteq \\mathsf{ul}^2$.but what if we consider the reachability problem with a restriction that there are at most $f(n)$ paths from the source to any node in the graph? this defines the problem $\\mathtt{reach}[f(n)]$, and we have $\\mathtt{reach}[2^n]$ to be the normal reachability problem. by setting $f(n)$ a polynomial, applying the above lemma together with an $\\mathsf{ul}$-algorithm for reachability in graphs with a minimum unique path (see the paper for more details), we can solve $\\mathtt{reach}[n^{o(1)}]$ in $\\mathsf{ul}$. (the result occurs in this recent paper. in fact they use a stronger version of the lemma, which isolates every paths in the graph by a constructive way.)if a weaker isolation lemma is known to have less restrictions on either the size of $\\mathcal{f}$, or the number of random bits being used, we can provide better bounds on the problem $\\mathtt{reach}[f(n)]$, with the help of some modifications to solve reachability in graphs with few minimum paths. hopefully if the lemma is weak enough to set $f(n) = 2^n$, and we can obtain some better bounds about the important class $\\mathsf{nl}$.any comments on the proof techniques or obstacles will give insight to the question, and i would like to know that whether this idea has a tiny little chance of success, or it is just a completely impossible concept.",
        "present_kp": [],
        "absent_kp": [
            "cc.complexity theory",
            "randomness"
        ],
        "predict": {
            "present_kp": [
                "isolation lemma",
                "weak isolation",
                "complexity theory",
                "randomization",
                "log-space computations"
            ],
            "absent_kp": [
                "deterministic algorithm",
                "graph theory",
                "polynomial time",
                "unique path",
                "reachability problem"
            ]
        }
    },
    {
        "text": "gnome boxes: proper network configuration?. i've got a problem with gnome boxes:i've 2 machines, cloned 1 from other, with centos. both machines get same ip addr on gnome boxes, so when i boot 2nd, 1st gets disconnected from network, and vice-versa. this is not acceptable behaviour,and i've been struggling to find out why does this happen for quite some time.",
        "present_kp": [
            "gnome"
        ],
        "absent_kp": [
            "networking",
            "virtual machine"
        ],
        "predict": {
            "present_kp": [
                "gnome boxes",
                "network configuration",
                "centos"
            ],
            "absent_kp": [
                "virtual machines",
                "ip conflict"
            ]
        }
    },
    {
        "text": "wordpress redirects to ip instead of domain. following this blog post i set up wordpress on an ec2 instance. i've pointed a records to my amazon ec elastic ip address. after propagation, when i open my website with the domain name, it works. but wheni click on other page, it just redirects me to my ip address instead.",
        "present_kp": [
            "wordpress"
        ],
        "absent_kp": [
            "domains",
            "amazon ec2"
        ],
        "predict": {
            "present_kp": [
                "wordpress",
                "domain",
                "ip address"
            ],
            "absent_kp": [
                "ec2 instance",
                "elastic ip"
            ]
        }
    },
    {
        "text": "how to view flash and other videos on linux systems?. i'm running fedora 17 and firefox 12. when i navigate to some sites i'm unable to view videos because i'm missing some plugin. when i click 'installl missing plugin' i'm still not able to view the video.any idea i can view them without installing flash player (not open source i believe)? what directory are these web plugins stored?",
        "present_kp": [
            "fedora",
            "firefox"
        ],
        "absent_kp": [
            "adobe flash"
        ],
        "predict": {
            "present_kp": [
                "linux",
                "fedora",
                "firefox",
                "plugin"
            ],
            "absent_kp": [
                "flash player",
                "open source",
                "web plugins"
            ]
        }
    },
    {
        "text": "delete string between two regex patterns. i have a file with following contents..\\..\\src\\modules\\core\\abc\\abc.cpp..\\..\\src\\modules\\core\\something\\xyz\\xyz.cpp..\\..\\src\\other_modules\\new_core\\something\\pqr\\pqr.cpp..\\..\\src\\other_modules\\new_core\\something\\pqr\\abc.cppthe result i am expecting is ..\\..\\src\\abc\\abc.cpp..\\..\\src\\xyz\\xyz.cpp..\\..\\src\\pqr\\pqr.cpp..\\..\\src\\pqr\\abc.cpphow can i achieve this using sed?i am unable to write an regular expression to capture two groups at the same time.initial group (....\\src) - this will be same in all the linesvariable group (abc\\abc.cpp) or (xyz\\xyz.cpp) or (pqr\\pqr.cpp) or (pqr\\abc.cpp)",
        "present_kp": [
            "sed"
        ],
        "absent_kp": [
            "text processing"
        ],
        "predict": {
            "present_kp": [
                "regex",
                "sed"
            ],
            "absent_kp": [
                "string manipulation",
                "file processing"
            ]
        }
    },
    {
        "text": "how to add a caa record on debian. i currently have a let's encrypt ssl certificate for my debian server, and would like to implement caa (certification authority authorization, rfc 6844) on it.i'm a bit confused as to how to implement it however, as i don't seem to be able to add it to my zone file on my registrar's site. (gandi.net)is it a file i add to the root of the web server, or do i have to wait until my registrar supports it?",
        "present_kp": [
            "ssl"
        ],
        "absent_kp": [
            "dns",
            "openssl"
        ],
        "predict": {
            "present_kp": [
                "caa record",
                "debian",
                "ssl certificate",
                "certification authority authorization",
                "rfc 6844",
                "zone file",
                "registrar"
            ],
            "absent_kp": [
                "web server",
                "gandi.net"
            ]
        }
    },
    {
        "text": "largest embeddable hypersphere given membership oracle. i have a membership oracle to tell me whether a point is inside of some set, s. i would like to find the radius of the largest (origin-centered) hypersphere that is contained in s.do you know any good references for this problem? (i'm looking for an algorithm along with a confidence that the hypersphere is fully contained in s).",
        "present_kp": [],
        "absent_kp": [
            "cg.comp geom"
        ],
        "predict": {
            "present_kp": [
                "hypersphere",
                "membership oracle"
            ],
            "absent_kp": [
                "algorithm",
                "confidence"
            ]
        }
    },
    {
        "text": "possible to open office documents in office web apps by default?. is it possible to set office web apps as the default application for office documents on windows? i figure this can't be done at the system level, so i'd be happy with a firefox extension or ie add-on that could accomplish this.i found a chrome extension that does what i'm looking for is there any equivalent in firefox or ie?",
        "present_kp": [],
        "absent_kp": [
            "office online"
        ],
        "predict": {
            "present_kp": [
                "office web apps",
                "default application",
                "windows",
                "firefox extension",
                "ie add-on",
                "chrome extension"
            ],
            "absent_kp": [
                "office documents",
                "system level",
                "firefox",
                "internet explorer"
            ]
        }
    },
    {
        "text": "is it good idea to write variable names that match application specific terms?. let us say i am writing a facebook like application.i write code like below$this->get_user_friends(); then next morning the boss says that we don't want to call friends friends anymore, we will call them pal. then all the templates are changed but what about the code above? after 2 years when some programmer will have to look into it he is likely to get confused isnt it ?what do you guys really do ?",
        "present_kp": [],
        "absent_kp": [
            "variables",
            "naming",
            "coding standards"
        ],
        "predict": {
            "present_kp": [
                "variable names",
                "application specific terms"
            ],
            "absent_kp": [
                "coding standards",
                "software maintenance"
            ]
        }
    },
    {
        "text": "new values for categorical variable in prediction dataset. i am performing regression task using *random forest**.in my prediction set i am having *new levels for categorical variable** which are not present in training data .currently i am performing one hot encoding to handle this.at present i am not getting considerable results. this article is saying that it is not good to perform one hot encoding.how to handle this case?",
        "present_kp": [
            "regression",
            "random forest"
        ],
        "absent_kp": [
            "machine learning",
            "statistics",
            "categorical data"
        ],
        "predict": {
            "present_kp": [
                "random forest",
                "categorical variable",
                "one hot encoding"
            ],
            "absent_kp": [
                "regression",
                "prediction dataset",
                "new levels"
            ]
        }
    },
    {
        "text": "how to use a shell command to only show the first column and last column in a text file?. i need some help to figure out how to use the sed command to only show the first column and last column in a text file. here is what i have so far for column 1:cat logfile | sed 's/\\|/ /'|awk '{print $1}'my feeble attempt at getting the last column to show as well was:cat logfile | sed 's/\\|/ /'|awk '{print $1}{print $8}'however this takes the first column and last column and merges them together in one list.is there a way to print the first column and last columns clearly with sed and awk commands?sample input:foo|dog|cat|mouse|lion|ox|tiger|bar",
        "present_kp": [
            "shell",
            "sed",
            "awk"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "shell command",
                "text file",
                "sed command",
                "awk"
            ],
            "absent_kp": [
                "linux",
                "command line",
                "data processing"
            ]
        }
    },
    {
        "text": "setting up a personal domain name. possible duplicate:how to find web hosting that meets my requirements? i'm looking to set up a personal website, but i know very little about web hosting. could somebody recommend a (not very expensive) host? what should i look for when choosing a host? also, i'm rather icky about atriyasen.com because people can't make out if i'm atriya sen (which i am) or atri yasen! would you recommend atriya-sen.com? atriya_sen.com? finally, what about other tlds like .name?",
        "present_kp": [],
        "absent_kp": [
            "domains",
            "looking for hosting"
        ],
        "predict": {
            "present_kp": [
                "personal domain name",
                "web hosting"
            ],
            "absent_kp": [
                "domain registration",
                "tlds"
            ]
        }
    },
    {
        "text": "creating emacs tags file. i am working through the emacs lisp intro book within emacs 23.4.1 on debian wheezy (crunchbang waldorf).section 4.1 discusses the find-tags command and the tags file. instructions are included to build/install the tags file if necessary.how do i do this in debian? the folders mentioned are not present on my system and i cannot locate a tags file.i'm not sure that i have the source of emacs installed? i installed it using apt-get. my sources.list file does not include any deb-src lines, if that is relevant.",
        "present_kp": [
            "debian",
            "emacs"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "emacs",
                "debian"
            ],
            "absent_kp": [
                "lisp",
                "tags file"
            ]
        }
    },
    {
        "text": "will uploading our .docx files on scribd and embedding the files on our website affect search engine rankings?. we have prepared notes for university students which are on .docx format. and we want it to put on our website for viewing. we tried one option. uploading the files on scribd and embedding it on our website for viewing on scribd viewer. will making documents available on srcibd viewer on our website affect search engine rankings ? will search engines treat it as duplicate content as those are already uploaded on scribd and we are embedding it on our website ?on scribd we have set the uploaded documents as 'private' though.and if it affects, can you suggest any suitable way to make .docx files to be viewed on our website that doesn't affect search engine rankings ?",
        "present_kp": [
            "embed"
        ],
        "absent_kp": [
            "seo"
        ],
        "predict": {
            "present_kp": [
                "search engine rankings",
                "scribd",
                "docx files"
            ],
            "absent_kp": [
                "duplicate content",
                "embedding",
                "website"
            ]
        }
    },
    {
        "text": "protocol agnostic robots sitemap. recently, i have enabled all my servers to serve everything over http and https. users can access any site via <url> or <url>. all pages are identical between the versions, so <url> is the same as <url> and so on.urls are relative, so they do not mention the protocol with one exception. in other words, if the page is loaded with http, it will link to other pages, images, css, javascript over http and the same with https, as to avoid mixed content warnings.now about that exception. it is in robots.txt:sitemap: <url> this url must be absolute.now the problem i see if that when google reads <url> it gets an http sitemap! the documentation on robots.org says that one can specify multiple sitemaps but if i am not sure that putting both the http and https sitemap is a good idea since they will contain each a list of identical pages (one with http and one with https).how should sitemap in robots.txt be handled for websites that accept http and https?some ideas that came to mind:specify both sitemaps (as mentioned above). afraid this would cause duplicate content issues.only specify the https sitemap. that gives access to all unique pages anyway.find a magical (apache) way to sent a different robots.txt via http and https. is that even possible? could it cause issues?",
        "present_kp": [
            "sitemap",
            "robots.txt"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "robots.txt",
                "sitemap",
                "http",
                "https"
            ],
            "absent_kp": [
                "protocol agnostic",
                "google",
                "apache"
            ]
        }
    },
    {
        "text": "configuring org-mode to open pdfs with evince. i'm starting to use org-mode to export text to latex.my problem is that it opens the generated pdf with ebook-viewer (it is a epub, chm reader) instead of using evince.questiondoes anyone know how to change this behaviour and configure evince to be the default viewer?",
        "present_kp": [
            "pdf"
        ],
        "absent_kp": [
            "emacs",
            "file opening",
            "org mode"
        ],
        "predict": {
            "present_kp": [
                "org-mode",
                "pdf",
                "evince"
            ],
            "absent_kp": [
                "latex",
                "ebook-viewer"
            ]
        }
    },
    {
        "text": "multi-threaded socket server high load. i'm trying to make a backend for quizup like application: user connects to a server, sends credentials and gets paired up with another user. after that server handles each pair, periodicaly sending server messages to each user in a pair and also redirecting user's mesages between them.server class:private static class server{ private static final int num_threads = 2400; private executorservice executorservice; private serversocket serversocket; private int listeningport; public volatile boolean isrunning; private thread mainthread; private volatile map<string, conn> playrequests; public server(int port){ try { executorservice = executors.newfixedthreadpool(num_threads); listeningport = port; serversocket = new serversocket(listeningport); isrunning = true; playrequests = new concurrenthashmap<string, conn>(); mainthread = new thread(new runnable(){ @override public void run() { handleincomingconnections(); } }); } catch (ioexception e) { system.out.println(e.tostring()); } } public void run(){ mainthread.start(); } private void handleincomingconnections(){ while(isrunning){ try { final socket client = serversocket.accept(); runnable gamerunnable = new runnable(){ @override public void run() { try{ bufferedreader reader = new bufferedreader(new inputstreamreader(client.getinputstream())); printwriter writer = new printwriter(new bufferedwriter(new outputstreamwriter(client.getoutputstream())), true); string read = null; string id = null; boolean isrequesting = false; string rid = null; while(!(read = reader.readline()).equals(fin_1)){ string[] str = read.split(#); if(str[0].equals(id)){ id = str[1]; }else if(str[0].equals(isrequesting)){ isrequesting = (str[1].equals(1)); }else if(str[0].equals(rid)){ rid = str[1]; } } conn connection = new conn(client, isrequesting, id, writer, reader); if(isrequesting){ playrequests.put(rid, connection); }else{ if(playrequests.containskey(id)){ conn conn = playrequests.get(id); playrequests.remove(id); handlegame(conn, connection); } } }catch(exception e){ system.out.println(e.tostring()); } } }; executorservice.execute(gamerunnable); } catch (ioexception e) { // todo auto-generated catch block e.printstacktrace(); } } } private void handlegame(conn a, conn b){ new gamehandler(a, b).execute(); }}gamehandler class:private class gamehandler{ private volatile conn a; private volatile conn b; private thread areadthread; private thread breadthread; private thread messagethread; private runnable areadrunnable; private runnable breadrunnable; private runnable messagerunnable; private volatile printwriter awriter; private volatile printwriter bwriter; private volatile bufferedreader areader; private volatile bufferedreader breader; private volatile boolean aisready; private volatile boolean bisready; private volatile boolean isgamerunning; public gamehandler(final conn s1, final conn s2){ this.a = s1; this.b = s2; isgamerunning = true; try { awriter = a.writer; bwriter = b.writer; areader = a.reader; breader = b.reader; } catch (exception e) { try { isgamerunning = false; a.close(); b.close(); } catch (ioexception e1) { // todo auto-generated catch block e1.printstacktrace(); } system.out.println(e.tostring()); } messagerunnable = new runnable(){ @override public void run() { system.out.println(a.id + + b.id); messagethread = thread.currentthread(); for(int i = 0; i < 6; i++){ if(isgamerunning){ try{ thread.sleep(4000); }catch(interruptedexception e){ } } } //end game isgamerunning = false; try { a.close(); b.close(); } catch (ioexception e) { // todo auto-generated catch block e.printstacktrace(); } } }; areadrunnable = new runnable(){ @override public void run() { areadthread = thread.currentthread(); string line = null; try { while (isgamerunning && (line = areader.readline()) != null && !(line = areader.readline()).equals(fin)){ bwriter.println(line); } a.close(); system.out.println(a.id + done); } catch (exception e) { try { isgamerunning = false; a.close(); b.close(); } catch (ioexception e1) { // todo auto-generated catch block e1.printstacktrace(); } system.out.println(e.tostring()); } } }; breadrunnable = new runnable(){ @override public void run() { breadthread = thread.currentthread(); string line = null; try { while (isgamerunning && (line = breader.readline()) != null && !(line = breader.readline()).equals(fin)){ awriter.println(line); } b.close(); system.out.println(b.id + done); } catch (exception e) { try { isgamerunning = false; a.close(); b.close(); } catch (ioexception e1) { // todo auto-generated catch block e1.printstacktrace(); } system.out.println(e.tostring()); } } }; } public void execute() { executorservice.execute(messagerunnable); executorservice.execute(areadrunnable); executorservice.execute(breadrunnable); } } and a container class for each users to hold open socket, in/out streams, credentials, etc:private class conn{ public socket s; public boolean isrequesting; public printwriter writer; public string id; public bufferedreader reader; conn(socket s, boolean isrequesting, string id, printwriter writer, bufferedreader reader){ this.s = s; this.isrequesting = isrequesting; this.id = id; this.writer = writer; this.reader = reader; } public void close() throws ioexception{ s.close(); } }the logic is following:server has a mainthread, where it accepts incoming connections and creates client sockets. for each new socket it creates a gamerunnable, where it listens for client's credentials (whether this client is the one requesting connection, id of the user it wants to connect to, id of itself). after receiving credentials, server creates a new conn object, storing all the info(id, and also socket and in/out streams, so it doesn't have to open it again after) there, and than places it in the map (playrequests) with requested user id as a key. if there is a matching pair in a map, server creates a new gamehandler for these two conn objects (all this still goes inside the gamerunnable). each gamehandler contains three runnables: messagerunnable to send messages from server to both users, and two runnables (areadrunnable and breadrunnable) to read incoming data from both sockets. so basically, each communication session (game) requires 4 threads (1 to get credentials and start a game, and three to maintain the game before the end). here are the questions i have:are there any design/implementation issues you see here? please be as picky as possible because i'd really not want it to crash under high load. if you see smth, you are more than welcome to give your solutionsi know that having large and uncontrolled number of threads is a bad practice, so i'm using an executer with fixed thread pool to execute all the runnables. however, due to the game features, i can't make users who are requesting connections wait for empty threads in a pool, what is obviously going to happen if i have a lot of incoming connections. so is usage of thread pool reasonable here? if yes, what number of threads should i use, given that i need 4 threads per game, and each game lasts approximately 2 minutes.am i closing all the sockets correctly? are there any memory leaks?other questions regard server deploying:i'm planning to run it on amazon ec2. should i use tomcat server for this, or can i just run it as a plain java program on jvm?i tested it on my laptop, and having many simultaneous connections, heap size is not enough to handle all of them. should i increase heap size before lunch as much as possible, or it may affect performance?",
        "present_kp": [
            "java",
            "socket",
            "server"
        ],
        "absent_kp": [
            "multithreading"
        ],
        "predict": {
            "present_kp": [
                "multi-threaded",
                "socket server",
                "high load",
                "thread pool",
                "amazon ec2"
            ],
            "absent_kp": [
                "quizup",
                "tomcat server",
                "heap size"
            ]
        }
    },
    {
        "text": "two exact copies of folder using ftp and command-line only. i'm looking for any solution that will allow me to emulate functionallity similar to dropbox/rsync using only ftp protocol and command-line.the general problem is, that i have to run it on a very limited linux (actually nas), so i can't install / use to complex solutions (not enough resources to run) and/or gui utils, as i have no gui there. ssh also works weak on that nas and is not present at all on one of destinations.what i need, is to have a command-line (bash) script or program that i will be able to run periodically (via cron) that will assure that source and destination are identical:all files on both sides copied to both sides,if two files of the same name exists, copy newest version of a file to both sides,delete on one side all files that are missing on another one.of course, this solution must support iteration on all subfolders of both source and destination, for this to work succesfully -- there will be a large directory structure on both sides.i've tried many backups solutions, but most of them failed for one of these reasons:unable to create exact duplicate copy of both sides and offering incremental backup instead,not able to be run directly from command-line, on very limited linux distro.i was advised to use unison and give it a good try. this really looks good, but it uses ssh, and i'm unable to establish ssh connection to one of my destinations (not supported) -- i.e. ftp access is the only available way.the perfect solution for me would be anything that i can run (command-line or configuration) like this:something.sh path/to/local/folder <url> it does exists at all...",
        "present_kp": [
            "backup",
            "ftp"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "ftp",
                "command-line",
                "bash"
            ],
            "absent_kp": [
                "rsync",
                "dropbox",
                "ssh"
            ]
        }
    },
    {
        "text": "how to get disk name that contains a specific partition. if i know that a partition is for example /dev/sda1 how can i get the disk name (/dev/sda in this case) that contains the partition ? the output should be only a path to disk (like /dev/sda). it shouldn't require string manipulation, because i need it to work for different disk types.",
        "present_kp": [
            "partition",
            "disk"
        ],
        "absent_kp": [
            "linux",
            "block device"
        ],
        "predict": {
            "present_kp": [
                "disk name",
                "partition"
            ],
            "absent_kp": [
                "linux",
                "storage"
            ]
        }
    },
    {
        "text": "create a file that's really a network port. i have a program running on a cluster, and the output of the program is written to a log file which i specify. however, instead of writing it to a file, i want to write it to a network port, so that it can be read with e.g. node.js.for example, i want to be able to specify a file along the lines of /dev/127.0.0.1:3000currently, i'm using node.js to watch the log file, re-read all the data when it changes, compare the new data with the old data to see what was added, and then process that. but that's quite inefficient.",
        "present_kp": [],
        "absent_kp": [
            "files",
            "filesystems",
            "serial port"
        ],
        "predict": {
            "present_kp": [
                "network port",
                "log file",
                "node.js"
            ],
            "absent_kp": [
                "cluster",
                "file monitoring"
            ]
        }
    },
    {
        "text": "rename the 10 most-recently modified files on aix. i have some .xls files in a defined directory (say in /a/b). i want to rename top 10 latest files and append -bkp in their names.i tried, not workingls -lt *.xls | head -1 | awk '{print mv $9 $9-bkp}' | shi tried find and -exec but how do we get top l0 latest modified files",
        "present_kp": [
            "files",
            "rename",
            "aix"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "aix",
                "files"
            ],
            "absent_kp": [
                "unix",
                "file management"
            ]
        }
    },
    {
        "text": "wpa supplicant: no network configuration found for current ap - carl9170-driven wifi adapter glitching on debian 7. i've got a debian 7 machine with linux3.2 kernel and a usb wifi adapter with atheros chipset (d-link dwa-16 xtreme n dual band), which in theory should work. indeed, i managed to establish a wifi communication with networkmanager and it worked more or less fine for ~30 minutes, but then disconnected and failed to reestablish the connection.i failed to reestablish the connection with networkmanager, it successfully associates and authenticates, starts 4-way handshake, but then deauthenticates due to reason 15 (4-way handshake timeout).then i tried to do the same via the good old ifupdown by creating an entry in /etc/network/interfaces:allow-hotplug wlan1iface wlan1 inet static wpa-ssid mynet wpa-psk <my key hash generated by 'wpa_passphrase mynet key'> address 192.168.1.2 netmask 255.255.255.0 broadcast 192.168.1.255 gateway 192.168.1.1 dns-nameservers a.b.c.dwhen i sudo ifup wlan1, it behaves reasonably, until:wpa_supplicant[8258]: wlan1: associated with <router's mac>wpa_supplicant[3402]: wlan1: no network configuration found for the current ap(from /var/log/syslog). wireshark sees arp packages going from my wifi adapter to the router, but the router doesn't reply.do you have any ideas about what could that mean and how to troubleshoot this?solution:thanks to suggestion by peterph, i tried to create wpa_supplicant.conf and run wpa_supplicant as a standalone program both in foreground and background and then used wpa-conf wpa_supplicant.conf in /etc/network/interfaces.sudo wpa_supplicant -iwlan1 -c/etc/wpa_supplicant/wpa_supplicant.conf -dsudo wpa_supplicant -iwlan1 -c/etc/wpa_supplicant/wpa_supplicant.conf -bi had the first part of troubles (with spontaneous disconnect after status: associated) disappear, when i killed a running instance of networkmanager. it seems to have interfered.second part of trouble was with the 4-way handshake failing. it passed ok, when i disabled mac address filtration on the access point. my wifi interface's mac was in the list of available macs, but for some reason it still was failing to connect with mac filtering on the router.update 2: the problems are back. 4-way handshake is failing again. reload of the driver won't help.",
        "present_kp": [
            "debian",
            "wifi",
            "wpa supplicant"
        ],
        "absent_kp": [
            "networking"
        ],
        "predict": {
            "present_kp": [
                "wpa supplicant",
                "network configuration",
                "wifi adapter",
                "debian 7",
                "networkmanager",
                "4-way handshake"
            ],
            "absent_kp": [
                "linux kernel",
                "usb wifi",
                "mac address"
            ]
        }
    },
    {
        "text": "how do i differentiate 404 messaging with no dynamic language ability on the web server?. depending on the type of page visited (for example, static content vs. product page), we have a requirement to show different error messaging when the page requested is not found. given that we are on a two tier architecture and have no dynamic language ability on the web servers, we are using http status codes in the 400 range to display this messaging using the errordocument setting in apache configuration.these status codes, of course, show up in google webmaster tools as http errors rather than not found errors and there is concern that these 400 range errors will not cause search engines to remove these pages from their indexes.is there anyone else who has had this type of requirement with the limitation of no dynamic languages on the web server - and if so - have you solved this problem differently?thanks",
        "present_kp": [
            "search engines",
            "apache"
        ],
        "absent_kp": [
            "seo"
        ],
        "predict": {
            "present_kp": [
                "404 messaging",
                "web server",
                "http status codes",
                "apache configuration"
            ],
            "absent_kp": [
                "dynamic language",
                "search engine optimization",
                "two tier architecture"
            ]
        }
    },
    {
        "text": "how can i perform a triangle inside test in polygon meshes?. i have 3 vertices (v1, v2, v3) randomly selected on a regular triangle mesh. for these 3 vertices, i have computed the geodesic distance and the path (by using dijkstra) among them and formed a triangle-like surface as in the above figure. now, i have the vertices that lie in each path and can compute geodesic distances from a given vertex.what i want to do is to get the vertices or triangles that lies in triangle-like area. how can i do this?",
        "present_kp": [
            "mesh"
        ],
        "absent_kp": [
            "triangulation"
        ],
        "predict": {
            "present_kp": [
                "triangle",
                "polygon meshes",
                "geodesic distance",
                "dijkstra"
            ],
            "absent_kp": [
                "mesh processing",
                "computational geometry"
            ]
        }
    },
    {
        "text": "mounting a shared drive from windows with cifs. what i'm trying to achieve is mounting some drives shared on my network (one a time capsule, 3 shared drives from a windows 10 machine) on a raspberry pi 2 running ubuntu 14.04 with read and write permissions. i have been able to get the drives to mount by using this://10.0.1.2/movies /home/kev/networkdrives/movies cifs username=user,password=password 0 0obviously using the correct info for username and password.using this line in the /etc/fstab file achieves mounting the drives.i am able to read the files and copy them to my local storage but i can not write to the mounted drives and i can not find out what is wrong.this is new territory for me so your help is appreciated.",
        "present_kp": [
            "mount",
            "ubuntu"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "shared drive",
                "windows",
                "cifs",
                "raspberry pi",
                "ubuntu"
            ],
            "absent_kp": [
                "network drive",
                "file system",
                "permissions"
            ]
        }
    },
    {
        "text": "how to install d-link dwa-510 for debian 7?. i just bought a d-link dwa-510. but i can't install it on my debian 7 32 bits. and i want to use in a runlevel 3. how can i use this d-link? thanks!",
        "present_kp": [
            "debian"
        ],
        "absent_kp": [
            "wifi",
            "drivers"
        ],
        "predict": {
            "present_kp": [
                "d-link dwa-510",
                "debian 7"
            ],
            "absent_kp": [
                "wireless adapter",
                "linux"
            ]
        }
    },
    {
        "text": "how do i create sequentially numbered file names in bash?. i need a script that will create a file with the next file in a sequence. each execution of the script should only create one file and the script could be run zero or more times on any given day. the files should be named after the current date in format %y%m%d with the second file having -01 appended, the third file to be created on a given date would have -02 etc. for example:20170125.txt // first file create on the day.20170125-01.txt // 2nd file20170125-02.txt // 3rd fileso far i've got this super basic script that creates my first daily file but i'm stumped as to how to do the incremental numbering after that.#! /bin/bashdate='date +%y%m%d'touch $date.txt",
        "present_kp": [
            "bash"
        ],
        "absent_kp": [
            "shell script",
            "scripting"
        ],
        "predict": {
            "present_kp": [
                "bash",
                "file names",
                "script"
            ],
            "absent_kp": [
                "sequentially numbered",
                "incremental numbering"
            ]
        }
    },
    {
        "text": "not getting libglib2.0-dev (updated libraries) even after upgrading from ubuntu 14.04 to 16.06. i wanted to compile and install something-for-reddit from git.as i went with ./configure.ac it showed below error../configure: line 5088: glib_gsettings: command not found./configure: line 5089: syntax error near unexpected token '1.42.0'./configure: line 5089: 'gobject_introspection_check(1.42.0)'as i searched here in stackoverflow, it showed me that the glib_gsettings are found in libglib2.0-dev but as i tried to install it through sudo apt-get install libglib2.0again it threw below error reading package lists... donebuilding dependency tree reading state information... donenote, selecting 'libglib2.0-0-refdbg' for regex 'libglib2.0'note, selecting 'libglib2.0-cil-dev' for regex 'libglib2.0'note, selecting 'libglib2.0-tests' for regex 'libglib2.0'note, selecting 'libglib2.0-0-dbg' for regex 'libglib2.0'note, selecting 'libglib2.0-bin' for regex 'libglib2.0'note, selecting 'libglib2.0-cil' for regex 'libglib2.0'note, selecting 'libglib2.0-dbg' for regex 'libglib2.0'note, selecting 'libglib2.0-dev' for regex 'libglib2.0'note, selecting 'libglib2.0-doc' for regex 'libglib2.0'note, selecting 'libglib2.0-data' for regex 'libglib2.0'note, selecting 'libglib2.0-0' for regex 'libglib2.0'libglib2.0-0 is already the newest version (2.48.1-1~ubuntu16.04.1).libglib2.0-data is already the newest version (2.48.1-1~ubuntu16.04.1).libglib2.0-bin is already the newest version (2.48.1-1~ubuntu16.04.1).some packages could not be installed. this may mean that you haverequested an impossible situation or if you are using the unstabledistribution that some required packages have not yet been createdor been moved out of incoming.the following information may help to resolve the situation:the following packages have unmet dependencies: libglib2.0-0-dbg : depends: libglib2.0-0 (= 2.48.0-1ubuntu4) but 2.48.1-1~ubuntu16.04.1 is to be installed libglib2.0-0-refdbg : depends: libglib2.0-0 (= 2.48.0-1ubuntu4) but 2.48.1-1~ubuntu16.04.1 is to be installed libglib2.0-dev : depends: libglib2.0-0 (= 2.48.0-1ubuntu4) but 2.48.1-1~ubuntu16.04.1 is to be installed depends: libglib2.0-bin (= 2.48.0-1ubuntu4)i have upgraded the system from 14.04 to 16.04 one month back, did i miss something or am i doing something wrong now .the something for reddit git source is here. <url> you can check all the dependencies here too, package details: something-for-reddit-git 0.1-1",
        "present_kp": [
            "ubuntu",
            "libraries"
        ],
        "absent_kp": [
            "github"
        ],
        "predict": {
            "present_kp": [
                "libglib2.0-dev",
                "ubuntu"
            ],
            "absent_kp": [
                "dependency",
                "package manager"
            ]
        }
    },
    {
        "text": "what should be in the /etc/shadow file if i want my root account to be disabled?. on one of my machines it's root::somenumber[...]::: with somenumber[...] being the same as for my actual account (after what appears to be the encrypted passphrase) and the logcheck account (after :*:). on another machine it's root :!:somenumber[...]::: with somenumber[...] being the same for all accounts until the most recently added ones starting with postfix:*:.i didn't enter a root password during installation for both of these machines. however i accidentally set it for one of them and had to remove it again using the passwd -d root command. i'm running debian 9.1 with kde.what exactly should be in there if i wish for my root account to be locked (i use the sudo command)? are those file contents fine?and related to this would also be this question: how can i view a history of changes to the shadow file including info on which user changed what and when.",
        "present_kp": [
            "debian",
            "root",
            "shadow"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "etc/shadow",
                "root account",
                "debian"
            ],
            "absent_kp": [
                "password management",
                "system security"
            ]
        }
    },
    {
        "text": "ssh-agent terminating on network change. my work computer is a painfully out of date ubuntu 16.04 machine. i have it configured such that that lightdm invokes ssh-agent (not gnome-keyring-daemon) and ssh-agent invokes my session. this is not too dissimilar to how i have my personal laptop configured, except there i use arch and use login on a tty instead of lightdm. my .xinitrc invokes ssh-agent which invokes my window manager. in the end, in both cases, x has been started in an environment that is a child process of ssh-agent, so every shell gets the ssh_auth_sock environment variable.the odd thing about my work computer is that when i switch networks (either one wifi network to another or wired <-> wifi) there is a high probability that ssh-agent will terminate and my session's process will get reparented. this is super frustrating because i can't easily start a new ssh-agent and reparent my session under it - i have to log out and back in again.i have not found any log messages containing ssh-agent or even agent in the systemd journal nor dmesg. the only debugging options ssh-agent seems to have is the -d option to run it in the foreground, i have not tried that yet because it is invoked by lightdm and i don't know if i would even be able to see the output.are there any debugging steps or known issues i should be aware of with the ssh-agent from openssh-client 1:7.2p2-4ubuntu2.2 ?",
        "present_kp": [],
        "absent_kp": [
            "ssh agent"
        ],
        "predict": {
            "present_kp": [
                "ssh-agent",
                "network change",
                "ubuntu 16.04",
                "lightdm",
                "openssh-client"
            ],
            "absent_kp": [
                "debugging steps",
                "systemd journal",
                "dmesg"
            ]
        }
    },
    {
        "text": "diamondize a matrix. the task is to output a representation of the matrix where the top left element is on top, the anti-diagonal is the central row and the bottom right element is at the bottom.for example, consider the following matrix:1 23 45 6the diamond version of this matrix is: 1 3 25 4 6which would correspond to a nested array of [[1],[3,2],[5,4],[6]].this is my program:fnulq=y+y=<email>_1=qxqn<@qnh;fnulq=qxqn_@qn;=q>q1;pq;how it works:(q is implicitly assigned to the evaluation of input)fnulq=y+y=qxqn+y@qn;fnulq ; for n in range(len(q)): =y+y y = y+ =qxqn+y@qn q[n] = y+q[n]=q.tq; assign('q',transpose(q))fnulq=hx@qninh_1=qxqn<@qnh;fnulq for n in range(len(q)): =hx@qn h = q[n].index_of() inh_1 if h != -1: =qxqn<@qnh q[n] = q[n][:h]fnulq=qxqn_@qn;fnulq ; for n in range(len(q)): =qxqn_@qn q[n] = reverse(q[n])=q>q1; q = q[1:]pq; print(q)i am especially interested in advice on the readability of my code.link to my first attempt.",
        "present_kp": [],
        "absent_kp": [
            "pyth"
        ],
        "predict": {
            "present_kp": [
                "matrix",
                "diamond version"
            ],
            "absent_kp": [
                "nested array",
                "code readability"
            ]
        }
    },
    {
        "text": "how to convert html to pdf using php?. pdf or portable document format is a popular file type that is often used for online documents. it's great for distributing downloadable written content, and is frequently used by governments and businesses alike. because it's a format that's familiar to all, many applications allow the user to convert other document types to the pdf format. php is one programming language that has a built-in ability to convert to pdf. php scripts can be used to transform file types such as html into pdf files.",
        "present_kp": [],
        "absent_kp": [
            "software"
        ],
        "predict": {
            "present_kp": [
                "php",
                "html",
                "pdf"
            ],
            "absent_kp": [
                "document conversion",
                "programming"
            ]
        }
    },
    {
        "text": "fast field extraction with grep. the problemi have a 32m lines file with the following formattoken^iname^iurl$where ^i is the tab escape sequence, and $ is the end-of-line.i need to get the url corresponding to not more than 10k matches with the field name.what i've done is# get second columncut -f2 <myfile> |# find the word and line numbergrep -nwi <matchword> |# get just the numbercut -f1 -d ':' |# not more than 10khead -n10000and then, for each entry of the previous output# print line number sed -n '<number>{p;q}' <myfile># get 3rd fieldcut -f3now, this last operation with sed is ridiculously slow.i am wondering how to get the all of this by using grep only, or any other way that doesn't slow down after the first 1k matches.ideait would be just perfect to be able to operate grep on the whole line (without cut -f2), targeting only the second column, and then cut -f3, but i don't have a clue of how to do it.exampleline xyzqwertyuiop^ibananas are yellow^ihttp://mignons.cool$match word yellow in field name -> give me <url> is needed, because i don't want to match stuff in the field token and url.if i send to grep a cut of myfile, then i no longer have access to the url field, which i am interested in.input and expected outputinput file:mxp4edoy-ixkuwsuofs0eq^ilegal yellow pad paper^i0/3/3031.jpg$aes7tgmlvffbhousr9yy5q^ihelicopter parking only sign^i0/3/3032.jpg$8dl-vixsjg4y0fpx9f5kha^iwritten list ^i0/3/3033.jpg$xyvkzc3d_jswly8spl-zlq^ihelicopter parking only road sign^i0/3/3034.jpg$xf6zpvphcmfphp2mmt2fvg^irun menu windows programming^i0/3/3035.jpg$mcjvv2rxomitlbkmzlyiwq^icoffee mug^i0/3/3040.jpg$ziobhk_dlsn-q921kpjuta^icarpet^i0/3/3197.jpg$xfrbgomfvml0weqvact27a^iwater jugs^i0/3/3199.jpg$where ^i is the tab escape sequence, and $ is the end-of-line.match word helicopter.expected output (not more than 10k lines):0/3/3032.jpg0/3/3034.jpgpotential solutionsince the url field contains only numbers, i couldcut -f 2,3 <myfile> | grep <matchword> | cut -f2 | head -n10000but it would be nicer to grep the second field only...",
        "present_kp": [
            "grep"
        ],
        "absent_kp": [
            "shell script",
            "regular expression"
        ],
        "predict": {
            "present_kp": [
                "grep",
                "field extraction"
            ],
            "absent_kp": [
                "bash",
                "file processing"
            ]
        }
    },
    {
        "text": "bash: count # of words in each line of a document. i need to identify patterns in a text file for further analysis. so the input files may contain semi-structured text as follows;file1905:john: abc123: <email> us 920:eric: ericaa: <email> us 1000: rio: ri0ri0: <email> in file2 nathen <tab> <email> <tab> 764323545 <tab> ukthomas <tab> <email> <tab> 563363421 <tab> ukian <tab> <email> <tab> <phone> <tab> spnumber of words in a line may vary for each document. delimiter also vary but unique for each document. what i want is to count number of words per each line in each document.output would be:for file1 5 5 5 5 for file2 4 4 4 4i want generalize this for any file with any delimiter. it could be - | : \\space+ ab+. some files are as follows:| <email> | er34532 | | <email> | 764474 |",
        "present_kp": [
            "bash",
            "patterns"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "bash",
                "count words",
                "document",
                "text file",
                "delimiter"
            ],
            "absent_kp": [
                "pattern analysis",
                "semi-structured text",
                "generalization"
            ]
        }
    },
    {
        "text": "how to edit a person's name in the new gmail chat?. previously in gmail chat, i was just able to click on the name of a person to edit his name. now, the name acts as a link which opens contacts, where the editing can happen. the old way was faster. can i edit the names the old way somehow?for example, here aardvark is a link, and is not directly editable.",
        "present_kp": [
            "gmail",
            "chat"
        ],
        "absent_kp": [
            "gmail contacts"
        ],
        "predict": {
            "present_kp": [
                "gmail chat",
                "edit a person's name"
            ],
            "absent_kp": [
                "contacts",
                "gmail interface"
            ]
        }
    },
    {
        "text": "auto-populate google apps hangouts roster?. i want every user in my organization to have every other user in their hangouts roster. is there a way to do this, or even a way to manually populate those rosters for them to give them a starting point?",
        "present_kp": [
            "google apps"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "google apps hangouts",
                "roster"
            ],
            "absent_kp": [
                "auto-populate",
                "organization"
            ]
        }
    },
    {
        "text": "lualatex not available in debian wheezy. i want to use lualatexfor compiling latex document based on luatex. i have installed those packages: luatex, texlive-binaries, texlive-luatex but no lualatexbinary can be found. what did i miss?",
        "present_kp": [
            "debian",
            "latex"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "lualatex",
                "debian wheezy",
                "luatex",
                "texlive-binaries",
                "texlive-luatex"
            ],
            "absent_kp": [
                "latex document",
                "binary"
            ]
        }
    },
    {
        "text": "even after i press publish release, github release stays as draft. creating a github release usually works well, but for one particular project i can't seem to be able to publish: the draft stays a draft and never gets published.it happens both for forks and for projects i created from scratch.happens with both firefox and chrome.",
        "present_kp": [
            "github"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "github release",
                "draft"
            ],
            "absent_kp": [
                "github actions",
                "version control"
            ]
        }
    },
    {
        "text": "dconf-warning **: failed to commit changes to dconf: the connection is closed. whenever i open any software through terminal i get following errors and eventually the software opensdconf-warning **: failed to commit changes to dconf: the connection is closed(gedit:3609): dconf-warning **: failed to commit changes to dconf: the connection is closed(gedit:3609): dconf-warning **: failed to commit changes to dconf: the connection is closederror creating proxy: the connection is closed (g-io-error-quark, 18)error creating proxy: the connection is closed (g-io-error-quark, 18)error creating proxy: the connection is closed (g-io-error-quark, 18)error creating proxy: the connection is closed (g-io-error-quark, 18)error creating proxy: the connection is closed (g-io-error-quark, 18)what can be the possible issue?",
        "present_kp": [
            "dconf"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "dconf",
                "terminal"
            ],
            "absent_kp": [
                "linux",
                "gnome"
            ]
        }
    },
    {
        "text": "banning unresolved ip addresses. my server has run out of memory a few times in the past two days, because of which the site crashed. i checked awstats and found that hundreds of unresolved ip addresss have been logged, and the number of hits from them seems unreasonable:i'm going to ban the top 10 ip addresses as they all seem spammy to me. but there are other hundreds of ip addresses that seems to be causing thousands of hits, and i really don't know if they are legitimate request from users using browsers. i could ban all of them using htaccess, but that doesn't seem very practical nor is a long term solution. (the site gets around 3 million pageviews/month.)my questions are:how do i filter out legitimate users from unwanted bots or ip addresses that are scraping content?should i go ahead and ban all unresolved ip addresses?is there an automated way of banning spammy ip addresses?",
        "present_kp": [
            "ip address"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "ip addresses",
                "server",
                "memory",
                "site",
                "awstats",
                "hits",
                "htaccess",
                "pageviews",
                "bots",
                "content scraping"
            ],
            "absent_kp": [
                "dns",
                "firewall",
                "load balancing",
                "cloudflare",
                "security",
                "traffic analysis",
                "ip filtering",
                "rate limiting",
                "web application firewall",
                "log analysis"
            ]
        }
    },
    {
        "text": "show that tqbf $ otin$ space$((\\log{n})^4)$?. how do i show that tqbf $ otin$ space$((\\log{n})^4)$? i know that tqbf is pspace complete, but is this the right approach?",
        "present_kp": [],
        "absent_kp": [
            "complexity theory"
        ],
        "predict": {
            "present_kp": [
                "tqbf",
                "pspace complete"
            ],
            "absent_kp": [
                "complexity theory",
                "logarithmic space"
            ]
        }
    },
    {
        "text": "new site not showing in google for unique name. i have launched a site for myself and now i'm doing some simple seo for it.the site uses my name and it's unique and has no competition on search engines, but i don't know why google crawlers haven't done anything on my site.i have added my site's url to google (via addurl) and i have also signed in to google webmaster tools.the title and the url of my site is also related to my name, but even when i google my name google returns nothing about my site, and google webmaster tools also shows no keywords for my site. do you have any idea why this happens?[update] ok, i won't link my site here, i launched it about 5 days ago, and it's on my name (completely unique). i just want it to be shown everytime some one googles my name - just that simple, but it seems google hasn't done anything with my site yet, is this normal?",
        "present_kp": [
            "google",
            "seo"
        ],
        "absent_kp": [
            "google search console"
        ],
        "predict": {
            "present_kp": [
                "google",
                "seo",
                "google webmaster tools"
            ],
            "absent_kp": [
                "search engine optimization",
                "website indexing"
            ]
        }
    },
    {
        "text": "automatically starting smuxi-server. smuxi (isn't that a weird name?) is an irc client, which has a decoupled server and client setup. the server sits in some always-on machine in the cloud and the client connects to it from a local machine. this is particularly useful if the client machine does not have good or reliable connectivity.if the client loses the connection, it can reconnect to the server, and not lose any of the ongoing chat.so, that brings me to my question. the smuxi server documentation is a little sparse, it saysif you want the smuxi-server to automatically start in the background when your system boots, continue reading the following sections. this is highly dependent of your operating system as each system provides its own way to auto start services.there are then some highly instructive blank spaces starting with words like debian, ubuntu, and other linux.the section then hasto always start the smuxi-server automatically when the linux server boots, add this to your /etc/rc.local file:sudo -u your_linux_user bash -c 'nohup smuxi-server > $home/smuxi-server.log &'i'm not sure whether i should be taking this advice.i use debian, and this script has the wordsthis script is executed at the end of each multiuser runlevel.i'm not sure what that means. does that mean it executes multiple times? isn't that a bad thing?anyway, i'm looking for advice (or possibly scripts) for a way to start the server automatically on boot, and also a way to run it manually and have it background automatically. i could run it inside screen, but that feels a little... hacky.since i'm using debian wheezy, i'd like a method that would work with that systems default setup.",
        "present_kp": [
            "debian"
        ],
        "absent_kp": [
            "init script"
        ],
        "predict": {
            "present_kp": [
                "smuxi-server",
                "debian"
            ],
            "absent_kp": [
                "irc client",
                "linux"
            ]
        }
    },
    {
        "text": "md5 string and file different. why do i become a different hash when i try:md5 <<< hellomd5 -s hellois it because of a possible line break in the first example?",
        "present_kp": [],
        "absent_kp": [
            "shell",
            "hashsum"
        ],
        "predict": {
            "present_kp": [
                "md5",
                "hash"
            ],
            "absent_kp": [
                "line break",
                "string comparison"
            ]
        }
    },
    {
        "text": "guessing game (heads or tails). you guess heads or tails by clicking one of the buttons on easygui. if it's right, you will get a good job message. if it's wrong, you will get a wrong message! after that, there is an option to play again.please give me some feedback on how i can make my code better, if there is room for improvement.import randomimport timeimport easyguiimport syswhile true: rand = random.choice([heads, tails]) firstguess = easygui.buttonbox(pick one, choices= [heads, tails]) if firstguess == rand: easygui.msgbox(wow you win!) else: easygui.msgbox(sorry you guessed wrong!) time.sleep(2) answer = easygui.buttonbox(play again?, choices=[yes,no]) if answer == yes: pass else: breakeasygui.msgbox(ok, see you later!)sys.exit(0)",
        "present_kp": [
            "game"
        ],
        "absent_kp": [
            "python",
            "beginner"
        ],
        "predict": {
            "present_kp": [
                "guessing game",
                "easygui"
            ],
            "absent_kp": [
                "python",
                "random module"
            ]
        }
    },
    {
        "text": "doubt when compiling usb/ip drivers. i need to install a usb 3g modem to a windows virtual machine (running on ovm).the modem will be plugged on a linux host. (would windows be easier?)i don't understand what the following readme file says. for newer kernels ( >=2.6.28 ), try linux-staging code!this directory contains the source code of usbip drivers for mainline kernel.[how to make usb/ip drivers] 1. cd $(top)/drivers/{version}/ 2. make ksource=/usr/src/kernel-source-2.6.20 ksource is the directory that your kernel was built. 3. if succeed, usbip_common_mod.ko, vhci-hcd.ko and usbip.ko are built. 4. copy these kernel modules to client and server hosts. 5. don't forget to make usb/ip tools. see $(top)/src/readme.the result of my cat /proc/version is:linux version 2.6.39-400.209.1.el6uek.x86_64 (<email>) (gcc version 4.4.6 20110731 (red hat 4.4.6-3) (gcc) )what is this linux-staging code? it says i'll need it.another thing is that the make call requires the ksource location, but i've checked it and it's empty. i should download and extract the 2.6.39 kernel source there, right?where should i put the generated .ko files as instructed by item 4?",
        "present_kp": [
            "kernel",
            "compiling",
            "drivers"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "usb/ip drivers",
                "linux host",
                "kernel modules"
            ],
            "absent_kp": [
                "windows virtual machine",
                "usb 3g modem",
                "linux-staging code"
            ]
        }
    },
    {
        "text": "updating a news pages written in html. i was wondering how to update a page with new stuff like a new news article or something. (the website is html based). the only thing i could think of is by uploading and replacing the whole .html page with the new article etc. but isn't there an easier way? like in wordpress where you just sign in and write the article and post it.",
        "present_kp": [
            "html"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "html",
                "news pages"
            ],
            "absent_kp": [
                "wordpress",
                "content management system"
            ]
        }
    },
    {
        "text": "can speech dysarthria occur in schizophrenia without other neurological or medication-induced disruptions?. as i understand it, acquired dysarthria of speech is caused due to problems with motor neurons or other neurological, cerebral and peripheral, conditions in the cns affecting those. and schizophrenia is primarily a problem in mesolimbic and mesocortical dopaminergic pathways, amongst possibly other problems. can a purely schizophrenic or psychiatric condition without neurological diagnoses cause dysarthria as a symptom by itself? or is acquired dysarthria always a sign of a comorbid neurological diagnosis?",
        "present_kp": [
            "schizophrenia",
            "speech"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "speech dysarthria",
                "schizophrenia",
                "neurological",
                "medication-induced disruptions",
                "motor neurons",
                "cns",
                "mesolimbic",
                "mesocortical dopaminergic pathways",
                "psychiatric condition",
                "comorbid neurological diagnosis"
            ],
            "absent_kp": [
                "acquired dysarthria",
                "cerebral",
                "peripheral conditions"
            ]
        }
    },
    {
        "text": "how to login by ssh when default shell is wrong. i have change default root shell to wrong path. now the first line of /etc/passwd is look like the following string:root:x:0:0:root:/root:/usr/bin/bashthen i logout from server and now cannot log in by root.there is no other user on server.i know, here is much stupid mistakes, but how i can fix it?access to server is available only by ssh.updatethe mission is impossible.i found a way to execute any command as www-data user.how can i change /etc/passwd as non-sudo user?",
        "present_kp": [
            "ssh"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "ssh",
                "default shell",
                "root",
                "login"
            ],
            "absent_kp": [
                "server access",
                "password recovery"
            ]
        }
    },
    {
        "text": "small command-line helper tool. i've coded a small command line helper tool for this library i'm working on. the library provides tools for the use of virtual texturing on ios devices (mainly games).this little command line helper is still pretty much a prototype that i coded as quick as i could, for testing a new file format. some feedback would be appreciated before i expand it further:// virtual texturing library:#include vt_tool_image.hpp#include vt_tool_pagefile_builder.hpp#include vt_tool_platform_utils.hpp// standard library:#include <cstdarg>#include <cstdio>#include <cstdlib>#include <cstring>#include <string>namespace {// ======================================================// local data:// ======================================================vt::tool::pagefilebuilderoptions cmdlineopts;std::string inputfile, outputfile;// ======================================================// printhelpandexit()// ======================================================void printhelpandexit(){ std::printf( usage: $ vtmake <input_file> <output_file> [--flags=] flags accepted: --help : prints help text with list of commands. --filter : (str) type of mipmapping filter: box, tri, quad, cubic, bspline, mitchell, lanczos, sinc, kaiser. --page_size : (int) total page size in pixels, including border. --content_size : (int) size in pixels of page content, not including border. --border_size : (int) size in pixels of the page border. --max_levels : (int) max mipmap levels to generate. --flip_v_src : (bool) flip the source image vertically. --flip_v_tiles : (bool) flip each individual tile/page vertically. --stop_on_1_mip : (bool) stop subdividing when mip 0 is reached. --add_debug_info : (bool) print debug text to each page. --dump_images : (bool) dump each page as an image file (tga format). --verbose : (bool) print stuff to stdout while running. ); std::exit(0);}// ======================================================// error():// ======================================================void error(const char * format, ...){ va_list valist; char buffer[1024]; va_start(valist, format); std::vsnprintf(buffer, sizeof(buffer), format, valist); va_end(valist); buffer[sizeof(buffer) - 1] = ''; // ensure a null at the end throw vt::tool::pagefilebuildererror(buffer);}// ======================================================// parsefiltername():// ======================================================vt::tool::filtertype parsefiltername(const char * str){ // find the value after the '=' sign, if any: while ((*str != '=') && (*str != '')) { ++str; } if (*str == '=') { ++str; } if (std::strcmp(str, box ) == 0) { return vt::tool::filtertype::box; } if (std::strcmp(str, tri ) == 0) { return vt::tool::filtertype::triangle; } if (std::strcmp(str, quad ) == 0) { return vt::tool::filtertype::quadratic; } if (std::strcmp(str, cubic ) == 0) { return vt::tool::filtertype::cubic; } if (std::strcmp(str, bspline ) == 0) { return vt::tool::filtertype::bspline; } if (std::strcmp(str, mitchell) == 0) { return vt::tool::filtertype::mitchell; } if (std::strcmp(str, lanczos ) == 0) { return vt::tool::filtertype::lanczos; } if (std::strcmp(str, sinc ) == 0) { return vt::tool::filtertype::sinc; } if (std::strcmp(str, kaiser ) == 0) { return vt::tool::filtertype::kaiser; } std::printf(warning: unknown filter '%s'! defaulting to box filter. , str); return vt::tool::filtertype::box;}// ======================================================// parseint():// ======================================================int parseint(const char * str){ // find the value after the '=' sign, if any: while ((*str != '=') && (*str != '')) { ++str; } if (*str == '=') { ++str; } return std::stoi(str);}// ======================================================// parsebool():// ======================================================bool parsebool(const char * str){ // find the value after the '=' sign, if any: while ((*str != '=') && (*str != '')) { ++str; } if (*str == '=') { ++str; } if ((std::strcmp(str, false) == 0) || (std::strcmp(str, no) == 0) || (std::strcmp(str, 0) == 0)) { return false; } // assume true for anything else, including an invalid value or an empty string. // (results in true for --flag with no =value part) return true;}// ======================================================// startswith():// ======================================================bool startswith(const char * str, const char * prefix){ const size_t prefixlen = std::strlen(prefix); if (prefixlen == 0) { return false; } return std::strncmp(str, prefix, prefixlen) == 0;}// ======================================================// parsecmdline():// ======================================================void parsecmdline(const int argc, const char * argv[]){ // possible --help call if ((argc == 2) && startswith(argv[1], --help)) { printhelpandexit(); } // must have at least argv[0], in_file and out_file if (argc < 3) { error(not enough arguments!); } /* argc[0] == vtmake (prog name) */ inputfile = argv[1]; outputfile = argv[2]; for (int i = 3; i < argc; ++i) { if (startswith(argv[i], --help)) { printhelpandexit(); } else if (startswith(argv[i], --filter)) { cmdlineopts.texturefilter = parsefiltername(argv[i]); } else if (startswith(argv[i], --page_size)) { cmdlineopts.pagesizepixels = parseint(argv[i]); } else if (startswith(argv[i], --content_size)) { cmdlineopts.pagecontentsizepixels = parseint(argv[i]); } else if (startswith(argv[i], --border_size)) { cmdlineopts.pagebordersizepixels = parseint(argv[i]); } else if (startswith(argv[i], --max_levels)) { cmdlineopts.maxmiplevels = parseint(argv[i]); } else if (startswith(argv[i], --flip_v_src)) { cmdlineopts.flipsourcevertically = parsebool(argv[i]); } else if (startswith(argv[i], --flip_v_tiles)) { cmdlineopts.fliptilesvertically = parsebool(argv[i]); } else if (startswith(argv[i], --stop_on_1_mip)) { cmdlineopts.stopon1pagemip = parsebool(argv[i]); } else if (startswith(argv[i], --add_debug_info)) { cmdlineopts.adddebuginfotopages = parsebool(argv[i]); } else if (startswith(argv[i], --dump_images)) { cmdlineopts.dumppageimages = parsebool(argv[i]); } else if (startswith(argv[i], --verbose)) { cmdlineopts.stdoutverbose = parsebool(argv[i]); } else { std::printf(warning: unknown command line argument: '%s' , argv[i]); } } if (cmdlineopts.stdoutverbose) { std::printf(input file: \\%s\\n, inputfile.c_str()); std::printf(output file: \\%s\\n, outputfile.c_str()); cmdlineopts.printself(); }}// ======================================================// runpagefilebuilder():// ======================================================void runpagefilebuilder(){ if (inputfile.empty()) { error(no input filename!); } if (outputfile.empty()) { error(no output filename!); } vt::tool::pagefilebuilder pagefilebuilder(inputfile, outputfile, cmdlineopts); pagefilebuilder.generatepagefile(); std::printf(done! );}} // namespace {}// ======================================================// main():// ======================================================int main(int argc, const char * argv[]){ try { parsecmdline(argc, argv); runpagefilebuilder(); return 0; } catch (std::exception & e) { std::printf(error: %s , e.what()); return -1; }}the main purpose of this code, as you can see, is to parse and validate command line args. the heavy work is then done by the library. it looks very c-ish, i'll admit. since i wrote it quickly for testing, i didn't bother much. also, command args validation is still pretty weak. i might consider refactoring it into a class and using less char* and more std::string.",
        "present_kp": [],
        "absent_kp": [
            "c++",
            "c++11",
            "console"
        ],
        "predict": {
            "present_kp": [
                "command line",
                "virtual texturing",
                "ios devices"
            ],
            "absent_kp": [
                "c++",
                "file format",
                "command line arguments"
            ]
        }
    },
    {
        "text": "can i use a project code which has new bsd license but uses a gpl license library?. i want to use the icsopenvpn project source code in my commercial application.if we see the icsopenvpn project, it states that its license is new bsd but the libopenvpn.so library it uses is under gnu gplv2 license.as per faq for version 2 of gnu gpl if a library is released under the gpl (not the lgpl), does that mean that any program which uses it has to be under the gpl? the answer says: yes, because the program as it is actually run includes the library.also, how could icsopenvpn change the license to new bsd?",
        "present_kp": [
            "gpl",
            "bsd license"
        ],
        "absent_kp": [
            "android"
        ],
        "predict": {
            "present_kp": [
                "new bsd license",
                "gpl license",
                "icsopenvpn project",
                "libopenvpn.so library",
                "gnu gplv2 license"
            ],
            "absent_kp": [
                "commercial application",
                "license compatibility",
                "open source licensing"
            ]
        }
    },
    {
        "text": "my reverse-ssh tunnel is using keepalives but they're not helping. i have an ssh client machine picard behind multiple unreliable internet connections - all with nat.i have my server time, reliable with a static ip.i want to be able to access picard thorugh time. i've done this before:ssh -n -r 19999:localhost:22 <email> works, but if there is a problem it exits and does not restart, and it doesn't start on boot, so now i add a sydtemd service to run:/bin/bash -c while true; do /usr/bin/ssh -i <unencrypted key> -o serveraliveinterval=10 -v -o serveralivecountmax=6 -n -r 19999:localhost:22 <email> sleep 5; donewhile true ... sleep 5 re-runs ssh if it exits-o serveraliveinterval=10 sends a keep-alive every 10 secnods-o serveralivecountmax=6 exits if 6 keep-alives go out with no response-v keeps debug info in /var/log/messages through systemdon the server side i added a couple of lines to sshd_config:keepalive yesclientaliveinterval 10clientalivecountmax 6same idea as the client - break the connection after 60s of inactivity.unfortunately it seems to take a lot longer than a minute to restart:< tunnel is up and keepalives are coming in >jun 7 17:31:02 picard bash[135]: debug1: client_input_global_request: rtype <email> want_reply 1jun 7 17:31:12 picard bash[135]: debug1: client_input_global_request: rtype <email> want_reply 1jun 7 17:31:15 picard bash[135]: debug1: client_input_channel_open: ctype forwarded-tcpip rchan 2 win <phone> max 32768jun 7 17:31:15 picard bash[135]: debug1: client_request_forwarded_tcpip: listen localhost port 19998, originator 127.0.0.1 port 38267jun 7 17:31:15 picard bash[135]: debug1: connect_next: host localhost ([127.0.0.1]:22) in progress, fd=4jun 7 17:31:15 picard bash[135]: debug1: channel 0: new [127.0.0.1]jun 7 17:31:15 picard bash[135]: debug1: confirm forwarded-tcpipjun 7 17:31:15 picard bash[135]: debug1: channel 0: connected to localhost port 22jun 7 17:31:20 picard systemd-logind[137]: new session 1 of user main_username.< i break eth0 and plug it back in after nm sees it's down >< eth0 is back up within a few seconds >< nothing happens with my ssh connection for a long time >jun 7 17:54:16 picard bash[135]: write failed: broken pipejun 7 17:54:22 picard bash[135]: openssh_6.1p1, openssl 1.0.1c-fips 10 may 2012jun 7 17:54:22 picard bash[135]: debug1: reading configuration data /etc/ssh/ssh_configjun 7 17:54:22 picard bash[135]: debug1: /etc/ssh/ssh_config line 50: applying options for *jun 7 17:54:22 picard bash[135]: debug1: connecting to my.domain [123.234.123.234] port 22.jun 7 17:54:22 picard bash[135]: debug1: connection established.jun 7 17:54:23 picard bash[135]: debug1: identity file /home/test/.ssh/id_rsa type 1jun 7 17:54:23 picard bash[135]: debug1: identity file /home/test/.ssh/id_rsa-cert type -1jun 7 17:54:23 picard bash[135]: debug1: remote protocol version 2.0, remote software version openssh_5.8p1 debian-1ubuntu3jun 7 17:54:23 picard bash[135]: debug1: match: openssh_5.8p1 debian-1ubuntu3 pat openssh_5*jun 7 17:54:23 picard bash[135]: debug1: enabling compatibility mode for protocol 2.0jun 7 17:54:23 picard bash[135]: debug1: local version string ssh-2.0-openssh_6.1jun 7 17:54:23 picard bash[135]: debug1: ssh2_msg_kexinit sentjun 7 17:54:23 picard bash[135]: debug1: ssh2_msg_kexinit receivedjun 7 17:54:23 picard bash[135]: debug1: kex: server->client aes128-ctr hmac-md5 nonejun 7 17:54:23 picard bash[135]: debug1: kex: client->server aes128-ctr hmac-md5 nonejun 7 17:54:23 picard bash[135]: debug1: ssh2_msg_kex_dh_gex_request(1024<1024<8192) sentjun 7 17:54:23 picard bash[135]: debug1: expecting ssh2_msg_kex_dh_gex_groupjun 7 17:54:23 picard bash[135]: debug1: ssh2_msg_kex_dh_gex_init sentjun 7 17:54:23 picard bash[135]: debug1: expecting ssh2_msg_kex_dh_gex_replyjun 7 17:54:23 picard bash[135]: debug1: server host key: rsa 7a:19:72:9d:f5:39:f5:03:cf:16:b2:ee:fc:a4:e6:bajun 7 17:54:23 picard bash[135]: debug1: host 'my.domain' is known and matches the rsa host key.jun 7 17:54:23 picard bash[135]: debug1: found key in /home/test/.ssh/known_hosts:1jun 7 17:54:23 picard bash[135]: debug1: ssh_rsa_verify: signature correctjun 7 17:54:23 picard bash[135]: debug1: ssh2_msg_newkeys sentjun 7 17:54:23 picard bash[135]: debug1: expecting ssh2_msg_newkeysjun 7 17:54:23 picard bash[135]: debug1: ssh2_msg_newkeys receivedjun 7 17:54:23 picard bash[135]: debug1: roaming not allowed by serverjun 7 17:54:23 picard bash[135]: debug1: ssh2_msg_service_request sentjun 7 17:54:23 picard bash[135]: debug1: ssh2_msg_service_accept receivedjun 7 17:54:23 picard bash[135]: debug1: authentications that can continue: publickey,passwordjun 7 17:54:23 picard bash[135]: debug1: next authentication method: publickeyjun 7 17:54:23 picard bash[135]: debug1: offering rsa public key: /home/test/.ssh/id_rsajun 7 17:54:23 picard bash[135]: debug1: server accepts key: pkalg ssh-rsa blen 279jun 7 17:54:23 picard bash[135]: debug1: read pem private key done: type rsajun 7 17:54:24 picard bash[135]: debug1: authentication succeeded (publickey).jun 7 17:54:24 picard bash[135]: authenticated to my.domain ([123.234.123.234]:22).jun 7 17:54:24 picard bash[135]: debug1: remote connections from localhost:19999 forwarded to local address localhost:22jun 7 17:54:24 picard bash[135]: debug1: requesting <email> 7 17:54:24 picard bash[135]: debug1: entering interactive session.jun 7 17:54:24 picard bash[135]: debug1: remote forward success for: listen 19999, connect localhost:22jun 7 17:54:24 picard bash[135]: debug1: all remote forwarding requests processedjun 7 17:54:44 picard bash[135]: debug1: client_input_global_request: rtype <email> want_reply 1jun 7 17:54:45 picard bash[135]: debug1: client_input_channel_open: ctype forwarded-tcpip rchan 2 win <phone> max 32768jun 7 17:54:45 picard bash[135]: debug1: client_request_forwarded_tcpip: listen localhost port 19999, originator 127.0.0.1 port 60222jun 7 17:54:45 picard bash[135]: debug1: connect_next: host localhost ([127.0.0.1]:22) in progress, fd=4jun 7 17:54:45 picard bash[135]: debug1: channel 0: new [127.0.0.1]jun 7 17:54:45 picard bash[135]: debug1: confirm forwarded-tcpipjun 7 17:54:45 picard bash[135]: debug1: channel 0: connected to localhost port 22jun 7 17:54:50 picard systemd-logind[137]: new session 3 of user main_username.< whenever i connect the keepalive debug messages stop coming, not sure if this is normal >i'm sure i've overlooked something. i've seen some projects like autossh that do pretty much the same thing i'm doing now, but i'd like to be able to fix this if possible. how do i get the delay down to 2-3 minutes instead of 23 minutes?",
        "present_kp": [
            "ssh",
            "sshd"
        ],
        "absent_kp": [
            "ssh tunneling"
        ],
        "predict": {
            "present_kp": [
                "reverse-ssh tunnel",
                "keepalives",
                "ssh client",
                "nat",
                "static ip",
                "systemd service",
                "debug info",
                "sshd_config",
                "broken pipe",
                "autossh"
            ],
            "absent_kp": [
                "vpn",
                "firewall",
                "port forwarding",
                "dynamic dns",
                "tcpdump"
            ]
        }
    },
    {
        "text": "how do i find out the license for each of my installed applications/packages?. i am using ubuntu-15.10.i have installed many applications apart from vanilla installations.now, i would like to find out how many installed packages are licensed under the gpl or third-party licenses (e.g. fluendo).is there any way to find this out? or do i need to check manually each and every license of each installed application?edit:following snippet i used to list out the name of various installed license files.find /usr/share/doc -type f -name copyright -exec grep license\\: {} + | cut -f3 -d: | sort -u",
        "present_kp": [
            "ubuntu",
            "licenses"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "license",
                "ubuntu"
            ],
            "absent_kp": [
                "gpl",
                "third-party licenses"
            ]
        }
    },
    {
        "text": "zsh: quickly bookmark commands. i just had an idea, i'm sure it must exist though i could not find anything on the web.this topic gets close to my idea but not enough:how to quickly store and access often used commands?i'd like to have a .bookmark.zsh file in which i store commands, not often used (i could create an alias for these), but more those which were a pain in the ass to write down, and that i could use some other times.like i just typed: rake db:drop --trace && echo 'dropped' && \\ rake db:create --trace && echo 'created' && \\ rake db:migrate --trace && echo 'migrated' && \\ rake db:seed --trace && echo 'seed'and i want to save it, so i type bm -save 'description' and it adds rake db:drop --trace && echo 'dropped' && \\ rake db:create --trace && echo 'created' && \\ rake db:migrate --trace && echo 'migrated' && \\ rake db:seed --trace && echo 'seed' # description'in my .bookmark.zsh file.and then i can do bm -find 'description' (or ideally 'descr' 'desc' etc.) and i find the command back. like bookmarks work!i'm pretty bad at shell so any tip would be super nicely welcomed!",
        "present_kp": [
            "zsh"
        ],
        "absent_kp": [
            "command history"
        ],
        "predict": {
            "present_kp": [
                "zsh",
                "bookmark commands"
            ],
            "absent_kp": [
                "shell scripting",
                "command line"
            ]
        }
    },
    {
        "text": "modeline for dell u2415 1920x1200 resolution. i'm trying here to run my secondary display (u2415) on the native resolution 1920x1200. xrandr unfortunately reports only 1920x1080, which is not satisfactory.$ xrandr screen 0: minimum 8 x 8, current 3286 x 1080, maximum 32767 x 32767lvds1 connected 1366x768+1920+312 (normal left inverted right x axis y axis) 344mm x 194mm 1366x768 59.97*+ 1024x768 60.00 800x600 60.32 56.25 640x480 59.94 dp1 disconnected (normal left inverted right x axis y axis)hdmi1 disconnected (normal left inverted right x axis y axis)vga1 connected 1920x1080+0+0 (normal left inverted right x axis y axis) 509mm x 286mm 1920x1080 60.00*+ 1680x1050 59.95 1280x1024 75.02 60.02 1440x900 74.98 59.89 1280x960 60.00 1280x800 59.81 1152x864 75.00 1024x768 75.08 70.07 60.00 800x600 75.00 60.32 640x480 75.00 60.00 os:linux arch 3.19.3-3-arch #1 smp preempt wed apr 8 14:10:00 cest 2015 x86_64 gnu/linuxhw:$ lspci -nnks 00:02.000:02.0 vga compatible controller [0300]: intel corporation core processor integrated graphics controller [8086:0046] (rev 02) subsystem: lenovo device [17aa:3920] kernel driver in use: i915 kernel modules: i915i've googled it and it seems that intel gma has no problem whatsoever to run 1920x1200 via vga.the following command gives me my modeline which i've tried and which works, but not quite really - it produces worse quality image than on the 1920x1080 and entire desktop is shifted to the left. furthermore, when i go the osd menus on the display i see that i'm getting 1600x1200 60hz :($ gtf <phone> 60 -x # 1920x1200 @ 60.00 hz (gtf) hsync: 74.52 khz; pclk: 193.16 mhz modeline 1920x1200_60.00 193.16 <phone> 2256 2592 <phone> <phone> -hsync +vsynci've also tried something called 915resolution, well it's patched version from this location. but it didn't work very well either. by following examples in their readme i've tried:# ./915resolution 30 1920 1200intel 800/900 series vbios hack : version 0.5.3chipset: hdgraphicsbios: type 1mode table offset: $c0000 + $268mode table entries: 36patch mode 30 to resolution 1920x1200 complete# ./915resolution -lintel 800/900 series vbios hack : version 0.5.3chipset: hdgraphicsbios: type 1mode table offset: $c0000 + $268mode table entries: 36mode 30 : 640x480, 8 bits/pixelmode 32 : 800x600, 8 bits/pixelmode 34 : 1024x768, 8 bits/pixelmode 38 : 1280x1024, 8 bits/pixelmode 3a : 1600x1200, 8 bits/pixelmode 3c : 1920x1440, 8 bits/pixelmode 41 : 640x480, 16 bits/pixelmode 43 : 800x600, 16 bits/pixelmode 45 : 1024x768, 16 bits/pixelmode 49 : 1280x1024, 16 bits/pixelmode 4b : 1600x1200, 16 bits/pixelmode 4d : 1920x1440, 16 bits/pixelmode 50 : 640x480, 32 bits/pixelmode 52 : 800x600, 32 bits/pixelmode 54 : 1024x768, 32 bits/pixelmode 58 : 1280x1024, 32 bits/pixelmode 5a : 1600x1200, 32 bits/pixelmode 5c : 1920x1440, 32 bits/pixelyou can see, that mode 30 remains on 640x480...and finally last but not least important, i'm running all this via qumox vga to hdmi converter. their specs clearly state, that they support 1920x1200 resolutions, but i've started to doubt it.so dear sirs, what else i may try, to make my ultra cool display operate on the correct resolution??thanks",
        "present_kp": [
            "kernel modules",
            "xrandr",
            "pci"
        ],
        "absent_kp": [
            "x11"
        ],
        "predict": {
            "present_kp": [
                "modeline",
                "resolution",
                "vga",
                "intel gma",
                "1920x1200"
            ],
            "absent_kp": [
                "linux",
                "xrandr",
                "hdmi",
                "qumox converter"
            ]
        }
    },
    {
        "text": "coffeescript beautification and refactoring. as much as i try, i cannot seem to get this coffeescript code to look beautiful (i'd like to think it is possible). i have tried both javascript and coffeescript. just to be clear, this code works fine, but it is hard to read, for reasons that i am unable to pinpoint.how can it be refactored, reorganized, and what changes to coding style can be made to make it more appealing to read?define [ plugins/ui/ui, ./js/highlight], (ui, highlight) -> editor = {} jquery ($) -> # a widget to view source code. $.widget 'core.editor', _create: -> $editor = $(this.element) $editor .addclass('editor') .append($('<ul spellcheck=false contenteditable> <li></li> </ul>')) # move the gutter along with the editable area. this.lines().bind 'scroll', (event) -> $this = $(this) $this.siblings(.gutter).css(top: $this.scrolltop() * -1) # highlight the sourceview using the given language. # the language's json rule file is loaded. highlight: (language) -> this.language = language require [text!plugins/editor/js/#{ language }.json], (json) => this._rules = json.parse(json) return # update the 'left' of the '<ul>' based on the gutter width. # each time the number of digits in the gutter changes, it becomes wider or # narrower, and the editor needs the shift accordingly. updategutterwidth: () -> # the '8' is the gutter's left padding. this.lines().css(left: this.gutter().width() + 8) # add or remove line numbers if the number of lines has changed. # 'change' is a modification the the line count (in case the character was not yet # typed). updatelinenumbers: (change = 0) -> $gutter = this.gutter() count = this.lines().children(li).length current = $gutter.children(span).length count += change # add lines if (count > current) for i in [current..(count - 1)] ele = document.createelement(span) ele.innertext = #{ i + 1 } $gutter[0].appendchild(ele) # remove lines else if (current > count) for j in [count..(current - 1)] $gutter.children(span:last-child).remove() this.updategutterwidth() if current != count return # set whether or not the gutter should be visible. linenumbers: (bool) -> if bool == true and !this.number $(this.element) .prepend('<div class=gutter></div>') this.lines() .css(left: 20) this.updatelinenumbers() else if bool == false and this.number this.gutter().remove() $(this.element) .css(left: 1) this.number = bool # return the gutter (a jquery object). gutter: () -> this._gutter ?= $(this.element).children(div.gutter) return this._gutter # return a jquery '<ul>'. each '<li>' is a line of the source viewer. lines: -> return $(this.element).children('ul') # a hash of syntax highlighting rules. rules: -> return this._rules # re-highlight the text. $(.editor > ul).live 'keyup', (event) -> # 13: enter # 37, 38, 39, 40: arrow keys # 33, 34: page up / down # 16, 17, 18, 91: shift, ctrl, alt, meta # 35, 36: home / end if !(event.which in [13, 37, 38, 39, 40, 33, 34, 16, 17, 18, 91, 35, 36]) and !event.altkey and !event.ctrlkey # prevent an annoying error when backspacing to the beginning of a line. selection = window.getselection() # store the cursor position before highlighting. cursorpos = selection.getrangeat(0) if cursorpos.getclientrects()[0] clickx = cursorpos.getclientrects()[0].left clicky = cursorpos.getclientrects()[0].top # highlight $li = $(selection.focusnode).closest(li) rules = $li.closest(.editor).editor('rules') highlight.highlight($li, rules) # restore cursor position. cursorpos = document.caretrangefrompoint(clickx, clicky) window.getselection().addrange(cursorpos) # line numbering update. $(.editor > ul).live 'keydown', (event) -> # redo line numbering for enter, backspace, delete. if (event.which in [13, 8, 46]) $this = $(this) newline = switch event.which when 13 then 1 when 8 then -1 else 0 $this.parent().editor('updatelinenumbers', newline) # correction settimeout(() -> $this.parent().editor('updatelinenumbers', 0) , 300) # ##################### main ########################## $(.frame).frame('tabs').last().tab(content) .append(<div id='sourceview'></div>) $(#sourceview) .css position: 'absolute' left: 1 right: 1 top: 1 bottom: 1 .editor() .editor('theme', 'plugins/editor/themes/idlefingers.css') .editor(highlight, javascript) .editor(linenumbers, true) return editor",
        "present_kp": [
            "javascript",
            "jquery",
            "coffeescript"
        ],
        "absent_kp": [
            "jquery ui"
        ],
        "predict": {
            "present_kp": [
                "coffeescript",
                "refactoring",
                "beautification"
            ],
            "absent_kp": [
                "javascript",
                "code readability",
                "coding style"
            ]
        }
    },
    {
        "text": "doubly linked list: iterator/pointer arithmetic. i'm using a doubly linked list container i've written as the working example. the textbook i'm using is from 2001, so feel free to point out where later versions of the c++ standard should be used instead.notes:list::iterator functionality mimics pointer arithmetic.unlike most containers, list beginning is marked by a specific element which holds no data, to make reverse iteration easier: [begins][data][data][data][data][data][ends]de-referencing begin/end will deference the closest data element, so *begin = data@0 & *end = <email> list container plays more elegantly with basic loops (see main).list.h:#ifndef guard_list_h#define guard_list_htemplate <class t>struct element { element<t> *prev = null; element<t> *next = null; t data = null; int elem_id = null; char t_flag = null;};template <class t>struct elem_iter { elem_iter() { target = null; } elem_iter(element<t>* e) { target = e; } element<t>* target; element<t>* elem_iter::operator++(void) { if (target->next->t_flag == 'e'){ return null; } target = target->next; return target; } element<t>* elem_iter::operator--(void){ if (target->prev->t_flag == 'b'){ return null; } target = target->prev; return target; } t elem_iter::operator*(void){ if (target->t_flag == 'e'){ target = target->prev; return target->data; } else if (target->t_flag == 'b'){ target = target->next; return target->data; } return target->data; } bool elem_iter::operator!=(elem_iter& rhs){ return (rhs.target != this->target); } bool elem_iter::operator>=(elem_iter& rhs){ return (this->target->elem_id >= rhs.target->elem_id); } bool elem_iter::operator<=(elem_iter& rhs){ return (this->target->elem_id <= rhs.target->elem_id); } bool elem_iter::operator>(elem_iter& rhs){ return (this->target->elem_id > rhs.target->elem_id); } bool elem_iter::operator<(elem_iter& rhs){ return (this->target->elem_id < rhs.target->elem_id); } elem_iter elem_iter::operator+(int val){ for (int i = 0; i < val; i++){ this->target = this->target->next; } return *this; } elem_iter elem_iter::operator-(int val){ for (int i = 0; i < val; i++){ this->target = this->target->prev; } return *this; }};template <typename t>class list {public: list::list(void) { element_count = 0; // create begin element<t>* b = new element <t>; b->t_flag = 'b'; begins = b; // create end element<t>* e = new element <t>; e->t_flag = 'e'; ends = e; // double link: begins & ends begins->next = ends; ends->prev = begins; element_count = 0; } typedef elem_iter<t> iterator; iterator begin(void) { iterator it(begins); return it; } iterator end(void) { iterator it(ends); return it; } void push_back(t val) { element<t>* elem = new element<t>; // create: new-elem elem->data = val; // set data elem->elem_id = element_count++; // set id elem->prev = ends->prev; // link: new-elem to last-data-elem ends->prev->next = elem; // link: last-data-elem to new-element elem->next = ends; // link: new-elem to list-end ends->prev = elem; // link: list-end to new-elem ends->elem_id = element_count; // update: ends-id when list grows } t at(size_t pos) { return get_element(pos)->data; } void del(size_t pos) { element<t>* elem = get_element(pos); // get: element for deletion elem->prev->next = elem->next; // rejoin: double link elem->next->prev = elem->prev; // rejoin: double link delete elem; ends->elem_id = (element_count--); // update: when list shrinks } void clear(void) { element<t>* ep = begins->next; element<t>* ep_next = ep->next; while (ep->t_flag != 'e'){ delete ep; ep = ep_next; ep_next = ep->next; } begins->next = ends; ends->prev = begins; begins->data = null; ends->elem_id = null; element_count = 0; } size_t size(void) const { return element_count; } bool empty(void) const { if (element_count == 0){ return true; } else { return false; } }private: element<t>* begins; // list begins element<t>* ends; // list ends size_t element_count; // list size element<t>* get_element(size_t pos) { if (empty()) { std::cerr << no element - empty list; throw; } if (pos < 0 || pos >= element_count){ std::cerr << no element - out of range; throw; } iterator it; // determine the more efficent iteration direction(forward or reverse) ? if ((element_count / 2) > pos) { it = begin(); for (size_t i = 0; i <= pos; i++){ it++; } } else { it = end(); for (size_t i = size() - pos; i > 0; i--){ it--; } } return it.target; }};#endiftypedef list<int> container;main.cpp:#include <iostream>#include <vector>#include <list>#include list.hint main() { container ls; container::iterator begin = ls.begin(); container::iterator end = ls.end(); container::iterator iter = begin; std::cout << attempt to retrieve data from empty list: ls.at(3) << std::endl; std::cout << -------------------------------------------------- << std::endl; //std::cout << ls.at(3) << std::endl << std::endl; std::cout << test: growing list does not invalidate iter << std::endl; std::cout << ------------------------------------------- << std::endl; std::cout << empty list << std::endl << std::endl; std::cout << begin addr: << &begin << << std::endl; std::cout << begin t_flag: << begin.target->t_flag << << std::endl; std::cout << end addr: << &end << << std::endl; std::cout << end t_flag: << end.target->t_flag << << std::endl; std::cout << std::endl << add data to list: 33 << std::endl << std::endl; ls.push_back(33); std::cout << begin addr: << &begin << << std::endl; std::cout << begin t_flag: << begin.target->t_flag << << std::endl; std::cout << end addr: << &end << << std::endl; std::cout << end t_flag: << end.target->t_flag << << std::endl; std::cout << std::endl << add data to list: 33 << std::endl << std::endl; ls.push_back(856); std::cout << begin addr: << &begin << << std::endl; std::cout << begin t_flag: << begin.target->t_flag << << std::endl; std::cout << end addr: << &end << << std::endl; std::cout << end t_flag: << end.target->t_flag << << std::endl << std::endl; std::cout << clear() << std::endl << std::endl; ls.clear(); std::cout << std::endl << std::endl; std::cout << add data to list: 0 1 2 3 4 5 6 7 8 9 << std::endl; std::cout << ------------------------------------------------- << std::endl; for (int i = 0; i != 10; i++){ ls.push_back(i); } std::cout << std::endl << std::endl; std::cout << data@ begin+4 << std::endl; std::cout << ------------- << std::endl; std::cout << *(iter + 4) << std::endl; std::cout << std::endl << std::endl; std::cout << data@ begin->end << std::endl; std::cout << ---------------- << std::endl; iter = begin; while (iter++){ std::cout << *iter << ; } std::cout << std::endl << std::endl << std::endl; std::cout << data@ end->begin << std::endl; std::cout << ---------------- << std::endl; iter = end; while (iter--){ std::cout << *iter << ; } std::cout << std::endl << std::endl << std::endl; std::cout << for/iter: begin->end << std::endl; std::cout << ---------------- << std::endl; for (iter = begin; iter++;){ std::cout << *iter << ; } std::cout << std::endl << std::endl << std::endl; std::cout << iter arith: +4 +1 -1 << std::endl; std::cout << -------------------- << std::endl; iter = ls.begin(); iter = iter + 4; std::cout << *iter << ; std::cout << *(iter + 1) << ; std::cout << *(iter - 1) << ; std::cout << std::endl << std::endl << std::endl; std::cout << data@: (0)(1)(2)(3)(4)(5)(6)(7)(8)(9) << std::endl; std::cout << ------------------------------------- << std::endl; for (int i = 0; i != 10; i++){ std::cout << ls.at(i) << ; } ls.clear(); return 0;}",
        "present_kp": [
            "c++",
            "linked list",
            "iterator"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "doubly linked list",
                "iterator",
                "pointer arithmetic"
            ],
            "absent_kp": [
                "c++",
                "data structure"
            ]
        }
    },
    {
        "text": "nginx reverse proxy - no user/password was provided for basic authentication. i've got nginx set up on a rpi (raspbian)as a reverse proxy using ssl between the remote user and the nginx instance. all seems to work well for two services mounted on the rpi (shellinabox and rpi monitor). however i can't get nginx to work with a couch potato instance that is held on another server on the same home network.when on the home network, i can access couch potato from any device on the network without authentication, but when trying to access it externally through the nginx reverse proxy, the nginx error log shows:2015/02/05 10:43:46 [error] 30557#0: *1 no user/password was provided for basic authentication, client: xxx.xxx.xxx.xxx, server: , request: get /couchpotato/ http/1.1, host: xxx.xxx.xxx.xxxand the nginx generated access log for couchpotato shows:xxx.xxx.xxx.xxx - - [05/feb/2015:10:43:46 +0000] get /couchpotato/ http/1.1 401 590 - mozilla/5.0 (compatible; msie 10.0; windows nt 6.1; trident/6.0) the nginx configuration file in sites-available is here: <url> read many of the other questions about this i'm guessing that nginx is trying to pass it's own authentication to couchpotato when i think i don't want anything passed, but i don't know.grateful for any help",
        "present_kp": [
            "nginx",
            "ssl"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "nginx",
                "reverse proxy",
                "basic authentication"
            ],
            "absent_kp": [
                "raspberry pi",
                "ssl",
                "couch potato"
            ]
        }
    },
    {
        "text": "kata: natural sort. i am choosing to learn f# for my own enjoyment. i am getting to the point where concepts of f# seem to be pretty easy, but understanding some of the whys and whens is a bit harder.before i get into the code and the explanation, let me put my question up front. i am asking where can i find better advice on formatting f# code for readability? or can someone give me a few guiding tips based off of the example given below?so what are the best practices for code format and file layout?now to the explanation of the code.i have started practicing coding kata's in f# just to allow me to flex the language a little. the following program is an implementation of a natural sort. this was my first attempt to solving the problem in a tdd fashion in f#, as such i chose to forgo any framework as i did not want to deal with figuring out how to use any of them and not break the functional paradigm.so the code below carries a light weight unit test framework.here is the code for the natural sort:namespace katas open system.linq module naturalsortkata = exception invalidexception of string type comparison = | equal | lesser | greater static member compare x y = if x = y then equal elif x > y then greater else lesser type chuncktype = | numbertype | stringtype | unknown static member gettype (c : char) = if system.char.isdigit(c) then numbertype else stringtype member this.compare other = match other with | ty when ty = this -> equal | unknown -> lesser | numbertype when this = unknown -> greater | numbertype -> lesser | stringtype -> greater let natualcompare (left : string) (right : string) = if left = right then equal else let fix str = new system.string( str |> list.rev |> list.toarray ) let gatherchunck str = let rec gather str acc = match str with | [] -> let (ty, l) = acc (ty, fix(l)) | fistletter::rest -> match acc with | (ty, _) when ty = unknown -> let t = chuncktype.gettype(fistletter) gather rest (t, fistletter :: []) | (ty, l) when ty = chuncktype.gettype(fistletter) -> gather rest (ty, fistletter::l) | (ty, l) -> (ty, fix(l)) gather str (unknown, []) let rec compare (left : string) (right : string) = if (not (left.any())) || (not (right.any())) then match left.length, right.length with | llen, rlen when llen = rlen -> equal | llen, rlen when llen > rlen -> greater | llen, rlen when llen < rlen -> lesser | _ -> raise (invalidexception bad data) else let lt, lchunk = left |> seq.tolist |> gatherchunck let rt, rchunk = right |> seq.tolist |> gatherchunck match lt.compare rt with | equal -> if lchunk = rchunk then let lval = left.replace(lchunk, ) let rval = right.replace(rchunk, ) compare lval rval else match lt with | numbertype -> comparison.compare (system.int64.parse(lchunk)) (system.int64.parse(rchunk)) | _ -> comparison.compare lchunk rchunk | _ -> lt.compare(rt) compare left righthere is the code for the tests:namespace katas.testing open katas.naturalsortkata module tests = let test left right expected title= let testrun x = let result = right |> natualcompare left if result = expected then x |> printfn %d good true else title + fails |> printfn %d %s x result |> sprintf %d actual: %a x |> sprintf %d expected: %a %s x expected |> printfn %s false testrun let testrunner tests= let rec runner x result tests = match tests with | [] -> result | head::tests -> let current = (head x) && result tests |> runner (x + 1) current tests |> runner 1 true let test01 = simple equality |> test one one equal let test02 = left < right |> test left right lesser let test03 = beta > alpha |> test beta alpha greater let test04 = \\9\\ < \\ |> test 9 10 lesser let test05 = lpha9\\ < lpha10\\ |> test alpha9 alpha10 lesser let test06 = lpha9centary9\\ < lpha9centary10\\ |> test alpha9centary9 alpha9centary10 lesser let test07 = \\ > \\9\\ |> test 10 9 greater let test08 = lpha10\\ > lpha9\\ |> test alpha10 alpha9 greater let test09 = lpha9centary9\\ < lpha9centary10\\ |> test alpha9centary10 alpha9centary9 greater let tests = test01 :: test02 :: test03 :: test04 :: test05 :: test06 :: test07 :: test08 :: test09 ::[] let runtests = tests |> testrunner |> printfn %bhere is the code that runs it all:open katas.naturalsortkataopen katas.testing.tests[<entrypoint>]let main argv = //katas.lockers.showlockerresults 300 runtests let _ = system.console.readkey(true) 1",
        "present_kp": [
            "f#"
        ],
        "absent_kp": [
            "unit testing"
        ],
        "predict": {
            "present_kp": [
                "natural sort",
                "f#",
                "code format",
                "file layout",
                "unit test framework"
            ],
            "absent_kp": [
                "c#",
                "tdd",
                "functional programming"
            ]
        }
    },
    {
        "text": "regular expression for a language which doesn't look regular. i'm trying to find a regular expression for the following language:$$l=\\{x0y : ext{$x$ contains same number of 0's as $y$ contains 1's}\\}. $$",
        "present_kp": [],
        "absent_kp": [
            "regular languages",
            "regular expressions"
        ],
        "predict": {
            "present_kp": [
                "regular expression"
            ],
            "absent_kp": [
                "language theory",
                "finite automata"
            ]
        }
    },
    {
        "text": "what's wrong about extending a class with prototype methods?. i was at a bar last night with a few of my colleagues. they said that it's a bad idea to extend the functionality of basic javascript objects with a prototype method.for example, let's say you created a method for finding the factorialnumber.prototype.factorial = function(n) { return n == 0 ? 1 : factorial(n - 1)}they said there was some danger to creating prototypes. why would this be a bad practice?",
        "present_kp": [
            "javascript"
        ],
        "absent_kp": [
            "functions",
            "prototyping"
        ],
        "predict": {
            "present_kp": [
                "prototype methods",
                "javascript"
            ],
            "absent_kp": [
                "class extension",
                "factorial"
            ]
        }
    },
    {
        "text": "pac learning model definition. the probably approximately correct (pac) learning model is defined as:a concept class $c$ is said to be pac-learnable if there exists an algorithm $a$ and a polynomial function $poly(,,,)$ such that for any $>0$ and $>0$, for all distributions $d$ on $x$ and for any target concept $cc$, the following holds for any sample size $mpoly(1/,1/,n,size(c))$:$pr[r(hs)]1-$where $r(hs)$ is the generalization error over a sample $s$ of size $m$ containing instances of variable $x$ following distribution $d$ and $size(c)$ is the maximal cost of the computational representation of $cc$.i know $poly(1/,1/,n,size(c))$ is a polynomial. but what is the explicit form of $poly(1/,1/,n,size(c))$? what are the variables? what is its degree?",
        "present_kp": [],
        "absent_kp": [
            "machine learning"
        ],
        "predict": {
            "present_kp": [
                "pac learning model",
                "concept class",
                "generalization error",
                "polynomial function"
            ],
            "absent_kp": [
                "machine learning",
                "algorithm",
                "sample size"
            ]
        }
    },
    {
        "text": "safari html for planck constant/reduced planck constant. i want to display the symbols commonly known as h and h-bar. this is for safari and ios. what do i enter into my html?if there is a table of such things somewhere, a reference to it would be great. thanks!",
        "present_kp": [
            "html",
            "safari"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "html",
                "safari"
            ],
            "absent_kp": [
                "planck constant",
                "ios"
            ]
        }
    },
    {
        "text": "disable moving cards between lists. i like the way trello on trello lists are locked to the degree where users cannot move cards between lists (only for board owner/admin). is there a setting that controls this feature?",
        "present_kp": [
            "trello"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "trello",
                "cards",
                "lists"
            ],
            "absent_kp": [
                "board owner",
                "admin"
            ]
        }
    },
    {
        "text": "is there a way to put todo marker in confluence pages, then find all pages with this marker?. i'm looking for a way to put todo markers on confluence pages. so i could then see todo items in all documents. just the way it's done in visual studio and other ides.",
        "present_kp": [
            "confluence"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "todo marker",
                "confluence pages"
            ],
            "absent_kp": [
                "visual studio",
                "ides"
            ]
        }
    },
    {
        "text": "java program to encrypt files using shamir secret sharing. shamir's secret sharing scheme essentially splits a secret into n parts, at least k of which are needed to recover it. i'm using that to encrypt/decrypt arbitrary files (it's part of a college project).here's a rough idea of what i'm doing:take a file, and read n bytes. treat those bytes as an integer, and encrypt it using this implementation of the shamir algorithm. i get n integers, and i write one each to a file. read n more bytes, and so on until i am done.similarly, when decrypting, take n files, read an integer from each (i write a byte to denote the length of an integer before each, so i know how many bytes to read), decrypt the n integers to get the original one, and convert it to bytes to get n bytes, and write them to a file, and then read more integers.this is working, but is rather slow - with encrypting to 3 files a 170 mb file, it takes me 5 minutes to encrypt, 4 minutes to decrypt. how can i speed it up? of course, any other suggestions are also welcome.package crypto;import com.tiemens.secretshare.main.cli.maincombine;import com.tiemens.secretshare.main.cli.mainsplit;import java.io.*;import java.math.biginteger;import java.nio.file.files;import java.nio.file.paths;import java.util.arraylist;import java.util.regex.matcher;import java.util.regex.pattern;import static java.lang.integer.min;import static java.util.arrays.copyofrange;/** * created by hooda on 2/3/2015. */public class shamir { //the encoding that will be used when splitting and combining files. static string encoding = iso-8859-1; //the number of bytes per piece (except maybe the last one)! static int piecesize = 128; //mode 0 for strings, 1 for ints. public static arraylist<string> shamirsplit(string inputstring, int numpieces, int minpieces, int mode) { string type = -ss; if (mode == 1) { type = -sn; } arraylist<string> parts = new arraylist<>(); string[] splitargs = {-n, integer.tostring(numpieces), -k, integer.tostring(minpieces), type, inputstring, -primenone}; mainsplit.splitinput splitinput = mainsplit.splitinput.parse(splitargs); mainsplit.splitoutput splitoutput = splitinput.output(); bytearrayoutputstream baos = new bytearrayoutputstream(); printstream ps = new printstream(baos); splitoutput.print(ps); string content = baos.tostring(); // e.g. iso-8859-1 bufferedreader reader = new bufferedreader(new stringreader(content)); string line; int i = 0; try { while ((line = reader.readline()) != null && i < numpieces) { if (line.startswith(share (x)) { i++; parts.add(line.trim()); } } } catch (exception e) { //todo catch } return parts; } //returns the integer that the decryption represents, but in string format. public static string shamircombineint(arraylist<string> parts, arraylist<integer> partnums, arraylist<string> flags, int k) { arraylist<string> args = new arraylist<>(); args.add(-primenone); args.add(-k); args.add(integer.tostring(k)); for (int i = 0; i < k; i++) { string partsecret = parts.get(i); string partnum = partnums.get(i).tostring(); args.add(-s.concat(partnum)); args.add(partsecret); } bytearrayoutputstream baos = new bytearrayoutputstream(); printstream ps = new printstream(baos); string[] combineargs = args.toarray(new string[args.size()]); maincombine.combineinput combineinput = maincombine.combineinput.parse(combineargs, null, ps); maincombine.combineoutput combineoutput = combineinput.output(); combineoutput.print(ps); string content = baos.tostring(); // e.g. iso-8859-1 pattern pattern = pattern.compile(secret.number = '); matcher matcher = pattern.matcher(content); if (matcher.find()) { int i = matcher.end(); char c = content.charat(matcher.end()); while (c != ''') { i++; c = content.charat(i); } return (content.substring(matcher.end(), i)); } else { return ; } } /** * splits the given file into numpieces, of which at least minpieces are needed to recover the original. * * @param filepath path to the file to be encrypted. * @param numpieces number of files to split into. * @param minpieces minimum splitted files needed to recover original. * @return * @throws ioexception */ public static arraylist<fileoutputstream> filesplit(string filepath, int numpieces, int minpieces) throws ioexception { long starttime = system.currenttimemillis(); //create files to which encrypted pieces will b written. arraylist<fileoutputstream> splitfiles = new arraylist<>(numpieces); for (int i = 0; i < numpieces; i++) { //todo splitfiles.add(i, new fileoutputstream(e://.concat(dummy.txt..concat(integer.tostring(i + 1))))); } //get the file as a byte array. byte[] fileasbytes = files.readallbytes(paths.get(filepath)); system.out.println(file had .concat(integer.tostring(fileasbytes.length))); //do the encryption. for (int i = 0; i < fileasbytes.length; ) { //we want to partition the byte array into pieces of length 4/8/16 whatever, but if length is not multiple (eg there are 15 bytes) //then the last piece should be shorter. j takes care of that. int j = min(fileasbytes.length - i, shamir.piecesize); byte[] piece = copyofrange(fileasbytes, i, i + j); i = i + j; shamir.encryptandwrite(piece, numpieces, minpieces, splitfiles); } for (fileoutputstream f : splitfiles) { f.close(); } long endtime = system.currenttimemillis(); system.out.println(encryption took + (endtime - starttime) / 1000.0 + seconds); //testing code. todo remove starttime = system.currenttimemillis(); system.out.println( testing the decryption ); arraylist<string> files = new arraylist<>(); files.add(e://dummy.txt.1); files.add(e://dummy.txt.2); files.add(e://dummy.txt.3); shamir.filecombine(files, minpieces); endtime = system.currenttimemillis(); system.out.println(decryption took + (endtime - starttime) / 1000.0 + seconds); return splitfiles; } /** * okay, this is a bit hacky. we want to take a piece of a file, encrypt/split it, and write the splits * to the given fileoutputstreams array. we want to treat the piece as an integer (treating it as string => large space overhead). * this is tricky because of two reasons: * 1. if all the bytes are zero, out piece will be zero, and we get an exception! it cannot be encrypted. * 2. if the piece has any zero bytes at start, they get lost in the encrypt/decrypt process. * 3. we cannot predict the length of the encrypted result. a 128 byte piece, when encrypted, can be 128, or 129 or whatever bytes. * * to fix this, we use prefixing and size byte. * we prefix each piece with a one byte - <phone>. this means our piece will never have zero bytes at start. takes care of 1 and 2. * and, when writing the encrypted data to files, we prefix each with one byte containing its size. * * then, when reading, here's what we do - we have n files. from each, we read the first byte. that will give us sizes n1,n2..nn. * from each file, we then read the corresponding number of bytes n1 bytes from 1.. nn bytes from n, and feed them to shamir decryptor. * finally, we convert the recovered number to byte array, and discard the first one - we inserted it ourselves. * * @param piece * @param numpieces * @param minpieces * @param files * @throws ioexception */ public static void encryptandwrite(byte[] piece, int numpieces, int minpieces, arraylist<fileoutputstream> files) throws ioexception {// printbytearray(piece); //prefixing a new 1 at the start of piece == add to 2^(no. of bytes*8). biginteger pieceasint = new biginteger(1, piece); biginteger toadd = (new biginteger(2)).pow(piece.length * 8); pieceasint = pieceasint.add(toadd); assert (pieceasint.tobytearray().length == piece.length + 1); arraylist<string> piecesplit = shamir.shamirsplit(pieceasint.tostring(), numpieces, minpieces, 1); //write to file. for (int i = 0; i < piecesplit.size(); i++) { string secret = piecesplit.get(i).split(=)[1].trim(); byte[] towrite = (new biginteger(secret)).tobytearray(); files.get(i).write((byte) towrite.length);// files.get(i).write(flag); files.get(i).write(towrite); } } public static void writebytestofiles(arraylist<string> shamiroutput, arraylist<fileoutputstream> files) throws ioexception { for (int i = 0; i < shamiroutput.size(); i++) { string partsecret = shamiroutput.get(i).split(=)[1].trim();// system.out.println(shamiroutput.get(i)); byte[] towrite = (new biginteger(partsecret)).tobytearray(); assert (towrite.length <= 255);// system.out.println(towrite.length); system.out.println(towrite.length); files.get(i).write((byte) (towrite.length)); files.get(i).write(towrite); } } public static void filecombine(arraylist<string> files, int k) throws ioexception { //create input streams, and part numbers (needed when decrypting) arraylist<fileinputstream> filestreams = new arraylist<>(files.size()); arraylist<integer> partnums = new arraylist<>(files.size()); for (int i = 0; i < files.size(); i++) { filestreams.add(i, new fileinputstream(files.get(i))); partnums.add(i, integer.parseint(files.get(i).substring(files.get(i).lastindexof(.) + 1, files.get(i).length()))); } arraylist<arraylist<biginteger>> filesasints = new arraylist<>(); for (int i = 0; i < filestreams.size(); i++) { arraylist<biginteger> temp = new arraylist<>(); long size = filestreams.get(i).getchannel().size(); for (int j = 0; j < size; ) { //need to bitmask because java stores integers as two's complement. //if we convert i>128 to a byte and back, we'll end up with negative value without this. int bytestoread = (int) (filestreams.get(i).read() & 0xff); j ++; byte[] intbytes = new byte[bytestoread]; filestreams.get(i).read(intbytes); biginteger biginteger = new biginteger(1, intbytes); j += bytestoread; temp.add(biginteger); } filesasints.add(i, temp); } arraylist<biginteger> decryptedints = new arraylist<>(filesasints.get(0).size()); for (int i = 0; i < filesasints.get(0).size(); i++) { arraylist<string> intsasstrings = new arraylist<>(); for (int j = 0; j < filesasints.size(); j++) { intsasstrings.add(filesasints.get(j).get(i).tostring()); } string decrypted = shamir.shamircombineint(intsasstrings, partnums, null, k); decryptedints.add(i,new biginteger(decrypted)); } fileoutputstream fileoutputstream = new fileoutputstream(files.get(0).substring(0, files.get(0).length() - 2)); for (int i = 0; i < decryptedints.size(); i++) { byte[] intbytes = decryptedints.get(i).tobytearray(); byte[] towrite = copyofrange(intbytes, 1, intbytes.length); fileoutputstream.write(towrite); } fileoutputstream.close(); system.out.println(file decrypted!); for(fileinputstream f : filestreams){ f.close(); } } }",
        "present_kp": [
            "java",
            "io"
        ],
        "absent_kp": [
            "performance",
            "cryptography"
        ],
        "predict": {
            "present_kp": [
                "shamir secret sharing",
                "file encryption",
                "java"
            ],
            "absent_kp": [
                "college project",
                "performance optimization"
            ]
        }
    },
    {
        "text": "can i use publicly mentioned algorithms for writing programs?. i want to write a program that solves sudoku. so, i found some sudoku algorithms on wikipedia. can i use them or do i need to develop my own algorithm? also, do i need to ask the specific license holder's permission?.. if so, how would i go about obtaining that permission?",
        "present_kp": [
            "algorithms"
        ],
        "absent_kp": [
            "open source",
            "licensing"
        ],
        "predict": {
            "present_kp": [
                "algorithms",
                "sudoku"
            ],
            "absent_kp": [
                "programming",
                "license"
            ]
        }
    },
    {
        "text": "fail2ban is not blocking ips trying to access my server via ssh. i installed fail2ban with the default settings because there's a bunch of bots trying to log in as root to my server. i installed it but nothing has changed, i checked fail2ban jail ip list and there's nothing there.this is how my secure log looks like: may 19 09:11:25 localhost sshd[6080]: failed password for root from 43.255.188.160 port 52111 ssh2may 19 09:11:25 localhost unix_chkpwd[6083]: password check failed for user (root)may 19 09:11:25 localhost sshd[6080]: pam_succeed_if(sshd:auth): requirement uid >= 1000 not met by user rootmay 19 09:11:28 localhost sshd[6080]: failed password for root from 43.255.188.160 port 52111 ssh2may 19 09:11:28 localhost unix_chkpwd[6084]: password check failed for user (root)may 19 09:11:28 localhost sshd[6080]: pam_succeed_if(sshd:auth): requirement uid >= 1000 not met by user rootmay 19 09:11:29 localhost sshd[6080]: failed password for root from 43.255.188.160 port 52111 ssh2may 19 09:11:29 localhost sshd[6080]: received disconnect from 43.255.188.160: 11: [preauth]may 19 09:11:29 localhost sshd[6080]: pam 2 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=43.255.188.160 user=rootmay 19 09:11:30 localhost unix_chkpwd[6087]: password check failed for user (root)may 19 09:11:30 localhost sshd[6085]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=43.255.188.160 user=rootmay 19 09:11:30 localhost sshd[6085]: pam_succeed_if(sshd:auth): requirement uid >= 1000 not met by user rootmay 19 09:11:31 localhost sshd[6085]: failed password for root from 43.255.188.160 port 39053 ssh2may 19 09:11:31 localhost unix_chkpwd[6088]: password check failed for user (root)may 19 09:11:31 localhost sshd[6085]: pam_succeed_if(sshd:auth): requirement uid >= 1000 not met by user rootmay 19 09:11:33 localhost sshd[6085]: failed password for root from 43.255.188.160 port 39053 ssh2may 19 09:11:33 localhost unix_chkpwd[6089]: password check failed for user (root)may 19 09:11:33 localhost sshd[6085]: pam_succeed_if(sshd:auth): requirement uid >= 1000 not met by user rootmay 19 09:11:36 localhost sshd[6085]: failed password for root from 43.255.188.160 port 39053 ssh2may 19 09:11:36 localhost sshd[6085]: received disconnect from 43.255.188.160: 11: [preauth]may 19 09:11:36 localhost sshd[6085]: pam 2 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=43.255.188.160 user=rootmay 19 09:11:36 localhost unix_chkpwd[6093]: password check failed for user (root)may 19 09:11:36 localhost sshd[6091]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=43.255.188.160 user=rootmay 19 09:11:36 localhost sshd[6091]: pam_succeed_if(sshd:auth): requirement uid >= 1000 not met by user rootmay 19 09:11:38 localhost sshd[6091]: failed password for root from 43.255.188.160 port 53516 ssh2may 19 09:11:38 localhost unix_chkpwd[6094]: password check failed for user (root)may 19 09:11:38 localhost sshd[6091]: pam_succeed_if(sshd:auth): requirement uid >= 1000 not met by user rootmay 19 09:11:40 localhost sshd[6091]: failed password for root from 43.255.188.160 port 53516 ssh2may 19 09:11:40 localhost unix_chkpwd[6095]: password check failed for user (root)may 19 09:11:40 localhost sshd[6091]: pam_succeed_if(sshd:auth): requirement uid >= 1000 not met by user rootmay 19 09:11:42 localhost sshd[6091]: failed password for root from 43.255.188.160 port 53516 ssh2may 19 09:11:42 localhost sshd[6091]: received disconnect from 43.255.188.160: 11: [preauth]may 19 09:11:42 localhost sshd[6091]: pam 2 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=43.255.188.160 user=rootmay 19 09:11:43 localhost unix_chkpwd[6098]: password check failed for user (root)may 19 09:11:43 localhost sshd[6096]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=43.255.188.160 user=rootmay 19 09:11:43 localhost sshd[6096]: pam_succeed_if(sshd:auth): requirement uid >= 1000 not met by user rootmay 19 09:11:44 localhost sshd[6096]: failed password for root from 43.255.188.160 port 40323 ssh2may 19 09:11:44 localhost unix_chkpwd[6099]: password check failed for user (root)may 19 09:11:44 localhost sshd[6096]: pam_succeed_if(sshd:auth): requirement uid >= 1000 not met by user rootmay 19 09:11:46 localhost sshd[6096]: failed password for root from 43.255.188.160 port 40323 ssh2may 19 09:11:46 localhost unix_chkpwd[6100]: password check failed for user (root)may 19 09:11:46 localhost sshd[6096]: pam_succeed_if(sshd:auth): requirement uid >= 1000 not met by user rooti enabled fail2ban, (here says that is already running) fail2ban-client starterror server already runningand the status since yesterday: fail2ban-client statusstatus|- number of jail: 0'- jail list:is there something that i'm not doing which is not enabling fail2ban?",
        "present_kp": [
            "ssh",
            "fail2ban"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "fail2ban",
                "ssh"
            ],
            "absent_kp": [
                "server security",
                "ip blocking"
            ]
        }
    },
    {
        "text": "how do i search only the displayed part of concealed text?. if i have syntax highlighting rules setup using conceal to hide or change certain characters in a file, how do i search what is displayed, as opposed to what the buffer actually contains?the concealed part may contain formatting markup, for example, which i wish to ignore.i'd like a method that:doesn't rely on the specific rules used to create the concealed text.provides some level of compatibility with the traditional search operators like n, *, etc.can this be done without re-implementing n, * and the like?related:how can i copy the displayed text, instead of the actual text?",
        "present_kp": [
            "search",
            "conceal"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "search",
                "concealed text",
                "syntax highlighting"
            ],
            "absent_kp": [
                "vim",
                "text manipulation"
            ]
        }
    },
    {
        "text": "read-only file system error on samba share, alternative share with identical options working (linux client). i'm experiencing a very strange problem which i've been trying to solve in the past few days and i already tried various methods of resolution to which i will get to in a moment.my home network consists of a micro server (ubuntu linux server 17.04, kernel 4.10.0-26-generic on x86_64, cli only) and several windows and linux clients. my goal is to set up a samba share with read-only guest access and read-write access for myself (linux mint 18.1, kernel 4.4.0-81-generic).in my smb.conf, there are two identical shares, media and temp (the later is for bug fixing purposes):[media]comment = smb sharepath = /mediabrowsable = yesguest ok = yeswritable = nowrite list = ap[temp]comment = smb sharepath = /tempbrowsable = yesguest ok = yeswritable = nowrite list = ap/media and /temp both are set to chmod -r 775 and chown -r ap:ap.now the strange thing is that i can access both shares from my linux client as ap but i only have write access on [temp]. when i try to create a folder or delete a file on [media], i get a read-only file system error. i also don't have any problems writing when i log onto my server (as root user ap) using ssl.it might also be of interest that i'm mounting three different internal hard drives to /media/*. their fstab entries look like this:uuid=954d122e-ef13-4248-acb3-5c95fb44d2ad /media/misc ext4 defaults 0 0uuid=d213c725-a715-40f3-8278-de890fad1168 /media/cinema ext4 defaults 0 0uuid=6469b3d0-bdb5-4b0c-8559-b9d26f69b332 /media/series ext4 defaults 0 0the following possible solutions didn't work:setting chmod to 777, setting chown back to root:rootfscking all partitions, including the system partitionplaying around with my smb.conf (trying out options like valid users, read list, create mask)adding ap as a samba usertrying to mount the share with mount -t cifs -o username=ap //server /mntpoint (and trying out options like rw,uid,gid,domain,forceuid,forcegid)remounting my internal hard disks with the rw optioni guess that i could switch to nfs but i'm still wondering if i missed something very obvious or what further steps i could take to solve this problem.",
        "present_kp": [
            "ubuntu",
            "samba"
        ],
        "absent_kp": [
            "networking",
            "readonly"
        ],
        "predict": {
            "present_kp": [
                "samba share",
                "read-only file system error",
                "linux client"
            ],
            "absent_kp": [
                "nfs",
                "file permissions"
            ]
        }
    },
    {
        "text": "script for getting cpu utilization of particular cores. i am looking for a script for checking cpu utilization of particular cpu cores. we have 80 cores, we need get cpu utilization percentage for particular 4 cores.can you help us on this.thanks.",
        "present_kp": [
            "cpu"
        ],
        "absent_kp": [
            "linux",
            "shell script",
            "shell"
        ],
        "predict": {
            "present_kp": [
                "cpu utilization",
                "cores"
            ],
            "absent_kp": [
                "script",
                "cpu cores"
            ]
        }
    },
    {
        "text": "what can be the consequences of using google webmaster tools for my youtube video downloader website?. will it be blocked/spam-listed if google detects that it provides youtube-video downloading ?",
        "present_kp": [
            "youtube"
        ],
        "absent_kp": [
            "google search console"
        ],
        "predict": {
            "present_kp": [
                "google webmaster tools",
                "youtube video downloader website"
            ],
            "absent_kp": [
                "youtube video downloading",
                "blocked",
                "spam-listed"
            ]
        }
    },
    {
        "text": "php not working (source file). i do not know why php is not working.i install nginx and php server { listen 80; server_name t.com t.com; location ~ \\.php$ { fastcgi_split_path_info ^(.+\\.php)(/.+)$; root /var/www/t.com/public_html/; index index.html index.htm; try_files $uri $uri/ =404; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; }}and php.conf#s# php is an html-embedded scripting language which attempts to make it# easy for developers to write dynamically generated webpages.#<ifmodule prefork.c>loadmodule php5_module /usr/lib64/httpd/modules/libphp5.so</ifmodule><ifmodule worker.c> loadmodule php5_module modules/libphp5-zts.so</ifmodule>## cause the php interpreter to handle files with a .php extension.#addhandler php5-script .phpaddtype text/html .phpaddtype application/x-httpd-php .php## add index.php to the list of files that will be served as directory# indexes.#directoryindex index.php## uncomment the following line to allow php to pretty-print .phps# files as php source code:##addtype application/x-httpd-php-source .phps<filesmatch \\.php$> sethandler application/x-httpd-php</filesmatch>and when browser file , show download file",
        "present_kp": [
            "php"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "php",
                "nginx"
            ],
            "absent_kp": [
                "web server",
                "configuration"
            ]
        }
    },
    {
        "text": "paper regarding the complexity of the longest path problem on weighted directed graphs of bounded treewidth. i would like to cite a paper/report/etc that solves the following problem polynomially in $n$:given a weighted directed graph $g=(v,e)$, $|v|=n$, of bounded treewidth $k \\in \\mathbb{n}$ and a source-destination pair $s,t\\in v$, find a longest path (not walk) from $s$ to $t$.the corresponding wikipedia article (<url>) implies that it is possible: the longest path problem is [...] fixed-parameter tractable when parameterized by the treewidth of the graphsadly, my knowledge of treewidth-techniques is rather small.i found the following paper, but it is on undirected (?unweighted?) graphs: confronting hardness using a hybrid approach: cites a paper of bodlaender (in section 2.2.1), where theorem 2.2 states that the longest path problem is solvable in $o(2^k k! n)$ (assuming a treewidth decomposition is given)is it easy to see that this technique also extends to directed weighted graphs? what happens when the weights are encoded in binary, i.e., they can be exponential in $n$?there is also a nice discussion here, but it also seems to rely on undirected (?unweighted?) graphs.thank you!",
        "present_kp": [
            "treewidth"
        ],
        "absent_kp": [
            "cc.complexity theory",
            "graph algorithms",
            "fixed parameter tractable",
            "dynamic programming"
        ],
        "predict": {
            "present_kp": [
                "longest path problem",
                "weighted directed graphs",
                "treewidth"
            ],
            "absent_kp": [
                "polynomial time",
                "undirected graphs",
                "binary weights"
            ]
        }
    },
    {
        "text": "what graphic languages are simpler than svg for database diagrams?. this is png was exported from inkscape. i created it manually and used theconnector tool to link tables.i find the plain svg for this diagram more complicated than i was expecting so would like to know if there are simpler graphic languages or specifications for this task?i would like to write something using python.here's the pastebin: svg for database diagram",
        "present_kp": [
            "diagram"
        ],
        "absent_kp": [
            "2d",
            "vector graphics"
        ],
        "predict": {
            "present_kp": [
                "svg",
                "database diagrams",
                "python"
            ],
            "absent_kp": [
                "graphic languages",
                "inkscape",
                "png"
            ]
        }
    },
    {
        "text": "adding a script to google forms to identify incorrect phone numbers. i currently use google forms as a way for applicants to submit their forms. one of the fields that is required is for their contact information. in my country phone numbers have 11 digits.is there a script that i can use that will not accept a number if it does not contain 11 digits and display an error message, like incorrect number, please check and try again?the number format should be 09xxxxxxxxx.",
        "present_kp": [
            "google forms",
            "phone number"
        ],
        "absent_kp": [
            "google apps script"
        ],
        "predict": {
            "present_kp": [
                "google forms",
                "script",
                "phone numbers"
            ],
            "absent_kp": [
                "validation",
                "error message"
            ]
        }
    },
    {
        "text": "why does google docs still ask me for permission when people try to view my doc?. i created a doc that i want anyone with a link to see. here are the settings i used:i gave that link to people on my site.for some reason, every week, i get emails from google docs saying that people want to access my document and need my permission.this is what those emails look like.what can i do to keep people who i gave the link to from needing my permission?",
        "present_kp": [],
        "absent_kp": [
            "google drive",
            "google documents",
            "permissions"
        ],
        "predict": {
            "present_kp": [
                "google docs",
                "permission"
            ],
            "absent_kp": [
                "document sharing",
                "access control"
            ]
        }
    },
    {
        "text": "live usb version of kali linux grub issues. i have installed kali linux on a 32gb usb drive, and i have a 9gb partition for persistence. also, the macbook pro i am running kali on requires the amd gpu to be disabled, so i am using this guide to disable the amd gpu (step 3). i want to disable it permanently, but the /etc/default/grub file is missing. i reinstalled grub 2.0 and the grub config file was there although there was an error: error: failed to get canonical path of 'overlay'after editing it , i couldn't get the grub-update command to run. it says that there is no such command. and after rebooting, the file disappeared again.please help me with this issue. i appreciate your assistance greatly.",
        "present_kp": [
            "kali linux",
            "live usb"
        ],
        "absent_kp": [
            "grub2"
        ],
        "predict": {
            "present_kp": [
                "kali linux",
                "grub",
                "usb drive"
            ],
            "absent_kp": [
                "macbook pro",
                "amd gpu",
                "persistence"
            ]
        }
    },
    {
        "text": "github pages do not appear. i have added a gh-pages branch to a project on github, but the github pages are not appearing.what could be going wrong?",
        "present_kp": [
            "github"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "github pages"
            ],
            "absent_kp": [
                "web hosting",
                "branch management"
            ]
        }
    },
    {
        "text": "using a destructuring assignment on a style object to extract two members. recently, one of my colleague decided to destructure the react-native styles object this way:const { header, headerhint, bodycontainer, body, normaltext, buttonwrapper, button, footer, footerlinks } = styles;and then used the variables in the react code:<view style={header}> <text style={headerhint}hint</text> ...</view>this felt quite wrong to me as it doesn't provide much value to the code, destructuring an object to grab more than 5-6 variables makes the code more difficult to read in my opinion. also, keeping the styles in the styles object let you know your styles comes from the stylesheet. in some cases we might want to create variables to merge several styles, they become much easier to spot.he argues destructuring reduces areas that you need to change - if you need to change them like if the styles object changes to something else, you don't have to remove all the references to styles.i'm wondering what is your opinion on this point and if you follow any style guide about destructuring object like this one?",
        "present_kp": [],
        "absent_kp": [
            "javascript",
            "react.js",
            "react native"
        ],
        "predict": {
            "present_kp": [
                "destructuring assignment",
                "style object",
                "react-native styles"
            ],
            "absent_kp": [
                "javascript",
                "react",
                "code readability"
            ]
        }
    },
    {
        "text": "working with classes (inheriting), @ properties and initialization. i'm working on this objective c programming assignment i found online. i'm not sure if i have met all the requirements, especially part c. any help or suggestion will be appreciated.part 6a) implement class a with properties a1, a2, and a3 (int, string, int).b) new objects are automatically initialized to 1, hello, 1.c) also provide initializer to any data and constructor (called without alloc) to do the same.d) make sure %@ ob object of a will print all data.e) then implement b inheriting from a. b adds property b (string). f) make sure b works as a, that is new object is initialized to 1, hello, 1, and 3 (the new data). the rest also must work on b. //classa.h file#import <foundation/foundation.h>@interface classa : nsobject// part 6a@property int a1;@property nsstring *a2;@property int a3;-(nsstring *) description;-(id) initwitha1: (int) x anda2: (nsstring *) s anda3: (int) y;-(id) init;@end//classa.m file#import classa.h@implementation classa-(id) initwitha1:(int)x anda2:(nsstring *)s anda3:(int)y { self = [super init]; if (self) { self.a1 = x; self.a2 = s; self.a3 = y; } return self;}// part 6b- (id) init { return [self initwitha1:1 anda2:@hello anda3:1];}// part 6d-(nsstring *) description { return [nsstring stringwithformat:@classa a1 = %d , a2 = %@ , a3 = %d, self.a1, self.a2, self.a3];}@end//classb.h file#import classa.h@interface classb : classa@property int a1;@property nsstring *a2;@property int a3;@property nsstring * b;-(nsstring *) description;-(id) initwitha1:(int)x anda2:(nsstring *)s anda3:(int)y andb: (nsstring *) z;-(id) init;@end//classb.m file#import classb.h@implementation classb-(id) initwitha1:(int)x anda2:(nsstring *)s anda3:(int)y andb:(nsstring *)z { self = [super init]; if (self) { self.a1 = x; self.a2 = s; self.a3 = y; self.b = z; } return self;}-(id) init { return [self initwitha1:1 anda2:@hello anda3:1 andb:@3];}-(nsstring *) description { return [nsstring stringwithformat:@classb a1 = %d , a2 = %@ , a3 = %d , b = %@ , self.a1, self.a2, self.a3, self.b];}@end//viewcontroller.m file#import viewcontroller.h#import classa.h#import classb.h@interface viewcontroller ()@end@implementation viewcontroller- (void)viewdidload{ [super viewdidload]; classa * a = [classa new]; nslog(@%@, a); classb * j = [classb new]; nslog (@%@, j);}- (void)didreceivememorywarning{ [super didreceivememorywarning]; // dispose of any resources that can be recreated.}@end",
        "present_kp": [
            "classes",
            "objective c"
        ],
        "absent_kp": [
            "inheritance"
        ],
        "predict": {
            "present_kp": [
                "objective c",
                "inheritance",
                "initialization"
            ],
            "absent_kp": [
                "swift",
                "class properties"
            ]
        }
    },
    {
        "text": "how can i make a perl script parse blocks more intelligently?. i have a file ~/bigfile.txt that consists of thousands of blocks of text of the formblock number : <block>size : <size1> <size2>extra : <extranumber><block of text>for this example say$ cat ~/bigfile.txtblock number : 1size : 7 6extra : 0john paulgeorge ringoblock number : 2size : 7 3extra : -10i amthewalrusblock number : 3size : 4 3extra : -1024hello worldi am trying to write a script that separates each block into a separate file named <block>-block.txt, nested into subdirectories of ~/data/ indexed by <size1> and <size2>. for instance, running the script should result in $ tree ~/data/~/data/|- 4-size1 |- 3-size2 |- 3-block.txt|- 7-size1 |- 3-size2 |- 2-block.txt |- 6-size2 |- 1-block.txtcurrently i have a script that simply dumps each block to a separate file in ~/data/ but i can't figure out how to alter it. i can post my current script if that would help but i suspect that it's quite inefficient and not suited to tackle this sort of organizational task.i would appreciate any pointers on how to accomplish this task with perl.",
        "present_kp": [
            "perl"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "perl script",
                "parse blocks",
                "file",
                "blocks of text",
                "script",
                "subdirectories"
            ],
            "absent_kp": [
                "text processing",
                "file organization",
                "data parsing"
            ]
        }
    },
    {
        "text": "can the following huffman tree be extended to include more letters?. the following table shows a possible set of huffman codes to be used for lossless compression of a text consisting only of the eight letters shown:e t o h l p w z10 01 111 110 0001 0000 0011 0010the codes will have been chosen by using an algorithm that has as input the frequencies of occurrence of each of the letters in the particular text. the choice of codes can be presented as a huffman tree. the tree is shown below: () / \\ () () / \\ / \\ () t e () / \\ / \\ () () h o / \\ / \\p l z wa) could this tree be extended to include more letters? if not, why not?b) can you suggest a modification of the tree to include two more letters?",
        "present_kp": [],
        "absent_kp": [
            "huffman coding"
        ],
        "predict": {
            "present_kp": [
                "huffman tree",
                "letters",
                "lossless compression"
            ],
            "absent_kp": [
                "algorithm",
                "frequency",
                "binary tree"
            ]
        }
    },
    {
        "text": "unit testing of curl based rest client library. i ran into some issues with unit testing in that getting the response body etc from the curl handle was tricky. i didn't want to necessarily use a mock or abstract the curl functionality.a colleague of mine suggested that i just use the localhost as an endpoint and reflect back the request for unit testing. i thought this was a novel approach.the directories in question are /test/echo/ and /test/unit/i'm also interested in seeing if anyone had any opinions on the assertions.githubthe echo endpoint: <?phpheader('content-type: application/json');$data = array('headers' => getallheaders(),// 'server' => $_server,'request_method' => $_server['request_method'],'get' => $_get,'post' => $_post,'put' => $_post,);//if the request is a put then get the file contents and try to parse the string into an arrayif($data['request_method'] == 'put'){parse_str(file_get_contents(php://input), $put_data);$data['put'] = $put_data;}echo json_encode($data);an example of one of the unit tests:<?phpclass transactiontest extends phpunit_framework_testcase{static $endpoint = '<url> function setup(){$options = array('username' => 'pj-ql-01','password' => 'pj-ql-01p','appkey' => '2489d40d-a74f-474f-9e8e-7b39507f3101');parent::setup();$this->client = new transactionclient($options);$this->client->setendpoint(self::$endpoint);}private function getrequestpath($client = null){if(!isset($client)) $client = $this->client;return str_replace($client->baseurl,'',curl_getinfo($client->curl)['url']);}/*** ensure that the correct verb and path are used for the create method*/public function testcreate(){$data = array('achroutingnumber' => '987654321','achaccountnumber' => '123456789','achaccounttype' => 'checking','foo' => 'bar');$transaction = $this->client->create($data);$this->assertequals($data, get_object_vars($transaction->post),'passed variables are not correct');$this->assertequals('post', $transaction->request_method,'the php verb is incorrect');$this->assertequals('/transactions', $this->getrequestpath(), 'the path is incorrect');}/*** ensure that the correct verb and path are used for the read method*/public function testread(){$transaction = $this->client->read(543);$this->assertequals('get', $transaction->request_method,'the php verb is incorrect');$this->assertequals('/transactions/543', $this->getrequestpath(), 'the path is incorrect');}/*** ensure that the correct verb and path are used for the read method*/public function testupdate(){$data = array('foo' => 'baz');$transaction = $this->client->update(654,$data);$this->assertequals($data, get_object_vars($transaction->put),'passed variables are not correct');$this->assertequals('put', $transaction->request_method,'the php verb is incorrect');$this->assertequals('/transactions/654', $this->getrequestpath(), 'the path is incorrect');}/*** ensure that the correct verb and path are used for the read method*/public function testaddsignature(){$data = array('foo' => 'baa');$transaction = $this->client->addsignature(655,$data);$this->assertequals($data, get_object_vars($transaction->post),'passed variables are not correct');$this->assertequals('post', $transaction->request_method,'the php verb is incorrect');$this->assertequals('/transactions/655/signature/capture', $this->getrequestpath(), 'the path is incorrect');}}the base model from which the various clients extend from: <?phpclass payjunctionclient{public $liveendpoint = '<url>';public $testendpoint = '<url>';public $packageversion = '0.0.1';public $useragent;public function __construct(){$this->useragent = 'payjunctionphpclient/' . $this->packageversion . '(brandedcreate; php/)'; //@todo add process.version$this->baseurl = $this->testendpoint;}public function setendpoint($endpoint){$this->baseurl = $endpoint;}/*** @description initializes the curl handle with default configuration and settings* @param null $handle* @return $this*/public function initcurl($handle = null){$this->curl = curl_init();curl_setopt($this->curl, curlopt_ssl_verifypeer, false); //don't worry about validating ssl @todo talk about security concernscurl_setopt($this->curl, curlopt_returntransfer, true);//if we have a password and username then set it by default to be passed for authenticationif (isset($this->defaults['password']) && isset($this->defaults['username'])) {curl_setopt($this->curl, curlopt_httpauth, curlauth_any);curl_setopt($this->curl, curlopt_userpwd, $this->defaults['username'] . : . $this->defaults['password']);}//if we have default headers to pass then pass themif (isset($this->defaults['headers']) && is_array($this->defaults['headers'])) {$headers = array();foreach ($this->defaults['headers'] as $key => $value) {array_push($headers, $key . ': ' . $value);}curl_setopt($this->curl, curlopt_httpheader, $headers);}return $this;}/*** @description generates a new client* @param null $options* @return $this*/public function generateclient($options = null){$this->baseurl = isset($options['endpoint']) ? $options['endpoint'] : $this->baseurl;$this->defaults['username'] = isset($options['username']) ? $options['username'] : '';$this->defaults['password'] = isset($options['password']) ? $options['password'] : '';$this->defaults['headers']['x-pj-application-key'] = isset($options['appkey']) ? $options['appkey'] : '';$this->defaults['headers']['user-agent'] = $this->useragent;$this->initcurl();return $this;}/*** @description takes the response from our curl request and turns it into an object if necessary* @param $response* @param null $contenttype* @return array|mixed*/public function processresponse($response){$contenttype = curl_getinfo($this->curl, curlinfo_content_type);if ($contenttype == 'text/html' || is_null($contenttype) || !isset($contenttype) || $contenttype = '' || $contenttype == false) {return $response;}try {$object = json_decode($response);return $object;} catch (exception $e) {return array('errors' => array(0 => 'invalid response type, error in processing response from payjunction'));}}/*** @description processes a curl post request* @param $path* @param null $params* @return array|mixed*/public function post($path, $params = null){curl_setopt($this->curl, curlopt_post, true);curl_setopt($this->curl, curlopt_url, $this->baseurl . $path);if (is_object($params) || is_array($params)) {curl_setopt($this->curl, curlopt_postfields, http_build_query($params));}return $this->processresponse(curl_exec($this->curl));}/*** @description processes a curl get request* @param $path* @param null $params* @return array|mixed*/public function get($path, $params = null){//create the query string if there are any parameters that need to be passed$query_string = ;if (!is_null($params)) {$query_string = ? . http_build_query($params,'','&');}curl_setopt($this->curl, curlopt_httpget, true);curl_setopt($this->curl, curlopt_url, $this->baseurl . $path . $query_string);return $this->processresponse(curl_exec($this->curl));}/*** @description processes a curl put request* @param $path* @param null $params* @return array|mixed*/public function put($path, $params = null){curl_setopt($this->curl, curlopt_customrequest, put);if (is_object($params) || is_array($params)) {curl_setopt($this->curl, curlopt_postfields, http_build_query($params));}curl_setopt($this->curl, curlopt_url, $this->baseurl . $path);return $this->processresponse(curl_exec($this->curl));}/*** @description processes a curl delete request* @param $path* @param null $params* @return array|mixed*/public function del($path, $params = null){curl_setopt($this->curl, curlopt_customrequest, delete);if (is_object($params) || is_array($params)) {curl_setopt($this->curl, curlopt_postfields, http_build_query($params));}curl_setopt($this->curl, curlopt_url, $this->baseurl . $path);return $this->processresponse(curl_exec($this->curl));}}the transactionclient related specifically to this unit test:<?phpclass transactionclient extends payjunctionclient{public function __construct($options){parent::__construct();$this->generateclient($options);}/*** @description create a new transaction* @param $params* @return array|mixed*/public function create($params){return $this->post('/transactions',$params);}/*** @description read from an existing transaction* @param $id* @return array|mixed*/public function read($id){return $this->get('/transactions/'.$id);}/*** @description update an existing transaction* @param $id* @param null $params* @return array|mixed*/public function update($id, $params = null){return $this->put('/transactions/'.$id, $params);}/*** @todo this does not appear to be working 405 method not allowed* @description add a signature to an existing transaction* @param $id* @param $params* @return array|mixed*/public function addsignature($id, $params){return $this->post('/transactions/'.$id.'/signature/capture',$params);}}",
        "present_kp": [
            "php",
            "unit testing",
            "phpunit"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "unit testing",
                "curl",
                "rest client",
                "php"
            ],
            "absent_kp": [
                "mock",
                "localhost",
                "assertions",
                "github"
            ]
        }
    },
    {
        "text": "burning a directory structure from a stdin pipe. i'm trying to do something tricky, i want to burn a directory structure onto a cd from a pipe stream. the reason is that it is coming from the network and i don't want it written on the hard drive of the cd-burning machine. i am unsure of how (or if possible) to pipe the output of tar (for example) into genisoimage or mkisofs. i noticed a stream option in genisoimage, but when i tried doing tar -cvf - /home/myuser | genisoimage --stream-media-size 200 -o test.isoi only got a test.iso which contained a stream.img (as specified in genisoimage(1)) which itself was the original tar-archive. this will not do, it needs to be a directory structure on the cd. i know that cdrskin (cli tool for burning) can take data from stdin and burn it. so how can i pipe data into genisoimage, have it create a directory structure of that data for the iso and then have it pipe that data back out to cdrskin to burn? i know also that genisoimage pipes the iso data to stdout by default so my only issue appears to be getting a stream of a directory structure piped into genisoimage and having that directory structure maintained in the iso data. the tags on this post are terrible because i couldn't find genisoimage, cdrskin, not even stdin. edit: this is not about data security, it's about lack of disk space. i don't care if the data is buffered on the hdd but i can't write the full iso. and of course genisoimage is just a suggestion, i'm open to any other method of creating the iso data. for your information it's intended for blu ray discs.",
        "present_kp": [
            "pipe"
        ],
        "absent_kp": [
            "debian"
        ],
        "predict": {
            "present_kp": [
                "directory structure",
                "pipe",
                "genisoimage",
                "cdrskin",
                "stdin"
            ],
            "absent_kp": [
                "cd burning",
                "tar",
                "blu ray discs"
            ]
        }
    },
    {
        "text": "should we use an outside cms?. i work at a web design/development shop. everything we do is centered around the joomla! cms. i'm a bit worried-if anything goes wrong with joomla (major security flaw revealed, joomla folds and ceases development) we're sunk. i'm meeting with the ceo to plan the next few steps for our company. should i recommend that we create our own in-house cms or am i just being paranoid about a single point of failure?",
        "present_kp": [
            "cms",
            "joomla"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "cms",
                "joomla"
            ],
            "absent_kp": [
                "web development",
                "security flaw"
            ]
        }
    },
    {
        "text": "auto labeling algorithm. i have a set of points (2d space), and for every point there's a label (like city names on a map).i want to find a real-time algorithm that allows labels to avoid overlapping, moving them from their original position if necessary.i've heard about simulated annealing algorithm, but i can't find a good source to learn how to do this. do you have some idea where i can find bibliography and practical examples?thanks in advance for your replies.",
        "present_kp": [],
        "absent_kp": [
            "computational geometry"
        ],
        "predict": {
            "present_kp": [
                "auto labeling algorithm",
                "simulated annealing algorithm"
            ],
            "absent_kp": [
                "2d space",
                "real-time algorithm",
                "label overlapping"
            ]
        }
    },
    {
        "text": "on the notion of positive rank of a matrix. the positive rank of a square matrix is defined in theorem $3$ of expressing combinatorial optimization problems by linear programs by mihalis yannakakis as follows: given a $n imes n$ matrix $a$, the positive rank $rank_{\\bbb r}^+(a)$ is the smallest $m$ such that $a=lr$ for a non-negative $n imes m$ matrix $l$, and non-negative $m imes n$ matrix $r$.this concept is valuable in communication complexity, since it was shown that if $rank_{\\bbb r}^+(a)$ and $rank_{\\bbb r}(a)$ could be subexponentially related for a $0/1$ matrix $a$, then the log-rank conjecture holds.is there an exponential separation between $rank_{\\bbb r}^+(a)$ and $rank_{\\bbb r}(a)$ for a general non-negative real matrix $a$ (as opposed to just $0/1$) or is this problem also open?i checked the references in jukna's book, but i am still unable to clarify the above question.",
        "present_kp": [
            "communication complexity"
        ],
        "absent_kp": [
            "definitions"
        ],
        "predict": {
            "present_kp": [
                "positive rank",
                "matrix"
            ],
            "absent_kp": [
                "communication complexity",
                "log-rank conjecture"
            ]
        }
    },
    {
        "text": "how to delete the filesytem on a raid device?. how do i delete the filesystem on a raid device?sudo blkid /dev/md1/dev/md1: uuid=9a27b794-12d7-4794-9764-dda623f12e58 type=ext4",
        "present_kp": [],
        "absent_kp": [
            "filesystems",
            "software raid"
        ],
        "predict": {
            "present_kp": [
                "raid device",
                "filesystem"
            ],
            "absent_kp": [
                "linux",
                "storage management"
            ]
        }
    },
    {
        "text": "powershell to quickly ping a number of machines. i came up with the below code to improve the peformance of pinging a large number of machines.at present it's fairly basic, but thought i should see what people thought before proceeding further.nb: this is my first time playing with workflows, so i've probably committed a few faux-pas there.clsworkflow test-connectionquickly { [cmdletbinding()] param( [parameter(mandatory = $true)] [string[]]$computers ) $computergroups = inlinescript { #group computers to throttle the number of threads [psobject]$itemcounter = [pscustomobject]@{itemno=0;groupsize=10} #alter groupsize per your preference $itemcounter | add-member -membertype scriptmethod -name 'groupno' -value { [math]::floor($this.itemno++ / $this.groupsize) } write-output $using:computers | group-object -property {$itemcounter.groupno()} } foreach -parallel ($computergroup in $computergroups) { $computergroup | select -expandproperty group | %{ $pingable = (test-connection $_ -count 2 -quiet -erroraction silentlycontinue) write-output (new-object -type psobject -property @{name=$_;online=$pingable}) } } }[string[]]$computers = (1..100 | %{(server{0:000} -f $_)}) #here's where we'd read in the computere from filetest-connectionquickly $computers | select name, online #here's where we'd pipe the output to filedesign notes:the inlinescript is used to group servers into small sets (of 10; an arbitrary size) in the hope of balancing serial's performance for small sets against parallel's performance for large sets of computers.the $itemcounter variable's an attempt at making grouping into defined set sizes simpler to read-count 2 is specified on the test-connection so that we have some tolerence for network glitches without too much affect on performance.-quiet is specified because we don't need output / and not having output will help performance.",
        "present_kp": [
            "powershell"
        ],
        "absent_kp": [
            "networking",
            "status monitoring"
        ],
        "predict": {
            "present_kp": [
                "powershell",
                "ping",
                "workflow"
            ],
            "absent_kp": [
                "network performance",
                "script optimization"
            ]
        }
    },
    {
        "text": "how to compile with third party libs properly?. this is a follow up question to confusion about linking boost library while compilation:what is to do, when i generate a makefile by qmake and i have only a third party boost lib installed (i uninstalled all boost libs from dependency management, because it always links to the boost lib from dependency management what i don't want) and i want it to compile only against this manually installed library as well as run against it.these are the important parts of a makefile generated by qmake:cc = gcccxx = g++defines = -dqt_gui -dboost_thread_use_lib -dboost_spirit_threadsafe -dboost_thread_provides_generic_shared_mutex_on_win -d__no_system_includes -duse_upnp=1 -dstaticlib -duse_qrcode -duse_dbus -dhave_build_info -dlinux -dqt_no_debug -dqt_dbus_lib -dqt_gui_lib -dqt_core_lib -dqt_sharedcflags = -m64 -pipe -o2 -wall -w -d_reentrant $(defines)cxxflags = -m64 -pipe -fstack-protector -o2 -fdiagnostics-show-option -wall -wextra -wformat -wformat-security -wno-unused-parameter -d_reentrant $(defines)incpath = -i/usr/share/qt4/mkspecs/linux-g++-64 -i/usr/include/qt4/qtcore -i/usr/include/qt4/qtgui -i/usr/include/qt4/qtdbus -i/usr/include/qt4 -isrc -isrc/json -isrc/qt -ic:/deps/ -ic:/deps/boost -ic:/deps/db/build_unix -ic:/deps/ssl/include -ic:/deps/libqrencode/ -ibuild -ibuildlink = g++lflags = -m64 -fstack-protector -wl,-o1libs = $(sublibs) -l/usr/lib/x86_64-linux-gnu -lc:/deps/miniupnpc -lminiupnpc -lqrencode -lrt -lc:/deps/boost/stage/lib -lc:/deps/db/build_unix -lc:/deps/ssl -lc:/deps/libqrencode/.libs -lssl -lcrypto -ldb_cxx -lboost_system-mgw46-mt-sd-1_54 -lboost_filesystem-mgw46-mt-sd-1_54 -lboost_program_options-mgw46-mt-sd-1_54 -lboost_thread-mgw46-mt-sd-1_54 -lqtdbus -lqtgui -lqtcore -lpthread this is the path to boost:/usr/local/lib/boost1.55/lib# ls -1libboost_atomic.alibboost_atomic.solibboost_atomic.so.1.55.0libboost_chrono.alibboost_chrono.solibboost_chrono.so.1.55.0libboost_context.alibboost_context.solibboost_context.so.1.55.0libboost_coroutine.alibboost_coroutine.solibboost_coroutine.so.1.55.0libboost_date_time.alibboost_date_time.solibboost_date_time.so.1.55.0libboost_exception.alibboost_filesystem.alibboost_filesystem.solibboost_filesystem.so.1.55.0libboost_graph.alibboost_graph.solibboost_graph.so.1.55.0libboost_locale.alibboost_locale.solibboost_locale.so.1.55.0libboost_log.alibboost_log_setup.alibboost_log_setup.solibboost_log_setup.so.1.55.0libboost_log.solibboost_log.so.1.55.0libboost_math_c99.alibboost_math_c99f.alibboost_math_c99f.solibboost_math_c99f.so.1.55.0libboost_math_c99l.alibboost_math_c99l.solibboost_math_c99l.so.1.55.0libboost_math_c99.solibboost_math_c99.so.1.55.0libboost_math_tr1.alibboost_math_tr1f.alibboost_math_tr1f.solibboost_math_tr1f.so.1.55.0libboost_math_tr1l.alibboost_math_tr1l.solibboost_math_tr1l.so.1.55.0libboost_math_tr1.solibboost_math_tr1.so.1.55.0libboost_prg_exec_monitor.alibboost_prg_exec_monitor.solibboost_prg_exec_monitor.so.1.55.0libboost_program_options.alibboost_program_options.solibboost_program_options.so.1.55.0libboost_random.alibboost_random.solibboost_random.so.1.55.0libboost_regex.alibboost_regex.solibboost_regex.so.1.55.0libboost_serialization.alibboost_serialization.solibboost_serialization.so.1.55.0libboost_signals.alibboost_signals.solibboost_signals.so.1.55.0libboost_system.alibboost_system.solibboost_system.so.1.55.0libboost_test_exec_monitor.alibboost_thread.alibboost_thread.solibboost_thread.so.1.55.0libboost_timer.alibboost_timer.solibboost_timer.so.1.55.0libboost_unit_test_framework.alibboost_unit_test_framework.solibboost_unit_test_framework.so.1.55.0libboost_wave.alibboost_wave.solibboost_wave.so.1.55.0libboost_wserialization.alibboost_wserialization.solibboost_wserialization.so.1.55.0this is the output of ldconfig -v concerning boost:# ldconfig -v/sbin/ldconfig.real: path '/lib/x86_64-linux-gnu' given more than once/sbin/ldconfig.real: path '/usr/lib/x86_64-linux-gnu' given more than once/usr/local/lib/boost1.55/lib: libboost_wave.so.1.55.0 -> libboost_wave.so.1.55.0 libboost_thread.so.1.55.0 -> libboost_thread.so.1.55.0 libboost_system.so.1.55.0 -> libboost_system.so.1.55.0 libboost_prg_exec_monitor.so.1.55.0 -> libboost_prg_exec_monitor.so.1.55.0 libboost_context.so.1.55.0 -> libboost_context.so.1.55.0 libboost_atomic.so.1.55.0 -> libboost_atomic.so.1.55.0 libboost_filesystem.so.1.55.0 -> libboost_filesystem.so.1.55.0 libboost_math_c99l.so.1.55.0 -> libboost_math_c99l.so.1.55.0 libboost_math_c99.so.1.55.0 -> libboost_math_c99.so.1.55.0 libboost_timer.so.1.55.0 -> libboost_timer.so.1.55.0 libboost_wserialization.so.1.55.0 -> libboost_wserialization.so.1.55.0 libboost_math_c99f.so.1.55.0 -> libboost_math_c99f.so.1.55.0 libboost_coroutine.so.1.55.0 -> libboost_coroutine.so.1.55.0 libboost_signals.so.1.55.0 -> libboost_signals.so.1.55.0 libboost_random.so.1.55.0 -> libboost_random.so.1.55.0 libboost_chrono.so.1.55.0 -> libboost_chrono.so.1.55.0 libboost_program_options.so.1.55.0 -> libboost_program_options.so.1.55.0 libboost_date_time.so.1.55.0 -> libboost_date_time.so.1.55.0 libboost_locale.so.1.55.0 -> libboost_locale.so.1.55.0 libboost_log.so.1.55.0 -> libboost_log.so.1.55.0 libboost_log_setup.so.1.55.0 -> libboost_log_setup.so.1.55.0 libboost_serialization.so.1.55.0 -> libboost_serialization.so.1.55.0 libboost_math_tr1f.so.1.55.0 -> libboost_math_tr1f.so.1.55.0 libboost_unit_test_framework.so.1.55.0 -> libboost_unit_test_framework.so.1.55.0 libboost_math_tr1l.so.1.55.0 -> libboost_math_tr1l.so.1.55.0 libboost_graph.so.1.55.0 -> libboost_graph.so.1.55.0 libboost_math_tr1.so.1.55.0 -> libboost_math_tr1.so.1.55.0 libboost_regex.so.1.55.0 -> libboost_regex.so.1.55.0what do i have exactly to do to compile and run the code properly? i tried combinations of:-l/usr/local/lib/boost1.55/lib/boost_thread-mgw46-mt-sd-1_54-l/usr/local/lib/boost1.55/lib/boost_thread-i/usr/local/lib/boost1.55/-i/usr/local/lib/boost1.55/lib/-lboost_system-mgw46-mt-sd-1_54-lboost_system-mgw46-mt-sd-1_55-lboost_systemall this never works when there is no boost installed by package manager, but i don't want it to use it from package manager. that means it doesn't compile. sometimes i get something like:/usr/bin/ld: cannot find -lboost_system-mgw46-mt-sd-1_54or /usr/bin/ld: cannot find -lboost_systemor addrman.cpp:(.text.startup+0x23): undefined reference to 'boost::system::generic_category()'...and so on.i don't get it. what's wrong here?[update]it turns out that there seems to be something wrong with boost lib itself.after modifying the important parts of the makefile to:libs = $(sublibs) -l/usr/lib/x86_64-linux-gnu -lminiupnpc -lqrencode -lrt -lssl -lcrypto -ldb_cxx -l/usr/local/lib/boost1.55/ -l/usr/local/lib/boost1.55/include/ -l/usr/local/lib/boost1.55/lib/ -lboost_system -lboost_filesystem -lboost_program_options -lpthread -lboost_thread -lqtdbus -lqtgui -lqtcoremake produced another error:build/json_spirit_reader.o: in function 'void boost::call_once<void (*)()>(boost::once_flag&, void (*)())':json_spirit_reader.cpp:(.text._zn5boost9call_onceipfvveeevrns_9once_flaget_[_zn5boost9call_onceipfvveeevrns_9once_flaget_]+0x14): undefined reference to 'boost::detail::get_once_per_thread_epoch()'json_spirit_reader.cpp:(.text._zn5boost9call_onceipfvveeevrns_9once_flaget_[_zn5boost9call_onceipfvveeevrns_9once_flaget_]+0x2c): undefined reference to 'boost::detail::once_epoch_mutex'json_spirit_reader.cpp:(.text._zn5boost9call_onceipfvveeevrns_9once_flaget_[_zn5boost9call_onceipfvveeevrns_9once_flaget_]+0x35): undefined reference to 'boost::detail::once_epoch_mutex'json_spirit_reader.cpp:(.text._zn5boost9call_onceipfvveeevrns_9once_flaget_[_zn5boost9call_onceipfvveeevrns_9once_flaget_]+0x72): undefined reference to 'boost::detail::once_epoch_mutex'json_spirit_reader.cpp:(.text._zn5boost9call_onceipfvveeevrns_9once_flaget_[_zn5boost9call_onceipfvveeevrns_9once_flaget_]+0x77): undefined reference to 'boost::detail::once_epoch_cv'json_spirit_reader.cpp:(.text._zn5boost9call_onceipfvveeevrns_9once_flaget_[_zn5boost9call_onceipfvveeevrns_9once_flaget_]+0xa8): undefined reference to 'boost::detail::once_epoch_mutex'json_spirit_reader.cpp:(.text._zn5boost9call_onceipfvveeevrns_9once_flaget_[_zn5boost9call_onceipfvveeevrns_9once_flaget_]+0xb0): undefined reference to 'boost::detail::once_epoch_mutex'json_spirit_reader.cpp:(.text._zn5boost9call_onceipfvveeevrns_9once_flaget_[_zn5boost9call_onceipfvveeevrns_9once_flaget_]+0xd9): undefined reference to 'boost::detail::once_global_epoch'json_spirit_reader.cpp:(.text._zn5boost9call_onceipfvveeevrns_9once_flaget_[_zn5boost9call_onceipfvveeevrns_9once_flaget_]+0xde): undefined reference to 'boost::detail::once_epoch_cv'json_spirit_reader.cpp:(.text._zn5boost9call_onceipfvveeevrns_9once_flaget_[_zn5boost9call_onceipfvveeevrns_9once_flaget_]+0xe9): undefined reference to 'boost::detail::once_global_epoch'json_spirit_reader.cpp:(.text._zn5boost9call_onceipfvveeevrns_9once_flaget_[_zn5boost9call_onceipfvveeevrns_9once_flaget_]+0x128): undefined reference to 'boost::detail::once_global_epoch'json_spirit_reader.cpp:(.text._zn5boost9call_onceipfvveeevrns_9once_flaget_[_zn5boost9call_onceipfvveeevrns_9once_flaget_]+0x19b): undefined reference to 'boost::detail::once_epoch_cv'collect2: error: ld returned 1 exit statusit seems that there is no such function (in this boost version?):$ objdump -t /usr/local/lib/boost1.55/lib/libboost_thread.so|c++filt|grep once_epochprints nothing as well as $ for i in /usr/local/lib/boost1.55/lib/libboost_*.so ; do if grep once_epoch_mutex <(objdump -t $i|c++filt) ; then echo $i ; fi ; donedoes not.[update 2]after adding -i/usr/local/lib/boost1.55/include/ -i/usr/local/lib/boost1.55/include/boost/to incpath and recompile the whole application within a fresh workspace, the error is different but now, i don't see any error message:/usr/local/lib/boost1.55/include/boost/bind/arg.hpp: in constructor boost::arg<i>::arg(const t&):/usr/local/lib/boost1.55/include/boost/bind/arg.hpp:37:22: warning: typedef t_must_be_placeholder locally defined but not used [-wunused-local-typedefs] typedef char t_must_be_placeholder[ i == is_placeholder<t>::value? 1: -1 ]; ^in file included from /usr/local/lib/boost1.55/include/boost/tuple/tuple.hpp:33:0, from /usr/local/lib/boost1.55/include/boost/thread/detail/async_func.hpp:37, from /usr/local/lib/boost1.55/include/boost/thread/future.hpp:22, from /usr/local/lib/boost1.55/include/boost/thread.hpp:24, from src/util.h:22, from src/bignum.h:13, from src/main.h:9, from src/wallet.h:9, from src/wallet.cpp:7:/usr/local/lib/boost1.55/include/boost/tuple/detail/tuple_basic.hpp: in function typename boost::tuples::access_traits<typename boost::tuples::element<n, boost::tuples::cons<ht, tt> >::type>::const_type boost::tuples::get(const boost::tuples::cons<ht, tt>&):/usr/local/lib/boost1.55/include/boost/tuple/detail/tuple_basic.hpp:228:45: warning: typedef cons_element locally defined but not used [-wunused-local-typedefs] typedef boost_deduced_typename impl::type cons_element; ^src/wallet.cpp: in member function bool cwallet::addtowallet(const cwallettx&):src/wallet.cpp:402:13: error: replace_all is not a member of boost boost::replace_all(strcmd, %s, wtxin.gethash().gethex()); ^in file included from /usr/local/lib/boost1.55/include/boost/system/system_error.hpp:14:0, from /usr/local/lib/boost1.55/include/boost/thread/exceptions.hpp:22, from /usr/local/lib/boost1.55/include/boost/thread/pthread/thread_data.hpp:10, from /usr/local/lib/boost1.55/include/boost/thread/thread_only.hpp:17, from /usr/local/lib/boost1.55/include/boost/thread/thread.hpp:12, from /usr/local/lib/boost1.55/include/boost/thread.hpp:13, from src/util.h:22, from src/bignum.h:13, from src/main.h:9, from src/wallet.h:9, from src/wallet.cpp:7:/usr/local/lib/boost1.55/include/boost/system/error_code.hpp: at global scope:/usr/local/lib/boost1.55/include/boost/system/error_code.hpp:222:36: warning: boost::system::posix_category defined but not used [-wunused-variable] static const error_category & posix_category = generic_category(); ^/usr/local/lib/boost1.55/include/boost/system/error_code.hpp:223:36: warning: boost::system::errno_ecat defined but not used [-wunused-variable] static const error_category & errno_ecat = generic_category(); ^/usr/local/lib/boost1.55/include/boost/system/error_code.hpp:224:36: warning: boost::system::native_ecat defined but not used [-wunused-variable] static const error_category & native_ecat = system_category(); ^make: *** [build/wallet.o] error 1",
        "present_kp": [
            "boost"
        ],
        "absent_kp": [
            "compiling",
            "libraries"
        ],
        "predict": {
            "present_kp": [
                "boost",
                "makefile",
                "compilation"
            ],
            "absent_kp": [
                "qmake",
                "third party library",
                "linker error"
            ]
        }
    },
    {
        "text": "locate a number in an array with ascending even and descending odd entries. below are two examples of arrays i have, with some given length:for some value z, i want to find the column the value is in. matlab code i wrote:if z == 1 column = ceil(length/2); elseif logical(mod(z,2)) column = length - (z-3)/2;else column = z/2;endit is correct i think. but ugly and possibly slow. is there a one-liner in matlab that can do this?",
        "present_kp": [
            "matlab"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "array",
                "matlab"
            ],
            "absent_kp": [
                "algorithm",
                "search"
            ]
        }
    },
    {
        "text": "plan variable and call dependencies. i'd like to write down the design of my program to understand the dependencies and calls better. i know there are class diagrams which show inheritance and attribute variables. however i'd also like to document the input parameters to method functions and in particular which calls the methods function executes inside (e.g. on the input parameters).also sometimes it might be useful to show how actual objects are connected (if there is a standard structure).this way i can have a better understanding of the modules and design before starting to program. can you suggest a method to do this software design? it should be one-to-one to programming code structure so that i really notice all quirks beforehand (instead of high-level design where thing are hard to implement without further work).maybe some special diagram or tool or a combination?it is static dependency and call design rather than time dependent execution monitoring.(i use python if you have any specialized recommendations).",
        "present_kp": [
            "design"
        ],
        "absent_kp": [
            "uml"
        ],
        "predict": {
            "present_kp": [
                "design",
                "dependencies",
                "program",
                "class diagrams",
                "input parameters",
                "method functions",
                "software design",
                "python"
            ],
            "absent_kp": [
                "uml",
                "code structure",
                "static analysis",
                "dependency graph"
            ]
        }
    },
    {
        "text": "numerical euler rotation equation. the problem i have may be really simple, but still getting a hard time solving it. so i have the euler rotation equations:$$i_{1}\\dot{\\omega}_{1}+\\left(i_{3}-i_{2} ight)\\omega_{2}\\omega_{3}=\\lambda_{1}$$$$i_{2}\\dot{\\omega}_{2}+\\left(i_{1}-i_{3} ight)\\omega_{3}\\omega_{1}=\\lambda_{2}$$$$i_{3}\\dot{\\omega}_{3}+\\left(i_{2}-i_{1} ight)\\omega_{1}\\omega_{2}=\\lambda_{3}$$where the $i_{i}$ are the moments of inertia about the principal axes of rotation and $\\omega_{i}$ the time dependent angular velocities about each axis. in general $i_{i} eq i_{j}, i eq j$, or don't satisfy any situations in which they can be reduced to easier relations. $\\lambda_{i}$ is a normal distributed random number.i know that these equations are non-linear and, in general, have no analytic solution. the question is:$\\qquad$ is there a good numerical integrator for these equations?i'm using c++ and have looked into the lapack package, but i'm kind of confused on how to use it.i know the reference from skowron and gould (arxiv:<phone> [astro-ph.ep]); however i really don't know how to implement this algorithm.if somebody could help me in finding an open source integrator or a reference where they talk about the implementation of this code, it would be great.",
        "present_kp": [],
        "absent_kp": [
            "computational physics",
            "software"
        ],
        "predict": {
            "present_kp": [
                "euler rotation equations",
                "numerical integrator"
            ],
            "absent_kp": [
                "c++",
                "lapack",
                "open source integrator"
            ]
        }
    },
    {
        "text": "lu factorization of a 0-1 matrix. i have a rather naive question on lu factorization which probably should be easy to answer. say i have a matrix with entries only from $\\{0,1\\}$. when can we expect to get an lu factorization of such a matrix(whenever it exists) with entries $(a)$ from integers? $(b)$ from $\\{-1,0,+1\\}$?",
        "present_kp": [],
        "absent_kp": [
            "linear algebra",
            "matrices",
            "na.numerical analysis"
        ],
        "predict": {
            "present_kp": [
                "lu factorization",
                "matrix"
            ],
            "absent_kp": [
                "linear algebra",
                "numerical methods"
            ]
        }
    },
    {
        "text": "why can't subjective utilities take probabilities into account?. if my understanding of expected utility theory is correct, it is rational for a decision maker to have subjective utilities for objective consequences. for example, it can be rational for a decision maker to value 5 dollars twice as much as 4 dollarshowever, it is not rational for a decision maker to value a 100% chance of 5 dollars twice as much as an 80% chance of 5 dollarsthat is, probabilities must be outside of- not taken into account by- subjective utilities. is this understanding correct, and if so, why is the theory structured this way? note: my understanding is based on hastie & dawes (2010), rational choice in an uncertain world.",
        "present_kp": [],
        "absent_kp": [
            "decision making",
            "economics",
            "rationality",
            "behavioral economics"
        ],
        "predict": {
            "present_kp": [
                "subjective utilities",
                "expected utility theory",
                "decision maker"
            ],
            "absent_kp": [
                "probabilities",
                "rational choice",
                "uncertain world"
            ]
        }
    },
    {
        "text": "sed into csv format. i have a file of the formvl-<phone>,30.000,49.000,1.000,21.901,2630.000,428861.000vl-<phone>,1071.000,570.000,35.000,3963.608,632.000,366563.000vl-<phone>,36.000,867.000,24.000,6523.005,3544.000,176054.000vl-<phone>,5:281185.000vl-<phone>,44.000,372.000,67.000,7029.358,293.000,446448.000vl-<phone>,5:48479.000vl-<phone>,0:2.000,2:7.000,3:80.222,4:1153.000it is supposed to be of the formvl-<phone>,1190.000,609.000,28.000,12676.158,1819.000,452813.000but when there are zeros in the file it only shows the column numbers that are nonzero such asvl-<phone>,0:2.000,2:7.000,3:80.222,4:1153.000i would like to write a sed command that makes a 7 length row with zeros included such as vl-<phone>,2.000,0,7.000,80.222,1153.000,0any ideas?",
        "present_kp": [
            "sed"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "csv format",
                "sed command"
            ],
            "absent_kp": [
                "data processing",
                "file manipulation"
            ]
        }
    },
    {
        "text": "open / closed principle. i found this code example explaining open / closed principle.code before application of principle:public class logger{ public void log(string message, logtype logtype) { switch (logtype) { case logtype.console: console.writeline(message); break; case logtype.file: // code to send message to printer break; } }}public enum logtype{ console, file}and refactored code:public class logger{ imessagelogger _messagelogger; public logger(imessagelogger messagelogger) { _messagelogger = messagelogger; } public void log(string message) { _messagelogger.log(message); }}public interface imessagelogger{ void log(string message);} public class consolelogger : imessagelogger{ public void log(string message) { console.writeline(message); }}public class printerlogger : imessagelogger{ public void log(string message) { // code to send message to printer }}can you explain me the reason to still keep logger class with private imessagelogger instance? i would simply avoid it by:public interface ilogger{ public void log(string message);}public class consolelogger : ilogger{ public void log(string message) { console.writeline(message); }} public class printerlogger : ilogger{ public void log(string message) { // code to send message to printer }}the only reason i can think about is, that in suggested solution with logger class, we could still refer to this class in client code, but we still need to modify all log(msg) calls to remove logtype arguments.",
        "present_kp": [],
        "absent_kp": [
            "c#",
            "interfaces",
            "solid",
            "open close"
        ],
        "predict": {
            "present_kp": [
                "open / closed principle",
                "logger class"
            ],
            "absent_kp": [
                "solid principles",
                "design patterns"
            ]
        }
    },
    {
        "text": "project euler #48 in c++. this question is similar to project euler #48 but constraints are different:$$n < <phone>$$just in case the link is unavailable, here is the problem statement:we've to print $$\\left( \\sum_{i=1}^n i^i ight) \\mod 10^{10}$$i've tried the following, but i need something faster than that, because the execution time limit is 2s, and this program can only compute values until \\$ n=30000\\$ (approx) in the given time limit.#include <iostream>using namespace std;#define mod 10000000000int main() { int n; cin>>n; long long temp,sum=0; for( int ii=1 ; ii<=n ; ii++ ) { if(ii%10==0) { continue; } temp=1; for( int jj=1 ; jj<=ii ; jj++ ) { temp*=ii; temp=temp%mod; } sum+=temp; sum=sum%mod; } cout<<sum; return 0;}i've added if(n%10==0){ continue;}because numbers of form $$ (k*10)^{k*10}=k^{k*10}*10^{k*10}=k^{k*10}*10^{k}*10^{10} $$ are not at all going to contribute to the answer.note: code is compiled using g++ 4.8.2, c++11 mode",
        "present_kp": [
            "c++",
            "c++11"
        ],
        "absent_kp": [
            "optimization",
            "programming challenge",
            "time limit exceeded"
        ],
        "predict": {
            "present_kp": [
                "c++",
                "project euler"
            ],
            "absent_kp": [
                "modular arithmetic",
                "optimization"
            ]
        }
    },
    {
        "text": "list all files that end in ball using ls command. i am trying to use ls command to find specific files thathave 4 letters in front of the word ballmust have the ending word balli have been trying to use ls *ball but this shows words with 4 or more words in front of the word ball. is there a specific command that ignores the word that has 4 or more letters before ball?",
        "present_kp": [
            "ls"
        ],
        "absent_kp": [
            "linux"
        ],
        "predict": {
            "present_kp": [
                "ls command"
            ],
            "absent_kp": [
                "file search",
                "linux"
            ]
        }
    },
    {
        "text": "what does chromium net::err_cert_common_name_invalid mean?. i'm trying to figure out why chromium is not happy with a tls certificate, and how to fix it:after upgrading & restarting chromium (now 58.0.3029.81, running on debian testing), i can no longer access our internal gitlab server (installed on debian jessie, via the omnibus package). i get:your connection is not privateattackers might be trying to steal your information from git.ourdomain.net (for example, passwords, messages, or credit cards). net::err_cert_common_name_invalidthe certificate is signed with our internal ca, which is installed in the system store (by putting it in /usr/local/share/ca-certificates). i checked the site with both firefox 52 and openssl s_client -verify 5 -verify_return_error -connect git.ourdomain.net:443; both are happy. openssl shows the chain as:certificate chain 0 s:/c=us/st=virginia/l=sterling/o=us/ou=servers/cn=git.ourdomain.net i:/c=us/st=virginia/l=sterling/o=us/cn=us certification authority/emailaddress=<email> openssl and firefox show strong signing (sha-512) and ciphers (aes-gcm). the certificate (according to openssl x509 -text) is sha512withrsaencryption, with a 4096-bit rsa key. it has a netscape cert type of ssl client, ssl server.note: us and ourdomain.net are redactions; the actual output has our company name for us and our actual domain for ourdomain.net. i carefully checked that all the ourdomain.net actually match.as far as i can tell, there is nothing wrong with the certificate, and the common name (git.ourdomain.net) is perfectly valid and matches the urlso what is chromium complaining about? and, presuming it's not a real issue, is there some way to override it?",
        "present_kp": [
            "ssl"
        ],
        "absent_kp": [
            "chrome",
            "https"
        ],
        "predict": {
            "present_kp": [
                "chromium",
                "tls certificate",
                "common name"
            ],
            "absent_kp": [
                "ssl",
                "certificate authority",
                "debian"
            ]
        }
    },
    {
        "text": "algorithm to project onto line segments. i have the following problem: a large number $n$ of (finite length) line segments in the plane (if it helps, we can assume non-intersecting except at end points, and forming a graph with a small number of components); and a smaller number $n$ of points. for each point, i wish to find the closest line segment.given one line segment, this is easy: orthogonally project onto the line, and if this doesn't fall on the line segment, choose the appropriate end point. this gives a naive $o(nn)$ algorithm.i am wondering if there is a clever data structure which, with some pre-processing on the lines, would give a faster algorithm?",
        "present_kp": [],
        "absent_kp": [
            "computational geometry"
        ],
        "predict": {
            "present_kp": [
                "line segments",
                "project onto line",
                "algorithm"
            ],
            "absent_kp": [
                "data structure",
                "pre-processing",
                "graph"
            ]
        }
    },
    {
        "text": "invoking shader in dx. so i am new to the dx12 world. i am currently trying to tweek the nbody_gravity dx12 sample, but appending or calling another compute shader which is to add 2 buffers to produce an output in a different buffer.i have written this code in computeshader.hlsl however when i tried to invoke it using the following, i get an error sayin file not foundwhen i printed the debug error it says the following: d3d12getdebuginterface: this method requires the d3d12 sdk layers for windows 10, but they are not present on the system.i am invoking my shader after the default invocation to the nbodygravity.hlsl is done:comptr<id3dblob> computeshader;comptr<id3dblob> computeshader_m;#if defined(_debug) // enable better shader debugging with the graphics debugging tools. uint compileflags = d3dcompile_debug | d3dcompile_skip_optimization;#else uint compileflags = 0;#endif throwiffailed(d3dcompilefromfile(getassetfullpath(lnbodygravitycs.hlsl).c_str(), nullptr, nullptr, csmain, cs_5_0, compileflags, 0, &computeshader, nullptr));//extra call for my shadercomptr<id3dblob> error_ptr; throwiffailed(d3dcompilefromfile(getassetfullpath(lcomputeshader.hlsl).c_str(), nullptr, nullptr, main, cs_5_0, compileflags, 0, &computeshader_m, &error_ptr));if (error_ptr){ outputdebugstringa((char *)error_ptr->getbufferpointer());}is there somthing amiss?",
        "present_kp": [],
        "absent_kp": [
            "directx"
        ],
        "predict": {
            "present_kp": [
                "shader",
                "dx12"
            ],
            "absent_kp": [
                "compute shader",
                "directx"
            ]
        }
    },
    {
        "text": "how to find release date of any .rpm before installing it. is there any way to get the release date (via command line) of an rpm without downloading or installing it?i can get detailed information via the command below but can't find the release date of that particular rpm:[root@connect ~]# yum info kernel-2.6.32-642.6.2.el6.x86_64loaded plugins: fastestmirrorloading mirror speeds from cached hostfile * base: centos.excellmedia.net * epel: ftp.jaist.ac.jp * extras: centos.excellmedia.net * updates: centos.excellmedia.netavailable packagesname : kernelarch : x86_64version : 2.6.32release : 642.6.2.el6size : 32 mrepo : updatessummary : the linux kernelurl : <url> : gplv2description : the kernel package contains the linux kernel (vmlinuz), the core of any : linux operating system. the kernel handles the basic functions : of the operating system: memory allocation, process allocation, device : input and output, etc.",
        "present_kp": [
            "linux",
            "rpm"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "release date",
                "rpm",
                "command line"
            ],
            "absent_kp": [
                "linux",
                "package manager"
            ]
        }
    },
    {
        "text": "can i disable the download all feature on a shared google photos album?. if i share a google photos album, other people who view it see a download all button at the top of the page, as shown in the screenshot below. you should be able to see it yourself if you visit this album.is there any way to prevent that button from being shown? i've looked in my google+ settings under photos and videos, and the option allow viewers to download my photos and video is already unchecked.",
        "present_kp": [
            "google photos"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "google photos album",
                "download all feature"
            ],
            "absent_kp": [
                "google+ settings",
                "screenshot"
            ]
        }
    },
    {
        "text": "kernel32.basethreadinitthunk without iat. on starting notepad.exe with ollydbg, i see that eax has a value that points at kernel32.basethreadinitthunk.notepad.exe does not seem to import kernel32.dll::basethreadinitthunk.i cannot find that function, by running dependency walker on notepad.exe.how can kernel32.dll::basethreadinitthunk function be executed without importing it ?",
        "present_kp": [
            "dll",
            "iat"
        ],
        "absent_kp": [
            "windows"
        ],
        "predict": {
            "present_kp": [
                "kernel32.basethreadinitthunk",
                "notepad.exe",
                "ollydbg"
            ],
            "absent_kp": [
                "import table",
                "dependency walker"
            ]
        }
    },
    {
        "text": "ssh over multiple server and save output of multiple commands in file on local server. i want to ssh over multiple server (host 1, host2 and host 3) and save output of multiple commands (cmd1, cmd2 etc) in file (output.properties) on local server. i know there are a few posts similar to this, but i a not sure where i am going wrong. below is the code snippet.folderpath=cd /usr/local/apps/tempdir;echo $folderpath;eval $folderpath;# host 1echo repcard1=$(expr $(grep -r sample text * | wc -l) / 2) >> /usr/local/host1/tempdir/output.properties;# server 2ssh -t user@host2 <<eof >>/usr/local/host1/tempdir/output.properties;#alias getduplicatecardstats=cd /usr/local/apps/tempdirfolderpath=cd /usr/local/apps/tempdir;echo $folderpath;eval $folderpath;echo repcard2=$(expr $(grep -r sample text * | wc -l) / 2);exiteof. /usr/local/host1/tempdir/output.properties;echo host 1echo $repcard1echo host 2echo $repcard2# repcard2 is always executed on host 1 and print value from host 1what is the mistake in the above script?",
        "present_kp": [
            "ssh"
        ],
        "absent_kp": [
            "shell script"
        ],
        "predict": {
            "present_kp": [
                "ssh",
                "server",
                "commands",
                "file"
            ],
            "absent_kp": [
                "local server",
                "output"
            ]
        }
    },
    {
        "text": "time-dependent state machine. i keep having to write state machines that depend on time for various experiments i run and i'd like to know how to write them better. this state machine is for training a neural network by feeding in keys and expected values.import numpy as npdt = 0.001period = 0.1class simpleenv(object): def __init__(self, keys, values, env_period=0.1): self.keys = keys self.values = values self.env_idx = np.arange(len(keys)) self.idx = 0 self.shuffled = false self.i_every = int(round(env_period/dt)) if self.i_every != env_period/dt: raise valueerror(dt (%s) does not divide period (%s) % (dt, period)) def get_key(self): return self.keys[self.idx] def get_val(self): return self.values[self.idx] def step(self, t): i = int(round((t - dt)/dt)) # t starts at dt ix = (i/self.i_every) % len(self.keys) if ix == 0 and not self.shuffled: print(shuffling) np.random.shuffle(self.env_idx) self.shuffled = true elif ix == 1: self.shuffled = false self.idx = self.env_idx[ix] return ix# note the toy keys and values for testing purposess_env = simpleenv(np.arange(4), np.arange(1, 5), env_period=period)key = -1val = -1ix = -1# iterate through keys and values twicerun_time = 4 * period * 2# the event loop# starts at dt because of reasonsfor t in np.arange(dt, run_time, dt): last_ix = ix ix = s_env.step(t) key = s_env.get_key() val = s_env.get_val() assert key + 1 == val if last_ix != ix: print(key: %s, value: %s %(key, val))the results should look something like:shufflingkey: 2, value: 3key: 0, value: 1key: 3, value: 4key: 1, value: 2shufflingkey: 2, value: 3key: 1, value: 2key: 3, value: 4key: 0, value: 1how can i write this better or more efficiently? is there a state machine library in python that would stop me from having to rewrite variations of this class all the time?",
        "present_kp": [
            "python",
            "state machine"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "state machine",
                "neural network",
                "python"
            ],
            "absent_kp": [
                "time-dependent",
                "library"
            ]
        }
    },
    {
        "text": "find the subarray with the max sum. in an interview i was asked to solve the following problem:find a subarray with max sumi have written a piece of code for the same. i need your help in reviewing this code.package com.ankit.rnd;public class maxsubarrsum { int largestsum=0; int previouslargestsum=0; public static void main(string[] args) { // int [] array = {-2,1,-3,4,-1,2,1,-5,4}; int [] array = {-5,1,-3,7,-1,2,1,-4,6}; // int [] array = {-2,-3,-4,2}; maxsubarrsum obj = new maxsubarrsum(); for(int varindex=0;varindex<array.length;varindex++){ // int sumis =new maxsubarrsum().findsum(varindex,array); // system.out.println(sumis:: +sumis); obj.splitcurrentarray(varindex,array); } } private void splitcurrentarray(int in, int[] arr) { int [] temparr = new int[arr.length-in]; for(int i=in;i<arr.length;i++){ if(in ==0){ temparr[i] = arr[i]; } else{ temparr[i-in] = arr[i]; } } int sum =findsum(in, temparr); system.out.println(previous largest sum:: + previouslargestsum); system.out.println(largest sum found: + sum); } @suppresswarnings(unused) private int findsum(int start,int [] array) { int[] currentarray ={}; int [] largestarray = new int[array.length]; int sum=0; /*for(int i=start;i<array.length;i++){*/ for(int i=0;i<array.length;i++){ //a little inefficient here as it always create an array with size more than total number of elements that should be there in the temp array. int psuedoindex=i; if(start==0){ currentarray = new int[i + 1]; for (int j = 0; j <= i; j++) { currentarray[j] = array[j]; psuedoindex=psuedoindex+1; } } else { currentarray = new int[i+1]; for (int j = 0; j <= i; j++) { currentarray[j] = array[j]; /* * if(psuedoindex == array.length){ //needs a fix. as we * have reached the end of the array. //currentarray[j] = * array[psuedoindex-1]; currentarray[j] = 0; break; } else{ * * currentarray[j] = array[psuedoindex]; is commented out * because it missed the element in the previous array. * * * //currentarray[j] = array[psuedoindex]; currentarray[j] = * array[j]; } */ psuedoindex=psuedoindex+1; } } if((sum = calculate(currentarray))>largestsum){ previouslargestsum=largestsum; largestsum=sum; for(int k=0;k<currentarray.length;k++){ system.out.print(currentarray[k] + |); } system.out.println(); } } return largestsum; } private int calculate(int [] currentarr){ int sumofelements =0; for(int index=0;index<currentarr.length;index++){ sumofelements +=currentarr[index]; } //system.out.println(sum is: + sumofelements); return sumofelements; } }",
        "present_kp": [
            "array"
        ],
        "absent_kp": [
            "java",
            "interview questions"
        ],
        "predict": {
            "present_kp": [
                "subarray",
                "max sum"
            ],
            "absent_kp": [
                "java",
                "algorithm"
            ]
        }
    },
    {
        "text": "complex search program. i am working on a fairly complex (at least it feels complex to me at the moment) search program and am looking to possibly increase the performance. everything works exactly how i want it to, but i'm just wondering if there are any slight performance increases i could benefit from. in this code, even minor performance increases are essential. this is due to the extensive amount of operations being performed.// split search into multiple terms and check the {x} longest terms against the cache.string[] word = searchterm.split(' ');array.sort(word, (x, y) => y.length.compareto(x.length));for (int i = 0; i < word.length; i++){ string searchvalue = word[i]; if (i <= max_word_iterations && (xmlsearchresults == blank_search_xml_schema || xmlsearchresults == string.empty)) { xmlsearchresults = getcachedrecord(thissearch, searchvalue); } if (xmlsearchresults != blank_search_xml_schema && xmlsearchresults != string.empty) { thissearch.value = searchvalue; ignorecache = true; break; }}xmlstring.append(string.format(@<{0}>, xmlheader));int objcount = 0;string prevresultname = string.empty;try{ if (searchresults.hitcount > 0) { list<searchitem> searchresults = new list<searchitem>(); foreach (node resultnode in results.nodes) { string code = resultnode.code.tostring(); string result = resultnode.id.tostring(); string name = resultnode.name_l.tostring(); string image = string.empty; if (!code.tolower().contains(ccb) && code.length <= max_code_length && name != prevresultname && objcount < maxresults) { boolean addresult = true; if (thissearch.filterdescription) { if (!name.tolower().contains(thissearch.value.tolower())) { addresult = false; } } if (addresult) { searchitem osearch = new searchitem(name, code); searchresults.add(osearch); prevresultname = name; } } } var sortedsearchresults = searchresults.orderbydescending(s => s.downloads).tolist(); for (int i = 0; i < sortedsearchresults.count(); i++) { if ((objcount < maxresults)) { string te = string.format(<resultname>{0}</resultname>, sortedsearchresults[i].description); if (!xmlstring.tostring().contains(te)) { xmlstring.append(sortedsearchresults[i].getxmlstring()); objcount += 1; } } } }}xmlstring.append(string.format(@</{0}>, xmlheader));return objcount;",
        "present_kp": [
            "performance",
            "xml",
            "search"
        ],
        "absent_kp": [
            "c#"
        ],
        "predict": {
            "present_kp": [
                "search program",
                "performance increase"
            ],
            "absent_kp": [
                "c#",
                "xml parsing"
            ]
        }
    },
    {
        "text": "software which will fetch all relevant included files for a web app?. hopefully this is the correct stack exchange site to be asking this question.i've inherited a fairly large web app from the previous it guy at my organisation. it's written in php and there's no documentation. my php is very rusty at best but i've been given the task of looking at the code for the web app, working out what it does, and preparing it so it can definitely be moved to another host with no problems. the directories in which the relevant php, js and css files sit have a whole load of other php, js and css files in them - things like early revisions of files, backups, slight changes etc. all very messy.i'd like to be able to just download locally all the files that are relevant to the app and not all the extra ones so that when i come to look at the code it'll be much easier to untangle. is there such a piece of software which, when given relevant permissions on the server, goes off and fetches an initial php file, looks for any code from includes and downloads them, does the same for css and js files. is there an ide which does something similar for when people need to debug and untangle others' web apps?",
        "present_kp": [
            "php"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "web app",
                "php",
                "js",
                "css",
                "server"
            ],
            "absent_kp": [
                "ide",
                "debug",
                "documentation"
            ]
        }
    },
    {
        "text": "the fastest desktop environment for mint 14. i'm using mint 14 with the cinnamon desktop environments, but some times it get's extremely slow.so i googled about other desktop environments and i found this great article:<url> i don't know which one is faster, i want to install the fastest desktop environment among them, because i have a low performances in my computer.",
        "present_kp": [
            "desktop environment"
        ],
        "absent_kp": [
            "linux mint"
        ],
        "predict": {
            "present_kp": [
                "desktop environment",
                "mint 14",
                "cinnamon"
            ],
            "absent_kp": [
                "linux",
                "performance optimization"
            ]
        }
    },
    {
        "text": "google apps script to check availability. i'm using google form+calendar to create a reservation system for my makers lab. the user fills out the form and an event is created on the calendar so the ta knows when to goto the lab to help the user.currently i'm using this script(which i found online and modified it a bit to fit my usage)+trigger to create events based on the form submissions var calendarid = <email> //below are the column ids of that represents the values used in the spreadsheet (these are non zero indexed) //column containing the start date+time for the event var startdtid = 6; //column containg the end date+time for the event var enddtid = 7; //column containing the first part of the title for the event (in this case, user id) var titleid = 4; //column containing the second part of the title for the event (in this case, user name) var titleid2 = 3; //column containing the user's mobile number var descid = 5; //column containing the time stamp for the event (this will always be 1) var formtimestampid = 1; //column containing the machine selected var mid = 8; function ensubmittocalendar() { //allow access to the spreadsheet var sheet = spreadsheetapp.getactivesheet(); var rows = sheet.getdatarange(); var numrows = rows.getnumrows(); var values = rows.getvalues(); var lr = rows.getlastrow(); var startdt = sheet.getrange(lr,startdtid,1,1).getvalue(); var enddt = sheet.getrange(lr,enddtid,1,1).getvalue(); //create an addition to the description to included when var subon = timestamp :+sheet.getrange(lr,formtimestampid,1,1).getvalue(); //setting the comments as the description, and adding in the time stamp var desc = subon; //create the title var title = sheet.getrange(lr,mid,1,1).getvalue()+-+sheet.getrange(lr,titleid,1,1).getvalue()+ +sheet.getrange(lr,titleid2,1,1).getvalue()+ +sheet.getrange(lr,descid,1,1).getvalue(); //run the crete event function createeventen(calendarid,title,startdt,enddt,desc); }; function createeventen(calendarid,title,startdt,enddt,desc) { var cal = calendarapp.getcalendarbyid(calendarid); var start = new date(startdt); var end = new date(enddt); //set the options, in this case we are only using description and location, as we do not need guests or sendinvites var event = cal.createevent(title, start, end, { description : desc, }); };it works well, and there's really no major problems to it, except when the user doesn't check availabilitysince we only have so many machines(3),if more than 3 users fill in the same time, the calendar will show more than 3 events, and it gets complicated.is there a way to script it so the script checks to see if there's 3 events during that time, and if so, it doesn't create a event?example:event a: 13:30-15:30event b: 13:30-14:30event c: 14:00-15:30user d submits a form that'll create event d: 14:00-15:30but because 14:00-14:30 there's already 3 events, event d isn't created (or is adjusted by the script to start at 14:30 instead)i'm thinking of using getevents(starttime, endtime, options) var checkavail = calendarapp.getdefaultcalendar().getevents(startdtid, enddtid);but i got no idea how to integrate this into my current scriptcan someone give me some pointers/comments? am i even on the right track?(sorry, i'm not at a level where i know how to write scripts, i only know how to modify existing scripts)",
        "present_kp": [
            "google apps script"
        ],
        "absent_kp": [
            "google spreadsheets",
            "google calendar"
        ],
        "predict": {
            "present_kp": [
                "google apps script",
                "calendar",
                "reservation system",
                "event creation",
                "availability check"
            ],
            "absent_kp": [
                "google form",
                "script modification",
                "event scheduling",
                "time conflict",
                "machine availability"
            ]
        }
    },
    {
        "text": "single line command to cat last file in ls -lrt output?. system log files are serialized and i use ls -lrt to show me the most recent file. i then cat that file. this requires typing a long serial number each time. how can i cat the last file appearing in my ls -lrt output in one command?i'm using cygwin and the the output from ls -lrt foobar_job* look like this:--rw-r--r-- 1 zundarz domain users 1133 jul 31 16:54 foobar_job4855125.log-rw-r--r-- 1 zundarz domain users 1256 jul 31 17:10 foobar_job4855127.log-rw-r--r-- 1 zundarz domain users 1389 aug 11 10:20 foobar_job4887829.log-rw-r--r-- 1 zundarz domain users 1228 aug 11 10:39 foobar_job4887834.log",
        "present_kp": [
            "ls",
            "cat"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "ls -lrt",
                "cat"
            ],
            "absent_kp": [
                "cygwin",
                "system log files"
            ]
        }
    },
    {
        "text": "detecting and joining series of line segments that run along each other. given: several circular series of map gps coordinates for several bus routes. the gps coordinates are not all equal when they run along the same road. the number of gps coordinates for a single bus route runs from 140-600.problem: when the points are downloaded drawn, the raw routes don't follow each other perfectly and especially when zoomed-out you can't see some of the routes. i want to somehow show the multiple routes that run along a road, mostly likely dashed with multiple colors. but in order to do that, i need to detect the segments of the route that run really close to each other.what would be perfect is finding the segments of the routes and mutating their gps coords to be shared among them. once that's done i can easily render it.so my question is: are there any good algorithms for this purpose, or some combination/tweaks of other algorithms i can use? it all has to be done autonomously in an app, not hand-done/hand-checked.if you need any other information please ask.",
        "present_kp": [],
        "absent_kp": [
            "computational geometry",
            "geometry"
        ],
        "predict": {
            "present_kp": [
                "gps coordinates",
                "bus routes",
                "line segments"
            ],
            "absent_kp": [
                "map rendering",
                "algorithm",
                "autonomous detection"
            ]
        }
    },
    {
        "text": "how to use google business without showing address. i'm working on a website and my understanding is, by having the place of business listed with google business, it will help with local searches.the problem we have is, this person is a self employed lady and her business is also her home address, and she doesn't want to provide this location to the world. her website shows the town where she lives which appears to be enough for visitors looking for her location, and only after an enquiry is made does she provide her location. the point is, the website does not expose this.is it possible to utilise the localisation for searches (google business) without it exposing the full address?",
        "present_kp": [
            "google"
        ],
        "absent_kp": [
            "local seo"
        ],
        "predict": {
            "present_kp": [
                "google business",
                "address"
            ],
            "absent_kp": [
                "local searches",
                "website"
            ]
        }
    },
    {
        "text": "is it fair to charge again for checking why my code not working on client's server?. so, i was tasked by client to help him convert his wp menu to javascript dropdown. i did on my development server. he did see the change and i was paid. i deliver the code he deploy it. but, no change on his server. so, i have to spent hours debugging it on his server. it turns out, his other plugin is not compatible with my change. that plugin is really custom. i have to change my code to make sure it's compatible with that plugin.my question is, is it fair for me to charge him for the hours i spent on debugging it and actually fixing it? or is it still my responsibility, to make sure my code deployed properly?",
        "present_kp": [],
        "absent_kp": [
            "project management"
        ],
        "predict": {
            "present_kp": [
                "code",
                "server",
                "debugging"
            ],
            "absent_kp": [
                "javascript",
                "wordpress",
                "client"
            ]
        }
    },
    {
        "text": "booting efi on kvm. i'm on ubuntu 14.04 running kernel 3.19. i'm trying to use virt-manager to start a virtual machine. i have installed ovmf for efi firmware and booted the machine with a gentoo iso in the cd slot.i see the ovmf firmware logo, then some log messages like this:boot failed. efi dvd/cdromboot failed. efi floppy.boot failed. efi floppy 1.it then initializes the network card, presumably for network boot, and then drops to an efi shell:how do i get efi booting working in kvm/libvirt?",
        "present_kp": [
            "virtual machine",
            "kvm"
        ],
        "absent_kp": [
            "uefi"
        ],
        "predict": {
            "present_kp": [
                "efi",
                "kvm",
                "ubuntu",
                "virt-manager",
                "ovmf",
                "gentoo"
            ],
            "absent_kp": [
                "virtual machine",
                "network boot",
                "efi shell"
            ]
        }
    },
    {
        "text": "os x ssh keeps asking for password. i connect from a linux machine to several macs over ssh using public/private keys.the setup is the identical on every ac, different oss from 10.5 to 10.9 and publickey working. only one of them, running os x 10.9.5, keeps asking for user's password instead of using publickey.actually there's no access using publickey from any machinessh -vvv is:...debug1: authentications that can continue: publickey,keyboard-interactivedebug3: start over, passed a different list publickey,keyboard-interactivedebug3: preferred publickey,keyboard-interactive,passworddebug3: authmethod_lookup publickeydebug3: remaining preferred: keyboard-interactive,passworddebug3: authmethod_is_enabled publickeydebug1: next authentication method: publickeydebug1: offering dsa public key: /users/akeeem/.ssh/id_dsadebug3: send_pubkey_testdebug2: we sent a publickey packet, wait for replydebug1: authentications that can continue: publickey,keyboard-interactivedebug1: trying private key: /users/akeeem/.ssh/id_rsadebug3: no such identity: /users/akeeem/.ssh/id_rsa: no such file or directorydebug2: we did not send a packet, disable methoddebug3: authmethod_lookup keyboard-interactivedebug3: remaining preferred: passworddebug3: authmethod_is_enabled keyboard-interactivedebug1: next authentication method: keyboard-interactivedebug2: userauth_kbdintdebug2: we sent a keyboard-interactive packet, wait for replydebug2: input_userauth_info_reqdebug2: input_userauth_info_req: num_prompts 1password:what should i check to make sure publickey is operational?",
        "present_kp": [
            "ssh"
        ],
        "absent_kp": [
            "osx",
            "macintosh",
            "public key authentication"
        ],
        "predict": {
            "present_kp": [
                "ssh",
                "publickey"
            ],
            "absent_kp": [
                "os x",
                "password authentication"
            ]
        }
    },
    {
        "text": "update url with new parameter value. i needed a way to update a parameter in the url, or add it if it doesn't exist, but keep any other variables the same value. i built this function to do the task, and while it works, i feel like it's taking longer than it should. does anyone have any suggestion on what i could change or a faster method?function changeurlparameter(svariable, snewvalue){ var aurlparams = []; var aparts; var aparams = (window.location.search).substring(1, (window.location.search).length).split('&'); for (var i = 0; i < aparams.length; i++) { aparts = aparams[i].split('='); aurlparams[aparts[0]] = aparts[1]; } if (aurlparams[svariable] != snewvalue) { if (snewvalue.touppercase() == all) aurlparams[svariable] = null; else aurlparams[svariable] = snewvalue; var snewurl = window.location.origin + window.location.pathname; var bfirst = true; for (var skey in aurlparams) { if (aurlparams[skey]) { if (bfirst) { snewurl += ? + skey + = + aurlparams[skey]; bfirst = false; } else snewurl += & + skey + = + aurlparams[skey]; } } return snewurl; }}",
        "present_kp": [
            "url"
        ],
        "absent_kp": [
            "javascript"
        ],
        "predict": {
            "present_kp": [
                "url",
                "parameter"
            ],
            "absent_kp": [
                "javascript",
                "query string"
            ]
        }
    },
    {
        "text": "is there a simple way to use underscorejs to achieve this?. i have an object, having key and values. i'd like to pick the object and call the appropriate function from the key:var requireviews = { breadcrumbview: true, headerview: false, footerview: false };var that = this; _.each(requireviews, function(value, key){ if(value){ //if true? if(_.functions(that, key)) that[key](); //checking is that function, and calling the function } })is it ok or can we minimize it further?",
        "present_kp": [],
        "absent_kp": [
            "javascript",
            "underscore.js"
        ],
        "predict": {
            "present_kp": [
                "underscorejs"
            ],
            "absent_kp": [
                "javascript",
                "object manipulation"
            ]
        }
    },
    {
        "text": "algorithm of communication with failures. i am interested in distributed algorithms especially in communication in network with failures.i look for the proof of the following randomized algorithm of communication in network with failures. for me it seems like very general result in the communication, nevertheless i havent found the proof yet. algorithm: initially only vertex $v_0$ has the message, at the end of the algorithm every vertex of the network should have the message. on every round every vertex that has the message choice the neighbour randomly and sends it the message.assumptions: only $f$ failures might happen on the edges between the vertices.$t = o(\\log n)$ - time complexity and the entire network will know the message with high probability, when $f<n/3$, where n - number of vertices.i will appreciate for link or reference to the paper.",
        "present_kp": [
            "algorithms"
        ],
        "absent_kp": [
            "distributed systems"
        ],
        "predict": {
            "present_kp": [
                "algorithm",
                "communication",
                "failures",
                "distributed algorithms",
                "network",
                "proof",
                "randomized algorithm",
                "vertex",
                "message",
                "time complexity"
            ],
            "absent_kp": [
                "distributed systems",
                "fault tolerance",
                "network protocols"
            ]
        }
    },
    {
        "text": "iptables: what the difference between filter and mangle. i am using iptables to to mark the package and want to route based on the marks. first i added the ip rule:sudo ip rule add fwmark 1 prohibit(the prohibit is just for test, i will change it to some route table later.)then i began to mark the packages:sudo iptables -a output -d 192.168.1.0/24 -j mark --set-mark 1but the computer can still access the 192.168.1.0/24 networks. after a long time's googling and struggling, i tried:sudo iptables -t mangle -a output -d 192.168.1.0/24 -j mark --set-mark 1it works and the connection was blocked.in the first case, the default table of filter is used. so my question is what is the difference between mangle table and filter table? which one should be used in what cases? as my understanding, all these tables will be consulted before the routing policy, then why the filter table doesn't work properly?",
        "present_kp": [
            "iptables",
            "routing"
        ],
        "absent_kp": [
            "linux",
            "iproute"
        ],
        "predict": {
            "present_kp": [
                "iptables",
                "filter",
                "mangle"
            ],
            "absent_kp": [
                "network routing",
                "package marking"
            ]
        }
    },
    {
        "text": "debian and centos, why ipv6 prevail over ipv4. every time i am configuring a debian or centos machine with a static ip address, i forget about that behavior of not taking in account my ipv4 configuration. then, i search for the 1000nd time the parameter to put in sysctl.conf to disable ipv6, and finally i reboot the beast.an example to illustrate :linux deb-router 3.2.0-4-amd64 #1 smp debian 3.2.81-2 x86_64 gnu/linuxit has 2 interfaces, eth0 is configured in ipv4 since some time (1 year maybe).eth1 is bridged on my physical network and was addressed by the dhcp, in ipv4. i talk to the vm through this interface.tonight, i lost my internet gateway, this device is also my dhcp server.i realized i cannot reach my vm anymore, so i checked ifconfig result and saw a nice ipv6 instead of the old ipv4 bound to eth1.so, action ! $ sudo vim /etc/network/interfacesallow-hotplug eth1# was dhcp beforeiface eth1 inet static address 192.168.0.15 netmask 255.255.255.0$ sudo ifdown eth1 && ifup eth1deception :(ifconfig output only an ipv6, i cannot reach my vm. i must disable ipv6 (net.ipv6.conf.eth1.disable_ipv6 = 1) and reboot to get the connection back.i don't understand this choice to favor ipv6 over ipv4, most of people are working with ipv4 from what i know.is it technical, or political to influence people to adopt ipv6 ?",
        "present_kp": [
            "debian",
            "centos",
            "ipv6",
            "ipv4"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "debian",
                "centos",
                "ipv6",
                "ipv4"
            ],
            "absent_kp": [
                "network configuration",
                "sysctl.conf",
                "dhcp"
            ]
        }
    },
    {
        "text": "trouble with pleskrestore the file you are trying to upload is not a valid backup file. i am having trouble with pleskrestore on plesk 10.4.1, centos 5. i have 6 files i created (also in plesk 10.4.1) with pleskbackup, split into 4gb:here is what i used ...$ /usr/local/psa/bin/pleskbackup --server --output file=<url> --split=4g... to make these files:colossus729_split colossus729_split.001 colossus729_split.002 colossus729_split.003 colossus729_split.004 colossus729_split.005 but plesk restore wouldnt work with those files (contrary to the documentation)# /usr/local/psa/bin/pleskrestore --restore colossus729_split -level server -licenseand get this error:the file you are trying to upload is not a valid backup file",
        "present_kp": [
            "plesk"
        ],
        "absent_kp": [
            "linux"
        ],
        "predict": {
            "present_kp": [
                "pleskrestore",
                "pleskbackup",
                "centos"
            ],
            "absent_kp": [
                "server backup",
                "file upload"
            ]
        }
    },
    {
        "text": "pass snmp trap packet to a php daemon on ubuntu. i have a ubuntu server which is collecting incoming snmp traps. currently these traps are handled and logged using a php script.file /etc/snmp/snmptrapd.conftraphandle default /home/svr/00-vhosts/nagios/scripts/snmptrap.phpthis script is quite long and it contains many database operations. usually the server receives thousands of traps per day and therefore this script is taking too much cpu time. my understand is this is due to high start-up cost of the php script every-time when a trap received.i got a request to re-write this and i was thinking of running this script as a daemon. i can create an ubuntu daemon. my question is how can i pass trap-handler to this daemon using snmptrapd.conf file?thank you in advance.",
        "present_kp": [
            "ubuntu",
            "php",
            "daemon",
            "snmp"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "snmp trap",
                "php daemon",
                "ubuntu"
            ],
            "absent_kp": [
                "snmp trap handler",
                "daemon configuration"
            ]
        }
    },
    {
        "text": "when is a requirement considered complete?. which elements must a requirement contain that it can be considered complete? or if this works better - which questions should i ask about a requirement to find out if it is complete. i am not talking about the implementation of the requirement but the requirement itself.i am asking this from the perspective of an analyst who wants to make sure that his requirements are complete before passing them on to the design team.",
        "present_kp": [
            "requirements"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "requirement",
                "complete"
            ],
            "absent_kp": [
                "implementation",
                "design team"
            ]
        }
    },
    {
        "text": "problem with storing an existing triangulation in a dcel. i am new to stackexchange, and i already made the mistake of posting a new question as a response to a previous question. here, i rewrote my question more clearly and separately.i am trying to store an existing 2d triangulation in a dcel data structure, and i have all of the vertices and edges.i was able to store all of the information correctly except the half_edge representative for each triangle. here is the algorithm i used:(taken from constructing of double connected edge list (dcel))algorithm:for each endpoint, create a vertex. for each input segment, create two half-edges and assign their tail vertices and twins. for each endpoint, sort the half-edges whose tail vertex is that endpoint in clockwise order. for every pair of half-edges e1, e2 in clockwise order, assign e1->twin->next = e2 and e2->prev = e1->twin. pick one of the half-edges and assign it as the representative for the endpoint. (degenerate case: if there's only one half-edge e in the sorted list, set e->twin->next = e and e->prev = e->twin.) the next pointers are a permutation on half-edges. for every cycle, allocate and assign a face structure.the last sentence seems to be easier said than done. how can i ensure that every triangle will have a representative, and that a representative will be assigned only once for each triangle? furthermore, which cycle is it referring to? if you have any other ideas, please share.thank you very much for your help. i've been struggling with this for a while.ps- i am working in c++. also, i am using the same structure as provided in the link above.",
        "present_kp": [],
        "absent_kp": [
            "algorithms",
            "data structures",
            "computational geometry"
        ],
        "predict": {
            "present_kp": [
                "dcel",
                "triangulation",
                "half-edge"
            ],
            "absent_kp": [
                "c++",
                "data structure"
            ]
        }
    },
    {
        "text": "find adsense earning from different websites. i am going to add my adsense code to one new website i've created recently.the problem is how to find out which site earned what in final earning. i need to share the earning of second site with my partner. i can setup different channel to see what each site earned but the final earning may not be the same.so how can i find out which site earned what in final earning page.thanks in advance.",
        "present_kp": [],
        "absent_kp": [
            "google adsense"
        ],
        "predict": {
            "present_kp": [
                "adsense",
                "website",
                "earning"
            ],
            "absent_kp": [
                "google adsense",
                "revenue sharing"
            ]
        }
    },
    {
        "text": "custom user input function. the program consists of 3 files: demo.c, mylib.c and mylib.h. the code is heavily commented.demo.c:/*********************************************************************** * * this is a program that prompts the user for an input string using a * custom input function. * ***********************************************************************/#include <stdio.h> /* printf */#include <stdlib.h> /* exit */#include mylib.h/* for testing purposes, let's make the length of the string 4 */#define string_length 4int main(int argc, char* argv[]) { char* s = udf_get_input(prompt: , string_length); printf(you entered: \\%s\\n, s); /* i put quotes around the string to better see what has actually been entered */ free(s); s = null; /* this is not really necessary */ exit(exit_success);}mylib.h:#ifndef mylib_h_#define mylib_h_size_t udf_strlen(const char* s);char* udf_get_input(const char* const prmpt, int str_len);#endif /* mylib_h_ */mylib.c:#include <stdio.h> /* printf, getchar */#include <stdlib.h> /* malloc, realloc, free, exit */#define error_msg error: could not allocate enough memory/*********************************************************************** * * this is just my own implementation of the c standard library's * strlen function. we're going to need it later. * ***********************************************************************/size_t udf_strlen(const char* s) { size_t i = 0; while (*s++) { i++; } return i;}/*********************************************************************** * * this is a function that takes in as arguments a pointer to a string * that represents the prompt the user sees when typing things at the * keyboard and the length of the string. the function returns a pointer * to the string that has been entered. * * how it works: * we are going to allocate a certain number of bytes on the heap. this * is going to be our buffer. then we will read whatever the user types * in into that buffer. after that, we will check if we need to tweak * the amount of memory that the string takes up so that no memory is * wasted unnecessarily. if the number of characters entered by the user * exceeds the buffer size, the rest of the string is discarded. * ***********************************************************************/char* udf_get_input(const char* const prmpt, int str_len) { int buffer_size = str_len + 1; /* number of characters allowed to be entered plus one to accommodate the null character */ char* buffer; /* temporary storage for the user's string */ if (!(buffer = malloc(buffer_size * sizeof(char)))) { printf(%s , error_msg); exit(exit_failure); } printf(%s, prmpt); /* display the prompt */ int ch; /* stores characters retrieved from stdin */ char* p = buffer; /* temporary pointer to traverse the buffer */ while ((ch = getc(stdin)) != eof) { /* if the character read is a newline character or buffer_size - 1 characters have been already entered, terminate the string with a null character and bail out of the loop */ if (ch == ' ' || !--buffer_size) { *p = ''; break; } *p++ = ch; } /* if buffer_size is more than zero, that means there are unused bytes in the buffer. so, we will reallocate memory to shirk it so that the string occupies as much memory as exactly necessary. otherwise no memory reallocation is needed and we can skip this step. */ if (buffer_size) { buffer = realloc(buffer, (udf_strlen(buffer) + 1) * sizeof(char)); if (!buffer) { printf(%s , error_msg); exit(exit_failure); } } return buffer; /* return the pointer to the string stored on the heap */}to test the program, make a separate directory and create these three files in it:touch demo.c mylib.c mylib.hto run the program, execute this command:gcc -c demo.c mylib.c && \\gcc demo.o mylib.o -o a.out && \\./a.out",
        "present_kp": [
            "c",
            "io"
        ],
        "absent_kp": [
            "strings"
        ],
        "predict": {
            "present_kp": [
                "custom input function",
                "user input"
            ],
            "absent_kp": [
                "c programming",
                "memory allocation"
            ]
        }
    },
    {
        "text": "how to make multipage book in calameo?. if i upload a pdf binder i get the correct amount of pages but with the content of the first page on each. if i upload a number of pdfs separately its just creates a new publication for each pdf.would anyone know how to correctly publish a multi page book with calameo from pdfs?",
        "present_kp": [
            "pdf"
        ],
        "absent_kp": [
            "books"
        ],
        "predict": {
            "present_kp": [
                "calameo",
                "pdf"
            ],
            "absent_kp": [
                "multipage book",
                "publication"
            ]
        }
    },
    {
        "text": "are derivative works a subset of combined works?. this question stems from discussion of a question on programmers.stackexchange.com.in the lgpl 3.0 section 4 combined works it states:you may convey a combined work under terms of your choice that, taken together, effectively do not restrict modification of the portions of the library contained in the combined work and reverse engineering for debugging such modifications, if you also do each of the following:generally, it seems the lgpl license is concerned with open source libraries and software that uses these libraries where a combined work would be a piece of software that includes a library.if one were to open source a standalone single file program and someone modified that and redistributed it, it would technically be a derivative work. would the statements regarding combined works map to derivative works?or more simply: are derivative works a subset of combined works?",
        "present_kp": [
            "derivative works",
            "lgpl"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "derivative works",
                "combined works"
            ],
            "absent_kp": [
                "lgpl 3.0",
                "open source"
            ]
        }
    },
    {
        "text": "need an example of convection-dominated problem to test on freefem++. can you all give me (at least) one example about convection-dominated problem in order that i can test it (them) on freefem++. if possible, please give me specific examples (it/they contain(s) full equations, boundary condition, initial condition, value of parameters, code in freefem++,...)",
        "present_kp": [],
        "absent_kp": [
            "finite element",
            "numerical analysis"
        ],
        "predict": {
            "present_kp": [
                "convection-dominated problem",
                "freefem++"
            ],
            "absent_kp": [
                "boundary condition",
                "initial condition",
                "parameters"
            ]
        }
    },
    {
        "text": "self join exercise. have i over-complicated it?. this my 3rd question on the same exercise, but by no means a duplicate. the two previous questions were posted on stackoverflow here and here.now i'm posting my oracle solution (below) that works. i wonder if the same could've been simpler and/or much more efficient. not in terms of cte or analytic expressions but with basic simple logic.data:create table readings ( user_id varchar(10), reading_time int , x decimal(10,2), y decimal(10,2));insert all into readings (user_id, reading_time, x, y) values ('u1', 60, 345, 400) into readings (user_id, reading_time, x, y) values ('u1', 100, 560, 300) into readings (user_id, reading_time, x, y) values ('u2', 35, 1024, 250) into readings (user_id, reading_time, x, y) values ('u1', 90, 450, 450) into readings (user_id, reading_time, x, y) values ('u3', 150, 600, 100) into readings (user_id, reading_time, x, y) values ('u3', 100, 500, 125)select * from dual;intermediate script:select r.user_id, rm.reading_time start_time, r.reading_time end_time, (r.reading_time-rm.reading_time) time_spent, (to_char(rm.x)||' ; '||to_char(rm.y)) start_point, (to_char(r.x)||' ; '||to_char(r.y)) end_point, sqrt(power(r.x-rm.x, 2)+power(r.x-rm.y, 2)) distance from readings r join readings rm on (r.user_id = rm.user_id and rm.reading_time = (select max(r2.reading_time) from readings r2 where r2.reading_time < r.reading_time))order by 1,2; final script:select rr.user_id, sum(rr.distance) total distance, sum(rr.time_spent) total time, sum(rr.distance)/sum(rr.time_spent) average speed from (select r.user_id, (r.reading_time-rm.reading_time) time_spent, sqrt(power(r.x-rm.x, 2)+power(r.x-rm.y, 2)) distance from readings r join readings rm on (r.user_id = rm.user_id and rm.reading_time = (select max(r2.reading_time) from readings r2 where r2.reading_time < r.reading_time))) rrgroup by rr.user_idorder by 1;exercise description multiple users roam a plain and at irregular time intervals report their coordinates (x, y). this information (user id, time-stamp and the coordinates) populate table readings. for each user that reported more than one set of coordinates we need to find total distance traveled, total time spent, and their average speed.for the sake of simplicity coordinates are cartesian and time-stamps are integers.",
        "present_kp": [
            "oracle"
        ],
        "absent_kp": [
            "sql"
        ],
        "predict": {
            "present_kp": [
                "self join",
                "oracle",
                "cartesian coordinates"
            ],
            "absent_kp": [
                "sql",
                "database",
                "query optimization"
            ]
        }
    },
    {
        "text": "converting cartesian pixels to polar pixels. (i've largely revamped this entire question, though the motivation remains the same.)revised questioni want to convert a raster of cartesian pixels into polar pixels. is there a sensible algorithm for doing this? for example, how do i compute the value of the shaded (polar) pixel in the image below, given the value of the three (cartesian) pixels that it overlaps? original questionis there a reasonable way to compute the area of the intersection of a square and an annular section, as shown in the orange section below?the motivation: i have a raster of square pixels, and i'm converting it to polar pixels -- i want to find out the contribution of each cartesian pixel to each polar pixel.",
        "present_kp": [],
        "absent_kp": [
            "polygon",
            "2d graphics"
        ],
        "predict": {
            "present_kp": [
                "cartesian pixels",
                "polar pixels",
                "raster",
                "algorithm"
            ],
            "absent_kp": [
                "image processing",
                "coordinate transformation"
            ]
        }
    },
    {
        "text": "function to log arguments and return value of any function in clojure. i have implemented a function, which logs the inputs and outputs of any function, indenting the log based on the depth of the call stack:(def depth (atom 0))(defn logfun [fun] (fn [& args] (do (swap! depth inc) (println (str (apply str (repeat @depth )) >>> ( fun args ))) (let [result (apply fun args)] (do (swap! depth dec) (println (str (apply str (repeat @depth )) <<< result)) result)))))usage example:(defn fact [n] (if (> n 1) (* n (fact (dec n))) n))(def fact (logfun fact))(fact 5)output: >>> (lang.core$fact@5eb14bce (5)) >>> (lang.core$fact@5eb14bce (4)) >>> (lang.core$fact@5eb14bce (3)) >>> (lang.core$fact@5eb14bce (2)) >>> (lang.core$fact@5eb14bce (1)) <<< 1 <<< 2 <<< 6 <<< 24 <<< 120main questions:is there a way to improve this function so that it is not dependent on global variables? (i have already attempted a solution based on with-local-vars / var-set -- see below, but it seems to me that they do not work as i wish they did).are there any edge cases, for which the above function does not work?are there any modifications by which this function can be made better adhering to clojure best practices (besides somehow getting rid of the global state)?is there a way to improve the output of the function names (e.g., instead of lang.core$fact@5eb14bce i would like to get lang) in a way that satisfies both conditions below:it must be available in the clojure core language (no third-party libs, no usage of repl utilities)it must not use string manipulation (e.g. find the @ and $ characters via regexp and then strip away the parts before/after them).in other words, i'm looking for something like: (get-pretty-printed-function-name fun). (i already did some research, and found only solutions which do not satisfy the criteria above. still asking, just in case ;) )the attempted, not working solution using var-set/with-local-vars:(defn logfun [fun] (with-local-vars [depth 0] (fn [& args] (do (var-set [depth (inc @depth)]) (println (str (apply str (repeat @depth )) >>> ( fun args ))) (let [result (apply fun args)] (do (println (str (apply str (repeat @depth )) <<< result)) (var-set [depth (dec @depth)]) result))))))edit: using bound-fn, i get one step further, but unluckily it still does not do what is expected, as it seems that the variable depth in the closure is reset to zero upon every invocation of the returned function:(defn logfun [fun] (with-local-vars [depth 0] (bound-fn [& args] (do (var-set depth (inc @depth)) (println (str (apply str (repeat @depth )) >>> ( fun args ))) (let [result (apply fun args)] (do (var-set depth (dec @depth)) (println (str (apply str (repeat @depth )) <<< result)) result))))))result: >>> (lang.core$fact@510da75f (5)) >>> (lang.core$fact@510da75f (4)) >>> (lang.core$fact@510da75f (3)) >>> (lang.core$fact@510da75f (2)) >>> (lang.core$fact@510da75f (1)) <<< 1 <<< 2 <<< 6 <<< 24 <<< 120",
        "present_kp": [
            "clojure"
        ],
        "absent_kp": [
            "logging"
        ],
        "predict": {
            "present_kp": [
                "clojure",
                "function",
                "log",
                "arguments",
                "return value",
                "call stack",
                "global variables",
                "edge cases",
                "best practices",
                "string manipulation"
            ],
            "absent_kp": [
                "python",
                "debugging",
                "recursion",
                "lambda",
                "closure",
                "functional programming",
                "logging library",
                "third-party libraries",
                "repl utilities",
                "regexp"
            ]
        }
    },
    {
        "text": "is it required to disclose source code for custom qt software?. i was hired to develop a custom qt application for a customer, but i'm concerned about the licensing.my software will not be publicly distributed and it is going to be used exclusively by this customer.do i need to disclose my source code for this customer, if i link my application with qt libraries, such as qtcore and qwebkit that can be installed from a linux distribution repository?",
        "present_kp": [
            "licensing",
            "qt"
        ],
        "absent_kp": [
            "open source",
            "closed source"
        ],
        "predict": {
            "present_kp": [
                "qt",
                "source code",
                "custom qt software"
            ],
            "absent_kp": [
                "software licensing",
                "linux distribution"
            ]
        }
    },
    {
        "text": "can a long option be shortened arbitrarily?. is that a long option can be shortened arbitrarily part of gnu conventions for options, or some other conventions/standards, or just provided by some special c function? for example, why do python and awk behave differently?$ python --versionpython 2.7.12$ python --versiounknown option: --usage: python [option] ... [-c cmd | -m mod | file | -] [arg] ...try 'python -h' for more information.$ awk --versiongnu awk 4.1.3, api: 1.1 (gnu mpfr 3.1.4, gnu mp 6.1.0)$ awk --versignu awk 4.1.3, api: 1.1 (gnu mpfr 3.1.4, gnu mp 6.1.0)thanks.",
        "present_kp": [
            "options"
        ],
        "absent_kp": [
            "command line"
        ],
        "predict": {
            "present_kp": [
                "gnu conventions",
                "long option",
                "python",
                "awk"
            ],
            "absent_kp": [
                "command line",
                "short option",
                "gnu standards"
            ]
        }
    },
    {
        "text": "google analytics not recording 99%+ of traffic. i have google analytics on a rails web app.lately traffic has picked up, and i get about 1000 page views per hour as seen from rails. but ga only picks up 25-100 per day, and i need help understanding this.i know ga doesn't pick up some kinds of traffic, like web crawlers, and my own traffic. what else does it deliberately ignore?is there a way i can recognize these categories in my rails code?",
        "present_kp": [
            "google analytics"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "google analytics",
                "rails"
            ],
            "absent_kp": [
                "web traffic",
                "page views"
            ]
        }
    },
    {
        "text": "discovering functionality from parallel class hierarchy. i have an abstract syntax tree which i want to compile down to different representations. i am now struggling to arrange the classes in a way that new representations can be added easily.the easiest way to achieve this is to add a method for each representation, e.g. compile_to_foo, compile_to_bar. additional representations can be added by monkey patching. the problem with this is that the compilation implementations are spread all over the place, and that it violates the single responsibility principle. the advantage is that compilation can be inherited.now, i could also define a compilation function containing a giant switch which dispatches on argument type. but this looses advantages of polymorphism, and makes inheriting behavior of the compilation more difficult. this is not a viable option.an interesting solution would use an abstract factory:this design looks fairly promising, but has some disadvantages:the ast node hierarchy cannot be extended without also extending the abstractcompiler, and in turn all concrete compilers and their parallel hierarchies of concrete nodes.the subtyping information of the ast nodes is spead across the whole system. it has to be specified between the abstractnodes in order to share behaviour (i will be using roles), between the concretenodes in order to share compile implementations, and in at least in the abstractcompiler to provide default implementations (e.g. method nodea() { return node() }). this could be partially solved via metaprogramming.when an ast is built, this can only compile down to one representation. if i want to have multiple outputs, i need to rebuild the ast with a different concretecompiler.ideally, i would just pass a concrete compiler instance as a parameter to the compile method: but i have no idea how the compile method could obtain the actual implementation from a parallel class hierarchy (without again using a giant switch on the node type).i also investigated the bridge pattern, but the solution does not seem applicable to my problem without creating a thousand little bridges.i carefully read through this previous question: designing a robust architecture for multiple export types?. the key difference is that the input data (there: equivalent, standalone data represenations) are now hierarchical ast nodes, so that inheritance between the compilation implementations is crucial.the system will be implemented in perl, so i'm not restricted to classic oop, but can also use metaprogramming, roles (aka. traits), and functional programming.what am i missing? is there an architecture i could use to elegantly structure this system? how can i make the corresponding class from the parallel class hierarchy discoverable to the node classes, without sacrificing polymorphism?",
        "present_kp": [
            "architecture",
            "perl",
            "single responsibility"
        ],
        "absent_kp": [
            "design patterns"
        ],
        "predict": {
            "present_kp": [
                "abstract syntax tree",
                "parallel class hierarchy",
                "compilation",
                "polymorphism",
                "metaprogramming"
            ],
            "absent_kp": [
                "perl",
                "functional programming",
                "roles",
                "traits",
                "bridge pattern"
            ]
        }
    },
    {
        "text": "using qt in open source app that communicates with closed source hardware. prefacei'm having trouble determining if i can use the qt framework for developing a cross platform desktop app.situationthe app is open source, and the libraries it uses are also open source.so i can provide the source code for the whole app without issue.however, the app will be communicating with hardware that has closed source firmware. the app will read and write data from the device. read data will be used to visualize what the hardware is doing. it will also update the hardware by feeding the device an encrypted firmware file, which its boot-loader (also closed source) will encrypt and commit to flash. lgpl and gpl licensed code will in no way be used in the firmware or boot-loader. questionsdoes the free lgpl version of the qt framework allow me to keep my firmware as closed source? if so, does the gpl license allow for this use-case as well? editi read something interesting on a similar question involving the lgpl and gpl licenses: as a rule of thumb, the gpl reaches as far as the address space of the licensed code.if i am reading this correctly, my qt app should have no licensing issues.",
        "present_kp": [
            "licensing",
            "gpl",
            "lgpl",
            "qt"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "qt framework",
                "open source app",
                "closed source hardware",
                "lgpl",
                "gpl"
            ],
            "absent_kp": [
                "cross platform desktop app",
                "firmware",
                "boot-loader"
            ]
        }
    },
    {
        "text": "best free blogging site that allows adsense and other advertising?. i'm looking to start my own blog. under the incredibly vain assumption that anyone cares what i think, i'd like to put up some ads to fund my, um, habits, yeah. i don't want to have to pay anything, nor do i want to host it myself (if possible).what blogging platform fits this bill?",
        "present_kp": [
            "advertising",
            "blog",
            "free"
        ],
        "absent_kp": [
            "google adwords"
        ],
        "predict": {
            "present_kp": [
                "blogging site",
                "adsense",
                "advertising"
            ],
            "absent_kp": [
                "free hosting",
                "self-hosting"
            ]
        }
    },
    {
        "text": "parsing odataqueryoptions to expression>. i am following up on this answer for a scenario that i am currently working on. like the op, i am too concerned about the longevity of the code.public iqueryable<tentity> emptyenumerable(){ return enumerable.empty<tentity>().asqueryable();}private expression<func<tentity, bool>> getfilterexpression(filterqueryoption filter){ var enumerable = this.emptyenumerable(); var param = expression.parameter(typeof(tentity)); if(filter != null) { enumerable = (iqueryable<tentity>)filter.applyto(enumerable, new odataquerysettings()); var mce = enumerable.expression as methodcallexpression; if(mce != null) { var quote = mce.arguments[1] as unaryexpression; if(quote != null) { return quote.operand as expression<func<tentity, bool>>; } } } return expression.lambda<func<tentity, bool>>(expression.constant(true), param);}my questions are:is there anything that i should be checking, which i am not already checking?is returning a truth expression a sensible default?any refactorings/improvements are welcome.",
        "present_kp": [],
        "absent_kp": [
            "c#",
            "api"
        ],
        "predict": {
            "present_kp": [
                "odataqueryoptions",
                "expression",
                "filterqueryoption"
            ],
            "absent_kp": [
                "c#",
                "linq"
            ]
        }
    },
    {
        "text": "what is the state-of-the-art in machine translation on chinese/cjk?. sorry for my unfamiliarity with the field. hopefully this question is on-topic here.currently the machine translation from western european languages to english is arguably quite robust, with google translate able to quite accurately translate articles on wikipedia or news site. however, it seems to me that the translation between chinese/japanese and english is still quite off: google translate is frequently unable to produce a reasonably coherent/meaningful translation for even just one chinese/japanese sentence.it gets me wondering, is the state-of-the-art in cjk machine translation still much off the pace compared with european languages? or is it just that some research advances haven't been applied to the industry yet? what is the current status of the cutting-edge researches on this field, e.g. novel translation models/improvements on precision rate. where should i go to look for more information?",
        "present_kp": [],
        "absent_kp": [
            "reference request",
            "natural language processing"
        ],
        "predict": {
            "present_kp": [
                "machine translation",
                "chinese",
                "cjk"
            ],
            "absent_kp": [
                "google translate",
                "research advances",
                "translation models"
            ]
        }
    },
    {
        "text": "setting .conf files for nginx. i use nginx as a server, and i currently use this for configuration of each my sites. basically, i have multiple files like this located in /etc/nginx/conf.d/.#example.confserver { listen 80; server_name <url> my-site.com; root /var/www/html/my-site.com; location / { rewrite ^/category/(.*)$ /category.php?id=$1 last; rewrite ^/profile/(.*)$ /profile.php?id=$1 last; try_files $uri $uri/ /index.php?$query_string; } index index.html index.htm index.php; error_page 404 /404.html; location = /var/www/html/nginx/error/404.html { root /var/www/html/nginx/error/; } error_page 500 502 503 504 /50x.html; location = /var/www/html/nginx/error/50x.html { root /var/www/html/nginx/error/error/; } location ~ \\.php$ { fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param script_filename $document_root$fastcgi_script_name; include fastcgi_params; }}so, i have 6 sites with their own file like this, and since i am about to do a complete re-install of my server, i would like to know how i can improve this configuration, both security and optimization wise.",
        "present_kp": [
            "server",
            "nginx"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "nginx",
                "conf files",
                "server configuration"
            ],
            "absent_kp": [
                "web server",
                "security optimization"
            ]
        }
    },
    {
        "text": "top for web browsers. this is a bit off the unix road, but i believe most people interested in the answer are linux or unix users so here goes.for a long time it seems that the number one process chewing up cpu time and memory is my web browser ( mainly firefox, but others too ). it is true that i have a lot of pages open at once, so i generally don't mind, but recently it's gotten to the point where the browser just bogs down the system, and when i close some pages some sanity is restored.what would be nice is if there were some tool or plugin that would tell me exactly what web pages/sites are using the most resources.",
        "present_kp": [
            "browser"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "web browsers",
                "cpu time",
                "memory",
                "firefox"
            ],
            "absent_kp": [
                "linux",
                "unix",
                "plugin"
            ]
        }
    },
    {
        "text": "openvpn: routing by destination name or port, not ip. on a dd-wrt router, openvpn uses policy based routing for 192.168.1.128/25. if the vpn goes down, the firewall denies access to clients in the range.i would like to add exceptions to the vpn routing.is it possible to specify that connection to certain fqdns will be sourced from the wan address not the vpn address (and of course be unencrypted)? if not possible as above, at least have connections to a certain port (587) be routed off vpn, across the board?",
        "present_kp": [
            "routing",
            "openvpn",
            "router",
            "route"
        ],
        "absent_kp": [
            "dd wrt"
        ],
        "predict": {
            "present_kp": [
                "openvpn",
                "routing",
                "dd-wrt router",
                "firewall",
                "vpn"
            ],
            "absent_kp": [
                "policy based routing",
                "wan address",
                "port 587"
            ]
        }
    },
    {
        "text": "complexity of knapsack-type problem with applications to computational workflows. consider the following problem:let there be a set a of $n$ items $a=\\{z_1, ..., z_n\\}$, and let $w$ be a strictly positive integer. each item $z_i$ has a value $v_i$ and a weight $w_i$. finding a subset $as$ of $a$ so that the weight of $as$ is less than $w$ and the value of $as$ (the sum of the value of its items) is maximized is the 0/1 knapsack problemnow, consider a deviation from it where the items in $a$ have certain dependency relationships between each other, and these dependencies can be captured by a directed acyclic graph $g(a, e)$. the value of the set $as$ is no longer the sum of the value of the items in $as$. for each item in $as$, its value depends on which other items are also in $as$. more formally, this is how we calculate the value of an item $v$ in $as$. let $a$ be the a closest ancestor of $v$ in $g$ that is also in $as$. then the contribution of $v$ to the value of $as$ would be its own value, plus the value of all the ancestors in the path between $v$ and $a$. (since this is a dag, there could be many of these ancestors. see formalization below).this problem has important applications in computational workflow systems where you have limited storage, and you want to optimize the computational time of running a workflow (represented by a dag) by storing some of the intermediate datasets for future use.my questionsthe problem is obviously np-hard because the knapsack problem can be reduced to it. i have a feeling that it is likely that no pseudo-polynomial algorithm exists for it. do you know of a problem that i could use to reduce to my problem to confirm those feelings? or do you think that it is possible to produce a pseudo-polynomial algorithm for this?edit: formalization on how to compute value of $as$to succinctly define the value of $as$ i will add to the notation from above a little bit. let $v(z_i)$ be equivalent to $v_i$ from above. then $v(z_i|as)$ reads: value of node $z_i$ given answer set $as$.if $z_i \\in as$, then $v(z_i|as)=0$. otherwise, if $z_i ot\\in as$, $v(z_i|as) = v(z_i) + \\sum_{z_j \\in parents(z_i)}{v(z_j|as)}$those two definitions are enough to then say that:$value(as) = \\sum_{z_i \\in as}{v(z_i|as-\\{z_i\\})}$end of edit",
        "present_kp": [],
        "absent_kp": [
            "np hardness",
            "time complexity",
            "reductions"
        ],
        "predict": {
            "present_kp": [
                "knapsack problem",
                "directed acyclic graph",
                "computational workflows"
            ],
            "absent_kp": [
                "np-hard",
                "pseudo-polynomial algorithm",
                "workflow optimization"
            ]
        }
    },
    {
        "text": "sending files over twitter. i want to be able to send a file to my followers. how do i do this?",
        "present_kp": [
            "twitter"
        ],
        "absent_kp": [
            "file send"
        ],
        "predict": {
            "present_kp": [
                "twitter"
            ],
            "absent_kp": [
                "file sharing",
                "social media"
            ]
        }
    },
    {
        "text": "does sitting idle for one month without work leads to depression?. does sitting idle for one month without work leads to depression?following case is an example:if a human do 8 hours job before computer. because of no work or a few work; generally sitting idle for one month. and when work came after a month; then that human do not want to do it because:symptoms which generally such human is facing:1) that humans head remain heavy; always feel like outing. even if that human go to outing and come back to office feel like go out again.2) it is like nothing is in that human head. just sounds shaaan shaaaan.3) sometimes it is like that human need to get my ear clean from ent; may be due to that head is heavy. but nothing is there in that human's ear , because that human use to clean my ears during bathing.4) life becoming dull. now work came after 1 month; that human being lazy not to attempt that work. that human do not want work now.5) not agreeing to any external motivation and self motivation is becoming zero.6) just looking at facebook, amazon and doing nothing. not a like on facebook; but looking just. it is like nothing new.7) if reading news forget the story after 15 minutes. forget the work that human's family member says.8) that human feel like that human is blunder; that human do mistakes; you all leave that human; throw that human out of computer company.could anyone please figure out situation of that human's mind!!! is that human getting mad or depressed?",
        "present_kp": [
            "depression"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "depression",
                "sitting idle",
                "work"
            ],
            "absent_kp": [
                "mental health",
                "motivation",
                "laziness"
            ]
        }
    },
    {
        "text": "importance of the empty string. in the sense of a string distinct from a null reference string, what is the importance of an empty string in cs (and specially in formal languages)? why do you need a separate concept, that of 'empty string', which even has it's own greek letter ()?couldn't just an eol character replace it?",
        "present_kp": [
            "formal languages"
        ],
        "absent_kp": [
            "terminology"
        ],
        "predict": {
            "present_kp": [
                "empty string",
                "formal languages"
            ],
            "absent_kp": [
                "null reference",
                "eol character"
            ]
        }
    },
    {
        "text": "best sources on data stream algorithms. i recently got interested in data stream algorithms to the point that i'd like to study the topic and then teach it to someone.i'd be thus grateful for pointers to really good sources on the topic, t.i. papers presenting major ideas in a particularly articulate way, papers with clever proofs of clever theorems, just good overviews of the state of the art, whatever.my two cents:lecture notes from the dartmouth university, 2009. this is the best source i've found so far.distributing frequency-dependent data stream computations, described in my answer to a different (also mine) question.the book data streams: algorithms and applications (i haven't read it yet)",
        "present_kp": [
            "data streams"
        ],
        "absent_kp": [
            "reference request",
            "big list"
        ],
        "predict": {
            "present_kp": [
                "data stream algorithms"
            ],
            "absent_kp": [
                "distributed computing",
                "algorithm analysis"
            ]
        }
    },
    {
        "text": "autostart bash script, debian 8. bash script#!/bin/shxflux -l 55 -g 37how autoboot it precisely in debian 8? i.e via startap application commandgnome-terminal -e /path_to_script/script.shdoesn't work",
        "present_kp": [
            "debian"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "bash script",
                "debian 8"
            ],
            "absent_kp": [
                "autostart",
                "gnome-terminal"
            ]
        }
    },
    {
        "text": "numerical solution of fractional integro-diffrential equ. using collocation method?. problem comes from numerical solution of fractional integro-differential , equations by collocation method , e.a. rawashdeh, department of mathematics, yarmouk university, irbid 21110, jordan$d^qy(t)=p(t)y(t)+f(t)+\\int_{0}^{1}{k(t,s)y(s)\\,ds} , t\\in i=[0,1]$i want to create a maple code to check if the results in given article is valid or not but i do not have any idea about collocation method!any reference to collocation method solution are welcome!",
        "present_kp": [
            "collocation"
        ],
        "absent_kp": [
            "finite difference",
            "matlab",
            "mathematica",
            "integral equations"
        ],
        "predict": {
            "present_kp": [
                "numerical solution",
                "fractional integro-differential equations",
                "collocation method"
            ],
            "absent_kp": [
                "maple code",
                "mathematics",
                "yarmouk university"
            ]
        }
    },
    {
        "text": "how to install osqa using xampp on windows 7 32bit?. i want to install osqa using xampp on windows 7 32bit. i've followed the instructions on this tutorial until the install the database server section. in this section, to create a database, i've used phpmyadmin and created a database (name: osqa; password: 1234). now my problem is the next step, edit settings. in this step i don't know how to fill in the settings_local.py's fields. i know that apache needs mod_wsgi.so so, i've placed mod_wsgi.so into my apache modules directory and then added loadmodule wsgi_module modules/mod_wsgi.so to the file httpd.conf. then i restarted the apache with no errors. then i followed the next steps, but after entering <url> in my browser, i just see a blank page! can someone please provide me with an instruction in full details? please note that i don't like to use bitnami osqa for some reasons.unfortunately, the official support for osqa is not very active in these days. p.s.: i'm using python 2.7.5 and django 1.6.2. also my osqa source files are in c:\\xampp\\htdocs\\osqa.",
        "present_kp": [
            "xampp",
            "django",
            "python"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "osqa",
                "xampp",
                "windows 7 32bit",
                "phpmyadmin",
                "apache",
                "mod_wsgi.so",
                "python",
                "django"
            ],
            "absent_kp": [
                "bitnami osqa",
                "database server",
                "settings_local.py",
                "mod_wsgi",
                "httpd.conf"
            ]
        }
    },
    {
        "text": "how to place / store a file in memory on linux?. i have read somewhere that one can put a file on a linux system into memory, and loading it will be superfast.how do i do this? how do i verify the file is loaded from memory?",
        "present_kp": [
            "linux",
            "memory"
        ],
        "absent_kp": [
            "files"
        ],
        "predict": {
            "present_kp": [
                "linux",
                "memory"
            ],
            "absent_kp": [
                "file storage",
                "system performance"
            ]
        }
    },
    {
        "text": "undecidability of the language with its elements(tm) having empty language. we can write a decider for the language:$e=\\{a\\; |\\; a \\mbox{ is a dfa and } l(a)=\\emptyset\\}$by marking method. why we cannot use the same method to write a decider for the language with tm as follows?$a = \\{ m \\;|\\; m \\mbox{ is a tm and } l(m)=\\emptyset\\}$",
        "present_kp": [],
        "absent_kp": [
            "turing machines"
        ],
        "predict": {
            "present_kp": [
                "language",
                "decider",
                "dfa",
                "tm"
            ],
            "absent_kp": [
                "undecidability",
                "empty language",
                "marking method"
            ]
        }
    },
    {
        "text": "table - replace values of a column. i have several tables (tab separated) in which the first column is as follow:month0.000.000.000.000.000.000.000.000.000.000.000.00i would like to replace those values by the actual month value, as follow:month123456789101112",
        "present_kp": [
            "table"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "table",
                "column"
            ],
            "absent_kp": [
                "data manipulation",
                "tab separated"
            ]
        }
    },
    {
        "text": "using a singleton for a collection of discounts to calculate?. i'm wondering if using a singleton for a storage of discounts is the right way to go. this is because i will be looping over all added discounts in another class.client code// dynamically add new discounts$collection = discountcollection::getinstance();$collection->add(new tenpercentdiscount());$collection->add(new twentypercentdiscount());discountcollection<?php namespace notflip\\discount;use notflip\\discount\\discounts\\discount;class discountcollection { private static $instance; private static $discounts; final static function getinstance() { if(static::$instance === null) { static::$instance = new static(); } return static::$instance; } public function add(discount $discount) { static::$discounts[] = $discount; } public function count() { return count(static::$discounts); } public function show() { return static::$discounts; }}",
        "present_kp": [
            "php",
            "singleton"
        ],
        "absent_kp": [
            "object oriented"
        ],
        "predict": {
            "present_kp": [
                "singleton",
                "discounts"
            ],
            "absent_kp": [
                "php",
                "design pattern"
            ]
        }
    },
    {
        "text": "given a string, return a string where every character in the original is doubled. for example, given the string xyz, return the string xxyyzz.i was given this as a part of a test. i would really appreciate if you can help me to find a better way of doing it.i came up with two methods to do the same thing but one was an extension method.i have a couple more questions which i will post in separate posts.public class repeatcharactersinstring{ public string repeatastring(string sinputstring, int repeatcount) { stringbuilder soutputstring = new stringbuilder(); if (string.isnullorempty(sinputstring)) { console.writeline(empty string !); } else { foreach (char c in sinputstring) { soutputstring.append(new string(c, repeatcount)); } } return soutputstring.tostring(); }}// method 2 using extension methodspublic static class repeatcharactersinstingextensions{ public static string repeatallcharactersinthisstring(this string sinputstring, int repeatcount) { stringbuilder soutputstring = new stringbuilder(); if (string.isnullorempty(sinputstring)) { console.writeline(empty string !); } else { foreach (char c in sinputstring) { soutputstring.append(new string(c, repeatcount)); } } return soutputstring.tostring(); }}",
        "present_kp": [],
        "absent_kp": [
            "c#",
            "strings"
        ],
        "predict": {
            "present_kp": [
                "string",
                "extension method"
            ],
            "absent_kp": [
                "c#",
                "stringbuilder"
            ]
        }
    },
    {
        "text": "linux software center cannot run anymore. i am using linux mint 18 cinnamon.linux mint has software center, just like ubuntu software center in ubuntu. after linuxmint installation, i could run the software center in my system. but someday, may be after some operation like remove openjdk, install oracle jdk,...the software center doesn't run anymore. when i click the software center icon or run from terminal, the os ask for super user password too, after entered the password, the round-spin (waiting) cursor appear for some seconds. after all, nothing happen. the software center doesn't run. something i tried:reinstall default jre (openjre)remove software-manager and reinstall software-managerthis is the output when i try to run sudo mintinstall$ sudo mintinstallvector smash protection is enabled.add_categories took 13.497 msbuild_matched_packages took 0.298 msadd_packages took 3828.769 msfirst run detected, initial set of reviews usedadd_reviews took 1022.018 mstraceback (most recent call last): file /usr/lib/linuxmint/mintinstall/mintinstall.py, line 1920, in <module> application() file /usr/lib/linuxmint/mintinstall/mintinstall.py, line 59, in wrapper res = func(*arg) file /usr/lib/linuxmint/mintinstall/mintinstall.py, line 617, in __init__ sans26 = imagefont.truetype(self.font, 26) file /usr/local/lib/python2.7/dist-packages/pil/imagefont.py, line 239, in truetype return freetypefont(font, size, index, encoding) file /usr/local/lib/python2.7/dist-packages/pil/imagefont.py, line 128, in __init__ self.font = core.getfont(font, size, index, encoding) file /usr/local/lib/python2.7/dist-packages/pil/imagefont.py, line 37, in __getattr__ raise importerror(the _imagingft c module is not installed)importerror: the _imagingft c module is not installed",
        "present_kp": [
            "linux mint"
        ],
        "absent_kp": [
            "software installation"
        ],
        "predict": {
            "present_kp": [
                "linux mint",
                "software center"
            ],
            "absent_kp": [
                "ubuntu",
                "python"
            ]
        }
    },
    {
        "text": "colordiff - how to retain color while saving to file. is it possible to retain the color while storing the diff output in a file?this is working and showing the colors in terminalcolordiff -yw 1000 --suppress-common-lines file1 file2 > tempfilebut when i redirect the output to a file its not showing the colors. colordiff -yw 1000 --suppress-common-lines file1 file2 > tempfile",
        "present_kp": [
            "diff",
            "colordiff"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "colordiff",
                "file"
            ],
            "absent_kp": [
                "terminal",
                "output"
            ]
        }
    },
    {
        "text": "backing up messages in s3 within a storm topology. in my project we are trying to build up a kind-of-a-lambda storm based architecture. the component would be responsible for indexing the site usage events so we expect a quite massive random load. the solution for real-time processing of the messages seems fine, but in parallel to the speed layer we want to back up the messages in a raw form (just as they are coming down from the queue) in amazon s3. since writing a file per message is obviously out of the question, we need to somehow buffer/aggregate the messages before posting to s3 - and here is where the problems begin. we have two concurrent approaches and none seems perfect:we used redis as a buffer. basically messages coming down from a queue (rabbitmq) are buffered in redis and once some preconfigured batch size (say 1000) is reached the buffer is flushed and stored to s3. the whole message proccesing cycle is not transactional. once message is stored to redis it is acknowledged in the queue. this means that if redis dies the whole batch gets lost and this is not acceptable. we can think of redis cluster to make the thing more bullet proof but... doesn't it seem like going in a wrong direction?the second approach would be to use trident topology. changing the input queue to kafka makes things looking more or less straightforward. the message processing cycle can be transactional. however, there is this one annoying keyword that keeps beeing repeated in the trident documentation - small batches - that trident assumes small batches. for example the transactionaltridentkafkaspout batches the messages into 2 seconds chunks. this is much too little for us. i'm not sure why the batches should be small, but if it's really required maybe the first approach is better?which one from above is better? maybe somoeone would come up with a third idea?",
        "present_kp": [
            "architecture",
            "redis"
        ],
        "absent_kp": [
            "real time",
            "aws",
            "apache kafka"
        ],
        "predict": {
            "present_kp": [
                "storm topology",
                "s3",
                "redis",
                "trident topology",
                "kafka"
            ],
            "absent_kp": [
                "lambda architecture",
                "amazon web services",
                "message queue",
                "batch processing"
            ]
        }
    },
    {
        "text": "how to remap keys using xkb/symbols/us file in ubuntu?. i have dropped some water on my keyboard and only one key is not working which is the down key. i want now to use some other keys such as right alt or menu key between right-alt and right-ctrl keys as i do not use them very often. i have open the us file in ...xkb/sybmols/us file, however, i am very confused and could not find the up, down, left and right keys. can anyone help me to remap the down key to the menu key on the keyboard? thanks",
        "present_kp": [
            "ubuntu",
            "xkb"
        ],
        "absent_kp": [
            "keyboard shortcuts",
            "keyboard layout"
        ],
        "predict": {
            "present_kp": [
                "remap keys",
                "xkb",
                "symbols",
                "us file",
                "ubuntu",
                "keyboard",
                "down key",
                "menu key"
            ],
            "absent_kp": [
                "linux",
                "key mapping",
                "keyboard layout"
            ]
        }
    },
    {
        "text": "why is mediawiki auto-linking the word files. our mediawiki installation is auto-linking the word files. sohere are some files: a, b, cwould result in the word files being linked to http://ourhost/mediawiki/files.why is that happening and how do i make it stop? i can use the nowiki tag, but perhaps it does not surprise you that the word files appears often, and it is aggravating to use that tag all the time.here is some info on our mediawiki installation from special:version. yes, it's old.installed softwareproduct versionmediawiki 1.16.5php 5.2.14-pl0-gentoo (apache2handler)mysql 5.0.84installed extensionsparser hooks googledocs4mw (version 1.1) adds tag for google docs' spreadsheets display jack phoenix syntaxhighlight (version 1.0.8.6) provides syntax highlighting using geshi highlighter brion vibber, tim starling, rob church and niklas laxstrmwebservicesequencediagram(version 1.0) render inline sequence diagrams using websequencediagrams.com eddie olsson other mwsearch mwsearch plugin kate turner and brion vibberextension functions efluceneprefixsetupparser extension tags gallery, googlespreadsheet, html, nowiki, pre, sequencediagram, source and syntaxhighlightparser function hooks anchorencode, basepagename, basepagenamee, defaultsort, displaytitle, filepath, formatdate, formatnum, fullpagename, fullpagenamee, fullurl, fullurle, gender, grammar, int, language, lc, lcfirst, localurl, localurle, namespace, namespacee, ns, nse, numberingroup, numberofactiveusers, numberofadmins, numberofarticles, numberofedits, numberoffiles, numberofpages, numberofusers, numberofviews, padleft, padright, pagename, pagenamee, pagesincategory, pagesize, plural, protectionlevel, special, subjectpagename, subjectpagenamee, subjectspace, subjectspacee, subpagename, subpagenamee, tag, talkpagename, talkpagenamee, talkspace, talkspacee, uc, ucfirst and urlencode",
        "present_kp": [
            "mediawiki"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "mediawiki"
            ],
            "absent_kp": [
                "auto-linking",
                "nowiki tag"
            ]
        }
    },
    {
        "text": "tensorflow rnn not learning when output included in training variables. i have been attempting to train a rnn on a set of time series data. the goal is to predict one of six categorical outputs. the input is given as 5 time steps of 14 inputs, six of which at one-hot attributes for the output. there is an output at each time step, but the goal is to use previous recorded time spots and their human-assigned outputs to assign an output to the most recent event. confusingly the rnn is unable to learn that one of the inputs is in fact the output of the classification. this is simply a sanity check for me, but it seems that it may indicate a larger underlying problem.the data is heavily imbalanced, 91%, 4%, 2%, 1%,<1%,<1%, but a cost function is being use to weight mis-classification inversely to it's make-up in the data set. could the imbalance cause this issue? i'm using 60,000 training examples right now, is this not enough?i'm working off of this dynamic rnn model: <url> the additional matrix multiplication for a hidden layer. is this done correctly?def dynamicrnn(x, seqlen, weights, biases):# prepare data shape to match 'rnn' function requirements# current data input shape: (batch_size, n_steps, n_input)# required shape: 'n_steps' tensors list of shape (batch_size, n_input)# permuting batch_size and n_stepsx = tf.transpose(x, [1, 0, 2])# reshaping to (n_steps*batch_size, n_input)x = tf.reshape(x, [-1,n_input])x = tf.matmul(x, weights['hidden'])+ biases['hidden']# split to get a list of 'n_steps' tensors of shape (batch_size, n_input)x = tf.split(0, n_steps, x)# define a lstm cell with tensorflowlstm_cell = rnn_cell.basiclstmcell(n_hidden, forget_bias=1.0)# get lstm cell output, providing 'sequence_length' will perform dynamic# calculation.outputs, states = tf.nn.rnn(lstm_cell, x, dtype=tf.float32, sequence_length=seqlen)# when performing dynamic calculation, we must retrieve the last# dynamically computed output, i.e, if a sequence length is 10, we need# to retrieve the 10th output.# however tensorflow doesn't support advanced indexing yet, so we build# a custom op that for each sample in batch size, get its length and# get the corresponding relevant output.# 'outputs' is a list of output at every timestep, we pack them in a tensor# and change back dimension to [batch_size, n_step, n_input]outputs = tf.pack(outputs)outputs = tf.transpose(outputs, [1, 0, 2])# hack to build the indexing and retrieve the right output.batch_size = tf.shape(outputs)[0]# start indices for each sampleindex = tf.range(0, batch_size) * n_steps + (seqlen - 1)# indexingoutputs = tf.gather(tf.reshape(outputs, [-1, n_hidden]), index)# linear activation, using outputs computed abovereturn tf.matmul(outputs, weights['out']) + biases['out']",
        "present_kp": [
            "tensorflow"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "tensorflow",
                "rnn",
                "training variables",
                "time series data",
                "categorical outputs",
                "cost function",
                "dynamic rnn model"
            ],
            "absent_kp": [
                "machine learning",
                "neural network",
                "data imbalance",
                "hidden layer",
                "lstm"
            ]
        }
    },
    {
        "text": "making a header row in a google spreadsheet. is there a way to put text in a row at the top of a spreadsheet, like a header? so it is separate and fixed from the columns below it? in other words, i want a header row at the top and the columns below it in different widths from the columns in the header row.",
        "present_kp": [],
        "absent_kp": [
            "google spreadsheets"
        ],
        "predict": {
            "present_kp": [
                "google spreadsheet",
                "header row"
            ],
            "absent_kp": [
                "excel",
                "data formatting"
            ]
        }
    },
    {
        "text": "variant of subset sum problem with changing bound. given a sequence of decreasing integers, i.e., $a_1 \\geq a_2 \\geq \\cdots \\geq a_t $ and a positive real $k\\geq 1$, find a subset $s$ such that$$\\max_{s\\subseteq \\{1,\\ldots,t\\}} \\sum_{i\\in s} a_i$$$$s.t., \\sum_{i\\in s/\\{t\\}} a_i \\leq k \\cdot a_t,$$where $a_t$ represents the smallest one of subset $s$. note that $a_t$ is the smallest of set $s$ and may be different for different set $s$. i am wondering whether this problem is still np-complete. any comments or suggestions will be very appreciated.",
        "present_kp": [
            "subset sum"
        ],
        "absent_kp": [
            "np hardness",
            "partition problem"
        ],
        "predict": {
            "present_kp": [
                "subset sum problem",
                "np-complete"
            ],
            "absent_kp": [
                "algorithm",
                "optimization"
            ]
        }
    },
    {
        "text": "port numbers for ssl. we have an existing web site with http on port 80 and https on port 443. i'm adding a second site to that now, and from what i understand, i cannot host two sites on the same ssl port. so my question is: which port number range is appropriate for me to use as my ssl port on the second site?",
        "present_kp": [
            "https"
        ],
        "absent_kp": [
            "server",
            "iis7",
            "iis",
            "configuration"
        ],
        "predict": {
            "present_kp": [
                "ssl",
                "port"
            ],
            "absent_kp": [
                "https",
                "web server"
            ]
        }
    },
    {
        "text": "best practice for ensuring name uniqueness/correctness in message queues in a microservice architecture/distributed system. i was wondering what a good solution would be for ensuring that queue-names are entered correctly and are only used by the correct applications in a large system which uses message queues to exchange messages.we have a large system written in java and apache camel. it is split into several microservices where they use message queues to communicate with each other. the queue names are as of now strings, which tend to get pretty simple, like incupdate or inccreate. when the system continues to grow and we continue to add more services i am worried that someone is going to reuse a queue-name that already exists, which would create bugs that would not show up in local testing and would be hard to debug. an easy solution to this is to simply add the service name as a prefix to the queue-name, this would ensure uniqueness between the services.but i was thinking, why not take it a little further?what if i created a reference, that all services had access to, for example an enum, where each service would only use entries in the reference as the queue-names? this way it would not only ensure uniqueness, but it would also ensure correctness (for example typos in queue-names). and it would also provide code-highlighting for wherever each queue-name is used.do you have any solutions or suggestions for a problem like this? is the reference-solution viable at all? i can see one problem with it, and it's that each service actually has to have access to the enum, which means injecting it from somewhere. i think we could use maven or spring for this, but i am not sure.",
        "present_kp": [
            "java",
            "spring",
            "message queue",
            "microservices"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "message queues",
                "microservice architecture",
                "distributed system",
                "java",
                "apache camel",
                "queue-names",
                "enum"
            ],
            "absent_kp": [
                "rabbitmq",
                "kafka",
                "service discovery"
            ]
        }
    },
    {
        "text": "saving a pointer to the n/4 node in avl tree. i have an avl tree which every node has a filed with a key which is an integer. i need to save a pointer to the minimum , maximum and the $\\left \\lfloor rac{n}{4} ight floor $ nodes. the first and the second were pretty easy i just saved a pointer between each node and its follow and back nodes. but i couldn't find a way to save the $\\left \\lfloor rac{n}{4} ight floor$ one. any ideas? thank you.",
        "present_kp": [],
        "absent_kp": [
            "data structures",
            "trees",
            "search trees",
            "balanced search trees"
        ],
        "predict": {
            "present_kp": [
                "avl tree",
                "node"
            ],
            "absent_kp": [
                "binary tree",
                "pointer"
            ]
        }
    },
    {
        "text": "kvm - how to use a usb as storage. i'm on debian testing.i'm trying to install a vm on a storage which is located on a ext4 formated usb stick. however i'm getting a permission error before the os can be installed. i'm using virtual machine manager:unable to complete install: 'cannot access storage file '/media/user/mnt/generic.qcow2' (as uid:121, gid:131): permission denied'traceback (most recent call last): file /usr/share/virt-manager/virtmanager/asyncjob.py, line 88, in cb_wrapper callback(asyncjob, *args, **kwargs) file /usr/share/virt-manager/virtmanager/create.py, line 2288, in _do_async_install guest.start_install(meter=meter) file /usr/share/virt-manager/virtinst/guest.py, line 461, in start_install doboot, transient) file /usr/share/virt-manager/virtinst/guest.py, line 396, in _create_guest self.domain = self.conn.createxml(install_xml or final_xml, 0) file /usr/lib/python2.7/dist-packages/libvirt.py, line 3777, in createxml if ret is none:raise libvirterror('virdomaincreatexml() failed', conn=self)libvirterror: cannot access storage file '/media/user/mnt/generic.qcow2' (as uid:121, gid:131): permission deniedi mounted the usb using the following command:sudo mount -t ext4 /dev/sdb1 /media/user/mnt -o rwi have also tried to chmod 777 on the mount point while the usb was mounted as well as on the storage file itself (generic.qcow2).further i changed the owner of the mount point to libvirt-qemu (uid=121) however the error still persists.how can i provide the appropriate permissions to be able to install the os?",
        "present_kp": [
            "permissions",
            "virtual machine",
            "kvm"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "kvm",
                "usb",
                "storage",
                "debian",
                "virtual machine manager",
                "permission error",
                "libvirt"
            ],
            "absent_kp": [
                "ext4",
                "qcow2",
                "mount",
                "chmod",
                "libvirt-qemu"
            ]
        }
    },
    {
        "text": "limiting number of processes by name. is it possible to limit number of processes for a given group or user using the process name? eg. i'd like to groups remotes have only 5 simultaneous ssh processes that are run on my server.i don't see any options in pam_limit (i can only limit number of process per user or group, regardless of process name) and i don't see ability in cgroups.do you have any ideas how to accomplish this? (script in cron is not an answer for me :))",
        "present_kp": [
            "process",
            "limit"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "processes",
                "process name"
            ],
            "absent_kp": [
                "cgroups",
                "pam_limit"
            ]
        }
    },
    {
        "text": "iptables nat on debian openvz. so i want to create a nat rule for an openvpn server.after getting trouble with the tap/tun devices, it's finally working i think.now i have to make a nat rule like so : >iptables -t nat -a postrouting -s 172.16.0.0/24 -o venet0:1 -j masquerade iptables v1.4.14: can't initialize iptables table 'nat': table does not exist (do you need to insmod?)perhaps iptables or your kernel needs to be upgraded.but it doesn't work. i searched a lot and found another commandiptables -t nat -a prerouting -i tun0 -j dnat --to-destination 5.135.###.###this command does the same result as the previous one.don't know what to do.i ask the host to enable nat but he tell me that i have to do it on my own.",
        "present_kp": [
            "debian",
            "iptables",
            "nat",
            "openvz"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "iptables",
                "nat",
                "debian",
                "openvz"
            ],
            "absent_kp": [
                "openvpn",
                "kernel",
                "insmod"
            ]
        }
    },
    {
        "text": "switching to a virtual terminal is slow. when switching to a virtual terminal using e.g. ctrl+alt+f2, it takes about a second to switch. not too horrible for something that's typically rarely used, but i would like to use it more and it's substantially slower than, say, alt+tab. it's particularly weird since switching back to the desktop environment (alt+f7 for me) is instant.i've noticed it before, but currently i'm running debian testing (stretch) with cinnamon 2.8.7 on x 1.18.3. the resolution of the virtual terminal is the same as x's resolution.what is this delay caused by and how can i improve it?",
        "present_kp": [],
        "absent_kp": [
            "console"
        ],
        "predict": {
            "present_kp": [
                "virtual terminal",
                "debian",
                "cinnamon"
            ],
            "absent_kp": [
                "linux",
                "performance optimization"
            ]
        }
    },
    {
        "text": "reference for dudley's chaining integral. dudley's chaining integral is commonly used to bound rademacher complexities. i recall seeing several papers give this as the reference@article{mr512411, author = {dudley, r. m.}, title = {central limit theorems for empirical measures}, journal = {ann. probab.}, year = {1978}, volume = {6}, pages = {899--929 (1979)}, number = {6}, coden = {apbyae}, fjournal = {the annals of probability}, issn = {<phone>}, mrclass = {60f05 (28c20 60b10 60f17)}, mrnumber = {mr512411 (81k:60029a)}, mrreviewer = {p. r{'e}v{'e}sz}}but i don't think the result in question actually appears in that paper. could anyone point me to the definitive reference?",
        "present_kp": [],
        "absent_kp": [
            "reference request",
            "machine learning",
            "lg.learning"
        ],
        "predict": {
            "present_kp": [
                "dudley's chaining integral",
                "rademacher complexities"
            ],
            "absent_kp": [
                "central limit theorems",
                "empirical measures"
            ]
        }
    },
    {
        "text": "requesting user input while reading file line by line. for class i need to write a bash script that will take the output from ispell and when i try and request user input inside the while loop it just saves the next line of the file as the user input.how could i go about requesting user input in the while loop?#!/bin/bash#returns the misspelled words#ispell -l < file#define varsispell_output_file=output.tmp;input_file=$1ispell -l < $input_file > $ispell_output_file;#echo a new line for give space between command#and the output generatedecho ;while read line;do echo '$line' is misspelled. press enter to keep; read -p this spelling, or type a correction here: user_input; if [ $user_input != ] then echo input: $user_input; fi echo ; #echo a new linedone < $ispell_output_file;rm $ispell_output_file;",
        "present_kp": [
            "bash",
            "user input"
        ],
        "absent_kp": [
            "shell script",
            "control flow"
        ],
        "predict": {
            "present_kp": [
                "bash script",
                "user input",
                "while loop"
            ],
            "absent_kp": [
                "file handling",
                "ispell"
            ]
        }
    },
    {
        "text": "is there any good/fundamental reason that python classvars, and javascript prototype inheritance, don't mutate the parent on assignment?. in python, if you have a classvar, it's accessible from an instance, but if you set the variable on the instance it doesn't actually change the classvar, rather it assigns a new name which shadows the parent value:>>> class foo:... classvar = 10...>>> f = foo()>>> (foo.classvar, f.classvar)(10, 10)>>> f.classvar = 30>>> (foo.classvar, f.classvar)(10, 30)>>> foo.classvar = 9>>> (foo.classvar, f.classvar, foo().classvar)(9, 30, 9)this is exactly akin to prototype inheritance in javascript:> var proto = {x: 10};> function bar() { }> bar.prototype = proto;> var b = new bar(); > [proto.x, b.x][10, 10]> b.x = 30> [proto.x, b.x][10, 30]> proto.x = 9> [proto.x, b.x, (new bar()).x][9, 30, 9]of course, if the child mutates the value, then it is seen in the parent, because the variable was not re-assigned and so no shadowing occurred:>>> class foo:... classvar = [10]...>>> f = foo(); f.classvar[10]>>> f.classvar[0] = 30>>> (foo.classvar, f.classvar)([30], [30])my question is: is there any good reason for this? it seems like it would be less of a gotcha if the assign semantics were assign on any parent if the value exists there, otherwise create the new value.i ask because i'm designing my own language and, as i get to choose what the semantics are, i'm wondering if i should break from the herd and do what seems like less of a gotcha.",
        "present_kp": [
            "design",
            "javascript",
            "python",
            "semantics"
        ],
        "absent_kp": [
            "programming languages"
        ],
        "predict": {
            "present_kp": [
                "python",
                "javascript",
                "prototype inheritance",
                "classvar"
            ],
            "absent_kp": [
                "language design",
                "assignment semantics"
            ]
        }
    },
    {
        "text": "easier way to dig into docs. i would to know a way to dig into vim and plugin doc easily. i saw mentions about doc ctrlp documentation or commandt.i would like easier documentation discovery for functions, motions, commands i don't know.thanks",
        "present_kp": [],
        "absent_kp": [
            "help system",
            "plugin ctrlp"
        ],
        "predict": {
            "present_kp": [
                "vim",
                "plugin doc"
            ],
            "absent_kp": [
                "documentation discovery",
                "functions",
                "motions",
                "commands"
            ]
        }
    },
    {
        "text": "hg:command not found. i am trying to clone a repository from bitbucket, and i installed mercurial, and copied the https command. when i run this command i still get hg: command not found, why is this?",
        "present_kp": [
            "mercurial"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "mercurial",
                "bitbucket"
            ],
            "absent_kp": [
                "repository",
                "clone"
            ]
        }
    },
    {
        "text": "can't log out of facebook. for two days, we have not been able to log out of our facebook account. if we cannot log out the normal way, how can we do this?",
        "present_kp": [
            "facebook"
        ],
        "absent_kp": [
            "security"
        ],
        "predict": {
            "present_kp": [
                "facebook",
                "log out"
            ],
            "absent_kp": [
                "social media",
                "account management"
            ]
        }
    },
    {
        "text": "how do you encode algebraic data types in a c#- or java-like language?. there are some problems which are easily solved by algebraic data types, for example a list type can be very succinctly expressed as:data conslist a = empty | conscell a (conslist a)consmap f empty = emptyconsmap f (conscell a b) = conscell (f a) (consmap f b)l = conscell 1 (conscell 2 (conscell 3 empty))consmap (+1) lthis particular example is in haskell, but it would be similar in other languages with native support for algebraic data types.it turns out that there is an obvious mapping to oo-style subtyping: the datatype becomes an abstract base class and every data constructor becomes a concrete subclass. here's an example in scala:sealed abstract class conslist[+t] { def map[u](f: t => u): conslist[u]}object empty extends conslist[nothing] { override def map[u](f: nothing => u) = this}final class conscell[t](first: t, rest: conslist[t]) extends conslist[t] { override def map[u](f: t => u) = new conscell(f(first), rest.map(f))}val l = (new conscell(1, new conscell(2, new conscell(3, empty)))l.map(1+)the only thing needed beyond naive subclassing is a way to seal classes, i.e. a way to make it impossible to add subclasses to a hierarchy.how would you approach this problem in a language like c# or java? the two stumbling blocks i found when trying to use algebraic data types in c# were:i couldn't figure out what the bottom type is called in c# (i.e. i couldn't figure out what to put into class empty : conslist< ??? >)i couldn't figure out a way to seal conslist so that no subclasses can be added to the hierarchywhat would be the most idiomatic way to implement algebraic data types in c# and/or java? or, if it isn't possible, what would be the idiomatic replacement?",
        "present_kp": [
            "java",
            "c#",
            "scala",
            "haskell",
            "algebraic data type"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "algebraic data types",
                "c#",
                "java"
            ],
            "absent_kp": [
                "haskell",
                "scala",
                "subtyping"
            ]
        }
    },
    {
        "text": "how to represent oop concepts in algorithms in a standard way?. i have usually been using the cormen algorithm format to teach some introductory courses in programming. i mean something like this:treesearch(k,n)1. if x==nil or k==x.key2. return x3. if k<x.key4. return treesearch(k.left,n)5. else return treesearch(k.right,n)actually i have not agree with a couple of lecturers in my institution that they insist to put the type of the variable that they are using in the algorithm. i mean, to do that, will it not be to make a bias toward the programming language and not to focus on the algorithm? for example what would happen if the student grab other programming language, like r or python, that really do not care about the type of variable.the other issue that i have is how to represent oop algorithms in a nice algorithmic way. for example when i make a constructor should i put something like:class: carattributes: wheelsconstructor car()or something likeclass: carfunction car()also when i come to the part of inheritance, one of my colleages put the word super() to define inheritance in an algorithmic way, but again i think that is too java-way to do this part. usually they teach in that way because the practical part is made in java, but again i think that the algorithm should be more freely, directly towards the logic, and not to an specific programming language.does anybody knows some standard to represent algorithms for oop?",
        "present_kp": [
            "algorithms"
        ],
        "absent_kp": [
            "terminology",
            "education",
            "object oriented",
            "didactics"
        ],
        "predict": {
            "present_kp": [
                "oop",
                "algorithms"
            ],
            "absent_kp": [
                "cormen algorithm",
                "java",
                "python",
                "inheritance"
            ]
        }
    },
    {
        "text": "filter output of command by color. i am running a utility that doesn't offer a way to filter its output. nothing in the text of the output indicates that a particular function failed but it does show in red. the output is so long that at the end when it reports some # of errors i can't always scroll to see the output where the error occurred.how can i filter out non-red text?pseudo code:dolongtask | grep -color rededitthe command outputs other colors as well and i need to be able to filter out all text that isn't red. also the text coloring is multiline.",
        "present_kp": [
            "grep",
            "colors",
            "text",
            "filter"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "filter output",
                "color"
            ],
            "absent_kp": [
                "command line",
                "text processing"
            ]
        }
    },
    {
        "text": "bash globbing variable substitution?. possible duplicate:batch renaming files i want to rename files using their existing name as a base for the new one.so if i can ls these files withls blue*+(.png)i'd want to rename them something likemv blue$(*)+(.png) $(1).pngexcept that doesn't work obviously. is there syntax for these kind of variables in bash globbing or is there an easier way?",
        "present_kp": [
            "bash",
            "rename",
            "variable substitution"
        ],
        "absent_kp": [
            "shell",
            "wildcards"
        ],
        "predict": {
            "present_kp": [
                "bash",
                "globbing",
                "variable substitution"
            ],
            "absent_kp": [
                "file renaming",
                "batch processing"
            ]
        }
    },
    {
        "text": "warshall's algorithm for transitive closure. i was going through this code for implementing warshall's algorithm. i think the time complexity for this simple problem is huge because there are too many loops running here. the time complexity for this code should be \\$o(n^3)\\$. is there a way to optimize this code so that the time complexity can be reduced a bit?#include<stdio.h>#include<unistd.h>#include<math.h>int maximum(int,int);void warshal(int p[10][10],int n){int i,j,k;for(i=1;i<=n;i++) for(j=1;j<=n;j++) for(k=1;k<=n;k++) p[i][j]=maximum(p[i][j],p[i][k]&&p[k][j]);}int maximum(int a,int b){ ;if(a>b)return(a);elsereturn(b);}void main(){int p[10][10]={0},n,e,u,v,i,j; printf( enter the number of vertices:); scanf(%d,&n); printf( input values now ); for(i=1;i<=n;i++) for(j=1;j<=n;j++) scanf(%d,&p[i][j]); printf( matrix of input data: ); for(i=1;i<=n;i++) { for(j=1;j<=n;j++) printf(%d ,p[i][j]); printf( ); } warshal(p,n); printf( transitive closure: ); for(i=1;i<=n;i++) { for(j=1;j<=n;j++) printf(%d ,p[i][j]); printf( ); } }",
        "present_kp": [
            "c"
        ],
        "absent_kp": [
            "performance",
            "beginner"
        ],
        "predict": {
            "present_kp": [
                "warshall's algorithm",
                "transitive closure",
                "time complexity"
            ],
            "absent_kp": [
                "graph theory",
                "optimization"
            ]
        }
    },
    {
        "text": "what makes a large and complex software product slow?. for a reason that is largely irrelevant, i installed delphi 7 once again in such a long time. i have to say, i was completely blown away - in a way i haven't been for rather a while. this is not how i remember things at all. the installation took around 30 seconds. launching it took 2 seconds, and it was immediately usable. i can press run the second after it started, and less than a second later the blank program is already visible and running. hurray for computers getting so much faster!but the reason i've been blown away like this is because usually i use visual studio 2010, that doesn't feel snappy like this at all. granted, delphi 7 is a much smaller system than visual studio 2010, but it does have the appearance of having all the really necessary things there: a control palette, a form designer, a code editor with code completion. i realise that the language might be simpler, and the code completion might be a lot less powerful, and the ide might not be nearly as extensible and feature-rich, but still: i do not understand how (i.e. through what mechanism) does having a lot of extra features (that i might not have even triggered yet) cause a system like visual studio to always feel sluggish in comparison.i would like to ask people experienced in working with systems the scale of visual studio: what is it that makes them slow? is it the layers upon layers of abstractions required to keep the codebase within the human comprehension capabilities? is it the sheer amount of code that needs to be run through? is it the modern tendency towards programmer-time-saving approaches at the (mindbogglingly huge) expense in the clock cycles / memory usage department?",
        "present_kp": [],
        "absent_kp": [
            "architecture",
            "performance"
        ],
        "predict": {
            "present_kp": [
                "software product",
                "visual studio",
                "delphi 7"
            ],
            "absent_kp": [
                "ide",
                "code completion",
                "programmer-time-saving approaches"
            ]
        }
    },
    {
        "text": "how iconify xterm when it loses focus?. i am attempting iconify a xterm terminal when it loses focus in fluxbox.i am following this wiki:<url> so far, i've got it appending this linecontrol t : if {matches (xterm)} {delay {iconify} 1}to ~/.fluxbox/keys file, which iconify the terminal when i press control t shortcut.how can i get that behavior when xterm loses the focus?",
        "present_kp": [
            "xterm",
            "fluxbox"
        ],
        "absent_kp": [
            "linux",
            "slackware"
        ],
        "predict": {
            "present_kp": [
                "xterm",
                "fluxbox"
            ],
            "absent_kp": [
                "terminal",
                "focus management"
            ]
        }
    },
    {
        "text": "not mounting nfs shared folders with vagrant libvirt provider in debian jessie. i have a problem with vm virtualization and vagrant libvirt provider for mounting nfa shared folders.with the help of @infernix in github, i was able to properly install the instance of libvirt but the problem now is to mount the shared directory with the vm.references to installation:<url> never see error in mount nfs with vagrant before. :o==> default: exporting nfs shared folders...==> default: preparing to edit /etc/exports. administrator privileges will be required... nfs-kernel-server.service - lsb: kernel nfs server supportloaded: loaded (/etc/init.d/nfs-kernel-server)active: active (exited) since sun 2016-12-18 20:21:10 brst; 15h ago=> default: mounting nfs shared folders...the following ssh command responded with a non-zero exit status.vagrant assumes that this means the command failed!mount -o vers=3,udp 192.168.121.1:/home/tosystems/documents/projects /home/vagrant/vagrant_projectsresult=$?if test $result -eq 0; thenif test -x /sbin/initctl && command -v /sbin/init && /sbin/init 2>/dev/null --version | grep upstart; then /sbin/initctl emit --no-wait vagrant-mounted mountpoint=/home/vagrant/vagrant_rojectsfielse exit $resultfistdout from the command:stderr from the command:stdin: is not a ttymount.nfs: rpc.statd is not running but is required for remote locking.mount.nfs: either use '-o nolock' to keep locks local, or start statd.mount.nfs: an incorrect mount option was specified.i follow the @halosghost hints in link below, but i not succeeded.mount linux nfs. rpc.statd is not runningi changed the synced_folders parameter in vagrant file and added:config.vm.synced_folder ~/documents/projects, /home/vagrant/vagrant_projects, type: nfs, nfs_version: 4, nfs_udp: false, mount_options: [rw, vers=4, tcp]but output here yet:==> default: preparing to edit /etc/exports. administrator privileges will be required... nfs-kernel-server.service - lsb: kernel nfs server support loaded: loaded (/etc/init.d/nfs-kernel-server) active: active (exited) since sun 2016-12-18 20:21:10 brst; 19h ago==> default: mounting nfs shared folders...the following ssh command responded with a non-zero exit status.vagrant assumes that this means the command failed!mount -o vers=4,rw,vers=4,tcp 192.168.121.1:/home/tosystems/documents/projects /home/vagrant/vagrant_projectsresult=$?if test $result -eq 0; thenif test -x /sbin/initctl && command -v /sbin/init && /sbin/init 2>/dev/null --version | grep upstart; then/sbin/initctl emit --no-wait vagrant-mounted mountpoint=/home/vagrant/vagrant_projectsfielseexit $resultfistdout from the command:stderr from the command:stdin: is not a ttymount.nfs: connection timed outour efforts did not help in the end.. edit: uzing rsync: ==> default: rsyncing folder: /home/tosystems/ => /vagrantthere was an error when attempting to rsync a synced folder.please inspect the error message below for more info.host path: /home/tosystems/guest path: /vagrantcommand: rsync --verbose --archive --delete -z --copy-links --no-owner --no-group --rsync-path sudo rsync -e ssh -p 22 -o loglevel=fatal -o controlmaster=auto -o controlpath=/tmp/ssh.661 -o controlpersist=10m -o identitiesonly=yes -o stricthostkeychecking=no -o userknownhostsfile=/dev/null -i '/home/tosystems/.vagrant/machines/default/libvirt/private_key' --exclude .vagrant/ /home/tosystems/ <email>/vagranterror: symlink has no referent: /home/tosystems/.config/google-chrome/singletoncookiesymlink has no referent: /home/tosystems/.config/google-chrome/singletonlockrsync: write failed on /vagrant/.vagrant.d/boxes/ubuntu-amd64/0/libvirt/box.img: no space left on device (28)rsync error: error in file io (code 11) at receiver.c(393) [receiver=3.1.1]rsync: [sender] write error: broken pipe (32)please, i need you help.",
        "present_kp": [
            "debian",
            "nfs",
            "vagrant",
            "libvirt"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "nfs",
                "vagrant",
                "libvirt",
                "debian jessie"
            ],
            "absent_kp": [
                "virtual machine",
                "shared folders",
                "rsync"
            ]
        }
    },
    {
        "text": "installing video driver on arch linux. i have arch linux installed in console mode on an intel-pc machine. my task is to write and run an opengl display program on the machine to check whether it meets the following conditions:it would take at most 10% of total cpu usageit would take at most 20% of total ramhere are more info about the device:uname -a3.6.5-1-arch #1 smp preempt wed oct 31 .. x86_64 gnu/linuxcpu mhz: 1866.717ram: 2gbpreemptible: yeslspci -v | grep -i graphic*vga compatible controller: intel corporation mobile 4 series chipset integrated graphics controller (rev 07) (prog-if 00 [vga controller])(same for subsystem and graphics controller)now, what i want if to install the relevant graphics driver. however, i cannot do anything as it is run in console mode. it has no x window. i try alt+f8 but it does not start x window. also tried startx,but i get -bash:startx: command not foundcould anyone guide me how to install the graphics driver please. considering that i think the kernel is compiled in preemptive mode.",
        "present_kp": [
            "arch linux",
            "console",
            "graphics"
        ],
        "absent_kp": [
            "drivers"
        ],
        "predict": {
            "present_kp": [
                "arch linux",
                "video driver",
                "graphics driver"
            ],
            "absent_kp": [
                "opengl",
                "x window",
                "console mode"
            ]
        }
    },
    {
        "text": "bash - reading user variable into bash script grep. i've tried every possible combination to get this bash script working. it's part of a larger script, and it basically prompts for a username (to check if it exists) and returns the appropriate response:#! /bin/bash# script to see if user existsclearecho -n enter user to check: read $uzergrep -c '^${uzer}:' /etc/passwdif [ $? -eq 0 ]; then echo user does exist :)else echo no such userfiin terminal the following works fine:grep -c '^devuser1:' /etc/passwdreturns: 1grep -c '^devuser1234:' /etc/passwdreturns: 0i've tried many combinations of passing the read variable into '^${uzer}:' with no joy. any ideas what else i can try?",
        "present_kp": [
            "bash",
            "variable"
        ],
        "absent_kp": [
            "regular expression"
        ],
        "predict": {
            "present_kp": [
                "bash",
                "script",
                "user"
            ],
            "absent_kp": [
                "grep",
                "variable"
            ]
        }
    },
    {
        "text": "filtering the colored output of grep. i'm on osx, but i suspect this doesn't make a big difference for this question.in my .bash_profile, i aliased grep to get color outputs by default:alias grep='grep --color=always'i commonly run searches for content within files in my repositories in ways similar to:grep --include=*.cpp -ern . -e (foo|bar)but i often want to further refine the results, typically piping with say grep -v colorbar.the problem is that the second grep command then runs on the colored ouput, and doesn't seem to be able to match the exclusion patterns because of this.obviously i could run the first search without colors and then everything would work fine, but i would prefer to keep them if there is a way to work around this?",
        "present_kp": [
            "grep",
            "colors"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "grep",
                "color output"
            ],
            "absent_kp": [
                "osx",
                "bash",
                "filtering"
            ]
        }
    },
    {
        "text": "merge lines between keywords into one-line comma separated values. between first occurrence of cat to next occurrence of cat, it should create a separate line with delimiter as ,.file input as below.cataabbcccataa-1bb-1cc-1output expected:cat,aa,bb,cccat,aa-1,bb-1,cc-1",
        "present_kp": [],
        "absent_kp": [
            "text processing",
            "awk",
            "sed",
            "perl"
        ],
        "predict": {
            "present_kp": [
                "merge lines",
                "keywords",
                "comma separated values",
                "delimiter"
            ],
            "absent_kp": [
                "file processing",
                "text manipulation"
            ]
        }
    },
    {
        "text": "does duplicate content on another site affect my ranking?. a competitors set up a new site and copy and pasted some copy from our home page.when doing a comparison with a duplicate content tool the result was that the pages where 21% similiar.does the fact they have copied our content affect our site?their site has seen an increase in their ranking recently and ours ahs dropped slightly",
        "present_kp": [
            "content",
            "ranking"
        ],
        "absent_kp": [
            "seo",
            "google"
        ],
        "predict": {
            "present_kp": [
                "duplicate content",
                "ranking"
            ],
            "absent_kp": [
                "seo",
                "website traffic"
            ]
        }
    },
    {
        "text": "is there a google docs (or -spreadsheets) api to scroll the view?. if i want to scroll to a particular paragraph in google docs, or a particular cell in google spreadsheets, is there a way to do that via google apps script?",
        "present_kp": [
            "google spreadsheets"
        ],
        "absent_kp": [
            "google drive"
        ],
        "predict": {
            "present_kp": [
                "google docs",
                "google spreadsheets",
                "google apps script"
            ],
            "absent_kp": [
                "api",
                "scroll view"
            ]
        }
    },
    {
        "text": "faster way to perform function calculation in python?. i'm interested in whether there is a way to further improve a fast version of a function used in a homework assignment i received recently (i've already submitted the completed work).from math import logdef func_fast(mass, density): return sum(map((log(mass * density)).__truediv__, range(1,10001)))def func_slow(mass, density): total = 0.0 for i in range(10000): masslog = log(mass * density) total += masslog/(i+1) return totalmass = 2.5 density = 12.0the fast version times in around 2-2.5ish seconds while the slow version nets 6-7 seconds.",
        "present_kp": [
            "python"
        ],
        "absent_kp": [
            "performance",
            "python 3.x"
        ],
        "predict": {
            "present_kp": [
                "python",
                "function calculation"
            ],
            "absent_kp": [
                "performance optimization",
                "algorithm"
            ]
        }
    },
    {
        "text": "how to achieve root privilege in metasploitable 2 linux?. suppose, i have just entered the metasploitable 2 linux like the following command:username : msfadminpassword : msfadminnow, i need to gain 'root' privilege so that i do not need to use 'sudo' - command again and again. for example, in order to shutdown the machine i just want to type:shutdown -h 1not,sudo shutdown -h 1how to do that?",
        "present_kp": [
            "metasploit"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "metasploitable 2 linux",
                "root privilege"
            ],
            "absent_kp": [
                "sudo",
                "linux security"
            ]
        }
    },
    {
        "text": "pf blocks all in/out traffic instead of just the one port i wanted to block. i need to block one incoming port with pf. i'm new to pf, and i can't figure out what i'm doing wrong here.here is my entire rule file, made to block incoming port 22:set block-policy droppass in all keep statepass out all keep stateblock in proto tcp to port 22after i start pf with sudo /sbin/pfctl -e -f /path/to/my/rule/file, all my network traffic is blocked. i try to load a webpage, and it won't load until i do sudo /sbin/pfctl -d to disable pf.if i remove the fourth line (block in proto tcp to port 22) from my rule list, nothing is blocked. so what did i do wrong on the fourth line that is causing it to block everything instead of just incoming tcp port 22? all the examples did this similarly.if it matters, my os is os x 10.8.5.",
        "present_kp": [
            "pf"
        ],
        "absent_kp": [
            "osx",
            "firewall"
        ],
        "predict": {
            "present_kp": [
                "pf",
                "port",
                "block incoming port"
            ],
            "absent_kp": [
                "firewall",
                "network traffic",
                "os x"
            ]
        }
    },
    {
        "text": "show only weekends in google calendar. is there a way to only display weekends in google calendar and not the week days?",
        "present_kp": [
            "google calendar"
        ],
        "absent_kp": [],
        "predict": {
            "present_kp": [
                "google calendar",
                "weekends"
            ],
            "absent_kp": [
                "week days"
            ]
        }
    }
]